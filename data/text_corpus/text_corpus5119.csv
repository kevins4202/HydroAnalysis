index,text
25595,flood simulation with two dimensional hydrodynamic models is subject to different sources of uncertainties in model configuration boundary conditions and model parametrization hec ras 2d is a widely used hydrodynamic model here we assess the sensitivity of the hec ras 2d to its configuration factors and parameters we evaluate the impacts of different model configuration factors including the floodplain and channel roughness coefficients terrain and mesh size as well as river boundary conditions on the dynamic of water levels maximum water level and flood extent and determine the importance of these contributing factors for reliable flood inundation modeling using both variogram and variance based analyses for our case study we found that depending on the perturbation scale the performance measure and the predicted output the sensitivity to input factors changes while for simulation of water level dynamics the dem and mesh resolutions are the most important factors for flood extent mapping the floodplain roughness coefficient and the upstream boundary condition are the key factors in addition our analyses indicated that for a reliable simulation of maximum flood water level modelers need to spend resources on the calibration of floodplain roughness coefficient while using fine dem and mesh resolutions here we also investigate the role of computational time interval and mesh resolution on the model s run time and accuracy our results suggested that reducing the computational time interval has minimum impact as it increases the model run time without much improving the model accuracy keywords flood inundation mapping global sensitivity analysis hec ras 2d hurricane harvey 1 introduction the human threatening nature of flooding and its costly impacts across the globe ahmadalipour and moradkhani 2019 khajehei et al 2020 kundzewicz 2019 together with the high frequency of floods with an increasing trend in the united states alipour et al 2020b boustan et al 2020 mallakpour and villarini 2015 emphasize the necessity for further and deeper investigations of flood risk analysis and inundation modeling apollonio et al 2021 darabi et al 2019 mojaddadi et al 2017 razavi et al 2020 numerous studies have been conducted globally to assess the risk of flooding and proposed plans for flood risk mitigation ahmadisharaf et al 2016 alipour et al 2020a costache 2019 de moel et al 2009 di baldassarre et al 2020 karimiziarani et al 2022 marsooli et al 2016 oddo et al 2020 a critical step for flood risk mapping and management is to use hydrodynamic models abebe et al 2019 ahmadisharaf et al 2018 li et al 2019 patel et al 2017 vojtek et al 2019 wang et al 2019 wu et al 2017 yin et al 2016 the hydrologic engineering center s river analysis system hec ras is one of the most commonly used hydrodynamic models in the united states it offers a wide range of analyses including one dimensional steady state one and two dimensional unsteady state simulations sediment transport and water temperature water quality modeling brunner 2016 joshi et al 2019 used the hec ras model to study the characteristics of sediment transported within maumee river in ohio garcia et al 2020 employed the hec ras 2d model for a combined hydrologic and hydraulic simulation and reservoir operations in west harris county texas during hurricane harvey 2017 studies have shown that 2d hydrodynamic models outperform 1d models when a detailed simulation of flood wave propagation is required tayefi et al 2007 werner et al 2005 the hec ras 2d has a user friendly environment compared to its one dimensional version though computationally demanding due to the attractive interface of this model for 2d simulation of river hydrodynamics and its high capacity for supporting different model configurations hec ras 2d has been widely used for flood inundation modeling in recent years farid et al 2017 farooq et al 2019 moya quiroga et al 2016 rangari et al 2019 surwase et al 2019 the hydrodynamic modeling is subject to different sources of uncertainties originating from the model structure parameters and boundary conditions these uncertain components can significantly affect the reliability and accuracy of the model performance the model structure uncertainty accounts for the imperfection of underlying equations numerical schemes of a hydrodynamic model while simulating the physical processes involved in a river system the main hydrodynamic model parameters are the river bathymetry and surface roughness coefficients khanarmuei et al 2020 the choice of roughness coefficient values for floodplain and channel in addition to the dem resolution and accuracy alters the hydrodynamic model outputs pappenberger et al 2006 furthermore the boundary conditions of a hydrodynamic model are the flow or water stage hydrographs that are derived from either streamflow gauges or hydrologic modeling in either case the hydrographs are associated with uncertainty abbaszadeh et al 2019 harmel et al 2006 the uncertainty of hydrodynamic models has been the focus of many studies in the literature chen 2012 papaioannou et al 2016 rathod and manekar 2020 thompson et al 2008 global sensitivity analysis gsa techniques are commonly used to quantify the impacts of model parameters forcings and boundary conditions on the model response and evaluate their relative importance baroni and francke 2020 şalap ayça et al 2021 song et al 2015 wu et al 2012 there are a variety of sa approaches that focus on different characteristic properties of the underlying response surfaces which lead to differences in the assessment of sensitivity borgonovo 2007 hornberger and spear 1981 liu et al 2005 pianosi et al 2015 most well established gsa methods rely on either an analysis of variances or an analysis of partial derivatives campolongo et al 2007 homma and saltelli 1996 sobol and kucherenko 2009 one of the commonly used variance based approaches is the sobol method sobol 1990 hall et al 2005 used the sobol method to investigate the role of upstream boundary condition flows the elevation of land surface channel bed channel width as well as channel and floodplain roughness coefficients in flood extent maps generated by the lisflood fp hydrodynamic model later savage et al 2016 applied the sobol gsa approach to explore the sensitivity of different lisflood fp model outputs including flood extent water depth and time of inundation to the channel and floodplain roughness coefficients boundary condition flows the spatial resolution of the model and dem resolution they used this method and evaluated the model performance based on 52 500 input combinations although the variance based gsa methods have been used over a wide range of applications reflecting nonlinear processes and the effects of interactions among variables hall et al 2009 they are unable to distinguish between response surfaces that have identical variance but different structures kucherenko et al 2012 oakley and o hagan 2004 razavi and gupta 2015 emulation techniques and data driven approaches are used to reduce the computational burden of gsa in emulation techniques gsa is performed on an emulator surrogate model that is trained using a set of available original model runs bomers et al 2019 marseguerra et al 2003 although cost effective they are only an approximation of the original model and are unreliable at unsampled regions outside of the training data set the data driven approaches drive the sensitivity metrics directly from a set of sample points input output data that contain enough information representing the system s behavior borgonovo et al 2017 li and mahadevan 2016 to employ this method the input output data for gsa may require sample points or observations that are well distributed across the factor space which enables exploring a full spectrum of the system s behavior sheikholeslami et al 2021 razavi and gupta 2016a proposed a new sensitivity analysis technique named variogram analysis of response surfaces vars vars is a computationally efficient and statistically robust method compared to its counterparts that provide stable estimates even for high dimensional response surfaces using a relatively small number of points sampled on the response surface becker 2020 puy et al 2020 2021 they demonstrated that derivative based e g morris and variance based e g sobol gsa methods ignore the spatially ordered structure of the response surface and their results can be misleading for many cases to overcome the limitation of these gsa methods they introduced vars that evaluates the sensitivity of the response surface across the full spectrum of perturbation scales guillaume et al 2019 haghnegahdar and razavi 2017 they also showed that derivative and variance based methods are two extreme realizations of vars when the scale factor approaches zero and infinity respectively therefore the vars method bridges the gap between these two gsa methods and provides additional information for other scales recently the linux version of hec ras 2d has become available this feature provides a new opportunity to run the model on a high performance computing hpc cluster and perform computationally intensive simulations the growing attention for 2d simulation of floods with hec ras 2d in the past years suggests an increasing demand for using this model in future studies to properly meet this demand a detailed investigation of the hec ras 2d sensitivity to its parameters inputs and boundary conditions is highly desired however up until recently this model could be only run on windows systems which is a major limitation for sensitivity analysis that needs extensive model simulations therefore the previous studies picked only one or two factors and defined a few model simulation scenarios for instance liu et al 2019 evaluated the sensitivity of hec ras 2d to different river geometries and surface roughness characterization they performed model simulations for 224 scenarios and concluded that although hec ras 2d is sensitive to channel roughness it is insensitive to floodplain roughness similarly yalcin 2020 assessed the sensitivity of hec ras 2d to different factors including terrain resolution roughness layer resolution and the computational mesh size using 19 different model simulations for the urban floodplain of kilicozu creek in turkey considering the recent hpc feature of hec ras 2d and the need for a better understanding of this model behavior this study aims to conduct a comprehensive sensitivity analysis of hec ras 2d and evaluate the importance of contributing factors for reliable flood modeling for this purpose we are targeting the sensitivity of different performance metrics that represent three model outputs including the dynamic of water levels flood extents maps and maximum water levels considering the advantages of vars we use this technique to evaluate the sensitivity of these model outputs to channel and floodplain roughness coefficients upstream boundary condition computational mesh size and dem resolution across the spectrum of scales in addition we use the sobol sensitivity analysis method to compare the two methods results on the system s behavior next we evaluate the model run time and the accuracy of the hec ras 2d based on different configurations of computational mesh size and time interval the overarching goal of this study is to address the following objectives 1 characterizing the behavior of three hydrodynamic model response surfaces i e water level dynamics maximum water level and the extent of flooding to the variability of five model factors i e channel roughness coefficient floodplain roughness coefficient dem resolution mesh resolution and inflow 2 identifying those factors that are more less influential in the hydrodynamic model response variability 3 investigating the relation between the model run time and accuracy considering different mesh sizes and computational time intervals the remainder of this paper is structured as follows section 2 explains the study area and the data section 3 illustrates the models and the framework used in this study the results and discussions are included in sections 4 and 5 respectively the findings of the study are summarized in the final section 2 study area and datasets hurricane harvey made landfall on texas and louisiana in august 2017 causing catastrophic flooding and considerable casualties this hurricane was the second costliest tropical cyclone of the united states history that caused more than 125 billion dollar damage song et al 2020 here we conduct our study over a flooded river stream in southeast texas eastern part of san jacinto basin that was highly affected by the torrential rainfall of hurricane harvey san jacinto is the main river in the region which flows across montgomery county to the south and forms lake houston the topography of the san jacinto basin is slightly hilly except for floodplain areas along the river the climate of the region is warm and humid and has an average annual rainfall and temperature of 1295 4 mm and 19 63 c respectively fig 1 shows the study area along with the stream networks and usgs streamflow gauges and 29 hurricane harvey high water marks 2 1 nldas 2 forcing data phase 2 of the north american land data assimilation system nldas 2 forcing data provides quality controlled and spatiotemporally consistent datasets from best available observations at 1 8 about 12 km spatial resolution and hourly temporal resolution from january 1979 to the present xia et al 2012 the nldas 2 forcing dataset including incoming shortwave radiation incoming longwave radiation specific humidity air temperature surface pressure near surface wind in two directions and precipitation rate were used by abbaszadeh et al 2020 to run the wrf hydro weather research and forecasting model hydrological modeling system model and generate streamflow predictions across the san jacinto watershed here we use the streamflow simulations provided by abbaszadeh et al 2020 as the model upstream flow boundary condition 2 2 ned data the national elevation dataset ned is a raster product of digital elevation models dems provided by the usgs this dataset is available at approximately 30 m for the entire united states and 10 and 3 m for some parts of the country here we use usgs 1 3 arc second dem approximately 10 m and resample it for larger dem resolutions 2 3 nlcd data national land cover database nlcd is generated through the collaboration of different federal agencies it provides the land cover information for the entire us and puerto rico the dataset is updated every five years and the latest iteration of this product is nlcd 2016 which is used in this study as the reference map to locate the floodplains 2 4 usgs streamflow gauges in this study we use two usgs gauges see fig 1 to assess the hec ras 2d model performance see section 3 9 for more detail usgs station 08070000 e fk san jacinto rv nr cleveland with a drainage area of 841 746 square kilometers and 08070200 e fk san jacinto rv nr new caney with a drainage area of 1004 92square kilometers are located in san jacinto river in san jacinto watershed the streamflow hydrographs at both gauges show that the study area has been heavily affected by hurricane harvey https webapps usgs gov harvey 2 5 inundation maps and high water marks in the immediate aftermath of the hurricane harvey flood induced event the usgs and federal emergency management agency fema initiated a cooperative study to evaluate the magnitude of flooding and map the extent of the flood in texas they analyzed seventy four usgs streamflow stations and surveyed 2123 high water marks watson et al 2018 using this information they performed geospatial analyses and generated maximum flood extent maps this study considers the usgs flood extent maps and available high water marks across the study region as a reference to evaluate the accuracy of flood extent maps simulated by the hec ras 2d 3 methodology in this study the upstream flow boundary conditions provided by the wrf hydro are fed into the hec ras 2d to predict the spatiotemporal distribution of water depth and inundation area the vars and sobol techniques are used to explore the sensitivity of hec ras 2d to different factors each of these components is explicitly described in the following subsections 3 1 wrf hydro hydrologic model the weather research and forecasting model hydrological modeling system wrf hydro is an open source community model recently developed by the national center for atmospheric research ncar for a range of studies including flood flash flood prediction regional hydroclimate assessment and water resources management gochis and chen 2003 the wrf hydro modeling system is not a singular model in fact it is a modeling architecture that couples multiple hydrological processes representations it is described as a group of modules and functions that couples atmospheric components to a set of land surface hydrology components gochis et al 2015 wrf hydro is a fully distributed model that uses different hydrological and hydraulic modules to simulate the surface overland flow subsurface saturated flow channel routing and baseflow processes wrf hydro has been applied in several recent studies arnault et al 2016 lahmers et al 2019 wehbe et al 2019 since there are not any gauges providing upstream flows for the river we use the wrf hydro streamflow predictions provided by abbaszadeh et al 2020 across the region for the period of hurricane harvey abbaszadeh et al 2020 generated 90 wrf hydro simulations across the region by taking into account the uncertainties in the meteorological forcing hydrologic model parameters and initial conditions recent studies have thoroughly discussed how the uncertainties associated with the meteorological forcings i e nldas 2 along with hydrologic model parameters and hydrodynamic modeling are quantified and reduced within the data assimilation framework abbaszadeh et al 2020 jafarzadegan et al 2021a muñoz et al 2022 for more information about the implementation of this model its calibration and the simulation process we refer the interested readers to this article 3 2 hec ras 2d hydrodynamic model hec ras 2d is the recent product of the united states army corps of engineers hydrologic engineering center the program performs two dimensional unsteady flow routing through either the full saint venant or the diffusion wave equations the software uses an implicit finite volume algorithm to solve the 2d unsteady flow equations the model uses computational meshes 2d area which contain a mixture of cells with different shapes and sizes the cell faces edges do not need to have a single elevation instead each computational cell is based on the details of the underlying dem usace 2015 this feature improves the efficiency of hec ras 2d modeling as the model is run over a large mesh size while it still generates accurate simulations owing to the use of fine resolution dems in addition to the areal and terrain data the hec ras 2d model needs boundary condition information two types of boundary conditions can be defined inside the model the internal and external boundary conditions the internal boundary conditions are optional and allow the user to add flow within a river reach while the external boundary is required to run an unsteady model the external boundary condition can be a flow hydrograph stage hydrograph normal depth rating curve or precipitation the normal depth and rating curve can be only used where flow leaves a 2d flow area while flow and stage hydrograph boundary conditions can be used either as inflow or outflow the precipitation option allows the user to apply rainfall excess directly to the 2d flow area finally to set the hec ras 2d model roughness parameters a single roughness coefficient can be assigned to the 2d area or distributed roughness coefficient values can be defined using an imported land classification map 3 3 variogram based sensitivity indices vars the vars sensitivity method is based on the variogram analysis variograms can be used to characterize the spatial pattern and variability of the model response surface across the factor space it is defined as the variance of the differences between model response values at pairs of points sampled from the factor space razavi and gupta 2016a used the variogram concept and proposed the vars method for sensitivity analysis of environmental and earth system models in this method the higher values of the variogram for a given constant distance scale represent the higher sensitivity of the model response to the factor of interest in the vars method the sensitivity metrics are estimated by integrating the variograms over any scale of interest ivars the ivars10 ivars30 and ivars50 are three recommended sensitivity metrics that correspond to 10 30 and 50 of the factor range respectively razavi and gupta 2016b designed a star based sampling strategy for the vars method which facilitates the computation of sensitivity indices to sample the star points first an n dimensional space is defined where each dimension represents a normalized factor then a set of points representing the star centers are randomly selected from the factor space in this method δ h represents the smallest distance value in the factor space for each center point we generate a cross section of points that are equally apart δ h along with each of the n dimensions of the factor space this results in n 1 δ h 1 new points for each star center the vars sensitivity indices are then calculated over the response surface of the generated points 3 4 variance based sensitivity indices sobol the variance based sensitivity analysis assesses the relative impacts of each factor on the variance of the model outputs saltelli 2008 in general there are two variance sensitivity indices the first order and the total order indices the first index estimates the direct influence of the individual factor on the output variance while the second one measures the total effect from both individual variations and the interactions with other factors on the output variance each index can be calculated using equations 1 and 2 1 s i v x i e x i y x i v y 2 s t i 1 v x i e x i y x i v y where x i is the i th input factor x i denotes the matrix of all factors but x i y is the model output e is the expected value and v is the variance saltelli et al 2010 the variance based total order effect is a by product of the vars framework in this study we use vars tool to employ the variance based total order effect of the sobol technique over the same sample space which results in further investigation into the sensitivity of the hec ras 2d model since the first order indices estimate the fractional contribution of the individual factor to the output variance their sum explains to what extent model factors are individually important and the remainder to one 1 s i implies the interaction effects here in addition to the total order effects we estimate the interaction effect to further learn the system s behavior 3 5 the proposed research framework in this study we utilize both variogram based vars and variance based sobol sensitivity analysis approaches to assess the impacts of dem resolution channel roughness coefficient floodplain roughness coefficient upstream boundary condition input flows and mesh size on the hec ras 2d model response and evaluate their relative importance fig 2 shows the schematic framework of the study part 1 illustrates the sensitivity analysis steps and part 2 displays the relation of accuracy and efficiency for finding the optimum configuration point in the hec ras 2d model table 1 shows the range and the unit of different factors used in this study following the previous studies we evaluated the hec ras 2d model using channel roughness coefficients with values between 0 02 and 0 1 s m 1 3 aronica et al 1998 2002 di baldassarre et al 2009 since using a distributed floodplain roughness usually adds no improvement to the model performance liu et al 2019 savage et al 2016 werner et al 2005 here we employ a unique floodplain roughness coefficient in each model ranging from 0 025 to 0 2 s m 1 3 di baldassarre et al 2009 therefore we utilize the nlcd map of the areas and assign a unique value to all the floodplain land cover categories to ensure that the river channel falls exactly on the main channel a shapefile that contains polygons and roughness of the rivers is used to override the land cover map in this study similar to other studies on sensitivity analysis of flood inundation models dottori et al 2013 hall et al 2005 pappenberger et al 2008 savage et al 2016 we use a 10 m dem and resample it to the larger scales up to 90 m the 30 and 90 m dems are also publicly available and widely used in many studies cook and merwade 2009 horritt and bates 2002 hu et al 2017 to explore the effect of mesh size we select different cell sizes according to table 1 as there is no usgs gauge neither at the upstream of the river nor at the tributaries we used the ensemble streamflow predictions generated by the wrf hydro model given the inherent model structural uncertainty and also the extreme precipitation during hurricane harvey as an outlier in model forcing even after careful calibration of the hydrologic model the model outputs remain biased to estimate the bias we calculate the difference between the downstream streamflow observation and the model prediction at gauge 08070200 jafarzadegan et al 2021b then we distribute the hydrologic bias value to the upstream boundary condition values based on the ratio of their streamflow value to the total input flows we calculate the difference between the total upstream flows and the downstream flow both derived from the hydrologic model as the amount of deficit that represents lateral flows and vertical fluxes applied to the main channel although the amount of deficit can be negligible during minor moderate floods in extreme cases such as hurricane harvey it is an effective boundary condition component that should be taken into account jafarzadegan et al 2021b we employed 90 wrf hydro streamflow simulations developed by abbaszadeh et al 2020 to calculate streamflow percentiles for defining the upstream boundary condition we used 5 and 95 streamflow percentiles as the upstream flow condition lower and upper bound respectively fig 3 shows the hydrographs used in our sensitivity analysis after identification of factor space all contributing factors are individually rescaled between zero and one we use δ h 0 1 as the smallest distance value between a pair of points we randomly select 100 model configurations as the star centers of the star sampling approach and then extract 4600 model configurations from all the possible configuration scenarios using the vars toolbox the red dashed arrows shown in fig 2 all the model configurations are set up and run on the high performance computing hpc cluster in order to evaluate the model outputs the simulated time series of water stages are compared with the observed values at two usgs gauges using two deterministic performance measures root mean square error rmse and kling gupta efficiency kge we also use rmse to evaluate the accuracy of simulated maximum water stages at usgs high water marks the rmse of usgs gauges represents the temporal error at specific points and explains how the model simulates the flood dynamics on the other hand the rmse of high water marks measures the spatial error of the model in simulation of the maximum flood water level over the domain and represents the performance of model simulations for flood inundation mapping rmse measures the difference between the predicted and observed values as follows 3 r m s e i 1 n o i p i 2 n where o i and p i are observed data and predicted data respectively n is the number of data kge was proposed by gupta et al 2009 to evaluate the performance of environmental models it measures the euclidean distance in a 3 dimensional space between the ideal point 1 1 1 and the pearson product moment correlation coefficient r relative variability α and ratio of mean β using the following equation 4 k g e 1 r 1 2 α 1 2 β 1 2 w h e r e α σ p σ o a n d β μ p μ o where σ p μ p and σ o μ p are the standard deviation and mean value of the simulated and observed variables respectively moreover the flood extent maps simulated by the hec ras 2d are compared with the reference maps provided by the geospatial analyses on the available high water marks and usgs gauges observations in the region for the period of hurricane harvey here we perform a binary comparison between the reference and simulated maps using the fit f performance measure presented in eq 5 5 f t r u e p o s i t i v e i n s t a n c e s t o t a l p o s i t i v e s f a l s e p o s i t i v e s 100 where true positive is flood cells predicted correctly false positives are nonflood cells predicted as flood total positives are the number of flood cells in the reference map jafarzadegan and merwade 2017 f index shows the goodness of overlap of the simulated and reference flood maps it quantifies both the underprediction and overprediction of the simulated flood extent maps sangwan and merwade 2015 since our sensitivity analysis methods are based on statistical sampling it is necessary to quantify the degree of confidence one can place in the results and their reliability bootstrapping method is a commonly used approach to quantify the confidence level of statistical estimates efron 1979 hesterberg 2011 sheikholeslami et al 2017 wehrens et al 2000 in the bootstrapping method we generate samples from the original sample point with replacement and estimate the statistical metrics based on the new samples repeating this process results in a distribution of the metrics since these new samples are driven from the original sample no additional model run is necessary this makes bootstrapping a very computationally efficient approach therefore here we use the method of statistical bootstrapping available in vars tool and assess the levels of confidence and reliability for all the sensitivity metrics where the reliability is calculated as the fraction of times among all bootstrap attempts that the factor sensitivity ranks are the same as the original sensitivity ranks driven from the original sample set in addition to the reliability analyses another vars tool feature is monitoring the convergence of the results such that reporting the estimates of factor sensitivities and rankings as the sample size increases we also setup multiple model configurations with different mesh resolutions and computational time intervals while keeping other factors unchanged this analysis considers the relation between model accuracy and efficiency and provides the optimum configuration point of the hec ras 2d step two in fig 2 the selected mesh sizes are 20 40 60 80 and 100 m along with time intervals of 5 10 15 20 and 30 s which collectively result in 25 scenarios 4 results the results are discussed in two subsections below section 4 1 presents the results of the sensitivity analysis of the hec ras 2d model using both vars and sobol methods and section 4 2 explains the relation between accuracy and efficiency of this model 4 1 sensitivity analysis of hec ras 2d here we use a star based sampling strategy and select 4600 hec ras 2d model configurations after running all the model simulations we assess model outputs using four different performance measures kge and rmse are used to compare the predicted water levels dynamics at both gauge locations along the river and f statistics is used to evaluate the predicted flood extent maps rmse is again used to show the overall bias between usgs high water marks and maximum flood level simulations fig 4 shows the distributions of these performance measures fig 4a and d depict the distribution of kge at both gauges and fig 4b e and 4f show the distribution of rmses in addition subplot c displays the distribution of the f index all the results show that the overall model performances are reasonable and mostly fall within an acceptable range the values of kge and rmse at usgs gauges indicate that the model performs better at gauge number 08070000 compared to gauge 08070200 in this study vars and sobol global sensitivity analyses are used to identify the impacts of model parameters configuration and boundary conditions on the model responses as explained earlier in section 3 3 the vars method is based on the analysis of a variogram that provides sensitivity information at different scales the smallest scale value used here is 0 1 the smaller values of scale h represent the local sensitivity of the model response to the factors while larger h values represent the sensitivity at larger scales based on this method a higher value of the variogram γ h i indicates higher sensitivity of the underlying response surface in the direction of i t h factor at the scale represented by h i fig 5 a b 5c and 5d represent the directional variograms corresponding to average rmses and average kges at two gauges as well as f and rmse at high water marks respectively in these figures some directional variograms are not monotonic and also some cross each other these show the structure of the response surface and indicate how different perturbation scales can yield different results fig 5a and b shows that dem resolution is the most sensitive factor at all scales for simulation of water levels dynamics although both indices evaluate the water level time series predictions at the observation gauges their behavior is slightly different because kge represents the correlation bias and variance between the simulated and observed data simultaneously considering both rmse and kge the local sensitivity of the hec ras model performance to boundary condition channel and floodplain roughness coefficients are relatively similar while at larger scales the floodplain roughness coefficient is more important than the other two factors fig 5c displays the directional variograms of f and illustrates the importance of contributing factors for the simulation of flood extent this index shows a different sensitivity ordering compared to rmse and kge based on this figure the boundary condition and floodplain roughness coefficient are equally ranked as the most important factors at smaller scales h 0 2 however at the large scale the floodplain roughness completely dominates the other factors fig 5d shows the directional variograms corresponding to rmse at high water marks while f measure only represents the flood extent this metric can be an indicator of both the maximum water level and the flood extent simultaneously this figure indicates that mesh resolution is the most important factor at all scales at small scales dem resolution stands as the second most important factor while at larger scales the floodplain roughness coefficient exceeds the dem resolution and is found to be comparable to the mesh resolution fig 6 shows a comparison between the vars sensitivity metrics namely ivars10 ivars30 ivas50 and sobol metric for different performance measures the ratio of factor sensitivity is the value of each metric divided by the summed values of that metric over all the factors fig 6 also shows the 90 percent confidence interval of the obtained sensitivity metrics representing the degree of uncertainty in the results using bootstrap resampling the wider interval is an indication of higher uncertainty in the results considering kge and rmse at usgs gauges fig 6a and b all three ivars10 ivars30 and ivars50 metrics identify dem resolution as the most important factor which is also in agreement with the sobol assessment fig 6a and b also show that mesh resolution has a higher impact on the water level dynamics prediction compared to the channel roughness coefficients floodplain roughness coefficients and upstream flow boundary condition there are some differences between the ranking based on rmse and kge according to the rmse there is a relatively smaller difference between the sensitivity of factors compared to the kge also unlike kge when rmse is used the upstream boundary condition is more important than the channel roughness coefficient in addition both rmse and kge plots show that the roughness coefficient factors are less or equally ranked compared to the boundary condition at the local scale ivars10 while moving toward larger scales they dominate the boundary condition factor based on the obtained confidence intervals the result of rmse sensitivity has more uncertainty compared to kge for the majority of factors the results from rmse indicate that the confidence interval associated with the dem for the sobol index is wider than the other sensitivity indices it is also noted that if we had used a different performance measure it is expected that depending on the nature and structure of the performance measure we would have seen slightly different results considering the f index fig 6c floodplain roughness coefficient and upstream boundary condition are the most sensitive factors for flood extent mapping according to all the sensitivity metrics for the smaller scale values the sensitivity of these two factors is comparable however as the scale value increases the sensitivity of the floodplain roughness coefficient rises significantly similar to kge and rmse analysis there is an inconsistency in the ranking of the factors using the f index based on f analysis mesh resolution is more important than terrain resolution moreover according to the f the floodplain roughness coefficient is less important than the boundary condition at the small scale analysis ivars10 while at medium large scale ivasr30 ivars50 floodplain roughness coefficient is more important considering the f statistics confidence intervals in general the uncertainty in the f sensitivity analysis is higher than the other measures particularly in ivars10 results according to rmse at high water marks mesh resolution is the most important factor considering all the vars sensitivity metrics and the sobol method although ivars10 shows less sensitivity to dem resolution ivars50 larger scale metric and sobol method demonstrate the importance of both floodplain roughness coefficient and dem resolution this result and what we have learned in the behavior of the f measure indicated that we cannot rely on a specific perturbation scale and should consider the full spectrum of perturbations scale for factor ranking identification similar to the kge results the confidence intervals are on average narrow for rmse at high water marks which implies lower uncertainty in the results overall both figs 4 and 5 demonstrate that depending on the purpose of modeling the rank of sensitive factors is completely different the dem resolution and mesh resolution are the most sensitive factors when the simulation of water level dynamics at gauges is the main purpose the floodplain roughness and the upstream river boundary conditions have the highest impact on the flood extent mapping respectively overall considering rmse at high water marks as a measure of both flood extent and flood level and summarizing the results of other three measures rmse at gauges kge and f mesh resolution dem resolution and floodplain roughness coefficient are found to be the three most influencing factors for flood inundation mapping in general the uncertainty associated with the ivars50 values remains low for all the metrics unlike the other sensitivity indices in addition the similarity of ivars50 and sobol confirms the fact that the vars results at the large scale approach the sobol results the primary interest of this study is identifying the most influential factors on hec ras 2d model output variability to learn more about the robustness and reliability of the obtained factor rankings we illustrated how the factor rankings change as the sample size increases and also estimated the reliability that one can place on the result fig 7 the first four rows in fig 7 show the convergence result of each factor ranking considering each sensitivity index and model output the bottom bar charts show the percentage of reliability of the final factor ranking the reliability is calculated as the percentage of times among 1000 bootstrap resamplings that the sensitivity ranks of the factors were identical to ranks obtained by the original sample based on the results rmse at high water marks factor ranking seems very stable and does not change as sample size increases we can also see high reliability in the final factor ranking for rmse at high water marks on the other hand the result of ranking for the f index is less reliable and robust overall both ivars50 and sobol show the highest robustness and reliability in the most influential factors the results also indicate that ivars50 is the most robust and reliable sensitivity metric compared to its counterparts using the variance based first order indices we can estimate the fraction contribution of interaction effects on the variance of the model outputs table 2 shows the first order indices main effects of different model factors used in this study based on the results the sum of the main effects the total fractional contribution of the individual factors to the output variance is close to one which indicates interactions do not play a significant role in the variability of the model outputs and thus the hec ras 2d model response can be considered approximately as an additive function of the factors 4 2 the optimum model configuration to explore the relation between the accuracy and model run time of hec ras 2d multiple model setups are defined here the accuracy is calculated by two performance measures namely f and kge floodplain roughness coefficient channel roughness coefficient and upstream boundary condition are set as equal for all simulations the entire model configurations are run with 16 cores with 3 80 ghz maximum frequency fig 8 shows the results of the comparison between the model run time and accuracy the shape and color of the marker represent the computational mesh size and time interval respectively the figure shows that although using the finer computational time interval reduces the model run time it usually does not affect the model accuracy changes in the model outputs are negligible in most of the scenarios this suggests that the modelers should pick the highest time interval that still simulates the flooding river with no significant error however changing the mesh resolution affects both model accuracy and model run time considering the f index we can see that the output range is very narrow and as we lower the mesh size the model performance slightly improves however this improvement might result in a significant computation time for example in our case study if we reduce the mesh size from 100 m to 20 m the f index can improve 0 38 percent while it significantly increases the execution time 30 times and results in highly inefficient flood inundation simulation this trades off reveals that the optimum configuration point that considers both accuracy and model run time should be chosen similarly considering the kge fig 8b shows that reducing the mesh size will mostly improve the model accuracy our result shows that selecting the mesh size of 20 m yields the highest kge values although very close to the result of using 40 and 60 m mesh sizes in general depending on the priority of accuracy vs model run time the optimum configuration points can be either the 30 20 or 30 40 while the former is slightly more accurate and takes less time to run it should be noted that the suggested model configuration points are specific for this study area and may vary for other sites with different topography and land cover characteristics however using a similar approach is highly recommended as it can improve the overall performance of flood inundation modeling 5 discussion an important decision that usually flood inundation modelers deal with is how to prioritize resources computational storage time for reliable and efficient flood risk simulations knowledge of the model s behavior enables us to understand the extent to which defining the model s factors affects the output of interest in this study we used multiple sets of simulations to evaluate the impact of different factors on the hec ras 2d model performance as mentioned earlier the vars method is based on the variogram analysis of the response surface across a full range of perturbation scales using variogram based analysis vars is advantageous compared to conventional sensitivity analysis methods namely derivative based morris and variance based sobol methods because 1 the vars method accounts for the response surface structure by evaluating the sensitivity of the model response surface across the full spectrum of scales and provides a more comprehensive understanding of the sensitivity on the other hand the derivative based and variance based approaches only reflect two specific realizations of the vars for very small and large scales respectively for example in this study the sobol method shows that floodplain roughness coefficient and upstream boundary condition are the most influential factors for flood extent mapping fig 6c and the impact of the rest of the model factors is much smaller however the vars method shows that the impact of mesh resolution and dem resolution are comparable with the former model factors when the perturbation scale is small therefore vars bridges the gap between these two conventional approaches by providing more information about the structures of the response surface razavi and gupta 2016a 2 the vars method is computationally more efficient compared to derivative based and variance based methods because it is based on the information contained in pairs of points rather than individual points using this efficient approach and applying a practically efficient sampling strategy named star based sampling we only implemented 4600 model simulations for assessing the sensitivity of hec ras 2d to five different factors we refer to the previous study by savage et al 2016 where they implemented 52000 model simulations to evaluate the sensitivity of lisflood fp by the sobol method since flood inundation modeling with hydrodynamic models is a computationally extensive task using vars which requires a much less number of model simulations is significantly advantageous for sensitivity analysis of hydrodynamic models especially for large scale domains our results also indicated the robustness and reliability of factor ranking using ivars50 compared to ivars 10 ivar30 and sobol total order index based on our analysis regardless of the perturbation scale dem resolution is the most important factor for predicting water level dynamics and is one of the three major factors that affect the flood inundation areas therefore depending on the interest in either the dynamic of inundation or the flood extent one should carefully consider the associated uncertainty with topography there are multiple methodologies that the modelers can adopt to have a better representation of small scale features in coarser dems and produce the best representation of topography fewtrell et al 2008 the importance of dem resolution has been highlighted by other studies conducted on the sensitivity of water level and inundation area to the hydrodynamic model factors for instance savage et al 2016 explained that the uncertainty introduced by resampling topographic data to coarser resolutions are more important for water depth predictions similarly oubennaceur et al 2019 found that the predicted water depth is more sensitive to topography data compared with the roughness coefficient and flow rate in addition muthusamy et al 2021 described the significant impact of dem resolution on the accuracy of predicted flood extent maps during the stimulation of flooding event caused by storm desmond in cockermounuth uk in 2015 using the hec ras 2d hydrodynamic model our results demonstrate that floodplain roughness coefficient and mesh resolution are two other important factors that need additional attention for maximum water level prediction this shows when a decision maker s concern is about the maximum floodwater level at specific locations dem resolution floodplain roughness coefficient and mesh resolution factors would be important in addition we showed that the uncertainty of the upstream boundary and floodplain roughness coefficients can have a significant impact on the flood extent prediction exceeding the importance of the other model parameters upstream condition in inundation models is often derived from either rating curves or hydrologic models which both involve considerable uncertainty moradkhani et al 2019 pelletier 2011 the high impact of inflow on the flood extent variability and the high level of uncertainty with inflow data indicates that modelers must consider the upstream boundary condition uncertainty when the extent of flooding is of interest the higher importance of floodplain roughness compared to the channel roughness is in contrast with the findings of other studies that showed a minimal contribution of floodplain roughness coefficient to flood extent prediction horritt 2006 liu et al 2019 savage et al 2016 this confirms the fact that sensitivity analysis depends on the model structure and the study area for instance hall et al 2005 and savage et al 2016 found that the channel roughness coefficient is more important than floodplain roughness for flood extent mapping however both of these studies have tested the sensitivity of the lisflood fp model which has a different model structure compared to the hec ras 2d moreover their investigations suggested that depending on the region of the study the relative sensitivity of the floodplain roughness coefficient can be different liu et al 2019 also found that the flood extents simulated by hec ras 2d are not that sensitive to the floodplain roughness however their results were based on a local sensitivity analysis that estimates the model response variation by changing the floodplain roughness values at a few discrete points on the other hand here we apply a comprehensive global sensitivity analysis with more than 4000 model configurations which provide a more robust sampling of the response surface according to our results the inundation area generated by the hec ras 2d is highly sensitive to the floodplain roughness coefficient due to the key role of this factor it is worth further investigating the floodplain roughness impacts on the accuracy and variance of hec ras 2d model results since we have assumed a uniform roughness coefficient across the floodplain we conduct a new experiment that evaluates the impacts of using distributed floodplain roughness coefficient on the results we set up 200 new model simulations where the floodplain roughness coefficient for each type of land use category is randomly selected from the range indicated in table 1 while keeping the rest of the model factors constant then we compare the variance and accuracy of new model outputs with a subset of our previous simulations that includes a range of different uniform floodplain roughness coefficients while other factors are identical to our new experiment fig 9 shows the distribution of performance measures for both randomly distributed and uniform floodplain roughness coefficient scenarios with yellow and red colors respectively based on this figure regardless of the types of the model output using a uniform floodplain roughness results in better or similar model performance compared to using distributed floodplain roughness this is consistent with the findings of liu et al 2019 who indicated that the result of using a unique floodplain manning s n value is either better or similar to the results of using distributed floodplain roughness comparison of two distributions also shows that using a uniform floodplain roughness leads to a larger variance of model outputs compared to the distributed floodplain roughness values this is because using unique values across the floodplain has a higher chance of generating extreme scenarios than when distributed floodplain roughness coefficients are used in the second part of the analysis we performed 25 new model simulations to investigate the relation between the model accuracy and run time our results suggested that for the prediction of both water levels and flood extent maps reducing the computation time interval has the minimum impact as it degrades the model efficiency without improving the model accuracy in addition selecting the optimum mesh size depends on the output of interest considering the water level prediction reducing the mesh size improves the model accuracy however when we use smaller mesh sizes 20 40 m the improvement rate decreases while the computational time of modeling significantly increases on the other hand for flood extent mapping using a smaller mesh size does not improve the accuracy therefore mesh resolution can be considered as a non influential factor on the prediction of flood extent maps the illustration of accuracy run time relation in fig 8 is beneficial as it provides user friendly guidelines for picking the optimum configuration points and can be generalized to different environmental models e g hydrologic and climate models in other fields our results demonstrate the role and importance of different factors that contributed to the generation of flood inundation maps via hec ras 2d hydrodynamic model it is worth mentioning that the findings of sensitivity analysis for flood simulation are highly dependent on the model structure and study area in addition it is impossible to find a unique solution indicating the most sensitive factors that are generalizable to different types of floods e g urban coastal fluvial and pluvial floods with different intensities extreme floods vs typical floods at different scales small medium and large scale therefore the findings of this study are exclusive to the hec ras 2d model and are more suitable for the simulation of extreme flood events at a medium scale dem 30 m future studies can focus on the sensitivity of the hec ras 2d model for other regions different types of floods and intensities at other scales as a side note since there is not a unique way to define sensitivity using different gsa methods other than those employed here might have led to a different factor ranking pappenberger et al 2008 while in this study we used random sampling which is extracted without considering the previously generated sample points using a different sampling strategy e g orthogonal latin hypercube sampling could better represent the real variability and reduce the inherent sampling error mckay et al 2000 owen 1992 in addition in this study we used four performance metrics to analyze the model sensitivity to its factors however these types of sensitivity analysis are more useful for model calibration and require some reference data which is one of their limitations therefore for future study one can focus on the model response directly i e the values of maximum water levels time of the inundation etc in general this information paves the way for better interpretation of hydrodynamic processes which results in a more efficient calibration and assimilation of the hec ras 2d model furthermore future studies conducted on the sensitivity analysis of hec ras 2d can also consider additional modeling factors such as the number and type of computational mesh different dem data sources the bathymetry and river geometry 6 summary and conclusion in this study we applied variogram and variance based gsa methods to assess the sensitivity of the hec ras 2d hydrodynamic model to the channel and floodplain roughness coefficients upstream boundary condition computational mesh size and dem resolution we evaluated the hec ras 2d model performance over the san jacinto river basin during the extensive flooding of hurricane harvey in 2017 using 4600 model configurations the response surfaces of the gsa approach used in this study are four performance measures of rmse at gauges kge rmse over the high water marks and f that represent the dynamic of water level maximum water level and flood extent simulations for our case study we found that depending on the scale of sensitivity the type of performance measure and predicted output the sensitivity to the various input factors changes for the simulation of water level dynamics the dem and mesh resolution are the most important factors respectively the floodplain roughness coefficient and the upstream boundary condition plays a key role in flood extent mapping finally this study demonstrates that hec ras 2d is more sensitive to mesh resolution floodplain roughness and dem resolution when the prediction of maximum water level is the main goal of simulations the fact that the results of sensitivity analysis vary depending on the model output target of simulation shows the complexity of flood modeling and indicates that it is unlikely to find a single factor to be the most influential for all types of model outputs our findings indicate that the sensitivity of hec ras 2d prediction is complex and the modelers should carefully prioritize the input factors depending on their target model output the results imply that regardless of the types of the model output using a uniform floodplain roughness results in better or similar model performance compared to using distributed floodplain roughness additionally in this study we investigated the relation between the accuracy and efficiency of hec ras 2d our findings indicated that the impact of computational time interval on the model accuracy is negligible and finer mesh resolution increases model accuracy while decreasing model efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments partial financial support for this study was provided by the usace contract a20 0545 001 we would like to acknowledge the data provided by the north american land data assimilation systems nldas 2 in addition we would like to express our appreciation to the resource management associates and hydrologic engineering center for providing us with the hec ras 2d linux version the authors declare no competing interests 
25595,flood simulation with two dimensional hydrodynamic models is subject to different sources of uncertainties in model configuration boundary conditions and model parametrization hec ras 2d is a widely used hydrodynamic model here we assess the sensitivity of the hec ras 2d to its configuration factors and parameters we evaluate the impacts of different model configuration factors including the floodplain and channel roughness coefficients terrain and mesh size as well as river boundary conditions on the dynamic of water levels maximum water level and flood extent and determine the importance of these contributing factors for reliable flood inundation modeling using both variogram and variance based analyses for our case study we found that depending on the perturbation scale the performance measure and the predicted output the sensitivity to input factors changes while for simulation of water level dynamics the dem and mesh resolutions are the most important factors for flood extent mapping the floodplain roughness coefficient and the upstream boundary condition are the key factors in addition our analyses indicated that for a reliable simulation of maximum flood water level modelers need to spend resources on the calibration of floodplain roughness coefficient while using fine dem and mesh resolutions here we also investigate the role of computational time interval and mesh resolution on the model s run time and accuracy our results suggested that reducing the computational time interval has minimum impact as it increases the model run time without much improving the model accuracy keywords flood inundation mapping global sensitivity analysis hec ras 2d hurricane harvey 1 introduction the human threatening nature of flooding and its costly impacts across the globe ahmadalipour and moradkhani 2019 khajehei et al 2020 kundzewicz 2019 together with the high frequency of floods with an increasing trend in the united states alipour et al 2020b boustan et al 2020 mallakpour and villarini 2015 emphasize the necessity for further and deeper investigations of flood risk analysis and inundation modeling apollonio et al 2021 darabi et al 2019 mojaddadi et al 2017 razavi et al 2020 numerous studies have been conducted globally to assess the risk of flooding and proposed plans for flood risk mitigation ahmadisharaf et al 2016 alipour et al 2020a costache 2019 de moel et al 2009 di baldassarre et al 2020 karimiziarani et al 2022 marsooli et al 2016 oddo et al 2020 a critical step for flood risk mapping and management is to use hydrodynamic models abebe et al 2019 ahmadisharaf et al 2018 li et al 2019 patel et al 2017 vojtek et al 2019 wang et al 2019 wu et al 2017 yin et al 2016 the hydrologic engineering center s river analysis system hec ras is one of the most commonly used hydrodynamic models in the united states it offers a wide range of analyses including one dimensional steady state one and two dimensional unsteady state simulations sediment transport and water temperature water quality modeling brunner 2016 joshi et al 2019 used the hec ras model to study the characteristics of sediment transported within maumee river in ohio garcia et al 2020 employed the hec ras 2d model for a combined hydrologic and hydraulic simulation and reservoir operations in west harris county texas during hurricane harvey 2017 studies have shown that 2d hydrodynamic models outperform 1d models when a detailed simulation of flood wave propagation is required tayefi et al 2007 werner et al 2005 the hec ras 2d has a user friendly environment compared to its one dimensional version though computationally demanding due to the attractive interface of this model for 2d simulation of river hydrodynamics and its high capacity for supporting different model configurations hec ras 2d has been widely used for flood inundation modeling in recent years farid et al 2017 farooq et al 2019 moya quiroga et al 2016 rangari et al 2019 surwase et al 2019 the hydrodynamic modeling is subject to different sources of uncertainties originating from the model structure parameters and boundary conditions these uncertain components can significantly affect the reliability and accuracy of the model performance the model structure uncertainty accounts for the imperfection of underlying equations numerical schemes of a hydrodynamic model while simulating the physical processes involved in a river system the main hydrodynamic model parameters are the river bathymetry and surface roughness coefficients khanarmuei et al 2020 the choice of roughness coefficient values for floodplain and channel in addition to the dem resolution and accuracy alters the hydrodynamic model outputs pappenberger et al 2006 furthermore the boundary conditions of a hydrodynamic model are the flow or water stage hydrographs that are derived from either streamflow gauges or hydrologic modeling in either case the hydrographs are associated with uncertainty abbaszadeh et al 2019 harmel et al 2006 the uncertainty of hydrodynamic models has been the focus of many studies in the literature chen 2012 papaioannou et al 2016 rathod and manekar 2020 thompson et al 2008 global sensitivity analysis gsa techniques are commonly used to quantify the impacts of model parameters forcings and boundary conditions on the model response and evaluate their relative importance baroni and francke 2020 şalap ayça et al 2021 song et al 2015 wu et al 2012 there are a variety of sa approaches that focus on different characteristic properties of the underlying response surfaces which lead to differences in the assessment of sensitivity borgonovo 2007 hornberger and spear 1981 liu et al 2005 pianosi et al 2015 most well established gsa methods rely on either an analysis of variances or an analysis of partial derivatives campolongo et al 2007 homma and saltelli 1996 sobol and kucherenko 2009 one of the commonly used variance based approaches is the sobol method sobol 1990 hall et al 2005 used the sobol method to investigate the role of upstream boundary condition flows the elevation of land surface channel bed channel width as well as channel and floodplain roughness coefficients in flood extent maps generated by the lisflood fp hydrodynamic model later savage et al 2016 applied the sobol gsa approach to explore the sensitivity of different lisflood fp model outputs including flood extent water depth and time of inundation to the channel and floodplain roughness coefficients boundary condition flows the spatial resolution of the model and dem resolution they used this method and evaluated the model performance based on 52 500 input combinations although the variance based gsa methods have been used over a wide range of applications reflecting nonlinear processes and the effects of interactions among variables hall et al 2009 they are unable to distinguish between response surfaces that have identical variance but different structures kucherenko et al 2012 oakley and o hagan 2004 razavi and gupta 2015 emulation techniques and data driven approaches are used to reduce the computational burden of gsa in emulation techniques gsa is performed on an emulator surrogate model that is trained using a set of available original model runs bomers et al 2019 marseguerra et al 2003 although cost effective they are only an approximation of the original model and are unreliable at unsampled regions outside of the training data set the data driven approaches drive the sensitivity metrics directly from a set of sample points input output data that contain enough information representing the system s behavior borgonovo et al 2017 li and mahadevan 2016 to employ this method the input output data for gsa may require sample points or observations that are well distributed across the factor space which enables exploring a full spectrum of the system s behavior sheikholeslami et al 2021 razavi and gupta 2016a proposed a new sensitivity analysis technique named variogram analysis of response surfaces vars vars is a computationally efficient and statistically robust method compared to its counterparts that provide stable estimates even for high dimensional response surfaces using a relatively small number of points sampled on the response surface becker 2020 puy et al 2020 2021 they demonstrated that derivative based e g morris and variance based e g sobol gsa methods ignore the spatially ordered structure of the response surface and their results can be misleading for many cases to overcome the limitation of these gsa methods they introduced vars that evaluates the sensitivity of the response surface across the full spectrum of perturbation scales guillaume et al 2019 haghnegahdar and razavi 2017 they also showed that derivative and variance based methods are two extreme realizations of vars when the scale factor approaches zero and infinity respectively therefore the vars method bridges the gap between these two gsa methods and provides additional information for other scales recently the linux version of hec ras 2d has become available this feature provides a new opportunity to run the model on a high performance computing hpc cluster and perform computationally intensive simulations the growing attention for 2d simulation of floods with hec ras 2d in the past years suggests an increasing demand for using this model in future studies to properly meet this demand a detailed investigation of the hec ras 2d sensitivity to its parameters inputs and boundary conditions is highly desired however up until recently this model could be only run on windows systems which is a major limitation for sensitivity analysis that needs extensive model simulations therefore the previous studies picked only one or two factors and defined a few model simulation scenarios for instance liu et al 2019 evaluated the sensitivity of hec ras 2d to different river geometries and surface roughness characterization they performed model simulations for 224 scenarios and concluded that although hec ras 2d is sensitive to channel roughness it is insensitive to floodplain roughness similarly yalcin 2020 assessed the sensitivity of hec ras 2d to different factors including terrain resolution roughness layer resolution and the computational mesh size using 19 different model simulations for the urban floodplain of kilicozu creek in turkey considering the recent hpc feature of hec ras 2d and the need for a better understanding of this model behavior this study aims to conduct a comprehensive sensitivity analysis of hec ras 2d and evaluate the importance of contributing factors for reliable flood modeling for this purpose we are targeting the sensitivity of different performance metrics that represent three model outputs including the dynamic of water levels flood extents maps and maximum water levels considering the advantages of vars we use this technique to evaluate the sensitivity of these model outputs to channel and floodplain roughness coefficients upstream boundary condition computational mesh size and dem resolution across the spectrum of scales in addition we use the sobol sensitivity analysis method to compare the two methods results on the system s behavior next we evaluate the model run time and the accuracy of the hec ras 2d based on different configurations of computational mesh size and time interval the overarching goal of this study is to address the following objectives 1 characterizing the behavior of three hydrodynamic model response surfaces i e water level dynamics maximum water level and the extent of flooding to the variability of five model factors i e channel roughness coefficient floodplain roughness coefficient dem resolution mesh resolution and inflow 2 identifying those factors that are more less influential in the hydrodynamic model response variability 3 investigating the relation between the model run time and accuracy considering different mesh sizes and computational time intervals the remainder of this paper is structured as follows section 2 explains the study area and the data section 3 illustrates the models and the framework used in this study the results and discussions are included in sections 4 and 5 respectively the findings of the study are summarized in the final section 2 study area and datasets hurricane harvey made landfall on texas and louisiana in august 2017 causing catastrophic flooding and considerable casualties this hurricane was the second costliest tropical cyclone of the united states history that caused more than 125 billion dollar damage song et al 2020 here we conduct our study over a flooded river stream in southeast texas eastern part of san jacinto basin that was highly affected by the torrential rainfall of hurricane harvey san jacinto is the main river in the region which flows across montgomery county to the south and forms lake houston the topography of the san jacinto basin is slightly hilly except for floodplain areas along the river the climate of the region is warm and humid and has an average annual rainfall and temperature of 1295 4 mm and 19 63 c respectively fig 1 shows the study area along with the stream networks and usgs streamflow gauges and 29 hurricane harvey high water marks 2 1 nldas 2 forcing data phase 2 of the north american land data assimilation system nldas 2 forcing data provides quality controlled and spatiotemporally consistent datasets from best available observations at 1 8 about 12 km spatial resolution and hourly temporal resolution from january 1979 to the present xia et al 2012 the nldas 2 forcing dataset including incoming shortwave radiation incoming longwave radiation specific humidity air temperature surface pressure near surface wind in two directions and precipitation rate were used by abbaszadeh et al 2020 to run the wrf hydro weather research and forecasting model hydrological modeling system model and generate streamflow predictions across the san jacinto watershed here we use the streamflow simulations provided by abbaszadeh et al 2020 as the model upstream flow boundary condition 2 2 ned data the national elevation dataset ned is a raster product of digital elevation models dems provided by the usgs this dataset is available at approximately 30 m for the entire united states and 10 and 3 m for some parts of the country here we use usgs 1 3 arc second dem approximately 10 m and resample it for larger dem resolutions 2 3 nlcd data national land cover database nlcd is generated through the collaboration of different federal agencies it provides the land cover information for the entire us and puerto rico the dataset is updated every five years and the latest iteration of this product is nlcd 2016 which is used in this study as the reference map to locate the floodplains 2 4 usgs streamflow gauges in this study we use two usgs gauges see fig 1 to assess the hec ras 2d model performance see section 3 9 for more detail usgs station 08070000 e fk san jacinto rv nr cleveland with a drainage area of 841 746 square kilometers and 08070200 e fk san jacinto rv nr new caney with a drainage area of 1004 92square kilometers are located in san jacinto river in san jacinto watershed the streamflow hydrographs at both gauges show that the study area has been heavily affected by hurricane harvey https webapps usgs gov harvey 2 5 inundation maps and high water marks in the immediate aftermath of the hurricane harvey flood induced event the usgs and federal emergency management agency fema initiated a cooperative study to evaluate the magnitude of flooding and map the extent of the flood in texas they analyzed seventy four usgs streamflow stations and surveyed 2123 high water marks watson et al 2018 using this information they performed geospatial analyses and generated maximum flood extent maps this study considers the usgs flood extent maps and available high water marks across the study region as a reference to evaluate the accuracy of flood extent maps simulated by the hec ras 2d 3 methodology in this study the upstream flow boundary conditions provided by the wrf hydro are fed into the hec ras 2d to predict the spatiotemporal distribution of water depth and inundation area the vars and sobol techniques are used to explore the sensitivity of hec ras 2d to different factors each of these components is explicitly described in the following subsections 3 1 wrf hydro hydrologic model the weather research and forecasting model hydrological modeling system wrf hydro is an open source community model recently developed by the national center for atmospheric research ncar for a range of studies including flood flash flood prediction regional hydroclimate assessment and water resources management gochis and chen 2003 the wrf hydro modeling system is not a singular model in fact it is a modeling architecture that couples multiple hydrological processes representations it is described as a group of modules and functions that couples atmospheric components to a set of land surface hydrology components gochis et al 2015 wrf hydro is a fully distributed model that uses different hydrological and hydraulic modules to simulate the surface overland flow subsurface saturated flow channel routing and baseflow processes wrf hydro has been applied in several recent studies arnault et al 2016 lahmers et al 2019 wehbe et al 2019 since there are not any gauges providing upstream flows for the river we use the wrf hydro streamflow predictions provided by abbaszadeh et al 2020 across the region for the period of hurricane harvey abbaszadeh et al 2020 generated 90 wrf hydro simulations across the region by taking into account the uncertainties in the meteorological forcing hydrologic model parameters and initial conditions recent studies have thoroughly discussed how the uncertainties associated with the meteorological forcings i e nldas 2 along with hydrologic model parameters and hydrodynamic modeling are quantified and reduced within the data assimilation framework abbaszadeh et al 2020 jafarzadegan et al 2021a muñoz et al 2022 for more information about the implementation of this model its calibration and the simulation process we refer the interested readers to this article 3 2 hec ras 2d hydrodynamic model hec ras 2d is the recent product of the united states army corps of engineers hydrologic engineering center the program performs two dimensional unsteady flow routing through either the full saint venant or the diffusion wave equations the software uses an implicit finite volume algorithm to solve the 2d unsteady flow equations the model uses computational meshes 2d area which contain a mixture of cells with different shapes and sizes the cell faces edges do not need to have a single elevation instead each computational cell is based on the details of the underlying dem usace 2015 this feature improves the efficiency of hec ras 2d modeling as the model is run over a large mesh size while it still generates accurate simulations owing to the use of fine resolution dems in addition to the areal and terrain data the hec ras 2d model needs boundary condition information two types of boundary conditions can be defined inside the model the internal and external boundary conditions the internal boundary conditions are optional and allow the user to add flow within a river reach while the external boundary is required to run an unsteady model the external boundary condition can be a flow hydrograph stage hydrograph normal depth rating curve or precipitation the normal depth and rating curve can be only used where flow leaves a 2d flow area while flow and stage hydrograph boundary conditions can be used either as inflow or outflow the precipitation option allows the user to apply rainfall excess directly to the 2d flow area finally to set the hec ras 2d model roughness parameters a single roughness coefficient can be assigned to the 2d area or distributed roughness coefficient values can be defined using an imported land classification map 3 3 variogram based sensitivity indices vars the vars sensitivity method is based on the variogram analysis variograms can be used to characterize the spatial pattern and variability of the model response surface across the factor space it is defined as the variance of the differences between model response values at pairs of points sampled from the factor space razavi and gupta 2016a used the variogram concept and proposed the vars method for sensitivity analysis of environmental and earth system models in this method the higher values of the variogram for a given constant distance scale represent the higher sensitivity of the model response to the factor of interest in the vars method the sensitivity metrics are estimated by integrating the variograms over any scale of interest ivars the ivars10 ivars30 and ivars50 are three recommended sensitivity metrics that correspond to 10 30 and 50 of the factor range respectively razavi and gupta 2016b designed a star based sampling strategy for the vars method which facilitates the computation of sensitivity indices to sample the star points first an n dimensional space is defined where each dimension represents a normalized factor then a set of points representing the star centers are randomly selected from the factor space in this method δ h represents the smallest distance value in the factor space for each center point we generate a cross section of points that are equally apart δ h along with each of the n dimensions of the factor space this results in n 1 δ h 1 new points for each star center the vars sensitivity indices are then calculated over the response surface of the generated points 3 4 variance based sensitivity indices sobol the variance based sensitivity analysis assesses the relative impacts of each factor on the variance of the model outputs saltelli 2008 in general there are two variance sensitivity indices the first order and the total order indices the first index estimates the direct influence of the individual factor on the output variance while the second one measures the total effect from both individual variations and the interactions with other factors on the output variance each index can be calculated using equations 1 and 2 1 s i v x i e x i y x i v y 2 s t i 1 v x i e x i y x i v y where x i is the i th input factor x i denotes the matrix of all factors but x i y is the model output e is the expected value and v is the variance saltelli et al 2010 the variance based total order effect is a by product of the vars framework in this study we use vars tool to employ the variance based total order effect of the sobol technique over the same sample space which results in further investigation into the sensitivity of the hec ras 2d model since the first order indices estimate the fractional contribution of the individual factor to the output variance their sum explains to what extent model factors are individually important and the remainder to one 1 s i implies the interaction effects here in addition to the total order effects we estimate the interaction effect to further learn the system s behavior 3 5 the proposed research framework in this study we utilize both variogram based vars and variance based sobol sensitivity analysis approaches to assess the impacts of dem resolution channel roughness coefficient floodplain roughness coefficient upstream boundary condition input flows and mesh size on the hec ras 2d model response and evaluate their relative importance fig 2 shows the schematic framework of the study part 1 illustrates the sensitivity analysis steps and part 2 displays the relation of accuracy and efficiency for finding the optimum configuration point in the hec ras 2d model table 1 shows the range and the unit of different factors used in this study following the previous studies we evaluated the hec ras 2d model using channel roughness coefficients with values between 0 02 and 0 1 s m 1 3 aronica et al 1998 2002 di baldassarre et al 2009 since using a distributed floodplain roughness usually adds no improvement to the model performance liu et al 2019 savage et al 2016 werner et al 2005 here we employ a unique floodplain roughness coefficient in each model ranging from 0 025 to 0 2 s m 1 3 di baldassarre et al 2009 therefore we utilize the nlcd map of the areas and assign a unique value to all the floodplain land cover categories to ensure that the river channel falls exactly on the main channel a shapefile that contains polygons and roughness of the rivers is used to override the land cover map in this study similar to other studies on sensitivity analysis of flood inundation models dottori et al 2013 hall et al 2005 pappenberger et al 2008 savage et al 2016 we use a 10 m dem and resample it to the larger scales up to 90 m the 30 and 90 m dems are also publicly available and widely used in many studies cook and merwade 2009 horritt and bates 2002 hu et al 2017 to explore the effect of mesh size we select different cell sizes according to table 1 as there is no usgs gauge neither at the upstream of the river nor at the tributaries we used the ensemble streamflow predictions generated by the wrf hydro model given the inherent model structural uncertainty and also the extreme precipitation during hurricane harvey as an outlier in model forcing even after careful calibration of the hydrologic model the model outputs remain biased to estimate the bias we calculate the difference between the downstream streamflow observation and the model prediction at gauge 08070200 jafarzadegan et al 2021b then we distribute the hydrologic bias value to the upstream boundary condition values based on the ratio of their streamflow value to the total input flows we calculate the difference between the total upstream flows and the downstream flow both derived from the hydrologic model as the amount of deficit that represents lateral flows and vertical fluxes applied to the main channel although the amount of deficit can be negligible during minor moderate floods in extreme cases such as hurricane harvey it is an effective boundary condition component that should be taken into account jafarzadegan et al 2021b we employed 90 wrf hydro streamflow simulations developed by abbaszadeh et al 2020 to calculate streamflow percentiles for defining the upstream boundary condition we used 5 and 95 streamflow percentiles as the upstream flow condition lower and upper bound respectively fig 3 shows the hydrographs used in our sensitivity analysis after identification of factor space all contributing factors are individually rescaled between zero and one we use δ h 0 1 as the smallest distance value between a pair of points we randomly select 100 model configurations as the star centers of the star sampling approach and then extract 4600 model configurations from all the possible configuration scenarios using the vars toolbox the red dashed arrows shown in fig 2 all the model configurations are set up and run on the high performance computing hpc cluster in order to evaluate the model outputs the simulated time series of water stages are compared with the observed values at two usgs gauges using two deterministic performance measures root mean square error rmse and kling gupta efficiency kge we also use rmse to evaluate the accuracy of simulated maximum water stages at usgs high water marks the rmse of usgs gauges represents the temporal error at specific points and explains how the model simulates the flood dynamics on the other hand the rmse of high water marks measures the spatial error of the model in simulation of the maximum flood water level over the domain and represents the performance of model simulations for flood inundation mapping rmse measures the difference between the predicted and observed values as follows 3 r m s e i 1 n o i p i 2 n where o i and p i are observed data and predicted data respectively n is the number of data kge was proposed by gupta et al 2009 to evaluate the performance of environmental models it measures the euclidean distance in a 3 dimensional space between the ideal point 1 1 1 and the pearson product moment correlation coefficient r relative variability α and ratio of mean β using the following equation 4 k g e 1 r 1 2 α 1 2 β 1 2 w h e r e α σ p σ o a n d β μ p μ o where σ p μ p and σ o μ p are the standard deviation and mean value of the simulated and observed variables respectively moreover the flood extent maps simulated by the hec ras 2d are compared with the reference maps provided by the geospatial analyses on the available high water marks and usgs gauges observations in the region for the period of hurricane harvey here we perform a binary comparison between the reference and simulated maps using the fit f performance measure presented in eq 5 5 f t r u e p o s i t i v e i n s t a n c e s t o t a l p o s i t i v e s f a l s e p o s i t i v e s 100 where true positive is flood cells predicted correctly false positives are nonflood cells predicted as flood total positives are the number of flood cells in the reference map jafarzadegan and merwade 2017 f index shows the goodness of overlap of the simulated and reference flood maps it quantifies both the underprediction and overprediction of the simulated flood extent maps sangwan and merwade 2015 since our sensitivity analysis methods are based on statistical sampling it is necessary to quantify the degree of confidence one can place in the results and their reliability bootstrapping method is a commonly used approach to quantify the confidence level of statistical estimates efron 1979 hesterberg 2011 sheikholeslami et al 2017 wehrens et al 2000 in the bootstrapping method we generate samples from the original sample point with replacement and estimate the statistical metrics based on the new samples repeating this process results in a distribution of the metrics since these new samples are driven from the original sample no additional model run is necessary this makes bootstrapping a very computationally efficient approach therefore here we use the method of statistical bootstrapping available in vars tool and assess the levels of confidence and reliability for all the sensitivity metrics where the reliability is calculated as the fraction of times among all bootstrap attempts that the factor sensitivity ranks are the same as the original sensitivity ranks driven from the original sample set in addition to the reliability analyses another vars tool feature is monitoring the convergence of the results such that reporting the estimates of factor sensitivities and rankings as the sample size increases we also setup multiple model configurations with different mesh resolutions and computational time intervals while keeping other factors unchanged this analysis considers the relation between model accuracy and efficiency and provides the optimum configuration point of the hec ras 2d step two in fig 2 the selected mesh sizes are 20 40 60 80 and 100 m along with time intervals of 5 10 15 20 and 30 s which collectively result in 25 scenarios 4 results the results are discussed in two subsections below section 4 1 presents the results of the sensitivity analysis of the hec ras 2d model using both vars and sobol methods and section 4 2 explains the relation between accuracy and efficiency of this model 4 1 sensitivity analysis of hec ras 2d here we use a star based sampling strategy and select 4600 hec ras 2d model configurations after running all the model simulations we assess model outputs using four different performance measures kge and rmse are used to compare the predicted water levels dynamics at both gauge locations along the river and f statistics is used to evaluate the predicted flood extent maps rmse is again used to show the overall bias between usgs high water marks and maximum flood level simulations fig 4 shows the distributions of these performance measures fig 4a and d depict the distribution of kge at both gauges and fig 4b e and 4f show the distribution of rmses in addition subplot c displays the distribution of the f index all the results show that the overall model performances are reasonable and mostly fall within an acceptable range the values of kge and rmse at usgs gauges indicate that the model performs better at gauge number 08070000 compared to gauge 08070200 in this study vars and sobol global sensitivity analyses are used to identify the impacts of model parameters configuration and boundary conditions on the model responses as explained earlier in section 3 3 the vars method is based on the analysis of a variogram that provides sensitivity information at different scales the smallest scale value used here is 0 1 the smaller values of scale h represent the local sensitivity of the model response to the factors while larger h values represent the sensitivity at larger scales based on this method a higher value of the variogram γ h i indicates higher sensitivity of the underlying response surface in the direction of i t h factor at the scale represented by h i fig 5 a b 5c and 5d represent the directional variograms corresponding to average rmses and average kges at two gauges as well as f and rmse at high water marks respectively in these figures some directional variograms are not monotonic and also some cross each other these show the structure of the response surface and indicate how different perturbation scales can yield different results fig 5a and b shows that dem resolution is the most sensitive factor at all scales for simulation of water levels dynamics although both indices evaluate the water level time series predictions at the observation gauges their behavior is slightly different because kge represents the correlation bias and variance between the simulated and observed data simultaneously considering both rmse and kge the local sensitivity of the hec ras model performance to boundary condition channel and floodplain roughness coefficients are relatively similar while at larger scales the floodplain roughness coefficient is more important than the other two factors fig 5c displays the directional variograms of f and illustrates the importance of contributing factors for the simulation of flood extent this index shows a different sensitivity ordering compared to rmse and kge based on this figure the boundary condition and floodplain roughness coefficient are equally ranked as the most important factors at smaller scales h 0 2 however at the large scale the floodplain roughness completely dominates the other factors fig 5d shows the directional variograms corresponding to rmse at high water marks while f measure only represents the flood extent this metric can be an indicator of both the maximum water level and the flood extent simultaneously this figure indicates that mesh resolution is the most important factor at all scales at small scales dem resolution stands as the second most important factor while at larger scales the floodplain roughness coefficient exceeds the dem resolution and is found to be comparable to the mesh resolution fig 6 shows a comparison between the vars sensitivity metrics namely ivars10 ivars30 ivas50 and sobol metric for different performance measures the ratio of factor sensitivity is the value of each metric divided by the summed values of that metric over all the factors fig 6 also shows the 90 percent confidence interval of the obtained sensitivity metrics representing the degree of uncertainty in the results using bootstrap resampling the wider interval is an indication of higher uncertainty in the results considering kge and rmse at usgs gauges fig 6a and b all three ivars10 ivars30 and ivars50 metrics identify dem resolution as the most important factor which is also in agreement with the sobol assessment fig 6a and b also show that mesh resolution has a higher impact on the water level dynamics prediction compared to the channel roughness coefficients floodplain roughness coefficients and upstream flow boundary condition there are some differences between the ranking based on rmse and kge according to the rmse there is a relatively smaller difference between the sensitivity of factors compared to the kge also unlike kge when rmse is used the upstream boundary condition is more important than the channel roughness coefficient in addition both rmse and kge plots show that the roughness coefficient factors are less or equally ranked compared to the boundary condition at the local scale ivars10 while moving toward larger scales they dominate the boundary condition factor based on the obtained confidence intervals the result of rmse sensitivity has more uncertainty compared to kge for the majority of factors the results from rmse indicate that the confidence interval associated with the dem for the sobol index is wider than the other sensitivity indices it is also noted that if we had used a different performance measure it is expected that depending on the nature and structure of the performance measure we would have seen slightly different results considering the f index fig 6c floodplain roughness coefficient and upstream boundary condition are the most sensitive factors for flood extent mapping according to all the sensitivity metrics for the smaller scale values the sensitivity of these two factors is comparable however as the scale value increases the sensitivity of the floodplain roughness coefficient rises significantly similar to kge and rmse analysis there is an inconsistency in the ranking of the factors using the f index based on f analysis mesh resolution is more important than terrain resolution moreover according to the f the floodplain roughness coefficient is less important than the boundary condition at the small scale analysis ivars10 while at medium large scale ivasr30 ivars50 floodplain roughness coefficient is more important considering the f statistics confidence intervals in general the uncertainty in the f sensitivity analysis is higher than the other measures particularly in ivars10 results according to rmse at high water marks mesh resolution is the most important factor considering all the vars sensitivity metrics and the sobol method although ivars10 shows less sensitivity to dem resolution ivars50 larger scale metric and sobol method demonstrate the importance of both floodplain roughness coefficient and dem resolution this result and what we have learned in the behavior of the f measure indicated that we cannot rely on a specific perturbation scale and should consider the full spectrum of perturbations scale for factor ranking identification similar to the kge results the confidence intervals are on average narrow for rmse at high water marks which implies lower uncertainty in the results overall both figs 4 and 5 demonstrate that depending on the purpose of modeling the rank of sensitive factors is completely different the dem resolution and mesh resolution are the most sensitive factors when the simulation of water level dynamics at gauges is the main purpose the floodplain roughness and the upstream river boundary conditions have the highest impact on the flood extent mapping respectively overall considering rmse at high water marks as a measure of both flood extent and flood level and summarizing the results of other three measures rmse at gauges kge and f mesh resolution dem resolution and floodplain roughness coefficient are found to be the three most influencing factors for flood inundation mapping in general the uncertainty associated with the ivars50 values remains low for all the metrics unlike the other sensitivity indices in addition the similarity of ivars50 and sobol confirms the fact that the vars results at the large scale approach the sobol results the primary interest of this study is identifying the most influential factors on hec ras 2d model output variability to learn more about the robustness and reliability of the obtained factor rankings we illustrated how the factor rankings change as the sample size increases and also estimated the reliability that one can place on the result fig 7 the first four rows in fig 7 show the convergence result of each factor ranking considering each sensitivity index and model output the bottom bar charts show the percentage of reliability of the final factor ranking the reliability is calculated as the percentage of times among 1000 bootstrap resamplings that the sensitivity ranks of the factors were identical to ranks obtained by the original sample based on the results rmse at high water marks factor ranking seems very stable and does not change as sample size increases we can also see high reliability in the final factor ranking for rmse at high water marks on the other hand the result of ranking for the f index is less reliable and robust overall both ivars50 and sobol show the highest robustness and reliability in the most influential factors the results also indicate that ivars50 is the most robust and reliable sensitivity metric compared to its counterparts using the variance based first order indices we can estimate the fraction contribution of interaction effects on the variance of the model outputs table 2 shows the first order indices main effects of different model factors used in this study based on the results the sum of the main effects the total fractional contribution of the individual factors to the output variance is close to one which indicates interactions do not play a significant role in the variability of the model outputs and thus the hec ras 2d model response can be considered approximately as an additive function of the factors 4 2 the optimum model configuration to explore the relation between the accuracy and model run time of hec ras 2d multiple model setups are defined here the accuracy is calculated by two performance measures namely f and kge floodplain roughness coefficient channel roughness coefficient and upstream boundary condition are set as equal for all simulations the entire model configurations are run with 16 cores with 3 80 ghz maximum frequency fig 8 shows the results of the comparison between the model run time and accuracy the shape and color of the marker represent the computational mesh size and time interval respectively the figure shows that although using the finer computational time interval reduces the model run time it usually does not affect the model accuracy changes in the model outputs are negligible in most of the scenarios this suggests that the modelers should pick the highest time interval that still simulates the flooding river with no significant error however changing the mesh resolution affects both model accuracy and model run time considering the f index we can see that the output range is very narrow and as we lower the mesh size the model performance slightly improves however this improvement might result in a significant computation time for example in our case study if we reduce the mesh size from 100 m to 20 m the f index can improve 0 38 percent while it significantly increases the execution time 30 times and results in highly inefficient flood inundation simulation this trades off reveals that the optimum configuration point that considers both accuracy and model run time should be chosen similarly considering the kge fig 8b shows that reducing the mesh size will mostly improve the model accuracy our result shows that selecting the mesh size of 20 m yields the highest kge values although very close to the result of using 40 and 60 m mesh sizes in general depending on the priority of accuracy vs model run time the optimum configuration points can be either the 30 20 or 30 40 while the former is slightly more accurate and takes less time to run it should be noted that the suggested model configuration points are specific for this study area and may vary for other sites with different topography and land cover characteristics however using a similar approach is highly recommended as it can improve the overall performance of flood inundation modeling 5 discussion an important decision that usually flood inundation modelers deal with is how to prioritize resources computational storage time for reliable and efficient flood risk simulations knowledge of the model s behavior enables us to understand the extent to which defining the model s factors affects the output of interest in this study we used multiple sets of simulations to evaluate the impact of different factors on the hec ras 2d model performance as mentioned earlier the vars method is based on the variogram analysis of the response surface across a full range of perturbation scales using variogram based analysis vars is advantageous compared to conventional sensitivity analysis methods namely derivative based morris and variance based sobol methods because 1 the vars method accounts for the response surface structure by evaluating the sensitivity of the model response surface across the full spectrum of scales and provides a more comprehensive understanding of the sensitivity on the other hand the derivative based and variance based approaches only reflect two specific realizations of the vars for very small and large scales respectively for example in this study the sobol method shows that floodplain roughness coefficient and upstream boundary condition are the most influential factors for flood extent mapping fig 6c and the impact of the rest of the model factors is much smaller however the vars method shows that the impact of mesh resolution and dem resolution are comparable with the former model factors when the perturbation scale is small therefore vars bridges the gap between these two conventional approaches by providing more information about the structures of the response surface razavi and gupta 2016a 2 the vars method is computationally more efficient compared to derivative based and variance based methods because it is based on the information contained in pairs of points rather than individual points using this efficient approach and applying a practically efficient sampling strategy named star based sampling we only implemented 4600 model simulations for assessing the sensitivity of hec ras 2d to five different factors we refer to the previous study by savage et al 2016 where they implemented 52000 model simulations to evaluate the sensitivity of lisflood fp by the sobol method since flood inundation modeling with hydrodynamic models is a computationally extensive task using vars which requires a much less number of model simulations is significantly advantageous for sensitivity analysis of hydrodynamic models especially for large scale domains our results also indicated the robustness and reliability of factor ranking using ivars50 compared to ivars 10 ivar30 and sobol total order index based on our analysis regardless of the perturbation scale dem resolution is the most important factor for predicting water level dynamics and is one of the three major factors that affect the flood inundation areas therefore depending on the interest in either the dynamic of inundation or the flood extent one should carefully consider the associated uncertainty with topography there are multiple methodologies that the modelers can adopt to have a better representation of small scale features in coarser dems and produce the best representation of topography fewtrell et al 2008 the importance of dem resolution has been highlighted by other studies conducted on the sensitivity of water level and inundation area to the hydrodynamic model factors for instance savage et al 2016 explained that the uncertainty introduced by resampling topographic data to coarser resolutions are more important for water depth predictions similarly oubennaceur et al 2019 found that the predicted water depth is more sensitive to topography data compared with the roughness coefficient and flow rate in addition muthusamy et al 2021 described the significant impact of dem resolution on the accuracy of predicted flood extent maps during the stimulation of flooding event caused by storm desmond in cockermounuth uk in 2015 using the hec ras 2d hydrodynamic model our results demonstrate that floodplain roughness coefficient and mesh resolution are two other important factors that need additional attention for maximum water level prediction this shows when a decision maker s concern is about the maximum floodwater level at specific locations dem resolution floodplain roughness coefficient and mesh resolution factors would be important in addition we showed that the uncertainty of the upstream boundary and floodplain roughness coefficients can have a significant impact on the flood extent prediction exceeding the importance of the other model parameters upstream condition in inundation models is often derived from either rating curves or hydrologic models which both involve considerable uncertainty moradkhani et al 2019 pelletier 2011 the high impact of inflow on the flood extent variability and the high level of uncertainty with inflow data indicates that modelers must consider the upstream boundary condition uncertainty when the extent of flooding is of interest the higher importance of floodplain roughness compared to the channel roughness is in contrast with the findings of other studies that showed a minimal contribution of floodplain roughness coefficient to flood extent prediction horritt 2006 liu et al 2019 savage et al 2016 this confirms the fact that sensitivity analysis depends on the model structure and the study area for instance hall et al 2005 and savage et al 2016 found that the channel roughness coefficient is more important than floodplain roughness for flood extent mapping however both of these studies have tested the sensitivity of the lisflood fp model which has a different model structure compared to the hec ras 2d moreover their investigations suggested that depending on the region of the study the relative sensitivity of the floodplain roughness coefficient can be different liu et al 2019 also found that the flood extents simulated by hec ras 2d are not that sensitive to the floodplain roughness however their results were based on a local sensitivity analysis that estimates the model response variation by changing the floodplain roughness values at a few discrete points on the other hand here we apply a comprehensive global sensitivity analysis with more than 4000 model configurations which provide a more robust sampling of the response surface according to our results the inundation area generated by the hec ras 2d is highly sensitive to the floodplain roughness coefficient due to the key role of this factor it is worth further investigating the floodplain roughness impacts on the accuracy and variance of hec ras 2d model results since we have assumed a uniform roughness coefficient across the floodplain we conduct a new experiment that evaluates the impacts of using distributed floodplain roughness coefficient on the results we set up 200 new model simulations where the floodplain roughness coefficient for each type of land use category is randomly selected from the range indicated in table 1 while keeping the rest of the model factors constant then we compare the variance and accuracy of new model outputs with a subset of our previous simulations that includes a range of different uniform floodplain roughness coefficients while other factors are identical to our new experiment fig 9 shows the distribution of performance measures for both randomly distributed and uniform floodplain roughness coefficient scenarios with yellow and red colors respectively based on this figure regardless of the types of the model output using a uniform floodplain roughness results in better or similar model performance compared to using distributed floodplain roughness this is consistent with the findings of liu et al 2019 who indicated that the result of using a unique floodplain manning s n value is either better or similar to the results of using distributed floodplain roughness comparison of two distributions also shows that using a uniform floodplain roughness leads to a larger variance of model outputs compared to the distributed floodplain roughness values this is because using unique values across the floodplain has a higher chance of generating extreme scenarios than when distributed floodplain roughness coefficients are used in the second part of the analysis we performed 25 new model simulations to investigate the relation between the model accuracy and run time our results suggested that for the prediction of both water levels and flood extent maps reducing the computation time interval has the minimum impact as it degrades the model efficiency without improving the model accuracy in addition selecting the optimum mesh size depends on the output of interest considering the water level prediction reducing the mesh size improves the model accuracy however when we use smaller mesh sizes 20 40 m the improvement rate decreases while the computational time of modeling significantly increases on the other hand for flood extent mapping using a smaller mesh size does not improve the accuracy therefore mesh resolution can be considered as a non influential factor on the prediction of flood extent maps the illustration of accuracy run time relation in fig 8 is beneficial as it provides user friendly guidelines for picking the optimum configuration points and can be generalized to different environmental models e g hydrologic and climate models in other fields our results demonstrate the role and importance of different factors that contributed to the generation of flood inundation maps via hec ras 2d hydrodynamic model it is worth mentioning that the findings of sensitivity analysis for flood simulation are highly dependent on the model structure and study area in addition it is impossible to find a unique solution indicating the most sensitive factors that are generalizable to different types of floods e g urban coastal fluvial and pluvial floods with different intensities extreme floods vs typical floods at different scales small medium and large scale therefore the findings of this study are exclusive to the hec ras 2d model and are more suitable for the simulation of extreme flood events at a medium scale dem 30 m future studies can focus on the sensitivity of the hec ras 2d model for other regions different types of floods and intensities at other scales as a side note since there is not a unique way to define sensitivity using different gsa methods other than those employed here might have led to a different factor ranking pappenberger et al 2008 while in this study we used random sampling which is extracted without considering the previously generated sample points using a different sampling strategy e g orthogonal latin hypercube sampling could better represent the real variability and reduce the inherent sampling error mckay et al 2000 owen 1992 in addition in this study we used four performance metrics to analyze the model sensitivity to its factors however these types of sensitivity analysis are more useful for model calibration and require some reference data which is one of their limitations therefore for future study one can focus on the model response directly i e the values of maximum water levels time of the inundation etc in general this information paves the way for better interpretation of hydrodynamic processes which results in a more efficient calibration and assimilation of the hec ras 2d model furthermore future studies conducted on the sensitivity analysis of hec ras 2d can also consider additional modeling factors such as the number and type of computational mesh different dem data sources the bathymetry and river geometry 6 summary and conclusion in this study we applied variogram and variance based gsa methods to assess the sensitivity of the hec ras 2d hydrodynamic model to the channel and floodplain roughness coefficients upstream boundary condition computational mesh size and dem resolution we evaluated the hec ras 2d model performance over the san jacinto river basin during the extensive flooding of hurricane harvey in 2017 using 4600 model configurations the response surfaces of the gsa approach used in this study are four performance measures of rmse at gauges kge rmse over the high water marks and f that represent the dynamic of water level maximum water level and flood extent simulations for our case study we found that depending on the scale of sensitivity the type of performance measure and predicted output the sensitivity to the various input factors changes for the simulation of water level dynamics the dem and mesh resolution are the most important factors respectively the floodplain roughness coefficient and the upstream boundary condition plays a key role in flood extent mapping finally this study demonstrates that hec ras 2d is more sensitive to mesh resolution floodplain roughness and dem resolution when the prediction of maximum water level is the main goal of simulations the fact that the results of sensitivity analysis vary depending on the model output target of simulation shows the complexity of flood modeling and indicates that it is unlikely to find a single factor to be the most influential for all types of model outputs our findings indicate that the sensitivity of hec ras 2d prediction is complex and the modelers should carefully prioritize the input factors depending on their target model output the results imply that regardless of the types of the model output using a uniform floodplain roughness results in better or similar model performance compared to using distributed floodplain roughness additionally in this study we investigated the relation between the accuracy and efficiency of hec ras 2d our findings indicated that the impact of computational time interval on the model accuracy is negligible and finer mesh resolution increases model accuracy while decreasing model efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments partial financial support for this study was provided by the usace contract a20 0545 001 we would like to acknowledge the data provided by the north american land data assimilation systems nldas 2 in addition we would like to express our appreciation to the resource management associates and hydrologic engineering center for providing us with the hec ras 2d linux version the authors declare no competing interests 
25596,modeling of sea level variation slv is a complicated phenomenon owing to multiple factors that happen at different spatial and temporal scales thus this paper presents an innovative multistep interdependent framework based on wavelet transformation wt and artificial intelligence ai algorithms for slv prediction a wavelet time frequency approach along with harmonic analysis is performed firstly to understand deeply slv behavior then neighborhood component analysis nca is applied for the feature selection fs purposes finally a deep learning neural network dlnn algorithm is utilized to predict precisely slv based on a newly compacted dataset the findings revealed the potential of the dlnn model over the machine learning models as it improves the slv prediction accuracy by 23 additionally the proposed dlnn model can predict slv for a time horizon of three days with a correlation coefficient 0 91 that can help in predicting slv early for disaster management purposes keywords sea level variation artificial intelligence neighborhood component analysis deep learning neural network abbreviations slv sea level variation wt wavelet transformation ai artificial intelligence nca neighborhood component analysis fs feature selection dlln deep learning neural network ml machine learning slr sea level rise tslv tidal sea level variation ntslv non tidal sea level variation cc correlation coefficient ann artificial neural network svr support vector regression rnn recurrent neural networks anfis adaptive neuro fuzzy inference system arma auto regressive moving average wanfis wavelet anfis wann wavelet ann lstm long short term memory wd wind direction ws wind speed r total rainfall rh relative humidity t air temperature e evaporation rate c cloudiness p atmospheric pressure sst sea surface temperature msl mean sea level sr solar radiation wr wind run dp dew point temperature wbt wet bulb temperature dwt discrete wt cwt continuous wavelet transform gbtr gradient boosted trees regression dt decision tree gpr gaussian processing regression rmse root mean squared error k nnr k nearest neighbor regressor nca neighborhood component analysis virf variable importance of random forest psd power spectrum density eda exploratory data analysis 1 introduction climate change is one of the main contributors to sea level rise slr this rise is expected to continue in the coming decades at rates higher than those projected in previous decades church et al 2013 understanding and analyzing temporary trends in sea level is of great importance in constraining future sea level projections and in preparing the adaptation of densely populated coastal areas of the world also slr is important for designing coastal water supply infrastructures such as water desalination plants panagopoulos 2021 panagopoulos and haralambous 2020 in egypt numerous studies carried out to assess slr risks due to climate change most of them concentrated on the nile delta only as it is featured as a low lying coastal plain which makes it exposed to high risks of shoreline recession and flooding risks sharaan and udo 2020 however a recent study investigated that urban growth over the past decades on the northwest of the egyptian coast had a greater impact with climate change on slr which threatens these areas with great danger hzami et al 2021 due to the fast growing infrastructure of coastal tourism resorts on the northwest coast of egypt wherein numerous water desalination plants are constructed precise information is needed about slr to mitigate its associated coastal hazards such as flooding and erosion therefore the conservation of coastal tourism areas is a significant problem that needs to be incorporated into the egyptian shoreline protection authority spa plans in order to protect beaches and prevent cost cutting economic losses in general sea level variation slv can be classified based on the processes influencing it into two categories zubier and eyouni 2020 the impact of isostatic earth related and global ice formation melting climates with both a temporary scale from decadal to geological age is the source of long term sea level fluctuations lange et al 2014 relatively short term variations in the sea level are primarily caused by short term astronomical tidal and atmospheric climatic non tidal astronomical impacts which vary from hourly up to decadal tidal sea level variation tslv owing to the gravitational forces of the moon and sun upon the seas are periodic that makes it possible to measure and forecast easily with harmonic analysis techniques till now harmonic analysis is performed by the application of fourier analysis to the tidal motions abubakar et al 2019 as there are hundreds of periodic motions of the earth sun and moon each of these motions or constituents are harmonic constants represented by a mathematical value describing the effect of cyclical motion on the tides however recent studies have shown that the principal harmonic components of tidal movement however recent studies have investigated the principal harmonic components of tidal movement and summarized them as shown in table 1 guo et al 2018 imani et al 2018 in contrast to tslv non tslv ntslv are not frequent and thus require sophisticated techniques to analyze its behavior the impacts of tides dominate the variability of water levels in some coastal regions li and han 2015 but when the tidal effects are smaller such as those of the mediterranean sea enríquez et al 2017 sammari et al 2006 other mechanisms including atmospheric variables play a major role in slv many researchers have investigated thoroughly the effect of atmospheric forces on slv worldwide afshar kaveh et al 2020 jan et al 2006 meena and agrawal 2015 pasarić et al 2000 raicich 2010 traditional approaches such as empirical formulations frequency analysis and numerical modeling are used earlier for slv predicting tools of the first approach seek empirical formulas to combine different effects into a single parameter or more for slv forecasting applying statistical analysis such as linear regression for observed slv data and associated atmospheric parameters gives satisfactory results with correlation coefficient cc 0 82 tilburg and garvine 2004 however they are not detailed enough to be used in actual cases because of the simplifications produced in these formulas in the second approach researchers studied the relation between ntslv and some meteorological forces such as wind and atmospheric pressure in both time and frequency domains afshar kaveh et al 2020 they found that the effect of wind stress was higher than atmospheric pressure on ntslv on the other hand the third approach predicts numerically ntslv based on atmospheric driving forces kurniawan et al 2015 shen et al 2006 in these studies the researchers employed numerous numerical models to investigate the effect of different atmospheric forces on ntslv they concluded that the modeled and measured ntslv have some differences with relative errors 10 50 as these numerical models are based on water body motion equations they may however be non precise for several reasons such as spatial and temporal approximations differential equations convergence approximations and initial boundary uncertainties moreover it s both time consuming and expensive at the same time as a consequence researchers have focused on developing new models to address the limitations of traditional approaches an alternate way of modeling and forecasting slv is artificial intelligence ai and wavelet transformation wt approaches most tidal researchers used the artificial neural network ann as an alternative to the traditional harmonic analysis technique for short term slv makarynska and makarynskyy 2008 meena and agrawal 2015 salim et al 2015 moreover the performance of ann is compared to other techniques such as genetic programming gp wherein both models perform well and could be regarded as alternatives to the harmonic analysis ali ghorbani et al 2010 other researchers employed support vector regression svr tools for storm surges predicting rajasekaran et al 2008 also few studies have been found in literature for slv prediction with deep learning dl approaches some researchers used recurrent neural networks rnn to integrate the effects of different slv phenomena such as sun and moon gravity attractions and climate change to study its effects on slv ishida et al 2020 furthermore many hybrid models are utilized for modelling slv and for many other applications el diasty et al 2018 huang and wu 2017 ou et al 2016 2017 ou and hong 2014 turki et al 2015 globally an extensive literature indicated that ann are the most applied ai based model for slv prediction table 2 summarizes the substantial improvement in ntslv prediction and modelling via ai approaches as can be seen in table 2 and considering the outcomes of ai approaches used in ntslv forecasting from these preliminaries the research gaps from this summary in the research area of slr can be summarized as 1 most of the researchers focused on investigating the effects of wd ws r rh t sst c p as meteorological predictors for ntslv forecasting without taking into consideration the effects of other important factors such as solar radiation sr wind run wr dew point temperature dp and wet bulb temperature wbt 2 in depth analysis is required to assess the effect of metrological factors on ntslv most of the studies investigated the relation between metrological factors and absolute msl as a target which may be influenced by both tidal and non tidal effects without extracting ntslv alone which seems to be non precise analysis 3 additional feature selection fs analysis should be carried out based on their significance alternative techniques such as machine learning and metaheuristic algorithms for selecting the most essential input variables aren t considered 4 dl algorithms have recently shown outstanding performance in classification tasks especially for those with large data however few studies in the field of slr have shown that these techniques can be used to predict precisely ishida et al 2020 further investigation in this field should be produced 5 as can be seen clearly in table 2 some earlier trials were made with the construction of a suitable ai model that captures the nonlinear relationship of ntslv with meteorological conditions to create effective and accurate short term 1 h head prediction there will also be little efficacy in managing coastal disaster prevention mitigation by readiness systems and early warning systems nevertheless the precise long term ntslv forecast remains challenging 2 research innovation and goals a thorough comprehension of the processes driving its dynamic spatial time evolution of sea behavior is the basis for reliable predictions of future slr in the aforementioned existing slv prediction studies several deficiencies are still essential to clarify the current study therefore introduces an integrated approach focused on sophisticated methodologies for precisely forecasting slv the major contribution of the current research can be summarized as 1 investigating the slr due to meteorological parameters through an extensive analysis by implementing frequency analysis and t tide algorithms to extract ntslv components only 2 due to existing large datasets which consists of 8758 series in the current study traditional ml algorithms can t obtain satisfactory results thus a novel approach called neighborhood component analysis nca is utilized to reduce the dataset dimensionality and get the optimal input features to enhance the prediction process most of nca algorithm applications are for classification tasks yang et al 2012 to the best of the authors knowledge no previous study utilized nca through applying the fs strategy for regression tasks in the research area of slr 3 dl algorithm is developed based on the selected features to develop a promising ai predictive model with higher performance 4 to validate the proposed model outcomes the obtained slv values are compared with similar previous studies to measure the prediction capability of the proposed model 3 material and methods 3 1 study area in this research the hourly recorded time series sea level data with associated meteorological variables are collected during the year 2019 by campbell scientific weather station deployed on the northwest coast of egypt 31 22 e 27 84 n near matruh city fig 1 this study area has been developed and egypt s government has built a number of new coastal towns in the past and current periods based on the fifth assessment report of the intergovernmental panel on climate change ipcc church et al 2013 the egyptian mediterranean coastal area is one of the most vulnerable to slr due to climate change impacts thus this location is primary chosen to develop an efficient ai predictive model which may play an essential role in early warning and coastal protection planning for this vital region 3 2 wavelet analysis unlike the fourier transform technique which only revealed frequencies existing in the whole time series and information on individual occurrence may be lost wavelet transform wt expands time series into time frequency space and thus localized intermediate periodicities can be identified grinsted et al 2004 generally there are two categories of wt the discrete counterpart dwt and its continuous wavelet transform cwt the dwt is a compact display of the data especially suitable for noise reduction and data compression whilst the cwt is better for the extraction of features muslim et al 2020b vu et al 2010 the morlet wavelet transform is used as a common cwt function in the current research to understand the spectral features and investigate the slv cycle terms tidal for stating the most significant tidal harmonic constituents using eq 1 muslim et al 2020a pancheva and mukhtarov 2000 vu et al 2010 this wt formula can be used to analyze time series containing non stationary power at various frequencies in the time and frequency domains 1 ψ a b t π 1 4 e i ω 0 η e γ 2 2 where ψ 0 t presents the wavelet function i is the imaginary unit number ω 0 is the non dimensional frequency and γ is the non dimensional time parameter at higher frequencies the width of the wavelet function is narrower and at lower frequencies wider grinsted et al 2004 the wt expresses arbitrary functions by superimposing the wavelet mother function and each function has a different scale and a corresponding resolution 3 3 machine learning algorithms in the current research a first attempt to develop a promising predictive model using traditional supervised ml algorithms it would be infeasible to evaluate all of the ml regression algorithms used for slv modeling published in the literature in reality the best machine learning algorithms relies on the problem at hand and the nature of dataset almaliki 2019 as a result the authors focused on a study of commonly used algorithms that have proven to be efficient when compared to other ones especially for large datasets issues as in the current research cervantes et al 2008 das et al 2018 franco arcega et al 2012 zhang et al 2018a thus four selected algorithms namely gradient boosted decision trees regression gbtr supported vector regression svr gaussian processing regression gpr and k neighbor regressor k nnr a brief description of the mechanism of each model and its important parameters will be introduced in the following subsections 3 3 1 gradient boosted trees regression gbtr gradient boosted trees regression gbtr is a type of decision tree dt meta learner as the name implies is a tree structure like flow that operates with the condition s theory it has efficient algorithms that are used for prediction and classification it mainly allocated internal nodes branches and terminal leaves pekel 2020 a regression tree is rendered almost in the same way as a classification tree with the exception of a regression measure replacing the impurity measures suitable for classification also boosting methods are intended to enhance the performance by turning weak learners into strong ones without a commonly re weighing mechanism gbtr uses a gradient descent algorithm for poor learners shortcomings gbtr has multiple parameters such as the loss function split sample leaf sample and the learning rate that can affect the accuracy of the estimate gbtr algorithm aims at reducing the loss function error root mean squared error rmse is one of the most commonly used loss functions in regression problems pekel 2020 so it will be used in the current research assume loss function rmse f y i f x i where y m is actual output and f x i is the model we want to fit in then the aim is to minimize rmse j function 2 r m s e 1 n i 1 n y i f x i 2 by applying gradient descent algorithm 3 f x i f x i α δ j δ j f x i where α is learning rate that accelerates or decelerates the learning process and δ j is the gradient of the loss function in the current research both the number of leaf samples and the learning rate will be fine tuning until reaching the maximum performance of the gbtr model 3 3 2 support vector regression svr svr is a statistical learning based neural network that successfully uses for several engineering regression problems patil et al 2012 the svr is classified as a supervised ml algorithm that uses a hyperplane for separating data from one dimension into a high dimensional space and then solves the regression problems using the following equation 4 y f x i 1 n w i x b where w is the weight vector b is the bias factor n is the size of the data set and i x is the kernel function which may be linear polynomial radial basis or sigmoid that maps the input variables in a large space of features in order to model the non linear regression using linear regression the most commonly used cost function for svr modeling is called ε svr which can be expressed as 5 ε y f x ε i f y f x ε y f x ε o t h e r w i s e the main idea of the ε svr function is to maximize the ε derivation the minimization function can be explained as 6 m i n i m i z e 1 2 w 2 c i 1 n ξ i ξ i subjected to yi w xi b 1 ξ i where ξ i 0 c 0 where c is a cost factor that is assumed to determine a trade off between empiric risk and model flatness ξ i and ξ i are slack variables that represent the distance from the real values to the corresponding boundary values in the present research different kernel functions and c values will be investigated via a trial and error technique to get the optimal performance of the svr algorithm 3 3 3 gaussian processing regression gpr gpr is categorized as a versatile non parametric and supervised ml algorithm gpr s primary advantages are the interpretability and probabilities of prediction and outcomes when embedding previous models in recent decades theoretical work and implementation have shown that gpr is an efficient tool for developing ml applications jiang et al 2019 considering a training dataset x i y i i 1 2 n the input data x rd n is called the dataset matrix and y rn is the desired output vector in light of the new input vector x n e w and based on the training results the gpr model addresses the predicted the new response variable y n e w can be estimated as 7 y n e w x n e w t β f x i ε i where β is the bias a vector of basis function coefficients f x i is a gp with zero mean and covariance function k x x and ε i is the gaussian noise with variance σ n 2 in the gaussian process the signal term f x i is often considered a random variable 8 f x i g p m x k x x where m x is a mean function and k x x is a covariance function that explains previous assumptions such as probable smoothness of data and trends g p term creates a non linear relationship between the input response variable including high data uncertainties additionally for more effective estimates numerous covariance functions with desired signal variances values will be utilized in the current study 3 3 4 k nearest neighbor regressor k nnr the k nearest neighbor regressor k nnr is one of the simplest nonparametric ml algorithms used for regression and classification tasks zhang et al 2018b it s having a higher predictive capability especially for large datasets that have little to no prior knowledge therefore it should be one of the first choices as it doesn t have any assumptions on underlying data and doesn t affect the outliers the knnr algorithm is based primarily on the high probability of the most related samples of the same class knn stores the data set during the training process and when new data is received it can easily be labeled into a well suite category that is very similar to new ones zhang et al 2018b considering s x i y i i 1 2 n to be a training set containing n observations for a regression problem where x i x i 1 x i 2 x i m is the i th instance denoted by m features with its response y i when new tested data x t y t is acquired it s required to know how close each testing point to each training point in s by calculating the distance metrics d which can be estimated then the distance d is sorted by its value to the closest i th instance which called the k nearest neighbor with output y i x lastly the final predicted output y a v is the average of the outcomes of its k nearest neighbors which can be estimated as follow 9 y a v 1 k i 1 k y i x in knr algorithm two issues are still challengeable for researchers the choice of the k value and the suitable distance metrics parameters kim et al 2016 these two parameters must therefore be investigated iteratively in order to get the best knr s performance 3 3 5 feature selection neighborhood component analysis nca feature selection recently feature selection became a key step for high dimensionality ml issues the main objective of the feature selection is to pick the best possible combination of features that will improve the regression model performance feature selection increases the prediction precision by reducing the data dimension zhang et al 2018a feature selection algorithms can be classified into three main categories filters wrappers and embedded techniques this classification is based on the usage of a learning algorithm in the approach gregorutti et al 2017 in the current research the filter type neighborhood component analysis nca and the embedded type variable importance of random forest virf will be investigated for these purposes nca is a supervised ml algorithm which functionally similar to a k nnr algorithm that explicitly uses a related term called the closest neighboring stochastic it selects significant features in order to optimize the predictive precision of regression models considering the same training set of k nnr a randomized regression model picks a point from the input variables in s say a point x j with its corresponding output value y j the probability p x j s that x j is selected from s as the reference point for x is higher if x j is closer to x as measured by the distance function d w which can be estimated by 10 d w x i x j r 1 p w r 2 x i r x i j where w r are the feature weights assuming that p x j s α k d w x i x j k is a kernel function that assumes large values when d w x i x j is small p x j s for all j must be equal to 1 and therefore the probability to select a feature x j from s can be formulated as 11 p x j s k d w x i x j j 1 n k d w x i x j herein let y b is the predicted output value by the regression model and y a is the true output of x i and l be the loss function that measures the difference between y a and y b the average value of l y a y b becomes l i l y a y b s i j 1 j i p p i j y i y j in the current study the mean squared error is utilized as the loss cost function mse y i y j 2 also for preventing overfitting of the regression model a regularization term λ is added to the final objective function w which can be finally formulated as 12 w 1 n j 1 n l i λ j 1 p w r 2 the criteria for selecting significant features based on the definition of a relative t threshold can be formulated as 13 t τ max w where τ is tolerance fixed to 0 02 as recommended by and w is updated weight of features yang et al 2012 on the other hand variable importance of random forest virf is one of the most important fs techniques used for quantifying the variable importance based on the permutation techniques breiman 2001 the principle behind measuring the importance of variable x j is to permute all of its values and the variable importance measure is defined as the difference in prediction accuracy generated by the permutation breiman 2001 proposes that the good performance of random forests is due to the high quality of each tree at least in terms of bias combined with the forest s small correlation where the correlation between trees is defined as the ordinary correlation of predictions on so called out of bag oob samples the difference in prediction accuracy before and after permuting x j is assumed to be a measure of variable relevance averaged over all trees let b t is the oob samples for a tree t ε t 1 2 n t r e e and l t t x i y i is the prediction accuracy at i th training example then the virf for a variable x j in a tree t can be calculated as 14 virf t x j i ε b t l t t x i y i l t t x i π j y i where x i π j x i 1 x π j i j x i j 1 x i p and π j is a random permutation of n integers in regression tasks the prediction accuracy is defined as the in the mean squared error mse finally the virf for each variable is computed as the average importance over all trees as follow 15 v i r f x j i ε b t virf t x j n t r e e 3 4 deep learning neural network dlnn one of the most popular models of the artificial neural networks anns in the field of deep learning is the multi layered perceptron mlp mlp consists of three main layers an input layer and an output layer with one or more hidden layers the mlp can be called a dlnn if the hidden layer consists of more than two layers zemouri et al 2020 mlps are fully connected anns each node in one layer connects to every node in the following layer with a certain weight every node is a neuron with a non linear activation function except for the inputs nodes for training mlp utilizes a supervised learning algorithm called backpropagation zemouri et al 2020 dlnn can differentiate non linear separable data thanks to its multi layer non linear activation functions mlp has a linear neuron activation function that represents a linear network of functions connecting weighted inputs to the output also linear algebra demonstrates that any number of layers can be reduced to a two input output layer the creation of the dlnn network via non linear activation functions is therefore important in order to enhance the precise model of biological neurons and better imitate the working mechanism in dlnn the use of sigmoid activation functions with the following formulas was widely adopted 16 y v i tanh v i y v i 1 1 e v the first formula includes a hyperbolic tangent ranging from 1 to 1 while the second formula is a similarly shaped logistic function with ranges from 0 to 1 herein y is the output of the i th node neuron and v i is the weighted sum of the input connections recently numerous activation functions such as rectified linear unit relu identity logistic and tanh are also proposed pham et al 2020 in mlp the initial connection weights are chosen randomly and then fixed by the results of the training process then backpropagation bp training process based on the least squares average algorithm is used to minimize the error e between predicted outputs and observed outputs in the training dataset and maintaining good generality of the networks 17 e i d i n y i n where d i n is the target observations at any n output node next node weights can be adjusted based on corrections that minimize the error in the entire output given by 18 ε n 1 2 j e j 2 n next using gradient descent the change in each weight will be 19 δ w i j n η ε n v i n y i n where y i n is the output of the previous neuron and η is the learning rate chosen to make sure weights converge rapidly without oscillations to the response next the calculated derivative depends on the induced local field that differs this derivative can be easily proven for an output node as 20 ε n v i n e j n φ v i n where φ is the derivative of the activation function finally the corresponding derivative can be formulated with the changes in weight associated with a hidden node as 21 ε n v i n φ v i n ε n v k n w k j n this relies on the change in weights of the k th nodes this algorithm represents the reverse phase of back propagation since the resulting weights vary according to the activation function derivative and the hidden layer weights therefore changed 3 5 models accuracy evaluation recently different evaluation measures have been widely considered to evaluate the ai model s prediction capability among those the rmse root mean square error the cc pearson s correlation coefficient these indices can be expressed as follows 22 r m s e 1 n i 1 n p i m i 2 23 c c i 1 n p i p m i m i 1 n p i m 2 i 1 n m i m 2 where m i p i are the measured and predicted values n is the number of datasets p m are the average values of measured and predictions respectively 3 6 the methodology framework of proposed model fig 2 presents the proposed framework flowchart for predicting the slv and the following are the steps of the proposed technique decomposition a preprocessing harmonic analysis via t tide algorithm is adopted to decompose effectively the measured sea level time series dataset to tslv and ntslv based on wavelet time frequency analysis dataset exploration and preprocessing models evaluation an exploratory data analysis is performed for searching extensively on variable correlations and system complexity then a preprocessing technique is performed via the standardization of datasets building preliminary ml model an initial attempt using traditional ml algorithms is performed for developing a predictive model using classical ml algorithms fs process an efficient nca algorithm is applied for fs purposes to reduce the data dimensionality and enhance the model s prediction capability dl predictive model a dlnn predictive model is developed based on the new datasets after the fs process for predicating ntslv proposed model evaluation numerous statistical and robust evaluation indicators are provided for deeply evaluating the prediction ability of the developed model furthermore the proposed model results are compared with previous studies to confirm its superiority 4 results and discussion aiming at validating of the effectiveness of the developed model in the current study several stages are established and presented in the following subsections 4 1 decomposition of sea level into tidal and non tidal components to understand the sea level trends and produce more precise slr prediction time frequency analysis is initially implemented using morlet cwt function in matlab software for stating the most significant tidal harmonic constituents and then a harmonic analysis is implemented using t tide algorithm for decomposing of tidal and non tidal sea level motions here the power spectrum of sea level data is extracted based on time series measurements of sea level from the time frequency spectrum resulted from wavelet analysis the magnitude and periods of the different frequency sea level signals are presented in fig 3 in fig 3 the three parameters measured time signal period and the power spectrum density psd are presented to estimate the powerful signal frequency the findings show that a significant energy content high psd is concentrated on tidal frequencies which can be clearly observed at 12 h period which demonstrates that the principal semi diurnal tidal constituents m2 and s2 as presented in table 1 are the most significant tidal components and this is consistent with the results of vu et al 2010 thus these constituents will be considered during harmonic analysis on the other hand a summary of the sea level variability can be computed by eq 24 as follow 24 x t z0 t tslv t ntslv t where x t is the observed sea level z0 t is the mean sea level tslv t is the tidal part of the variation ntslv t is the non tidal residual part of the variation and t is time to extract the ntslv from the above equation harmonic analysis is performed using matlab based t tide for the recorded data pawlowicz et al 2002 the decomposition results of observed sea level are presented in fig 4 it can be seen that the annual ntslv ranged from 13 5 cm to 16 cm 4 2 data exploration and preprocessing after extracting ntslv from the measured sea level and based on the previous review wherein no previous study considered all possible factors which may affect ntslv the final input predictors for the current study as presented in table 3 are 1 ws 2 wd 3 wr 4 sst 5 t 6 rh 7 p 8 r 9 sr 10 dp and 11 wp also ntslv is the unique output as presented in table 3 to gain maximum insight into the data set and its underlying structure an exploratory data analysis eda is performed to take a bird s eye view of the data and tries to make some sense of it for this purpose a correlation heatmap matrix is executed and automatized by using python scripting including seaborn and matplotlib powerful visualization libraries to describe visually the relation between the different variables as outlined in fig 5 additionally dataset is investigated by means of statistical metrics such as max min standard deviation and variables skewness as presented in the lower part of table 3 the key insights from the conducted eda in this section can be concluded as 1 there is no strong correlation between any climatic variables and ntslv as shown in fig 6 which indicate that ntslv modeling is a highly complicated process and 2 climatic variables are also interrelated in a complex way as shown by the weak interrelationships these conclusions demonstrate the complexity of the studied system i e the manner in which the predictors and response variables are altogether related thereby asserting the complex nature of slv and the associated climatic variables this sophistication has led to the selection of a predictive data analysis method to create an efficient ntslv predictive model as will be investigated in the next sections in this research ml algorithms are chosen from other methods for predictive data analysis such as statistical learning sl because of its ability to avoid the limitations of any strictly programmed model structure instructions and all hypothesis concerning distributions of data frequencies as exists in sl technique however preprocessing procedures are required for data set to ensure its quality for processing with ml algorithms first according to the statistical skewness indicator in table 3 most of the inputs are well distributed and suitable for training with ml algorithms second it s well recognized that ml algorithms forecast recklessly for unscaled variables since the parameters presented in table 3 have different magnitudes units and ranges e g wind velocity ws is meters per second while direction wd is in degree this leads to magnitude conflicts so that all variables must be added at the same scale data standardization has therefore been used to solve this issue eq 23 in addition data standardization guarantees that all variables have a zero based distribution to ensure efficient data processing 23 z x μ σ where z is the scaled value of the unscaled variable x with a mean μ and a standard deviation σ 4 3 prediction capability of the traditional ml algorithms in the current stage the ml approaches were designed and evaluated using matlab software for evaluating the performance of ml algorithms k fold cross validation has been applied as it is considered to be a reliable approach mitigating the bias and variance in other validation methods such as a holdout in the k fold validation technique the complete dataset is split into k separate subsets or folds that are approximately identical where k is a positive integer seong et al 2018 then the holdout technique is repeated k times one k folds is rotated every time during the test stage and the other k 1 folds are set up for training this mechanism guarantees that the entire dataset is used in both the training and testing stages wei et al 2013 thus the best number of k folds will be investigated for each ml algorithm in the current study as an initial attempt for developing ntslv predictive model this section deals with the issues of selecting the right parameters and functions for the traditional ml algorithms each ml algorithm has been run 50 times with different hyperparameters discussed previously in ml sections then the best performance with associated hyperparameters for each algorithm is presented in table 4 additionally for validating the stability of the utilized ml models via the k fold technique another hold out cross validation technique is performed and its output is presented in table ain the supplementary material it can be seen that the results of the two techniques are close which reflects the stability of the models performance in comparison to all ml algorithms gbtr and svr show poor generalization performance with cc 0 51 and 0 54 respectively for testing data also it is evident that the knnr algorithm is slightly better than the gbr algorithm the best performance for the knr algorithm is obtained with distance metrics cityblock and four neighbors with cc equals 0 78 for the testing data thus it s inferred that the knr model outperformed other ml algorithms in general it can be inferred that conventional ml can offer a little precision in large and complex datasets based on the preliminary results of the applied ml models to overcome these issues fs is utilized to solve the complexity of the dataset by reducing data dimensionality and hence increase the prediction capability to this end the superior knr function with associated hyperparameters which resulted from this stage will be utilized as the fitness function during the fs process 4 4 feature selection of the proposed model in this section the results of the fs process using the nca algorithm are presented the initialization parameters of nca utilized in the current analysis are outlined in table 5 the initial regularization parameter λ lambda is first set to a default value of 1 n where n is a total number of observations in the training set as shown in fig 6 the feature weights of the 11 input parameters are determined if its weight is greater than the tolerance value 0 002 a parameter is considered significant as seen from fig 6 only one parameter with irrelevant weight can be omitted from data space which seems not the significant effect on data dimensionality to improve the performance of the proposed nca algorithm the regularization parameter λ is tuned using different k values via cross validation numerous values of λ are investigated by including the standard deviation of the response std ytrain in the λ values which balances the default loss function mse mean squared error term and the regularization term in the objective function thus the λ values are assumed and randomized as λ linspace 0 50 20 std ytrain n in addition the dataset is divided into partitions equal to k one partition is used for the testing process while the remaining partitions are used for the training throughout 20 independent runs the nca algorithm is applied as a function of k λ and the average results of the loss cost function measurements represent final results are obtained as outlined in table 6 as shown in fig 7 the nca function included many points of local minima but the unique global optimum value located at k 5 and λ 0 028 using the obtained optimum hyper parameters to refit the nca regression algorithm the evolution of mse values after 50 iterations is shown in fig 8 the mse value can be seen to have gradually decreased at the first iteration the lowest mse is 5 62 cm and decreased to 4 31 at the 5th iteration and remained unchanged till the end of the run it s also visible from fig 9 that the number of irrelevant parameters has increased from 1 to 3 indicating that only the most significant features are chosen as a result eight input variables only will be included for a compact dataset for further investigation the performance of nca is validated by comparing its results with the embedded virf technique it can be seen from fig 10 that the redundant variables are the third eighth and tenth ones as the same results obtained from nca which reflect the accuracy obtained results from nca 4 5 dlnn prediction capability in this section the evolutionary results of the ntslv prediction via the dlnn model are assessed to emphasize the role of the fs process in ntslv modelling two different scenarios are performed for comparison purposes the first one uses the initial 11 input data set space denoted as 11 input model dlnn the second one uses the reduced dataset with 8 input denoted with 8 input model dlnn the ideal dlnn architecture with the right number of hidden layers neurons number in each hidden layer and neurons connected functions are universally difficult to identify intensive research has been conducted on a large and continuous discussion of this issue given that different possibilities for constructing the final network structure must be assembled in the dlnn algorithm the process of proper selection of combined parameters is difficult to achieve therefore in the current study a total of 100 runs takes numerous dlnn parameters into account for tunning its performance as outlined in table 7 will be investigated the summary of the dlnn predictive capabilities model corresponding to 8 and 11 input parameters is shown in table 8 it can be noticed that after the reduction of input space the output of the dlnn model is enhanced clearly the average cc value increased from 0 97 to 0 98 during the training phase the most significant improvement can be observed in the testing set in which the average cc increased from 0 91 to 0 94 moreover for checking the stability of the proposed model statistical indicators such as min average and standard deviation sd are calculated for cc and rmse values it can be observed on the testing stage the 8 input dlnn with sd 0 0107 smaller than the 11 input dlnn model sd 0 0113 for cc values indicating a more stable network for the 8 input dlnn model scatter plots are also provided to compare the performance of both dlnn models fig 11 the figure shows that 8 input dlnn has a higher agreement between measured and predicted values than 11 input dlnn during the testing phase the results of the best models predictability with associated optimal hyperparameters are summarized in table 9 also fig 12 illustrates the evolution of the dlnn model through 5000 iterations for the best model performance in both scenarios it can be seen that the 8 input dlnn model has better precision with rmse 0 52 cm and cc 0 97 on the testing phase additionally table 9 demonstrates that both models choose the same activation function relu and the number of hidden layers varies between 4 and 10 with the associated number of neurons range between 9 and 80 meanwhile each model chooses a different type of algorithm for optimizing the network it is also worth pointing out that the average computational time for the 8 input dlnn models is much lower than the 10 input dlnn with a 35 lesser time for further investigation the prediction capability for the proposed model is examined for prediction of ntslv using new metrological dataset and compared with results of previous studies using cc indicator as presented in table 10 the results of the current study as well as previous studies confirmed the effectiveness of ml techniques for predicating ntslv for short periods however the current study outperforms other previous studies for long term prediction of ntslv based on cc values it should be noted that the proposed dlnn model with cc 0 96 enhances the prediction capability significantly by 23 in comparison with the initial attempt with the best ml model with cc 0 78 at 1 h lead time prediction on the other hand for long term predication e g 72h the developed model in the current study with cc 0 91 is also improves the prediction capability by 19 in comparison with previous studies with max cc 0 76 5 conclusions slr as a result of climate change is a worldwide issue furthermore on the egyptian north west coast the slr issue is becoming increasingly severe especially with the accelerating urbanization there thus an accurate and reliable slr model must be designed for early warning to help coastal authorities before hazardous occurrences recently numerous slr forecasting models have been produced nonetheless these models suffer from many deficiencies due to the sophisticated nature of the sea trend these deficiencies including the simplicity of time series models based on historical recorded mean sea level data only that ignoring the importance of climatic variables on slv also for few studies that take climatic variables into account those studies don t include all possible climatic variables that may affect ntslv meanwhile these models don t investigate deeply the behavior of slv by decomposing measured sea level into tidal and nontidal components to precisely model slv so the aforementioned models contain weak initial basics that have a huge impact on their predictive stability thus in the current research an integrated approach is proposed to improve the accuracy and stability of slr prediction models specifically powerful data pre processing techniques are applied including wt time frequency analysis alongside t tide harmonic analysis is performed to decompose slv into tslv and ntslv also an eda is performed to investigate the correlation and interdependence of the system variables next four ml algorithms namely gbtr svr gpr and k nnr are examined initially to investigate their capability to build an efficient ntslv model but those algorithms don t give the desired output thus the nca is designed for fs purposes to reduce data dimensionality by tuning its hyperparameters with the hope of achieving high accuracy finally an efficient dlnn model is developed based on the compacted dataset from the fs process to enhance ntslv predictive model furthermore different statistical criteria and visual evaluations are performed to validate the proposed wt dlnn model the results confirm the superiority of the developed wt dlnn model models as it enhances the prediction capability by 19 in comparison with previous studies especially for long term forecasting of ntslv e g 72h therefore integrated nca and wt dlnn can be used as a potential soft computing technique for modelling ntslv whilst current research made substantial improvements in forecasting and modelling the slv process it is important not to overlook the limitations the major limitation of this research is that it is only relevant to marine study locations in the mediterranean sea with similar climatic and hydrodynamic conditions however the approach adopted here can be applied to various research locations with varying environmental conditions for future work the used nca as an fs tool in the proposed model needs to be tested with other metaheuristic algorithms like simulated annealing genetic algorithms and particle swarm optimization to evaluate its performance authorship contribution ahmed alshouny investigation validation writing review editing funding acquisition mohamed t elnabwy conceptualization resources data curation data assessment investigation methodology software formal analysis validation writing original draft writing review editing mosbeh r kaloop data curation data assessment investigation validation review editing ahmad baik validation review yehia miky investigation validation review software availability the ml models training is implemented through matlab environment and regression learner of matlab r2020b also the eda and dlnn python codes are available via the github link https github com ahmedalshouny the eda and dlnn python codes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors extend their appreciation to the deputyship for research innovation ministry of education in saudi arabia for funding this research work through the project no ifphi 234 137 2020 and king abdulaziz university dsr jeddah saudi arabia appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105399 
25596,modeling of sea level variation slv is a complicated phenomenon owing to multiple factors that happen at different spatial and temporal scales thus this paper presents an innovative multistep interdependent framework based on wavelet transformation wt and artificial intelligence ai algorithms for slv prediction a wavelet time frequency approach along with harmonic analysis is performed firstly to understand deeply slv behavior then neighborhood component analysis nca is applied for the feature selection fs purposes finally a deep learning neural network dlnn algorithm is utilized to predict precisely slv based on a newly compacted dataset the findings revealed the potential of the dlnn model over the machine learning models as it improves the slv prediction accuracy by 23 additionally the proposed dlnn model can predict slv for a time horizon of three days with a correlation coefficient 0 91 that can help in predicting slv early for disaster management purposes keywords sea level variation artificial intelligence neighborhood component analysis deep learning neural network abbreviations slv sea level variation wt wavelet transformation ai artificial intelligence nca neighborhood component analysis fs feature selection dlln deep learning neural network ml machine learning slr sea level rise tslv tidal sea level variation ntslv non tidal sea level variation cc correlation coefficient ann artificial neural network svr support vector regression rnn recurrent neural networks anfis adaptive neuro fuzzy inference system arma auto regressive moving average wanfis wavelet anfis wann wavelet ann lstm long short term memory wd wind direction ws wind speed r total rainfall rh relative humidity t air temperature e evaporation rate c cloudiness p atmospheric pressure sst sea surface temperature msl mean sea level sr solar radiation wr wind run dp dew point temperature wbt wet bulb temperature dwt discrete wt cwt continuous wavelet transform gbtr gradient boosted trees regression dt decision tree gpr gaussian processing regression rmse root mean squared error k nnr k nearest neighbor regressor nca neighborhood component analysis virf variable importance of random forest psd power spectrum density eda exploratory data analysis 1 introduction climate change is one of the main contributors to sea level rise slr this rise is expected to continue in the coming decades at rates higher than those projected in previous decades church et al 2013 understanding and analyzing temporary trends in sea level is of great importance in constraining future sea level projections and in preparing the adaptation of densely populated coastal areas of the world also slr is important for designing coastal water supply infrastructures such as water desalination plants panagopoulos 2021 panagopoulos and haralambous 2020 in egypt numerous studies carried out to assess slr risks due to climate change most of them concentrated on the nile delta only as it is featured as a low lying coastal plain which makes it exposed to high risks of shoreline recession and flooding risks sharaan and udo 2020 however a recent study investigated that urban growth over the past decades on the northwest of the egyptian coast had a greater impact with climate change on slr which threatens these areas with great danger hzami et al 2021 due to the fast growing infrastructure of coastal tourism resorts on the northwest coast of egypt wherein numerous water desalination plants are constructed precise information is needed about slr to mitigate its associated coastal hazards such as flooding and erosion therefore the conservation of coastal tourism areas is a significant problem that needs to be incorporated into the egyptian shoreline protection authority spa plans in order to protect beaches and prevent cost cutting economic losses in general sea level variation slv can be classified based on the processes influencing it into two categories zubier and eyouni 2020 the impact of isostatic earth related and global ice formation melting climates with both a temporary scale from decadal to geological age is the source of long term sea level fluctuations lange et al 2014 relatively short term variations in the sea level are primarily caused by short term astronomical tidal and atmospheric climatic non tidal astronomical impacts which vary from hourly up to decadal tidal sea level variation tslv owing to the gravitational forces of the moon and sun upon the seas are periodic that makes it possible to measure and forecast easily with harmonic analysis techniques till now harmonic analysis is performed by the application of fourier analysis to the tidal motions abubakar et al 2019 as there are hundreds of periodic motions of the earth sun and moon each of these motions or constituents are harmonic constants represented by a mathematical value describing the effect of cyclical motion on the tides however recent studies have shown that the principal harmonic components of tidal movement however recent studies have investigated the principal harmonic components of tidal movement and summarized them as shown in table 1 guo et al 2018 imani et al 2018 in contrast to tslv non tslv ntslv are not frequent and thus require sophisticated techniques to analyze its behavior the impacts of tides dominate the variability of water levels in some coastal regions li and han 2015 but when the tidal effects are smaller such as those of the mediterranean sea enríquez et al 2017 sammari et al 2006 other mechanisms including atmospheric variables play a major role in slv many researchers have investigated thoroughly the effect of atmospheric forces on slv worldwide afshar kaveh et al 2020 jan et al 2006 meena and agrawal 2015 pasarić et al 2000 raicich 2010 traditional approaches such as empirical formulations frequency analysis and numerical modeling are used earlier for slv predicting tools of the first approach seek empirical formulas to combine different effects into a single parameter or more for slv forecasting applying statistical analysis such as linear regression for observed slv data and associated atmospheric parameters gives satisfactory results with correlation coefficient cc 0 82 tilburg and garvine 2004 however they are not detailed enough to be used in actual cases because of the simplifications produced in these formulas in the second approach researchers studied the relation between ntslv and some meteorological forces such as wind and atmospheric pressure in both time and frequency domains afshar kaveh et al 2020 they found that the effect of wind stress was higher than atmospheric pressure on ntslv on the other hand the third approach predicts numerically ntslv based on atmospheric driving forces kurniawan et al 2015 shen et al 2006 in these studies the researchers employed numerous numerical models to investigate the effect of different atmospheric forces on ntslv they concluded that the modeled and measured ntslv have some differences with relative errors 10 50 as these numerical models are based on water body motion equations they may however be non precise for several reasons such as spatial and temporal approximations differential equations convergence approximations and initial boundary uncertainties moreover it s both time consuming and expensive at the same time as a consequence researchers have focused on developing new models to address the limitations of traditional approaches an alternate way of modeling and forecasting slv is artificial intelligence ai and wavelet transformation wt approaches most tidal researchers used the artificial neural network ann as an alternative to the traditional harmonic analysis technique for short term slv makarynska and makarynskyy 2008 meena and agrawal 2015 salim et al 2015 moreover the performance of ann is compared to other techniques such as genetic programming gp wherein both models perform well and could be regarded as alternatives to the harmonic analysis ali ghorbani et al 2010 other researchers employed support vector regression svr tools for storm surges predicting rajasekaran et al 2008 also few studies have been found in literature for slv prediction with deep learning dl approaches some researchers used recurrent neural networks rnn to integrate the effects of different slv phenomena such as sun and moon gravity attractions and climate change to study its effects on slv ishida et al 2020 furthermore many hybrid models are utilized for modelling slv and for many other applications el diasty et al 2018 huang and wu 2017 ou et al 2016 2017 ou and hong 2014 turki et al 2015 globally an extensive literature indicated that ann are the most applied ai based model for slv prediction table 2 summarizes the substantial improvement in ntslv prediction and modelling via ai approaches as can be seen in table 2 and considering the outcomes of ai approaches used in ntslv forecasting from these preliminaries the research gaps from this summary in the research area of slr can be summarized as 1 most of the researchers focused on investigating the effects of wd ws r rh t sst c p as meteorological predictors for ntslv forecasting without taking into consideration the effects of other important factors such as solar radiation sr wind run wr dew point temperature dp and wet bulb temperature wbt 2 in depth analysis is required to assess the effect of metrological factors on ntslv most of the studies investigated the relation between metrological factors and absolute msl as a target which may be influenced by both tidal and non tidal effects without extracting ntslv alone which seems to be non precise analysis 3 additional feature selection fs analysis should be carried out based on their significance alternative techniques such as machine learning and metaheuristic algorithms for selecting the most essential input variables aren t considered 4 dl algorithms have recently shown outstanding performance in classification tasks especially for those with large data however few studies in the field of slr have shown that these techniques can be used to predict precisely ishida et al 2020 further investigation in this field should be produced 5 as can be seen clearly in table 2 some earlier trials were made with the construction of a suitable ai model that captures the nonlinear relationship of ntslv with meteorological conditions to create effective and accurate short term 1 h head prediction there will also be little efficacy in managing coastal disaster prevention mitigation by readiness systems and early warning systems nevertheless the precise long term ntslv forecast remains challenging 2 research innovation and goals a thorough comprehension of the processes driving its dynamic spatial time evolution of sea behavior is the basis for reliable predictions of future slr in the aforementioned existing slv prediction studies several deficiencies are still essential to clarify the current study therefore introduces an integrated approach focused on sophisticated methodologies for precisely forecasting slv the major contribution of the current research can be summarized as 1 investigating the slr due to meteorological parameters through an extensive analysis by implementing frequency analysis and t tide algorithms to extract ntslv components only 2 due to existing large datasets which consists of 8758 series in the current study traditional ml algorithms can t obtain satisfactory results thus a novel approach called neighborhood component analysis nca is utilized to reduce the dataset dimensionality and get the optimal input features to enhance the prediction process most of nca algorithm applications are for classification tasks yang et al 2012 to the best of the authors knowledge no previous study utilized nca through applying the fs strategy for regression tasks in the research area of slr 3 dl algorithm is developed based on the selected features to develop a promising ai predictive model with higher performance 4 to validate the proposed model outcomes the obtained slv values are compared with similar previous studies to measure the prediction capability of the proposed model 3 material and methods 3 1 study area in this research the hourly recorded time series sea level data with associated meteorological variables are collected during the year 2019 by campbell scientific weather station deployed on the northwest coast of egypt 31 22 e 27 84 n near matruh city fig 1 this study area has been developed and egypt s government has built a number of new coastal towns in the past and current periods based on the fifth assessment report of the intergovernmental panel on climate change ipcc church et al 2013 the egyptian mediterranean coastal area is one of the most vulnerable to slr due to climate change impacts thus this location is primary chosen to develop an efficient ai predictive model which may play an essential role in early warning and coastal protection planning for this vital region 3 2 wavelet analysis unlike the fourier transform technique which only revealed frequencies existing in the whole time series and information on individual occurrence may be lost wavelet transform wt expands time series into time frequency space and thus localized intermediate periodicities can be identified grinsted et al 2004 generally there are two categories of wt the discrete counterpart dwt and its continuous wavelet transform cwt the dwt is a compact display of the data especially suitable for noise reduction and data compression whilst the cwt is better for the extraction of features muslim et al 2020b vu et al 2010 the morlet wavelet transform is used as a common cwt function in the current research to understand the spectral features and investigate the slv cycle terms tidal for stating the most significant tidal harmonic constituents using eq 1 muslim et al 2020a pancheva and mukhtarov 2000 vu et al 2010 this wt formula can be used to analyze time series containing non stationary power at various frequencies in the time and frequency domains 1 ψ a b t π 1 4 e i ω 0 η e γ 2 2 where ψ 0 t presents the wavelet function i is the imaginary unit number ω 0 is the non dimensional frequency and γ is the non dimensional time parameter at higher frequencies the width of the wavelet function is narrower and at lower frequencies wider grinsted et al 2004 the wt expresses arbitrary functions by superimposing the wavelet mother function and each function has a different scale and a corresponding resolution 3 3 machine learning algorithms in the current research a first attempt to develop a promising predictive model using traditional supervised ml algorithms it would be infeasible to evaluate all of the ml regression algorithms used for slv modeling published in the literature in reality the best machine learning algorithms relies on the problem at hand and the nature of dataset almaliki 2019 as a result the authors focused on a study of commonly used algorithms that have proven to be efficient when compared to other ones especially for large datasets issues as in the current research cervantes et al 2008 das et al 2018 franco arcega et al 2012 zhang et al 2018a thus four selected algorithms namely gradient boosted decision trees regression gbtr supported vector regression svr gaussian processing regression gpr and k neighbor regressor k nnr a brief description of the mechanism of each model and its important parameters will be introduced in the following subsections 3 3 1 gradient boosted trees regression gbtr gradient boosted trees regression gbtr is a type of decision tree dt meta learner as the name implies is a tree structure like flow that operates with the condition s theory it has efficient algorithms that are used for prediction and classification it mainly allocated internal nodes branches and terminal leaves pekel 2020 a regression tree is rendered almost in the same way as a classification tree with the exception of a regression measure replacing the impurity measures suitable for classification also boosting methods are intended to enhance the performance by turning weak learners into strong ones without a commonly re weighing mechanism gbtr uses a gradient descent algorithm for poor learners shortcomings gbtr has multiple parameters such as the loss function split sample leaf sample and the learning rate that can affect the accuracy of the estimate gbtr algorithm aims at reducing the loss function error root mean squared error rmse is one of the most commonly used loss functions in regression problems pekel 2020 so it will be used in the current research assume loss function rmse f y i f x i where y m is actual output and f x i is the model we want to fit in then the aim is to minimize rmse j function 2 r m s e 1 n i 1 n y i f x i 2 by applying gradient descent algorithm 3 f x i f x i α δ j δ j f x i where α is learning rate that accelerates or decelerates the learning process and δ j is the gradient of the loss function in the current research both the number of leaf samples and the learning rate will be fine tuning until reaching the maximum performance of the gbtr model 3 3 2 support vector regression svr svr is a statistical learning based neural network that successfully uses for several engineering regression problems patil et al 2012 the svr is classified as a supervised ml algorithm that uses a hyperplane for separating data from one dimension into a high dimensional space and then solves the regression problems using the following equation 4 y f x i 1 n w i x b where w is the weight vector b is the bias factor n is the size of the data set and i x is the kernel function which may be linear polynomial radial basis or sigmoid that maps the input variables in a large space of features in order to model the non linear regression using linear regression the most commonly used cost function for svr modeling is called ε svr which can be expressed as 5 ε y f x ε i f y f x ε y f x ε o t h e r w i s e the main idea of the ε svr function is to maximize the ε derivation the minimization function can be explained as 6 m i n i m i z e 1 2 w 2 c i 1 n ξ i ξ i subjected to yi w xi b 1 ξ i where ξ i 0 c 0 where c is a cost factor that is assumed to determine a trade off between empiric risk and model flatness ξ i and ξ i are slack variables that represent the distance from the real values to the corresponding boundary values in the present research different kernel functions and c values will be investigated via a trial and error technique to get the optimal performance of the svr algorithm 3 3 3 gaussian processing regression gpr gpr is categorized as a versatile non parametric and supervised ml algorithm gpr s primary advantages are the interpretability and probabilities of prediction and outcomes when embedding previous models in recent decades theoretical work and implementation have shown that gpr is an efficient tool for developing ml applications jiang et al 2019 considering a training dataset x i y i i 1 2 n the input data x rd n is called the dataset matrix and y rn is the desired output vector in light of the new input vector x n e w and based on the training results the gpr model addresses the predicted the new response variable y n e w can be estimated as 7 y n e w x n e w t β f x i ε i where β is the bias a vector of basis function coefficients f x i is a gp with zero mean and covariance function k x x and ε i is the gaussian noise with variance σ n 2 in the gaussian process the signal term f x i is often considered a random variable 8 f x i g p m x k x x where m x is a mean function and k x x is a covariance function that explains previous assumptions such as probable smoothness of data and trends g p term creates a non linear relationship between the input response variable including high data uncertainties additionally for more effective estimates numerous covariance functions with desired signal variances values will be utilized in the current study 3 3 4 k nearest neighbor regressor k nnr the k nearest neighbor regressor k nnr is one of the simplest nonparametric ml algorithms used for regression and classification tasks zhang et al 2018b it s having a higher predictive capability especially for large datasets that have little to no prior knowledge therefore it should be one of the first choices as it doesn t have any assumptions on underlying data and doesn t affect the outliers the knnr algorithm is based primarily on the high probability of the most related samples of the same class knn stores the data set during the training process and when new data is received it can easily be labeled into a well suite category that is very similar to new ones zhang et al 2018b considering s x i y i i 1 2 n to be a training set containing n observations for a regression problem where x i x i 1 x i 2 x i m is the i th instance denoted by m features with its response y i when new tested data x t y t is acquired it s required to know how close each testing point to each training point in s by calculating the distance metrics d which can be estimated then the distance d is sorted by its value to the closest i th instance which called the k nearest neighbor with output y i x lastly the final predicted output y a v is the average of the outcomes of its k nearest neighbors which can be estimated as follow 9 y a v 1 k i 1 k y i x in knr algorithm two issues are still challengeable for researchers the choice of the k value and the suitable distance metrics parameters kim et al 2016 these two parameters must therefore be investigated iteratively in order to get the best knr s performance 3 3 5 feature selection neighborhood component analysis nca feature selection recently feature selection became a key step for high dimensionality ml issues the main objective of the feature selection is to pick the best possible combination of features that will improve the regression model performance feature selection increases the prediction precision by reducing the data dimension zhang et al 2018a feature selection algorithms can be classified into three main categories filters wrappers and embedded techniques this classification is based on the usage of a learning algorithm in the approach gregorutti et al 2017 in the current research the filter type neighborhood component analysis nca and the embedded type variable importance of random forest virf will be investigated for these purposes nca is a supervised ml algorithm which functionally similar to a k nnr algorithm that explicitly uses a related term called the closest neighboring stochastic it selects significant features in order to optimize the predictive precision of regression models considering the same training set of k nnr a randomized regression model picks a point from the input variables in s say a point x j with its corresponding output value y j the probability p x j s that x j is selected from s as the reference point for x is higher if x j is closer to x as measured by the distance function d w which can be estimated by 10 d w x i x j r 1 p w r 2 x i r x i j where w r are the feature weights assuming that p x j s α k d w x i x j k is a kernel function that assumes large values when d w x i x j is small p x j s for all j must be equal to 1 and therefore the probability to select a feature x j from s can be formulated as 11 p x j s k d w x i x j j 1 n k d w x i x j herein let y b is the predicted output value by the regression model and y a is the true output of x i and l be the loss function that measures the difference between y a and y b the average value of l y a y b becomes l i l y a y b s i j 1 j i p p i j y i y j in the current study the mean squared error is utilized as the loss cost function mse y i y j 2 also for preventing overfitting of the regression model a regularization term λ is added to the final objective function w which can be finally formulated as 12 w 1 n j 1 n l i λ j 1 p w r 2 the criteria for selecting significant features based on the definition of a relative t threshold can be formulated as 13 t τ max w where τ is tolerance fixed to 0 02 as recommended by and w is updated weight of features yang et al 2012 on the other hand variable importance of random forest virf is one of the most important fs techniques used for quantifying the variable importance based on the permutation techniques breiman 2001 the principle behind measuring the importance of variable x j is to permute all of its values and the variable importance measure is defined as the difference in prediction accuracy generated by the permutation breiman 2001 proposes that the good performance of random forests is due to the high quality of each tree at least in terms of bias combined with the forest s small correlation where the correlation between trees is defined as the ordinary correlation of predictions on so called out of bag oob samples the difference in prediction accuracy before and after permuting x j is assumed to be a measure of variable relevance averaged over all trees let b t is the oob samples for a tree t ε t 1 2 n t r e e and l t t x i y i is the prediction accuracy at i th training example then the virf for a variable x j in a tree t can be calculated as 14 virf t x j i ε b t l t t x i y i l t t x i π j y i where x i π j x i 1 x π j i j x i j 1 x i p and π j is a random permutation of n integers in regression tasks the prediction accuracy is defined as the in the mean squared error mse finally the virf for each variable is computed as the average importance over all trees as follow 15 v i r f x j i ε b t virf t x j n t r e e 3 4 deep learning neural network dlnn one of the most popular models of the artificial neural networks anns in the field of deep learning is the multi layered perceptron mlp mlp consists of three main layers an input layer and an output layer with one or more hidden layers the mlp can be called a dlnn if the hidden layer consists of more than two layers zemouri et al 2020 mlps are fully connected anns each node in one layer connects to every node in the following layer with a certain weight every node is a neuron with a non linear activation function except for the inputs nodes for training mlp utilizes a supervised learning algorithm called backpropagation zemouri et al 2020 dlnn can differentiate non linear separable data thanks to its multi layer non linear activation functions mlp has a linear neuron activation function that represents a linear network of functions connecting weighted inputs to the output also linear algebra demonstrates that any number of layers can be reduced to a two input output layer the creation of the dlnn network via non linear activation functions is therefore important in order to enhance the precise model of biological neurons and better imitate the working mechanism in dlnn the use of sigmoid activation functions with the following formulas was widely adopted 16 y v i tanh v i y v i 1 1 e v the first formula includes a hyperbolic tangent ranging from 1 to 1 while the second formula is a similarly shaped logistic function with ranges from 0 to 1 herein y is the output of the i th node neuron and v i is the weighted sum of the input connections recently numerous activation functions such as rectified linear unit relu identity logistic and tanh are also proposed pham et al 2020 in mlp the initial connection weights are chosen randomly and then fixed by the results of the training process then backpropagation bp training process based on the least squares average algorithm is used to minimize the error e between predicted outputs and observed outputs in the training dataset and maintaining good generality of the networks 17 e i d i n y i n where d i n is the target observations at any n output node next node weights can be adjusted based on corrections that minimize the error in the entire output given by 18 ε n 1 2 j e j 2 n next using gradient descent the change in each weight will be 19 δ w i j n η ε n v i n y i n where y i n is the output of the previous neuron and η is the learning rate chosen to make sure weights converge rapidly without oscillations to the response next the calculated derivative depends on the induced local field that differs this derivative can be easily proven for an output node as 20 ε n v i n e j n φ v i n where φ is the derivative of the activation function finally the corresponding derivative can be formulated with the changes in weight associated with a hidden node as 21 ε n v i n φ v i n ε n v k n w k j n this relies on the change in weights of the k th nodes this algorithm represents the reverse phase of back propagation since the resulting weights vary according to the activation function derivative and the hidden layer weights therefore changed 3 5 models accuracy evaluation recently different evaluation measures have been widely considered to evaluate the ai model s prediction capability among those the rmse root mean square error the cc pearson s correlation coefficient these indices can be expressed as follows 22 r m s e 1 n i 1 n p i m i 2 23 c c i 1 n p i p m i m i 1 n p i m 2 i 1 n m i m 2 where m i p i are the measured and predicted values n is the number of datasets p m are the average values of measured and predictions respectively 3 6 the methodology framework of proposed model fig 2 presents the proposed framework flowchart for predicting the slv and the following are the steps of the proposed technique decomposition a preprocessing harmonic analysis via t tide algorithm is adopted to decompose effectively the measured sea level time series dataset to tslv and ntslv based on wavelet time frequency analysis dataset exploration and preprocessing models evaluation an exploratory data analysis is performed for searching extensively on variable correlations and system complexity then a preprocessing technique is performed via the standardization of datasets building preliminary ml model an initial attempt using traditional ml algorithms is performed for developing a predictive model using classical ml algorithms fs process an efficient nca algorithm is applied for fs purposes to reduce the data dimensionality and enhance the model s prediction capability dl predictive model a dlnn predictive model is developed based on the new datasets after the fs process for predicating ntslv proposed model evaluation numerous statistical and robust evaluation indicators are provided for deeply evaluating the prediction ability of the developed model furthermore the proposed model results are compared with previous studies to confirm its superiority 4 results and discussion aiming at validating of the effectiveness of the developed model in the current study several stages are established and presented in the following subsections 4 1 decomposition of sea level into tidal and non tidal components to understand the sea level trends and produce more precise slr prediction time frequency analysis is initially implemented using morlet cwt function in matlab software for stating the most significant tidal harmonic constituents and then a harmonic analysis is implemented using t tide algorithm for decomposing of tidal and non tidal sea level motions here the power spectrum of sea level data is extracted based on time series measurements of sea level from the time frequency spectrum resulted from wavelet analysis the magnitude and periods of the different frequency sea level signals are presented in fig 3 in fig 3 the three parameters measured time signal period and the power spectrum density psd are presented to estimate the powerful signal frequency the findings show that a significant energy content high psd is concentrated on tidal frequencies which can be clearly observed at 12 h period which demonstrates that the principal semi diurnal tidal constituents m2 and s2 as presented in table 1 are the most significant tidal components and this is consistent with the results of vu et al 2010 thus these constituents will be considered during harmonic analysis on the other hand a summary of the sea level variability can be computed by eq 24 as follow 24 x t z0 t tslv t ntslv t where x t is the observed sea level z0 t is the mean sea level tslv t is the tidal part of the variation ntslv t is the non tidal residual part of the variation and t is time to extract the ntslv from the above equation harmonic analysis is performed using matlab based t tide for the recorded data pawlowicz et al 2002 the decomposition results of observed sea level are presented in fig 4 it can be seen that the annual ntslv ranged from 13 5 cm to 16 cm 4 2 data exploration and preprocessing after extracting ntslv from the measured sea level and based on the previous review wherein no previous study considered all possible factors which may affect ntslv the final input predictors for the current study as presented in table 3 are 1 ws 2 wd 3 wr 4 sst 5 t 6 rh 7 p 8 r 9 sr 10 dp and 11 wp also ntslv is the unique output as presented in table 3 to gain maximum insight into the data set and its underlying structure an exploratory data analysis eda is performed to take a bird s eye view of the data and tries to make some sense of it for this purpose a correlation heatmap matrix is executed and automatized by using python scripting including seaborn and matplotlib powerful visualization libraries to describe visually the relation between the different variables as outlined in fig 5 additionally dataset is investigated by means of statistical metrics such as max min standard deviation and variables skewness as presented in the lower part of table 3 the key insights from the conducted eda in this section can be concluded as 1 there is no strong correlation between any climatic variables and ntslv as shown in fig 6 which indicate that ntslv modeling is a highly complicated process and 2 climatic variables are also interrelated in a complex way as shown by the weak interrelationships these conclusions demonstrate the complexity of the studied system i e the manner in which the predictors and response variables are altogether related thereby asserting the complex nature of slv and the associated climatic variables this sophistication has led to the selection of a predictive data analysis method to create an efficient ntslv predictive model as will be investigated in the next sections in this research ml algorithms are chosen from other methods for predictive data analysis such as statistical learning sl because of its ability to avoid the limitations of any strictly programmed model structure instructions and all hypothesis concerning distributions of data frequencies as exists in sl technique however preprocessing procedures are required for data set to ensure its quality for processing with ml algorithms first according to the statistical skewness indicator in table 3 most of the inputs are well distributed and suitable for training with ml algorithms second it s well recognized that ml algorithms forecast recklessly for unscaled variables since the parameters presented in table 3 have different magnitudes units and ranges e g wind velocity ws is meters per second while direction wd is in degree this leads to magnitude conflicts so that all variables must be added at the same scale data standardization has therefore been used to solve this issue eq 23 in addition data standardization guarantees that all variables have a zero based distribution to ensure efficient data processing 23 z x μ σ where z is the scaled value of the unscaled variable x with a mean μ and a standard deviation σ 4 3 prediction capability of the traditional ml algorithms in the current stage the ml approaches were designed and evaluated using matlab software for evaluating the performance of ml algorithms k fold cross validation has been applied as it is considered to be a reliable approach mitigating the bias and variance in other validation methods such as a holdout in the k fold validation technique the complete dataset is split into k separate subsets or folds that are approximately identical where k is a positive integer seong et al 2018 then the holdout technique is repeated k times one k folds is rotated every time during the test stage and the other k 1 folds are set up for training this mechanism guarantees that the entire dataset is used in both the training and testing stages wei et al 2013 thus the best number of k folds will be investigated for each ml algorithm in the current study as an initial attempt for developing ntslv predictive model this section deals with the issues of selecting the right parameters and functions for the traditional ml algorithms each ml algorithm has been run 50 times with different hyperparameters discussed previously in ml sections then the best performance with associated hyperparameters for each algorithm is presented in table 4 additionally for validating the stability of the utilized ml models via the k fold technique another hold out cross validation technique is performed and its output is presented in table ain the supplementary material it can be seen that the results of the two techniques are close which reflects the stability of the models performance in comparison to all ml algorithms gbtr and svr show poor generalization performance with cc 0 51 and 0 54 respectively for testing data also it is evident that the knnr algorithm is slightly better than the gbr algorithm the best performance for the knr algorithm is obtained with distance metrics cityblock and four neighbors with cc equals 0 78 for the testing data thus it s inferred that the knr model outperformed other ml algorithms in general it can be inferred that conventional ml can offer a little precision in large and complex datasets based on the preliminary results of the applied ml models to overcome these issues fs is utilized to solve the complexity of the dataset by reducing data dimensionality and hence increase the prediction capability to this end the superior knr function with associated hyperparameters which resulted from this stage will be utilized as the fitness function during the fs process 4 4 feature selection of the proposed model in this section the results of the fs process using the nca algorithm are presented the initialization parameters of nca utilized in the current analysis are outlined in table 5 the initial regularization parameter λ lambda is first set to a default value of 1 n where n is a total number of observations in the training set as shown in fig 6 the feature weights of the 11 input parameters are determined if its weight is greater than the tolerance value 0 002 a parameter is considered significant as seen from fig 6 only one parameter with irrelevant weight can be omitted from data space which seems not the significant effect on data dimensionality to improve the performance of the proposed nca algorithm the regularization parameter λ is tuned using different k values via cross validation numerous values of λ are investigated by including the standard deviation of the response std ytrain in the λ values which balances the default loss function mse mean squared error term and the regularization term in the objective function thus the λ values are assumed and randomized as λ linspace 0 50 20 std ytrain n in addition the dataset is divided into partitions equal to k one partition is used for the testing process while the remaining partitions are used for the training throughout 20 independent runs the nca algorithm is applied as a function of k λ and the average results of the loss cost function measurements represent final results are obtained as outlined in table 6 as shown in fig 7 the nca function included many points of local minima but the unique global optimum value located at k 5 and λ 0 028 using the obtained optimum hyper parameters to refit the nca regression algorithm the evolution of mse values after 50 iterations is shown in fig 8 the mse value can be seen to have gradually decreased at the first iteration the lowest mse is 5 62 cm and decreased to 4 31 at the 5th iteration and remained unchanged till the end of the run it s also visible from fig 9 that the number of irrelevant parameters has increased from 1 to 3 indicating that only the most significant features are chosen as a result eight input variables only will be included for a compact dataset for further investigation the performance of nca is validated by comparing its results with the embedded virf technique it can be seen from fig 10 that the redundant variables are the third eighth and tenth ones as the same results obtained from nca which reflect the accuracy obtained results from nca 4 5 dlnn prediction capability in this section the evolutionary results of the ntslv prediction via the dlnn model are assessed to emphasize the role of the fs process in ntslv modelling two different scenarios are performed for comparison purposes the first one uses the initial 11 input data set space denoted as 11 input model dlnn the second one uses the reduced dataset with 8 input denoted with 8 input model dlnn the ideal dlnn architecture with the right number of hidden layers neurons number in each hidden layer and neurons connected functions are universally difficult to identify intensive research has been conducted on a large and continuous discussion of this issue given that different possibilities for constructing the final network structure must be assembled in the dlnn algorithm the process of proper selection of combined parameters is difficult to achieve therefore in the current study a total of 100 runs takes numerous dlnn parameters into account for tunning its performance as outlined in table 7 will be investigated the summary of the dlnn predictive capabilities model corresponding to 8 and 11 input parameters is shown in table 8 it can be noticed that after the reduction of input space the output of the dlnn model is enhanced clearly the average cc value increased from 0 97 to 0 98 during the training phase the most significant improvement can be observed in the testing set in which the average cc increased from 0 91 to 0 94 moreover for checking the stability of the proposed model statistical indicators such as min average and standard deviation sd are calculated for cc and rmse values it can be observed on the testing stage the 8 input dlnn with sd 0 0107 smaller than the 11 input dlnn model sd 0 0113 for cc values indicating a more stable network for the 8 input dlnn model scatter plots are also provided to compare the performance of both dlnn models fig 11 the figure shows that 8 input dlnn has a higher agreement between measured and predicted values than 11 input dlnn during the testing phase the results of the best models predictability with associated optimal hyperparameters are summarized in table 9 also fig 12 illustrates the evolution of the dlnn model through 5000 iterations for the best model performance in both scenarios it can be seen that the 8 input dlnn model has better precision with rmse 0 52 cm and cc 0 97 on the testing phase additionally table 9 demonstrates that both models choose the same activation function relu and the number of hidden layers varies between 4 and 10 with the associated number of neurons range between 9 and 80 meanwhile each model chooses a different type of algorithm for optimizing the network it is also worth pointing out that the average computational time for the 8 input dlnn models is much lower than the 10 input dlnn with a 35 lesser time for further investigation the prediction capability for the proposed model is examined for prediction of ntslv using new metrological dataset and compared with results of previous studies using cc indicator as presented in table 10 the results of the current study as well as previous studies confirmed the effectiveness of ml techniques for predicating ntslv for short periods however the current study outperforms other previous studies for long term prediction of ntslv based on cc values it should be noted that the proposed dlnn model with cc 0 96 enhances the prediction capability significantly by 23 in comparison with the initial attempt with the best ml model with cc 0 78 at 1 h lead time prediction on the other hand for long term predication e g 72h the developed model in the current study with cc 0 91 is also improves the prediction capability by 19 in comparison with previous studies with max cc 0 76 5 conclusions slr as a result of climate change is a worldwide issue furthermore on the egyptian north west coast the slr issue is becoming increasingly severe especially with the accelerating urbanization there thus an accurate and reliable slr model must be designed for early warning to help coastal authorities before hazardous occurrences recently numerous slr forecasting models have been produced nonetheless these models suffer from many deficiencies due to the sophisticated nature of the sea trend these deficiencies including the simplicity of time series models based on historical recorded mean sea level data only that ignoring the importance of climatic variables on slv also for few studies that take climatic variables into account those studies don t include all possible climatic variables that may affect ntslv meanwhile these models don t investigate deeply the behavior of slv by decomposing measured sea level into tidal and nontidal components to precisely model slv so the aforementioned models contain weak initial basics that have a huge impact on their predictive stability thus in the current research an integrated approach is proposed to improve the accuracy and stability of slr prediction models specifically powerful data pre processing techniques are applied including wt time frequency analysis alongside t tide harmonic analysis is performed to decompose slv into tslv and ntslv also an eda is performed to investigate the correlation and interdependence of the system variables next four ml algorithms namely gbtr svr gpr and k nnr are examined initially to investigate their capability to build an efficient ntslv model but those algorithms don t give the desired output thus the nca is designed for fs purposes to reduce data dimensionality by tuning its hyperparameters with the hope of achieving high accuracy finally an efficient dlnn model is developed based on the compacted dataset from the fs process to enhance ntslv predictive model furthermore different statistical criteria and visual evaluations are performed to validate the proposed wt dlnn model the results confirm the superiority of the developed wt dlnn model models as it enhances the prediction capability by 19 in comparison with previous studies especially for long term forecasting of ntslv e g 72h therefore integrated nca and wt dlnn can be used as a potential soft computing technique for modelling ntslv whilst current research made substantial improvements in forecasting and modelling the slv process it is important not to overlook the limitations the major limitation of this research is that it is only relevant to marine study locations in the mediterranean sea with similar climatic and hydrodynamic conditions however the approach adopted here can be applied to various research locations with varying environmental conditions for future work the used nca as an fs tool in the proposed model needs to be tested with other metaheuristic algorithms like simulated annealing genetic algorithms and particle swarm optimization to evaluate its performance authorship contribution ahmed alshouny investigation validation writing review editing funding acquisition mohamed t elnabwy conceptualization resources data curation data assessment investigation methodology software formal analysis validation writing original draft writing review editing mosbeh r kaloop data curation data assessment investigation validation review editing ahmad baik validation review yehia miky investigation validation review software availability the ml models training is implemented through matlab environment and regression learner of matlab r2020b also the eda and dlnn python codes are available via the github link https github com ahmedalshouny the eda and dlnn python codes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors extend their appreciation to the deputyship for research innovation ministry of education in saudi arabia for funding this research work through the project no ifphi 234 137 2020 and king abdulaziz university dsr jeddah saudi arabia appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105399 
25597,evapotranspiration et measures the amount of water lost from the earth s surface to the atmosphere and is an integral metric for both agricultural and environmental sciences understanding and quantifying et is critical for achieving effective management of freshwater and irrigation systems however current et estimation models suffer from a trade off between accuracy and spatial coverage in this study we introduce our model quench a neural network architecture that achieves highly accurate et estimates over large continuous spatial extents quench uses our novel attention based convolutional capsule for its neural network layers to identify areas of focus and efficiently extract et information from satellite imagery benchmarks that profile our model s performance show substantive improvements in accuracy with up to 128 increase in accuracy compared to traditional convolutional based and process based models quench also demonstrates consistent model performance over high geospatial variability and a diverse array of regions seasons climates and vegetations keywords evapotranspiration satellite imagery neural networks capsule networks residual learning attention based learning 1 introduction watering schedules for farms are informed by irrigation models the precision of these models dictate the farm s ability to conserve water resources while keeping their crops healthy the key metric used by these irrigation models is evapotranspiration more commonly known as et monteith 1965 et is the amount of water lost from the earth s surface to the atmosphere and is measured in millimeters of water lost per day et is a combination of two separate processes evaporation and transpiration evaporation is when water on the soil s surface changes from a liquid to a gas due to heat transpiration is when water inside of a plant is exhaled from the plants stomata as a gas if we can accurately estimate the rate of et we can then determine how much water an irrigation system needs at any given time there are various instruments and methods available for measuring et a lysimeter is a high precision albeit expensive and cumbersome equipment for measuring et allan et al 2018 another high precision et measuring device the flux tower has been widely accepted and is a less costly solution for measuring et liang et al 2012 however both of these direct on site measurements of et are challenging and or prohibitively expensive also due to the nature of on site data which only captures information from the surrounding area instruments like lysimeters and flux towers cannot measure et across large continuous extents senay 2008 as a result a rich body of work focusing on modeling based approaches for estimating et at scale has been produced gowda et al 2007 mcshane and driscoll roy 2017 recent advances in computational resources satellite imagery and geospatial analysis have allowed these modeling efforts to expand to large continuous extents casu et al 2017 machine learning models in particular have been able to achieve highly accurate et estimations at scale these machine learning models such as cnns convolutional neural networks achieve this by capturing dynamic relationships between remote sensing data and on site et measurements lecun et al 2000 remote sensing data is information captured from a distance e g satellite imagery that covers large contiguous extents using remote sensing data machine learning models are able to estimate et across large continuous areas with precision accuracy however machine learning models often encounter challenges in their ability to generalize and real world applicability karpatne et al 2017 despite recent advances in machine learning and the large amount of available satellite imagery there are still significant challenges in leveraging et estimation at scale this is mostly due to inconsistencies in these models ability to generalize especially over large continuous areas that have a high amount of geospatial variability to overcome the aforementioned challenges we propose quench a model that provides highly accurate et estimates across a diverse range of regional seasonal climatic and vegetative conditions at a high level quench is a neural network architecture that uses our novel neural network layer the attnconvcap attention based convolutional capsule by leveraging the attnconvcap quench accurately captures dynamic relationships between the surrounding area s environmental conditions and on site et measurements resulting in highly accurate et estimates over large continuous extents to address issues with generalization quench incorporates the process based model ssebop operational simplified surface energy balance gabriel et al 2011 into its neural network architecture this improves the model s ability to generalize and addresses the issues that stem from the geospatial sparsity of the on site et measurements in our dataset 1 1 research questions in this study we investigate the following research questions rq1 et related data derived from remote sensing observations have mismatching spatiotemporal resolutions and coverages how can we integrate this data into a single dataset that captures the fine grained environmental meteorological and agricultural conditions of the areas surrounding our et measurement sites rq2 many of the existing machine learning et models have encountered challenges with consistency in their model s performance stemming from a lack of geospatial variability how can we construct a model that performs uniformly over large continuous extents that have high amounts of geospatial variability while achieving state of the art accuracy rq3 reliable et observations are a critical part of our training data however there are only highly sparse et measurements available across the continental u s how can we generate an et estimation model that provides a consistent and uniform ability to generalize despite the sparsity of these et measurements in the training data 1 2 approach overview quench aims to generate highly accurate et estimates that are also resilient to the high amounts of geospatial variability observed over large geospatial extents our model factors in environmental meteorological and agricultural remote sensing data from satellite imagery environmental data source meteorological models and agricultural databases from the satellite imagery we use both the visible e g red green and blue and non visible bands e g thermal infrared from the landsat 8 satellite see section iii also our model retrieves gridded meteorological data from gridmet the university of idaho gridded surface meteorological dataset abatzoglou 2011 the agricultural data we use includes the clim climate and veg vegetation type and date temporal metadata quench uses a deep capsule network to encode and extract information from the satellite imagery to achieve highly accurate et estimates across large continuous extents with significant geospatial variability we propose an approach that uses our novel attconvcap as the neural network layer for quench s deep capsule network capsule based computation is less susceptible to information loss than traditional convolutional layers and are more effective at identifying objects with varying positions and orientations sabour et al 2017 by using capsule based computation quench is able to capture the interactions between the surrounding area s environmental conditions and et more effectively in addition to the deep capsule network quench utilizes the process based model ssebop to overcome the sparsity of the on site et measurements in our training data by utilizing the ssebop model s output in quench s neural network architecture we improve quench s ability to generalize significantly while also preserving the high accuracy of the deep capsule network with its attnconvcap layers we have evaluated quench s performance using k fold cross validation raschka 2018 our model demonstrates a mae mean absolute error of 0 4577 mm on average which represents a 128 improvement in accuracy compared to both the cnn and ssebop models we performed a variogram analysis to evaluate quench s performance over a diverse array of locations and regions quench demonstrated highly consistent model performance across the continental u s the semivariance of quench s accuracy was 0 06 mm2 for up to 50 of latitude longitude in our model analyses see section v we show that quench also performs uniformly over a diverse array of seasons climates and vegetations indicating promising results for quench s ability to generalize over large spatial extents 1 3 paper contributions in this study we propose quench a highly accurate et estimation model with strong resiliency to geospatial variability our specific contributions include c1 we have designed and developed quench an et estimation model that uses both a ssebop model and deep capsule network with attnconvcap layers in its neural network architecture quench effectively captures interactions between environmental meteorological and agricultural conditions and local et measurements to generate highly accurate et estimates at scale our model can generate et estimates for any given location across the continental u s with high accuracy c2 quench s neural network architecture incorporates the process based ssebop model to address the sparsity of et measurements sites in the training data as evidenced by our benchmarks utilizing the ssebop model increases quench s accuracy and ability to generalize c3 this study uses three years of remote sensing observations from across the continental u s that are sourced from the landsat 8 satellite the ameriflux network and gridmet after collecting the data we integrate and process it using interpolation encoding and normalization methods 1 4 paper organization we present our related work in section ii in section iii we describe the data wrangling operations for our et dataset section iv describes our model s methodology the system benchmarks for quench are presented in section v lastly we discuss our conclusions of quench in section vi 2 related work there have been many different types of neural network models used to estimate et in recent years in the paper adeloye et al 2012 the authors use soms self organizing maps an unsupervised neural network model to estimate et soms train their network weights to group inputted data points to neurons with similar feature values e g humidity temperature et once trained the som can then be used to group new unseen data to neurons with similar features values the som is able to group this new input data even if some of its feature values are missing once grouped to neurons with similar feature values the new data replaces its missing values with the neurons corresponding feature values because the som can fill in these missing feature values it can now nowcast et as long as some of the other feature values are available quench s supervised learning differs from the som s unsupervised learning in that it has data grouped into input e g humidity temperature and target et data the som on the other hand can estimate a value for any feature value that is missing from the data not just the target data this unsupervised learning approach works quite well in adeloye et al 2012 and they are able to accurately estimate et from 1 dimensional meteorological data another type of neural network used to estimate et is the rbfn radial basis function network panda et al 2019 like quench the rbfn uses supervised training to train its et model on remotely sensed satellite imagery however like the som the rbfn has neurons that are made up of feature values that are representative of the training data the rbfn s input data is first inputted into these rbf neurons and the similarity between the input data and rbf neuron s feature values are measured this measured similarity is then inputted into a neural network layer which outputs the estimated et the data used to train the som and rbfn models are point based and estimate et using 1d data for a singular point quench is an area based model and extracts spatial patterns from 2d satellite imagery to estimate et for an area consisting of many adjacent points similar to quench the area based u net model in the paper sadeghi et al 2020 is used to nowcast precipitation instead of evapotranspiration using satellite imagery this u net model is an image to image mapper and maps the inputted satellite imagery to an outputted precipitation map of the same size and shape because the u net model is analyzing an entire area instead of a single point it can utilize spatial patterns found in the surrounding area to inform its precipitation estimates in the paper sadeghi et al 2020 the u net model was better able to estimate precipitation especially outlier severe weather events such as hurricane harvey than traditional process based models we posit that our quench model s hybrid neural network and process based approach is also able to achieve highly accurate estimates due to its neural network model while also maintaining highly consistent estimates due to its process based model 3 dataset training quench involves three different types of data environmental meteorological and agricultural we source this data from the following publicly available datasets the landsat 8 satellite the ameriflux network gridmet and the colorado state university arkansas valley research center 3 1 ameriflux the ameriflux network is a network of research sites that measures changes in the atmosphere started in 1996 this relatively large network contains over one hundred sites that can be found throughout north and south america these sites collect on site data from the surrounding area such as changes in the amount of water and carbon dioxide in the atmosphere the metric we will be using from these sites is latent heat flux which is the measurement of heat associated with evaporation and transpiration lost from the earth s surface to the atmosphere by using the eddy covariance model liang et al 2012 latent heat flux can be used to directly measure the amount of water lost from the earth s surface to the atmosphere because it is directly measuring the amount of water lost the eddy covariance model is a highly accurate way to measure et our et dataset covers a three year period from 2015 to 2017 in which only 54 ameriflux network sites are available hobbie et al 2020 anderson 2020 anderson 2020 2020 hollinger and richardson 2019 barron gafford 2021 brunsell 2020a brunsell 2020b 2021 desai 2021a 2021b 2021c 2021d perez ruiz and vivoni 2020 green and kelsey 2020 foster 2021 giblin 2021 ross hinkle 2019 neon national ecological observatory network 2021a neon national ecological observatory network 2021b neon national ecological observatory network 2021c neon national ecological observatory network 2021d neon national ecological observatory network 2021e neon national ecological observatory network 2021f neon national ecological observatory network 2021g neon national ecological observatory network 2021h neon national ecological observatory network 2021i neon national ecological observatory network 2021j neon national ecological observatory network 2021k neon national ecological observatory network 2021l neon national ecological observatory network 2021m neon national ecological observatory network 2021n neon national ecological observatory network 2021o neon national ecological observatory network 2021p neon national ecological observatory network 2021q neon national ecological observatory network 2021r neon national ecological observatory network 2021s neon national ecological observatory network 2021t neon national ecological observatory network 2021u neon national ecological observatory network 2021v neon national ecological observatory network 2021w neon national ecological observatory network 2021x olson 2021 scott 2021 russell 2020 2021a 2021b 2021c silveira 2021 starr 2021a 2021b 2021c craig 2021 vivoni 2020 each of these sites have a flux tower which is an atmospheric measuring device that measures latent heat flux from the surrounding area every 30 min the eddy covariance model takes all 48 of these latent heat flux measurements taken throughout the day and converts them to daily et the eddy covariance model does this by first averaging these latent heat flux measurements and then converting the average to millimeters of water lost throughout the day latent heat flux is measured in watts per square meter w m2 where 28 356 w m2 is equal to 1 mm of water lost through et senay 2008 this process can be seen in formula 1 the outputted daily et value is highly accurate but spatially sparse due to the nature of on site data a flux tower accurately measures approximately 500 m2 of the area surrounding it 1 d a i l y e t 1 48 n 1 48 l a t e n t h e a t f l u x 1 28 356 3 2 lysimeter lysimeters are soil measurement devices comprised of large metal tanks that are buried in the ground and filled with soil underneath the tanks are sensors that measure changes in the soil s mass using these measurements a lysimeter is able to calculate the amount of water gained or lost in the soil throughout the day this is a highly accurate way to measure et because it is directly measuring the amount of water lost from the soil through evaporation and transpiration lysimeters measure the soil s mass every 15 min and calculate the daily et based on these 96 measurements allan et al 2018 like flux towers lysimeters suffer from a sparse spatial coverage due to the nature of on site data also like flux towers lysimeters can accurately measure the et for the 500 m2 of area surrounding the site however unlike the ameriflux network there is not a robust data repository of lysimeters that cover a large range of areas instead lysimeter data is confined to small areas and is usually gathered by independent groups and organizations making collecting this data difficult allan et al 2018 homogenizing this data is also difficult due to the fact that these sites often use different variations of equipment and configurations for their lysimeters we have taken the et measurements from the colorado state university arkansas valley research center s lysimeter site near rocky ford colorado we use this lysimeter s et measurements alongside the et measurements we gathered from the ameriflux network s flux towers as our et dataset s target data 3 3 landsat 8 the landsat 8 satellite was launched in a joint effort between nasa national aeronautics and space administration and usgs united states geological survey it orbits the entire earth approximately every two weeks unfortunately this low temporal resolution can be further exacerbated by cloud coverage in the satellite imagery roughly two thirds of the earth s surface is covered with clouds at any given moment king et al 2013 cloud coverage in satellite imagery can create a lot of noise often to the point where it causes inaccurate results especially in machine learning models we collected the bi weekly satellite imagery for each of our 54 ameriflux sites and lysimeter site over a three year period jan 1 2015 to dec 31 2017 we then removed any satellite images with cloud coverage obscuring a flux tower or lysimeter site from our dataset after doing this we were left with 955 satellite images because these landsat 8 images are remote sensing data they have a large fine grained spatial coverage unlike the on site data from the flux tower and lysimeter sites each point of the satellite imagery represents an area of 30 m2 in order to match the flux tower and lysimeters 500 m2 spatial coverage our satellite images have a resolution of 16pixels2 which represent an area of 480 m2 these landsat 8 images consist of a variety of different bands bands measure an image in different ranges of frequency along the electromagnetic spectrum the eleven landsat 8 bands we use are coastal aerosol red green blue near infrared shortwave infrared 1 shortwave infrared 2 panchromatic cirrus thermal infrared 1 and thermal infrared 2 also gathered from the landsat 8 s satellite imagery are the images corresponding lat latitude lon longitude and elev elevation these bands represent the surrounding area s environmental conditions which are highly influential to the area s et for example temperature heavily influences et and the thermal infrared 1 and thermal infrared 2 bands give us a detailed image of the lst land surface temperature of the surrounding area 3 4 gridmet gridmet is a model that outputs gridded values of meteorological variables the variables we use for training quench are etr reference evapotranspiration sph humidity srad solar radiation tmin minimum daily temperature and tmax maximum daily temperature the grids of these variables are calculated daily and cover the entire continental u s because gridmet is remote sensing data it has a large spatial coverage each point of gridmet covers an area of 4 km2 which is relatively coarse compared to the landsat 8 s 30 m2 resolution to compensate for this low spatial resolution we use bilinear interpolation which increases the gridmet data s resolution to 30 m2 to match the landsat 8 s resolution 3 5 agricultural data in addition to the previously mentioned environmental and meteorologi cal data we have also collected agricultural data for training quench this data includes clim veg and date like lat lon and elev clim and veg are static and do not change over time this makes gathering them relatively easy there are multiple ways this can be done e g map apis and agricultural databases however for our case both the flux tower and lysimeter sites collect this agricultural data alongside their on site data so we have sourced this agricultural data from them once we have gathered all the data for our et dataset we must now encode and normalize it before we can begin training quench 3 6 encoding input data rq1 encoding variables is necessary for machine learning model inputs when the variables are not accurately represented as is the continuous variables from our dataset that require encoding are lat lon and date while the categorical variables that require encoding are clim and veg we input lat and lon into quench because different coordinates will have different environmental factors that affect et one example of this is areas with a higher lat northern u s will have colder environments that lose less water through et than warmer areas which have a lower lat southern u s another example is areas with a higher lon eastern u s will have more humid climates and lose less water through et compared to dryer areas with a lower lon western u s to encode and normalize lat and lon we first convert them into spherical coordinates we do this because 2d x y coordinates do not properly represent the distance between two points on the surface of a sphere we solve this by adding an extra dimension converting lat and lon to 3d cartesian coordinates x y z this process is known as cyclical feature encoding and can be seen in formulas 2 3 and 4 adams and vamplew 1998 after applying cyclical feature encoding to each pair of lat lon coordinates the outputted x y z coordinates are ranged from 1 to 1 2 x s i n π 2 l a t π 180 c o s l o n π 180 3 y s i n π 2 l a t π 180 s i n l o n π 180 4 z c o s π 2 l a t π 180 we input date into quench because different times of the year will have different agricultural conditions that influence et for example a date in the summer will have a higher et because vegetation consumes more water then than a date in the winter when the vegetation has a lower rate of water consumption like lat and lon date also requires cyclical encoding but instead of going from a 2d encoding to a 3d encoding it goes from a 1d encoding to a 2d encoding we do this because date suffers from the same problem as lat and lon where a single dimension does not accurately represent the distance between two points for example the date pairs jul 31 aug 1 and jan 1 dec 31 both have a distance of one day however when calculating the euclidean distance between jan 1 dec 31 we have a distance of 364 days once we convert the date to the day of the year n 365 we then apply the 1d to 2d cyclical feature encoding as shown in formulas 5 and 6 after converting date to the day of the year and applying the cyclical encoding we have 2d x y coordinates accurately representing the distance between different date variables these outputted encoded values also range between 1 and 1 5 x s i n 2 π d a y o f t h e y e a r 1 364 6 y c o s 2 π d a y o f t h e y e a r 364 for the categorical variables veg and clim we use one hot encoding there are nine different veg types in our et dataset one hot encoding the third veg type would result in an array of 0 0 1 0 0 0 0 0 0 the nine veg types we use are from the international geosphere biosphere programme igbp land cover classification system igbp classifies areas of land based on the type and amount of vegetation present the agricultural variable veg heavily influences et because different types of vegetation consume and lose water at different rates similar to veg different clim types are associated with different agricultural conditions that affect et for the clim types we use the köppen climate classification which classifies areas of land based on specific criteria unique to each classification these criteria influence the type of vegetation present in the surrounding area and thus affect the rate of et like veg clim also uses one hot encoding after encoding the previously mentioned data we now normalize the remaining environmental data by subtracting each value by the minimum value and then dividing that by the difference between the maximum and minimum values we normalize each band of the satellite imagery as well as elev 4 methodology to estimate highly accurate et at any given location we have designed a novel neural network architecture quench quench uses a deep capsule networks that captures dynamic relations between local environmental conditions e g lst and et in a hierarchical fashion these conditions often encompass diverse spatial extents at different positions and orientations capsule networks or capsnets are advantageous in the sense that they do not require a pooling operation which makes them less susceptible to information loss than traditional cnns sabour et al 2017 also capsnets have demonstrable performance improvements when identifying objects with different positions or orientations a k a pose a task that cnns often struggle with in capsule theory a capsule is a representation of an object where each object is arranged in a hierarchical fashion for example a low level capsule could represent an ear or a nose while a high level capsule could represent a face in capsnets these capsules are represented with activation vectors of an arbitrary length each element in these activation vectors represents an attribute of that object and its pose these concepts are well aligned with our goals to identify and capture the relationships between fast evolving environmental phenomena and et over a relatively large and diverse geospatial extent 4 1 model architecture fig 1 is a hierarchical representation of quench where quench s inputs include environmental meteorological and agricultural data the details about preprocessing this data are described in section iii f quench is made up of two separate models the ssebop model and deep capsule network the ssebop model is inputted environmental meteorological and agricultural data while the deep capsule network is only inputted the environmental data the deep capsule network uses attnconvcap layers which are particularly designed for capturing relations between attributes in satellite images using both capsule based computation and attention based learning our ssebop model implementation uses a cnn to extract information from the 2d data and a two layer neural network to extract information from the 1d data this information is then used to calculate et using ssebop s thermal index approach finally the knowledge learned from the deep capsule network and ssebop models are combined and outputted as quench s final et estimate 4 1 1 satellite imagery encoder an attention based capsule network rq2 as part of quench we propose a new neural network layer the attnconvcap particularly tailored for capturing dynamic relations between surface observations from satellite imagery and et quench s deep capsule network model is a capsnet that uses residual learning and is made up of attnconvcap layers these attnconvcap layers aim to decrease the computational cost of the capsnet by using a newer implementation of capsule based computation the convolutional capsule which have a significantly lower computational footprint than previous capsnet implementations also the attnconvcap increases the convolutional capsule s accuracy by guiding its capsule based computation to focus on the areas of an image with a higher impact on the model s performance i e attention based learning by combining the convolutional capsule with attention based learning quench s attnconvcap successfully addresses the computational challenges existing in the original capsnet implementation without compromising model accuracy 4 1 2 building a capsule based layer rq2 attention based learning is a machine learning method that enables the model to focus on certain areas of the image and has been used successfully in both natural language processing and computer vision applications vaswani et al 2017 ramachandran et al 2019 quench proposes the novel attconvcap as the neural network layer of the deep capsule network used as quench s satellite imagery encoder as depicted in fig 1 a sam spatial attention mapper is paired with a convolutional capsule layer to generate a single channel attention map that is the same size as the inputted image quench s sam is comprised of seven convolutional layers and eight rnns recurrent neural networks these can be divided up into an input layer two directional layers an attention layer and an output layer the idea behind the sam s architecture is to move the area of attention in varying degrees in four different directions up down left and right to find the area of attention that produces the best results the input layer consists of a single convolutional layer that simply encodes the features of the inputted image each directional layer has four rnns one for each direction and a proceeding convolutional layer to combine their four directional matrices into one the attention layer uses three sequential convolutional layers to create four single channel attention maps one for each direction the outputted directional matrices from each of the directional layer s rnns are first multiplied by their corresponding single channel attention maps from the attention layer before being combined into a single directional matrix by the proceeding convolutional layer by multiplying the attention maps with the directional matrices we move the area of attention in the directions that provide the highest accuracy we do this twice using two sequential directional layers to further improve the accuracy of the outputted directional matrix this directional matrix is then put through the output layer s single convolutional layer which maps the directional matrix to a single channel attention map this single channel attention map comprises of values between 0 and 1 where values closer to 1 require a higher amount of the model s attention and values closer to 0 require a lower amount although the original implementation of capsnets demonstrated a high accuracy on the mnist modified national institute of standards and technology dataset 28pixels2 it could not successfully scale to images with a resolution larger than 64pixels2 the surface conditions of the surrounding area are critical to achieving a high accuracy with quench we designed the attnconvcap with a convolutional capsule layer to address the high computational cost of capsnet layers and a sam to boost the layer s accuracy in contrast to capsnet layers convolutional capsules use a convolutional layer in their routing algorithm to reduce the number of dimensions needed when calculating the routing matrix û the convolutional capsule s routing algorithm is known as locally connected routing while the original capsnet layer s routing algorithm is known as dynamic routing locally connected routing can result in a slightly lower accuracy compared to dynamic routing there are two key reasons for this first low level capsules must be within a certain distance of a high level capsule to be routed to it this distance is equal to the size of the kernel used in the locally connected routing s convolutional layer this distance requirement is not the case with dynamic routing where different tiered capsules can be at any distance from one another and still be routed second each low level capsule at different x y locations must share a single transformation matrix in locally connected routing while in dynamic routing each low level capsule has a unique transformation for each x y location however in our scenario the smaller computational costs i e memory usage and training times achieved by leveraging convolutional capsules layers outweighs the slight loss of accuracy from the original capsnet layer implementation the sam is inputted the same low level capsule matrix that the convolutional layer is inputted and outputs an attention map for every low level capsule this attention map is then multiplied against û which routes low level capsules to high level capsules this attention map guides the focus of routing low level capsules towards more important areas of the image while ignoring capsules in less important areas 4 1 3 constructing a deep capsule network rq2 we implemented our attconvcap neural network layer in a deep capsule network with multiple layers of attconvcap used in its residual learning strategy residual learning models such as the resnet residual neural network or densenet densely connected convolutional neural network are neural networks that input a feature map into a neural network layer and then adds resnet or concatenates densenet the outputted feature map to the original inputted feature map he et al 2015 huang and liu kilian 2016 in contrast non residual learning models do not combine the inputted and outputted feature maps and simply use the latter as the encoded vector residual learning is designed to address the degradation problem which arises when the accuracy of a model decreases as the number of layers in the model increases this occurs because the update to weights in a neural network decreases exponentially the farther a neural network layer is from the final layer in the model also referred to as the vanishing gradients problem these farther away layers weights are then being updated by values that are virtually zero resulting in the weights not changing during training causing the model to perform poorly residual learning not only solves the degradation problem but increases the model s overall accuracy he et al 2015 to address the degradation problem and improve the model s overall accuracy quench stacks multiple attnconvcap layers using the residual learning strategy unlike existing approaches in gugglberger david peer and rodriguezsanchez 2021 bhamidi and el sharkawy 2019 2020 quench uses a convolutional capsule based layer attnconvcap instead of the original capsnet layer implementation we have explored two different methods to stack these layers adding rescaps and concatenating densecaps the inputted and outputted capsule matrices which are made up of activation vectors in our implementations of these deep capsule networks the initial lowest level capsule matrix has eight capsules each with an activation vector of length eight because the rescaps adds the inputted and outputted capsule matrices the capsule matrices stay at that same 8x8 size throughout training we chose this 8x8 configuration because it was large enough in size to maintain a high accuracy for our rescaps model while also small enough in size to initialize our densecaps capsule matrix without running out of memory on the nvidia quadro p2200 gpus we used for training our models the densecaps capsule matrices grow in size because it concatenates the inputted and outputted capsule matrices instead of adding them increasing the outputted capsule matrix s activation vector length by 8 for each densecaps block both the rescaps and densecaps blocks are made up of two attnconvcap layers that output a capsule matrix which is then combined with the inputted capsule matrix for both the rescaps and densecaps implementations we group these blocks into four groups of anderson 2020 brunsell 2020b he et al 2015 desai 2021c deep capsule blocks respectively in between each of these four groups are transition layers that each reduce the length and height of the x y grid of capsules by half we have evaluated the performance for these two strategies see section v and the rescaps model outperforms the densecaps by 11 1 4 1 4 improving model generalization rq3 training a model with highly sparse training data may result in poor model generalization generalization refers to a model s ability to adapt effectively to new previously unseen data as described in section iii a there are approximately one hundred ameriflux sites that measure et in north and south america which is relatively sparse when training a model with reasonable generalization for the entire continental u s to address this challenge we have integrated usgs s ssebop model into quench so that it can estimate et values accurately even when the target location has never been exposed to the model before quench uses the ssebop model to capture relationships between environmental meteorological and agricultural observations and et that are present at any geospatial location the usgs s ssebop model estimates et based on variables such as lst and sph this model is derived from the ssebop approach in senay 2008 gabriel et al 2011 but with tailored parameterizations for operational applications it uses remote sensing data to generate et fractions which are then combined with etr to approximate et using the thermal index approach the ssebop model has demonstrated that it is capable of providing accurate et estimations over large continuous spatial extents although there are several ways of implementing the ssebop model each is built around the same thermal index formula that outputs the estimated et this formula approximates et based on the variables tmax cfactor dt lst and etr which can be seen in formula 7 below 7 e t t m a x c f a c t o r d t l s t d t e t r tmax represents the daily maximum temperature gathered from gridmet lst represents the land surface temperature which can be gathered from either the landsat 8 s thermal infrared 1 or 2 bands in our case we use the former in order to have matching resolutions between the tmax and lst images we use bilinear interpolation on the tmax image etr estimates the water usage of a well watered reference crop such as alfalfa or grass under a set of local weather conditions and is calculated using the penman monteith equation we retrieve the precalculated etr values directly from gridmet which also has the penman monteith equation s parameters if calculating etr on the client side is needed the dt represents the vertical difference between the theoretical dry bare surface temperatures and canopy level air temperatures for each pixel the cfactor a k a temperature correction is used to correspond the tmax values with the cold wet environmental conditions the cfactor can be calculated in a number of ways but usually involves dividing the lst pixels by their corresponding tmax pixels and then filtering out the pixels with low ndvi or lst values ndvi measures the amount of live green vegetation in an area and can be calculated by taking the normalized difference between the near infrared and red bands from the landsat 8 s satellite imagery for our implementation of ssebop we estimate the cfactor using a neural network to address the following challenges first other implementations used to calculate the cfactor are often limited to images with high ndvi and lst values this can be especially limiting in barren areas with little vegetation and or cold temperatures second other cfactor implementations are usually gridded algorithms that are based on focal and zonal statistical methods that often require hand tuning parameters e g kernel sizes number of layers depending on the number and location of pixels with high ndvi and lst values a neural network on the other hand has a single set of parameters for the entire dataset and requires no parameter adjustments between images or areas this makes neural networks well suited for calculating cfactor over a large diverse selection of images and areas one downside to our neural network cfactor implementation is that it outputs a single value to represent the cfactor for an entire area while gridded algorithms output a grid of cfactor values for the area which is a more fine grained representation however like et values cfactor values in an area are quite similar and will have little variation over a small area like 480 m2 likely making the difference in accuracy from using a 16x16 grid to represent the cfactor instead of a single point very low our cfactor model needs to be trained before training the deep capsule network model can begin the neural network for cfactor estimation comprises a cnn for the 2d data a two layer neural network for the 1d data and a final output layer for combining the 2d and 1d encoder s outputs into a single cfactor value two channels are inputted into the cnn the lst image divided by the tmax image and the ndvi image six variables are inputted into the cfactor neural network s 1d encoder the clim veg date elev lat and lon variables the cnn and two layer neural network s outputs are then flattened concatenated and inputted into the final neural network layer which outputs the final cfactor value 5 empirical benchmarks and performance evaluation in order to evaluate how our attnconvcap neural network layer performs we compare seven machine learning models used for quench s satellite imagery encoder we use a cnn convcaps convolutional capsule network attnconvcaps convolutional capsule network with attnconvcaps densecaps denseattncaps densecaps with attnconvcaps rescaps and resattncaps rescaps with attnconvcaps for quench s satellite imagery encoder and compare their results in order to compare these machine learning models we use k fold cross validation splitting our dataset into five subsets of approximately the same size we do this by first sorting the data by location and then sequentially binning each data into five bins e g 1 2 3 4 5 1 2 we then train each of these models on an 80 20 training testing split with the training data comprised of four of the five bins and the testing data comprised of the single remaining bin for each permutation of the bins for a total of five different combinations we train each of our seven models one hundred times for each of these five permutations one hundred trials and seven model types we use random weight initializations resulting in 3500 individually trained model instances for consistency sake we only have five ssebop models instances one for each permutation of the binned data each of the 3500 individually trained model instances uses the same ssebop model weights corresponding to their bin permutation in order to preserve the training testing split for example all the cnn convcaps attnconvcaps densecaps denseattncaps rescaps and resattncaps model instances with a bin permutation of 1 3 4 5 2 for the training testing split uses the weights from the ssebop model which trained with the same 1 3 4 5 2 training testing bin permutation once all the models are trained we record each model s et estimation error for each image in the testing data 5 1 software implementation quench was implemented in pytorch and uses an adagrad optimizer with a step based learning rate decay scheduler we used mse mean squared error as the loss function and the model was trained over three hundred epochs quench s hyperparameters such as the number of layers kernel size etc were chosen based on empirical observations combined with hand tuning the model to ensure a high accuracy also factoring into the choosing of these hyperparameters was that each satellite imagery encoder should have a similar number of layers and nodes in order to make a comparison between them fair especially computation wise our dataset creation involved selecting environmental meteorological and agricultural variables based on their model performance impact each time we removed a variable that did not boost the model s accuracy we retested each of the remaining variables 5 2 model analysis the cnn model is used as the base machine learning model to measure the improvements we get from using capsule based computation attention based learning and or residual learning in the other satellite imagery encoder models comparing the convcaps model to the cnn model gives a good comparison of the changes in both accuracy and computational costs when using convolutional capsule layers instead of convolutional layers comparing the densecaps and the rescaps performance to the convcaps allows us to quantify the increases in performance from using residual learning with convolutional capsules an analysis of the densecaps and rescaps performance gives a clear indication of whether adding or concatenating the capsule matrices is more effective especially in regards to the computational cost finally by comparing the performance of the convcaps to the attnconvcaps the rescaps to the resattncaps and the densecaps to the denseattncaps we are able to evaluate the effectiveness of using our attnconvcap layer instead of a standard convolutional capsule layer cnn the cnn satellite imagery encoder consists of four convolutional layers each followed by a leakyrelu leaky rectified linear unit layer a batch normalization layer and an average pooling layer respectively after these layers a single neural network layer is used to estimate et from the outputted encoded vector the cnn is the least complex of the machine learning models but is a tried and true machine learning model convcaps and attnconvcaps the convcaps architecture is similar to the cnn s except that the last three convolutional layers are replaced with convolutional capsule layers and no average pooling layers are used the number of capsules starts at 32 and is then halved after each layer i e 16 8 and 4 each of these capsules has the same activation vector length of 32 using convolutional capsule layers instead of convolutional layers improves the accuracy of quench s satellite imagery encoder this is due to the convolutional capsule layers increased ability to identify objects with different poses and lower information loss the attnconvcaps model is identical to the convcaps model except instead of convolutional capsule layers it uses attnconvcap layers densecaps and denseattncaps the densecaps model is composed of four groups with anderson 2020 brunsell 2020b he et al 2015 desai 2021c layers of densecaps blocks that each increase the activation vector length by 8 each of the transitional layers between these blocks reduces the activation vector length by half in addition to halving the capsule matrices x y axes length halving the activation vector length is done to prevent the deepcaps capsule matrices from becoming exceedingly large the end result is the final outputted capsule matrix having 8 capsules each with an activation vector of length 255 this large increase in the activation vector length over training causes the densecaps to have a relatively large computational cost similar to what we saw in sun et al 2021 the denseattncaps model is identical to the densecaps with the sole exception of each convolutional capsule layer being replaced by an attnconvcap layer rescaps and resattncaps unlike densecaps our rescaps model adds the inputted and outputted capsule matrices instead of concatenating them keeping the activation vector length the same throughout training also the rescaps transition layers do not halve the length of the activation vectors but still halve the length of the x y axes halving the activation vector lengths is not needed for the rescaps because the activation vector lengths are not growing exceedingly large due to concatenation of the capsule matrices due to the resnet s residual learning strategy the rescaps maintains a relatively low computational footprint while also gaining the benefits of residual learning this rescaps model has been used with the original capsnet implementation to achieve state of the art results in gugglberger david peer and rodriguezsanchez 2021 bhamidi and el sharkawy 2019 2020 but to our knowledge has not been used with multiple convolutional capsule layers gugglberger david peer and rodriguezsanchez 2021 rajasegaran et al 2019 similar to the convcaps and attnconvcaps and the densecaps and denseattncaps the sole difference between the rescaps and resattncaps is the use of attnconvcap layers instead of convolutional capsule layers model accuracy after we trained the different machine learning models used for quench s satellite imagery encoder we average the error of the testing data for each model the proposed model resattncaps demonstrated the lowest mae of 0 4577 mm on the testing data which is a 56 75 decrease in the error compared to the lone ssebop model which achieved a mae of 1 0562 mm the next best performing model was the rescaps model which had a mae of 0 4973 mm 53 01 decrease from ssebop using the attnconvcap in the rescaps model resulted in a 7 96 decrease in the model s mae which is equivalent to 0 04 mm of et the third best performing model was the denseattncaps which had a mae of 0 5149 mm 51 34 decrease from ssebop unsurprisingly the fourth best model was the densecaps which had a mae of 0 5250 mm 50 39 decrease from ssebop the difference between the error of the denseattncaps and densecaps models was 1 92 or 0 01 mm of et using our novel attnconvcap improved the accuracy of both the rescaps and densecaps models but improved the rescaps by four times more than the densecaps the fifth and sixth best performing models were the attnconvcaps followed by the convcaps model which achieved a mae of 0 9285 mm and 0 9395 mm 12 25 and 11 22 decrease from ssebop respectively this again shows that using the attnconvcap instead of the standard convolutional capsule layer improves the model s overall accuracy finally the seventh best performing model was our baseline cnn model which achieved a mae of 1 0442 mm 1 32 decrease from ssebop each of these models mae on the testing data can be seen in fig 2 computational cost let us now take a look at each satellite imagery encoder s cost see table 1 we measure this by taking the memory size of the model s saved weights which are saved in a pt file pytorch tensor file unsurprisingly the largest model is the denseattncaps with a size of 255 mb which had the third best mae the second largest model is thedensecaps with a size of 79 mb which had the fourth best mae the densecaps model is 35 the size of the denseattncaps indicating that the attnconvcap increases the size of this model by almost three times its original size the third largest and best performing model was the resattncaps with a size of 6 mb interestingly the fourth largest was the attnconvcaps with a size of 3 9 mb which had the fifth best model performance the fifth and sixth largest models were the rescaps the second best performing model followed by the convcaps the sixth best performing model each with a size of 3 5 mb the rescaps model is 58 the size of the resattncaps which means using the attnconvcap layers in the rescaps model is roughly doubling the size of the model the smallest model was the cnn which was the worst performing model the key takeaway from these results is that although the denseattncaps and densecaps have significantly larger footprints than the resattncaps and rescaps they perform slightly worse we posit that the smaller rescaps 8x8 capsule matrix is likely representing the dynamic relationships between the environmental data and et more efficiently than the larger densecaps 8x255 capsule matrix this may be due to the densecaps larger capsule matrix having more noise and unuseful elements in its activation vector compared to the rescaps capsule matrix which has a more concise less noisy representation of the dynamic relationships between the environmental data and et as another measure of each satellite imagery encoder s model complexity we timed each of their epochs during training and averaged them each satellite imagery encoder s training time uses the same batch size of 32 the shortest training time is the cnn which took 2 80s to train for each epoch following the cnn was the convcap which took 8 18s to train each epoch which confirms that the convolutional capsule layer based model is more complex than the convolutional layer based model in both memory usage and training time the model with the third shortest training time was the attnconvcap which took 11 41s for each epoch to train the model with the fourth longest training time was the rescaps which took 15 82s to train each epoch the rescaps is followed by the densecaps which had an average epoch time of 20 14s the sixth and seventh shortest epoch times were the resattncaps at 37 08s followed by the denseattncaps at 43 78s the densecaps model takes roughly 27 more time to train than the rescaps even though the rescaps model achieved a higher accuracy this is also true with the denseattncaps which takes 18 longer than the resattncaps but has a lower accuracy one interesting take away from these training times is that using attnconvcap layers in the convcaps model instead of convolutional capsule layers increases the training time by only 28 compare this to using the attnconvcap layers in the rescaps and densecaps models which significantly increases the training time more than doubling both of them this indicates that although using attnconvcap layers instead of standard convolutional capsule layers increases the models accuracy they increase both the computational footprint and training time of the models one key difference between the memory usage and the training times is that the resattncaps and rescaps have a relatively low memory cost compared to their more expensive training times this is because the matrices used in locally connected routing are not saved in the pt file while the convolutional layers weights are the training times unlike the memory usage do get increased when capsule based computation is used because they accurately measure the use of the locally connected routing s matrices overall the resattncaps and rescaps models seem to provide better results than the corresponding denseattncaps and densecaps models while also costing less in both memory and training time 5 3 spatial analysis now that we know that the resattncaps provides the best accuracy of our seven satellite imagery encoders we can now use it in our final quench model for our remaining analyses let us first take a look at the performance of this quench model relative to each flux tower and lysimeter site s geospatial location the flux tower and lysimeter sites are distributed across the continental u s and can be seen in fig 3 although these sites are spaced out some of them tend to be grouped into smaller clusters e g central california southeastern arizona resulting in some of the 54 site markers in fig 3 having some overlap however the visible markers allow us to see which areas perform better than others there is only one marker that has a shade of dark blue showing a mae higher than 1 0 which is located in eastern kansas however on either side of this site are markers with significantly better results indicating that the poorly performing marker is not the product of its geospatial positioning aside from that single poorly performing marker the other markers seem to be evenly distributed and in the range of 0 0 0 8 mm mae let us now look at the variogram of our final quench model in fig 4 variograms are used to measure spatial semivariance among multidimensional coordinates the x axis represents the distance between coordinates in respect to degrees of lat and lon the semivariance metric on the y axis represents the amount of variance from the mae what really stands out in this variogram is the shortness of the range the height of the plotted line which is approximately 0 06 mm2 which is relatively low having a small range indicates that quench s absolute error is not dependent on the location while having a large range indicates that quench s absolute error is heavily influenced by its geospatial location a variogram will always have some amount of range due to the random variability found in statistical data however having a range of 0 06 mm2 indicates that quench performs uniformly across our et dataset s ameriflux and lysimeter sites with the location of sites having no significant correlation to quench s absolute error we can confirm this with the nugget of the variogram the starting height of the plotted line which is virtually zero the nugget represents the small scale variability of our model s error both the low range and low nugget value of the variogram suggests that quench s absolute error is uniform across each of the geospatial locations in our et dataset at both a local and regional scale now that we have analyzed quench s performance in relation to its geospatial coordinates let us now take a look and see if quench s performance is affected by seasonal climatic and vegetative factors 5 4 seasonal analysis first let us take a look at how our final quench model performs in each month of the year in the test data we have a fairly uniform distribution of data in each month the percentage of our et dataset for each month ranges from 6 to 10 which can be seen in fig 5 above in fig 6 we can see a clear pattern that the model performs better in the colder seasons than the hotter ones the winter months perform the best with a mae of 0 33 mm followed by the fall months with a mae of 0 41 mm the spring months with a mae of 0 49 mm and finally the summer months with an mae of 0 62 mm we are able to confirm this by taking the kendall correlation coefficient of each season and their corresponding error which results in winter 0 1216 fall 0 0275 spring 0 0285 and summer 0 1256 although each of these correlation coefficients have a relatively weak relationship greater than 0 2 and less than 0 2 we see that the winter and fall months have a negative correlation while the spring and summer months have a positive correlation this is likely due to the fact that et changes more in the hotter months making it harder to estimate we can confirm this by taking the same kendall correlation coefficients for the ssebop model which are winter 0 0258 fall 0 0477 spring 0 0045 and summer 0 0733 which follow the same pattern of the hotter seasons having a positive correlation and the colder seasons having a negative correlation overall our quench model performs relatively uniform across the different months which we can see in the weak correlation coefficients and small changes in seasonal mae although quench does appear to perform slightly better in the colder seasons 5 5 climate analysis there are a total of ten different clim types in our dataset which in ascending order of mae are bsh hot semi arid csa hot summer mediterranean dfa hot summer humid continental bwk cold desert bsk cold semi arid cwa hurricane influenced humid subtropical dfb warm summer humid continental dwb hurricane influenced warm summer humid continental dfc subarctic and cfa humid subtropical each of their performances can be seen in fig 7 the clim types of our et dataset are less well distributed than the veg type each clim type s percentage of our et dataset ranges from 0 2 to 33 which can be seen in fig 5 the range of mae by clim types seems similar to the range of mae by season where each clim type and season s mae ranges between 0 2 mm and 0 7 mm the exceptions to this are the bsh and csa clim types which each make up less than 1 0 of the total data likely resulting in their lower maes to confirm this we take the kendall correlation coefficient for each clim type which are bsh 0 0423 csa 0 0558 dfa 0 0883 bwk 0 1050 bsk 0 0991 cwa 0 0340 dfb 0 0502 dwb 0 0136 dfc 0 0339 and cfa 0 2340 excluding the cfa clim type each of these correlation coefficients have relatively weak correlations indicating that quench performs uniformly on each of them the cfa humid subtropical clim type on the other hand barely has a medium strength correlation 0 2 0 4 and the highest mae of 0 68 mm the cfa clim type has a relatively high temperature and more complex plant and water ecosystems than the other clim types which is likely the reason that quench performs slightly worse on the cfa clim type another possible reason is that the cfa clim type makes up a third of the total data resulting in a larger distribution of absolute error however the cfa error still has a relatively low correlation coefficient just making it into the medium strength correlation range also the cfa s mae of 0 68 mm is not that much larger than the mae that quench achieved on the remaining clim types 0 45 mm indicating quench performs uniformly on each clim type with some minor variations in its performance on the cfa clim type 5 6 vegetative analysis our dataset contains nine different veg types which in ascending order of mae are cro croplands osh open shrublands wet permanent wetlands wsa woody savannas mf mixed forests dbf deciduous broadleaf forests enf evergreen needleleaf forests gra grasslands and sav savannas each of these veg types performance can be seen in fig 8 the veg types are more evenly distributed than clim types but less uniform than the months with a range of 7 20 excluding the cro which makes up less than 2 of the data when pairing the veg types with their corresponding kendall correlation coefficient we get cro 0 0921 osh 0 1090 wet 0 0955 wsa 0 0552 mf 0 0390 dbf 0 0173 enf 0 0081 gra 0 03981 and sav 0 2227 each of these again have a weak correlation except for the sav veg type the sav veg type makes up approximately one fifth of the data and has a medium correlation coefficient strength likely the reason for this poor performance is that the sav veg type has a higher variety in its tree coverage 10 30 than the other veg types which are either predominantly forested or canopy free areas this diverse mix of herbaceous and other understory systems that the sav veg type has could make estimating accurate et values more difficult for quench compared to the other veg types despite the higher mae of sav 0 79 mm a correlation coefficient of 0 2227 is still fairly low and just reaches the medium strength correlation threshold although quench does appear to struggle more on the sav veg it performs remarkably well on the remaining 80 of the data where quench achieved a mae of less than 0 5 mm on each remaining veg type overall quench is able to achieve a high overall accuracy while maintaining a high quality consistent ability to generalize over a diverse set of regions seasons climates and vegetations with some minor effects to its performance in the hotter months the cfa clim type and the sav veg type 5 7 effectiveness of integrating outputs from the ssebop model to evaluate the effectiveness of using the ssebop model in quench we trained quench with and without the ssebop model and compared their performance we found that using the ssebop model increased quench s overall accuracy by 11 5 we also found that using the ssebop model reduces the number of error outliers in the testing data by 16 2 this indicates that by combining the ssebop and deep capsule network models into quench s architecture we increase quench s accuracy and ability to generalize 6 conclusion we described our model quench as accurately estimating et over large geospatial extents quench addresses the model performance challenges stemming from geospatial variability with its uniquely designed neural network architecture that incorporates a ssebop model and deep capsule network comprised of our novel attnconvcap layers rq1 we integrate environmental meteorological and agricultural datasets and align these based on their spatiotemporal characteristics data collected over three years are encoded e g cyclical encoding and one hot encoding and normalized in preparation for training quench and analyzing its performance rq2 quench s deep capsule network captures the dynamic relationships between local ancillary conditions regardless of its orientation or spatial positioning quench uses attconvcap layers to construct its deep capsule network the novel attconvcap effectively determines the area that quench focuses on while efficiently extracting et information from satellite imagery rq3 quench improves its generalization by leveraging outputs from the process based ssebop model this addresses the input data s sparsity issues which stem from the low number of available et measurement sites overall quench demonstrates highly accurate results and consistent model performance that is suitable for estimating et over large geospatial extents quench accomplishes an mae of 0 4577 mm significantly outperforming the other base models such as the cnn and lone ssebop model quench has demonstrably consistent model performance across our et dataset which encompasses the continental u s and a large variety of regions seasons climates and vegetations software and data availability all the code for the cnn convcaps attnconvcaps densecaps denseattncaps rescaps and resattncaps models are freely and publicly available on github at https github com samarmy attention based convolutional capsules for evapotranspiration estimation at scale as of september 01 2022 this code was developed and is maintained by samuel armstrong who can be contacted at sam armstrong colostate edu python version 3 6 8 was the programming language used for data processing and modeling the neural network library used for developing and training our models was pytorch version 1 6 0 we recommend using hardware with a cuda compatible gpu when running our code although this is not necessary the total size of the model files is 256 kb the data used to train these models is also available on the same github repository this data is made up of numpy files which have a collective size of 42 mb this dataset is comprised of publicly available data from the ameriflux data portal https ameriflux lbl gov data download data usgs earth explorer https earthexplorer usgs gov and gridmet website https www climatologylab org gridmet html the lysimeter data used in this manuscript was provided by the colorado state university arkansas valley research center and can be found on our github repository at https github com samarmy attention based convolutional capsules for evapotranspiration estimation at scale declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national science foundation oac 1931363 aci 1553685 the national institute of food and agriculture col0 fact 2019 and a cochran family professorship funding for the ameriflux data portal was provided by the u s department of energy office of science landsat 8 images were courtesy of the u s geological survey any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25597,evapotranspiration et measures the amount of water lost from the earth s surface to the atmosphere and is an integral metric for both agricultural and environmental sciences understanding and quantifying et is critical for achieving effective management of freshwater and irrigation systems however current et estimation models suffer from a trade off between accuracy and spatial coverage in this study we introduce our model quench a neural network architecture that achieves highly accurate et estimates over large continuous spatial extents quench uses our novel attention based convolutional capsule for its neural network layers to identify areas of focus and efficiently extract et information from satellite imagery benchmarks that profile our model s performance show substantive improvements in accuracy with up to 128 increase in accuracy compared to traditional convolutional based and process based models quench also demonstrates consistent model performance over high geospatial variability and a diverse array of regions seasons climates and vegetations keywords evapotranspiration satellite imagery neural networks capsule networks residual learning attention based learning 1 introduction watering schedules for farms are informed by irrigation models the precision of these models dictate the farm s ability to conserve water resources while keeping their crops healthy the key metric used by these irrigation models is evapotranspiration more commonly known as et monteith 1965 et is the amount of water lost from the earth s surface to the atmosphere and is measured in millimeters of water lost per day et is a combination of two separate processes evaporation and transpiration evaporation is when water on the soil s surface changes from a liquid to a gas due to heat transpiration is when water inside of a plant is exhaled from the plants stomata as a gas if we can accurately estimate the rate of et we can then determine how much water an irrigation system needs at any given time there are various instruments and methods available for measuring et a lysimeter is a high precision albeit expensive and cumbersome equipment for measuring et allan et al 2018 another high precision et measuring device the flux tower has been widely accepted and is a less costly solution for measuring et liang et al 2012 however both of these direct on site measurements of et are challenging and or prohibitively expensive also due to the nature of on site data which only captures information from the surrounding area instruments like lysimeters and flux towers cannot measure et across large continuous extents senay 2008 as a result a rich body of work focusing on modeling based approaches for estimating et at scale has been produced gowda et al 2007 mcshane and driscoll roy 2017 recent advances in computational resources satellite imagery and geospatial analysis have allowed these modeling efforts to expand to large continuous extents casu et al 2017 machine learning models in particular have been able to achieve highly accurate et estimations at scale these machine learning models such as cnns convolutional neural networks achieve this by capturing dynamic relationships between remote sensing data and on site et measurements lecun et al 2000 remote sensing data is information captured from a distance e g satellite imagery that covers large contiguous extents using remote sensing data machine learning models are able to estimate et across large continuous areas with precision accuracy however machine learning models often encounter challenges in their ability to generalize and real world applicability karpatne et al 2017 despite recent advances in machine learning and the large amount of available satellite imagery there are still significant challenges in leveraging et estimation at scale this is mostly due to inconsistencies in these models ability to generalize especially over large continuous areas that have a high amount of geospatial variability to overcome the aforementioned challenges we propose quench a model that provides highly accurate et estimates across a diverse range of regional seasonal climatic and vegetative conditions at a high level quench is a neural network architecture that uses our novel neural network layer the attnconvcap attention based convolutional capsule by leveraging the attnconvcap quench accurately captures dynamic relationships between the surrounding area s environmental conditions and on site et measurements resulting in highly accurate et estimates over large continuous extents to address issues with generalization quench incorporates the process based model ssebop operational simplified surface energy balance gabriel et al 2011 into its neural network architecture this improves the model s ability to generalize and addresses the issues that stem from the geospatial sparsity of the on site et measurements in our dataset 1 1 research questions in this study we investigate the following research questions rq1 et related data derived from remote sensing observations have mismatching spatiotemporal resolutions and coverages how can we integrate this data into a single dataset that captures the fine grained environmental meteorological and agricultural conditions of the areas surrounding our et measurement sites rq2 many of the existing machine learning et models have encountered challenges with consistency in their model s performance stemming from a lack of geospatial variability how can we construct a model that performs uniformly over large continuous extents that have high amounts of geospatial variability while achieving state of the art accuracy rq3 reliable et observations are a critical part of our training data however there are only highly sparse et measurements available across the continental u s how can we generate an et estimation model that provides a consistent and uniform ability to generalize despite the sparsity of these et measurements in the training data 1 2 approach overview quench aims to generate highly accurate et estimates that are also resilient to the high amounts of geospatial variability observed over large geospatial extents our model factors in environmental meteorological and agricultural remote sensing data from satellite imagery environmental data source meteorological models and agricultural databases from the satellite imagery we use both the visible e g red green and blue and non visible bands e g thermal infrared from the landsat 8 satellite see section iii also our model retrieves gridded meteorological data from gridmet the university of idaho gridded surface meteorological dataset abatzoglou 2011 the agricultural data we use includes the clim climate and veg vegetation type and date temporal metadata quench uses a deep capsule network to encode and extract information from the satellite imagery to achieve highly accurate et estimates across large continuous extents with significant geospatial variability we propose an approach that uses our novel attconvcap as the neural network layer for quench s deep capsule network capsule based computation is less susceptible to information loss than traditional convolutional layers and are more effective at identifying objects with varying positions and orientations sabour et al 2017 by using capsule based computation quench is able to capture the interactions between the surrounding area s environmental conditions and et more effectively in addition to the deep capsule network quench utilizes the process based model ssebop to overcome the sparsity of the on site et measurements in our training data by utilizing the ssebop model s output in quench s neural network architecture we improve quench s ability to generalize significantly while also preserving the high accuracy of the deep capsule network with its attnconvcap layers we have evaluated quench s performance using k fold cross validation raschka 2018 our model demonstrates a mae mean absolute error of 0 4577 mm on average which represents a 128 improvement in accuracy compared to both the cnn and ssebop models we performed a variogram analysis to evaluate quench s performance over a diverse array of locations and regions quench demonstrated highly consistent model performance across the continental u s the semivariance of quench s accuracy was 0 06 mm2 for up to 50 of latitude longitude in our model analyses see section v we show that quench also performs uniformly over a diverse array of seasons climates and vegetations indicating promising results for quench s ability to generalize over large spatial extents 1 3 paper contributions in this study we propose quench a highly accurate et estimation model with strong resiliency to geospatial variability our specific contributions include c1 we have designed and developed quench an et estimation model that uses both a ssebop model and deep capsule network with attnconvcap layers in its neural network architecture quench effectively captures interactions between environmental meteorological and agricultural conditions and local et measurements to generate highly accurate et estimates at scale our model can generate et estimates for any given location across the continental u s with high accuracy c2 quench s neural network architecture incorporates the process based ssebop model to address the sparsity of et measurements sites in the training data as evidenced by our benchmarks utilizing the ssebop model increases quench s accuracy and ability to generalize c3 this study uses three years of remote sensing observations from across the continental u s that are sourced from the landsat 8 satellite the ameriflux network and gridmet after collecting the data we integrate and process it using interpolation encoding and normalization methods 1 4 paper organization we present our related work in section ii in section iii we describe the data wrangling operations for our et dataset section iv describes our model s methodology the system benchmarks for quench are presented in section v lastly we discuss our conclusions of quench in section vi 2 related work there have been many different types of neural network models used to estimate et in recent years in the paper adeloye et al 2012 the authors use soms self organizing maps an unsupervised neural network model to estimate et soms train their network weights to group inputted data points to neurons with similar feature values e g humidity temperature et once trained the som can then be used to group new unseen data to neurons with similar features values the som is able to group this new input data even if some of its feature values are missing once grouped to neurons with similar feature values the new data replaces its missing values with the neurons corresponding feature values because the som can fill in these missing feature values it can now nowcast et as long as some of the other feature values are available quench s supervised learning differs from the som s unsupervised learning in that it has data grouped into input e g humidity temperature and target et data the som on the other hand can estimate a value for any feature value that is missing from the data not just the target data this unsupervised learning approach works quite well in adeloye et al 2012 and they are able to accurately estimate et from 1 dimensional meteorological data another type of neural network used to estimate et is the rbfn radial basis function network panda et al 2019 like quench the rbfn uses supervised training to train its et model on remotely sensed satellite imagery however like the som the rbfn has neurons that are made up of feature values that are representative of the training data the rbfn s input data is first inputted into these rbf neurons and the similarity between the input data and rbf neuron s feature values are measured this measured similarity is then inputted into a neural network layer which outputs the estimated et the data used to train the som and rbfn models are point based and estimate et using 1d data for a singular point quench is an area based model and extracts spatial patterns from 2d satellite imagery to estimate et for an area consisting of many adjacent points similar to quench the area based u net model in the paper sadeghi et al 2020 is used to nowcast precipitation instead of evapotranspiration using satellite imagery this u net model is an image to image mapper and maps the inputted satellite imagery to an outputted precipitation map of the same size and shape because the u net model is analyzing an entire area instead of a single point it can utilize spatial patterns found in the surrounding area to inform its precipitation estimates in the paper sadeghi et al 2020 the u net model was better able to estimate precipitation especially outlier severe weather events such as hurricane harvey than traditional process based models we posit that our quench model s hybrid neural network and process based approach is also able to achieve highly accurate estimates due to its neural network model while also maintaining highly consistent estimates due to its process based model 3 dataset training quench involves three different types of data environmental meteorological and agricultural we source this data from the following publicly available datasets the landsat 8 satellite the ameriflux network gridmet and the colorado state university arkansas valley research center 3 1 ameriflux the ameriflux network is a network of research sites that measures changes in the atmosphere started in 1996 this relatively large network contains over one hundred sites that can be found throughout north and south america these sites collect on site data from the surrounding area such as changes in the amount of water and carbon dioxide in the atmosphere the metric we will be using from these sites is latent heat flux which is the measurement of heat associated with evaporation and transpiration lost from the earth s surface to the atmosphere by using the eddy covariance model liang et al 2012 latent heat flux can be used to directly measure the amount of water lost from the earth s surface to the atmosphere because it is directly measuring the amount of water lost the eddy covariance model is a highly accurate way to measure et our et dataset covers a three year period from 2015 to 2017 in which only 54 ameriflux network sites are available hobbie et al 2020 anderson 2020 anderson 2020 2020 hollinger and richardson 2019 barron gafford 2021 brunsell 2020a brunsell 2020b 2021 desai 2021a 2021b 2021c 2021d perez ruiz and vivoni 2020 green and kelsey 2020 foster 2021 giblin 2021 ross hinkle 2019 neon national ecological observatory network 2021a neon national ecological observatory network 2021b neon national ecological observatory network 2021c neon national ecological observatory network 2021d neon national ecological observatory network 2021e neon national ecological observatory network 2021f neon national ecological observatory network 2021g neon national ecological observatory network 2021h neon national ecological observatory network 2021i neon national ecological observatory network 2021j neon national ecological observatory network 2021k neon national ecological observatory network 2021l neon national ecological observatory network 2021m neon national ecological observatory network 2021n neon national ecological observatory network 2021o neon national ecological observatory network 2021p neon national ecological observatory network 2021q neon national ecological observatory network 2021r neon national ecological observatory network 2021s neon national ecological observatory network 2021t neon national ecological observatory network 2021u neon national ecological observatory network 2021v neon national ecological observatory network 2021w neon national ecological observatory network 2021x olson 2021 scott 2021 russell 2020 2021a 2021b 2021c silveira 2021 starr 2021a 2021b 2021c craig 2021 vivoni 2020 each of these sites have a flux tower which is an atmospheric measuring device that measures latent heat flux from the surrounding area every 30 min the eddy covariance model takes all 48 of these latent heat flux measurements taken throughout the day and converts them to daily et the eddy covariance model does this by first averaging these latent heat flux measurements and then converting the average to millimeters of water lost throughout the day latent heat flux is measured in watts per square meter w m2 where 28 356 w m2 is equal to 1 mm of water lost through et senay 2008 this process can be seen in formula 1 the outputted daily et value is highly accurate but spatially sparse due to the nature of on site data a flux tower accurately measures approximately 500 m2 of the area surrounding it 1 d a i l y e t 1 48 n 1 48 l a t e n t h e a t f l u x 1 28 356 3 2 lysimeter lysimeters are soil measurement devices comprised of large metal tanks that are buried in the ground and filled with soil underneath the tanks are sensors that measure changes in the soil s mass using these measurements a lysimeter is able to calculate the amount of water gained or lost in the soil throughout the day this is a highly accurate way to measure et because it is directly measuring the amount of water lost from the soil through evaporation and transpiration lysimeters measure the soil s mass every 15 min and calculate the daily et based on these 96 measurements allan et al 2018 like flux towers lysimeters suffer from a sparse spatial coverage due to the nature of on site data also like flux towers lysimeters can accurately measure the et for the 500 m2 of area surrounding the site however unlike the ameriflux network there is not a robust data repository of lysimeters that cover a large range of areas instead lysimeter data is confined to small areas and is usually gathered by independent groups and organizations making collecting this data difficult allan et al 2018 homogenizing this data is also difficult due to the fact that these sites often use different variations of equipment and configurations for their lysimeters we have taken the et measurements from the colorado state university arkansas valley research center s lysimeter site near rocky ford colorado we use this lysimeter s et measurements alongside the et measurements we gathered from the ameriflux network s flux towers as our et dataset s target data 3 3 landsat 8 the landsat 8 satellite was launched in a joint effort between nasa national aeronautics and space administration and usgs united states geological survey it orbits the entire earth approximately every two weeks unfortunately this low temporal resolution can be further exacerbated by cloud coverage in the satellite imagery roughly two thirds of the earth s surface is covered with clouds at any given moment king et al 2013 cloud coverage in satellite imagery can create a lot of noise often to the point where it causes inaccurate results especially in machine learning models we collected the bi weekly satellite imagery for each of our 54 ameriflux sites and lysimeter site over a three year period jan 1 2015 to dec 31 2017 we then removed any satellite images with cloud coverage obscuring a flux tower or lysimeter site from our dataset after doing this we were left with 955 satellite images because these landsat 8 images are remote sensing data they have a large fine grained spatial coverage unlike the on site data from the flux tower and lysimeter sites each point of the satellite imagery represents an area of 30 m2 in order to match the flux tower and lysimeters 500 m2 spatial coverage our satellite images have a resolution of 16pixels2 which represent an area of 480 m2 these landsat 8 images consist of a variety of different bands bands measure an image in different ranges of frequency along the electromagnetic spectrum the eleven landsat 8 bands we use are coastal aerosol red green blue near infrared shortwave infrared 1 shortwave infrared 2 panchromatic cirrus thermal infrared 1 and thermal infrared 2 also gathered from the landsat 8 s satellite imagery are the images corresponding lat latitude lon longitude and elev elevation these bands represent the surrounding area s environmental conditions which are highly influential to the area s et for example temperature heavily influences et and the thermal infrared 1 and thermal infrared 2 bands give us a detailed image of the lst land surface temperature of the surrounding area 3 4 gridmet gridmet is a model that outputs gridded values of meteorological variables the variables we use for training quench are etr reference evapotranspiration sph humidity srad solar radiation tmin minimum daily temperature and tmax maximum daily temperature the grids of these variables are calculated daily and cover the entire continental u s because gridmet is remote sensing data it has a large spatial coverage each point of gridmet covers an area of 4 km2 which is relatively coarse compared to the landsat 8 s 30 m2 resolution to compensate for this low spatial resolution we use bilinear interpolation which increases the gridmet data s resolution to 30 m2 to match the landsat 8 s resolution 3 5 agricultural data in addition to the previously mentioned environmental and meteorologi cal data we have also collected agricultural data for training quench this data includes clim veg and date like lat lon and elev clim and veg are static and do not change over time this makes gathering them relatively easy there are multiple ways this can be done e g map apis and agricultural databases however for our case both the flux tower and lysimeter sites collect this agricultural data alongside their on site data so we have sourced this agricultural data from them once we have gathered all the data for our et dataset we must now encode and normalize it before we can begin training quench 3 6 encoding input data rq1 encoding variables is necessary for machine learning model inputs when the variables are not accurately represented as is the continuous variables from our dataset that require encoding are lat lon and date while the categorical variables that require encoding are clim and veg we input lat and lon into quench because different coordinates will have different environmental factors that affect et one example of this is areas with a higher lat northern u s will have colder environments that lose less water through et than warmer areas which have a lower lat southern u s another example is areas with a higher lon eastern u s will have more humid climates and lose less water through et compared to dryer areas with a lower lon western u s to encode and normalize lat and lon we first convert them into spherical coordinates we do this because 2d x y coordinates do not properly represent the distance between two points on the surface of a sphere we solve this by adding an extra dimension converting lat and lon to 3d cartesian coordinates x y z this process is known as cyclical feature encoding and can be seen in formulas 2 3 and 4 adams and vamplew 1998 after applying cyclical feature encoding to each pair of lat lon coordinates the outputted x y z coordinates are ranged from 1 to 1 2 x s i n π 2 l a t π 180 c o s l o n π 180 3 y s i n π 2 l a t π 180 s i n l o n π 180 4 z c o s π 2 l a t π 180 we input date into quench because different times of the year will have different agricultural conditions that influence et for example a date in the summer will have a higher et because vegetation consumes more water then than a date in the winter when the vegetation has a lower rate of water consumption like lat and lon date also requires cyclical encoding but instead of going from a 2d encoding to a 3d encoding it goes from a 1d encoding to a 2d encoding we do this because date suffers from the same problem as lat and lon where a single dimension does not accurately represent the distance between two points for example the date pairs jul 31 aug 1 and jan 1 dec 31 both have a distance of one day however when calculating the euclidean distance between jan 1 dec 31 we have a distance of 364 days once we convert the date to the day of the year n 365 we then apply the 1d to 2d cyclical feature encoding as shown in formulas 5 and 6 after converting date to the day of the year and applying the cyclical encoding we have 2d x y coordinates accurately representing the distance between different date variables these outputted encoded values also range between 1 and 1 5 x s i n 2 π d a y o f t h e y e a r 1 364 6 y c o s 2 π d a y o f t h e y e a r 364 for the categorical variables veg and clim we use one hot encoding there are nine different veg types in our et dataset one hot encoding the third veg type would result in an array of 0 0 1 0 0 0 0 0 0 the nine veg types we use are from the international geosphere biosphere programme igbp land cover classification system igbp classifies areas of land based on the type and amount of vegetation present the agricultural variable veg heavily influences et because different types of vegetation consume and lose water at different rates similar to veg different clim types are associated with different agricultural conditions that affect et for the clim types we use the köppen climate classification which classifies areas of land based on specific criteria unique to each classification these criteria influence the type of vegetation present in the surrounding area and thus affect the rate of et like veg clim also uses one hot encoding after encoding the previously mentioned data we now normalize the remaining environmental data by subtracting each value by the minimum value and then dividing that by the difference between the maximum and minimum values we normalize each band of the satellite imagery as well as elev 4 methodology to estimate highly accurate et at any given location we have designed a novel neural network architecture quench quench uses a deep capsule networks that captures dynamic relations between local environmental conditions e g lst and et in a hierarchical fashion these conditions often encompass diverse spatial extents at different positions and orientations capsule networks or capsnets are advantageous in the sense that they do not require a pooling operation which makes them less susceptible to information loss than traditional cnns sabour et al 2017 also capsnets have demonstrable performance improvements when identifying objects with different positions or orientations a k a pose a task that cnns often struggle with in capsule theory a capsule is a representation of an object where each object is arranged in a hierarchical fashion for example a low level capsule could represent an ear or a nose while a high level capsule could represent a face in capsnets these capsules are represented with activation vectors of an arbitrary length each element in these activation vectors represents an attribute of that object and its pose these concepts are well aligned with our goals to identify and capture the relationships between fast evolving environmental phenomena and et over a relatively large and diverse geospatial extent 4 1 model architecture fig 1 is a hierarchical representation of quench where quench s inputs include environmental meteorological and agricultural data the details about preprocessing this data are described in section iii f quench is made up of two separate models the ssebop model and deep capsule network the ssebop model is inputted environmental meteorological and agricultural data while the deep capsule network is only inputted the environmental data the deep capsule network uses attnconvcap layers which are particularly designed for capturing relations between attributes in satellite images using both capsule based computation and attention based learning our ssebop model implementation uses a cnn to extract information from the 2d data and a two layer neural network to extract information from the 1d data this information is then used to calculate et using ssebop s thermal index approach finally the knowledge learned from the deep capsule network and ssebop models are combined and outputted as quench s final et estimate 4 1 1 satellite imagery encoder an attention based capsule network rq2 as part of quench we propose a new neural network layer the attnconvcap particularly tailored for capturing dynamic relations between surface observations from satellite imagery and et quench s deep capsule network model is a capsnet that uses residual learning and is made up of attnconvcap layers these attnconvcap layers aim to decrease the computational cost of the capsnet by using a newer implementation of capsule based computation the convolutional capsule which have a significantly lower computational footprint than previous capsnet implementations also the attnconvcap increases the convolutional capsule s accuracy by guiding its capsule based computation to focus on the areas of an image with a higher impact on the model s performance i e attention based learning by combining the convolutional capsule with attention based learning quench s attnconvcap successfully addresses the computational challenges existing in the original capsnet implementation without compromising model accuracy 4 1 2 building a capsule based layer rq2 attention based learning is a machine learning method that enables the model to focus on certain areas of the image and has been used successfully in both natural language processing and computer vision applications vaswani et al 2017 ramachandran et al 2019 quench proposes the novel attconvcap as the neural network layer of the deep capsule network used as quench s satellite imagery encoder as depicted in fig 1 a sam spatial attention mapper is paired with a convolutional capsule layer to generate a single channel attention map that is the same size as the inputted image quench s sam is comprised of seven convolutional layers and eight rnns recurrent neural networks these can be divided up into an input layer two directional layers an attention layer and an output layer the idea behind the sam s architecture is to move the area of attention in varying degrees in four different directions up down left and right to find the area of attention that produces the best results the input layer consists of a single convolutional layer that simply encodes the features of the inputted image each directional layer has four rnns one for each direction and a proceeding convolutional layer to combine their four directional matrices into one the attention layer uses three sequential convolutional layers to create four single channel attention maps one for each direction the outputted directional matrices from each of the directional layer s rnns are first multiplied by their corresponding single channel attention maps from the attention layer before being combined into a single directional matrix by the proceeding convolutional layer by multiplying the attention maps with the directional matrices we move the area of attention in the directions that provide the highest accuracy we do this twice using two sequential directional layers to further improve the accuracy of the outputted directional matrix this directional matrix is then put through the output layer s single convolutional layer which maps the directional matrix to a single channel attention map this single channel attention map comprises of values between 0 and 1 where values closer to 1 require a higher amount of the model s attention and values closer to 0 require a lower amount although the original implementation of capsnets demonstrated a high accuracy on the mnist modified national institute of standards and technology dataset 28pixels2 it could not successfully scale to images with a resolution larger than 64pixels2 the surface conditions of the surrounding area are critical to achieving a high accuracy with quench we designed the attnconvcap with a convolutional capsule layer to address the high computational cost of capsnet layers and a sam to boost the layer s accuracy in contrast to capsnet layers convolutional capsules use a convolutional layer in their routing algorithm to reduce the number of dimensions needed when calculating the routing matrix û the convolutional capsule s routing algorithm is known as locally connected routing while the original capsnet layer s routing algorithm is known as dynamic routing locally connected routing can result in a slightly lower accuracy compared to dynamic routing there are two key reasons for this first low level capsules must be within a certain distance of a high level capsule to be routed to it this distance is equal to the size of the kernel used in the locally connected routing s convolutional layer this distance requirement is not the case with dynamic routing where different tiered capsules can be at any distance from one another and still be routed second each low level capsule at different x y locations must share a single transformation matrix in locally connected routing while in dynamic routing each low level capsule has a unique transformation for each x y location however in our scenario the smaller computational costs i e memory usage and training times achieved by leveraging convolutional capsules layers outweighs the slight loss of accuracy from the original capsnet layer implementation the sam is inputted the same low level capsule matrix that the convolutional layer is inputted and outputs an attention map for every low level capsule this attention map is then multiplied against û which routes low level capsules to high level capsules this attention map guides the focus of routing low level capsules towards more important areas of the image while ignoring capsules in less important areas 4 1 3 constructing a deep capsule network rq2 we implemented our attconvcap neural network layer in a deep capsule network with multiple layers of attconvcap used in its residual learning strategy residual learning models such as the resnet residual neural network or densenet densely connected convolutional neural network are neural networks that input a feature map into a neural network layer and then adds resnet or concatenates densenet the outputted feature map to the original inputted feature map he et al 2015 huang and liu kilian 2016 in contrast non residual learning models do not combine the inputted and outputted feature maps and simply use the latter as the encoded vector residual learning is designed to address the degradation problem which arises when the accuracy of a model decreases as the number of layers in the model increases this occurs because the update to weights in a neural network decreases exponentially the farther a neural network layer is from the final layer in the model also referred to as the vanishing gradients problem these farther away layers weights are then being updated by values that are virtually zero resulting in the weights not changing during training causing the model to perform poorly residual learning not only solves the degradation problem but increases the model s overall accuracy he et al 2015 to address the degradation problem and improve the model s overall accuracy quench stacks multiple attnconvcap layers using the residual learning strategy unlike existing approaches in gugglberger david peer and rodriguezsanchez 2021 bhamidi and el sharkawy 2019 2020 quench uses a convolutional capsule based layer attnconvcap instead of the original capsnet layer implementation we have explored two different methods to stack these layers adding rescaps and concatenating densecaps the inputted and outputted capsule matrices which are made up of activation vectors in our implementations of these deep capsule networks the initial lowest level capsule matrix has eight capsules each with an activation vector of length eight because the rescaps adds the inputted and outputted capsule matrices the capsule matrices stay at that same 8x8 size throughout training we chose this 8x8 configuration because it was large enough in size to maintain a high accuracy for our rescaps model while also small enough in size to initialize our densecaps capsule matrix without running out of memory on the nvidia quadro p2200 gpus we used for training our models the densecaps capsule matrices grow in size because it concatenates the inputted and outputted capsule matrices instead of adding them increasing the outputted capsule matrix s activation vector length by 8 for each densecaps block both the rescaps and densecaps blocks are made up of two attnconvcap layers that output a capsule matrix which is then combined with the inputted capsule matrix for both the rescaps and densecaps implementations we group these blocks into four groups of anderson 2020 brunsell 2020b he et al 2015 desai 2021c deep capsule blocks respectively in between each of these four groups are transition layers that each reduce the length and height of the x y grid of capsules by half we have evaluated the performance for these two strategies see section v and the rescaps model outperforms the densecaps by 11 1 4 1 4 improving model generalization rq3 training a model with highly sparse training data may result in poor model generalization generalization refers to a model s ability to adapt effectively to new previously unseen data as described in section iii a there are approximately one hundred ameriflux sites that measure et in north and south america which is relatively sparse when training a model with reasonable generalization for the entire continental u s to address this challenge we have integrated usgs s ssebop model into quench so that it can estimate et values accurately even when the target location has never been exposed to the model before quench uses the ssebop model to capture relationships between environmental meteorological and agricultural observations and et that are present at any geospatial location the usgs s ssebop model estimates et based on variables such as lst and sph this model is derived from the ssebop approach in senay 2008 gabriel et al 2011 but with tailored parameterizations for operational applications it uses remote sensing data to generate et fractions which are then combined with etr to approximate et using the thermal index approach the ssebop model has demonstrated that it is capable of providing accurate et estimations over large continuous spatial extents although there are several ways of implementing the ssebop model each is built around the same thermal index formula that outputs the estimated et this formula approximates et based on the variables tmax cfactor dt lst and etr which can be seen in formula 7 below 7 e t t m a x c f a c t o r d t l s t d t e t r tmax represents the daily maximum temperature gathered from gridmet lst represents the land surface temperature which can be gathered from either the landsat 8 s thermal infrared 1 or 2 bands in our case we use the former in order to have matching resolutions between the tmax and lst images we use bilinear interpolation on the tmax image etr estimates the water usage of a well watered reference crop such as alfalfa or grass under a set of local weather conditions and is calculated using the penman monteith equation we retrieve the precalculated etr values directly from gridmet which also has the penman monteith equation s parameters if calculating etr on the client side is needed the dt represents the vertical difference between the theoretical dry bare surface temperatures and canopy level air temperatures for each pixel the cfactor a k a temperature correction is used to correspond the tmax values with the cold wet environmental conditions the cfactor can be calculated in a number of ways but usually involves dividing the lst pixels by their corresponding tmax pixels and then filtering out the pixels with low ndvi or lst values ndvi measures the amount of live green vegetation in an area and can be calculated by taking the normalized difference between the near infrared and red bands from the landsat 8 s satellite imagery for our implementation of ssebop we estimate the cfactor using a neural network to address the following challenges first other implementations used to calculate the cfactor are often limited to images with high ndvi and lst values this can be especially limiting in barren areas with little vegetation and or cold temperatures second other cfactor implementations are usually gridded algorithms that are based on focal and zonal statistical methods that often require hand tuning parameters e g kernel sizes number of layers depending on the number and location of pixels with high ndvi and lst values a neural network on the other hand has a single set of parameters for the entire dataset and requires no parameter adjustments between images or areas this makes neural networks well suited for calculating cfactor over a large diverse selection of images and areas one downside to our neural network cfactor implementation is that it outputs a single value to represent the cfactor for an entire area while gridded algorithms output a grid of cfactor values for the area which is a more fine grained representation however like et values cfactor values in an area are quite similar and will have little variation over a small area like 480 m2 likely making the difference in accuracy from using a 16x16 grid to represent the cfactor instead of a single point very low our cfactor model needs to be trained before training the deep capsule network model can begin the neural network for cfactor estimation comprises a cnn for the 2d data a two layer neural network for the 1d data and a final output layer for combining the 2d and 1d encoder s outputs into a single cfactor value two channels are inputted into the cnn the lst image divided by the tmax image and the ndvi image six variables are inputted into the cfactor neural network s 1d encoder the clim veg date elev lat and lon variables the cnn and two layer neural network s outputs are then flattened concatenated and inputted into the final neural network layer which outputs the final cfactor value 5 empirical benchmarks and performance evaluation in order to evaluate how our attnconvcap neural network layer performs we compare seven machine learning models used for quench s satellite imagery encoder we use a cnn convcaps convolutional capsule network attnconvcaps convolutional capsule network with attnconvcaps densecaps denseattncaps densecaps with attnconvcaps rescaps and resattncaps rescaps with attnconvcaps for quench s satellite imagery encoder and compare their results in order to compare these machine learning models we use k fold cross validation splitting our dataset into five subsets of approximately the same size we do this by first sorting the data by location and then sequentially binning each data into five bins e g 1 2 3 4 5 1 2 we then train each of these models on an 80 20 training testing split with the training data comprised of four of the five bins and the testing data comprised of the single remaining bin for each permutation of the bins for a total of five different combinations we train each of our seven models one hundred times for each of these five permutations one hundred trials and seven model types we use random weight initializations resulting in 3500 individually trained model instances for consistency sake we only have five ssebop models instances one for each permutation of the binned data each of the 3500 individually trained model instances uses the same ssebop model weights corresponding to their bin permutation in order to preserve the training testing split for example all the cnn convcaps attnconvcaps densecaps denseattncaps rescaps and resattncaps model instances with a bin permutation of 1 3 4 5 2 for the training testing split uses the weights from the ssebop model which trained with the same 1 3 4 5 2 training testing bin permutation once all the models are trained we record each model s et estimation error for each image in the testing data 5 1 software implementation quench was implemented in pytorch and uses an adagrad optimizer with a step based learning rate decay scheduler we used mse mean squared error as the loss function and the model was trained over three hundred epochs quench s hyperparameters such as the number of layers kernel size etc were chosen based on empirical observations combined with hand tuning the model to ensure a high accuracy also factoring into the choosing of these hyperparameters was that each satellite imagery encoder should have a similar number of layers and nodes in order to make a comparison between them fair especially computation wise our dataset creation involved selecting environmental meteorological and agricultural variables based on their model performance impact each time we removed a variable that did not boost the model s accuracy we retested each of the remaining variables 5 2 model analysis the cnn model is used as the base machine learning model to measure the improvements we get from using capsule based computation attention based learning and or residual learning in the other satellite imagery encoder models comparing the convcaps model to the cnn model gives a good comparison of the changes in both accuracy and computational costs when using convolutional capsule layers instead of convolutional layers comparing the densecaps and the rescaps performance to the convcaps allows us to quantify the increases in performance from using residual learning with convolutional capsules an analysis of the densecaps and rescaps performance gives a clear indication of whether adding or concatenating the capsule matrices is more effective especially in regards to the computational cost finally by comparing the performance of the convcaps to the attnconvcaps the rescaps to the resattncaps and the densecaps to the denseattncaps we are able to evaluate the effectiveness of using our attnconvcap layer instead of a standard convolutional capsule layer cnn the cnn satellite imagery encoder consists of four convolutional layers each followed by a leakyrelu leaky rectified linear unit layer a batch normalization layer and an average pooling layer respectively after these layers a single neural network layer is used to estimate et from the outputted encoded vector the cnn is the least complex of the machine learning models but is a tried and true machine learning model convcaps and attnconvcaps the convcaps architecture is similar to the cnn s except that the last three convolutional layers are replaced with convolutional capsule layers and no average pooling layers are used the number of capsules starts at 32 and is then halved after each layer i e 16 8 and 4 each of these capsules has the same activation vector length of 32 using convolutional capsule layers instead of convolutional layers improves the accuracy of quench s satellite imagery encoder this is due to the convolutional capsule layers increased ability to identify objects with different poses and lower information loss the attnconvcaps model is identical to the convcaps model except instead of convolutional capsule layers it uses attnconvcap layers densecaps and denseattncaps the densecaps model is composed of four groups with anderson 2020 brunsell 2020b he et al 2015 desai 2021c layers of densecaps blocks that each increase the activation vector length by 8 each of the transitional layers between these blocks reduces the activation vector length by half in addition to halving the capsule matrices x y axes length halving the activation vector length is done to prevent the deepcaps capsule matrices from becoming exceedingly large the end result is the final outputted capsule matrix having 8 capsules each with an activation vector of length 255 this large increase in the activation vector length over training causes the densecaps to have a relatively large computational cost similar to what we saw in sun et al 2021 the denseattncaps model is identical to the densecaps with the sole exception of each convolutional capsule layer being replaced by an attnconvcap layer rescaps and resattncaps unlike densecaps our rescaps model adds the inputted and outputted capsule matrices instead of concatenating them keeping the activation vector length the same throughout training also the rescaps transition layers do not halve the length of the activation vectors but still halve the length of the x y axes halving the activation vector lengths is not needed for the rescaps because the activation vector lengths are not growing exceedingly large due to concatenation of the capsule matrices due to the resnet s residual learning strategy the rescaps maintains a relatively low computational footprint while also gaining the benefits of residual learning this rescaps model has been used with the original capsnet implementation to achieve state of the art results in gugglberger david peer and rodriguezsanchez 2021 bhamidi and el sharkawy 2019 2020 but to our knowledge has not been used with multiple convolutional capsule layers gugglberger david peer and rodriguezsanchez 2021 rajasegaran et al 2019 similar to the convcaps and attnconvcaps and the densecaps and denseattncaps the sole difference between the rescaps and resattncaps is the use of attnconvcap layers instead of convolutional capsule layers model accuracy after we trained the different machine learning models used for quench s satellite imagery encoder we average the error of the testing data for each model the proposed model resattncaps demonstrated the lowest mae of 0 4577 mm on the testing data which is a 56 75 decrease in the error compared to the lone ssebop model which achieved a mae of 1 0562 mm the next best performing model was the rescaps model which had a mae of 0 4973 mm 53 01 decrease from ssebop using the attnconvcap in the rescaps model resulted in a 7 96 decrease in the model s mae which is equivalent to 0 04 mm of et the third best performing model was the denseattncaps which had a mae of 0 5149 mm 51 34 decrease from ssebop unsurprisingly the fourth best model was the densecaps which had a mae of 0 5250 mm 50 39 decrease from ssebop the difference between the error of the denseattncaps and densecaps models was 1 92 or 0 01 mm of et using our novel attnconvcap improved the accuracy of both the rescaps and densecaps models but improved the rescaps by four times more than the densecaps the fifth and sixth best performing models were the attnconvcaps followed by the convcaps model which achieved a mae of 0 9285 mm and 0 9395 mm 12 25 and 11 22 decrease from ssebop respectively this again shows that using the attnconvcap instead of the standard convolutional capsule layer improves the model s overall accuracy finally the seventh best performing model was our baseline cnn model which achieved a mae of 1 0442 mm 1 32 decrease from ssebop each of these models mae on the testing data can be seen in fig 2 computational cost let us now take a look at each satellite imagery encoder s cost see table 1 we measure this by taking the memory size of the model s saved weights which are saved in a pt file pytorch tensor file unsurprisingly the largest model is the denseattncaps with a size of 255 mb which had the third best mae the second largest model is thedensecaps with a size of 79 mb which had the fourth best mae the densecaps model is 35 the size of the denseattncaps indicating that the attnconvcap increases the size of this model by almost three times its original size the third largest and best performing model was the resattncaps with a size of 6 mb interestingly the fourth largest was the attnconvcaps with a size of 3 9 mb which had the fifth best model performance the fifth and sixth largest models were the rescaps the second best performing model followed by the convcaps the sixth best performing model each with a size of 3 5 mb the rescaps model is 58 the size of the resattncaps which means using the attnconvcap layers in the rescaps model is roughly doubling the size of the model the smallest model was the cnn which was the worst performing model the key takeaway from these results is that although the denseattncaps and densecaps have significantly larger footprints than the resattncaps and rescaps they perform slightly worse we posit that the smaller rescaps 8x8 capsule matrix is likely representing the dynamic relationships between the environmental data and et more efficiently than the larger densecaps 8x255 capsule matrix this may be due to the densecaps larger capsule matrix having more noise and unuseful elements in its activation vector compared to the rescaps capsule matrix which has a more concise less noisy representation of the dynamic relationships between the environmental data and et as another measure of each satellite imagery encoder s model complexity we timed each of their epochs during training and averaged them each satellite imagery encoder s training time uses the same batch size of 32 the shortest training time is the cnn which took 2 80s to train for each epoch following the cnn was the convcap which took 8 18s to train each epoch which confirms that the convolutional capsule layer based model is more complex than the convolutional layer based model in both memory usage and training time the model with the third shortest training time was the attnconvcap which took 11 41s for each epoch to train the model with the fourth longest training time was the rescaps which took 15 82s to train each epoch the rescaps is followed by the densecaps which had an average epoch time of 20 14s the sixth and seventh shortest epoch times were the resattncaps at 37 08s followed by the denseattncaps at 43 78s the densecaps model takes roughly 27 more time to train than the rescaps even though the rescaps model achieved a higher accuracy this is also true with the denseattncaps which takes 18 longer than the resattncaps but has a lower accuracy one interesting take away from these training times is that using attnconvcap layers in the convcaps model instead of convolutional capsule layers increases the training time by only 28 compare this to using the attnconvcap layers in the rescaps and densecaps models which significantly increases the training time more than doubling both of them this indicates that although using attnconvcap layers instead of standard convolutional capsule layers increases the models accuracy they increase both the computational footprint and training time of the models one key difference between the memory usage and the training times is that the resattncaps and rescaps have a relatively low memory cost compared to their more expensive training times this is because the matrices used in locally connected routing are not saved in the pt file while the convolutional layers weights are the training times unlike the memory usage do get increased when capsule based computation is used because they accurately measure the use of the locally connected routing s matrices overall the resattncaps and rescaps models seem to provide better results than the corresponding denseattncaps and densecaps models while also costing less in both memory and training time 5 3 spatial analysis now that we know that the resattncaps provides the best accuracy of our seven satellite imagery encoders we can now use it in our final quench model for our remaining analyses let us first take a look at the performance of this quench model relative to each flux tower and lysimeter site s geospatial location the flux tower and lysimeter sites are distributed across the continental u s and can be seen in fig 3 although these sites are spaced out some of them tend to be grouped into smaller clusters e g central california southeastern arizona resulting in some of the 54 site markers in fig 3 having some overlap however the visible markers allow us to see which areas perform better than others there is only one marker that has a shade of dark blue showing a mae higher than 1 0 which is located in eastern kansas however on either side of this site are markers with significantly better results indicating that the poorly performing marker is not the product of its geospatial positioning aside from that single poorly performing marker the other markers seem to be evenly distributed and in the range of 0 0 0 8 mm mae let us now look at the variogram of our final quench model in fig 4 variograms are used to measure spatial semivariance among multidimensional coordinates the x axis represents the distance between coordinates in respect to degrees of lat and lon the semivariance metric on the y axis represents the amount of variance from the mae what really stands out in this variogram is the shortness of the range the height of the plotted line which is approximately 0 06 mm2 which is relatively low having a small range indicates that quench s absolute error is not dependent on the location while having a large range indicates that quench s absolute error is heavily influenced by its geospatial location a variogram will always have some amount of range due to the random variability found in statistical data however having a range of 0 06 mm2 indicates that quench performs uniformly across our et dataset s ameriflux and lysimeter sites with the location of sites having no significant correlation to quench s absolute error we can confirm this with the nugget of the variogram the starting height of the plotted line which is virtually zero the nugget represents the small scale variability of our model s error both the low range and low nugget value of the variogram suggests that quench s absolute error is uniform across each of the geospatial locations in our et dataset at both a local and regional scale now that we have analyzed quench s performance in relation to its geospatial coordinates let us now take a look and see if quench s performance is affected by seasonal climatic and vegetative factors 5 4 seasonal analysis first let us take a look at how our final quench model performs in each month of the year in the test data we have a fairly uniform distribution of data in each month the percentage of our et dataset for each month ranges from 6 to 10 which can be seen in fig 5 above in fig 6 we can see a clear pattern that the model performs better in the colder seasons than the hotter ones the winter months perform the best with a mae of 0 33 mm followed by the fall months with a mae of 0 41 mm the spring months with a mae of 0 49 mm and finally the summer months with an mae of 0 62 mm we are able to confirm this by taking the kendall correlation coefficient of each season and their corresponding error which results in winter 0 1216 fall 0 0275 spring 0 0285 and summer 0 1256 although each of these correlation coefficients have a relatively weak relationship greater than 0 2 and less than 0 2 we see that the winter and fall months have a negative correlation while the spring and summer months have a positive correlation this is likely due to the fact that et changes more in the hotter months making it harder to estimate we can confirm this by taking the same kendall correlation coefficients for the ssebop model which are winter 0 0258 fall 0 0477 spring 0 0045 and summer 0 0733 which follow the same pattern of the hotter seasons having a positive correlation and the colder seasons having a negative correlation overall our quench model performs relatively uniform across the different months which we can see in the weak correlation coefficients and small changes in seasonal mae although quench does appear to perform slightly better in the colder seasons 5 5 climate analysis there are a total of ten different clim types in our dataset which in ascending order of mae are bsh hot semi arid csa hot summer mediterranean dfa hot summer humid continental bwk cold desert bsk cold semi arid cwa hurricane influenced humid subtropical dfb warm summer humid continental dwb hurricane influenced warm summer humid continental dfc subarctic and cfa humid subtropical each of their performances can be seen in fig 7 the clim types of our et dataset are less well distributed than the veg type each clim type s percentage of our et dataset ranges from 0 2 to 33 which can be seen in fig 5 the range of mae by clim types seems similar to the range of mae by season where each clim type and season s mae ranges between 0 2 mm and 0 7 mm the exceptions to this are the bsh and csa clim types which each make up less than 1 0 of the total data likely resulting in their lower maes to confirm this we take the kendall correlation coefficient for each clim type which are bsh 0 0423 csa 0 0558 dfa 0 0883 bwk 0 1050 bsk 0 0991 cwa 0 0340 dfb 0 0502 dwb 0 0136 dfc 0 0339 and cfa 0 2340 excluding the cfa clim type each of these correlation coefficients have relatively weak correlations indicating that quench performs uniformly on each of them the cfa humid subtropical clim type on the other hand barely has a medium strength correlation 0 2 0 4 and the highest mae of 0 68 mm the cfa clim type has a relatively high temperature and more complex plant and water ecosystems than the other clim types which is likely the reason that quench performs slightly worse on the cfa clim type another possible reason is that the cfa clim type makes up a third of the total data resulting in a larger distribution of absolute error however the cfa error still has a relatively low correlation coefficient just making it into the medium strength correlation range also the cfa s mae of 0 68 mm is not that much larger than the mae that quench achieved on the remaining clim types 0 45 mm indicating quench performs uniformly on each clim type with some minor variations in its performance on the cfa clim type 5 6 vegetative analysis our dataset contains nine different veg types which in ascending order of mae are cro croplands osh open shrublands wet permanent wetlands wsa woody savannas mf mixed forests dbf deciduous broadleaf forests enf evergreen needleleaf forests gra grasslands and sav savannas each of these veg types performance can be seen in fig 8 the veg types are more evenly distributed than clim types but less uniform than the months with a range of 7 20 excluding the cro which makes up less than 2 of the data when pairing the veg types with their corresponding kendall correlation coefficient we get cro 0 0921 osh 0 1090 wet 0 0955 wsa 0 0552 mf 0 0390 dbf 0 0173 enf 0 0081 gra 0 03981 and sav 0 2227 each of these again have a weak correlation except for the sav veg type the sav veg type makes up approximately one fifth of the data and has a medium correlation coefficient strength likely the reason for this poor performance is that the sav veg type has a higher variety in its tree coverage 10 30 than the other veg types which are either predominantly forested or canopy free areas this diverse mix of herbaceous and other understory systems that the sav veg type has could make estimating accurate et values more difficult for quench compared to the other veg types despite the higher mae of sav 0 79 mm a correlation coefficient of 0 2227 is still fairly low and just reaches the medium strength correlation threshold although quench does appear to struggle more on the sav veg it performs remarkably well on the remaining 80 of the data where quench achieved a mae of less than 0 5 mm on each remaining veg type overall quench is able to achieve a high overall accuracy while maintaining a high quality consistent ability to generalize over a diverse set of regions seasons climates and vegetations with some minor effects to its performance in the hotter months the cfa clim type and the sav veg type 5 7 effectiveness of integrating outputs from the ssebop model to evaluate the effectiveness of using the ssebop model in quench we trained quench with and without the ssebop model and compared their performance we found that using the ssebop model increased quench s overall accuracy by 11 5 we also found that using the ssebop model reduces the number of error outliers in the testing data by 16 2 this indicates that by combining the ssebop and deep capsule network models into quench s architecture we increase quench s accuracy and ability to generalize 6 conclusion we described our model quench as accurately estimating et over large geospatial extents quench addresses the model performance challenges stemming from geospatial variability with its uniquely designed neural network architecture that incorporates a ssebop model and deep capsule network comprised of our novel attnconvcap layers rq1 we integrate environmental meteorological and agricultural datasets and align these based on their spatiotemporal characteristics data collected over three years are encoded e g cyclical encoding and one hot encoding and normalized in preparation for training quench and analyzing its performance rq2 quench s deep capsule network captures the dynamic relationships between local ancillary conditions regardless of its orientation or spatial positioning quench uses attconvcap layers to construct its deep capsule network the novel attconvcap effectively determines the area that quench focuses on while efficiently extracting et information from satellite imagery rq3 quench improves its generalization by leveraging outputs from the process based ssebop model this addresses the input data s sparsity issues which stem from the low number of available et measurement sites overall quench demonstrates highly accurate results and consistent model performance that is suitable for estimating et over large geospatial extents quench accomplishes an mae of 0 4577 mm significantly outperforming the other base models such as the cnn and lone ssebop model quench has demonstrably consistent model performance across our et dataset which encompasses the continental u s and a large variety of regions seasons climates and vegetations software and data availability all the code for the cnn convcaps attnconvcaps densecaps denseattncaps rescaps and resattncaps models are freely and publicly available on github at https github com samarmy attention based convolutional capsules for evapotranspiration estimation at scale as of september 01 2022 this code was developed and is maintained by samuel armstrong who can be contacted at sam armstrong colostate edu python version 3 6 8 was the programming language used for data processing and modeling the neural network library used for developing and training our models was pytorch version 1 6 0 we recommend using hardware with a cuda compatible gpu when running our code although this is not necessary the total size of the model files is 256 kb the data used to train these models is also available on the same github repository this data is made up of numpy files which have a collective size of 42 mb this dataset is comprised of publicly available data from the ameriflux data portal https ameriflux lbl gov data download data usgs earth explorer https earthexplorer usgs gov and gridmet website https www climatologylab org gridmet html the lysimeter data used in this manuscript was provided by the colorado state university arkansas valley research center and can be found on our github repository at https github com samarmy attention based convolutional capsules for evapotranspiration estimation at scale declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national science foundation oac 1931363 aci 1553685 the national institute of food and agriculture col0 fact 2019 and a cochran family professorship funding for the ameriflux data portal was provided by the u s department of energy office of science landsat 8 images were courtesy of the u s geological survey any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25598,the increasingly common occurrence of mid winter breakups mwbs in canadian rivers consisting of the early breakup of ice cover outside of the typical spring season is a cause for concern this study applied various data driven modelling techniques to predict mwbs occurrence and timing with sufficient lead times on a national scale using a new canadian river ice database crid coupled with national resources canada gridded climate data a two level machine learning model structure was developed with the first level model predicting mwb occurrence within a given period and the second level model predicting the timing of an mwb occurrence within that period machine learning techniques that can handle class imbalance were employed to address many of the issues inherent in rare event forecasting including the implementation of data preprocessing the selection of algorithms and performance metrics suited to rare events multiple configurations of both model levels including variations on time series arrangement and input variables were tested to select the optimal model structure the best performing configuration focussing on a biweekly time period attained overall accuracies of 80 1 and 77 6 for the first and second level models respectively on the 452 mwbs in the crid in addition probabilistic prediction results were analyzed to evaluate model uncertainty and robustness this new modelling framework provides the first tool capable of predicting mwbs on a national scale with easily extendable methodology to locations that have not yet experienced mwbs and can form the basis of vital decision making support to affected communities keywords river ice mid winter breakup rare event forecasting imbalanced learning prediction data driven modelling software and data availability analysis in this study was completed using python v3 7 https www python org downloads release python 370 data used in this study is available through environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d and natural resources canada https cfs nrcan gc ca projects 3 4 1 introduction the spring breakup of river ice cover is a yearly occurrence on rivers throughout cold regions of the world with many environmental processes depending on both the timing and mechanism of the breakup beltaos et al 2003 the combination of runoff from snowmelt and melting of ice cover is important to ecosystems especially those that depend on the higher than usual water levels generated by these types of events beltaos et al 2006 however in recent years it has been noted that many rivers in canada have become vulnerable to mid winter breakups mwbs which are the early breakup of river ice cover caused by unseasonably high temperatures and the occurrence of liquid precipitation prowse et al 2002 carr and vuyovich 2014 while the ice cover typically re establishes as temperatures return to seasonal values the flow regime for the remainder of the season is significantly altered huntington et al 2003 spring breakups following these events are often milder with lower flows and water levels which can have significant environmental impacts to those processes that rely on spring breakups to drive higher water levels beltaos et al 2003 however mwbs also carry the risk of river ice jam formation where the ice rubble can freeze in place creating a semi permanent dam on the river that can trigger severe flooding beltaos 2003 to reduce such risk of severe flooding and their long term impacts on the river system effective means of predicting mwb events are of great interest to affected communities though past studies have attempted to forecast these events they have been limited both by the effectiveness of the methodology and the availability of data newton et al 2017 analysis of the spring breakup processes on rivers typically driven by more gradual temperature increases in combination with snowmelt runoff beltaos et al 2006 has been conducted through application of large scale data analysis techniques at regional scales for example de rham et al 2008a studied the high water events driven by the changing spatial distribution of breakups in the mackenzie river basin in canada through analysis of water survey of canada gauges within the basin it was concluded that spring breakups are trending towards earlier occurrence and longer affected periods de rham et al 2008b trend analysis of these events identified factors that influenced the timing and magnitude of breakups while also reinforcing the trend towards earlier and longer breakups goulding et al 2009 with additional spatial mapping and analysis of peak flows reinforcing these findings the breakup trends in the peace athabasca delta in canada has also been well studied the delta has seen a notable decreasing trend in ice jam occurrence leading to perched basins which rely on the flooding triggered by these events to be replenished beginning to reduce timoney 2002 regulation on the peace river upstream has been generally accepted as the cause and subsequent research has focussed on investigating changes in ice jam frequency and severity through statistical and modelling based means in the complex delta beltaos 2017 timoney et al 2018 jasek 2019a 2019b statistical data analysis has also identified factors that are best indicators of ice jam occurrence in the delta such as winter temperatures and precipitation lamontagne et al 2021 though these studies have investigated factors driving breakup and breakup related events on a regional scale their focus was limited to the more easily monitored spring breakup events and limited to singular regions research into mwbs has historically encountered issues related to data availability and spatial coverage with trends being successfully modelled while forecasting is often difficult a trend towards earlier spring breakups on the saint john river triggered by an increase in mwbs was noted by beltaos 1999 with a notable increasing trend in the occurrence of mild winter days beltaos and prowse 2009 subsequently noted an increasing risk of spring and mid winter ice jams on the river as a results of climate change effects prowse et al 2002 and carr and vuyovich 2014 conducted research into the drivers of mwbs producing predictive thresholds based on trends in temperature and precipitation though these thresholds were entirely accurate for the specific location for which they were developed they were found to not translate with a meaningful accuracy to other locations through a regional investigation of mwbs in western canada and the united states newton et al 2017 their research identified 52 mwbs in the considered region with the aforementioned thresholds being accurate for only 11 and 60 of the events respectively as those studies were limited by the available data the mwb database developed by newton et al 2017 which covers the western portion of canada and the united states was one of the largest collections of data on these types of events though these studies developed some methods of simple predictions of mwbs there has been no attempt at the creation of a comprehensive forecasting tool for these events in comparison to mwbs the prediction of spring breakup events timing through machine learning has been much more thoroughly researched though these projects often focussed on a single river or river network and are based around events with a more reliable timing simple single model algorithms such as artificial neural networks zhao et al 2012 guo et al 2018 and support vector machines wang et al 2010 barzegar et al 2019 as well as more complicated algorithmic structures such as adaptive neuro fuzzy inference systems wang et al 2012 sun and trevor 2015 2018 and stacking ensembles sun 2018 have had ranges of success in spring breakup prediction one such example is the prediction of spring breakup ice jams which have been predicted with logistic regression white 1996 artificial neural networks massie et al 2002 hybrid neuro fuzzy systems mahabir et al 2006 and stacking ensembles de coste et al 2021 like prediction of breakup timing these studies were often limited to specific rivers with an added problem of accounting for rare event forecasting a challenging data issue in machine learning that can lead to biased results from models the rare event forecasting issue would extend to mwb forecasting attempts as well and has never been addressed so far this paper focusses on the development and application of a two level modelling framework for the prediction of mwb occurrence and timing on a national scale applying machine learning and probabilistic analysis techniques while avoiding common issues that challenge rare event forecasting the study combines data of 452 mwbs from affected 52 rivers throughout canada with a national scale climate dataset to facilitate development of four machine learning algorithms adaptive boosting class switching k nearest neighbors and adaptive resampling and combining x4 this new approach allows for the prediction of an mwbs occurrence within a given time period using the first level model and the subsequent prediction of the mwbs timing within the period using the second level model the developed models are tested and validated from both a classification and probabilistic standpoint allowing the best model configuration to be selected this is the first successful application of these modelling techniques in the prediction of mwbs on a national scale it provides an easily configured methodology for the development of models predicting these events that is easily transferable to other locations 2 data 2 1 crid the primary source of historical ice data for this study was the canadian river ice database crid an extensive collection of data taken from a select set of water survey of canada gauges located across the country de rham et al 2020 this dataset was compiled through analysis and extraction of water level time series utilising visual inspection techniques first proposed by beltaos 1990 to identify the key river ice events of each season based on the observed changes of water levels data from 196 river gauges in canada are included in the crid with 46 located on regulated rivers and 150 on unregulated rivers fifteen river ice related events e g freeze up winter low flows and levels mwbs secondary low flows and levels breakups etc are included with corresponding water levels flows and dates recorded if available included in these events are mwbs at gauges where they have been historically observed identified as peaks in the flow record outside of typical spring breakup periods for the gauge and associated with higher than average air temperatures recorded at nearby climate gauges each mwb includes the date flow and water level at both the initiation of the event and the peak flow if available fig 1 shows the 196 gauges of the crid with the 52 gauges that have had historical mwb events highlighted these gauges were concentrated in the southwest british columbia and alberta and southeast ontario quebec new brunswick and newfoundland and labrador portions of the country these regions fall within the relatively warmer portions of the country that are more vulnerable to mid winter thaws that trigger mwbs in total 452 mwbs are included in the crid amongst the 52 identified stations the time span covered by the crid varies between gauges with the earliest mwb on record occurring in 1955 but most 301 out of 452 mwbs occurring after 1990 water levels at seven gauges with mwbs 28 station years were corrected for outliers using historical notes relating to gauge datum changes and in consultation with environment and climate change canada 2 2 nrcan gridded climate data climate data corresponding to mwb stations were extracted from the national resources canada nrcan daily gridded climate dataset https cfs nrcan gc ca projects 3 4 the nrcan dataset provides daily maximum and minimum air temperatures and total precipitation data with a resolution of 0 1 at a national scale the data spans 1950 2015 and is derived from quality controlled non homogenized station data taken from the national climate data archive ncda of environment and climate change canada data hutchinson et al 2009 this station data has been interpolated onto a high resolution grid i e 0 1 0 1 through thin plate splines mckenney et al 2011 hopkinson et al 2011 in this study climate data at the closest grid point to each crid gauge was extracted using geographic information system mapping techniques providing a complete time series of the three extracted variables for all crid gauges maximum and minimum air temperatures were averaged to obtain daily mean air temperatures then the accumulated freezing degree days afdd was calculated as the sum of temperatures below 0 c beginning from a reference date of october 1st which is the beginning of the canadian water year boyd 1979 total precipitation cumulatively summed from the freeze up date of river ice cover was also calculated these two values represent how cold a particular season has been and how much snowpack has fallen respectively 3 methods 3 1 model construction with imbalanced data data imbalance is a common issue in rare event forecasting especially in the case of binary classification where there is a significant difference in the number of observations in the majority class and the minority class maalouf and trafalis 2011 li et al 2016 in this study there is a massive imbalance present in the data between the majority class periods or days with no mwb and minority class periods or days with an mwb the majority class makes up 92 99 of the dataset depending on the configuration without any modifications to the traditional algorithmic construction process a common result of these imbalances is the classification of all cases as belonging to the majority class while this would represent an accuracy of 92 99 the results would be useless in practice as none of the rare events were properly classified to address these issues changes to the algorithm construction process can be implemented to data preprocessing the algorithm selection and the performance metric selection haixiang et al 2017 3 1 1 data preprocessing and resampling resampling is the most common and versatile data preprocessing technique for imbalanced datasets due to its independence from the selected classifiers lopez et al 2013 two common methods of resampling are over sampling which works by duplicating instances of the minority class chawla et al 2002 and under sampling which works by removing instances of the majority class tahir et al 2009 hybrid methods combining both over sampling and under sampling exist but their utilisation is rare under sampling was selected in this study due to its versatility and effectiveness in computation time when there are hundreds of samples in the minority class as there are in the mwb datasets napierala and stefanowski 2015 loyola gonzález et al 2016 an even balance between the classes was used due to its past successes in application to classification without affecting generalization of the model results koziarski 2020 the under sampling and even balance techniques have had previous success in rare event forecasting of class imbalanced data li et al 2013 kumar et al 2014 sun et al 2015 3 1 2 algorithm selection ensemble techniques which train multiple classifiers and some traditional single model classifiers are recommended for imbalanced learning including cases where data preprocessing has been applied galar et al 2012 in this study four model algorithms with previous success in addressing class imbalances in combination with preprocessing and performance metric selection haixiang et al 2017 were tested and are detailed below adaptive boosting adaboost the adaboost algorithm develops an ensemble of iterative classification trees with associated class labels zhu et al 2009 an initial classifier is constructed and used to classify observations until an incorrect classification occurs a subsequent tree is then developed with the weight of that point boosted using the stagewise additive modelling using a multi class exponential samme loss function this process continues until all training data has been run through the iterative ensemble the overall goal of the process is the minimization of misclassification rates through the boosting of the weight of a weak learners accuracy the final model label output by the ensemble is calculated using equations 1 and 2 1 f t x t 1 t c t h t x 2 f t x s i g n f t x where h t x is the ensemble of t hypotheses tuned by the user from the input vector x ranging from 0 to 1 c t is the weight of each learner with c t satisfying the conditions c t 0 and t 1 t c t 1 f t x is the weighted hypothesis and f t x is the final model hypothesis ratsch et al 2001 these models have been applied in the forecasting of rainfall runoff and rare dust storms liu et al 2014 zhang et al 2014 k nearest neighbors knn knn produces single models which function by classifying unlabelled samples based on the applied labels of the k nearest neighbors with k tuned by the user dudani 1976 each new observation classified to the model is labelled based on a weighting factor w j described by equation 3 3 w j j 1 k d k d j d k d 1 d k d 1 1 d k d 1 where the distance between neighboring points is described by d j j 1 k and the weight for an individual point is w j guo et al 2003 these models have been successfully applied to the rare event forecasting of ice jam occurrence semenova et al 2020 and real time flood forecasting liu et al 2020 class switching class switching is an ensemble approach where class labels are randomly flipped during model training allowing a more diverse classifier to be developed breiman 2000 the ensemble is composed of classification trees similar to the adaboost algorithm with each individual classifier generated based on the original training data with class labels switched at a rate defined by equations 4 6 4 p j i w p j f o r i j 5 p i i 1 w 1 p i 6 w p 1 j p j 2 where p j i is the probability that an element with the label i gets labelled as j p i i the probability that an element with the label i remains labelled i p i the proportion of elements in the training set labelled i p j the proportion of elements in the training set labelled j and w is proportional to the switching rate p a factor tuned by the user martinez munoz and suarez 2005 applied the algorithm successfully to a variety of binary classification tasks including rare event forecasting adaptive resampling and combining x4 arcx4 arcx4 is an algorithm belonging to the adaptive resampling and combining arc family of iterative ensemble classifiers breiman 1996 at each stage of model construction the model resamples a portion of the training set to generate a classifier c n where n ranges from 1 to k based on a probability described by equation 7 7 p n 1 m n 4 1 m n 4 where p n is the probability for resampling of the nth classifier and m n is the number of misclassifications of the n 1st classifier the results of each classifier are combined through unweighted voting with the number of classifiers k tuned by the user when applied to imbalanced classification the algorithm was found to outrank several other ensemble techniques narassiguin et al 2016 each of the above described models requires the tuning of hyperparameters to select the best model configuration for the given task for this application models were trained using five fold cross validation the data is subdivided into five portions with a training and testing process being conducted five times such that each partition is used as the testing data the possible considered values of the parameter s of each model are organized into grids and an exhaustive grid search was used to test every possible combination of parameters for each partition of data refaeilzadeh et al 2009 the ability of each configuration to correctly classify mwbs was averaged to give a performance assessment and the configuration with the best performance was used to select the optimum model topology finally a stratified 80 20 training testing split was used to develop the classification models 3 1 3 performance metrics performance metrics for the assessment of model tuning and final model accuracy were selected to focus on the correct classification of the minority case or rare event the occurrence of an mwb these metrics are based around a comparison of the actual class and the predicted class of an observation assessing it with one of four outcomes true positive tp denotes the correct prediction of an mwb true negative tn denotes the correct prediction of no mwb false positive fp denotes the prediction of an mwb when one does not occur and false negative fn denotes the prediction of no mwb when one does occur the primary metric for assessment is recall which assesses the correct classification of the minority class with additional assessment metrics of specificity correct classification of the majority case and balanced accuracy also calculated broderson et al 2010 for each values will range between 0 and 1 with 1 representing perfect classification and are calculated using equation 8 10 8 r e c a l l t p t p f n 9 s p e c i f i c i t y t n t n f p 10 b a l a n c e d a c c u r a c y s p e c i f i c i t y r e c a l l 2 each of the four considered algorithms in section 3 1 2 internally computes binary classifications by first producing a probabilistic prediction that an observation falls into a given class before conversion into a final binary value in the case of a single model algorithm a single probability is produced while ensemble algorithms use a mean of the values produced by each ensemble member pedregosa et al 2011 these initial probabilistic predictions were also extracted and assessed using the metrics of brier loss and log loss roulston 2007 both loss functions compare the predicted probability of a sample being in the minority class to the actual binary value according to equations 11 and 12 11 b r i e r l o s s 1 n i 1 n p y i y i 2 12 l o g l o s s 1 n i 1 n y i log p y i 1 y i log 1 p y i where y i is the binary value of the event and p y i is the predicted probability of the event for n events for each of the above metrics a value of 0 represents perfect probabilistic prediction while values above 0 25 for brier loss and 0 675 for log loss represent unacceptable accuracy no better than chance roulston 2007 and vovk 2015 calibration curves plotting the mean of predicted probabilities against the mean of the target variable for a set of observations were also used to evaluate the model performance a perfectly calibrated probabilistic classifier would output a straight line from 0 to 1 niculescu mizil and caruana 2005 3 2 two level model framework a two level modeling framework was developed to allow for versatility in monitoring practice the first level model focuses on an individual period of the year producing a binary prediction of whether or not an mwb would occur within that period if the first level model predicts an mwbs occurrence the second level model would then be used to predict the timing of the event producing a three day ahead binary prediction of an mwb fig 2 shows a flowchart of the model structure both models would use unique datasets featuring data easily monitored well in advance of an mwb though the first level model is used to trigger the use of the second level model there is no direct feed of data or results from the first level model into the second level 3 2 1 first level model the primary goal of the first level model was to identify whether an mwb would occur in a given time period between december 15 to march 15 the duration of the target time period was the first aspect of model development requiring testing four different time series configurations were considered weekly biweekly triweekly and monthly as illustrated in fig 3 in this figure three hypothetical example mwb timings are provided to illustrate how data would be included with all time periods preceding an mwb included and periods after the mwb has occurred excluded in years where an mwb does not occur all periods would be included the different input variables for each shown time series configuration are listed in table 1 these variables include values related to the key river ice conditions preceding an mwb and the climatic conditions at the start of freeze up and directly preceding the period of interest suggested by sun 2018 and de coste et al 2021 once the optimum time series configuration was identified it would be used to train and test the four algorithms to assess the accuracy of the chosen methods 3 2 2 second level model the second level model focusses on three day ahead prediction of mwb occurrence within an identified period of interest in the first level model providing sufficient medium range notice of potential alterations to flow le et al 2019 jurlina et al 2019 this model would look at each possible day within the period and classify the mwb occurrence based on data available up to three days ahead of the day of interest variables would be limited to climatic variables as factors such as warming and precipitation preceding the event have been found to be the primary drivers of mwb occurrence carr and vuyovich 2014 ice event related variables such as those associated with freeze low winter flow and low winter level used as inputs for the first level model would be too far from this event on a time scale and would provide no useful information at this prediction resolution the initial pool of variables would include daily temperatures afdd and changes in afdd and daily precipitation and total seasonal precipitation an input omission process would be used to select the optimum variable set for this prediction a generalized adaboost model would be constructed using all variables to be sure that the effects of each was considered followed by the exclusion of each variable in turn to see how the recall and balanced accuracy change with the removal if the values increased or did not change it would indicate that the variable in question is potentially redundant and can be removed from the model without negative impact this would be further verified through the testing of the omission of larger groups of potentially redundant variables to reduce the input set to the most relevant values snieder et al 2020 the final input set would then be used to test and train the four algorithms detailed above for the prediction of mwb timing 3 3 model implementation this study utilised python version 3 7 for all analysis van rossum and drake 2009 additional analysis was conducted using the packages numpy oliphant 2006 seaborn waskom and the seaborn development team 2020 pandas mckinney and others 2010 scikit learn pedregosa et al 2011 ensemble narassiguin et al 2016 imblearn lemaitre et al 2017 and scipy virtanen et al 2020 4 results and analysis 4 1 first level model 4 1 1 time series configuration the first level model aimed to predict the occurrence of an mwb in a given time period between december 15 to march 15 with the four time periods considered being weekly biweekly triweekly and monthly the initial number of observations was 4098 for the monthly configuration 6965 for the triweekly configuration 8316 for the biweekly configuration and 10 456 for the weekly configuration the variables for each configuration were first normalised through division by means and then balanced between positive and negative cases using under sampling an adaboost model was trained for each configuration using 5 fold cross validation with an 80 20 training testing split the performance of each configuration using two classification metrics i e balanced accuracy and recall is presented in table 2 the biweekly model was selected as the most successful of the tested configurations greatly outperforming the monthly and triweekly models and producing comparable performance to the weekly model however with a simpler initial dataset 4 1 2 first level model results and analysis additional knn class switching and arcx4 models were trained on the same data from the biweekly time series configuration as the adaboost model with the results of each for both classification and probabilistic predictions shown in table 3 deterministic classification metrics indicate that the arcx4 model performed the strongest at binary classification with the highest obtained balanced accuracy and recall the arcx4 also performed well with probabilistic predictions however it was slightly outperformed by the class switching model in all considered metrics while the adaboost and knn models performed comparably well to the arcx4 and class switching models in deterministic classification they were found to have a more significant reduction in accuracy in probabilistic predictions while their performance was reduced in comparison each model did achieve an acceptable level of accuracy for each of the probabilistic prediction metrics for a binary classification the success of each of the models is further demonstrative of the effectiveness of the selected biweekly model configuration as a change to triweekly or monthly would degrade the obtained results based on the performance of the adaboost model while selection of the weekly configuration would greatly increase data requirements fig 4 shows the binary classification accuracy of each model at each gauge some of the gauges particularly those with relatively few mwbs in their flow history like the elbow river below glenmore dam in alberta were challenging for some of the developed models but no gauges were completely unclassifiable by all of the developed algorithms demonstrating both the strength of the selected variables and the trained algorithms in modelling all considered mwbs fig 5 shows calibration curves for each model a representation of the calibration error listed in table 3 the smoothest curves were those of the arcx4 and class switching models which also had the widest ranges of probabilistic prediction values the adaboost model concentrated its probabilistic predictions between 0 4 and 0 6 this is further emphasized in fig 6 which shows the distribution of probabilistic predictions produced by each model with arcx4 and class switching again having the widest range of predictions followed by knn with the adaboost model greatly concentrating its predicted probabilities around 50 this stems from the adaboost algorithms tendency to fit predicted probabilities to a transformation of the true probabilities rather than the actual probabilities niculescu mizil and caruana 2012 fig 7 shows the probabilities predicted by each model against the corresponding correct classification knn arcx4 and cs performed very well on mwb predictions with the majority of their incorrect predictions falling very close to the 50 cut off conversely the predictions of the adaboost model are so concentrated around 50 that all predictions whether they are for an mwb or are not are very close to each other thus the practical use of these predictions can be called into question 4 2 second level model 4 2 1 variable selection the goal of the second level model was to identify if an mwb would occur in 3 days from a day of interest in the mwb period identified by the first level model input omission was used to test the removal of variables from an initial variable pool listed in table 4 these variables consist of climatic variables representing air temperature and precipitation in the days immediately preceding the day of interest the input omission process first started with the removal of each variable from the pool one at a time with the reduced pool then used to train and test an adaboost model in a similar fashion to the first level model the accuracies obtained from the removal of each individual variable were similar thus no immediate conclusions could be drawn a subsequent input omission procedure removing combinations of variables from each of the five groups identified in table 4 starting from 15 days before the day of interest and moving closer was then conducted with the resulting model accuracies shown in fig 8 with these larger combinations of variable removals tested it was identified that the majority of considered variables were redundant and the dataset was reduced to the variables shown in table 5 which were the main variables whose removal began to show larger reductions in accuracy and recall in fig 8 the selected variables were those illustrating short term changes in the days immediately preceding an mwb with afdd and daily precipitation excluded entirely these short term changes in trends can be inferred to have a stronger impact on mwb occurrence than those over a longer timespan 4 2 2 second level model performance data used for the second level models was first normalised and balanced using under sampling in the same fashion as the first level model with an 80 20 training testing split an adaboost knn class switching and arcx4 model was trained separately using the prepared data with the results of each model for both deterministic classification and probabilistic prediction detailed in table 6 the obtained values for the considered performance metrics indicate that the knn model had the highest accuracy for binary classification with the other three models performing quite similarly the class switching and arcx4 models achieved the highest performance in probabilistic predictions with knn and adaboost performing slightly worse in both briar loss and calibration error notably both the knn and adaboost fall outside of the acceptable range of 0 675 for log loss score for a binary calibration similar to the first level model fig 9 shows the classification accuracies of each model at each gauge much like the first level model the majority of the gauges were properly classified by at least one model demonstrating the models strength and the importance of the selected variables the calibration curves of each model for probabilistic predictions are shown in fig 10 the smoothest curves were those of the arcx4 and class switching models which also had the widest ranges of probabilistic prediction values while the knn model had a smaller range the values of the adaboost model were heavily concentrated at 0 5 resulting in a curve very far off from a perfect calibration this shortcoming in predictions is further emphasized in fig 11 which shows the distribution of probabilistic predictions of each model with all values from the adaboost model being concentrated at 0 5 fig 12 shows the probabilistic predictions against the correct classification knn arcx4 and class switching again performed well in predicting both classes with most incorrect predictions very close to the 50 cut off the adaboost model produced predictions so concentrated around 50 that there is again very little distinction between what the model is classifying between the two classes 4 3 discussion at both of the developed model levels the arcx4 and class switching models achieved the best performance the other two models obtained comparable performance in deterministic classification but showed a deterioration when probabilistic results were considered though the values of the performance metrics for the adaboost model were comparable to the knn the adaboost models had significant issues in the distribution of predicted probabilities with their predictions so concentrated around 0 5 because of this shift in the classification threshold which could be possible for other models would ruin its results especially in the second level unless additional calibration of the results was performed which is not necessary for the other models considering both the deterministic and probabilistic predictions can provide a clearer picture of the accuracy of the developed models because of factors such as this on a national scale the arcx4 performs well with no clear examples of a gauge being difficult to classify on both levels while also obtaining some of the highest performance of the considered models for both prediction methods some spatial variability was present in the results of the second level model with select gauges in british columbia and maritime provinces having slightly reduced accuracy stemming from a combination of variations in the amounts of data available for these gauges and the milder climates of these regions however despite the lowered performance at some of these locations the minimum balanced classification accuracy for this model was 68 indicating an overall reliable performance the utilised two level structure also provides versatility in predictions predicting the date of an mwb from the start of the season would be far less accurate based on the degrading performance of the models used to test the time series configuration in the first level as the time step increased in size conversely predicting mwb occurrence using solely the second level model would be very data intensive as it would have to be run for every possible day in the considered period of mwb vulnerability the two level structure reduces the data complexity while increasing the model accuracy though the model accuracy of each level was assessed independently due to the differing timescales objectives and input variables at each model level the results obtained are very promising especially given the large spatial scale of the data used in the analysis due to the ease of configuration both binary classification and probabilistic predictions are easily output from the models with errors and shortcomings of both prediction methods well known this can provide strong decision making support for the affected rivers producing predictions of occurrence with associated probabilities rather than just a binary prediction allowing the strength of the prediction to be considered alongside the value of the prediction additionally the methods used were successfully tested on seasons without any mwbs thus the models could be applied to rivers that have not yet experienced mwbs but may encounter them in the future 5 conclusions this study focussed on the development of a modelling methodology for the prediction of mwbs on a national scale an event becoming increasingly common due to climate change and having an affect on both ecosystems and communities an extensive database on river ice events in canada the crid along with gridded climate data were used to build data driven models predicting both the occurrence and timing of these events on 52 rivers with historical mwbs across canada due to the rarity of these events and the subsequent issues introduced by the resulting imbalanced dataset modifications were implemented throughout model construction to ensure the best results these changes affected the data preprocessing algorithm selection and performance metric selection to strengthen the results against imbalanced data issues the developed framework utilised a two level structure with multiple dataset configurations tested for the first level model with detailed input selection performed for the second level model the finalized model included a first level predicting the occurrence of mwbs within a given two week period and a second level predicting the timing within that period with a three day lead time several machine learning algorithms with both deterministic and probabilistic outcomes were considered with the objective of achieving best accuracy in the final models the final recommended model utilising an arcx4 algorithm at each model level produced a high level of accuracy given the timescale of the predictions this model was easily modified to output probabilistic and classification results where the model performance was similar in both cases this was not the case for the other considered models which were found to have unacceptable performances from a probabilistic standpoint the two level structure was demonstrated to be a versatile method of predicting mwbs avoiding the issues of degrading performance with increasing timescale that would be introduced using solely the first level model while providing a far less data intensive method than would be required using solely the second level the models themselves were tested both on years with mwbs as well as years where they did not occur demonstrating the capacity for the model to be applied to rivers that have not yet experienced mwbs the models are dependent solely on climatic hydrometric and river ice data that can be monitored in advance of events allowing for ease of use in forecasting the model was constructed at a national scale not previously possible because of data scarcity with the possibility of easy reconfiguration for other northern regions vulnerable to mwbs future work may include detailed investigation of the transferability of this method requiring data from other vulnerable regions further testing of additional machine learning algorithms and using additional climate data such as snowpack depths and historical weather forecasts that can further increase the dependability of predictions will also be considered declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by natural sciences and engineering research council of canada nserc data used is available from environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d and natural resources canada https cfs nrcan gc ca projects 3 4 
25598,the increasingly common occurrence of mid winter breakups mwbs in canadian rivers consisting of the early breakup of ice cover outside of the typical spring season is a cause for concern this study applied various data driven modelling techniques to predict mwbs occurrence and timing with sufficient lead times on a national scale using a new canadian river ice database crid coupled with national resources canada gridded climate data a two level machine learning model structure was developed with the first level model predicting mwb occurrence within a given period and the second level model predicting the timing of an mwb occurrence within that period machine learning techniques that can handle class imbalance were employed to address many of the issues inherent in rare event forecasting including the implementation of data preprocessing the selection of algorithms and performance metrics suited to rare events multiple configurations of both model levels including variations on time series arrangement and input variables were tested to select the optimal model structure the best performing configuration focussing on a biweekly time period attained overall accuracies of 80 1 and 77 6 for the first and second level models respectively on the 452 mwbs in the crid in addition probabilistic prediction results were analyzed to evaluate model uncertainty and robustness this new modelling framework provides the first tool capable of predicting mwbs on a national scale with easily extendable methodology to locations that have not yet experienced mwbs and can form the basis of vital decision making support to affected communities keywords river ice mid winter breakup rare event forecasting imbalanced learning prediction data driven modelling software and data availability analysis in this study was completed using python v3 7 https www python org downloads release python 370 data used in this study is available through environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d and natural resources canada https cfs nrcan gc ca projects 3 4 1 introduction the spring breakup of river ice cover is a yearly occurrence on rivers throughout cold regions of the world with many environmental processes depending on both the timing and mechanism of the breakup beltaos et al 2003 the combination of runoff from snowmelt and melting of ice cover is important to ecosystems especially those that depend on the higher than usual water levels generated by these types of events beltaos et al 2006 however in recent years it has been noted that many rivers in canada have become vulnerable to mid winter breakups mwbs which are the early breakup of river ice cover caused by unseasonably high temperatures and the occurrence of liquid precipitation prowse et al 2002 carr and vuyovich 2014 while the ice cover typically re establishes as temperatures return to seasonal values the flow regime for the remainder of the season is significantly altered huntington et al 2003 spring breakups following these events are often milder with lower flows and water levels which can have significant environmental impacts to those processes that rely on spring breakups to drive higher water levels beltaos et al 2003 however mwbs also carry the risk of river ice jam formation where the ice rubble can freeze in place creating a semi permanent dam on the river that can trigger severe flooding beltaos 2003 to reduce such risk of severe flooding and their long term impacts on the river system effective means of predicting mwb events are of great interest to affected communities though past studies have attempted to forecast these events they have been limited both by the effectiveness of the methodology and the availability of data newton et al 2017 analysis of the spring breakup processes on rivers typically driven by more gradual temperature increases in combination with snowmelt runoff beltaos et al 2006 has been conducted through application of large scale data analysis techniques at regional scales for example de rham et al 2008a studied the high water events driven by the changing spatial distribution of breakups in the mackenzie river basin in canada through analysis of water survey of canada gauges within the basin it was concluded that spring breakups are trending towards earlier occurrence and longer affected periods de rham et al 2008b trend analysis of these events identified factors that influenced the timing and magnitude of breakups while also reinforcing the trend towards earlier and longer breakups goulding et al 2009 with additional spatial mapping and analysis of peak flows reinforcing these findings the breakup trends in the peace athabasca delta in canada has also been well studied the delta has seen a notable decreasing trend in ice jam occurrence leading to perched basins which rely on the flooding triggered by these events to be replenished beginning to reduce timoney 2002 regulation on the peace river upstream has been generally accepted as the cause and subsequent research has focussed on investigating changes in ice jam frequency and severity through statistical and modelling based means in the complex delta beltaos 2017 timoney et al 2018 jasek 2019a 2019b statistical data analysis has also identified factors that are best indicators of ice jam occurrence in the delta such as winter temperatures and precipitation lamontagne et al 2021 though these studies have investigated factors driving breakup and breakup related events on a regional scale their focus was limited to the more easily monitored spring breakup events and limited to singular regions research into mwbs has historically encountered issues related to data availability and spatial coverage with trends being successfully modelled while forecasting is often difficult a trend towards earlier spring breakups on the saint john river triggered by an increase in mwbs was noted by beltaos 1999 with a notable increasing trend in the occurrence of mild winter days beltaos and prowse 2009 subsequently noted an increasing risk of spring and mid winter ice jams on the river as a results of climate change effects prowse et al 2002 and carr and vuyovich 2014 conducted research into the drivers of mwbs producing predictive thresholds based on trends in temperature and precipitation though these thresholds were entirely accurate for the specific location for which they were developed they were found to not translate with a meaningful accuracy to other locations through a regional investigation of mwbs in western canada and the united states newton et al 2017 their research identified 52 mwbs in the considered region with the aforementioned thresholds being accurate for only 11 and 60 of the events respectively as those studies were limited by the available data the mwb database developed by newton et al 2017 which covers the western portion of canada and the united states was one of the largest collections of data on these types of events though these studies developed some methods of simple predictions of mwbs there has been no attempt at the creation of a comprehensive forecasting tool for these events in comparison to mwbs the prediction of spring breakup events timing through machine learning has been much more thoroughly researched though these projects often focussed on a single river or river network and are based around events with a more reliable timing simple single model algorithms such as artificial neural networks zhao et al 2012 guo et al 2018 and support vector machines wang et al 2010 barzegar et al 2019 as well as more complicated algorithmic structures such as adaptive neuro fuzzy inference systems wang et al 2012 sun and trevor 2015 2018 and stacking ensembles sun 2018 have had ranges of success in spring breakup prediction one such example is the prediction of spring breakup ice jams which have been predicted with logistic regression white 1996 artificial neural networks massie et al 2002 hybrid neuro fuzzy systems mahabir et al 2006 and stacking ensembles de coste et al 2021 like prediction of breakup timing these studies were often limited to specific rivers with an added problem of accounting for rare event forecasting a challenging data issue in machine learning that can lead to biased results from models the rare event forecasting issue would extend to mwb forecasting attempts as well and has never been addressed so far this paper focusses on the development and application of a two level modelling framework for the prediction of mwb occurrence and timing on a national scale applying machine learning and probabilistic analysis techniques while avoiding common issues that challenge rare event forecasting the study combines data of 452 mwbs from affected 52 rivers throughout canada with a national scale climate dataset to facilitate development of four machine learning algorithms adaptive boosting class switching k nearest neighbors and adaptive resampling and combining x4 this new approach allows for the prediction of an mwbs occurrence within a given time period using the first level model and the subsequent prediction of the mwbs timing within the period using the second level model the developed models are tested and validated from both a classification and probabilistic standpoint allowing the best model configuration to be selected this is the first successful application of these modelling techniques in the prediction of mwbs on a national scale it provides an easily configured methodology for the development of models predicting these events that is easily transferable to other locations 2 data 2 1 crid the primary source of historical ice data for this study was the canadian river ice database crid an extensive collection of data taken from a select set of water survey of canada gauges located across the country de rham et al 2020 this dataset was compiled through analysis and extraction of water level time series utilising visual inspection techniques first proposed by beltaos 1990 to identify the key river ice events of each season based on the observed changes of water levels data from 196 river gauges in canada are included in the crid with 46 located on regulated rivers and 150 on unregulated rivers fifteen river ice related events e g freeze up winter low flows and levels mwbs secondary low flows and levels breakups etc are included with corresponding water levels flows and dates recorded if available included in these events are mwbs at gauges where they have been historically observed identified as peaks in the flow record outside of typical spring breakup periods for the gauge and associated with higher than average air temperatures recorded at nearby climate gauges each mwb includes the date flow and water level at both the initiation of the event and the peak flow if available fig 1 shows the 196 gauges of the crid with the 52 gauges that have had historical mwb events highlighted these gauges were concentrated in the southwest british columbia and alberta and southeast ontario quebec new brunswick and newfoundland and labrador portions of the country these regions fall within the relatively warmer portions of the country that are more vulnerable to mid winter thaws that trigger mwbs in total 452 mwbs are included in the crid amongst the 52 identified stations the time span covered by the crid varies between gauges with the earliest mwb on record occurring in 1955 but most 301 out of 452 mwbs occurring after 1990 water levels at seven gauges with mwbs 28 station years were corrected for outliers using historical notes relating to gauge datum changes and in consultation with environment and climate change canada 2 2 nrcan gridded climate data climate data corresponding to mwb stations were extracted from the national resources canada nrcan daily gridded climate dataset https cfs nrcan gc ca projects 3 4 the nrcan dataset provides daily maximum and minimum air temperatures and total precipitation data with a resolution of 0 1 at a national scale the data spans 1950 2015 and is derived from quality controlled non homogenized station data taken from the national climate data archive ncda of environment and climate change canada data hutchinson et al 2009 this station data has been interpolated onto a high resolution grid i e 0 1 0 1 through thin plate splines mckenney et al 2011 hopkinson et al 2011 in this study climate data at the closest grid point to each crid gauge was extracted using geographic information system mapping techniques providing a complete time series of the three extracted variables for all crid gauges maximum and minimum air temperatures were averaged to obtain daily mean air temperatures then the accumulated freezing degree days afdd was calculated as the sum of temperatures below 0 c beginning from a reference date of october 1st which is the beginning of the canadian water year boyd 1979 total precipitation cumulatively summed from the freeze up date of river ice cover was also calculated these two values represent how cold a particular season has been and how much snowpack has fallen respectively 3 methods 3 1 model construction with imbalanced data data imbalance is a common issue in rare event forecasting especially in the case of binary classification where there is a significant difference in the number of observations in the majority class and the minority class maalouf and trafalis 2011 li et al 2016 in this study there is a massive imbalance present in the data between the majority class periods or days with no mwb and minority class periods or days with an mwb the majority class makes up 92 99 of the dataset depending on the configuration without any modifications to the traditional algorithmic construction process a common result of these imbalances is the classification of all cases as belonging to the majority class while this would represent an accuracy of 92 99 the results would be useless in practice as none of the rare events were properly classified to address these issues changes to the algorithm construction process can be implemented to data preprocessing the algorithm selection and the performance metric selection haixiang et al 2017 3 1 1 data preprocessing and resampling resampling is the most common and versatile data preprocessing technique for imbalanced datasets due to its independence from the selected classifiers lopez et al 2013 two common methods of resampling are over sampling which works by duplicating instances of the minority class chawla et al 2002 and under sampling which works by removing instances of the majority class tahir et al 2009 hybrid methods combining both over sampling and under sampling exist but their utilisation is rare under sampling was selected in this study due to its versatility and effectiveness in computation time when there are hundreds of samples in the minority class as there are in the mwb datasets napierala and stefanowski 2015 loyola gonzález et al 2016 an even balance between the classes was used due to its past successes in application to classification without affecting generalization of the model results koziarski 2020 the under sampling and even balance techniques have had previous success in rare event forecasting of class imbalanced data li et al 2013 kumar et al 2014 sun et al 2015 3 1 2 algorithm selection ensemble techniques which train multiple classifiers and some traditional single model classifiers are recommended for imbalanced learning including cases where data preprocessing has been applied galar et al 2012 in this study four model algorithms with previous success in addressing class imbalances in combination with preprocessing and performance metric selection haixiang et al 2017 were tested and are detailed below adaptive boosting adaboost the adaboost algorithm develops an ensemble of iterative classification trees with associated class labels zhu et al 2009 an initial classifier is constructed and used to classify observations until an incorrect classification occurs a subsequent tree is then developed with the weight of that point boosted using the stagewise additive modelling using a multi class exponential samme loss function this process continues until all training data has been run through the iterative ensemble the overall goal of the process is the minimization of misclassification rates through the boosting of the weight of a weak learners accuracy the final model label output by the ensemble is calculated using equations 1 and 2 1 f t x t 1 t c t h t x 2 f t x s i g n f t x where h t x is the ensemble of t hypotheses tuned by the user from the input vector x ranging from 0 to 1 c t is the weight of each learner with c t satisfying the conditions c t 0 and t 1 t c t 1 f t x is the weighted hypothesis and f t x is the final model hypothesis ratsch et al 2001 these models have been applied in the forecasting of rainfall runoff and rare dust storms liu et al 2014 zhang et al 2014 k nearest neighbors knn knn produces single models which function by classifying unlabelled samples based on the applied labels of the k nearest neighbors with k tuned by the user dudani 1976 each new observation classified to the model is labelled based on a weighting factor w j described by equation 3 3 w j j 1 k d k d j d k d 1 d k d 1 1 d k d 1 where the distance between neighboring points is described by d j j 1 k and the weight for an individual point is w j guo et al 2003 these models have been successfully applied to the rare event forecasting of ice jam occurrence semenova et al 2020 and real time flood forecasting liu et al 2020 class switching class switching is an ensemble approach where class labels are randomly flipped during model training allowing a more diverse classifier to be developed breiman 2000 the ensemble is composed of classification trees similar to the adaboost algorithm with each individual classifier generated based on the original training data with class labels switched at a rate defined by equations 4 6 4 p j i w p j f o r i j 5 p i i 1 w 1 p i 6 w p 1 j p j 2 where p j i is the probability that an element with the label i gets labelled as j p i i the probability that an element with the label i remains labelled i p i the proportion of elements in the training set labelled i p j the proportion of elements in the training set labelled j and w is proportional to the switching rate p a factor tuned by the user martinez munoz and suarez 2005 applied the algorithm successfully to a variety of binary classification tasks including rare event forecasting adaptive resampling and combining x4 arcx4 arcx4 is an algorithm belonging to the adaptive resampling and combining arc family of iterative ensemble classifiers breiman 1996 at each stage of model construction the model resamples a portion of the training set to generate a classifier c n where n ranges from 1 to k based on a probability described by equation 7 7 p n 1 m n 4 1 m n 4 where p n is the probability for resampling of the nth classifier and m n is the number of misclassifications of the n 1st classifier the results of each classifier are combined through unweighted voting with the number of classifiers k tuned by the user when applied to imbalanced classification the algorithm was found to outrank several other ensemble techniques narassiguin et al 2016 each of the above described models requires the tuning of hyperparameters to select the best model configuration for the given task for this application models were trained using five fold cross validation the data is subdivided into five portions with a training and testing process being conducted five times such that each partition is used as the testing data the possible considered values of the parameter s of each model are organized into grids and an exhaustive grid search was used to test every possible combination of parameters for each partition of data refaeilzadeh et al 2009 the ability of each configuration to correctly classify mwbs was averaged to give a performance assessment and the configuration with the best performance was used to select the optimum model topology finally a stratified 80 20 training testing split was used to develop the classification models 3 1 3 performance metrics performance metrics for the assessment of model tuning and final model accuracy were selected to focus on the correct classification of the minority case or rare event the occurrence of an mwb these metrics are based around a comparison of the actual class and the predicted class of an observation assessing it with one of four outcomes true positive tp denotes the correct prediction of an mwb true negative tn denotes the correct prediction of no mwb false positive fp denotes the prediction of an mwb when one does not occur and false negative fn denotes the prediction of no mwb when one does occur the primary metric for assessment is recall which assesses the correct classification of the minority class with additional assessment metrics of specificity correct classification of the majority case and balanced accuracy also calculated broderson et al 2010 for each values will range between 0 and 1 with 1 representing perfect classification and are calculated using equation 8 10 8 r e c a l l t p t p f n 9 s p e c i f i c i t y t n t n f p 10 b a l a n c e d a c c u r a c y s p e c i f i c i t y r e c a l l 2 each of the four considered algorithms in section 3 1 2 internally computes binary classifications by first producing a probabilistic prediction that an observation falls into a given class before conversion into a final binary value in the case of a single model algorithm a single probability is produced while ensemble algorithms use a mean of the values produced by each ensemble member pedregosa et al 2011 these initial probabilistic predictions were also extracted and assessed using the metrics of brier loss and log loss roulston 2007 both loss functions compare the predicted probability of a sample being in the minority class to the actual binary value according to equations 11 and 12 11 b r i e r l o s s 1 n i 1 n p y i y i 2 12 l o g l o s s 1 n i 1 n y i log p y i 1 y i log 1 p y i where y i is the binary value of the event and p y i is the predicted probability of the event for n events for each of the above metrics a value of 0 represents perfect probabilistic prediction while values above 0 25 for brier loss and 0 675 for log loss represent unacceptable accuracy no better than chance roulston 2007 and vovk 2015 calibration curves plotting the mean of predicted probabilities against the mean of the target variable for a set of observations were also used to evaluate the model performance a perfectly calibrated probabilistic classifier would output a straight line from 0 to 1 niculescu mizil and caruana 2005 3 2 two level model framework a two level modeling framework was developed to allow for versatility in monitoring practice the first level model focuses on an individual period of the year producing a binary prediction of whether or not an mwb would occur within that period if the first level model predicts an mwbs occurrence the second level model would then be used to predict the timing of the event producing a three day ahead binary prediction of an mwb fig 2 shows a flowchart of the model structure both models would use unique datasets featuring data easily monitored well in advance of an mwb though the first level model is used to trigger the use of the second level model there is no direct feed of data or results from the first level model into the second level 3 2 1 first level model the primary goal of the first level model was to identify whether an mwb would occur in a given time period between december 15 to march 15 the duration of the target time period was the first aspect of model development requiring testing four different time series configurations were considered weekly biweekly triweekly and monthly as illustrated in fig 3 in this figure three hypothetical example mwb timings are provided to illustrate how data would be included with all time periods preceding an mwb included and periods after the mwb has occurred excluded in years where an mwb does not occur all periods would be included the different input variables for each shown time series configuration are listed in table 1 these variables include values related to the key river ice conditions preceding an mwb and the climatic conditions at the start of freeze up and directly preceding the period of interest suggested by sun 2018 and de coste et al 2021 once the optimum time series configuration was identified it would be used to train and test the four algorithms to assess the accuracy of the chosen methods 3 2 2 second level model the second level model focusses on three day ahead prediction of mwb occurrence within an identified period of interest in the first level model providing sufficient medium range notice of potential alterations to flow le et al 2019 jurlina et al 2019 this model would look at each possible day within the period and classify the mwb occurrence based on data available up to three days ahead of the day of interest variables would be limited to climatic variables as factors such as warming and precipitation preceding the event have been found to be the primary drivers of mwb occurrence carr and vuyovich 2014 ice event related variables such as those associated with freeze low winter flow and low winter level used as inputs for the first level model would be too far from this event on a time scale and would provide no useful information at this prediction resolution the initial pool of variables would include daily temperatures afdd and changes in afdd and daily precipitation and total seasonal precipitation an input omission process would be used to select the optimum variable set for this prediction a generalized adaboost model would be constructed using all variables to be sure that the effects of each was considered followed by the exclusion of each variable in turn to see how the recall and balanced accuracy change with the removal if the values increased or did not change it would indicate that the variable in question is potentially redundant and can be removed from the model without negative impact this would be further verified through the testing of the omission of larger groups of potentially redundant variables to reduce the input set to the most relevant values snieder et al 2020 the final input set would then be used to test and train the four algorithms detailed above for the prediction of mwb timing 3 3 model implementation this study utilised python version 3 7 for all analysis van rossum and drake 2009 additional analysis was conducted using the packages numpy oliphant 2006 seaborn waskom and the seaborn development team 2020 pandas mckinney and others 2010 scikit learn pedregosa et al 2011 ensemble narassiguin et al 2016 imblearn lemaitre et al 2017 and scipy virtanen et al 2020 4 results and analysis 4 1 first level model 4 1 1 time series configuration the first level model aimed to predict the occurrence of an mwb in a given time period between december 15 to march 15 with the four time periods considered being weekly biweekly triweekly and monthly the initial number of observations was 4098 for the monthly configuration 6965 for the triweekly configuration 8316 for the biweekly configuration and 10 456 for the weekly configuration the variables for each configuration were first normalised through division by means and then balanced between positive and negative cases using under sampling an adaboost model was trained for each configuration using 5 fold cross validation with an 80 20 training testing split the performance of each configuration using two classification metrics i e balanced accuracy and recall is presented in table 2 the biweekly model was selected as the most successful of the tested configurations greatly outperforming the monthly and triweekly models and producing comparable performance to the weekly model however with a simpler initial dataset 4 1 2 first level model results and analysis additional knn class switching and arcx4 models were trained on the same data from the biweekly time series configuration as the adaboost model with the results of each for both classification and probabilistic predictions shown in table 3 deterministic classification metrics indicate that the arcx4 model performed the strongest at binary classification with the highest obtained balanced accuracy and recall the arcx4 also performed well with probabilistic predictions however it was slightly outperformed by the class switching model in all considered metrics while the adaboost and knn models performed comparably well to the arcx4 and class switching models in deterministic classification they were found to have a more significant reduction in accuracy in probabilistic predictions while their performance was reduced in comparison each model did achieve an acceptable level of accuracy for each of the probabilistic prediction metrics for a binary classification the success of each of the models is further demonstrative of the effectiveness of the selected biweekly model configuration as a change to triweekly or monthly would degrade the obtained results based on the performance of the adaboost model while selection of the weekly configuration would greatly increase data requirements fig 4 shows the binary classification accuracy of each model at each gauge some of the gauges particularly those with relatively few mwbs in their flow history like the elbow river below glenmore dam in alberta were challenging for some of the developed models but no gauges were completely unclassifiable by all of the developed algorithms demonstrating both the strength of the selected variables and the trained algorithms in modelling all considered mwbs fig 5 shows calibration curves for each model a representation of the calibration error listed in table 3 the smoothest curves were those of the arcx4 and class switching models which also had the widest ranges of probabilistic prediction values the adaboost model concentrated its probabilistic predictions between 0 4 and 0 6 this is further emphasized in fig 6 which shows the distribution of probabilistic predictions produced by each model with arcx4 and class switching again having the widest range of predictions followed by knn with the adaboost model greatly concentrating its predicted probabilities around 50 this stems from the adaboost algorithms tendency to fit predicted probabilities to a transformation of the true probabilities rather than the actual probabilities niculescu mizil and caruana 2012 fig 7 shows the probabilities predicted by each model against the corresponding correct classification knn arcx4 and cs performed very well on mwb predictions with the majority of their incorrect predictions falling very close to the 50 cut off conversely the predictions of the adaboost model are so concentrated around 50 that all predictions whether they are for an mwb or are not are very close to each other thus the practical use of these predictions can be called into question 4 2 second level model 4 2 1 variable selection the goal of the second level model was to identify if an mwb would occur in 3 days from a day of interest in the mwb period identified by the first level model input omission was used to test the removal of variables from an initial variable pool listed in table 4 these variables consist of climatic variables representing air temperature and precipitation in the days immediately preceding the day of interest the input omission process first started with the removal of each variable from the pool one at a time with the reduced pool then used to train and test an adaboost model in a similar fashion to the first level model the accuracies obtained from the removal of each individual variable were similar thus no immediate conclusions could be drawn a subsequent input omission procedure removing combinations of variables from each of the five groups identified in table 4 starting from 15 days before the day of interest and moving closer was then conducted with the resulting model accuracies shown in fig 8 with these larger combinations of variable removals tested it was identified that the majority of considered variables were redundant and the dataset was reduced to the variables shown in table 5 which were the main variables whose removal began to show larger reductions in accuracy and recall in fig 8 the selected variables were those illustrating short term changes in the days immediately preceding an mwb with afdd and daily precipitation excluded entirely these short term changes in trends can be inferred to have a stronger impact on mwb occurrence than those over a longer timespan 4 2 2 second level model performance data used for the second level models was first normalised and balanced using under sampling in the same fashion as the first level model with an 80 20 training testing split an adaboost knn class switching and arcx4 model was trained separately using the prepared data with the results of each model for both deterministic classification and probabilistic prediction detailed in table 6 the obtained values for the considered performance metrics indicate that the knn model had the highest accuracy for binary classification with the other three models performing quite similarly the class switching and arcx4 models achieved the highest performance in probabilistic predictions with knn and adaboost performing slightly worse in both briar loss and calibration error notably both the knn and adaboost fall outside of the acceptable range of 0 675 for log loss score for a binary calibration similar to the first level model fig 9 shows the classification accuracies of each model at each gauge much like the first level model the majority of the gauges were properly classified by at least one model demonstrating the models strength and the importance of the selected variables the calibration curves of each model for probabilistic predictions are shown in fig 10 the smoothest curves were those of the arcx4 and class switching models which also had the widest ranges of probabilistic prediction values while the knn model had a smaller range the values of the adaboost model were heavily concentrated at 0 5 resulting in a curve very far off from a perfect calibration this shortcoming in predictions is further emphasized in fig 11 which shows the distribution of probabilistic predictions of each model with all values from the adaboost model being concentrated at 0 5 fig 12 shows the probabilistic predictions against the correct classification knn arcx4 and class switching again performed well in predicting both classes with most incorrect predictions very close to the 50 cut off the adaboost model produced predictions so concentrated around 50 that there is again very little distinction between what the model is classifying between the two classes 4 3 discussion at both of the developed model levels the arcx4 and class switching models achieved the best performance the other two models obtained comparable performance in deterministic classification but showed a deterioration when probabilistic results were considered though the values of the performance metrics for the adaboost model were comparable to the knn the adaboost models had significant issues in the distribution of predicted probabilities with their predictions so concentrated around 0 5 because of this shift in the classification threshold which could be possible for other models would ruin its results especially in the second level unless additional calibration of the results was performed which is not necessary for the other models considering both the deterministic and probabilistic predictions can provide a clearer picture of the accuracy of the developed models because of factors such as this on a national scale the arcx4 performs well with no clear examples of a gauge being difficult to classify on both levels while also obtaining some of the highest performance of the considered models for both prediction methods some spatial variability was present in the results of the second level model with select gauges in british columbia and maritime provinces having slightly reduced accuracy stemming from a combination of variations in the amounts of data available for these gauges and the milder climates of these regions however despite the lowered performance at some of these locations the minimum balanced classification accuracy for this model was 68 indicating an overall reliable performance the utilised two level structure also provides versatility in predictions predicting the date of an mwb from the start of the season would be far less accurate based on the degrading performance of the models used to test the time series configuration in the first level as the time step increased in size conversely predicting mwb occurrence using solely the second level model would be very data intensive as it would have to be run for every possible day in the considered period of mwb vulnerability the two level structure reduces the data complexity while increasing the model accuracy though the model accuracy of each level was assessed independently due to the differing timescales objectives and input variables at each model level the results obtained are very promising especially given the large spatial scale of the data used in the analysis due to the ease of configuration both binary classification and probabilistic predictions are easily output from the models with errors and shortcomings of both prediction methods well known this can provide strong decision making support for the affected rivers producing predictions of occurrence with associated probabilities rather than just a binary prediction allowing the strength of the prediction to be considered alongside the value of the prediction additionally the methods used were successfully tested on seasons without any mwbs thus the models could be applied to rivers that have not yet experienced mwbs but may encounter them in the future 5 conclusions this study focussed on the development of a modelling methodology for the prediction of mwbs on a national scale an event becoming increasingly common due to climate change and having an affect on both ecosystems and communities an extensive database on river ice events in canada the crid along with gridded climate data were used to build data driven models predicting both the occurrence and timing of these events on 52 rivers with historical mwbs across canada due to the rarity of these events and the subsequent issues introduced by the resulting imbalanced dataset modifications were implemented throughout model construction to ensure the best results these changes affected the data preprocessing algorithm selection and performance metric selection to strengthen the results against imbalanced data issues the developed framework utilised a two level structure with multiple dataset configurations tested for the first level model with detailed input selection performed for the second level model the finalized model included a first level predicting the occurrence of mwbs within a given two week period and a second level predicting the timing within that period with a three day lead time several machine learning algorithms with both deterministic and probabilistic outcomes were considered with the objective of achieving best accuracy in the final models the final recommended model utilising an arcx4 algorithm at each model level produced a high level of accuracy given the timescale of the predictions this model was easily modified to output probabilistic and classification results where the model performance was similar in both cases this was not the case for the other considered models which were found to have unacceptable performances from a probabilistic standpoint the two level structure was demonstrated to be a versatile method of predicting mwbs avoiding the issues of degrading performance with increasing timescale that would be introduced using solely the first level model while providing a far less data intensive method than would be required using solely the second level the models themselves were tested both on years with mwbs as well as years where they did not occur demonstrating the capacity for the model to be applied to rivers that have not yet experienced mwbs the models are dependent solely on climatic hydrometric and river ice data that can be monitored in advance of events allowing for ease of use in forecasting the model was constructed at a national scale not previously possible because of data scarcity with the possibility of easy reconfiguration for other northern regions vulnerable to mwbs future work may include detailed investigation of the transferability of this method requiring data from other vulnerable regions further testing of additional machine learning algorithms and using additional climate data such as snowpack depths and historical weather forecasts that can further increase the dependability of predictions will also be considered declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by natural sciences and engineering research council of canada nserc data used is available from environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d and natural resources canada https cfs nrcan gc ca projects 3 4 
25599,this paper aims to solve three issues frequently present in the optimal placement of water quality sensors for protecting water distribution systems wdss from both accidental and intentional contamination namely i computational intractability of the optimization problem as the size of the wds increases ii unrealistic assumption that sensors are positioned at nodes rather than on system pipes and iii neglecting site specific practical conditions impacting on sensor installation the three drawbacks were tackled by i restraining the optimization to the hydraulic topological wise most important pipes ii introducing dummy nodes in the middle of these pipes as potential sensor locations iii applying a multi criteria decision making tool incorporating urbanistic and economic factors for selecting the most effective sensor locations the method is tested on the wds of the town of parete italy showing the manyfold benefits of the solution obtained keywords contamination warning system complex network theory edge betweenness sensor placement water distribution system topology water network protection 1 introduction water distribution systems wdss are an essential part of the critical infrastructure of a city since the availability of clean water affects both socio economic prosperity and population safety wdss are considered inherently vulnerable to both intentional and accidental contaminations due to their large size up to tens or hundreds of kilometres of pipes complexity large number of served users and access points e g hydrants consumer connections tanks reservoir and leak points oliker et al 2016 the assessment of contamination risk comes with an uncertainty related to the type of contaminant and its consequences and the characteristics of its intrusion time duration and location making it one of the most difficult problems to address for wds management a widely used strategy for securing wdss against contamination is the installation of a water quality sensor system wqss awwa 2005 the aim is to quickly assess water quality enabling early detection of potentially dangerous conditions a wqss provides indications on contamination events janke et al 2006 and helps locating their source by using time and location of the actual detection ung et al 2017 considering that the first hours after contamination are crucial for mitigating its impacts zulkifli et al 2018 the continuous monitoring of water quality parameters plays a key role for implementing and maximizing the benefits of an early warning system hu et al 2017 to maximize the wqss detection capability water operators should address the issue of identifying the most suitable locations for sensor placement by balancing both performance aspects and economic investment murray et al 2008 usually securing the entire network is infeasible in practice due to budget constraints that often limit the number of sensors a water utility can deploy in this regard sensors should be placed in strategic locations at the same time easily accessible and assuring maximum capability of detecting and identifying contaminants in a short time since 1991 lee et al 1991 researchers and practitioners have explored the optimal sensor placement problem in wdss since the events of september 11th in the united states protecting critical infrastructures from potential terrorist acts has become an absolute priority various methodologies have been proposed to define an optimal wqss lee and deininger 1992 uber et al 2004 chang et al 2013 these are generally arranged in top down decision support frameworks khorshidi et al 2019 with the upper and lower decision levels related to public safety and operational costs respectively however the optimal sensor placement challenge is still open from different viewpoints e g identification of most suitable locations performance evaluation applicability to real world scenarios and no general optimality criteria have been found rathi et al 2015 divided models and algorithms for solving the sensor placement problem in two categories a single objective approaches such as the work of kessler et al 1998 woo et al 2001 ostfeld and salomons 2005 berry et al 2009 b multi objective approaches such as the methods proposed by propato and piller 2006 huang et al 2006 wu and walski 2006 dorini et al 2008 in the battle of the water sensor networks bwsn approaches of the two categories were compared and tested ostfeld et al 2008 overall the objective functions developed in the literature are related to detection likelihood expected contaminated water volume detection time and exposed population extensive critical reviews of the topic can be found in hu et al 2018 as well as in adedoja et al 2019 who further classify the existing methods into four categories opinion based rule based optimization based and theory based generally due to the huge number of potential contamination scenarios and to the wds complexity the problem of optimal wqss layout comes with high computational costs especially for large wdss as reported by xu et al 2013 the optimal sensor placement in a network represents a np hard combinatorial optimization problem in this regard together with the investigation of several objective functions continuous efforts have been made to develop increasingly efficient numerical techniques hart and murray 2010 diao and rauch 2013 in this regard during the recent years several aspects of the optimal water quality sensor placement have been addressed zhao et al 2016 proposed a branch and bound sensor placement algorithm based on greedy heuristics and convex relaxation to minimize the consumption of contaminated water prior to contamination detection rathi and gupta 2017 maximized the demand coverage and the detection probability with a time constraint for the early detection tinelli et al 2018 discussed the impact of objective function selection on the optimal sensor placement problem ciaponi et al 2019 proposed a combined management strategy for monitoring wdss based on water network sectorization and installation of water quality sensors giudicianni et al 2020 presented a topological approach for the case of limited information about the system which relies on a priori clustering of the wds and on the installation of water quality sensors at the topologically most central nodes of each cluster hooshmand et al 2020 addressed the sensor placement problem by minimizing the number of vulnerable nodes and assuming a limited sensor budget availability lee and yoo 2020 suggested a methodology for defining water quality sensor locations considering the variability in water flow directions due to abnormal functioning conditions taha et al 2021 considered the previously overlooked metric of state estimation and network wide observability of the water quality dynamics to find optimal sensor placement with kalman filtering fasaee et al 2021 developed a new model to identify the optimal location of sensors to effectively support hydrant flushing for ensuring an efficient discharge of contaminants the variation in node contamination probability due to population density and user properties has been addressed in several works he et al 2018 in this regard hu et al 2021 proposed a multi objective approach based on the different characteristics of each node and the risk levels of contamination events showing the effect of such variability on the selection of sensor locations naserizade et al 2018 used the nsga ii and included cost and probability of undetected events and uncertainties related to a contamination injection in the optimization process while cardoso et al 2020 considered four contamination probability functions combined with a clustering based post processing method for a pareto front analysis the characteristics of sensors have been considered by zeng et al 2018 who maximized the quality of sensing and considered two types of sensors with different prices and communication capabilities sankary and ostfeld 2018 simultaneously minimised the affected population and the expected number of false positive detections while de winter et al 2019 investigated the influence of sensor imperfection by means of two greedy algorithms and by considering multiple objective functions different techniques and algorithms have been explored and adopted for solving the problem of the optimal water quality sensor placement for example the information entropy theory was used by khorshidi et al 2018 and by brentan et al 2021 for reducing the computational burden of the problem and developing a multi criteria decision making technique for the selection of an optimal solution respectively hu et al 2020 and jafari et al 2021 adopted the nsga iii algorithm to solve the multi objective sensor placement problem by considering the graph connectivity for the selection of individuals and the effect of contamination in important junctions in terms of social consequences respectively finally the resilience of water quality sensor placement strategies was investigated by zhang et al 2020 and nikolopoulos et al 2021 zhang et al 2020 considered all likely sensor failures and defined metrics for ranking alternatives nikolopoulos et al 2021 developed a novel methodology to assess the resilience under cyber physical attacks this paper presents a novel method and a new perspective for the water quality sensor placement problem in a wds compared to the previously developed methods the major novelty lies in considering more realistically the placement of sensor on pipes rather than in wds nodes before carrying out the optimization complex network theory tools are applied to define the most important pipes on which subsequently locating dummy nodes as possible sensor locations the graph of the wds is weighted with a pipe hydraulic resistance surrogate parameter in order to consider also hydraulic and contaminant transport aspects accordingly weighting the graph allows definition of the most important pipes from both topological and hydraulic viewpoints as a result the computational burden is significantly reduced since a small subset of pipes is defined for the following optimization phase four different objective functions are investigated and optimised to define the most suitable sensor placement layouts by exploring the new reduced solution space in order to identify the most efficient and effective monitoring system besides economic incremental benefit and detection performance criteria detection time detection likelihood population exposed and extent of contamination further logistic site specific conditions are considered such as surrogate metrics of accessibility and easiness of installation finally this paper provides a general multi criteria decision making tool for supporting decisional processes in wqss design 2 methodology as mentioned before optimal sensor placement in a water distribution network represents an np hard combinatorial optimization problem xu et al 2013 as the computational complexity exponentially increases with the growth of wds size a constraint on the number of sensors should be added to this problem for obvious economic and practical reasons the proposed methodology consists of five steps that progressively reduce the number of potentially adoptable solutions according to different criteria based on safety logistic and economic viewpoints a modelling of the wds as a weighted graph calculation of the most central pipes and insertion of dummy nodes topological step b classification of locations through an accessibility criterion logistic step c heuristic optimization with four objective functions optimization step d calculation of sensor installation cost and setting up of an incremental economic benefit threshold economic step e design of a decision support system based on a weighted normalised matrix of the detection criteria decision step the overall methodology adopted for the search of the most suitable wqss is summarised in fig 1 as subsequently described in detail the logistic step intervenes twice in the methodology before the optimization step and during the economic one 2 1 topological step the starting point of the method consists of modelling the wds as an undirected weighted graph g v e w taking advantage of the topological properties of the wds graph giudicianni et al 2018 indeed perelman and ostfeld 2011 showed how adopting graph theory can help in gaining insight in to the wds behaviour by simplifying its operation sitzenfrei 2021 showed graph theory s potential for assessing water quality of the wds furthermore giudicianni et al 2021 identified the most critical contamination sources by means of topological metrics in particular v is the set of n nodes v i junctions reservoirs and tanks e is the set of m links l ij v i v j from node v i to node v j pipes valves and pumps and w ij w is a weight characterising the physical characteristics of i th link the graph is considered weighted with a surrogate measure of pipe hydraulic resistance herrera et al 2016 specifically the weight w i l i d i has been assigned to each link with l i and d i the length and diameter of pipes to obtain a graph model that also considers the hydraulic behaviour of the wds the aim is to take into account the phenomenology of the system through a non dimensional weight that is linearly dependent only on geometric characteristics of pipes not related to a specific head loss formula and not simulation based in such a way as to make it as general as possible subsequently the application of complex network theory algorithms allows considering simultaneously the topological structure and the hydraulic characteristics of the system in this paper the networkx python package is used for the topological analysis hagberg et al 2022 this aspect represents an improvement compared to previous works giudicianni et al 2020 on the application of topological approaches to wqss design where the graph was considered unweighted the second phase of the topological step is the search of major links through the edge betweenness b c l a centrality metric borrowed from complex network theory to this aim the shortest path between two nodes is defined as the sequence of links connecting two nodes crossing the links associated with the minimum sum of weights dijikstra 1959 the edge betweenness b c l of a link l newman and girvan 2004 is defined as the sum of the ratios of the number σ vi vj l of shortest paths between pairs of nodes v i and v j that run through that link l and the total number σ vi vj of shortest paths connecting pairs of nodes v i and v j the edge betweenness centrality b c l of a link l is then mathematically defined by equation 1 1 b c l v i v j v σ v i v j l σ v i v j this metric allows identifying which links in a network appear more often along the shortest paths connecting pairs of nodes therefore it can be used as a measure of the influence of a link over the information water flow throughout the network a link with a high value of the edge betweenness usually represents a bridge like connector between two parts of a network the removal of which may affect the communication between many pairs of nodes through the shortest paths between them lu and zhang 2013 after weighting the links of the graph with the weight w i l i d i defined above the search for the highest edge betweenness links will enable identifying the links that have simultaneously a higher connectivity with pipes characterized by lower resistance allowing to define the most central pipes from a topological hydraulic viewpoint indeed by weighting the graph the shortest path between two nodes becomes the minimum weighted distance between two nodes i e with the minimum sum of weights assigned to the corresponding pipes which in the present study corresponds to the minimum sum of pipe surrogate resistances sensors located on these pipes are supposed to detect contamination intrusion and spreading in an easier and faster way the solution space will be narrowed after selecting the most central pipes using the edge betweenness criterion indeed this phase allows focusing the following optimization step on a much smaller subset of pipes strongly reducing the computational burden of the entire process the last point of the topological step is the insertion of dummy nodes in the middle of the selected major pipes characterised by null base demand and with elevation and coordinates based on a linear interpolation between the end points of the considered pipe this point makes the current methodology closer to real world applications since sensors are installed on pipes and not at nodes as it was assumed by all previous theoretical works on the topic while being simple in its computational implementation this assumption is infeasible from a practical point of view especially in correspondence to a cross or tee junction where the samples would be very different depending which of the converging pipes is actually fitted with the sensor by inserting dummy nodes on the most central pipes and narrowing the search of the possible sensor locations only to them the aforementioned practical aspects are considered furthermore the same computational simplicity as that associated with the search for optimal sensor locations at nodes is kept it is worth highlighting that though a single potential sensor location was considered in this work for each pipe the methodology can be easily extended to consider more locations in the long pipes as an example after setting a threshold length value the number of potential locations present in the generic pipe can be calculated as the ratio of the pipe length to the threshold length value rounded to the closest integer 2 2 logistic step generally the problem of water quality sensor placement is faced by using a single or multi objective optimization approach according to the operators choice without considering practical aspects related to site specific conditions and the spatial variability in logistic conditions i e accessibility to the sensor placement solution areas as well as to the underground services nearby for the full functioning of the monitoring stations all the locations are assumed to be equally good candidates for sensors and therefore they are considered equally desirable from a cost and accessibility standpoint this constitutes a strong simplification compared to real world applications berry et al 2005 and a field survey should be performed to ensure that the generic selected site is suitable for an easy installation of the sensor i e protected room for housing the instrumentation easy access for installation and maintenance activities electric power supply wired or wireless connection for transmitting acquired data giudicianni et al 2020 in order to also consider economic and accessibility aspects an analysis of the city map needs to be included to identify more less desirable positions for locating sensors results of the analysis will be spatially visualised on the layout of the case study considered thus identifying areas of interest that can be classified as follows most desirable locations green pipes water company sites and public buildings i e fire or police stations regularly visited by water utility maintenance personnel these locations do not need construction works to install the monitoring station to ensure power and scada connection least desirable locations red pipes highways rivers busy crossroads for which there are issues with confined space entry necessity of specific equipment and traffic control neutral locations blue pipes those not belonging to the previous two classes the number and typology of locations as defined above constitute another parameter for the assessment of the most feasible wqss in particular the least desirable locations are eliminated from the suitable sensor locations in such a way as to further reduce the solution space 2 3 optimization step after identifying the most central pipes and inserting the dummy nodes in their middle and eliminating the least desirable locations an optimization run is carried out by using the threat ensemble vulnerability assessment and sensor placement optimization tool teva spot developed by the us environmental protection agency epa janke et al 2012 u s epa 2008 in this context four objective functions detection time detection likelihood population exposed through ingestion and extent of contamination are used it is worth highlighting that in this work water demand and therefore served population is concentrated at nodes hence to calculate the objective functions reference is made to nodes and to the time when they are reached by the contaminant let us denote with s the total number of considered contamination scenarios in particular the following assumptions have been made for the setting up of the set s of contamination events considered for the wqss design all the demand nodes and the reservoirs have been one by one considered as potential locations for contaminant injection contamination starting time at the beginning of any of the 24 h of a day 1 single value of the mass injection rate 1 single value of the injection duration only one couple of values for mass injection rate and duration were sampled from those proposed by preis and ostfeld 2008 using the procedure of tinelli et al 2017 aiming to obtain a small but still statistically significant set of contamination events the following objective functions are adopted 1 detection time 2 t m e a n t s where t s for each contamination scenario s s represents the elapsed time from the start of the contamination to the first presence of a nonzero contaminant concentration identified by a sensor of the monitoring system i e the time of the first contaminant detection in this context a perfect sensor for the generic contaminant is assumed the characteristic detection time of a generic sensor layout is defined as the average of all t s for all the contamination events considered t is minimised with the objective to reduce the time of detection for all contamination scenarios considered for the wqss design 2 detection likelihood 3 p s 1 s s 1 s d s where d s 1 if contamination scenario s th is detected and d s 0 otherwise p s represents the probability of detecting the contamination p s is maximized so to detect as many contamination scenarios as possible 3 population exposed through ingestion 4 p m e a n p s where p s is the number of people that ingest contaminated water for the generic contamination scenario s before the first detection the five fixed times ingestion model davis and janke 2009 is considered for modelling the water consumption according to which users use tap water at five fixed times during the day 7 30 a m 10 30 a m 12 00 a m 3 00 p m and 6 00 p m the duration of the ingestion is considered instantaneous then if any contamination reaches a consumption node at one of such five fixed times the population allocated to the node is assumed to be exposed through ingestion then p is minimised to lessen the impact of contamination on the population 4 extent of contamination 5 e c m e a n l c s where l c s is the total length of the contaminated pipeline the length of pipe contaminated during a contamination event s will be the sum of the length l c s of all the contaminated pipes in the period t s ec is minimised to lessen the impact of contamination on the network 2 4 economic step the proposed method offers the possibility of introducing an economic criterion for selecting the most cost effective solutions among the set of configurations that satisfy topological detection performance criteria for each of the four objective functions described above optimization step 10 wqss are defined with an increasing number of sensors from 1 to 10 sensors to define four pareto fronts as a function of the number of sensors then a simple economic analysis is performed to evaluate the installation which includes only the purchase cost c sens of the sensor or c sens the civil work cost c cw for the desirable or neutral locations respectively logistic step according to the above mentioned factors the cost of a monitoring station is equal to 6 c s t c s e n s f o r t h e m o s t d e s i r a b l e l o c a t i o n c s e n s c c w f o r t h e n e u t r a l l o c a t i o n accordingly the total cost c tot of each wqss is defined as the sum of the costs of all its monitoring stations this allows the pareto fronts to be rearranged considering the costs associated with the installation of sensors based on their logistic features then an incremental economic benefit threshold is defined and applied to the new pareto fronts for further reducing the set of possible adoptable solutions 2 5 decision step the last step consists of the design of a decision support system based on four detection performance metrics p i e the same metrics as those used in the optimization step it will result in a multi criteria matrix for the selection of the most suitable wqss for each solution the four parameters will be calculated and normalised with respect to the best value of the corresponding category partial score s p in order to also consider the importance of the parameters a weight wt p 0 1 is assigned to each of them in such a way that wt p 1 finally for each solution a total quality score s q is assigned equal to the sum of all the weighted partial scores 7 s q s p w t p theoretically the values of s q range between 0 the worst monitoring option and 1 the best monitoring option the five step method described above results in a tool for the decision making process to choose the most suitable appropriate wqss layout which considers detection performance logistic and economic aspects it can be straightforwardly extended by also adding other criteria to lead the utility manager towards an even more informed choice 3 case study the proposed method was tested on the real wds serving the town of parete located in a densely populated area situated 20 km to the north of naples italy with a population of around 11 000 inhabitants see fig 2 a b and 2c for the spatial distribution of pipe diameters lengths and weights w i l i d i respectively the wds of parete has 182 demand nodes with ground elevation between 53 m a s l and 79 m a s l 282 pipes made of cast iron with length ranging between 10 4 m and 542 m and diameter ranging between 0 06 m and 0 20 m and 2 reservoirs with fixed pressure head of 110 m a s l daily variation in the users demand has been simulated through an hourly demand pattern with multiplier values ranging from 0 2 to 3 1 accordingly the total demand at nodes ranges from 7 4 l s at night to 113 9 l s in the morning and midday peaks with an average value of 54 6 l s regarding the set s of contamination events considered for the wqss design all the 182 demand nodes and the 2 reservoirs were considered as potential locations for contaminant injection the value of the mass injection rate and the injection duration are set equal to 100 gr min and 60 min respectively the total number of considered contamination events was s 184 24 1 1 4416 in the context of the optimization the hydraulic and quality simulations were carried out using the hydraulic simulation software epanet rossman 2000 embedded in the teva spot software assuming a conservative contaminant a water quality time step of 5 min and a reporting time step of 5 min regarding the logistic analysis the location classification is shown on the map of the wds serving the city of parete in fig 3 five least desirable locations were defined around the middle of the wds and disregarded in the subsequent steps accordingly the investment cost assessment concerned only neutral and most desirable locations in particular the preliminary financial analysis assumed the cost of a multiple parameters and continuous monitoring sensor to be c sens 10 000 civil work cost c cw was estimated at 30 of sensor costs c sens then c cw 3000 therefore the cost of a monitoring station for neutral locations is c st c sens c cw 13 000 it is worth highlighting the generalizability of this step indeed the financial analysis can be carried out by further detailing the costs associated with the installation of the monitoring station as well as by considering a different cost values 4 results and discussion the first step was the calculation of the most central pipes according to the values of the edge betweenness centrality fig 4 shows the results of the weighted topological analysis fig 4a is a scatter plot of the weighted edge betweenness eb w of pipes sorted in descending order in which it is possible to spot a knee in the distribution in correspondence to the first 50 pipes the value of eb w corresponding to the knee was assumed as a threshold to select the most central pipes for the subsequent steps of the proposed method thus strongly reducing the set of potential sensor locations fig 4b shows the eb w for each pipe of the wds of parete in red higher values of the centrality and therefore the most central pipes from fig 4b it is also clear that the most central pipe eb w 0 31 is close to the middle of the wds the last point of the topological step is the insertion of dummy nodes in the middle of the 50 most central pipes selected in fig 5 a new sketch of parete wds is reported introducing the dummy nodes red circles to be considered as potential installation locations for the optimization after the further elimination of the least desirable locations it is evident that the most central pipes selected are spread throughout the entire network a visual analysis of the location of the dummy nodes fig 5 indicates that the topological step ensures a uniform spatial distribution of the potential locations of the sensors by covering all the geographical extension of the wds this is in agreement with nazempour et al 2018 s statement since a water distribution system is a geographically distributed network so should be the sensors the merging of information in figs 5 and 2 points out that the pipes with the largest diameters are preferred candidates for locating sensors the longest pipes at the border of the system are instead penalized in fact the betweenness centrality tends to favour more linked and internal pipes by considering the position of each pipe with respect to the rest of the network an additional benefit of this step is the possibility to significantly reduce the solution space by narrowing the set of possible sensor locations to only the pipes characterised by high values of eb w indeed to allocate n sens sensors within a network of m pipes the total number of possible wqss combinations is expressed in equation 6 8 m n s e n s m n s e n s m n s e n s if a number n sens 6 of sensors is assumed for the case study of parete it would give 6 62 1011 combinations the selection of the 50 most central pipes from which the 5 least desirable locations are removed would give 8 15 106 combinations with an almost 1 10 000 reduction of the solution space simulation results are reported in table 1 in terms of detection quality performance for the wqss layouts obtained by adopting one by one the four objective functions defined above and for an increasing number of sensors from 0 to 10 the maximum number of ten sensors is reasonable for a small medium sized wds like that tested in this work giudicianni et al 2020 specifically impact represents the value of the objective function corresponding to the wqss detection time detection likelihood exposed population and extent of contamination while benefit represents the percentage reduction for the detection time the exposed population and the extent of contamination or percentage increase for the detection likelihood of the impact in comparison with the no sensor scenario as a result of the installation of an increasing number of sensors fig 6 a reports the graphs of benefit for the four objective functions for an increasing number of sensors the fronts show increasing values of benefit as the number of sensors increases up to 10 with the additional benefit due to the installation of a further sensor progressively decreasing for all the four objective functions this suggests the possibility to set a threshold of profitability economic step for the choice of the most suitable number of sensors to install in the network especially in the presence of budget constraints in this regard the total cost c tot associated with each wqss was calculated and the pareto fronts were rearranged as shown in fig 6b the incremental economic benefit was defined as the ratio of the total cost c tot of the wqss to the corresponding cumulative benefit table 2 it can be interpreted as the average cost of each percentage point of benefit provided by that wqss the threshold is the maximum acceptable value and represents the highest price the water utility is willing to pay for each percentage point in this case study was assumed equal to 1000 therefore n sens 6 was chosen and the corresponding four wqss layouts were selected as possible feasible monitoring solutions for the water system of parete highlighted in bold in tables 1 and 2 the total cost associated with these four wqss see fig 6b is 72 000 for the t mean based and ec based layouts with two of the sensors being located in the most desirable locations and 75 000 for the p s based and p based layouts with one sensor being located in one of the most desirable locations these investment costs are acceptable for protecting the served population for a medium size water utility also considering that the per capita cost of this investment would correspond to roughly 7 for each inhabitant the small difference in terms of costs between the solutions is due to the accessibility feature of the locations and to the fact that the least desirable locations to which much higher investment cost would have been associated were preliminarily disregarded from the simulations after that the four selected solutions are reprocessed in terms of all the four quality criteria and globally compared the results of the postprocessing are reported in table 3 highlighting that none of the sensor layouts is capable of simultaneously getting the best values of all the performance indices however as expected each of them respectively optimizes the performance used for the optimization highlighted in bold moreover the t mean based p s based and p based layouts get very similar performance in terms of detection time t mean 213 min 214 min and 219 min respectively and detection likelihood p s 78 4 78 6 and 78 0 respectively instead these three layouts have slightly different values of exposed population p 65 72 and 61 respectively and extent of contamination ec 2293 m 2070 m and 2238 m respectively on the other way around the ec based layout shows completely different and generally worse values of all the performance indices obviously except for the extent of contamination with a value of ec 1623 m a criterion for ranking the solutions is proposed this is based on the normalization of the quality parameters with respect to the best one partial score s p in table 4 as discussed in the decision step without loss of generality the same importance was considered for the four criteria by assigning them a weight wt p 0 25 equidistribution of weights note that different weights can be assigned based on the most appealing criterion to target according to the specific monitoring priority and operators opinion for each solution in table 4 a total quality score s q last row in bold is attributed equal to the sum of all the weighted partial scores wt p s p in bold in table 4 this last step allows the operators to select the most feasible solution as a suitable trade off between all the selected monitoring criteria the highest value is reached by the p based solution s q 0 923 which makes this layout the most desirable one well balancing 3 out of the 4 quality detection performance parameters t mean p s and obviously p while featuring a slightly worse but still acceptable value for the extent of contamination ec the corresponding wqss layout is shown on the map of the parete wds colour pipes according to the accessibility of the site in fig 7 the final solution selected represents a suitable compromise between all the defined detection criteria yielding managerial and economic benefits because of considering logistic aspects and guaranteeing an efficient monitoring and warning system it is worth highlighting that even if the first steps allow for keeping the possible sensor locations uniformly spread throughout the wds as it is indeed desirable the final wqss solution selected presents some quite close sensors this is due to the objective function i e the population exposed through ingestion indeed since most of the population is concentrated in some areas the optimization step tends to locate sensors there the selection of a higher number of sensors would have yielded sensors more spread over the entire system the main advantage of the multi criteria method proposed in this paper besides the significant computational reduction thanks to the preliminary topological step is the possibility to select at each stage the most desirable solutions by combining several criteria and balancing the power of the heuristic tools optimization step with the opinion of the experts urbanistic economic decision steps resulting in a well balanced compromise between different viewpoints furthermore the topological step allows us to switch to a different perspective regarding the management and the monitoring of water systems in general indeed in this first stage besides considering the hydraulics and therefore the physics of the system the problem of sensor placement is shifted from the nodes to the links through the insertion of dummy nodes on the most central links this makes the approach more realistic since the devices are installed on the pipes which are the real asset accordingly this allows addressing a generally disregarded but crucial aspect from a practical point of view on which pipe must the sensor be installed considering that the samples would be very different depending on which pipe is tapped in this work the sensor layouts are directly optimised by considering the actual position on pipes therefore making the proposed multi criteria method even more appealing for the water utilities finally it should be admitted that the choice of the ultimate solution is sensitive to the change of the experts opinion in this regard a sensitivity analysis can be carried out to assess the dependence of the final selection on the variability in the weights assigned to the detection parameters during the decision making process the main goal hereto is to show the potential of this general framework for helping water utility managers in selecting objectively the most feasible solution for the wqss during the decision making process by considering several aspects 5 conclusions this paper proposes a multi criteria methodology for the design of water quality sensor system the topological characteristics of the water distribution system were exploited for the identification of the most central topologically weighted pipes the results shown were useful to define dummy nodes on those pipes which were considered as decisional variables for optimal sensor placement thus resulting in the reduction of the solution space and computational burden a further reduction was provided by disregarding from the analysis the dummy nodes located in the least desirable locations from an accessibility viewpoint solutions were searched for in the trade off between number of installed sensors and four quality parameters for an assigned set of contamination events subsequently the logistic economic criteria in order to consider the accessibility of the locations engineering experience domain knowledge and investment cost were included to further narrow the solution space to the most desirable solution indeed the aim was to provide a general tool for the decision making process particularly tailored to the frequent constraint of limited budget by considering simultaneously several quality assessment criteria and the practical aspect that monitoring devices are installed on pipes instead of at nodes indeed the exclusive use of complex optimization procedures based on thousands of hydraulic water quality simulations is often worthless since the choice of number and locations of water quality sensors should be a trade off between economic operational aspects and the aim of protecting populations by quickly detecting contamination events it is worth pointing out that some of the future research directions identified by ostfeld et al 2008 during the battle of the water sensor networks and later confirmed by hart and murray 2010 were addressed in the present study highlighting the opportunities offered by the methodology proposed specific reference was made to aggregation the possibility of using a reduced but still significant sample of locations as potential sensor placement by focusing only on the most topologically central pipes and by eliminating uneasily accessible locations this is particularly advantageous in the case of big sized wdss for which the problem of optimal sensor placement may become computationally untreatable selection of number of sensors the possibility to identify the marginal returns in terms of protection and costs for additional sensors for establishing the profitable number of sensors especially in the case of limited budgets a novel approach was proposed for the selection of the most desirable solution based on the urbanistic features of sites which has the advantage of reducing the subjectivity of the choice without disregarding the know how of the operators multiobjective analysis the possibility to guide water utility operators in the decision making process by means of different protection objectives a multi criteria method and leaning on the design of a weighted normalised decision matrix all the issues addressed contribute to reduce the computational complexity of the methods for optimal sensor placement furthermore by considering the practical and operational aspects of the problem they contribute to fill the knowledge gap that was identified as responsible for limiting the widespread application of sensor placement technologies in drinking water distribution systems future work will investigate the possibility of including the dynamic behaviour of the system by considering the temporal variability in operating conditions normal and abnormal situations by directly implementing them in the graph of the wds and assessing the impact on the sensor layout another potential avenue to explore is the sensitivity analysis of the central pipes selected to the weights attributed to the graph finally the possibility to formalise the shift of the modelling paradigm from nodes to pipes will also be investigated and the decision making step will be enriched by adding other managerial criteria declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the research was conducted as part of the activities financed with the awarding of the v alere 2019 project of the università degli studi della campania l vanvitelli 
25599,this paper aims to solve three issues frequently present in the optimal placement of water quality sensors for protecting water distribution systems wdss from both accidental and intentional contamination namely i computational intractability of the optimization problem as the size of the wds increases ii unrealistic assumption that sensors are positioned at nodes rather than on system pipes and iii neglecting site specific practical conditions impacting on sensor installation the three drawbacks were tackled by i restraining the optimization to the hydraulic topological wise most important pipes ii introducing dummy nodes in the middle of these pipes as potential sensor locations iii applying a multi criteria decision making tool incorporating urbanistic and economic factors for selecting the most effective sensor locations the method is tested on the wds of the town of parete italy showing the manyfold benefits of the solution obtained keywords contamination warning system complex network theory edge betweenness sensor placement water distribution system topology water network protection 1 introduction water distribution systems wdss are an essential part of the critical infrastructure of a city since the availability of clean water affects both socio economic prosperity and population safety wdss are considered inherently vulnerable to both intentional and accidental contaminations due to their large size up to tens or hundreds of kilometres of pipes complexity large number of served users and access points e g hydrants consumer connections tanks reservoir and leak points oliker et al 2016 the assessment of contamination risk comes with an uncertainty related to the type of contaminant and its consequences and the characteristics of its intrusion time duration and location making it one of the most difficult problems to address for wds management a widely used strategy for securing wdss against contamination is the installation of a water quality sensor system wqss awwa 2005 the aim is to quickly assess water quality enabling early detection of potentially dangerous conditions a wqss provides indications on contamination events janke et al 2006 and helps locating their source by using time and location of the actual detection ung et al 2017 considering that the first hours after contamination are crucial for mitigating its impacts zulkifli et al 2018 the continuous monitoring of water quality parameters plays a key role for implementing and maximizing the benefits of an early warning system hu et al 2017 to maximize the wqss detection capability water operators should address the issue of identifying the most suitable locations for sensor placement by balancing both performance aspects and economic investment murray et al 2008 usually securing the entire network is infeasible in practice due to budget constraints that often limit the number of sensors a water utility can deploy in this regard sensors should be placed in strategic locations at the same time easily accessible and assuring maximum capability of detecting and identifying contaminants in a short time since 1991 lee et al 1991 researchers and practitioners have explored the optimal sensor placement problem in wdss since the events of september 11th in the united states protecting critical infrastructures from potential terrorist acts has become an absolute priority various methodologies have been proposed to define an optimal wqss lee and deininger 1992 uber et al 2004 chang et al 2013 these are generally arranged in top down decision support frameworks khorshidi et al 2019 with the upper and lower decision levels related to public safety and operational costs respectively however the optimal sensor placement challenge is still open from different viewpoints e g identification of most suitable locations performance evaluation applicability to real world scenarios and no general optimality criteria have been found rathi et al 2015 divided models and algorithms for solving the sensor placement problem in two categories a single objective approaches such as the work of kessler et al 1998 woo et al 2001 ostfeld and salomons 2005 berry et al 2009 b multi objective approaches such as the methods proposed by propato and piller 2006 huang et al 2006 wu and walski 2006 dorini et al 2008 in the battle of the water sensor networks bwsn approaches of the two categories were compared and tested ostfeld et al 2008 overall the objective functions developed in the literature are related to detection likelihood expected contaminated water volume detection time and exposed population extensive critical reviews of the topic can be found in hu et al 2018 as well as in adedoja et al 2019 who further classify the existing methods into four categories opinion based rule based optimization based and theory based generally due to the huge number of potential contamination scenarios and to the wds complexity the problem of optimal wqss layout comes with high computational costs especially for large wdss as reported by xu et al 2013 the optimal sensor placement in a network represents a np hard combinatorial optimization problem in this regard together with the investigation of several objective functions continuous efforts have been made to develop increasingly efficient numerical techniques hart and murray 2010 diao and rauch 2013 in this regard during the recent years several aspects of the optimal water quality sensor placement have been addressed zhao et al 2016 proposed a branch and bound sensor placement algorithm based on greedy heuristics and convex relaxation to minimize the consumption of contaminated water prior to contamination detection rathi and gupta 2017 maximized the demand coverage and the detection probability with a time constraint for the early detection tinelli et al 2018 discussed the impact of objective function selection on the optimal sensor placement problem ciaponi et al 2019 proposed a combined management strategy for monitoring wdss based on water network sectorization and installation of water quality sensors giudicianni et al 2020 presented a topological approach for the case of limited information about the system which relies on a priori clustering of the wds and on the installation of water quality sensors at the topologically most central nodes of each cluster hooshmand et al 2020 addressed the sensor placement problem by minimizing the number of vulnerable nodes and assuming a limited sensor budget availability lee and yoo 2020 suggested a methodology for defining water quality sensor locations considering the variability in water flow directions due to abnormal functioning conditions taha et al 2021 considered the previously overlooked metric of state estimation and network wide observability of the water quality dynamics to find optimal sensor placement with kalman filtering fasaee et al 2021 developed a new model to identify the optimal location of sensors to effectively support hydrant flushing for ensuring an efficient discharge of contaminants the variation in node contamination probability due to population density and user properties has been addressed in several works he et al 2018 in this regard hu et al 2021 proposed a multi objective approach based on the different characteristics of each node and the risk levels of contamination events showing the effect of such variability on the selection of sensor locations naserizade et al 2018 used the nsga ii and included cost and probability of undetected events and uncertainties related to a contamination injection in the optimization process while cardoso et al 2020 considered four contamination probability functions combined with a clustering based post processing method for a pareto front analysis the characteristics of sensors have been considered by zeng et al 2018 who maximized the quality of sensing and considered two types of sensors with different prices and communication capabilities sankary and ostfeld 2018 simultaneously minimised the affected population and the expected number of false positive detections while de winter et al 2019 investigated the influence of sensor imperfection by means of two greedy algorithms and by considering multiple objective functions different techniques and algorithms have been explored and adopted for solving the problem of the optimal water quality sensor placement for example the information entropy theory was used by khorshidi et al 2018 and by brentan et al 2021 for reducing the computational burden of the problem and developing a multi criteria decision making technique for the selection of an optimal solution respectively hu et al 2020 and jafari et al 2021 adopted the nsga iii algorithm to solve the multi objective sensor placement problem by considering the graph connectivity for the selection of individuals and the effect of contamination in important junctions in terms of social consequences respectively finally the resilience of water quality sensor placement strategies was investigated by zhang et al 2020 and nikolopoulos et al 2021 zhang et al 2020 considered all likely sensor failures and defined metrics for ranking alternatives nikolopoulos et al 2021 developed a novel methodology to assess the resilience under cyber physical attacks this paper presents a novel method and a new perspective for the water quality sensor placement problem in a wds compared to the previously developed methods the major novelty lies in considering more realistically the placement of sensor on pipes rather than in wds nodes before carrying out the optimization complex network theory tools are applied to define the most important pipes on which subsequently locating dummy nodes as possible sensor locations the graph of the wds is weighted with a pipe hydraulic resistance surrogate parameter in order to consider also hydraulic and contaminant transport aspects accordingly weighting the graph allows definition of the most important pipes from both topological and hydraulic viewpoints as a result the computational burden is significantly reduced since a small subset of pipes is defined for the following optimization phase four different objective functions are investigated and optimised to define the most suitable sensor placement layouts by exploring the new reduced solution space in order to identify the most efficient and effective monitoring system besides economic incremental benefit and detection performance criteria detection time detection likelihood population exposed and extent of contamination further logistic site specific conditions are considered such as surrogate metrics of accessibility and easiness of installation finally this paper provides a general multi criteria decision making tool for supporting decisional processes in wqss design 2 methodology as mentioned before optimal sensor placement in a water distribution network represents an np hard combinatorial optimization problem xu et al 2013 as the computational complexity exponentially increases with the growth of wds size a constraint on the number of sensors should be added to this problem for obvious economic and practical reasons the proposed methodology consists of five steps that progressively reduce the number of potentially adoptable solutions according to different criteria based on safety logistic and economic viewpoints a modelling of the wds as a weighted graph calculation of the most central pipes and insertion of dummy nodes topological step b classification of locations through an accessibility criterion logistic step c heuristic optimization with four objective functions optimization step d calculation of sensor installation cost and setting up of an incremental economic benefit threshold economic step e design of a decision support system based on a weighted normalised matrix of the detection criteria decision step the overall methodology adopted for the search of the most suitable wqss is summarised in fig 1 as subsequently described in detail the logistic step intervenes twice in the methodology before the optimization step and during the economic one 2 1 topological step the starting point of the method consists of modelling the wds as an undirected weighted graph g v e w taking advantage of the topological properties of the wds graph giudicianni et al 2018 indeed perelman and ostfeld 2011 showed how adopting graph theory can help in gaining insight in to the wds behaviour by simplifying its operation sitzenfrei 2021 showed graph theory s potential for assessing water quality of the wds furthermore giudicianni et al 2021 identified the most critical contamination sources by means of topological metrics in particular v is the set of n nodes v i junctions reservoirs and tanks e is the set of m links l ij v i v j from node v i to node v j pipes valves and pumps and w ij w is a weight characterising the physical characteristics of i th link the graph is considered weighted with a surrogate measure of pipe hydraulic resistance herrera et al 2016 specifically the weight w i l i d i has been assigned to each link with l i and d i the length and diameter of pipes to obtain a graph model that also considers the hydraulic behaviour of the wds the aim is to take into account the phenomenology of the system through a non dimensional weight that is linearly dependent only on geometric characteristics of pipes not related to a specific head loss formula and not simulation based in such a way as to make it as general as possible subsequently the application of complex network theory algorithms allows considering simultaneously the topological structure and the hydraulic characteristics of the system in this paper the networkx python package is used for the topological analysis hagberg et al 2022 this aspect represents an improvement compared to previous works giudicianni et al 2020 on the application of topological approaches to wqss design where the graph was considered unweighted the second phase of the topological step is the search of major links through the edge betweenness b c l a centrality metric borrowed from complex network theory to this aim the shortest path between two nodes is defined as the sequence of links connecting two nodes crossing the links associated with the minimum sum of weights dijikstra 1959 the edge betweenness b c l of a link l newman and girvan 2004 is defined as the sum of the ratios of the number σ vi vj l of shortest paths between pairs of nodes v i and v j that run through that link l and the total number σ vi vj of shortest paths connecting pairs of nodes v i and v j the edge betweenness centrality b c l of a link l is then mathematically defined by equation 1 1 b c l v i v j v σ v i v j l σ v i v j this metric allows identifying which links in a network appear more often along the shortest paths connecting pairs of nodes therefore it can be used as a measure of the influence of a link over the information water flow throughout the network a link with a high value of the edge betweenness usually represents a bridge like connector between two parts of a network the removal of which may affect the communication between many pairs of nodes through the shortest paths between them lu and zhang 2013 after weighting the links of the graph with the weight w i l i d i defined above the search for the highest edge betweenness links will enable identifying the links that have simultaneously a higher connectivity with pipes characterized by lower resistance allowing to define the most central pipes from a topological hydraulic viewpoint indeed by weighting the graph the shortest path between two nodes becomes the minimum weighted distance between two nodes i e with the minimum sum of weights assigned to the corresponding pipes which in the present study corresponds to the minimum sum of pipe surrogate resistances sensors located on these pipes are supposed to detect contamination intrusion and spreading in an easier and faster way the solution space will be narrowed after selecting the most central pipes using the edge betweenness criterion indeed this phase allows focusing the following optimization step on a much smaller subset of pipes strongly reducing the computational burden of the entire process the last point of the topological step is the insertion of dummy nodes in the middle of the selected major pipes characterised by null base demand and with elevation and coordinates based on a linear interpolation between the end points of the considered pipe this point makes the current methodology closer to real world applications since sensors are installed on pipes and not at nodes as it was assumed by all previous theoretical works on the topic while being simple in its computational implementation this assumption is infeasible from a practical point of view especially in correspondence to a cross or tee junction where the samples would be very different depending which of the converging pipes is actually fitted with the sensor by inserting dummy nodes on the most central pipes and narrowing the search of the possible sensor locations only to them the aforementioned practical aspects are considered furthermore the same computational simplicity as that associated with the search for optimal sensor locations at nodes is kept it is worth highlighting that though a single potential sensor location was considered in this work for each pipe the methodology can be easily extended to consider more locations in the long pipes as an example after setting a threshold length value the number of potential locations present in the generic pipe can be calculated as the ratio of the pipe length to the threshold length value rounded to the closest integer 2 2 logistic step generally the problem of water quality sensor placement is faced by using a single or multi objective optimization approach according to the operators choice without considering practical aspects related to site specific conditions and the spatial variability in logistic conditions i e accessibility to the sensor placement solution areas as well as to the underground services nearby for the full functioning of the monitoring stations all the locations are assumed to be equally good candidates for sensors and therefore they are considered equally desirable from a cost and accessibility standpoint this constitutes a strong simplification compared to real world applications berry et al 2005 and a field survey should be performed to ensure that the generic selected site is suitable for an easy installation of the sensor i e protected room for housing the instrumentation easy access for installation and maintenance activities electric power supply wired or wireless connection for transmitting acquired data giudicianni et al 2020 in order to also consider economic and accessibility aspects an analysis of the city map needs to be included to identify more less desirable positions for locating sensors results of the analysis will be spatially visualised on the layout of the case study considered thus identifying areas of interest that can be classified as follows most desirable locations green pipes water company sites and public buildings i e fire or police stations regularly visited by water utility maintenance personnel these locations do not need construction works to install the monitoring station to ensure power and scada connection least desirable locations red pipes highways rivers busy crossroads for which there are issues with confined space entry necessity of specific equipment and traffic control neutral locations blue pipes those not belonging to the previous two classes the number and typology of locations as defined above constitute another parameter for the assessment of the most feasible wqss in particular the least desirable locations are eliminated from the suitable sensor locations in such a way as to further reduce the solution space 2 3 optimization step after identifying the most central pipes and inserting the dummy nodes in their middle and eliminating the least desirable locations an optimization run is carried out by using the threat ensemble vulnerability assessment and sensor placement optimization tool teva spot developed by the us environmental protection agency epa janke et al 2012 u s epa 2008 in this context four objective functions detection time detection likelihood population exposed through ingestion and extent of contamination are used it is worth highlighting that in this work water demand and therefore served population is concentrated at nodes hence to calculate the objective functions reference is made to nodes and to the time when they are reached by the contaminant let us denote with s the total number of considered contamination scenarios in particular the following assumptions have been made for the setting up of the set s of contamination events considered for the wqss design all the demand nodes and the reservoirs have been one by one considered as potential locations for contaminant injection contamination starting time at the beginning of any of the 24 h of a day 1 single value of the mass injection rate 1 single value of the injection duration only one couple of values for mass injection rate and duration were sampled from those proposed by preis and ostfeld 2008 using the procedure of tinelli et al 2017 aiming to obtain a small but still statistically significant set of contamination events the following objective functions are adopted 1 detection time 2 t m e a n t s where t s for each contamination scenario s s represents the elapsed time from the start of the contamination to the first presence of a nonzero contaminant concentration identified by a sensor of the monitoring system i e the time of the first contaminant detection in this context a perfect sensor for the generic contaminant is assumed the characteristic detection time of a generic sensor layout is defined as the average of all t s for all the contamination events considered t is minimised with the objective to reduce the time of detection for all contamination scenarios considered for the wqss design 2 detection likelihood 3 p s 1 s s 1 s d s where d s 1 if contamination scenario s th is detected and d s 0 otherwise p s represents the probability of detecting the contamination p s is maximized so to detect as many contamination scenarios as possible 3 population exposed through ingestion 4 p m e a n p s where p s is the number of people that ingest contaminated water for the generic contamination scenario s before the first detection the five fixed times ingestion model davis and janke 2009 is considered for modelling the water consumption according to which users use tap water at five fixed times during the day 7 30 a m 10 30 a m 12 00 a m 3 00 p m and 6 00 p m the duration of the ingestion is considered instantaneous then if any contamination reaches a consumption node at one of such five fixed times the population allocated to the node is assumed to be exposed through ingestion then p is minimised to lessen the impact of contamination on the population 4 extent of contamination 5 e c m e a n l c s where l c s is the total length of the contaminated pipeline the length of pipe contaminated during a contamination event s will be the sum of the length l c s of all the contaminated pipes in the period t s ec is minimised to lessen the impact of contamination on the network 2 4 economic step the proposed method offers the possibility of introducing an economic criterion for selecting the most cost effective solutions among the set of configurations that satisfy topological detection performance criteria for each of the four objective functions described above optimization step 10 wqss are defined with an increasing number of sensors from 1 to 10 sensors to define four pareto fronts as a function of the number of sensors then a simple economic analysis is performed to evaluate the installation which includes only the purchase cost c sens of the sensor or c sens the civil work cost c cw for the desirable or neutral locations respectively logistic step according to the above mentioned factors the cost of a monitoring station is equal to 6 c s t c s e n s f o r t h e m o s t d e s i r a b l e l o c a t i o n c s e n s c c w f o r t h e n e u t r a l l o c a t i o n accordingly the total cost c tot of each wqss is defined as the sum of the costs of all its monitoring stations this allows the pareto fronts to be rearranged considering the costs associated with the installation of sensors based on their logistic features then an incremental economic benefit threshold is defined and applied to the new pareto fronts for further reducing the set of possible adoptable solutions 2 5 decision step the last step consists of the design of a decision support system based on four detection performance metrics p i e the same metrics as those used in the optimization step it will result in a multi criteria matrix for the selection of the most suitable wqss for each solution the four parameters will be calculated and normalised with respect to the best value of the corresponding category partial score s p in order to also consider the importance of the parameters a weight wt p 0 1 is assigned to each of them in such a way that wt p 1 finally for each solution a total quality score s q is assigned equal to the sum of all the weighted partial scores 7 s q s p w t p theoretically the values of s q range between 0 the worst monitoring option and 1 the best monitoring option the five step method described above results in a tool for the decision making process to choose the most suitable appropriate wqss layout which considers detection performance logistic and economic aspects it can be straightforwardly extended by also adding other criteria to lead the utility manager towards an even more informed choice 3 case study the proposed method was tested on the real wds serving the town of parete located in a densely populated area situated 20 km to the north of naples italy with a population of around 11 000 inhabitants see fig 2 a b and 2c for the spatial distribution of pipe diameters lengths and weights w i l i d i respectively the wds of parete has 182 demand nodes with ground elevation between 53 m a s l and 79 m a s l 282 pipes made of cast iron with length ranging between 10 4 m and 542 m and diameter ranging between 0 06 m and 0 20 m and 2 reservoirs with fixed pressure head of 110 m a s l daily variation in the users demand has been simulated through an hourly demand pattern with multiplier values ranging from 0 2 to 3 1 accordingly the total demand at nodes ranges from 7 4 l s at night to 113 9 l s in the morning and midday peaks with an average value of 54 6 l s regarding the set s of contamination events considered for the wqss design all the 182 demand nodes and the 2 reservoirs were considered as potential locations for contaminant injection the value of the mass injection rate and the injection duration are set equal to 100 gr min and 60 min respectively the total number of considered contamination events was s 184 24 1 1 4416 in the context of the optimization the hydraulic and quality simulations were carried out using the hydraulic simulation software epanet rossman 2000 embedded in the teva spot software assuming a conservative contaminant a water quality time step of 5 min and a reporting time step of 5 min regarding the logistic analysis the location classification is shown on the map of the wds serving the city of parete in fig 3 five least desirable locations were defined around the middle of the wds and disregarded in the subsequent steps accordingly the investment cost assessment concerned only neutral and most desirable locations in particular the preliminary financial analysis assumed the cost of a multiple parameters and continuous monitoring sensor to be c sens 10 000 civil work cost c cw was estimated at 30 of sensor costs c sens then c cw 3000 therefore the cost of a monitoring station for neutral locations is c st c sens c cw 13 000 it is worth highlighting the generalizability of this step indeed the financial analysis can be carried out by further detailing the costs associated with the installation of the monitoring station as well as by considering a different cost values 4 results and discussion the first step was the calculation of the most central pipes according to the values of the edge betweenness centrality fig 4 shows the results of the weighted topological analysis fig 4a is a scatter plot of the weighted edge betweenness eb w of pipes sorted in descending order in which it is possible to spot a knee in the distribution in correspondence to the first 50 pipes the value of eb w corresponding to the knee was assumed as a threshold to select the most central pipes for the subsequent steps of the proposed method thus strongly reducing the set of potential sensor locations fig 4b shows the eb w for each pipe of the wds of parete in red higher values of the centrality and therefore the most central pipes from fig 4b it is also clear that the most central pipe eb w 0 31 is close to the middle of the wds the last point of the topological step is the insertion of dummy nodes in the middle of the 50 most central pipes selected in fig 5 a new sketch of parete wds is reported introducing the dummy nodes red circles to be considered as potential installation locations for the optimization after the further elimination of the least desirable locations it is evident that the most central pipes selected are spread throughout the entire network a visual analysis of the location of the dummy nodes fig 5 indicates that the topological step ensures a uniform spatial distribution of the potential locations of the sensors by covering all the geographical extension of the wds this is in agreement with nazempour et al 2018 s statement since a water distribution system is a geographically distributed network so should be the sensors the merging of information in figs 5 and 2 points out that the pipes with the largest diameters are preferred candidates for locating sensors the longest pipes at the border of the system are instead penalized in fact the betweenness centrality tends to favour more linked and internal pipes by considering the position of each pipe with respect to the rest of the network an additional benefit of this step is the possibility to significantly reduce the solution space by narrowing the set of possible sensor locations to only the pipes characterised by high values of eb w indeed to allocate n sens sensors within a network of m pipes the total number of possible wqss combinations is expressed in equation 6 8 m n s e n s m n s e n s m n s e n s if a number n sens 6 of sensors is assumed for the case study of parete it would give 6 62 1011 combinations the selection of the 50 most central pipes from which the 5 least desirable locations are removed would give 8 15 106 combinations with an almost 1 10 000 reduction of the solution space simulation results are reported in table 1 in terms of detection quality performance for the wqss layouts obtained by adopting one by one the four objective functions defined above and for an increasing number of sensors from 0 to 10 the maximum number of ten sensors is reasonable for a small medium sized wds like that tested in this work giudicianni et al 2020 specifically impact represents the value of the objective function corresponding to the wqss detection time detection likelihood exposed population and extent of contamination while benefit represents the percentage reduction for the detection time the exposed population and the extent of contamination or percentage increase for the detection likelihood of the impact in comparison with the no sensor scenario as a result of the installation of an increasing number of sensors fig 6 a reports the graphs of benefit for the four objective functions for an increasing number of sensors the fronts show increasing values of benefit as the number of sensors increases up to 10 with the additional benefit due to the installation of a further sensor progressively decreasing for all the four objective functions this suggests the possibility to set a threshold of profitability economic step for the choice of the most suitable number of sensors to install in the network especially in the presence of budget constraints in this regard the total cost c tot associated with each wqss was calculated and the pareto fronts were rearranged as shown in fig 6b the incremental economic benefit was defined as the ratio of the total cost c tot of the wqss to the corresponding cumulative benefit table 2 it can be interpreted as the average cost of each percentage point of benefit provided by that wqss the threshold is the maximum acceptable value and represents the highest price the water utility is willing to pay for each percentage point in this case study was assumed equal to 1000 therefore n sens 6 was chosen and the corresponding four wqss layouts were selected as possible feasible monitoring solutions for the water system of parete highlighted in bold in tables 1 and 2 the total cost associated with these four wqss see fig 6b is 72 000 for the t mean based and ec based layouts with two of the sensors being located in the most desirable locations and 75 000 for the p s based and p based layouts with one sensor being located in one of the most desirable locations these investment costs are acceptable for protecting the served population for a medium size water utility also considering that the per capita cost of this investment would correspond to roughly 7 for each inhabitant the small difference in terms of costs between the solutions is due to the accessibility feature of the locations and to the fact that the least desirable locations to which much higher investment cost would have been associated were preliminarily disregarded from the simulations after that the four selected solutions are reprocessed in terms of all the four quality criteria and globally compared the results of the postprocessing are reported in table 3 highlighting that none of the sensor layouts is capable of simultaneously getting the best values of all the performance indices however as expected each of them respectively optimizes the performance used for the optimization highlighted in bold moreover the t mean based p s based and p based layouts get very similar performance in terms of detection time t mean 213 min 214 min and 219 min respectively and detection likelihood p s 78 4 78 6 and 78 0 respectively instead these three layouts have slightly different values of exposed population p 65 72 and 61 respectively and extent of contamination ec 2293 m 2070 m and 2238 m respectively on the other way around the ec based layout shows completely different and generally worse values of all the performance indices obviously except for the extent of contamination with a value of ec 1623 m a criterion for ranking the solutions is proposed this is based on the normalization of the quality parameters with respect to the best one partial score s p in table 4 as discussed in the decision step without loss of generality the same importance was considered for the four criteria by assigning them a weight wt p 0 25 equidistribution of weights note that different weights can be assigned based on the most appealing criterion to target according to the specific monitoring priority and operators opinion for each solution in table 4 a total quality score s q last row in bold is attributed equal to the sum of all the weighted partial scores wt p s p in bold in table 4 this last step allows the operators to select the most feasible solution as a suitable trade off between all the selected monitoring criteria the highest value is reached by the p based solution s q 0 923 which makes this layout the most desirable one well balancing 3 out of the 4 quality detection performance parameters t mean p s and obviously p while featuring a slightly worse but still acceptable value for the extent of contamination ec the corresponding wqss layout is shown on the map of the parete wds colour pipes according to the accessibility of the site in fig 7 the final solution selected represents a suitable compromise between all the defined detection criteria yielding managerial and economic benefits because of considering logistic aspects and guaranteeing an efficient monitoring and warning system it is worth highlighting that even if the first steps allow for keeping the possible sensor locations uniformly spread throughout the wds as it is indeed desirable the final wqss solution selected presents some quite close sensors this is due to the objective function i e the population exposed through ingestion indeed since most of the population is concentrated in some areas the optimization step tends to locate sensors there the selection of a higher number of sensors would have yielded sensors more spread over the entire system the main advantage of the multi criteria method proposed in this paper besides the significant computational reduction thanks to the preliminary topological step is the possibility to select at each stage the most desirable solutions by combining several criteria and balancing the power of the heuristic tools optimization step with the opinion of the experts urbanistic economic decision steps resulting in a well balanced compromise between different viewpoints furthermore the topological step allows us to switch to a different perspective regarding the management and the monitoring of water systems in general indeed in this first stage besides considering the hydraulics and therefore the physics of the system the problem of sensor placement is shifted from the nodes to the links through the insertion of dummy nodes on the most central links this makes the approach more realistic since the devices are installed on the pipes which are the real asset accordingly this allows addressing a generally disregarded but crucial aspect from a practical point of view on which pipe must the sensor be installed considering that the samples would be very different depending on which pipe is tapped in this work the sensor layouts are directly optimised by considering the actual position on pipes therefore making the proposed multi criteria method even more appealing for the water utilities finally it should be admitted that the choice of the ultimate solution is sensitive to the change of the experts opinion in this regard a sensitivity analysis can be carried out to assess the dependence of the final selection on the variability in the weights assigned to the detection parameters during the decision making process the main goal hereto is to show the potential of this general framework for helping water utility managers in selecting objectively the most feasible solution for the wqss during the decision making process by considering several aspects 5 conclusions this paper proposes a multi criteria methodology for the design of water quality sensor system the topological characteristics of the water distribution system were exploited for the identification of the most central topologically weighted pipes the results shown were useful to define dummy nodes on those pipes which were considered as decisional variables for optimal sensor placement thus resulting in the reduction of the solution space and computational burden a further reduction was provided by disregarding from the analysis the dummy nodes located in the least desirable locations from an accessibility viewpoint solutions were searched for in the trade off between number of installed sensors and four quality parameters for an assigned set of contamination events subsequently the logistic economic criteria in order to consider the accessibility of the locations engineering experience domain knowledge and investment cost were included to further narrow the solution space to the most desirable solution indeed the aim was to provide a general tool for the decision making process particularly tailored to the frequent constraint of limited budget by considering simultaneously several quality assessment criteria and the practical aspect that monitoring devices are installed on pipes instead of at nodes indeed the exclusive use of complex optimization procedures based on thousands of hydraulic water quality simulations is often worthless since the choice of number and locations of water quality sensors should be a trade off between economic operational aspects and the aim of protecting populations by quickly detecting contamination events it is worth pointing out that some of the future research directions identified by ostfeld et al 2008 during the battle of the water sensor networks and later confirmed by hart and murray 2010 were addressed in the present study highlighting the opportunities offered by the methodology proposed specific reference was made to aggregation the possibility of using a reduced but still significant sample of locations as potential sensor placement by focusing only on the most topologically central pipes and by eliminating uneasily accessible locations this is particularly advantageous in the case of big sized wdss for which the problem of optimal sensor placement may become computationally untreatable selection of number of sensors the possibility to identify the marginal returns in terms of protection and costs for additional sensors for establishing the profitable number of sensors especially in the case of limited budgets a novel approach was proposed for the selection of the most desirable solution based on the urbanistic features of sites which has the advantage of reducing the subjectivity of the choice without disregarding the know how of the operators multiobjective analysis the possibility to guide water utility operators in the decision making process by means of different protection objectives a multi criteria method and leaning on the design of a weighted normalised decision matrix all the issues addressed contribute to reduce the computational complexity of the methods for optimal sensor placement furthermore by considering the practical and operational aspects of the problem they contribute to fill the knowledge gap that was identified as responsible for limiting the widespread application of sensor placement technologies in drinking water distribution systems future work will investigate the possibility of including the dynamic behaviour of the system by considering the temporal variability in operating conditions normal and abnormal situations by directly implementing them in the graph of the wds and assessing the impact on the sensor layout another potential avenue to explore is the sensitivity analysis of the central pipes selected to the weights attributed to the graph finally the possibility to formalise the shift of the modelling paradigm from nodes to pipes will also be investigated and the decision making step will be enriched by adding other managerial criteria declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the research was conducted as part of the activities financed with the awarding of the v alere 2019 project of the università degli studi della campania l vanvitelli 
