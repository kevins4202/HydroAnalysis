index,text
3685,in lieu of process based models evolutionary artificial intelligence techniques can yield accurate expressions describing complex phenomena in the current study closed form expressions are developed to predict solute transport in a fracture matrix system as a function of the parameters that describe relevant physical and chemical processes the study adopts a multi gene genetic programming approach to approximate a solution of the classical advection dispersion equation for reactive transport in single parallel plate fractures the approach is employed to obtain an accurate relationship between the hydraulic geological and chemical parameters of the fracture matrix system as inputs and an ensemble of breakthrough curves as outputs solutions generated by the developed model showed good agreement with those of corresponding analytical and numerical models computationally the developed approach is highly efficient particularly when compared with the analytical solution which typically requires relatively fine discretization to calculate the long tailed breakthrough curves therefore future work could extend the developed model to simulate field scale networks and include additional and more complex transport phenomena this approach advances solute transport behavior predictions through being simpler and computationally more efficient than currently adopted techniques which is important as the scale of simulation increases from that of a single fracture to a network keywords closed form solution fractured rock matrix diffusion multi gene genetic programming solute transport 1 introduction improper handling and disposal of hazardous material have led to the contamination of thousands of aquifers globally resulting in extensive dissolved plumes and the diffusion of dissolved compounds into low permeability zones e g rock matrix muskus and falta 2018 once a compound enters a low permeability zone the options for remedial strategies are limited and reverse diffusion results in plume persistence dual porosity models are required to couple the fracture and matrix systems to account for the transfer of groundwater and solute mass between them such models are challenging as one source of porosity i e fracture provides the transmissive path for transport with little storage and the other i e matrix provides most of the storage capacity but its conductivity is minimal trinchero et al 2020 trinchero and iraola 2020 neretnieks 1980 presented an analytical solution for solute transport in dual porosity systems based on similar solutions for heat transport carslaw and jaeger 1959 which considers an infinite rock matrix and neglects longitudinal dispersion in the fracture subsequently the solution was generalized by tang et al 1981 who implemented in plane dispersion and sudicky and frind 1982 who considered a finite rock matrix more recently other researchers cvetkovic 2010 mahmoudzadeh et al 2013 neretnieks 2006 extended these analytical solutions through the incorporation of additional mechanisms and phenomena neretnieks 2006 developed a mathematical model that accounts for diffusion from the flowing zone into a stagnant layer adjacent to the fracture through which solutes can diffuse into the rock matrix in addition they considered reversible diffusion in both the stagnant layer and the matrix they conceptualized the fractures as both tube like and slit like channels and considered the presence of intersecting fractures but they did not consider mixing at fracture intersections these solutions are useful for modelling solute transport in complex three dimensional networks that consist of a large number of fractures with different properties and widely varying flow rates mahmoudzadeh et al 2013 extended neretnieks 2006 work through exploring various contributions of the matrix and stagnant layer and their relative significance on solute transport in fractured rocks their analytical model accounted for diffusion between the fracture and an adjacent matrix composed of different geological layers limited layered matrix through implementing a stepped diffusion process from the flowing water through the stagnant layer and subsequently the adjacent matrix the model can be extended to describe contaminant transport in heterogeneous fractured media consisting of different fracture and matrix characteristics they found that in narrow fractures the stagnant water layer and adjacent rock matrix may lead to considerable retardation their work recommended accounting for both the stagnant layer and at least two matrix layers when assessing the performance and safety of deep geological repositories for spent nuclear fuel as the stagnant water layer allows the solutes to access a larger fracture surface from which to diffuse into the rock furthermore equilibrium between the fracture and matrix was found to be reached more rapidly when the matrix is composed of layers of altered rock and coating as opposed to being intact since the former is more porous moreno and crawford 2009 cvetkovic 2010 conducted short and long term tracer studies at the äspö hard rock laboratory site sweden to examine the impact of heterogeneity of the rim zone altered rock which experienced decreasing porosity they concluded that the macroscopic effects of the rim zone microstructure must be considered in order to extrapolate retention properties to larger scales and longer times neuman 2005 matrix retention is often described using retention time distributions which are in turn derived from the above discussed analytical solutions for transport in a single fracture e g cvetkovic 2010 sudicky frind 1982 tang et al 1981 these retention time distributions are typically precomputed and included in look up tables as these solutions consider additional mechanisms and phenomena under wider ranges of conditions the lookup tables are more computationally intensive due to the large number of values in each table and thus require multivariate interpolation methods it is for these reasons that numerical approaches are considered attractive alternatives to complex analytical solutions for simulating contaminant fate and transport phenomena e g hammond lichtner and rockhold 2011 iraola et al 2019 stein et al 2017 in this respect random walk methods are particularly appealing as they are not vulnerable to numerical dispersion they are able to accommodate complex model parameterizations and they are numerically efficient khafagy et al 2020 noetinger et al 2016 painter et al 2008 some researchers have implemented mass retention in the matrix within random walk simulations using retention time distributions derived from analytical solutions for transport in a single fracture e g cvetkovic 2010 sudicky and frind 1982 tang et al 1981 however process based models of fractured systems i e mass balance advection dispersion chemical reactions are generally computationally intensive moreover dual porosity systems are highly nonlinear due to the difference in time scales between the fracture and matrix as a result numerical simulation of dual porosity systems requires even more extensive time and resources appropriately trained data driven models have recently been proposed as an alternative approach to simulating complex physical and geochemical processes in fractured media esfahani and datta 2016 when validated such models may substantially decrease computational time and improve feasibility and reliability of solute transport simulations such models can generally specify the relationship between input patterns e g mass flux at system boundaries hydraulic geological and chemical properties and output patterns i e solute spatiotemporal concentration variation within the system genetic programming gp techniques inspired by biological evolution were first proposed by koza 1994 compared to other machine learning methods the key advantage of gp lies in its ability to optimize both the variables and constants of the candidate models while being free of the constraints imposed by initial model structure definitions to reach a solution gp initially generates a random population using various functions and terminals assigned by the user at each subsequent generation a new population is created by selecting the best generation defined by the most accurate relationship between the independent and dependent variables heydari et al 2016 sheikh khozani et al 2020 i e best gene gp is a robust method with several advantages over other commonly employed data driven methods e g artificial neural networks hadi and tombul 2018 including 1 generation of explicit expressions or glass box models 2 automatic discovery of model structure utilizing given data 3 adaptive evolutionary ability to generate global solutions without becoming trapped in local optima and 4 not requiring any specific prior domain knowledge as such gp has been applied widely in water resource related research including hydrogeologic aryafar et al 2019 cianflone et al 2017 esfahani and datta 2016 sadat noori et al 2020 river stage ghorbani et al 2018 hadi and tombul 2018 mehr and gandomi 2021 real time wave forecasting kambekar and deo 2014 water quality jamei et al 2020 and rainfall runoff modelling chadalawada et al 2020 heřmanovský et al 2017 multi gene genetic programming mggp advances gp through linearly combining low depth gp blocks to improve the accuracy of solutions evolved by single gene gp additionally mggp develops less complex models than single gene gp as it uses fewer functions gandomi et al 2015 although mggp is being adopted its use in water resource applications is not yet ubiquitous a recent review of gp applications in hydrology found that the mggp variant was used in nine of 142 papers danandeh mehr et al 2018 the purpose of this study is to provide an accurate closed form data driven expression describing reactive solute transport in single saturated fractures with matrix diffusion that is substantially more computationally efficiency than current methods i e numerical and analytical approaches mggp is employed to obtain an accurate relationship between the coefficients of the lognormal distributions obtained from an ensemble of breakthrough curves btc as outputs i e dependent variable and the hydraulic geological and chemical parameters of the fracture matrix system as inputs i e independent variable this more computationally efficient model will facilitate the simulation of complex network scale problems 2 model development a dataset was generated containing an ensemble of btcs using an analytical solution developed by sudicky and frind 1982 based on a set of input parameters that span a specified range for the hydraulic geological and chemical properties subsequently mggp is used to generate expressions that accurately describe the relationship between the inputs i e hydraulic geological and chemical properties and the outputs i e parameters of the lognormal distribution describing the btc ensemble the analytical solution sudicky and frind 1982 simulates solute transport in one dimensional fractures with a constant concentration at the inlet boundary the solution considers advection molecular diffusion and mechanical dispersion within the fracture lateral matrix diffusion adsorption within the matrix and on the fracture wall and radioactive decay and provides the btcs at a specified distance from the injection source see appendix a for further details 2 1 dataset the dataset consists of the btcs calculated using eq a 1 with the parameter values listed in table 1 which provide a possible 352 800 combinations of hydraulic geological and chemical properties the ranges of these properties are selected to represent the values most commonly observed in recent literature as referenced in table 1 the integrals in eq a 1 were solved numerically using a scanning procedure to determine the numerically significant ranges of ε and ξ subsequently the midpoint rule was employed to discretize the integration function into 50 000 and 1 000 connected rectangles for ε and ξ respectively within the significant range this approach is expected to result in more efficient integration with minimal loss in accuracy as each integral is only discretized within the significant range however even with this discretization procedure it would take many years to conduct all of these simulations on a typical pc as such the shared hierarchical academic research computing network sharcnet was used the input file for the script contains approximately 350 sub input files which are distributed amongst three computing clusters to generate the btcs via parallel processing where l l is the fracture length v l t is groundwater velocity in the fracture d l 2 t is the molecular diffusion coefficient for the solute in water 2 b l is the aperture width 2 b l is the distance between centerline of two fractures α l is the dispersivity and θ is the matrix porosity a lognormal distribution was fit to each btc using non linear least squares regression and the best fitting parameters were calculated by minimizing the root mean square error rmse between the analytical solution btc and the corresponding lognormal distribution as follows 1 rmse i 1 n c 1 i c 2 i 2 n where c 1 i and c 2 i are the solute concentrations calculated at the fracture outlet at a specified time i obtained from the analytical solution and the appropriate lognormal distribution respectively and n is the number of concentration points on the curve used in this analysis matrix retardation is implemented in the pdf and cdf of the lognormal distribution by dividing the time t by the retardation coefficient in the matrix r m khafagy et al 2020 zhang et al 2012 as follows pdf 2 1 t r m σ s 2 π e x p l n t r m μ 2 2 σ 2 cdf 3 1 2 1 2 e r f ln t r m μ 2 σ where μ t is the mean σ t is the standard deviation and t t is the solute arrival time at the outlet the matrix tortuosity τ is accounted for in the matrix diffusion coefficient d m which is obtained by multiplying the molecular diffusion coefficient d by τ 2 2 multi gene genetic programming evolutionary algorithms eas are capable of approximately simulating complex models effectively using stochastic search methods gp models are a class of eas considers nodes which are elements composed from either a function tree or terminal leaf set function sets may include arithmetic operators or mathematical functions sin cos tanh ln etc boolean operators and or not etc logical expressions if or then or any other suitable functions defined by the user whereas terminal sets include variables constants or both fig 1 the gp tree is randomly formed from chosen functions and terminals the root node is functional and has one or more branches extending from it that end in terminal nodes initially a set of gp trees are randomly generated using functions and terminals determined by the user the number of gp trees initially generated forms the initial population size initial gp gene and is based on the maximum allowable number of genes as defined by the user mggp models provide solutions through a linear combination of the individual genes sub genes a sub gene is a mathematical solution linking some or all of the input and output parameters and each sub gene forms a part of the solution for its generation fig 1 the multi gene model shown in fig 1 predicts an output variable y using input variables x 1 x 2 and x 3 this model structure contains non linear terms i e sine and square root but is linear in terms of the coefficients c 0 c 1 and c 2 the linear combination of these genes is referred to as a multi gene the linear coefficients weights of genes and the model bias are obtained from the training data using ordinary least squares regression the initial population of the mggp model is generated from individuals that contain randomly evolved genes and similar to the standard gp model the mggp population is subjected to evolution mechanisms i e reproduction crossover mutation and architecture altering operations to generate an enhanced generation the evolved improved population replaces the existing population these evolutionary mechanisms mimic biological evolution natural selection processes reflecting survival of the fittest however some special crossovers mechanisms in addition to the standard evolution mechanisms are introduced to allow exchange of genes mutation between individuals the mutation operation provides six methods for mutation between genes 1 sub tree mutation 2 mutation of constants using an additive gaussian perturbation 3 substitution of a randomly selected input node with another randomly selected input node 4 set a randomly selected constant to zero 5 substitute a randomly selected constant with another randomly generated constant and 6 set a randomly selected constant to one the evolutionary mechanisms are grouped into categories referred to as events and set by the user in the form of probabilities such that the sum of reproduction crossover and mutation probabilities is 1 the reader is referred to searson et al 2010 for a more detailed explanation of mggp and the evolutionary mechanisms the fitness function describes the accuracy between the individual input and the target output and is evaluated by an objective function that is pre defined by the user e g r2 rmse gp continues to produce new generations until it reaches a specified termination criterion based on either the desired fitness function or a maximum number of generations as defined by the user it is important to consider the trade off between accuracy and complexity in mggp model development which are influenced by the maximum depth of the gp tree that reflects the number of nodes and the maximum allowable number of genes as specified by the user respectively fig 2 shows the flowchart of the mggp method for obtaining the best fit model two mggp models were developed in this work to establish closed form expressions reflecting the relationships between the hydraulic geological and chemical parameters i e input variables and lognormal distribution parameters i e mean μ o and standard deviation σ o as output variables in this study the gptips toolbox searson et al 2010 for matlab version 2021a was used to develop the mggp models based on the parameters and settings in table 2 the data were classified into training 80 validation 10 and testing 10 sets to avoid overfitting k fold cross validation was conducted using the training and validation datasets with 500 realizations to ensure model robustness in each of the 500 realizations the training and validation sets i e 90 of the dataset are divided into nine folds such that eight folds i e 80 of the dataset are used to train the mggp model and one fold i e 10 of the dataset is used to validate the trained model the fitness of each model was determined based on the minimized objective function which was the rmse between the predicted and actual values of the lognormal distribution parameters with a termination value of 0 0002 if the objective function did not reach the termination value the optimal model was achieved when the mggp procedure reached 1000 generations table 2 the best model from the 500 realizations was selected based on the lowest rmse this process was repeated until either the fitness termination value 0 0002 or the maximum number of generations 1000 was reached the testing dataset was then used to select a single optimal mggp model from the best models representing each generation to minimize individual effects of random data assignment to folds and to ensure the model was not overfit this optimal mggp model is hereafter referred to as the single fracture solute transport mggp sf st model it must be noted that increasing the mggp model accuracy by specifying the population size and maximum number of generations leads to increasing model complexity as such there is a trade off between model accuracy and complexity the latter of which is controlled by specifying the maximum number of genes and the tree depth this trade off was achieved herein by limiting both the maximum number of generations and population size to 1000 for accuracy and the maximum tree depth and maximum number of genes to five and 10 respectively to minimize model complexity the closed form expressions generated using mggp will develop μ p and σ p that can be implemented in eqs 2 and 3 to calculate the btcs for pulse and constant solute injections respectively a sensitivity analysis was conducted using variance based global sensitivity analysis to identify the level of influence of each input variable on the model by calculating their total sensitivity indices the indices are calculated by generating a random value uniformly distributed for each variable within the specified range in the model and considering the contribution of the other variables on the simulation ten thousand samples are generated for each variable using a quasi random monte carlo simulation to calculate the total sensitivity indices the mggp sf st model was verified against both the analytical solution eq a 1 and the random walk particle tracking rwpt model described by khafagy et al 2020 with an instantaneous release of 10 000 particles at the upstream end of the fracture the hydraulic geological and chemical properties used in the validation process are l 30 m v 1 m day d 1 10 9 m2 s 2 b 80 µm 2 b 1 m α 1 m θ 0 1 τ 1 r 1 and r m 1 2 3 where r is the retardation coefficient in the fracture 3 results and discussion the final dataset contained 101 740 btcs of the potential 352 800 parameter combinations while some of the btcs generated required more refined discretization of the analytical solution eq a 1 for the parameter combinations they represent the computational time required for all 352 800 possible parameter combinations exceeded the computational time available through sharcnet thus the remaining 251 060 btcs were not generated nevertheless the 101 740 btcs that form the dataset are based on the full range of all parameters listed in table 2 and thus the volume and range of data generated are sufficient to implement mggp in parallel to the dataset generation it was important to determine which combinations of parameters required further discretization as such the parameter combinations for the btcs in the final dataset were visualized through a network graph i e nodes and edges using the network analysis software gephi version 0 9 2 fig 3 the nodes are identified as the hydraulic geological and chemical property values and the edges are connections between the nodes that form a unique set of parameter combinations representing a btc the line edge thickness between each two nodes in fig 3 is based on the weighted adjacency and represents the number of btcs generated based on combinations of the two nodes for example the properties 2 b and l 1000 m have a thinner edge than 2 b and l 1 m fig 3 indicating fewer btcs were generated using the former combination the nodes are sized based on their degree centrality which reflects the number of connections to other nodes properties with similar weighted adjacency and degree centrality for all values investigated were combined into a single node i e 2 b α and θ note that fig 3 shows only the links that all properties share with 2 b α and θ to maintain readability of the figure fig 3 shows that the weighted adjacency i e number of btcs generated decreases with increasing velocity increasing fracture length decreasing molecular diffusion coefficient and increasing aperture size this is because the ε and ξ curves appendix a which describe the integrals in eq a 1 tend to be more complex within these parameter ranges this is demonstrated in fig 4 which shows the concentration vs log ε curves for two select cases from the network analysis fig 3 each discretized into 50 000 points a log scale was chosen because the discretization is not the same among curves it decreases with increasing ε the parameters in case a were selected to reflect a combination with a low weighted adjacency indicating that finer discretization is typically required to generate an accurate ε curve for this parameter combination l 0 5 m v 0 1 m day d 1 10 6 m2 s 2 b 0 001 µm 2 b 1 m α 10 m θ 0 1 τ 1 r 1 r m 1 these parameter combinations also typically resulted in long tailed breakthrough curves generally due to a very small diffusion coefficient 1 10 10 m2 s with a corresponding high velocity 10 m day these results are in agreement with those obtained by wang et al 2018 who concluded that the standard advection dispersion model cannot be used to characterize the late time tailing of tracer btcs likely due to solute retention mechanisms e g matrix diffusion sorption the parameters in case b were selected to reflect a high weighted adjacency indicating that the discretization applied normally generated an accurate ε curve for this parameter combination l 50 m v 5 m day d 1 10 9 m2 s 2 b 10 µm 2 b 1 m α 1 m θ 0 1 τ 1 r 1 r m 1 as a result case b was generated for the dataset but case a was not as there were not sufficient computing resources to implement the additional discretization required for all similar cases in the dataset the rmse and r2 for the lognormal distribution describing each btc generated for the dataset i e 101 740 btcs exhibit excellent accuracy compared to the analytical solution the median and highest rmse are approximately 0 0051 and 0 023 respectively and the median and lowest r2 values are approximately 0 9995 and 0 9932 respectively fig 5 3 1 mggp model structure and evaluation mggp was employed to develop a relationship considering various combinations of fracture properties i e l v d 2 b 2 b α and θ and the parameters describing the lognormal distribution of the generated dataset i e μ o and σ o the best set of variable combinations represented by x 1 to x 7 was selected considering the most accurate relationship between those combinations and the parameters describing the lognormal distribution of the generated dataset i e μ o and σ o this relationship hereafter referred to as the mggp sf st model is described by the expressions for μ p and σ p in eqs 4 and 5 4 μ p 0 0431 x 4 0 999 x 2 0 224 x 1 0 542 e x 7 x 1 x 3 0 267 e e x 1 0 0431 e x 3 2 0 267 e x 7 0 903 x 3 x 4 e e x 4 8 81 1 0 6 x 5 x 1 2 0 812 x 3 e e x 4 0 143 e x 1 x 3 2 x 1 2 x 3 x 3 2 0 174 x 7 x 1 2 x 1 2 x 3 2 0 0862 x 1 0 402 3 x 1 e x 1 e x 4 1 94 5 σ p 1 16 e x 3 0 5 x 4 1 01 e x 1 x 3 0 947 e e x 4 0 0573 x 3 l n x 3 0 00105 x 1 e x 3 0 00209 x 4 e x 3 1 31 x 3 e e x 1 3 07 e x 1 3 91 e x 1 1 4 3 84 1 0 6 x 4 x 1 x 6 x 4 x 1 x 3 2 3 35 1 0 6 x 1 x 3 x 1 x 3 x 3 e x 3 1 05 where x 1 v 2 b 2 b l d θ x 2 l n v 2 b l 2 b θ x 3 α l x 4 d v l x 5 l n l x 6 d x 7 2 b where v is in meters year l is in meters d is in meters2 year 2 b is in meters 2 b is in meters α is in meters the effluent btc represents normalized concentration versus years and is obtained by substituting eqs 4 and 5 into either eqs 2 or 3 for pulse or constant injections respectively note that the mggp technique is empirical and thus the resulting mggp sf st model is not dimensionally consistent attention must thus be paid to utilize the correct input and output units e g hadi and tombul 2018 jamei et al 2020 yan et al 2021 it is also worth noting the physical meanings of some variable combinations specifically x 1 represents a modified péclet number p e for the matrix x 3 represents the unique correlation between dispersivity and fracture length that has been observed in lab experiments e g zech et al 2015 and x 4 is the reciprocal of p e in the fracture figs 6 and 7 show the mggp tree structures of each gene in the generated expressions for the mean and standard deviation respectively each expression was developed from seven input variables i e x 1 x 2 x 3 x 4 x 5 x 6 x 7 and has ten genes and a bias as shown in eq 4 mean and eq 5 standard deviation table 3 shows the pearson correlation coefficients r between the input variables x 1 to x 7 and the coefficients of the lognormal distributions from the dataset i e μ o and σ o the results show that these relationships are highly non linear indicating that the model complexity offered by mggp is required the best fitness rmse values are plotted against mggp generation for the expressions representing μ p and σ p fig 8 this analysis shows that the model accuracy is not significantly improved beyond 290 and 220 generations for expressions representing μ p and σ p respectively the selected models representing μ p and σ p have rmses of 0 0493 and 0 0425 respectively which correspond to generations 870 and 842 respectively while the selected model accuracies are slightly lower than those achieved by models with a larger number of generations they were chosen to balance accuracy with complexity fig 9 compares the btcs predicted by the mggp sf st model eqs 4 and 5 to the observed best fitting lognormal btcs for all 101 740 datapoints in the dataset the observed and predicted μ and σ for the entire dataset achieved r2 0 9998 and r2 0 9974 respectively note that of the 101 740 data points only 18 847 18 have a σ greater than two fig 9b shows that of the btcs with σ greater than two the mggp sf st model both over and under estimates σ values values of σ greater than two are associated with long tailed btcs for which the analytical solution typically requires additional discretization to achieve sufficient accuracy in this dataset curves with a large dispersion i e 2 represent a small fraction of the total number of curves 18 fig 10 shows the specific btcs associated with points a b and c in fig 9 to demonstrate the model sensitivity to the range of standard deviation residuals the comparison between the observed and predicted btcs shows very good agreement even for the cases in which σ is overestimated or underestimated i e cases a and c respectively the mggp sf st model is assessed using non parametric statistical tests at a 95 confidence level to compare the mean and variance of μ p and σ p with that of μ o and σ o specifically the wilcoxon rank sum method bickel and lehmann 1975 conover and iman 1980 evaluates the difference between the means of the observed and predicted parameters whereas levene s test levene et al 1960 evaluates the difference between the variances of the observed and predicted parameters the complete dataset training validation and testing sets is used for these statistical tests with respect to the difference between the observed and predicted means the p values at the 95 confidence level are above 0 05 table 4 indicating that the model error is insignificant the calculated p values for difference between the variances are also above 0 05 at the 95 confidence level table 4 indicating that the mggp sf st model variability is not significantly different from the observed data thus the mggp sf st model is able to predict the observed data at a 95 confidence level a residual analysis was performed to ensure that the relationship between the genes and lognormal parameters is linear i e the mggp sf st model is linear and the residuals are independent fig 11 the box and whisker plots showing residuals from the training and validation datasets and testing dataset show that they are all mostly clustered around zero and are approximately normally distributed indicating that the model is independent of the residuals and therefore valid the box and whisker plots also show that μ residuals from the testing dataset have a slightly higher standard deviation and median e g standard deviation 0 1044 median 0 0303 than those from the training and validation datasets e g standard deviation 0 0542 median 0 005 however these parameters are within an acceptable range for all datasets fig 12 illustrates the influence of each input variable on the mggp sf st model using the variance based global sensitivity analysis the results show that x 1 x 2 and x 4 have the greatest impact on the μ expression while x 1 and x 3 have the greatest impact on the σ expression 3 2 mggp model verification the comparisons between the mggp sf st model analytical solution and rwpt simulations show excellent agreement with the mggp sf st model predicting slightly later arrival times 1 3 later on average based on t 50 than the analytical solution fig 13 the mggp sf st demonstrated excellent computational efficiency it can predict the btc for any combination of fracture properties within the developed model parameter range in less than one second through matlab version 2021a in comparison the analytical solution and rwpt based models compute the same btcs in the order of hours in part due to the fact that the analytical solution typically requires more refined discretization to calculate the long tailed btcs this efficiency becomes extremely important as the scale of a problem increases from that of a single fracture to a network as it will be magnified 4 conclusion mggp was employed to develop a closed form expression mggp sf st model to simulate reactive transport in a single parallel plate fracture under a range of hydraulic geological and chemical conditions for either a constant concentration or pulse injection at the inlet the model was developed based on a large dataset of btcs 101 740 datapoints generated from a published analytical solution for solute transport in a single fracture under a range of conditions i e velocity dispersion along the fracture molecular diffusion within the fracture and into the matrix and adsorption within the matrix a network graph was created to visualize the combinations of parameters for which btcs were generated in the dataset this analysis showed that the analytical solution for certain parameter combinations typically those that result in long tailed btcs require more refined discretization which facilitated the generation of btc for the full range of parameter combinations the dataset was split into training validation and testing datasets and mggp was employed to develop the model due to the nonlinear relationship between the model inputs x 1 to x 7 and outputs i e μ o and σ o a lognormal distribution was fit to each curve in the dataset best fitting curves and the parameters i e μ o and σ o were compared to those predicted by the mggp sf st model i e μ p and σ p the plot of the observed versus predicted μ and σ for the entire dataset fig 9 achieved r2 values of 0 9998 and 0 9974 respectively the mggp sf st model was evaluated through nonparametric tests and an examination of the residuals wilcoxon rank sum and levene s tests were used to assesses the differences between the means and variances of the of the observed and predicted parameters respectively at the 95 confidence level and found the model errors to be insignificant the residual analysis confirmed that the relationship between the genes and lognormal parameters is linear and the residuals are independent the mggp sf st model was then validated against an analytical solution and a rwpt based model and the comparisons showed good agreement between all three models the mggp sf st model represents an important contribution as it is relatively easy to implement and provides a computationally efficient tool relative to existing analytical solutions and numerical models this is particularly important as simulations increase in complexity and scale i e from a single fracture to a network credit authorship contribution statement mohamed khafagy conceptualization methodology software formal analysis writing original draft writing review editing wael el dakhakhni conceptualization writing review editing supervision funding acquisition sarah dickson anderson conceptualization writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the financial support for the study was provided through the canrisk create program of the natural science and engineering research council nserc of canada grant number creat 482707 2016 appendix a sudicky and frind 1982 proposed an analytical solution for a constant concentration boundary at the inlet considering matrix diffusion advection and hydrodynamic dispersion in the fracture molecular diffusion into the matrix adsorption onto the walls and within the matrix and first order decay a 1 c c o 2 π 3 2 e x p v z l e x p ξ 2 v 2 z 2 4 ξ 2 r λ z 2 4 d ξ 2 0 ε λ 2 ε 4 4 e x p ε r e x p λ t ε 2 2 s i n ε l t λ c o s ε l t ε 2 2 s i n ω λ c o s ω d ε d ξ where y v 2 k 2 z 2 4 a ξ 2 k 2 4 r d v 2 ω y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε ω ω r z 2 ε 2 8 ξ 2 σ g b b g r m d m a br θ r m d m d α l v d d m τ d v v 2 d ε l ε 2 t 2 y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε ε r y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε t t r z 2 4 d ξ 2 t 0 r 1 k f b r m 1 ρ b θ k m λ l n 2 t 1 2 where z l is the fracture length t t is the time c o m l 3 is the source concentration v l t is groundwater velocity in the fracture b l is half of the aperture width b l is half of the distance between centerline of two fractures d l 2 t is the molecular diffusion coefficient for the solute in water d l 2 t is the hydrodynamic dispersion coefficient along the fracture d m l 2 t is the diffusion coefficient in the porous matrix α l l is the dispersivity τ is the matrix tortuosity θ is the matrix porosity λ t 1 is the first order decay constant t 1 2 t is the half life of the solute r and r m are retardation coefficients in the fracture and matrix respectively based on linear adsorption isotherms as represented by freeze and cherry 1980 k f l is the fracture distribution coefficient ρ b m l 3 is the bulk density of the porous matrix and k m l 3 m is the distribution coefficient of the porous matrix 
3685,in lieu of process based models evolutionary artificial intelligence techniques can yield accurate expressions describing complex phenomena in the current study closed form expressions are developed to predict solute transport in a fracture matrix system as a function of the parameters that describe relevant physical and chemical processes the study adopts a multi gene genetic programming approach to approximate a solution of the classical advection dispersion equation for reactive transport in single parallel plate fractures the approach is employed to obtain an accurate relationship between the hydraulic geological and chemical parameters of the fracture matrix system as inputs and an ensemble of breakthrough curves as outputs solutions generated by the developed model showed good agreement with those of corresponding analytical and numerical models computationally the developed approach is highly efficient particularly when compared with the analytical solution which typically requires relatively fine discretization to calculate the long tailed breakthrough curves therefore future work could extend the developed model to simulate field scale networks and include additional and more complex transport phenomena this approach advances solute transport behavior predictions through being simpler and computationally more efficient than currently adopted techniques which is important as the scale of simulation increases from that of a single fracture to a network keywords closed form solution fractured rock matrix diffusion multi gene genetic programming solute transport 1 introduction improper handling and disposal of hazardous material have led to the contamination of thousands of aquifers globally resulting in extensive dissolved plumes and the diffusion of dissolved compounds into low permeability zones e g rock matrix muskus and falta 2018 once a compound enters a low permeability zone the options for remedial strategies are limited and reverse diffusion results in plume persistence dual porosity models are required to couple the fracture and matrix systems to account for the transfer of groundwater and solute mass between them such models are challenging as one source of porosity i e fracture provides the transmissive path for transport with little storage and the other i e matrix provides most of the storage capacity but its conductivity is minimal trinchero et al 2020 trinchero and iraola 2020 neretnieks 1980 presented an analytical solution for solute transport in dual porosity systems based on similar solutions for heat transport carslaw and jaeger 1959 which considers an infinite rock matrix and neglects longitudinal dispersion in the fracture subsequently the solution was generalized by tang et al 1981 who implemented in plane dispersion and sudicky and frind 1982 who considered a finite rock matrix more recently other researchers cvetkovic 2010 mahmoudzadeh et al 2013 neretnieks 2006 extended these analytical solutions through the incorporation of additional mechanisms and phenomena neretnieks 2006 developed a mathematical model that accounts for diffusion from the flowing zone into a stagnant layer adjacent to the fracture through which solutes can diffuse into the rock matrix in addition they considered reversible diffusion in both the stagnant layer and the matrix they conceptualized the fractures as both tube like and slit like channels and considered the presence of intersecting fractures but they did not consider mixing at fracture intersections these solutions are useful for modelling solute transport in complex three dimensional networks that consist of a large number of fractures with different properties and widely varying flow rates mahmoudzadeh et al 2013 extended neretnieks 2006 work through exploring various contributions of the matrix and stagnant layer and their relative significance on solute transport in fractured rocks their analytical model accounted for diffusion between the fracture and an adjacent matrix composed of different geological layers limited layered matrix through implementing a stepped diffusion process from the flowing water through the stagnant layer and subsequently the adjacent matrix the model can be extended to describe contaminant transport in heterogeneous fractured media consisting of different fracture and matrix characteristics they found that in narrow fractures the stagnant water layer and adjacent rock matrix may lead to considerable retardation their work recommended accounting for both the stagnant layer and at least two matrix layers when assessing the performance and safety of deep geological repositories for spent nuclear fuel as the stagnant water layer allows the solutes to access a larger fracture surface from which to diffuse into the rock furthermore equilibrium between the fracture and matrix was found to be reached more rapidly when the matrix is composed of layers of altered rock and coating as opposed to being intact since the former is more porous moreno and crawford 2009 cvetkovic 2010 conducted short and long term tracer studies at the äspö hard rock laboratory site sweden to examine the impact of heterogeneity of the rim zone altered rock which experienced decreasing porosity they concluded that the macroscopic effects of the rim zone microstructure must be considered in order to extrapolate retention properties to larger scales and longer times neuman 2005 matrix retention is often described using retention time distributions which are in turn derived from the above discussed analytical solutions for transport in a single fracture e g cvetkovic 2010 sudicky frind 1982 tang et al 1981 these retention time distributions are typically precomputed and included in look up tables as these solutions consider additional mechanisms and phenomena under wider ranges of conditions the lookup tables are more computationally intensive due to the large number of values in each table and thus require multivariate interpolation methods it is for these reasons that numerical approaches are considered attractive alternatives to complex analytical solutions for simulating contaminant fate and transport phenomena e g hammond lichtner and rockhold 2011 iraola et al 2019 stein et al 2017 in this respect random walk methods are particularly appealing as they are not vulnerable to numerical dispersion they are able to accommodate complex model parameterizations and they are numerically efficient khafagy et al 2020 noetinger et al 2016 painter et al 2008 some researchers have implemented mass retention in the matrix within random walk simulations using retention time distributions derived from analytical solutions for transport in a single fracture e g cvetkovic 2010 sudicky and frind 1982 tang et al 1981 however process based models of fractured systems i e mass balance advection dispersion chemical reactions are generally computationally intensive moreover dual porosity systems are highly nonlinear due to the difference in time scales between the fracture and matrix as a result numerical simulation of dual porosity systems requires even more extensive time and resources appropriately trained data driven models have recently been proposed as an alternative approach to simulating complex physical and geochemical processes in fractured media esfahani and datta 2016 when validated such models may substantially decrease computational time and improve feasibility and reliability of solute transport simulations such models can generally specify the relationship between input patterns e g mass flux at system boundaries hydraulic geological and chemical properties and output patterns i e solute spatiotemporal concentration variation within the system genetic programming gp techniques inspired by biological evolution were first proposed by koza 1994 compared to other machine learning methods the key advantage of gp lies in its ability to optimize both the variables and constants of the candidate models while being free of the constraints imposed by initial model structure definitions to reach a solution gp initially generates a random population using various functions and terminals assigned by the user at each subsequent generation a new population is created by selecting the best generation defined by the most accurate relationship between the independent and dependent variables heydari et al 2016 sheikh khozani et al 2020 i e best gene gp is a robust method with several advantages over other commonly employed data driven methods e g artificial neural networks hadi and tombul 2018 including 1 generation of explicit expressions or glass box models 2 automatic discovery of model structure utilizing given data 3 adaptive evolutionary ability to generate global solutions without becoming trapped in local optima and 4 not requiring any specific prior domain knowledge as such gp has been applied widely in water resource related research including hydrogeologic aryafar et al 2019 cianflone et al 2017 esfahani and datta 2016 sadat noori et al 2020 river stage ghorbani et al 2018 hadi and tombul 2018 mehr and gandomi 2021 real time wave forecasting kambekar and deo 2014 water quality jamei et al 2020 and rainfall runoff modelling chadalawada et al 2020 heřmanovský et al 2017 multi gene genetic programming mggp advances gp through linearly combining low depth gp blocks to improve the accuracy of solutions evolved by single gene gp additionally mggp develops less complex models than single gene gp as it uses fewer functions gandomi et al 2015 although mggp is being adopted its use in water resource applications is not yet ubiquitous a recent review of gp applications in hydrology found that the mggp variant was used in nine of 142 papers danandeh mehr et al 2018 the purpose of this study is to provide an accurate closed form data driven expression describing reactive solute transport in single saturated fractures with matrix diffusion that is substantially more computationally efficiency than current methods i e numerical and analytical approaches mggp is employed to obtain an accurate relationship between the coefficients of the lognormal distributions obtained from an ensemble of breakthrough curves btc as outputs i e dependent variable and the hydraulic geological and chemical parameters of the fracture matrix system as inputs i e independent variable this more computationally efficient model will facilitate the simulation of complex network scale problems 2 model development a dataset was generated containing an ensemble of btcs using an analytical solution developed by sudicky and frind 1982 based on a set of input parameters that span a specified range for the hydraulic geological and chemical properties subsequently mggp is used to generate expressions that accurately describe the relationship between the inputs i e hydraulic geological and chemical properties and the outputs i e parameters of the lognormal distribution describing the btc ensemble the analytical solution sudicky and frind 1982 simulates solute transport in one dimensional fractures with a constant concentration at the inlet boundary the solution considers advection molecular diffusion and mechanical dispersion within the fracture lateral matrix diffusion adsorption within the matrix and on the fracture wall and radioactive decay and provides the btcs at a specified distance from the injection source see appendix a for further details 2 1 dataset the dataset consists of the btcs calculated using eq a 1 with the parameter values listed in table 1 which provide a possible 352 800 combinations of hydraulic geological and chemical properties the ranges of these properties are selected to represent the values most commonly observed in recent literature as referenced in table 1 the integrals in eq a 1 were solved numerically using a scanning procedure to determine the numerically significant ranges of ε and ξ subsequently the midpoint rule was employed to discretize the integration function into 50 000 and 1 000 connected rectangles for ε and ξ respectively within the significant range this approach is expected to result in more efficient integration with minimal loss in accuracy as each integral is only discretized within the significant range however even with this discretization procedure it would take many years to conduct all of these simulations on a typical pc as such the shared hierarchical academic research computing network sharcnet was used the input file for the script contains approximately 350 sub input files which are distributed amongst three computing clusters to generate the btcs via parallel processing where l l is the fracture length v l t is groundwater velocity in the fracture d l 2 t is the molecular diffusion coefficient for the solute in water 2 b l is the aperture width 2 b l is the distance between centerline of two fractures α l is the dispersivity and θ is the matrix porosity a lognormal distribution was fit to each btc using non linear least squares regression and the best fitting parameters were calculated by minimizing the root mean square error rmse between the analytical solution btc and the corresponding lognormal distribution as follows 1 rmse i 1 n c 1 i c 2 i 2 n where c 1 i and c 2 i are the solute concentrations calculated at the fracture outlet at a specified time i obtained from the analytical solution and the appropriate lognormal distribution respectively and n is the number of concentration points on the curve used in this analysis matrix retardation is implemented in the pdf and cdf of the lognormal distribution by dividing the time t by the retardation coefficient in the matrix r m khafagy et al 2020 zhang et al 2012 as follows pdf 2 1 t r m σ s 2 π e x p l n t r m μ 2 2 σ 2 cdf 3 1 2 1 2 e r f ln t r m μ 2 σ where μ t is the mean σ t is the standard deviation and t t is the solute arrival time at the outlet the matrix tortuosity τ is accounted for in the matrix diffusion coefficient d m which is obtained by multiplying the molecular diffusion coefficient d by τ 2 2 multi gene genetic programming evolutionary algorithms eas are capable of approximately simulating complex models effectively using stochastic search methods gp models are a class of eas considers nodes which are elements composed from either a function tree or terminal leaf set function sets may include arithmetic operators or mathematical functions sin cos tanh ln etc boolean operators and or not etc logical expressions if or then or any other suitable functions defined by the user whereas terminal sets include variables constants or both fig 1 the gp tree is randomly formed from chosen functions and terminals the root node is functional and has one or more branches extending from it that end in terminal nodes initially a set of gp trees are randomly generated using functions and terminals determined by the user the number of gp trees initially generated forms the initial population size initial gp gene and is based on the maximum allowable number of genes as defined by the user mggp models provide solutions through a linear combination of the individual genes sub genes a sub gene is a mathematical solution linking some or all of the input and output parameters and each sub gene forms a part of the solution for its generation fig 1 the multi gene model shown in fig 1 predicts an output variable y using input variables x 1 x 2 and x 3 this model structure contains non linear terms i e sine and square root but is linear in terms of the coefficients c 0 c 1 and c 2 the linear combination of these genes is referred to as a multi gene the linear coefficients weights of genes and the model bias are obtained from the training data using ordinary least squares regression the initial population of the mggp model is generated from individuals that contain randomly evolved genes and similar to the standard gp model the mggp population is subjected to evolution mechanisms i e reproduction crossover mutation and architecture altering operations to generate an enhanced generation the evolved improved population replaces the existing population these evolutionary mechanisms mimic biological evolution natural selection processes reflecting survival of the fittest however some special crossovers mechanisms in addition to the standard evolution mechanisms are introduced to allow exchange of genes mutation between individuals the mutation operation provides six methods for mutation between genes 1 sub tree mutation 2 mutation of constants using an additive gaussian perturbation 3 substitution of a randomly selected input node with another randomly selected input node 4 set a randomly selected constant to zero 5 substitute a randomly selected constant with another randomly generated constant and 6 set a randomly selected constant to one the evolutionary mechanisms are grouped into categories referred to as events and set by the user in the form of probabilities such that the sum of reproduction crossover and mutation probabilities is 1 the reader is referred to searson et al 2010 for a more detailed explanation of mggp and the evolutionary mechanisms the fitness function describes the accuracy between the individual input and the target output and is evaluated by an objective function that is pre defined by the user e g r2 rmse gp continues to produce new generations until it reaches a specified termination criterion based on either the desired fitness function or a maximum number of generations as defined by the user it is important to consider the trade off between accuracy and complexity in mggp model development which are influenced by the maximum depth of the gp tree that reflects the number of nodes and the maximum allowable number of genes as specified by the user respectively fig 2 shows the flowchart of the mggp method for obtaining the best fit model two mggp models were developed in this work to establish closed form expressions reflecting the relationships between the hydraulic geological and chemical parameters i e input variables and lognormal distribution parameters i e mean μ o and standard deviation σ o as output variables in this study the gptips toolbox searson et al 2010 for matlab version 2021a was used to develop the mggp models based on the parameters and settings in table 2 the data were classified into training 80 validation 10 and testing 10 sets to avoid overfitting k fold cross validation was conducted using the training and validation datasets with 500 realizations to ensure model robustness in each of the 500 realizations the training and validation sets i e 90 of the dataset are divided into nine folds such that eight folds i e 80 of the dataset are used to train the mggp model and one fold i e 10 of the dataset is used to validate the trained model the fitness of each model was determined based on the minimized objective function which was the rmse between the predicted and actual values of the lognormal distribution parameters with a termination value of 0 0002 if the objective function did not reach the termination value the optimal model was achieved when the mggp procedure reached 1000 generations table 2 the best model from the 500 realizations was selected based on the lowest rmse this process was repeated until either the fitness termination value 0 0002 or the maximum number of generations 1000 was reached the testing dataset was then used to select a single optimal mggp model from the best models representing each generation to minimize individual effects of random data assignment to folds and to ensure the model was not overfit this optimal mggp model is hereafter referred to as the single fracture solute transport mggp sf st model it must be noted that increasing the mggp model accuracy by specifying the population size and maximum number of generations leads to increasing model complexity as such there is a trade off between model accuracy and complexity the latter of which is controlled by specifying the maximum number of genes and the tree depth this trade off was achieved herein by limiting both the maximum number of generations and population size to 1000 for accuracy and the maximum tree depth and maximum number of genes to five and 10 respectively to minimize model complexity the closed form expressions generated using mggp will develop μ p and σ p that can be implemented in eqs 2 and 3 to calculate the btcs for pulse and constant solute injections respectively a sensitivity analysis was conducted using variance based global sensitivity analysis to identify the level of influence of each input variable on the model by calculating their total sensitivity indices the indices are calculated by generating a random value uniformly distributed for each variable within the specified range in the model and considering the contribution of the other variables on the simulation ten thousand samples are generated for each variable using a quasi random monte carlo simulation to calculate the total sensitivity indices the mggp sf st model was verified against both the analytical solution eq a 1 and the random walk particle tracking rwpt model described by khafagy et al 2020 with an instantaneous release of 10 000 particles at the upstream end of the fracture the hydraulic geological and chemical properties used in the validation process are l 30 m v 1 m day d 1 10 9 m2 s 2 b 80 µm 2 b 1 m α 1 m θ 0 1 τ 1 r 1 and r m 1 2 3 where r is the retardation coefficient in the fracture 3 results and discussion the final dataset contained 101 740 btcs of the potential 352 800 parameter combinations while some of the btcs generated required more refined discretization of the analytical solution eq a 1 for the parameter combinations they represent the computational time required for all 352 800 possible parameter combinations exceeded the computational time available through sharcnet thus the remaining 251 060 btcs were not generated nevertheless the 101 740 btcs that form the dataset are based on the full range of all parameters listed in table 2 and thus the volume and range of data generated are sufficient to implement mggp in parallel to the dataset generation it was important to determine which combinations of parameters required further discretization as such the parameter combinations for the btcs in the final dataset were visualized through a network graph i e nodes and edges using the network analysis software gephi version 0 9 2 fig 3 the nodes are identified as the hydraulic geological and chemical property values and the edges are connections between the nodes that form a unique set of parameter combinations representing a btc the line edge thickness between each two nodes in fig 3 is based on the weighted adjacency and represents the number of btcs generated based on combinations of the two nodes for example the properties 2 b and l 1000 m have a thinner edge than 2 b and l 1 m fig 3 indicating fewer btcs were generated using the former combination the nodes are sized based on their degree centrality which reflects the number of connections to other nodes properties with similar weighted adjacency and degree centrality for all values investigated were combined into a single node i e 2 b α and θ note that fig 3 shows only the links that all properties share with 2 b α and θ to maintain readability of the figure fig 3 shows that the weighted adjacency i e number of btcs generated decreases with increasing velocity increasing fracture length decreasing molecular diffusion coefficient and increasing aperture size this is because the ε and ξ curves appendix a which describe the integrals in eq a 1 tend to be more complex within these parameter ranges this is demonstrated in fig 4 which shows the concentration vs log ε curves for two select cases from the network analysis fig 3 each discretized into 50 000 points a log scale was chosen because the discretization is not the same among curves it decreases with increasing ε the parameters in case a were selected to reflect a combination with a low weighted adjacency indicating that finer discretization is typically required to generate an accurate ε curve for this parameter combination l 0 5 m v 0 1 m day d 1 10 6 m2 s 2 b 0 001 µm 2 b 1 m α 10 m θ 0 1 τ 1 r 1 r m 1 these parameter combinations also typically resulted in long tailed breakthrough curves generally due to a very small diffusion coefficient 1 10 10 m2 s with a corresponding high velocity 10 m day these results are in agreement with those obtained by wang et al 2018 who concluded that the standard advection dispersion model cannot be used to characterize the late time tailing of tracer btcs likely due to solute retention mechanisms e g matrix diffusion sorption the parameters in case b were selected to reflect a high weighted adjacency indicating that the discretization applied normally generated an accurate ε curve for this parameter combination l 50 m v 5 m day d 1 10 9 m2 s 2 b 10 µm 2 b 1 m α 1 m θ 0 1 τ 1 r 1 r m 1 as a result case b was generated for the dataset but case a was not as there were not sufficient computing resources to implement the additional discretization required for all similar cases in the dataset the rmse and r2 for the lognormal distribution describing each btc generated for the dataset i e 101 740 btcs exhibit excellent accuracy compared to the analytical solution the median and highest rmse are approximately 0 0051 and 0 023 respectively and the median and lowest r2 values are approximately 0 9995 and 0 9932 respectively fig 5 3 1 mggp model structure and evaluation mggp was employed to develop a relationship considering various combinations of fracture properties i e l v d 2 b 2 b α and θ and the parameters describing the lognormal distribution of the generated dataset i e μ o and σ o the best set of variable combinations represented by x 1 to x 7 was selected considering the most accurate relationship between those combinations and the parameters describing the lognormal distribution of the generated dataset i e μ o and σ o this relationship hereafter referred to as the mggp sf st model is described by the expressions for μ p and σ p in eqs 4 and 5 4 μ p 0 0431 x 4 0 999 x 2 0 224 x 1 0 542 e x 7 x 1 x 3 0 267 e e x 1 0 0431 e x 3 2 0 267 e x 7 0 903 x 3 x 4 e e x 4 8 81 1 0 6 x 5 x 1 2 0 812 x 3 e e x 4 0 143 e x 1 x 3 2 x 1 2 x 3 x 3 2 0 174 x 7 x 1 2 x 1 2 x 3 2 0 0862 x 1 0 402 3 x 1 e x 1 e x 4 1 94 5 σ p 1 16 e x 3 0 5 x 4 1 01 e x 1 x 3 0 947 e e x 4 0 0573 x 3 l n x 3 0 00105 x 1 e x 3 0 00209 x 4 e x 3 1 31 x 3 e e x 1 3 07 e x 1 3 91 e x 1 1 4 3 84 1 0 6 x 4 x 1 x 6 x 4 x 1 x 3 2 3 35 1 0 6 x 1 x 3 x 1 x 3 x 3 e x 3 1 05 where x 1 v 2 b 2 b l d θ x 2 l n v 2 b l 2 b θ x 3 α l x 4 d v l x 5 l n l x 6 d x 7 2 b where v is in meters year l is in meters d is in meters2 year 2 b is in meters 2 b is in meters α is in meters the effluent btc represents normalized concentration versus years and is obtained by substituting eqs 4 and 5 into either eqs 2 or 3 for pulse or constant injections respectively note that the mggp technique is empirical and thus the resulting mggp sf st model is not dimensionally consistent attention must thus be paid to utilize the correct input and output units e g hadi and tombul 2018 jamei et al 2020 yan et al 2021 it is also worth noting the physical meanings of some variable combinations specifically x 1 represents a modified péclet number p e for the matrix x 3 represents the unique correlation between dispersivity and fracture length that has been observed in lab experiments e g zech et al 2015 and x 4 is the reciprocal of p e in the fracture figs 6 and 7 show the mggp tree structures of each gene in the generated expressions for the mean and standard deviation respectively each expression was developed from seven input variables i e x 1 x 2 x 3 x 4 x 5 x 6 x 7 and has ten genes and a bias as shown in eq 4 mean and eq 5 standard deviation table 3 shows the pearson correlation coefficients r between the input variables x 1 to x 7 and the coefficients of the lognormal distributions from the dataset i e μ o and σ o the results show that these relationships are highly non linear indicating that the model complexity offered by mggp is required the best fitness rmse values are plotted against mggp generation for the expressions representing μ p and σ p fig 8 this analysis shows that the model accuracy is not significantly improved beyond 290 and 220 generations for expressions representing μ p and σ p respectively the selected models representing μ p and σ p have rmses of 0 0493 and 0 0425 respectively which correspond to generations 870 and 842 respectively while the selected model accuracies are slightly lower than those achieved by models with a larger number of generations they were chosen to balance accuracy with complexity fig 9 compares the btcs predicted by the mggp sf st model eqs 4 and 5 to the observed best fitting lognormal btcs for all 101 740 datapoints in the dataset the observed and predicted μ and σ for the entire dataset achieved r2 0 9998 and r2 0 9974 respectively note that of the 101 740 data points only 18 847 18 have a σ greater than two fig 9b shows that of the btcs with σ greater than two the mggp sf st model both over and under estimates σ values values of σ greater than two are associated with long tailed btcs for which the analytical solution typically requires additional discretization to achieve sufficient accuracy in this dataset curves with a large dispersion i e 2 represent a small fraction of the total number of curves 18 fig 10 shows the specific btcs associated with points a b and c in fig 9 to demonstrate the model sensitivity to the range of standard deviation residuals the comparison between the observed and predicted btcs shows very good agreement even for the cases in which σ is overestimated or underestimated i e cases a and c respectively the mggp sf st model is assessed using non parametric statistical tests at a 95 confidence level to compare the mean and variance of μ p and σ p with that of μ o and σ o specifically the wilcoxon rank sum method bickel and lehmann 1975 conover and iman 1980 evaluates the difference between the means of the observed and predicted parameters whereas levene s test levene et al 1960 evaluates the difference between the variances of the observed and predicted parameters the complete dataset training validation and testing sets is used for these statistical tests with respect to the difference between the observed and predicted means the p values at the 95 confidence level are above 0 05 table 4 indicating that the model error is insignificant the calculated p values for difference between the variances are also above 0 05 at the 95 confidence level table 4 indicating that the mggp sf st model variability is not significantly different from the observed data thus the mggp sf st model is able to predict the observed data at a 95 confidence level a residual analysis was performed to ensure that the relationship between the genes and lognormal parameters is linear i e the mggp sf st model is linear and the residuals are independent fig 11 the box and whisker plots showing residuals from the training and validation datasets and testing dataset show that they are all mostly clustered around zero and are approximately normally distributed indicating that the model is independent of the residuals and therefore valid the box and whisker plots also show that μ residuals from the testing dataset have a slightly higher standard deviation and median e g standard deviation 0 1044 median 0 0303 than those from the training and validation datasets e g standard deviation 0 0542 median 0 005 however these parameters are within an acceptable range for all datasets fig 12 illustrates the influence of each input variable on the mggp sf st model using the variance based global sensitivity analysis the results show that x 1 x 2 and x 4 have the greatest impact on the μ expression while x 1 and x 3 have the greatest impact on the σ expression 3 2 mggp model verification the comparisons between the mggp sf st model analytical solution and rwpt simulations show excellent agreement with the mggp sf st model predicting slightly later arrival times 1 3 later on average based on t 50 than the analytical solution fig 13 the mggp sf st demonstrated excellent computational efficiency it can predict the btc for any combination of fracture properties within the developed model parameter range in less than one second through matlab version 2021a in comparison the analytical solution and rwpt based models compute the same btcs in the order of hours in part due to the fact that the analytical solution typically requires more refined discretization to calculate the long tailed btcs this efficiency becomes extremely important as the scale of a problem increases from that of a single fracture to a network as it will be magnified 4 conclusion mggp was employed to develop a closed form expression mggp sf st model to simulate reactive transport in a single parallel plate fracture under a range of hydraulic geological and chemical conditions for either a constant concentration or pulse injection at the inlet the model was developed based on a large dataset of btcs 101 740 datapoints generated from a published analytical solution for solute transport in a single fracture under a range of conditions i e velocity dispersion along the fracture molecular diffusion within the fracture and into the matrix and adsorption within the matrix a network graph was created to visualize the combinations of parameters for which btcs were generated in the dataset this analysis showed that the analytical solution for certain parameter combinations typically those that result in long tailed btcs require more refined discretization which facilitated the generation of btc for the full range of parameter combinations the dataset was split into training validation and testing datasets and mggp was employed to develop the model due to the nonlinear relationship between the model inputs x 1 to x 7 and outputs i e μ o and σ o a lognormal distribution was fit to each curve in the dataset best fitting curves and the parameters i e μ o and σ o were compared to those predicted by the mggp sf st model i e μ p and σ p the plot of the observed versus predicted μ and σ for the entire dataset fig 9 achieved r2 values of 0 9998 and 0 9974 respectively the mggp sf st model was evaluated through nonparametric tests and an examination of the residuals wilcoxon rank sum and levene s tests were used to assesses the differences between the means and variances of the of the observed and predicted parameters respectively at the 95 confidence level and found the model errors to be insignificant the residual analysis confirmed that the relationship between the genes and lognormal parameters is linear and the residuals are independent the mggp sf st model was then validated against an analytical solution and a rwpt based model and the comparisons showed good agreement between all three models the mggp sf st model represents an important contribution as it is relatively easy to implement and provides a computationally efficient tool relative to existing analytical solutions and numerical models this is particularly important as simulations increase in complexity and scale i e from a single fracture to a network credit authorship contribution statement mohamed khafagy conceptualization methodology software formal analysis writing original draft writing review editing wael el dakhakhni conceptualization writing review editing supervision funding acquisition sarah dickson anderson conceptualization writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the financial support for the study was provided through the canrisk create program of the natural science and engineering research council nserc of canada grant number creat 482707 2016 appendix a sudicky and frind 1982 proposed an analytical solution for a constant concentration boundary at the inlet considering matrix diffusion advection and hydrodynamic dispersion in the fracture molecular diffusion into the matrix adsorption onto the walls and within the matrix and first order decay a 1 c c o 2 π 3 2 e x p v z l e x p ξ 2 v 2 z 2 4 ξ 2 r λ z 2 4 d ξ 2 0 ε λ 2 ε 4 4 e x p ε r e x p λ t ε 2 2 s i n ε l t λ c o s ε l t ε 2 2 s i n ω λ c o s ω d ε d ξ where y v 2 k 2 z 2 4 a ξ 2 k 2 4 r d v 2 ω y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε ω ω r z 2 ε 2 8 ξ 2 σ g b b g r m d m a br θ r m d m d α l v d d m τ d v v 2 d ε l ε 2 t 2 y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε ε r y ε 2 sinh σ ε sin σ ε cosh σ ε cos σ ε t t r z 2 4 d ξ 2 t 0 r 1 k f b r m 1 ρ b θ k m λ l n 2 t 1 2 where z l is the fracture length t t is the time c o m l 3 is the source concentration v l t is groundwater velocity in the fracture b l is half of the aperture width b l is half of the distance between centerline of two fractures d l 2 t is the molecular diffusion coefficient for the solute in water d l 2 t is the hydrodynamic dispersion coefficient along the fracture d m l 2 t is the diffusion coefficient in the porous matrix α l l is the dispersivity τ is the matrix tortuosity θ is the matrix porosity λ t 1 is the first order decay constant t 1 2 t is the half life of the solute r and r m are retardation coefficients in the fracture and matrix respectively based on linear adsorption isotherms as represented by freeze and cherry 1980 k f l is the fracture distribution coefficient ρ b m l 3 is the bulk density of the porous matrix and k m l 3 m is the distribution coefficient of the porous matrix 
3686,empirical approaches such as the height above nearest drainage method in conjunction with synthetic rating curves hand src have emerged as particularly appealing alternatives to the traditional flood mapping techniques due to their lower complexity and fewer data requirements however srcs use digital elevation model dem derived reach averaged hydraulic properties and assume one dimensional steady state flow condition with normal depth these implicit model assumptions may introduce errors in flood stage and extent estimates using the hand src approach this study investigates the reliability of src across continental united states conus by comparing them to the united states geological survey s usgs gauge rating curves results from this comparison show that the implicit model assumptions used in the src hand approach add significant error in the src derivation the accuracy of the src is found to be related to the stream characteristics including the bathymetry area slope of the main channel two year flow and drainage area results also show that srcs in coastal areas characterized by low slopes and large drainage areas have higher error and tend to overpredict the stage height in comparison to the usgs rating curves whereas they tend to underpredict stage height in the mountainous regions the srcs are most reliable for the midwestern plains of ohio mid atlantic tennessee and upper mississippi regions and least reliable higher error for the rocky mountains further the study finds that deep neural network models can be effectively used to judge the performance of src for ungauged river reaches keywords synthetic rating curve hand flood mapping deep learning application continental scale 1 introduction flooding is one of the most threatening natural hazards in terms of mortal as well as economic losses worldwide these losses continue to increase with the changing climate growing population and rapidly urbanizing environment flood losses account for about 6 billion per year in the united states alone over the past 30 years noaa 2016 and are projected to increase to 52 billion per year worldwide by 2050 hallegatte et al 2013 large scale accurate flood maps spanning an entire continent have the potential to significantly improve our flood preparedness and risk management in an attempt to improve nationwide resilience and mitigation of flood risks the national oceanic and atmospheric administration s noaa s national water model nwm in the united states forecasts streamflow for 2 7 million river reaches spread across the 48 contiguous states and the district of columbia conus however creating detailed flood inundation maps across the conus from nwm forecasted streamflow remains a challenge johnson and cole 2017 traditionally flood maps were created using hydrodynamic models for individual streams or rivers for a city or county due to high computational demands to execute these models at river network scale and availability of accurate floodplain topographic data at such scales most traditional hydrodynamic models are based on solving one or two dimensional saint venant equations shallow water equations for example hydrologic engineering center s river analysis system hec ras us army corps of hydraulic engineers 2002 can be used to produce detailed flood maps but their applicability to map real time flood events at continental scale is restricted by their complexity and high computational and data requirements fekete vörösmarty 2007 simple empirical approaches with lower computational and data requirements are a particularly appealing alternative for continental scale flood inundation mapping cfim one such empirical approach is the height above nearest drainage hand method in conjunction with synthetic rating curves hand src zheng et al 2018 which can be used to produce flood inundation maps across large spatial domains efficiently hand rennó et al 2008 rodda 2005 is defined as the height of each grid cell digital elevation model dem grid above the nearest stream cell it drains to it has been effectively used to determine soil water potential nobre et al 2011 ground water potential rahmati et al 2018 and for flood inundation mapping nobre et al 2016 for a given stage height at a stream cell any grid cell draining to that stream cell with a hand value less than the stage is flooded and the difference between stage height and hand defines the inundation depth the nwm only produces streamflow and to use them for real time flood inundation maps they need to be converted to a stage height at a stream cell to deal with this shortcoming zheng et al 2018 developed synthetic rating curves src as a viable alternative to actual rating curves in the hand src framework these srcs convert the discharge into stage height and then hand converts the stage height into flood inundation the srcs that lie at the heart of this hand src approach are based on using reach averaged channel properties in manning s equation to relate water depth and discharge for any given water depth spanning over a river reach meaning same water depth in the entire reach the inundation zone marked by all the grid cells with a hand value less than the water level is used to estimate the total surface area volume and total bed area of the inundation zone a reach average of the total surface area volume and bed area subsequently results in averaged channel width cross sectional area and wetted perimeter of the channel reach respectively furthermore zheng et al 2018 assumes normal depth along all reaches and equates friction slope to the reach averaged channel slope by using a constant manning s n for all the reaches across conus these assumptions of reach averaged channel properties normal depth and constant manning s n are not always valid and act as potential sources of error the src s which derive their channel properties from a dem based hand raster also inherit the errors associated with the dem including the error due to absence of bathymetry river bed information in a dem the importance of bathymetry in hydrodynamic models has been well quantified by previous studies dey 2016 dey et al 2019 grimaldi et al 2018 neal et al 2015 trigg et al 2009 a significant amount of work is already being done to deal with the dem based errors particularly in the surface water and ocean topography swot mission biancamaria et al 2016 jung et al 2010 yoon et al 2012 therefore there is a need to analyze the residual errors in the srcs after removing the errors due to inaccurate bathymetry representation previously garousi nejad et al 2019 evaluated the reliability of hand src based flood inundation maps by comparing them against high resolution satellite imagery based maps they found that reliability of the hand src maps can be improved by using high resolution hydrography water drainage network and dem they also suggested an approach to obtain reach specific manning s n but their work focused on dem based improvements and did not tackle other errors due to implicit assumptions in the src johnson et al 2019 compared the national water model height above nearest neighbor nwm hand generated flood maps with remotely sensed flood maps for 54 catchments the study concluded that there is a tendency for the nwm hand to underpredict inundated cells for lower order streams and that most of the error in the approach can be associated with error in the src but the study did not focus on analyzing the errors in srcs scriven et al 2021 proposed that combining minimum manning s n derived src and median manning s n derived src for each reach improves the src s reliability and also allows for a prediction window to accommodate the uncertainty in choosing the roughness parameter however the study just focused on tackling the uncertainty due to manning s n and does not consider the error due to missing bathymetry and other implicit assumptions godbout et al 2019 focused on errors in src and found that reach characteristics like lower gradient and shorter length can be associated with poor performing srcs but their study was limited to a few rivers in texas and compared srcs to hec ras rating curves thus their findings cannot be generalized for all the reaches across the conus the errors in srcs can be divided into types one due to topography and the other due to implicit assumptions the error due to topography may be reduced in the near future by using high resolution both horizontal and vertical resolution dems and improving bathymetric representation example lidar dem with swot derived bathymetry this study attempts to address the error due to implicit assumptions such as using reach averaged channel properties a constant channel roughness and uniform flow specifically this study attempts to quantify the src errors by using rating curves at gauged locations and develop a methodology to estimate these errors at ungauged stream reaches accordingly the objectives of this study are i devise a simple method to remove the errors due to inaccurate bathymetry representation ii estimate the error due to the implicit assumptions in srcs at gauged locations across the conus iii relate the error in srcs with watershed and stream characteristics and iv propose a method for estimating the performance of srcs in ungauged reaches these objectives are accomplished by using rating curves for 6189 united states geological survey usgs streamflow gauging stations river and watershed characteristics are derived by using publicly available data related to topography hydrography and land use the relationship between src and stream or watershed characteristics is developed by using a deep neural network more details about the data and methodology is presented in the following sections 2 study area and data the study focuses on the comparison of measured rating curves and srcs across the conus the usgs maintains a network of 10 606 streamflow gauging stations within the conus but rating curve is not available for all these stations thus this study uses data from only 6189 stations shown in fig 1 for which rating curves are available rating curve data for the selected stations are downloaded from the national water information system nwis table 1 the stage associated with each rating curve is converted to water surface elevation by adding the gauge datum associated with each station similarly srcs provide stage relative to the surface elevation derived from 10 m national elevation dataset ned the src s stage is converted to water surface elevation by adding the surface elevation from ned of the point on national hydrography dataset nhd streamline uepa et al 2012 nearest to the gauge this ensures that both sets of rating curves are now represented by water surface elevation wse and therefore can be compared consistently the relationship between src and reach watershed characteristics is developed by using eight geographic terrain and channel characteristics for each usgs gauging station these include latitude longitude surface elevation total drainage area at the gauging station two year flow slope of river reach percent impervious cover in the huc 12 watershed draining to the gauge location and bathymetric area the data used to derive these characteristics are provided in table 1 3 methodology this study analyzes the accuracy of srcs with respect to usgs gauge rating curves the src gives a reach average relationship between discharge and stage while the usgs rating curves are valid only for a particular point in order to compare the two each usgs station is associated to the nearest national hydrography dataset nhd reach medium resolution and it is assumed that the src related to that reach would be valid for that particular gauging station the error due to bathymetry is first removed by comparing the water surface elevation level produced from the two rating curves and subsequently producing area corrected curves the error in these corrected rating curves is then quantified using appropriate metrics next the error correlation with different watershed and stream characteristics is investigated and the reliability of hand src method is mapped across the conus finally a deep neural network dnn model is developed for evaluating the reliability of src at ungauged reaches the dnn model learns the complex correlation between key watershed and stream characteristics and the src error and thereafter it can be used to predict the performance of src for any stream the methodology for each of these procedures is described below 3 1 removing error due to bathymetry srcs are computed by using reach averaged values of hydraulic properties such as manning s n cross section channel slope and wetted perimeter in the manning s equation by assuming these properties to be constant across the entire river reach zheng et al 2018 these hydraulic properties of srcs are based on hand raster which is derived from ned dem the lack of bathymetric data in the ned dem also contributes to some error in src values in addition to the error due to the implicit assumptions the src represents discharge versus water depth only above the base water surface the wse of the river represented in the dem during it formation as they lack bathymetric information and it would be inaccurate to compare them directly to usgs rating curves thus to compare the two curves one needs to account for the bathymetric area a0 and corresponding discharge q0 by using the finding from the theoretical exploration fig a1 and table a1 appendix a that low flow estimates are impacted by missing bathymetry a simple approach is used to account for the bathymetric area specifically to account for missing bathymetry area src is shifted to match the lowest discharge record for that particular reach fig 2 b it is assumed that for this low water depth above dem the error in src would be mostly due to the topographic error and the implicit errors would be negligible so if δy δy 0 fig 2 a is the water depth above riverbed from dem and y is the elevation of riverbed of dem then the discharge at wse with respect to actual riverbed of y δy for the src should be same as the discharge from gauge rc for wse of y δy fig 2 b the difference in these discharges obtained from src and gauge rc is then assumed to be missing discharge in src due to bathymetry and can be termed as discharge correction q0 fig 2 assuming the same hydraulic properties of src are valid for the bathymetric section of the channel as well the bathymetric area correction a0 is calculated for discharge q0 now if q is the discharge and a is the cross sectional area corresponding to any wse y1 for the src the area corrected srcs should have a cross sectional area of a a0 corresponding to this y1 using the same hydraulic properties of src discharge as q1 calculated for this new a a0 cross sectional area and this q1 and y1 form the discharge wse pair for the area corrected src all the further analysis is done using these area corrected src 3 2 metrics for error analysis as there is a lack of an established method to compare the usgs rating curves with the src the performance srcs can be assessed using several statistical techniques namely the mean absolute bias mab root mean square error rmse r2 or coefficient of determination normalized root mean square error range percent bias mean absolute error mae in this study a modified version of rmse herein the rmse value is multiplied with the sign of the bias is used rmse ensures that for any particular station one takes the absolute difference between gauge rc stage height and src stage height and simultaneously does not allow the overpredictions and underpredictions to cancel each other out however rmse alone is not able to describe if overall the src overpredicts or underpredicts the gauge rc s stage height this is addressed by the modified metric as the magnitude of the metric gives us the absolute value of the rmse while the negative positive sign indicates underprediction overprediction the study also uses the normalized rmse nrmse which is defined as rmse normalized by the range of gauge rc stage height nrmse is not affected by the size of the river and can be used for comparing rivers of different scales godbout et al 2019 the nrmse is also multiplied with the sign of bias rmse between usgs rating curve and src at a usgs gauging station is computed as 3 1 rmse b 1 n y b y b 2 n where the stage height from src y b and the stage height from usgs rating curve y b are evaluated for a set of return period discharges here n represents the number of discharge stage height pairs for which the src and the usgs rating curve are compared for a given usgs station 3 3 extracting watershed and stream characteristics to analyze the performance of src across the conus a total of eight geographic terrain and channel characteristics for each usgs gauging station are chosen these include the latitude longitude surface elevation total drainage area two year flow percent impervious cover at the gauging station location slope of river reach and bathymetric area are used the study used the bathymetric area correction a0 calculated above as an estimate of the bathymetric area the latitude and longitude here represent the geographic characteristics whereas the surface elevation and slope of river reach represent the topographic characteristics associated with each station the latitude and longitude values are obtained from usgs gauge information and surface elevation is the dem value associated with each station the slope of river reach which is obtained from the attribute table of the nhdplus mr dataset also crudely represents the riverbed characteristics and flow velocities impervious cover is included as a parameter to account for the urban cover of the watershed draining at the gauging station each station is associated with it its respective huc 12 subwatershed the huc 12 unit subwatersheds are derived from the watershed boundary dataset wbd the average size of huc 12 units is 103 km2 the percentage impervious cover is then calculated for these huc 12 units using the nlcd 2016 percent developed imperviousness which is then assigned as percent impervious cover for its associated gauging station total drainage area and two year flow represent the size of the stream associated with each station however two year flow provides additional information as it also represents if the stream is perennial intermittent or ephemeral two year flow cannot replace the total drainage area as for example an ephemeral stream with a significant drainage area may have a lower two year flow total drainage area values are obtained from the attribute table of nhdplus mr dataset the two year flow is calculated using the usgs daily streamflow readings average precipitation soil porosity soil depth precipitation frequency permeability average temperature in entire watershed are a few other characteristics that may have been included in the analysis but jafarzadegan moradkhani 2020 found them to be non significant and are thus not included in this study 3 4 deep neural network model one of the key objectives of the study is to analyze the reliability of src to create flood inundation maps especially at ungauged reaches in this section a machine learning model is developed for correlating the error in src to reach and watershed characteristics after training and validating this model for gauged reaches it can potentially be implemented for estimating the reliability of src at ungauged reaches across the conus knowledge of absolute error in srcs represented by rmse is more valuable for their use for emergency responses nrmse on the other hand is good for the comparison of streams of different scales in this study the performance of an src at a station is classified as satisfactory or unsatisfactory based on the absolute rmse value the stations with rmse between 0 and 0 61 m 2 ft are labeled as satisfactory category 0 and those with rmse 0 91 m 3 ft category 2 are labeled as unsatisfactory performing while the stations in between are termed as average performing category 1 the relationship between watershed and stream characteristics and the error in src is complex which cannot be captured using a simple regression model a deep neural network dnn is better suited to learn the complex relationship between predictors and outputs goodfellow et al 2016 lecun et al 2015 learning this complex relationship is particularly useful because using a dnn model one can predict the reliability of src at ungauged river reaches using just their watershed and stream characteristics a dnn classifier model is developed to classify the src as satisfactory or unsatisfactory performing based on the characteristic parameters the model takes the above normalized eight predictor fig 3 as input and produces the best fit category 0 1 or 2 for each station fig 3 fig 3 also presents the structure of the neural network used the forward pass of the neural network is defined by the following equations 3 2 x 1 r e l u a 4000 i b 4000 3 3 x 2 d b n 4000 x 1 3 4 x 3 r e l u a 2000 x 2 b 2000 3 5 x 4 d b n 2000 x 3 3 6 x 5 a o u t s i z e x 4 b o u t s i z e 3 7 y l o g e x 5 j e x 5 j 3 8 r e l u x m a x 0 x x r 3 9 b a t c h n o r m 1 d b n z x y x e x v a r x γ β where i represents the normalized inputs to the model ai and bi represent learnable linear weights and bias parameters of size i respectively x i with different subscripts are the intermediate model states relu is rectified linear unit a continuous piecewise linear activation function used in the model bn z x is the batch normalization function calculated as shown above eq 3 9 the mean and standard deviation are calculated separately over mini batches and γ β are the learnable parameter vectors of size z batch normalization helps to significantly improve deep layer training rate and regulate rapidly varying internal features ioffe szegedy 2015 d represents the dropouts which helps to regularize weights prevent co adaptation and overfitting of the model hinton et al 2012 only 0 1 and 1 of the values are dropped dropouts in each forward run of the model are completely random and do not depend on the dropouts in the previous runs some important inputs may be dropped in a particular run but the same inputs may not be dropped in the next run of training thus ensuring that dropouts do not significantly alter the model performance the backward run or back propagation of error is the key to any neural network s ability to learn a task on completion of the forward run in the training phase the predicted outcome values are compared to the expected outcome values using a loss function and compute the training error training loss mean squared error loss mse mean absolute error mae multicategory cross entropy are among the few popularly used loss functions mse and mae are more suitable for regression problems and are sensitive to the quality of training data provided goodfellow et al 2016 mse in particular excessively weights outliers and a few bad training examples can dramatically degrade model results thus this study uses cross entropy loss eq 3 10 as the loss function 3 10 l a b i a i l o g b i where a is the predicted outcome and b is the expected outcome the training loss calculated using eq 3 10 is then propagated back to the model in form of loss gradient and the model adjusts the parameters according to this feedback the aim of training is to iteratively adjust the model parameters and then minimize the training loss numerous algorithms which cater to specific needs have been developed to minimize the training loss freund schapire 1997 kingma ba 2014 ruder 2016 and the study chose to use nesterov momentum version of stochastic gradient decent sgd algorithm loshchilov hutter 2016 ruder 2016 sgd is able to perform faster gradient computation which is key drawback of the most popularly used gradient decent algorithm moreover the momentum term prevents the algorithm from being stuck at the local minima the order of magnitude of the predictor parameters varies significantly their use in any clustering algorithm prior to preconditioning would result in biased results so the predictors are scaled to result in a similar order of magnitude for all predictors percentage impervious cover is converted to fraction impervious cover the location predictors latitude and longitude are referenced to the minimum value and then normalized by dividing it with the predictor range as shown in eq 3 11 below 3 11 x i x i x min x max x min where xi is either latitude or longitude at any station x i is the normalized value of latitude or longitude at that station while xmax xmin represent maximum minimum latitude or longitude in the entire dataset histograms of the distribution of remaining predictors surface elevation total drainage area two year flow slope and bathymetric area all indicate significant skewness so these five parameters are first log transformed and then normalized using standardization as shown in eq 3 12 below 3 12 y i y i y mean σ y where yi is either log transformed surface elevation total drainage area two year flow slope or bathymetric area at any station y i represent the normalized value of yi ymean and σ y are the mean and standard deviation of the log transformed predictor under consideration calculated for the entire dataset these scaled predictors are then fed as input to the model the entire dataset comprising of error and associated watershed and stream characteristics for each station is randomly divided into training validation and testing dataset 70 of the data is used for testing while 15 of data each is used to validate and test the deep neural network classifier model the dnn developed above is trained on the training dataset to learn the complex relationship of different characteristics and the error the model performance is subsequently tested on the testing dataset 4 results 4 1 comparison of bathymetry area estimates the study removes the error due to the missing bathymetry area and then assesses the accuracy of srcs stage height by comparing their difference with the usgs gauge rc stage height it is found that the estimated bathymetric area ranges from 0 m2 to 12816 m2 with a mean of 228 m2 and a median of 43 m2 the simple method used in this study to estimate the bathymetric area correction is not applicable for stations where src s minimum wse exceeds gauge rc s maximum wse such stations are classified as stations having very large bathymetric area and are excluded from the analysis on the other hand zero area is estimated for stations with approximately similar water surface elevation for the minimum discharge value on src and the gauge rc the bathymetric area estimates for all gauging stations are not validated due to the lack of surveyed bathymetric data however a summary of the comparison of estimated bathymetric area with surveyed hydroswot canova et al 2016 data for randomly selected forty five stations across conus is provided in fig 4 as described in the methodology the bathymetric area are estimated by computing the cross section area for lowest discharges from the area corrected src and then compared with hydroswot the simplified method used to estimate bathymetric area correction for the study tends to underestimate the bathymetric area with about 66 67 stations being underestimated among the remaining 33 33 stations where simplified method overestimates bathymetric area about 40 have less than 6 overprediciton fig 4 a shows that about 86 67 uncorrected srcs tend to have more than 70 error in cross section area at low discharges however about 66 67 area corrected srcs have less than 50 remaining error in cross section area with about 48 89 area corrected srcs having even less than 30 remaining error in cross section area the comparison indicates that even though the area corrected srcs might still have some error in cross section area but the simplified method is able to always reduce the error in cross section fig 4 b shows that about 53 33 of the stations showed more 50 improvement in cross section area when using area corrected src inplace of uncorrected src 4 2 variation of implicit error in src with different watershed and stream characteristics and across conus the normalised rmse nrmse for the stations within conus ranges from 16 12 to 174 08 with a mean of 0 65 65 and a median of 0 12 12 nrmse value can be influenced by the number of stage and discharge pairs in src and gauge rc if the number is low a smaller number of such pairs generally result in a low range of gauge rc stage height which in turn results in very high nrmse even for relatively small rmse values an analysis of the number of stage discharge pairs with nrmse indicates that this influence is not significant if the number of pairs is greater than 10 the nrmse for stations with more than 10 discharge height pairs ranges from 16 12 to 104 49 with a mean of 0 16 16 6 and a median nrmse of 0 12 11 7 the nrmse values computed in this study for select stations in texas are similar to the values reported in godbout et al 2019 with 15 to 15 difference in the values this difference in nrmse values may be attributed to the fact that godbout et al 2019 compared src to hec ras derived src and not to usgs gauge rc moreover this also validates that simplified bathymetry estimates are reasonable for this region the correlation of predictors and assessment metrics is based on using the spearman s rank correlation coefficient method spearman 1904 the spearman correlation coefficient between predictors and assessment metrics is shown in fig 5 as expected the nrmse and rmse are largely but not perfectly correlated which can be associated to the effect of number of stage discharge pairs mentioned earlier both the evaluation metrics have a significant association with the slope of the river reach and the bathymetric area estimates the performance of src s decreases with an increase in slope however their performance improves with an increase in bathymetric area estimates two year flow which is an estimate of the size of the river is moderately related to the src performance as two year flow increases both nrmse and rmse tend to decrease drainage area ratio also has a minor inverse relationship with the performance of the srcs even though the correlation matrix gives us an idea of how different predictors impact the performance concrete conclusions cannot be drawn from the correlation matrix alone because of the complex intertwined relation among the predictors the rmse between corrected src and gauge rc stage height varies from 12 22 m to 314 44 m with a mean of 0 78 m and a median of 0 32 m however nearly 2 3rd of the points lie within a range of 1 3 m to 1 37 m 4 5 ft rmse with a mean rmse of 0 165 m the stations are divided into three categories based on their rmse values the first category includes srcs at stations with absolute rmse value ranging from 0 to 0 61 m 2 ft and are labeled as satisfactory performing srcs second category includes srcs with absolute rmse between 0 61 m and 0 91 m 2 3 ft average performing srcs and the third category includes all points with absolute rmse 0 91 m 3 ft and are labeled as unsatisfactory performing srcs fig 6 shows the distribution of these categories across conus src s associated with larger rivers appear to have high rmse particularly in areas with lower slopes in midwestern us and along the gulf coast src s located in river deltas are particularly unsatisfactory performing especially in northern florida mississippi and texas urban coastal areas in texas and washington also appear to have unsatisfactory performing src s the variation of different categories with other parameters is shown in fig 7 the choice of predictors in fig 7 is based on their relative significance on src performance howbeit fig 5 suggests that elevation and percent impervious cover are not significantly correlated to src performance as individual predictors but fig 7 implies that these characteristics may be significant when used in combination with other characteristics fig 7 indicates that srcs at stations with low slopes and higher elevations can be associated with unsatisfactory performance the performance of src also tends to decrease as the values of the bathymetric area and two year flow increase in order to analyze the performance across different watershed regions table 2 presents the summary of the two assessment metrics in different huc 02 regions src s seem to work best for the ohio region with a median 0 0 nrmse and 0 12 m 0 38 ft rmse even the variation of nrmse and rmse is not very large with 25th and 75th percentile nrmse being 0 32 32 and 0 25 25 respectively and 25th and 75th percentile rmse ranging from 3 10 m to 2 52 m both nrmse and rmse values indicate that overall the src s are relatively more reliable in the relatively flatter regions comprising of ohio mid atlantic tennessee and upper mississippi fig 8 in contrast the srcs are less reliable in the mountainous regions spanning across upper colorado lower colorado the great basin and the rio grande with a median nrmse greater than 60 and median rmse greater than 0 82 m 2 7 ft the srcs in these regions tend to largely underpredict the usgs rating curves there is a general trend in srcs to overpredict in the lower mississippi region fig 8 the srcs in this region which are dominated by large rivers and very low slopes are poor performing and have large negative rmse values 4 3 deep neural network performance the entire dataset is randomly divided into training validation and testing dataset the validation and training datasets are normalized together while the testing dataset is treated as an entirely separate dataset and normalized separately the model is trained on the training dataset and is validated on the validation set as a check to prevent the model from overfitting the training data the validation and training datasets are kept completely separate and the model is trained for 150 epochs the trained model is able to classify 68 18 of the test data accurately table 3 here presents the confusion matrix of the model performance on the testing dataset of the total 225 stations in the satisfactory performing category category 0 the model accurately categorized 69 33 while the model misclassified 30 67 as unsatisfactory performing stations category 3 similarly 82 37 of the unsatisfactory performing category 3 stations are accurately predicted as unsatisfactory and 17 63 percent are predicted to be satisfactory category 0 the model is not able to predict any of the average performing points accurately 54 80 are predicted as satisfactory performing and 45 20 are predicted as unsatisfactory none of the testing data set points are predicted as average performing the model predicted a station to be either satisfactory or unsatisfactory performing fig 9 shows the performance of the test set the performance of the test set is categorized into three categories correct prediction then missed by one category and missed by two categories correct prediction represents the scenario wherein the model predicted label matches the actual label for that station in case the model predicts a satisfactory unsatisfactory performing point as average performing point or the model predicts an average performing point as satisfactory unsatisfactory those stations are labelled as missed by one category however if the model predicts a satisfactory performing station as unsatisfactory or an unsatisfactory performing point as satisfactory this is the worst possible prediction and the testing performance is labeled as missed by two categories the missed by two category labels of the dnn are spread over conus however a significant number of such stations can be associated with the plains of kansas missouri and indiana one possible explanation could lie in the fact that most stations in these plains have very similar channel and watershed characteristics 5 discussion and conclusions this study evaluated the reliability of srcs by comparing them to usgs gauge rating curves further the study assessed the applicability of using a deep neural network model to extrapolate the performance of src for all the ungauged river reaches the error in srcs can be divided into two parts one due to inaccurate topography that does not include bathymetry and the other due to implicit assumptions in srcs reach average properties using manning s equation uniform flow assumption and constant channel roughness in an attempt to accommodate the missing bathymetry in src a simple framework is proposed to estimate the missing area using src and usgs gauge rating curve this simple method tends to underpredict the area but can be used as a good estimate the study finds that even upon accounting for the bathymetry area a significant error remains in srcs and can be related to the implicit assumptions the implicit error due to assumptions in src is significantly correlated to the associated bathymetry area of the stream the slope of the stream two year flow reflects the size of the river and the drainage area with the bathymetry area being most significant impact and impact decreases as we move to drainage area the study finds that the tendency of src to overpredict or underpredict can be connected to the watershed and stream characteristics the low lying coastal areas of texas mississippi and northern florida particularly the areas dominated with river deltas are more prone to overprediction while on the other hand srcs tend to underpredict in the mountainous regions both rocky and appalachian mountain ranges this can be attributed to the fact that the assumption of constant channel roughness does not hold good for all the regions the low lying coastal areas have fine sand riverbeds and would generally have a lower roughness as compared to the value used for src calculation this is one possible explanation for overprediction of src in these regions similarly the rivers in mountainous regions are lined with boulders and generally have a higher channel roughness thus lower roughness srcs tend to underpredict poor performance in urban environments can be associated to the manning s n this is coherent with the findings of scriven et al 2021 who concluded that using appropriated manning s n results in lower nrmse in heavily urbanized catchments godbout et al 2019 study presents a general trend of srcs overpredicting the stage height which can be attributed to the fact that the study focused on few reaches in texas lying in relatively low relief and planar areas moreover the channel characteristics like cross section and slope tend to be more erratic in the mountainous regions thus higher rmse in these regions is also a byproduct of using reach averaged channel properties in src derivation on the other hand the channel roughness used in srcs and using reach average properties is a good approximation for plains in the midwestern united states comprising of ohio mid atlantic tennessee and upper mississippi huc02 regions the srcs therefore provide relatively more reliable estimates in this mid western region the relatively higher rmse values in rocky mountains also highlight the limitation of using the manning s equation and the uniform flow assumption specifically manning s equation does not hold good for very steep slopes and uniform flow conditions also do not hold valid for the streams with steep and irregular slopes the poor performance in coastal regions is coherent with the findings of garousi nejad et al 2019 godbout et al 2019 johnson et al 2019 the resolution of the underlying dem is another significant source of error in these conditions low resolution dem in low relief regions typically lack terrain convergence thus resulting in a poor hand raster which lies at the heart of src formation using more appropriate and different channel roughness and different reach averaging techniques for midwestern plains mountains and coastal areas can improve the reliability of srcs the deep neural network model shows promise to learn the complex relationship between the src performance and the water and stream characteristics one may think that model performance is affected by the data used in training and testing and would give different results if the data are randomly sampled differently to create a different training and testing dataset however the analysis is repeated several times randomly sampling the data into training and testing dataset each time and the performance of the model is found to be consistent this assures that the method can be used to reliably extrapolate the src performance to ungauged basins the bathymetry area is a key input to the model and the model performance decreases substantially if it is removed this emphasizes the fact that bathymetry plays a crucial role in the src reliability and the deep neural network model requires some parameter that can represent this bathymetry area even though the hand src method is prone to errors simplifying assumptions of uniform flow and applicability manning s equation are in fact much needed for effective regional and national scale flood mapping the incorporation of higher resolution terrain data and methods to incorporate river bathymetry into the higher resolution dems can significantly improve the reliability of this empirical method moreover estimating channel roughness for each reach is tedious but instead of using single constant channel roughness for all the reaches one can use different appropriated values for different regions for instance a higher channel roughness value can be used in the mountainous region and lower roughness value can be used in coastal areas there is also scope for significant improvement in neural network model performance using other algorithms and normalization techniques the dnn model performance is inconsistent for plains in kansas missouri and indiana as most stations in the region possess very similar characteristics however incorporation of some additional parameters which can significantly differentiate the channel and watershed characteristics would also increase the model performance in this region missing bathymetry area can also be estimated using the regional hydraulic geometry curves which relate the bankfull cross section area and width with the drainage area leopold and maddock 1953 dunne and leopold 1978 bieger et al 2015 blackburn lynch et al 2017 however the accuracy of bathymetry area estimated using regional hydraulic geometry curves would depend significantly on the accuracy of hydraulic geometry curves for the associated physiographic region appendix b other possible alternative to using the bathymetry area estimates in the neural network could be to train a second network that estimates the bathymetry area using channel characteristics and then feeds this estimated area to the first network a study on incorporating a similar structure wherein smaller networks learn intermediate physical parameters and then the outcome of these networks if fed as input to the main neural network was used for streamflow estimation and shows promising results khandelwal et al 2020 the performance of srcs varies significantly across conus with rmse ranging from 12 22 m to 314 44 m but most of the points lie confide to a range of 1 37 m to 1 37 m and a median rmse of 0 32 m the median values are encouraging but the range of rmse is high and suggests that even after incorporation of bathymetry estimates there is a need for improvement in the hand src method before it could be reliably used for real time flood forecasting at a continental scale the findings from this study are consistent with earlier results found for smaller areas garousi nejad et al 2019 godbout et al 2019 johnson et al 2019 which suggest a need for improvement and confirms the conclusion across conus the study improves upon these previous studies by segregating the error in srcs due to implicit assumptions and due to topography and then expanding the analysis to conus finally the results from this study show a general tendency of srcs to overpredict in coastal areas dominated with river deltas and to underpredict in mountainous regions further work to improve hand src model capability to wide catchment conditions particularly for coastal areas and steep slopes is recommended credit authorship contribution statement ankit ghanghas conceptualization methodology formal analysis data curation writing original draft writing review editing sayan dey methodology formal analysis writing review editing venkatesh merwade methodology formal analysis writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a as the srcs are based on manning s equation the sensitivity of these rating curves to different components of the manning s equation is first analyzed by assuming a theoretical trapezoidal channel cross section with a base channel width of 10 m a side slope of 1 5 1 for the main channel and a side slope of 4 1 h v for the banks floodplains a channel slope of 0 0008 and manning s n of 0 05 is assumed for the entire cross section with uniform flow conditions fig a1 a to analyze the effect of missing bathymetry different channel slope and different manning s n rating curves are produced while varying individual parameters independently fig a1 b a reduction in the channel roughness results in a decreased water depth for any discharge while an increase in roughness results in an increased water depth the extent of increase or decrease in water depth due to varying roughness is more prominent at higher discharges establishing that high flows are more sensitive to channel roughness in comparison to the low flows fig a1 b and table a1 similarly it is found that the depth of water for any discharge decreases as the channel slope increases and that that high flows are relatively more sensitive to changes in channel slope fig a1 b and table a1 on the other hand the impact of adding missing bathymetry is significant for low flows and it diminishes with increasing discharge this theoretical exploration establishes that manning s roughness and channel slope affects the srcs at high flows whereas missing bathymetry affects the srcs at low flows the effect of manning s roughness and slope is relatively insignificant on src for low flows appendix b appendix b presents the comparison of the estimated bathymetric area acorr calculated as per section 3 1 with empirical bathymetry area ab calculated using hydraulic geometry curves based on the parameters presented in blackburn lynch et al 2017 now the hydraulic geometry curves presented in blackburn lynch et al 2017 do not provide a direct estimate of the bathymetry they relate the drainage area to the bankfull width and bankfull area estimating the bathymetry area using the hydraulic geometry is a two step process first for any stream the bankfull area abkf and bankfull width wbkf are calculated using the empirical equations second estimate the stage height h0 when the top width estimated from the dem is equal to wbkf for this stage height h0 estimate the cross section area using the dem a0 the empirical bathymetry area ab is then estimated as ab abkf a0 the empirical hydraulic geometry curves and their accuracy vary across the different physiographic regions in the us so the srcs were clustered into respective physiographic regions and fig b1 and table b1 present the relation between acorr and ab for one region with good accuracy of empirical hydraulic curves one with moderate accuracy and one with poor accuracy of the empirical hydraulic curves fig b1 here shows that the estimated bathymetry area acorr can be directly calculated using the hydraulic geometry curves however the accuracy of calculating the estimated bathymetry area acorr is a function of the accuracy of the hydraulic geometry curves and varies across physiographic regions ab empirical bathymetry area is a combination of both abkf and wbkf and therefore for a given physiographic region the accuracy of the resulting empirical bathymetry area would be relatively less than the accuracy of calculating abkf and wbkf comparing r2 values for different provinces in fig b1 and table b1 
3686,empirical approaches such as the height above nearest drainage method in conjunction with synthetic rating curves hand src have emerged as particularly appealing alternatives to the traditional flood mapping techniques due to their lower complexity and fewer data requirements however srcs use digital elevation model dem derived reach averaged hydraulic properties and assume one dimensional steady state flow condition with normal depth these implicit model assumptions may introduce errors in flood stage and extent estimates using the hand src approach this study investigates the reliability of src across continental united states conus by comparing them to the united states geological survey s usgs gauge rating curves results from this comparison show that the implicit model assumptions used in the src hand approach add significant error in the src derivation the accuracy of the src is found to be related to the stream characteristics including the bathymetry area slope of the main channel two year flow and drainage area results also show that srcs in coastal areas characterized by low slopes and large drainage areas have higher error and tend to overpredict the stage height in comparison to the usgs rating curves whereas they tend to underpredict stage height in the mountainous regions the srcs are most reliable for the midwestern plains of ohio mid atlantic tennessee and upper mississippi regions and least reliable higher error for the rocky mountains further the study finds that deep neural network models can be effectively used to judge the performance of src for ungauged river reaches keywords synthetic rating curve hand flood mapping deep learning application continental scale 1 introduction flooding is one of the most threatening natural hazards in terms of mortal as well as economic losses worldwide these losses continue to increase with the changing climate growing population and rapidly urbanizing environment flood losses account for about 6 billion per year in the united states alone over the past 30 years noaa 2016 and are projected to increase to 52 billion per year worldwide by 2050 hallegatte et al 2013 large scale accurate flood maps spanning an entire continent have the potential to significantly improve our flood preparedness and risk management in an attempt to improve nationwide resilience and mitigation of flood risks the national oceanic and atmospheric administration s noaa s national water model nwm in the united states forecasts streamflow for 2 7 million river reaches spread across the 48 contiguous states and the district of columbia conus however creating detailed flood inundation maps across the conus from nwm forecasted streamflow remains a challenge johnson and cole 2017 traditionally flood maps were created using hydrodynamic models for individual streams or rivers for a city or county due to high computational demands to execute these models at river network scale and availability of accurate floodplain topographic data at such scales most traditional hydrodynamic models are based on solving one or two dimensional saint venant equations shallow water equations for example hydrologic engineering center s river analysis system hec ras us army corps of hydraulic engineers 2002 can be used to produce detailed flood maps but their applicability to map real time flood events at continental scale is restricted by their complexity and high computational and data requirements fekete vörösmarty 2007 simple empirical approaches with lower computational and data requirements are a particularly appealing alternative for continental scale flood inundation mapping cfim one such empirical approach is the height above nearest drainage hand method in conjunction with synthetic rating curves hand src zheng et al 2018 which can be used to produce flood inundation maps across large spatial domains efficiently hand rennó et al 2008 rodda 2005 is defined as the height of each grid cell digital elevation model dem grid above the nearest stream cell it drains to it has been effectively used to determine soil water potential nobre et al 2011 ground water potential rahmati et al 2018 and for flood inundation mapping nobre et al 2016 for a given stage height at a stream cell any grid cell draining to that stream cell with a hand value less than the stage is flooded and the difference between stage height and hand defines the inundation depth the nwm only produces streamflow and to use them for real time flood inundation maps they need to be converted to a stage height at a stream cell to deal with this shortcoming zheng et al 2018 developed synthetic rating curves src as a viable alternative to actual rating curves in the hand src framework these srcs convert the discharge into stage height and then hand converts the stage height into flood inundation the srcs that lie at the heart of this hand src approach are based on using reach averaged channel properties in manning s equation to relate water depth and discharge for any given water depth spanning over a river reach meaning same water depth in the entire reach the inundation zone marked by all the grid cells with a hand value less than the water level is used to estimate the total surface area volume and total bed area of the inundation zone a reach average of the total surface area volume and bed area subsequently results in averaged channel width cross sectional area and wetted perimeter of the channel reach respectively furthermore zheng et al 2018 assumes normal depth along all reaches and equates friction slope to the reach averaged channel slope by using a constant manning s n for all the reaches across conus these assumptions of reach averaged channel properties normal depth and constant manning s n are not always valid and act as potential sources of error the src s which derive their channel properties from a dem based hand raster also inherit the errors associated with the dem including the error due to absence of bathymetry river bed information in a dem the importance of bathymetry in hydrodynamic models has been well quantified by previous studies dey 2016 dey et al 2019 grimaldi et al 2018 neal et al 2015 trigg et al 2009 a significant amount of work is already being done to deal with the dem based errors particularly in the surface water and ocean topography swot mission biancamaria et al 2016 jung et al 2010 yoon et al 2012 therefore there is a need to analyze the residual errors in the srcs after removing the errors due to inaccurate bathymetry representation previously garousi nejad et al 2019 evaluated the reliability of hand src based flood inundation maps by comparing them against high resolution satellite imagery based maps they found that reliability of the hand src maps can be improved by using high resolution hydrography water drainage network and dem they also suggested an approach to obtain reach specific manning s n but their work focused on dem based improvements and did not tackle other errors due to implicit assumptions in the src johnson et al 2019 compared the national water model height above nearest neighbor nwm hand generated flood maps with remotely sensed flood maps for 54 catchments the study concluded that there is a tendency for the nwm hand to underpredict inundated cells for lower order streams and that most of the error in the approach can be associated with error in the src but the study did not focus on analyzing the errors in srcs scriven et al 2021 proposed that combining minimum manning s n derived src and median manning s n derived src for each reach improves the src s reliability and also allows for a prediction window to accommodate the uncertainty in choosing the roughness parameter however the study just focused on tackling the uncertainty due to manning s n and does not consider the error due to missing bathymetry and other implicit assumptions godbout et al 2019 focused on errors in src and found that reach characteristics like lower gradient and shorter length can be associated with poor performing srcs but their study was limited to a few rivers in texas and compared srcs to hec ras rating curves thus their findings cannot be generalized for all the reaches across the conus the errors in srcs can be divided into types one due to topography and the other due to implicit assumptions the error due to topography may be reduced in the near future by using high resolution both horizontal and vertical resolution dems and improving bathymetric representation example lidar dem with swot derived bathymetry this study attempts to address the error due to implicit assumptions such as using reach averaged channel properties a constant channel roughness and uniform flow specifically this study attempts to quantify the src errors by using rating curves at gauged locations and develop a methodology to estimate these errors at ungauged stream reaches accordingly the objectives of this study are i devise a simple method to remove the errors due to inaccurate bathymetry representation ii estimate the error due to the implicit assumptions in srcs at gauged locations across the conus iii relate the error in srcs with watershed and stream characteristics and iv propose a method for estimating the performance of srcs in ungauged reaches these objectives are accomplished by using rating curves for 6189 united states geological survey usgs streamflow gauging stations river and watershed characteristics are derived by using publicly available data related to topography hydrography and land use the relationship between src and stream or watershed characteristics is developed by using a deep neural network more details about the data and methodology is presented in the following sections 2 study area and data the study focuses on the comparison of measured rating curves and srcs across the conus the usgs maintains a network of 10 606 streamflow gauging stations within the conus but rating curve is not available for all these stations thus this study uses data from only 6189 stations shown in fig 1 for which rating curves are available rating curve data for the selected stations are downloaded from the national water information system nwis table 1 the stage associated with each rating curve is converted to water surface elevation by adding the gauge datum associated with each station similarly srcs provide stage relative to the surface elevation derived from 10 m national elevation dataset ned the src s stage is converted to water surface elevation by adding the surface elevation from ned of the point on national hydrography dataset nhd streamline uepa et al 2012 nearest to the gauge this ensures that both sets of rating curves are now represented by water surface elevation wse and therefore can be compared consistently the relationship between src and reach watershed characteristics is developed by using eight geographic terrain and channel characteristics for each usgs gauging station these include latitude longitude surface elevation total drainage area at the gauging station two year flow slope of river reach percent impervious cover in the huc 12 watershed draining to the gauge location and bathymetric area the data used to derive these characteristics are provided in table 1 3 methodology this study analyzes the accuracy of srcs with respect to usgs gauge rating curves the src gives a reach average relationship between discharge and stage while the usgs rating curves are valid only for a particular point in order to compare the two each usgs station is associated to the nearest national hydrography dataset nhd reach medium resolution and it is assumed that the src related to that reach would be valid for that particular gauging station the error due to bathymetry is first removed by comparing the water surface elevation level produced from the two rating curves and subsequently producing area corrected curves the error in these corrected rating curves is then quantified using appropriate metrics next the error correlation with different watershed and stream characteristics is investigated and the reliability of hand src method is mapped across the conus finally a deep neural network dnn model is developed for evaluating the reliability of src at ungauged reaches the dnn model learns the complex correlation between key watershed and stream characteristics and the src error and thereafter it can be used to predict the performance of src for any stream the methodology for each of these procedures is described below 3 1 removing error due to bathymetry srcs are computed by using reach averaged values of hydraulic properties such as manning s n cross section channel slope and wetted perimeter in the manning s equation by assuming these properties to be constant across the entire river reach zheng et al 2018 these hydraulic properties of srcs are based on hand raster which is derived from ned dem the lack of bathymetric data in the ned dem also contributes to some error in src values in addition to the error due to the implicit assumptions the src represents discharge versus water depth only above the base water surface the wse of the river represented in the dem during it formation as they lack bathymetric information and it would be inaccurate to compare them directly to usgs rating curves thus to compare the two curves one needs to account for the bathymetric area a0 and corresponding discharge q0 by using the finding from the theoretical exploration fig a1 and table a1 appendix a that low flow estimates are impacted by missing bathymetry a simple approach is used to account for the bathymetric area specifically to account for missing bathymetry area src is shifted to match the lowest discharge record for that particular reach fig 2 b it is assumed that for this low water depth above dem the error in src would be mostly due to the topographic error and the implicit errors would be negligible so if δy δy 0 fig 2 a is the water depth above riverbed from dem and y is the elevation of riverbed of dem then the discharge at wse with respect to actual riverbed of y δy for the src should be same as the discharge from gauge rc for wse of y δy fig 2 b the difference in these discharges obtained from src and gauge rc is then assumed to be missing discharge in src due to bathymetry and can be termed as discharge correction q0 fig 2 assuming the same hydraulic properties of src are valid for the bathymetric section of the channel as well the bathymetric area correction a0 is calculated for discharge q0 now if q is the discharge and a is the cross sectional area corresponding to any wse y1 for the src the area corrected srcs should have a cross sectional area of a a0 corresponding to this y1 using the same hydraulic properties of src discharge as q1 calculated for this new a a0 cross sectional area and this q1 and y1 form the discharge wse pair for the area corrected src all the further analysis is done using these area corrected src 3 2 metrics for error analysis as there is a lack of an established method to compare the usgs rating curves with the src the performance srcs can be assessed using several statistical techniques namely the mean absolute bias mab root mean square error rmse r2 or coefficient of determination normalized root mean square error range percent bias mean absolute error mae in this study a modified version of rmse herein the rmse value is multiplied with the sign of the bias is used rmse ensures that for any particular station one takes the absolute difference between gauge rc stage height and src stage height and simultaneously does not allow the overpredictions and underpredictions to cancel each other out however rmse alone is not able to describe if overall the src overpredicts or underpredicts the gauge rc s stage height this is addressed by the modified metric as the magnitude of the metric gives us the absolute value of the rmse while the negative positive sign indicates underprediction overprediction the study also uses the normalized rmse nrmse which is defined as rmse normalized by the range of gauge rc stage height nrmse is not affected by the size of the river and can be used for comparing rivers of different scales godbout et al 2019 the nrmse is also multiplied with the sign of bias rmse between usgs rating curve and src at a usgs gauging station is computed as 3 1 rmse b 1 n y b y b 2 n where the stage height from src y b and the stage height from usgs rating curve y b are evaluated for a set of return period discharges here n represents the number of discharge stage height pairs for which the src and the usgs rating curve are compared for a given usgs station 3 3 extracting watershed and stream characteristics to analyze the performance of src across the conus a total of eight geographic terrain and channel characteristics for each usgs gauging station are chosen these include the latitude longitude surface elevation total drainage area two year flow percent impervious cover at the gauging station location slope of river reach and bathymetric area are used the study used the bathymetric area correction a0 calculated above as an estimate of the bathymetric area the latitude and longitude here represent the geographic characteristics whereas the surface elevation and slope of river reach represent the topographic characteristics associated with each station the latitude and longitude values are obtained from usgs gauge information and surface elevation is the dem value associated with each station the slope of river reach which is obtained from the attribute table of the nhdplus mr dataset also crudely represents the riverbed characteristics and flow velocities impervious cover is included as a parameter to account for the urban cover of the watershed draining at the gauging station each station is associated with it its respective huc 12 subwatershed the huc 12 unit subwatersheds are derived from the watershed boundary dataset wbd the average size of huc 12 units is 103 km2 the percentage impervious cover is then calculated for these huc 12 units using the nlcd 2016 percent developed imperviousness which is then assigned as percent impervious cover for its associated gauging station total drainage area and two year flow represent the size of the stream associated with each station however two year flow provides additional information as it also represents if the stream is perennial intermittent or ephemeral two year flow cannot replace the total drainage area as for example an ephemeral stream with a significant drainage area may have a lower two year flow total drainage area values are obtained from the attribute table of nhdplus mr dataset the two year flow is calculated using the usgs daily streamflow readings average precipitation soil porosity soil depth precipitation frequency permeability average temperature in entire watershed are a few other characteristics that may have been included in the analysis but jafarzadegan moradkhani 2020 found them to be non significant and are thus not included in this study 3 4 deep neural network model one of the key objectives of the study is to analyze the reliability of src to create flood inundation maps especially at ungauged reaches in this section a machine learning model is developed for correlating the error in src to reach and watershed characteristics after training and validating this model for gauged reaches it can potentially be implemented for estimating the reliability of src at ungauged reaches across the conus knowledge of absolute error in srcs represented by rmse is more valuable for their use for emergency responses nrmse on the other hand is good for the comparison of streams of different scales in this study the performance of an src at a station is classified as satisfactory or unsatisfactory based on the absolute rmse value the stations with rmse between 0 and 0 61 m 2 ft are labeled as satisfactory category 0 and those with rmse 0 91 m 3 ft category 2 are labeled as unsatisfactory performing while the stations in between are termed as average performing category 1 the relationship between watershed and stream characteristics and the error in src is complex which cannot be captured using a simple regression model a deep neural network dnn is better suited to learn the complex relationship between predictors and outputs goodfellow et al 2016 lecun et al 2015 learning this complex relationship is particularly useful because using a dnn model one can predict the reliability of src at ungauged river reaches using just their watershed and stream characteristics a dnn classifier model is developed to classify the src as satisfactory or unsatisfactory performing based on the characteristic parameters the model takes the above normalized eight predictor fig 3 as input and produces the best fit category 0 1 or 2 for each station fig 3 fig 3 also presents the structure of the neural network used the forward pass of the neural network is defined by the following equations 3 2 x 1 r e l u a 4000 i b 4000 3 3 x 2 d b n 4000 x 1 3 4 x 3 r e l u a 2000 x 2 b 2000 3 5 x 4 d b n 2000 x 3 3 6 x 5 a o u t s i z e x 4 b o u t s i z e 3 7 y l o g e x 5 j e x 5 j 3 8 r e l u x m a x 0 x x r 3 9 b a t c h n o r m 1 d b n z x y x e x v a r x γ β where i represents the normalized inputs to the model ai and bi represent learnable linear weights and bias parameters of size i respectively x i with different subscripts are the intermediate model states relu is rectified linear unit a continuous piecewise linear activation function used in the model bn z x is the batch normalization function calculated as shown above eq 3 9 the mean and standard deviation are calculated separately over mini batches and γ β are the learnable parameter vectors of size z batch normalization helps to significantly improve deep layer training rate and regulate rapidly varying internal features ioffe szegedy 2015 d represents the dropouts which helps to regularize weights prevent co adaptation and overfitting of the model hinton et al 2012 only 0 1 and 1 of the values are dropped dropouts in each forward run of the model are completely random and do not depend on the dropouts in the previous runs some important inputs may be dropped in a particular run but the same inputs may not be dropped in the next run of training thus ensuring that dropouts do not significantly alter the model performance the backward run or back propagation of error is the key to any neural network s ability to learn a task on completion of the forward run in the training phase the predicted outcome values are compared to the expected outcome values using a loss function and compute the training error training loss mean squared error loss mse mean absolute error mae multicategory cross entropy are among the few popularly used loss functions mse and mae are more suitable for regression problems and are sensitive to the quality of training data provided goodfellow et al 2016 mse in particular excessively weights outliers and a few bad training examples can dramatically degrade model results thus this study uses cross entropy loss eq 3 10 as the loss function 3 10 l a b i a i l o g b i where a is the predicted outcome and b is the expected outcome the training loss calculated using eq 3 10 is then propagated back to the model in form of loss gradient and the model adjusts the parameters according to this feedback the aim of training is to iteratively adjust the model parameters and then minimize the training loss numerous algorithms which cater to specific needs have been developed to minimize the training loss freund schapire 1997 kingma ba 2014 ruder 2016 and the study chose to use nesterov momentum version of stochastic gradient decent sgd algorithm loshchilov hutter 2016 ruder 2016 sgd is able to perform faster gradient computation which is key drawback of the most popularly used gradient decent algorithm moreover the momentum term prevents the algorithm from being stuck at the local minima the order of magnitude of the predictor parameters varies significantly their use in any clustering algorithm prior to preconditioning would result in biased results so the predictors are scaled to result in a similar order of magnitude for all predictors percentage impervious cover is converted to fraction impervious cover the location predictors latitude and longitude are referenced to the minimum value and then normalized by dividing it with the predictor range as shown in eq 3 11 below 3 11 x i x i x min x max x min where xi is either latitude or longitude at any station x i is the normalized value of latitude or longitude at that station while xmax xmin represent maximum minimum latitude or longitude in the entire dataset histograms of the distribution of remaining predictors surface elevation total drainage area two year flow slope and bathymetric area all indicate significant skewness so these five parameters are first log transformed and then normalized using standardization as shown in eq 3 12 below 3 12 y i y i y mean σ y where yi is either log transformed surface elevation total drainage area two year flow slope or bathymetric area at any station y i represent the normalized value of yi ymean and σ y are the mean and standard deviation of the log transformed predictor under consideration calculated for the entire dataset these scaled predictors are then fed as input to the model the entire dataset comprising of error and associated watershed and stream characteristics for each station is randomly divided into training validation and testing dataset 70 of the data is used for testing while 15 of data each is used to validate and test the deep neural network classifier model the dnn developed above is trained on the training dataset to learn the complex relationship of different characteristics and the error the model performance is subsequently tested on the testing dataset 4 results 4 1 comparison of bathymetry area estimates the study removes the error due to the missing bathymetry area and then assesses the accuracy of srcs stage height by comparing their difference with the usgs gauge rc stage height it is found that the estimated bathymetric area ranges from 0 m2 to 12816 m2 with a mean of 228 m2 and a median of 43 m2 the simple method used in this study to estimate the bathymetric area correction is not applicable for stations where src s minimum wse exceeds gauge rc s maximum wse such stations are classified as stations having very large bathymetric area and are excluded from the analysis on the other hand zero area is estimated for stations with approximately similar water surface elevation for the minimum discharge value on src and the gauge rc the bathymetric area estimates for all gauging stations are not validated due to the lack of surveyed bathymetric data however a summary of the comparison of estimated bathymetric area with surveyed hydroswot canova et al 2016 data for randomly selected forty five stations across conus is provided in fig 4 as described in the methodology the bathymetric area are estimated by computing the cross section area for lowest discharges from the area corrected src and then compared with hydroswot the simplified method used to estimate bathymetric area correction for the study tends to underestimate the bathymetric area with about 66 67 stations being underestimated among the remaining 33 33 stations where simplified method overestimates bathymetric area about 40 have less than 6 overprediciton fig 4 a shows that about 86 67 uncorrected srcs tend to have more than 70 error in cross section area at low discharges however about 66 67 area corrected srcs have less than 50 remaining error in cross section area with about 48 89 area corrected srcs having even less than 30 remaining error in cross section area the comparison indicates that even though the area corrected srcs might still have some error in cross section area but the simplified method is able to always reduce the error in cross section fig 4 b shows that about 53 33 of the stations showed more 50 improvement in cross section area when using area corrected src inplace of uncorrected src 4 2 variation of implicit error in src with different watershed and stream characteristics and across conus the normalised rmse nrmse for the stations within conus ranges from 16 12 to 174 08 with a mean of 0 65 65 and a median of 0 12 12 nrmse value can be influenced by the number of stage and discharge pairs in src and gauge rc if the number is low a smaller number of such pairs generally result in a low range of gauge rc stage height which in turn results in very high nrmse even for relatively small rmse values an analysis of the number of stage discharge pairs with nrmse indicates that this influence is not significant if the number of pairs is greater than 10 the nrmse for stations with more than 10 discharge height pairs ranges from 16 12 to 104 49 with a mean of 0 16 16 6 and a median nrmse of 0 12 11 7 the nrmse values computed in this study for select stations in texas are similar to the values reported in godbout et al 2019 with 15 to 15 difference in the values this difference in nrmse values may be attributed to the fact that godbout et al 2019 compared src to hec ras derived src and not to usgs gauge rc moreover this also validates that simplified bathymetry estimates are reasonable for this region the correlation of predictors and assessment metrics is based on using the spearman s rank correlation coefficient method spearman 1904 the spearman correlation coefficient between predictors and assessment metrics is shown in fig 5 as expected the nrmse and rmse are largely but not perfectly correlated which can be associated to the effect of number of stage discharge pairs mentioned earlier both the evaluation metrics have a significant association with the slope of the river reach and the bathymetric area estimates the performance of src s decreases with an increase in slope however their performance improves with an increase in bathymetric area estimates two year flow which is an estimate of the size of the river is moderately related to the src performance as two year flow increases both nrmse and rmse tend to decrease drainage area ratio also has a minor inverse relationship with the performance of the srcs even though the correlation matrix gives us an idea of how different predictors impact the performance concrete conclusions cannot be drawn from the correlation matrix alone because of the complex intertwined relation among the predictors the rmse between corrected src and gauge rc stage height varies from 12 22 m to 314 44 m with a mean of 0 78 m and a median of 0 32 m however nearly 2 3rd of the points lie within a range of 1 3 m to 1 37 m 4 5 ft rmse with a mean rmse of 0 165 m the stations are divided into three categories based on their rmse values the first category includes srcs at stations with absolute rmse value ranging from 0 to 0 61 m 2 ft and are labeled as satisfactory performing srcs second category includes srcs with absolute rmse between 0 61 m and 0 91 m 2 3 ft average performing srcs and the third category includes all points with absolute rmse 0 91 m 3 ft and are labeled as unsatisfactory performing srcs fig 6 shows the distribution of these categories across conus src s associated with larger rivers appear to have high rmse particularly in areas with lower slopes in midwestern us and along the gulf coast src s located in river deltas are particularly unsatisfactory performing especially in northern florida mississippi and texas urban coastal areas in texas and washington also appear to have unsatisfactory performing src s the variation of different categories with other parameters is shown in fig 7 the choice of predictors in fig 7 is based on their relative significance on src performance howbeit fig 5 suggests that elevation and percent impervious cover are not significantly correlated to src performance as individual predictors but fig 7 implies that these characteristics may be significant when used in combination with other characteristics fig 7 indicates that srcs at stations with low slopes and higher elevations can be associated with unsatisfactory performance the performance of src also tends to decrease as the values of the bathymetric area and two year flow increase in order to analyze the performance across different watershed regions table 2 presents the summary of the two assessment metrics in different huc 02 regions src s seem to work best for the ohio region with a median 0 0 nrmse and 0 12 m 0 38 ft rmse even the variation of nrmse and rmse is not very large with 25th and 75th percentile nrmse being 0 32 32 and 0 25 25 respectively and 25th and 75th percentile rmse ranging from 3 10 m to 2 52 m both nrmse and rmse values indicate that overall the src s are relatively more reliable in the relatively flatter regions comprising of ohio mid atlantic tennessee and upper mississippi fig 8 in contrast the srcs are less reliable in the mountainous regions spanning across upper colorado lower colorado the great basin and the rio grande with a median nrmse greater than 60 and median rmse greater than 0 82 m 2 7 ft the srcs in these regions tend to largely underpredict the usgs rating curves there is a general trend in srcs to overpredict in the lower mississippi region fig 8 the srcs in this region which are dominated by large rivers and very low slopes are poor performing and have large negative rmse values 4 3 deep neural network performance the entire dataset is randomly divided into training validation and testing dataset the validation and training datasets are normalized together while the testing dataset is treated as an entirely separate dataset and normalized separately the model is trained on the training dataset and is validated on the validation set as a check to prevent the model from overfitting the training data the validation and training datasets are kept completely separate and the model is trained for 150 epochs the trained model is able to classify 68 18 of the test data accurately table 3 here presents the confusion matrix of the model performance on the testing dataset of the total 225 stations in the satisfactory performing category category 0 the model accurately categorized 69 33 while the model misclassified 30 67 as unsatisfactory performing stations category 3 similarly 82 37 of the unsatisfactory performing category 3 stations are accurately predicted as unsatisfactory and 17 63 percent are predicted to be satisfactory category 0 the model is not able to predict any of the average performing points accurately 54 80 are predicted as satisfactory performing and 45 20 are predicted as unsatisfactory none of the testing data set points are predicted as average performing the model predicted a station to be either satisfactory or unsatisfactory performing fig 9 shows the performance of the test set the performance of the test set is categorized into three categories correct prediction then missed by one category and missed by two categories correct prediction represents the scenario wherein the model predicted label matches the actual label for that station in case the model predicts a satisfactory unsatisfactory performing point as average performing point or the model predicts an average performing point as satisfactory unsatisfactory those stations are labelled as missed by one category however if the model predicts a satisfactory performing station as unsatisfactory or an unsatisfactory performing point as satisfactory this is the worst possible prediction and the testing performance is labeled as missed by two categories the missed by two category labels of the dnn are spread over conus however a significant number of such stations can be associated with the plains of kansas missouri and indiana one possible explanation could lie in the fact that most stations in these plains have very similar channel and watershed characteristics 5 discussion and conclusions this study evaluated the reliability of srcs by comparing them to usgs gauge rating curves further the study assessed the applicability of using a deep neural network model to extrapolate the performance of src for all the ungauged river reaches the error in srcs can be divided into two parts one due to inaccurate topography that does not include bathymetry and the other due to implicit assumptions in srcs reach average properties using manning s equation uniform flow assumption and constant channel roughness in an attempt to accommodate the missing bathymetry in src a simple framework is proposed to estimate the missing area using src and usgs gauge rating curve this simple method tends to underpredict the area but can be used as a good estimate the study finds that even upon accounting for the bathymetry area a significant error remains in srcs and can be related to the implicit assumptions the implicit error due to assumptions in src is significantly correlated to the associated bathymetry area of the stream the slope of the stream two year flow reflects the size of the river and the drainage area with the bathymetry area being most significant impact and impact decreases as we move to drainage area the study finds that the tendency of src to overpredict or underpredict can be connected to the watershed and stream characteristics the low lying coastal areas of texas mississippi and northern florida particularly the areas dominated with river deltas are more prone to overprediction while on the other hand srcs tend to underpredict in the mountainous regions both rocky and appalachian mountain ranges this can be attributed to the fact that the assumption of constant channel roughness does not hold good for all the regions the low lying coastal areas have fine sand riverbeds and would generally have a lower roughness as compared to the value used for src calculation this is one possible explanation for overprediction of src in these regions similarly the rivers in mountainous regions are lined with boulders and generally have a higher channel roughness thus lower roughness srcs tend to underpredict poor performance in urban environments can be associated to the manning s n this is coherent with the findings of scriven et al 2021 who concluded that using appropriated manning s n results in lower nrmse in heavily urbanized catchments godbout et al 2019 study presents a general trend of srcs overpredicting the stage height which can be attributed to the fact that the study focused on few reaches in texas lying in relatively low relief and planar areas moreover the channel characteristics like cross section and slope tend to be more erratic in the mountainous regions thus higher rmse in these regions is also a byproduct of using reach averaged channel properties in src derivation on the other hand the channel roughness used in srcs and using reach average properties is a good approximation for plains in the midwestern united states comprising of ohio mid atlantic tennessee and upper mississippi huc02 regions the srcs therefore provide relatively more reliable estimates in this mid western region the relatively higher rmse values in rocky mountains also highlight the limitation of using the manning s equation and the uniform flow assumption specifically manning s equation does not hold good for very steep slopes and uniform flow conditions also do not hold valid for the streams with steep and irregular slopes the poor performance in coastal regions is coherent with the findings of garousi nejad et al 2019 godbout et al 2019 johnson et al 2019 the resolution of the underlying dem is another significant source of error in these conditions low resolution dem in low relief regions typically lack terrain convergence thus resulting in a poor hand raster which lies at the heart of src formation using more appropriate and different channel roughness and different reach averaging techniques for midwestern plains mountains and coastal areas can improve the reliability of srcs the deep neural network model shows promise to learn the complex relationship between the src performance and the water and stream characteristics one may think that model performance is affected by the data used in training and testing and would give different results if the data are randomly sampled differently to create a different training and testing dataset however the analysis is repeated several times randomly sampling the data into training and testing dataset each time and the performance of the model is found to be consistent this assures that the method can be used to reliably extrapolate the src performance to ungauged basins the bathymetry area is a key input to the model and the model performance decreases substantially if it is removed this emphasizes the fact that bathymetry plays a crucial role in the src reliability and the deep neural network model requires some parameter that can represent this bathymetry area even though the hand src method is prone to errors simplifying assumptions of uniform flow and applicability manning s equation are in fact much needed for effective regional and national scale flood mapping the incorporation of higher resolution terrain data and methods to incorporate river bathymetry into the higher resolution dems can significantly improve the reliability of this empirical method moreover estimating channel roughness for each reach is tedious but instead of using single constant channel roughness for all the reaches one can use different appropriated values for different regions for instance a higher channel roughness value can be used in the mountainous region and lower roughness value can be used in coastal areas there is also scope for significant improvement in neural network model performance using other algorithms and normalization techniques the dnn model performance is inconsistent for plains in kansas missouri and indiana as most stations in the region possess very similar characteristics however incorporation of some additional parameters which can significantly differentiate the channel and watershed characteristics would also increase the model performance in this region missing bathymetry area can also be estimated using the regional hydraulic geometry curves which relate the bankfull cross section area and width with the drainage area leopold and maddock 1953 dunne and leopold 1978 bieger et al 2015 blackburn lynch et al 2017 however the accuracy of bathymetry area estimated using regional hydraulic geometry curves would depend significantly on the accuracy of hydraulic geometry curves for the associated physiographic region appendix b other possible alternative to using the bathymetry area estimates in the neural network could be to train a second network that estimates the bathymetry area using channel characteristics and then feeds this estimated area to the first network a study on incorporating a similar structure wherein smaller networks learn intermediate physical parameters and then the outcome of these networks if fed as input to the main neural network was used for streamflow estimation and shows promising results khandelwal et al 2020 the performance of srcs varies significantly across conus with rmse ranging from 12 22 m to 314 44 m but most of the points lie confide to a range of 1 37 m to 1 37 m and a median rmse of 0 32 m the median values are encouraging but the range of rmse is high and suggests that even after incorporation of bathymetry estimates there is a need for improvement in the hand src method before it could be reliably used for real time flood forecasting at a continental scale the findings from this study are consistent with earlier results found for smaller areas garousi nejad et al 2019 godbout et al 2019 johnson et al 2019 which suggest a need for improvement and confirms the conclusion across conus the study improves upon these previous studies by segregating the error in srcs due to implicit assumptions and due to topography and then expanding the analysis to conus finally the results from this study show a general tendency of srcs to overpredict in coastal areas dominated with river deltas and to underpredict in mountainous regions further work to improve hand src model capability to wide catchment conditions particularly for coastal areas and steep slopes is recommended credit authorship contribution statement ankit ghanghas conceptualization methodology formal analysis data curation writing original draft writing review editing sayan dey methodology formal analysis writing review editing venkatesh merwade methodology formal analysis writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a as the srcs are based on manning s equation the sensitivity of these rating curves to different components of the manning s equation is first analyzed by assuming a theoretical trapezoidal channel cross section with a base channel width of 10 m a side slope of 1 5 1 for the main channel and a side slope of 4 1 h v for the banks floodplains a channel slope of 0 0008 and manning s n of 0 05 is assumed for the entire cross section with uniform flow conditions fig a1 a to analyze the effect of missing bathymetry different channel slope and different manning s n rating curves are produced while varying individual parameters independently fig a1 b a reduction in the channel roughness results in a decreased water depth for any discharge while an increase in roughness results in an increased water depth the extent of increase or decrease in water depth due to varying roughness is more prominent at higher discharges establishing that high flows are more sensitive to channel roughness in comparison to the low flows fig a1 b and table a1 similarly it is found that the depth of water for any discharge decreases as the channel slope increases and that that high flows are relatively more sensitive to changes in channel slope fig a1 b and table a1 on the other hand the impact of adding missing bathymetry is significant for low flows and it diminishes with increasing discharge this theoretical exploration establishes that manning s roughness and channel slope affects the srcs at high flows whereas missing bathymetry affects the srcs at low flows the effect of manning s roughness and slope is relatively insignificant on src for low flows appendix b appendix b presents the comparison of the estimated bathymetric area acorr calculated as per section 3 1 with empirical bathymetry area ab calculated using hydraulic geometry curves based on the parameters presented in blackburn lynch et al 2017 now the hydraulic geometry curves presented in blackburn lynch et al 2017 do not provide a direct estimate of the bathymetry they relate the drainage area to the bankfull width and bankfull area estimating the bathymetry area using the hydraulic geometry is a two step process first for any stream the bankfull area abkf and bankfull width wbkf are calculated using the empirical equations second estimate the stage height h0 when the top width estimated from the dem is equal to wbkf for this stage height h0 estimate the cross section area using the dem a0 the empirical bathymetry area ab is then estimated as ab abkf a0 the empirical hydraulic geometry curves and their accuracy vary across the different physiographic regions in the us so the srcs were clustered into respective physiographic regions and fig b1 and table b1 present the relation between acorr and ab for one region with good accuracy of empirical hydraulic curves one with moderate accuracy and one with poor accuracy of the empirical hydraulic curves fig b1 here shows that the estimated bathymetry area acorr can be directly calculated using the hydraulic geometry curves however the accuracy of calculating the estimated bathymetry area acorr is a function of the accuracy of the hydraulic geometry curves and varies across physiographic regions ab empirical bathymetry area is a combination of both abkf and wbkf and therefore for a given physiographic region the accuracy of the resulting empirical bathymetry area would be relatively less than the accuracy of calculating abkf and wbkf comparing r2 values for different provinces in fig b1 and table b1 
3687,rainfall runoff modeling is of great importance in hydrological sciences several different models have been developed for runoff modeling in three main categories i e physically based conceptual and empirical models data driven models are of the most widely used models in runoff modeling besides process based models different studies have been done to assess the performance of various models and the effect of input datasets data length and disparate signal processing methods on the modeling performance however each of these studies has examined one of these factors separately and didn t assess the effect of these factors on the accuracy of runoff forecasting therefore assessing the importance of each of the mentioned factors as well as determining the optimum structure that produces the best accuracy is still challenging the main aim of this study was to determine the importance and the optimal combination of these factors in daily runoff modeling in order to achieve this goal taguchi method was used first five levels were defined for each of the abovementioned factors five different input data combinations five data driven models i e adaptive neuro fuzzy inference system anfis support vector regression svr group method of data handling gmdh random forest rf and partial least square regression pls four different signal processing methods i e normalization wavelet ensemble empirical mode decomposition eemd andsingular spectrum analysis ssa as well as no pre processing condition and five data lengths i e 2 5 10 15 and 20 years were considered the l25 taguchi orthogonal array was selected accordingly the required 25 tests were implemented according to the l25 taguchi orthogonal array in three different basins to achieve more generalizable results the results were then used in taguchi analysis in order to attain the optimal combination of the levels of the mentioned factors and the importance of these factors in accurate prediction of runoff results showed that the hybrid wavelet gmdh model with a complete dataset as input and 20 year data length provides the highest accuracy it was also shown that the order of mentioned factors in terms of their importance and effect on runoff prediction accuracy is as follow input dataset data length preprocessing and model type gmdh and svr had the best performance and wavelet and eemd signal processing methods had the highest effect on the data driven models performance keywords artificial intelligence data driven model optimization signal processing taguchi method 1 introduction precise estimation and prediction of runoff is an integral part of hydrological modeling rainfall runoff process is one of the most important parts of the hydrologic cycle however this process is very complex and has different uncertainties rainfall runoff is a challenging task and has straight practical applications especially in ungauged or poorly gauged catchments zhang et al 2015 runoff modeling can be used for different purposes such as designing hydraulic structures estimating environmental flow flood and drought mitigation purposes water resources management etc li et al 2014 parajka et al 2013 several different models have been developed to estimate or predict runoff in a catchment these models can be classified into three main categories i e empirical conceptual and physically based models efficient data driven models are extensively used in different studies such as najafzadeh and niazmardi 2021 najafzadeh et al 2021 these models are also widely used for runoff estimation and prediction bartoletti et al 2018 jain and kumar 2007 morales et al 2021 negi et al 2021 okkan et al 2021 safari et al 2020 sedighi et al 2016 tikhamarine et al 2020 wu and chau 2011 however because of some deficiencies in data driven models such as their limitations in handling non stationary data several researches have coupled these models with pre processing approaches to cope with this shortcoming in different fields himanshu et al 2017 kim and valdés 2003 nourani et al 2014 different pre processing methods such as signal processing approaches e g wavelet decomposition lei et al 2021 moosavi et al 2017 moosavi et al 2013 nayak et al 2013 nguyen et al 2020 quilty and adamowski 2020 seo and kim 2016 wu et al 2021 singularspectrumanalysis ssa unnikrishnan and jothiprakash 2018 wu and chau 2011 zubaidi et al 2018 and empirical mode decomposition emd ali et al 2020 huang et al 2021 rezaie balf et al 2019 have been used in different studies some other researchers focused on the selection of input data and its effect on the performance of the data driven models bowden et al 2005 fernando et al 2009 sharma 2000 a few number of studies also highlighted the effect of data length on the performance of data driven models boughton 2007 xu and vandewiele 1994 as shown before a huge number of studies have been conducted to compare the performance of different models the effect of different preprocessing methods and also different input variables and data sizes however each of the above mentioned studies have considered one of the main aspects of the modeling process i e input data data length model type and preprocessing approaches they showed that different models can produce different results and different preprocessing methods input variables and data sizes can affect the performance of the models to some extent however to the best of our knowledge no study has been conducted to evaluate the effect of input data model type preprocessing and data length on daily runoff forecasting accuracy and to optimize these variables however assessing the importance and effect of these factors on the forecasting accuracy and determining the optimum combination needs performing a huge number of modeling tests using a full factorial design it would be a very time consuming work therefore fractional factorial designs can be good alternative to cope with this problem in a full factorial design all possible combination of factors and their underlying levels are considered but in fractional factorial designs only a limited portion of the mentioned combinations are taken into account taguchi method was developed as a fractional factorial design to minimize the overall testing time and experimental costs taguchi 1990 several studies have been conducted using this method in different fields such as soil erosion moosavi and sadeghi 2021 sadeghi et al 2012 electronics terzioğlu 2020 chemistry terzioğlu 2020 industry arıcı and keleştemur 2019 avikal et al 2020 mining sivakumar et al 2015 biology khusro et al 2018 remote sensing moosavi et al 2014 and several other research fields this method maybe useful for determination of the factors importance in rainfall runoff modeling and to optimize the combination of affecting factors the main goal of this study was to evaluate the importance of these components on the accuracy of daily runoff forecasting and to find the optimum combination of the mentioned components our initial hypothesis was that the model type and preprocessing would have the highest effect on the runoff forecasting accuracy and the optimum combination of the mentioned factors would be gmdh model combined with eemd signal processing approach and a complete set of input data and a 20 year data length 2 materials and methods 2 1 study area and data the study was performed in three watersheds i e kasilian watershed latian dam watershed and bar erieh watershed with different physical and climatological conditions fig 1 shows the location besides the 3 d demonstration of these basins the kasilian watershed part a with an area of 66 75 km2 is located in northern part of iran between 53 8 to 53 15 e and 35 58 to 36 15 n a great portion of the area is covered by forests and the average precipitation slope and elevation of this watershed are 791 mm 15 8 and 1691 m respectively bar erieh watershed part b is located in the eastern part of iran and covers an area of 113 km2 this basin has an average elevation precipitation and slope of 2226 m 330 mm and 11 9 respectively latian dam watershed part c with an area of 436 km2 is located in tehran province between 51 23 to 51 49 e and 35 46 to 36 03 n this watershed has a severe and rugged topography the average precipitation slope and elevation of this watershed are 867 mm 45 6 and 2830 m respectively climatic and hydrometric data including rainfall air temperature evaporation air humidity wind speed and discharge data were obtained from iran meteorological organization iran water resources management company and regional water authorities as the signal processing methods such as wavelet transform and eemd are used in this study the signals original datasets such as discharge rainfall etc should be remained in their original form the data were divided into two train and test datasets using a 70 30 ratio in order to refrain from systematic errors the 30 percent test data were selected from the first and the last part of the signal and the average of the errors were considered as the error for the modeling as the performance of different models are compared in this study the same datasets were used to test the performance of the models 2 2 design of experiment fig 2 shows the flowchart of the study as shown in this flowchart the factors affecting the accuracy of daily runoff forecasting and the levels of each factor are defined in the first step in this study as mentioned in the introduction section four important factors were considered i e input data model type pre processing and data length for each factor 5 levels were defined table 1 shows the factors and levels used in this study several different variables can be used in the runoff modeling process a comprehensive literature review on runoff modeling were performed and the most important variables were selected accordingly in fact the most important variables in the hydrologic cycle were selected five different data combinations were defined as input data by adding important variables step by step the first combination is the simplest one by considering only precipitation as input and the last combination is the complete one by considering several variables i e precipitation besides its time lags i e precipitation for previous days temperature air humidity wind speed discharge besides its time lags i e discharge for previous days antecedent precipitation index and antecedent discharge index the mentioned combinations are listed below a p t b p t t t e t h t and ws t c p t t t e t h t ws t p t 1 and p t 2 d p t t t e t h t ws t p t 1 p t 2 d t d t 1 and d t 2 e p t t t e t h t ws t p t 1 p t 2 d t d t 1 d t 2 api and adi in which p t e h ws d api and adi stand for precipitation temperature evaporation air humidity wind speed discharge antecedent precipitation index and antecedent discharge index api and adi were calculated using equations 1 and 2 kohler and linsley 1951 1 api t 1 m p t k t 2 adi t 1 m d t k t where pt and dt are precipitation and discharge in the tth antecedent day m is the statistical number of antecedent days k is the decay constant five different data driven models were used in this study i e adaptive network based fuzzy inference system anfis support vector regression svr group method of data handling gmdh partial least square regression pls and random forest rf these models were used to relatively cover different types of data driven models anfis is a combination of artificial neural network and fuzzy logic svr is a data driven model which works based on kernel functions and minimizes structural risk instead of just minimizing error gmdh works based on binomials and has a layered structure in which the best nodes are selected in each layer linear regression is one of the simplest and most common data driven models and random forest is a kind of decision trees in addition four different preprocessing approaches i e normalization wavelet transform ensemble empirical mode decomposition eemd andsingular spectrum analysis ssa as well as a standard form i e no preprocessing were used in this study five different data lengths i e 2 5 10 15 and 20 years were also considered to assess the effect of data length on the accuracy of runoff prediction as mentioned there are four factors each of which has five levels according to these numbers of factors and levels l25 taguchi orthogonal array were selected to define the required tests table 2 shows the l25 taguchi orthogonal array used in this study the numbers in this table show the level of each factor for example for test 11 number 3 for data means the third data combination i e p t t t e t h t ws t p t 1 and p t 2 number 1 for model means the first model i e anfis number 3 for preprocessing means the third preprocessing method i e wavelet and number 5 for data length means the fifth data length i e 20 years see flowchart in fact in this test runoff modeling is performed using the combined wavelet anfis model with 20 years data of the mentioned data combination 2 3 models and preprocessing approaches 2 3 1 models in this section a brief explanation about the above mentioned models is provided as mentioned before 5 models i e anfis svr gmdh pls and rf were used in this study adaptive network based fuzzy inference system or anfis is the combination of fuzzy logic and neural network model jang 1993 fuzzy logic works based on if then rules according to the human knowledge while neural network is a data driven model that can extract the relations between dependent and independent variables from the data combination of these two methods can be used to provide an adaptive neuro fuzzy inference system anfis has five layers a in the first layer membership functions and membership grades of inputs are determined b in the second layer the weights of membership functions are calculated c in the third layer which is called rule layer the relative weights of the rules are computed d the outputs of the forth layer that is called defuzzification layer are the product of a first order polynomial and the outputs of the third layer e the final layer is the output layer which is the summation of all incoming signals in this study several membership functions i e trapmf gbellmf trimf gaussmf were tested to find the best membership function in the anfis structure anfis training process was done by updating tunable parameters which are membership function parameters a b c in case of triangular membership function and consequent parameters p q r anfis models with grid partitioning genfis1 subtractive clustering genfis2 and fuzzy c mean clustering genfis3 were implemented in this study different number of membership functions were also tested the anfis parameters were optimized by testing different combination of the mentioned parameters using a trial and error paradigm support vector regression svr is the regression form of support vector machine svm svm is used for classification problems and svr is used for regression problems svr is one of the most important data driven models which can determine the relations between dependent and independent variables properly this method can minimize the structural risk instead of error vapnik 1995 it is used to solve non linear problems and its main function can be defined as equation 3 3 f x w t φ x b in which f x shows the estimated data w shows the vector containing weights and b demonstrates the bias penalty parameter c and kernel function are the main parameters in the structure of the svr c is the degree of punishment that is performed to assess the incorrectly divided sample it is a compromise between the algorithm complexity and degree of wrongly classified data small values of c shows small punishments and larger values of c shows the larger empirical error penalty and higher experience risk therefore it is of great importance to select the appropriate punishment coefficient different values for c were tested in order to obtain the optimum c value one of the other important factors in the performance of svr model is kernel function different kernel functions i e linear polynomial radial basis and gauss kernel functions were tested to select the best kernel function gmdh is also an important data driven model which works based on the polynomials and is widely used in different fields najafzadeh et al 2013a najafzadeh et al 2013b najafzadeh et al 2013c najafzadeh and barani 2011 in this method the relation between dependent and independent variables is extracted using a polynomial series called volterra series volterra 2005 that can be written in a discrete form as kolmogorov gabor polynomial using equation 4 ivakhnenko 1971 4 y a 0 i n a i x i i n i n a ij x i x j i n i n i n a ijk x i x j x k in which x a and n denote inputs coefficients and the number of inputs respectively in this method nodes that have a performance less than a specific threshold are eliminated from importing to the next layer the main parameters of the structure of gmdh i e such as leverage α which controls the threshold for selecting neurons in each layer the number of network layers and maximum acceptable number of neurons in each layer were optimized by testing different values in a trial and error manner several different combination of these parameters were tested to obtain the best structure that produces the least amount of error partial least square regression pls was used in this study as a robust regression method pls is a combination of principle component analysis and linear regression this method decreases the number of independent variables to a set of uncorrelated components then it performs the least squares regression on the produced components instead of performing it on the raw data haenlein and kaplan 2004 random forest rf was the other advanced data driven approach that was used in this study it uses bagging methodology to average noise and minimize the variance rf is a tree based regression method in which tree can grow adequately deep to find the relations between dependent and independent variables breiman 2001 rf can combine abundant decision trees and provide a robust forest a decision tree can be defined as an algorithm with the ability to include recursive separating of data into some categories by a series of splitting rules the most instinctive advantage of rf is its ability to cope with the ordinary regression s disadvantage of overfitting in order to optimize the structure of the rf model there are several hyper parameters that should be tuned i e the number of decision trees number of estimators that are generally correlated to the size of data the maximum levels permitted in a decision tree maximum depth that if is set to nothing the decision tree will continue splitting to reach purity bootstrap minimum samples split which is used to decide the minimum number of samples needed for splitting an internal node and minimum samples leaf which sets the minimum number of data point requirements in a node like other methods these parameters were tuned using a trial and error method by testing a huge number of combinations of these parameters 2 3 2 preprocessing approaches as mentioned before four signal processing methods i e normalization wavelet transform ensemble empirical mode decomposition and singularspectrumanalysis were implemented in this study in normalization the input data were re ranged in a specific range as the range of different input data are sometimes meaningfully different and this may cause some problems for models normalization can homogenized input data this preprocessing method can prevent domination of small range variables by large range variables in this study a simple and efficient min max normalization method were used min max normalization is one of the most common normalization methods used to transform data to a new range regularly between 0 and 1 crone et al 2006 the following equation were used to normalize input data 5 n n min r max r min r newmax r newmin r newmin r in which n is the normalized data n is the original data min r and max r are the minimum and maximum value of the original data newmax r and newmin r are the maximum and minimum value for the new range as the data were going to be rearranged to 0 1 newmax r and newmin r were considered 1 and 0 respectively wavelet transform is the other signal processing method that were used in combination with data driven models this method was used to decompose the original signals i e input variables this decomposition consists of high pass and low pass filters and produces detail and approximation coefficients using following equation cohen and kovacevic 1996 6 c j k f x ψ j k x d x f x j k c j k ψ j k x in which ψj k x is calculated using a mother wavelet ψ x which is expanded by j and translated by k and cj k is the approximate coefficient of a signal two kinds of coefficients are produced performing wavelet transform i e approximation and detail coefficients approximation coefficients show low frequency fluctuations of the original data while detail coefficients demonstrates the high frequency fluctuations of the original data different mother wavelets i e haar daubechies coiflets symlets morlet and meyer and different decomposition levels were tested to optimize the structure of wavelet transformation using a trial and error approach eemd is the next signal processing method that was used to decompose original data this method decomposes the original signal into a number of intrinsic mode functions imfs manjula and sarma 2012 it can cope with the mode mixing which is a specific imf that includes signals with completely different scales or a signal of the same scale appears in changed imf components as the main problem of emd method ren et al 2014 in the first step a white noise series were added to the original signal then the decomposition was performed on the produced signal using eemd the mentioned steps were repetitively done by different white noises to produce the equivalent imf components the last signal processing method used in this study was singular spectrum analysis ssa this method decomposes the original signal using an embedding operation and a singular value decomposition in the embedding operation a matrix x was produced as follow 7 x y 1 y 2 y k y 2 3 y k 1 y l y l 1 y n in the next step singular value decomposition were performed on the matrix 8 y i 1 d λ i u i v i t where u i and v i are the left and right singular vectors of x respectively and λ i is the eigenvalues of x in this equation λ i u i v i t can be considered as the ith triple or eigentriple 2 4 performance evaluation different evaluation criteria are usually used in order to assess the performance of rainfall runoff models coefficient of determination r2 normalized rmse nrmse and nash sutcliffe model efficiency coefficient nse were calculated for the 25 tests offered by taguchi orthogonal array 9 r 2 i 1 n o i o e i e i 1 n o i o 2 i 1 n e i e 2 2 10 nrmse i 1 n o i e i 2 n range o f o b s e r v e d d a t a 11 nse 1 t 1 n o i e i 2 t 1 n o i o 2 in which o denotes observed runoff and e shows estimated runoff as the range of data in different tests are different and it can affect the performance of the models nrmse was used as the final value to compare the performance of the models therefore nrmse values were imported to taguchi analysis the uncertainty and reliability of the results were assessed using the u95 which can be calculated using the following equations saberi movahed et al 2020 12 u 95 1 96 n i 1 n d i o b s e r v e d d i o b s e r v e d 2 i 1 n d i o b s e r v e d d i p r e d i c t e d 2 where u95 is the uncertainty at level of 95 d i o b s e r v e d is the observed discharge and d i p r e d i c t e d is the predicted discharge 13 reliability 100 n i 1 n k i where k i is obtained by calculating the relative error 14 relative e r r o r d i o b s e r v e d d i p r e d i c t e d d i o b s e r v e d if the relative error is less than a specific threshold then k i is equal to 1 and otherwise k i is equal to 0 the optimum value of 0 2 where considered based on saberi movahed et al 2020 2 5 taguchi analysis taguchi uses an analysis of the signal to noise s n ratio to assess the experimental results three types of s n ratio can be defined in taguchi analysis i e higher is better lower is better and the nominal is the best as the goal was to minimize the error lower is better s n ratio was used in this study 15 s n s 10 l o g 10 1 n y i 2 in which n shows the number of repetitions under the same experimental conditions 3 basins in this study and y shows the measured results i e nrmse for each test then an analysis of variance anova was implemented to assess the weight or the importance of each factor on the accuracy of daily runoff prediction the importance of the factors wf was calculated using following equations taguchi 1990 16 w f s s f d o f f v er s s t 100 where ssf was calculated as 17 s s f mn l k 1 l y f k y t 2 in which m denotes the number of tests run in this study and n shows the number of replications in each test y f k shows the average value of the nrmses in a certain factor in the kth level doff is the degree of freedom for factors ver sst and yt are also computed using following equations 18 v er s s t f a d s s f m n 1 19 s s t j 1 m i 1 n y i 2 j m n y t 2 20 y t j 1 m i 1 n y i j mn 3 results and discussion as mentioned before taguchi gets only one evaluation criterion and as the range of data in different tests are different because of the different data lengths nrmse was selected as the best criteria to be imported to taguchi analysis using nrmse provides a situation in which the models are more comparable table 3 shows the results of the 25 tests offered by taguchi l25 orthogonal array maximum nrmse for kasilian latian dam and bar erieh watersheds are 171 74 423 13 and 196 46 and minimum nrmse for the mentioned basins are 28 51 16 78 and 19 9 respectively fig 3 shows the plots for test 22 which has the best average performance among the 25 tests as an example parts a b and c show the plots of kasilian latian dam and bar erieh basins respectively in each part there are 3 sections which are shown in greek numbers section i shows the variations of observed and predicted daily runoff during the time section ii shows the scatter plot of observed vs predicted daily runoff and the amount of r2 nash sutcliffe and nrmse for the model higher amounts of r2 shows that the model predictions are in good relation to the observed data and demonstrated the precision of the model lower amounts of nrmse and higher values of nash sutcliffe shows that the model predictions are close to the observed values and it exhibits the accuracy of the model part iii shows the estimated quantiles vs standard normal quantiles in this section when the data are close to the line it shows that the model is performing well in the next step the results were used to perform taguchi analysis fig 4 shows the main plot of means produced from taguchi method parts a b c and d in this plot shows the results for data model pre processing and data length respectively as part a of this plot shows the best result is related to the combination 5 and after that combination 4 axis y is the mean error and axis x shows different levels of the factor i e different input combinations in this section combinations 1 2 and 3 produced worse results the difference between combination 3 and 4 is higher than others it shows the effect of including discharge for previous days which can show the base flow or the antecedent situation of flow in the river to some extent when it was not included in the modeling the models couldn t perform satisfactorily it shows the importance of this input in the rainfall runoff modeling process it is demonstrated that time delays for input time series are very effective and can significantly enhance the performance of the models the difference between combination 4 and 5 is somewhat less and the better performance of combination 5 may be related to the use of antecedent precipitation and discharge indices high δ y δ x in this section denotes the importance of this factor it shows that changing input combinations produces significant changes in the prediction accuracy in fact this part shows that the variables used in the models as input can play a substantial role in data driven models part b of the graph shows the performance of different models it can be concluded that gmdh had the highest performance while pls had the worst performance the performance of the models are in this order gmdh svr rf anfis pls however it should be noted that these are overall results and doesn t mean that it is irrevocable it means that gmdh had a better average performance it may be related to the structure of the gmdh it can automatically select important independent variables among explanatory variables this method imports the inputs to the nodes of the first layer performs computations and selects the best nodes as inputs for the next layer this process is done in an iterative manner until reaching desired accuracy this selection process provides a good opportunity to reach better results and to avoid trapping in local optimal results when minimizing the error in the modeling process it is in accordance with the results of several other studies such as kim et al 2020 moosavi et al 2017 shahabi et al 2015 svr is robust to the outliers and high dimensional spaces but does not perform very well when the data set has more noise the performance of gmdh and svr are very similar however computation cost of the svr is significantly higher than gmdh in fact tuning the structural parameters of svr is more time consuming rather than gmdh therefore gmdh is relatively more efficient rather than svr despite their similar performance in terms of accuracy assessment criteria pls had the worst performance among the selected models it may be related to its simplicity and the complex relations between dependent and independent variables in rainfall runoff process pls is the simplest model used in this study however it couldn t deal with the high non linearity and non stationarity of the data in the rainfall runoff process this result is in accordance with those of different studies such as lee et al 2003 thissen et al 2004 part c shows the main plot of means for preprocessing it shows that wavelet transform and eemd had the best performance fig 5 shows the decomposition of the discharge signal by wavelet part a and eemd part b signal processing approaches as the best signal processing methods these methods could improve the performance of data driven models by extracting the components of the input signals and imported them to the models wavelet transform decomposes the input signals into high and low frequency components they can provide a synchronized localization in time and frequency domain this signal processing method have the ability to separate the fine details in a signal this method can reveal some hidden aspects of the input signal such as trends breakdown points and discontinuities wavelet transformation is a lossless process being lossless means that a signal can be decomposes using wavelet transformation and then reconstructed using the obtained coefficients to the original signal without losing any information however for fine analysis the computational cost is relatively high and determining the best mother wavelet and decomposition level can be somewhat time consuming eemd also provides a time frequency analysis of the original signal similar to wavelet transform it can decompose the signal into disparate frequency components the imfs produced in eemd method demonstrates different frequencies from high to low the last imf like the first component of wavelet transform called approximation shows the low frequency fluctuations in the signal other imfs show the components with higher frequencies the main advantage of eemd is that it does not require prearranged mathematical functions and parameters and can efficiently deal with non stationary signals part d of fig 4 shows the main plot of means for data length it demonstrated the great importance of data length in data driven models as shown in this part the worst performance belongs to 2 year data length and the best one belongs to 20 year data length it is reasonable because data driven models work based on a training process they should be trained using observed data in order to find the relations between dependent and independent variables therefore data length can significantly affect the performance of the data driven rainfall runoff models this analysis showed that a 2 year dataset cannot produce reasonable and satisfactory results the performance improvement can be seen by increasing data length there was a significant difference between 2 year and 5 year data length it shows that 2 year data length couldn t provide enough information for the data driven models to be trained it can be also determined that there was no significant difference between 15 and 20 year data length in terms of model performance it suggests that a 15 year data length can be acceptable in data scarce situations we tried to make sure that the hydro meteorological conditions are similar in different time series in order to avoid dissimilarities in the condition of different time series with different situations for shorter data lengths i e 2 and 5 year datasets more than one dataset were used and the average of the results were used as the final performance based on the main plot of means produced by taguchi analysis it can be concluded that a hybrid wavelet gmdh model with the first combination as input data and 25 year data length can provide the best results in terms of prediction accuracy fig 6 shows the results of runoff modeling using the optimized model structure for the three basins parts a b and c belongs to kasilian latian dam and bar erieh basins respectively this figure shows a great improvement in the performance comparing with all previous tests the uncertainty and reliability of the model for the three basins are 0 12 0 51 0 10 0 59 and 0 15 0 58 for kasilian basin bar erieh basin and latian dam basin respectively table 4 shows the y k f the average of the nrmse for a certain factor in the kth level after calculating y k f ssf sst and ver the weight or importance of each factor wf was calculated using equation 16 table 5 demonstrates final weights and importance of the factors table 5 shows that the most important factor affecting the accuracy of runoff prediction is input data and after that data length it means that when the input data are not satisfactory and do not include the main and most important explanatoryvariables models and preprocessing methods cannot provide acceptable results as the mentioned models are data driven models the length of data can play an important role the next important factor is preprocessing and then the model type 4 conclusion the main aim of this study was to determine the optimum input combination model type preprocessing and data length to predict daily runoff as well as assessing the weight or importance of each mentioned factor affecting the accuracy of daily runoff prediction as performing a full factorial design of experiment needs a large number of tests taguchi fractional factorial design of experiment was used in this study taguchi efficiently determined the optimum modeling structure and the weights of affecting factors using this method several factors can be optimized at the same time and more quantitative information can be obtained from fewer experimental trials this method is also straightforward and easy to apply it was shown that wavelet transform can be used efficiently to improve the performance of data driven models this study showed that the most important factor affecting the accuracy of runoff prediction is input data both from the aspect of the variables used in modeling process and the length of data however it should be taken into account that the results obtained from this method are only relative and do not indicate the absolute weight of the factors actually the lower weights of model and pre processing does not mean that these factors are not important their weights are lower in comparison with data combination and data length comparing the results of the optimized model with the simple gmdh model without preprocessing and considering combination 5 complete dataset and 20 year data length showed that wavelet processing could improve the runoff prediction accuracy from the aspect of nrmse from 20 43 29 02 and 60 82 to 20 27 13 57 and 22 91 for kasilian latian dam and bar erieh basins respectively it is a considerable improvement if the dataset and data length is satisfactory using more accurate models and preprocessing method can improve the prediction accuracy sometimes considerably the other important point is that these results are just achieved in three basins these results are coming from the results of the modeling in the three basins i e kasilian basin which is located in the northern parts of iran with cold and humid climate and mainly forest land cover which can be a representative of such areas in the world latian dam basin with a cold and semiarid climate and rough topography and bar erieh basin which is representative of arid areas however the results may change in different conditions to some extent this study can be done in more different basins with different physical and hydro climatological conditions so that the results are more generalizable credit authorship contribution statement vahid moosavi conceptualization methodology validation supervision writing original draft software zeinab gheisoori fard software resources visualization data curation mehdi vafakhah writing review editing formal analysis validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3687,rainfall runoff modeling is of great importance in hydrological sciences several different models have been developed for runoff modeling in three main categories i e physically based conceptual and empirical models data driven models are of the most widely used models in runoff modeling besides process based models different studies have been done to assess the performance of various models and the effect of input datasets data length and disparate signal processing methods on the modeling performance however each of these studies has examined one of these factors separately and didn t assess the effect of these factors on the accuracy of runoff forecasting therefore assessing the importance of each of the mentioned factors as well as determining the optimum structure that produces the best accuracy is still challenging the main aim of this study was to determine the importance and the optimal combination of these factors in daily runoff modeling in order to achieve this goal taguchi method was used first five levels were defined for each of the abovementioned factors five different input data combinations five data driven models i e adaptive neuro fuzzy inference system anfis support vector regression svr group method of data handling gmdh random forest rf and partial least square regression pls four different signal processing methods i e normalization wavelet ensemble empirical mode decomposition eemd andsingular spectrum analysis ssa as well as no pre processing condition and five data lengths i e 2 5 10 15 and 20 years were considered the l25 taguchi orthogonal array was selected accordingly the required 25 tests were implemented according to the l25 taguchi orthogonal array in three different basins to achieve more generalizable results the results were then used in taguchi analysis in order to attain the optimal combination of the levels of the mentioned factors and the importance of these factors in accurate prediction of runoff results showed that the hybrid wavelet gmdh model with a complete dataset as input and 20 year data length provides the highest accuracy it was also shown that the order of mentioned factors in terms of their importance and effect on runoff prediction accuracy is as follow input dataset data length preprocessing and model type gmdh and svr had the best performance and wavelet and eemd signal processing methods had the highest effect on the data driven models performance keywords artificial intelligence data driven model optimization signal processing taguchi method 1 introduction precise estimation and prediction of runoff is an integral part of hydrological modeling rainfall runoff process is one of the most important parts of the hydrologic cycle however this process is very complex and has different uncertainties rainfall runoff is a challenging task and has straight practical applications especially in ungauged or poorly gauged catchments zhang et al 2015 runoff modeling can be used for different purposes such as designing hydraulic structures estimating environmental flow flood and drought mitigation purposes water resources management etc li et al 2014 parajka et al 2013 several different models have been developed to estimate or predict runoff in a catchment these models can be classified into three main categories i e empirical conceptual and physically based models efficient data driven models are extensively used in different studies such as najafzadeh and niazmardi 2021 najafzadeh et al 2021 these models are also widely used for runoff estimation and prediction bartoletti et al 2018 jain and kumar 2007 morales et al 2021 negi et al 2021 okkan et al 2021 safari et al 2020 sedighi et al 2016 tikhamarine et al 2020 wu and chau 2011 however because of some deficiencies in data driven models such as their limitations in handling non stationary data several researches have coupled these models with pre processing approaches to cope with this shortcoming in different fields himanshu et al 2017 kim and valdés 2003 nourani et al 2014 different pre processing methods such as signal processing approaches e g wavelet decomposition lei et al 2021 moosavi et al 2017 moosavi et al 2013 nayak et al 2013 nguyen et al 2020 quilty and adamowski 2020 seo and kim 2016 wu et al 2021 singularspectrumanalysis ssa unnikrishnan and jothiprakash 2018 wu and chau 2011 zubaidi et al 2018 and empirical mode decomposition emd ali et al 2020 huang et al 2021 rezaie balf et al 2019 have been used in different studies some other researchers focused on the selection of input data and its effect on the performance of the data driven models bowden et al 2005 fernando et al 2009 sharma 2000 a few number of studies also highlighted the effect of data length on the performance of data driven models boughton 2007 xu and vandewiele 1994 as shown before a huge number of studies have been conducted to compare the performance of different models the effect of different preprocessing methods and also different input variables and data sizes however each of the above mentioned studies have considered one of the main aspects of the modeling process i e input data data length model type and preprocessing approaches they showed that different models can produce different results and different preprocessing methods input variables and data sizes can affect the performance of the models to some extent however to the best of our knowledge no study has been conducted to evaluate the effect of input data model type preprocessing and data length on daily runoff forecasting accuracy and to optimize these variables however assessing the importance and effect of these factors on the forecasting accuracy and determining the optimum combination needs performing a huge number of modeling tests using a full factorial design it would be a very time consuming work therefore fractional factorial designs can be good alternative to cope with this problem in a full factorial design all possible combination of factors and their underlying levels are considered but in fractional factorial designs only a limited portion of the mentioned combinations are taken into account taguchi method was developed as a fractional factorial design to minimize the overall testing time and experimental costs taguchi 1990 several studies have been conducted using this method in different fields such as soil erosion moosavi and sadeghi 2021 sadeghi et al 2012 electronics terzioğlu 2020 chemistry terzioğlu 2020 industry arıcı and keleştemur 2019 avikal et al 2020 mining sivakumar et al 2015 biology khusro et al 2018 remote sensing moosavi et al 2014 and several other research fields this method maybe useful for determination of the factors importance in rainfall runoff modeling and to optimize the combination of affecting factors the main goal of this study was to evaluate the importance of these components on the accuracy of daily runoff forecasting and to find the optimum combination of the mentioned components our initial hypothesis was that the model type and preprocessing would have the highest effect on the runoff forecasting accuracy and the optimum combination of the mentioned factors would be gmdh model combined with eemd signal processing approach and a complete set of input data and a 20 year data length 2 materials and methods 2 1 study area and data the study was performed in three watersheds i e kasilian watershed latian dam watershed and bar erieh watershed with different physical and climatological conditions fig 1 shows the location besides the 3 d demonstration of these basins the kasilian watershed part a with an area of 66 75 km2 is located in northern part of iran between 53 8 to 53 15 e and 35 58 to 36 15 n a great portion of the area is covered by forests and the average precipitation slope and elevation of this watershed are 791 mm 15 8 and 1691 m respectively bar erieh watershed part b is located in the eastern part of iran and covers an area of 113 km2 this basin has an average elevation precipitation and slope of 2226 m 330 mm and 11 9 respectively latian dam watershed part c with an area of 436 km2 is located in tehran province between 51 23 to 51 49 e and 35 46 to 36 03 n this watershed has a severe and rugged topography the average precipitation slope and elevation of this watershed are 867 mm 45 6 and 2830 m respectively climatic and hydrometric data including rainfall air temperature evaporation air humidity wind speed and discharge data were obtained from iran meteorological organization iran water resources management company and regional water authorities as the signal processing methods such as wavelet transform and eemd are used in this study the signals original datasets such as discharge rainfall etc should be remained in their original form the data were divided into two train and test datasets using a 70 30 ratio in order to refrain from systematic errors the 30 percent test data were selected from the first and the last part of the signal and the average of the errors were considered as the error for the modeling as the performance of different models are compared in this study the same datasets were used to test the performance of the models 2 2 design of experiment fig 2 shows the flowchart of the study as shown in this flowchart the factors affecting the accuracy of daily runoff forecasting and the levels of each factor are defined in the first step in this study as mentioned in the introduction section four important factors were considered i e input data model type pre processing and data length for each factor 5 levels were defined table 1 shows the factors and levels used in this study several different variables can be used in the runoff modeling process a comprehensive literature review on runoff modeling were performed and the most important variables were selected accordingly in fact the most important variables in the hydrologic cycle were selected five different data combinations were defined as input data by adding important variables step by step the first combination is the simplest one by considering only precipitation as input and the last combination is the complete one by considering several variables i e precipitation besides its time lags i e precipitation for previous days temperature air humidity wind speed discharge besides its time lags i e discharge for previous days antecedent precipitation index and antecedent discharge index the mentioned combinations are listed below a p t b p t t t e t h t and ws t c p t t t e t h t ws t p t 1 and p t 2 d p t t t e t h t ws t p t 1 p t 2 d t d t 1 and d t 2 e p t t t e t h t ws t p t 1 p t 2 d t d t 1 d t 2 api and adi in which p t e h ws d api and adi stand for precipitation temperature evaporation air humidity wind speed discharge antecedent precipitation index and antecedent discharge index api and adi were calculated using equations 1 and 2 kohler and linsley 1951 1 api t 1 m p t k t 2 adi t 1 m d t k t where pt and dt are precipitation and discharge in the tth antecedent day m is the statistical number of antecedent days k is the decay constant five different data driven models were used in this study i e adaptive network based fuzzy inference system anfis support vector regression svr group method of data handling gmdh partial least square regression pls and random forest rf these models were used to relatively cover different types of data driven models anfis is a combination of artificial neural network and fuzzy logic svr is a data driven model which works based on kernel functions and minimizes structural risk instead of just minimizing error gmdh works based on binomials and has a layered structure in which the best nodes are selected in each layer linear regression is one of the simplest and most common data driven models and random forest is a kind of decision trees in addition four different preprocessing approaches i e normalization wavelet transform ensemble empirical mode decomposition eemd andsingular spectrum analysis ssa as well as a standard form i e no preprocessing were used in this study five different data lengths i e 2 5 10 15 and 20 years were also considered to assess the effect of data length on the accuracy of runoff prediction as mentioned there are four factors each of which has five levels according to these numbers of factors and levels l25 taguchi orthogonal array were selected to define the required tests table 2 shows the l25 taguchi orthogonal array used in this study the numbers in this table show the level of each factor for example for test 11 number 3 for data means the third data combination i e p t t t e t h t ws t p t 1 and p t 2 number 1 for model means the first model i e anfis number 3 for preprocessing means the third preprocessing method i e wavelet and number 5 for data length means the fifth data length i e 20 years see flowchart in fact in this test runoff modeling is performed using the combined wavelet anfis model with 20 years data of the mentioned data combination 2 3 models and preprocessing approaches 2 3 1 models in this section a brief explanation about the above mentioned models is provided as mentioned before 5 models i e anfis svr gmdh pls and rf were used in this study adaptive network based fuzzy inference system or anfis is the combination of fuzzy logic and neural network model jang 1993 fuzzy logic works based on if then rules according to the human knowledge while neural network is a data driven model that can extract the relations between dependent and independent variables from the data combination of these two methods can be used to provide an adaptive neuro fuzzy inference system anfis has five layers a in the first layer membership functions and membership grades of inputs are determined b in the second layer the weights of membership functions are calculated c in the third layer which is called rule layer the relative weights of the rules are computed d the outputs of the forth layer that is called defuzzification layer are the product of a first order polynomial and the outputs of the third layer e the final layer is the output layer which is the summation of all incoming signals in this study several membership functions i e trapmf gbellmf trimf gaussmf were tested to find the best membership function in the anfis structure anfis training process was done by updating tunable parameters which are membership function parameters a b c in case of triangular membership function and consequent parameters p q r anfis models with grid partitioning genfis1 subtractive clustering genfis2 and fuzzy c mean clustering genfis3 were implemented in this study different number of membership functions were also tested the anfis parameters were optimized by testing different combination of the mentioned parameters using a trial and error paradigm support vector regression svr is the regression form of support vector machine svm svm is used for classification problems and svr is used for regression problems svr is one of the most important data driven models which can determine the relations between dependent and independent variables properly this method can minimize the structural risk instead of error vapnik 1995 it is used to solve non linear problems and its main function can be defined as equation 3 3 f x w t φ x b in which f x shows the estimated data w shows the vector containing weights and b demonstrates the bias penalty parameter c and kernel function are the main parameters in the structure of the svr c is the degree of punishment that is performed to assess the incorrectly divided sample it is a compromise between the algorithm complexity and degree of wrongly classified data small values of c shows small punishments and larger values of c shows the larger empirical error penalty and higher experience risk therefore it is of great importance to select the appropriate punishment coefficient different values for c were tested in order to obtain the optimum c value one of the other important factors in the performance of svr model is kernel function different kernel functions i e linear polynomial radial basis and gauss kernel functions were tested to select the best kernel function gmdh is also an important data driven model which works based on the polynomials and is widely used in different fields najafzadeh et al 2013a najafzadeh et al 2013b najafzadeh et al 2013c najafzadeh and barani 2011 in this method the relation between dependent and independent variables is extracted using a polynomial series called volterra series volterra 2005 that can be written in a discrete form as kolmogorov gabor polynomial using equation 4 ivakhnenko 1971 4 y a 0 i n a i x i i n i n a ij x i x j i n i n i n a ijk x i x j x k in which x a and n denote inputs coefficients and the number of inputs respectively in this method nodes that have a performance less than a specific threshold are eliminated from importing to the next layer the main parameters of the structure of gmdh i e such as leverage α which controls the threshold for selecting neurons in each layer the number of network layers and maximum acceptable number of neurons in each layer were optimized by testing different values in a trial and error manner several different combination of these parameters were tested to obtain the best structure that produces the least amount of error partial least square regression pls was used in this study as a robust regression method pls is a combination of principle component analysis and linear regression this method decreases the number of independent variables to a set of uncorrelated components then it performs the least squares regression on the produced components instead of performing it on the raw data haenlein and kaplan 2004 random forest rf was the other advanced data driven approach that was used in this study it uses bagging methodology to average noise and minimize the variance rf is a tree based regression method in which tree can grow adequately deep to find the relations between dependent and independent variables breiman 2001 rf can combine abundant decision trees and provide a robust forest a decision tree can be defined as an algorithm with the ability to include recursive separating of data into some categories by a series of splitting rules the most instinctive advantage of rf is its ability to cope with the ordinary regression s disadvantage of overfitting in order to optimize the structure of the rf model there are several hyper parameters that should be tuned i e the number of decision trees number of estimators that are generally correlated to the size of data the maximum levels permitted in a decision tree maximum depth that if is set to nothing the decision tree will continue splitting to reach purity bootstrap minimum samples split which is used to decide the minimum number of samples needed for splitting an internal node and minimum samples leaf which sets the minimum number of data point requirements in a node like other methods these parameters were tuned using a trial and error method by testing a huge number of combinations of these parameters 2 3 2 preprocessing approaches as mentioned before four signal processing methods i e normalization wavelet transform ensemble empirical mode decomposition and singularspectrumanalysis were implemented in this study in normalization the input data were re ranged in a specific range as the range of different input data are sometimes meaningfully different and this may cause some problems for models normalization can homogenized input data this preprocessing method can prevent domination of small range variables by large range variables in this study a simple and efficient min max normalization method were used min max normalization is one of the most common normalization methods used to transform data to a new range regularly between 0 and 1 crone et al 2006 the following equation were used to normalize input data 5 n n min r max r min r newmax r newmin r newmin r in which n is the normalized data n is the original data min r and max r are the minimum and maximum value of the original data newmax r and newmin r are the maximum and minimum value for the new range as the data were going to be rearranged to 0 1 newmax r and newmin r were considered 1 and 0 respectively wavelet transform is the other signal processing method that were used in combination with data driven models this method was used to decompose the original signals i e input variables this decomposition consists of high pass and low pass filters and produces detail and approximation coefficients using following equation cohen and kovacevic 1996 6 c j k f x ψ j k x d x f x j k c j k ψ j k x in which ψj k x is calculated using a mother wavelet ψ x which is expanded by j and translated by k and cj k is the approximate coefficient of a signal two kinds of coefficients are produced performing wavelet transform i e approximation and detail coefficients approximation coefficients show low frequency fluctuations of the original data while detail coefficients demonstrates the high frequency fluctuations of the original data different mother wavelets i e haar daubechies coiflets symlets morlet and meyer and different decomposition levels were tested to optimize the structure of wavelet transformation using a trial and error approach eemd is the next signal processing method that was used to decompose original data this method decomposes the original signal into a number of intrinsic mode functions imfs manjula and sarma 2012 it can cope with the mode mixing which is a specific imf that includes signals with completely different scales or a signal of the same scale appears in changed imf components as the main problem of emd method ren et al 2014 in the first step a white noise series were added to the original signal then the decomposition was performed on the produced signal using eemd the mentioned steps were repetitively done by different white noises to produce the equivalent imf components the last signal processing method used in this study was singular spectrum analysis ssa this method decomposes the original signal using an embedding operation and a singular value decomposition in the embedding operation a matrix x was produced as follow 7 x y 1 y 2 y k y 2 3 y k 1 y l y l 1 y n in the next step singular value decomposition were performed on the matrix 8 y i 1 d λ i u i v i t where u i and v i are the left and right singular vectors of x respectively and λ i is the eigenvalues of x in this equation λ i u i v i t can be considered as the ith triple or eigentriple 2 4 performance evaluation different evaluation criteria are usually used in order to assess the performance of rainfall runoff models coefficient of determination r2 normalized rmse nrmse and nash sutcliffe model efficiency coefficient nse were calculated for the 25 tests offered by taguchi orthogonal array 9 r 2 i 1 n o i o e i e i 1 n o i o 2 i 1 n e i e 2 2 10 nrmse i 1 n o i e i 2 n range o f o b s e r v e d d a t a 11 nse 1 t 1 n o i e i 2 t 1 n o i o 2 in which o denotes observed runoff and e shows estimated runoff as the range of data in different tests are different and it can affect the performance of the models nrmse was used as the final value to compare the performance of the models therefore nrmse values were imported to taguchi analysis the uncertainty and reliability of the results were assessed using the u95 which can be calculated using the following equations saberi movahed et al 2020 12 u 95 1 96 n i 1 n d i o b s e r v e d d i o b s e r v e d 2 i 1 n d i o b s e r v e d d i p r e d i c t e d 2 where u95 is the uncertainty at level of 95 d i o b s e r v e d is the observed discharge and d i p r e d i c t e d is the predicted discharge 13 reliability 100 n i 1 n k i where k i is obtained by calculating the relative error 14 relative e r r o r d i o b s e r v e d d i p r e d i c t e d d i o b s e r v e d if the relative error is less than a specific threshold then k i is equal to 1 and otherwise k i is equal to 0 the optimum value of 0 2 where considered based on saberi movahed et al 2020 2 5 taguchi analysis taguchi uses an analysis of the signal to noise s n ratio to assess the experimental results three types of s n ratio can be defined in taguchi analysis i e higher is better lower is better and the nominal is the best as the goal was to minimize the error lower is better s n ratio was used in this study 15 s n s 10 l o g 10 1 n y i 2 in which n shows the number of repetitions under the same experimental conditions 3 basins in this study and y shows the measured results i e nrmse for each test then an analysis of variance anova was implemented to assess the weight or the importance of each factor on the accuracy of daily runoff prediction the importance of the factors wf was calculated using following equations taguchi 1990 16 w f s s f d o f f v er s s t 100 where ssf was calculated as 17 s s f mn l k 1 l y f k y t 2 in which m denotes the number of tests run in this study and n shows the number of replications in each test y f k shows the average value of the nrmses in a certain factor in the kth level doff is the degree of freedom for factors ver sst and yt are also computed using following equations 18 v er s s t f a d s s f m n 1 19 s s t j 1 m i 1 n y i 2 j m n y t 2 20 y t j 1 m i 1 n y i j mn 3 results and discussion as mentioned before taguchi gets only one evaluation criterion and as the range of data in different tests are different because of the different data lengths nrmse was selected as the best criteria to be imported to taguchi analysis using nrmse provides a situation in which the models are more comparable table 3 shows the results of the 25 tests offered by taguchi l25 orthogonal array maximum nrmse for kasilian latian dam and bar erieh watersheds are 171 74 423 13 and 196 46 and minimum nrmse for the mentioned basins are 28 51 16 78 and 19 9 respectively fig 3 shows the plots for test 22 which has the best average performance among the 25 tests as an example parts a b and c show the plots of kasilian latian dam and bar erieh basins respectively in each part there are 3 sections which are shown in greek numbers section i shows the variations of observed and predicted daily runoff during the time section ii shows the scatter plot of observed vs predicted daily runoff and the amount of r2 nash sutcliffe and nrmse for the model higher amounts of r2 shows that the model predictions are in good relation to the observed data and demonstrated the precision of the model lower amounts of nrmse and higher values of nash sutcliffe shows that the model predictions are close to the observed values and it exhibits the accuracy of the model part iii shows the estimated quantiles vs standard normal quantiles in this section when the data are close to the line it shows that the model is performing well in the next step the results were used to perform taguchi analysis fig 4 shows the main plot of means produced from taguchi method parts a b c and d in this plot shows the results for data model pre processing and data length respectively as part a of this plot shows the best result is related to the combination 5 and after that combination 4 axis y is the mean error and axis x shows different levels of the factor i e different input combinations in this section combinations 1 2 and 3 produced worse results the difference between combination 3 and 4 is higher than others it shows the effect of including discharge for previous days which can show the base flow or the antecedent situation of flow in the river to some extent when it was not included in the modeling the models couldn t perform satisfactorily it shows the importance of this input in the rainfall runoff modeling process it is demonstrated that time delays for input time series are very effective and can significantly enhance the performance of the models the difference between combination 4 and 5 is somewhat less and the better performance of combination 5 may be related to the use of antecedent precipitation and discharge indices high δ y δ x in this section denotes the importance of this factor it shows that changing input combinations produces significant changes in the prediction accuracy in fact this part shows that the variables used in the models as input can play a substantial role in data driven models part b of the graph shows the performance of different models it can be concluded that gmdh had the highest performance while pls had the worst performance the performance of the models are in this order gmdh svr rf anfis pls however it should be noted that these are overall results and doesn t mean that it is irrevocable it means that gmdh had a better average performance it may be related to the structure of the gmdh it can automatically select important independent variables among explanatory variables this method imports the inputs to the nodes of the first layer performs computations and selects the best nodes as inputs for the next layer this process is done in an iterative manner until reaching desired accuracy this selection process provides a good opportunity to reach better results and to avoid trapping in local optimal results when minimizing the error in the modeling process it is in accordance with the results of several other studies such as kim et al 2020 moosavi et al 2017 shahabi et al 2015 svr is robust to the outliers and high dimensional spaces but does not perform very well when the data set has more noise the performance of gmdh and svr are very similar however computation cost of the svr is significantly higher than gmdh in fact tuning the structural parameters of svr is more time consuming rather than gmdh therefore gmdh is relatively more efficient rather than svr despite their similar performance in terms of accuracy assessment criteria pls had the worst performance among the selected models it may be related to its simplicity and the complex relations between dependent and independent variables in rainfall runoff process pls is the simplest model used in this study however it couldn t deal with the high non linearity and non stationarity of the data in the rainfall runoff process this result is in accordance with those of different studies such as lee et al 2003 thissen et al 2004 part c shows the main plot of means for preprocessing it shows that wavelet transform and eemd had the best performance fig 5 shows the decomposition of the discharge signal by wavelet part a and eemd part b signal processing approaches as the best signal processing methods these methods could improve the performance of data driven models by extracting the components of the input signals and imported them to the models wavelet transform decomposes the input signals into high and low frequency components they can provide a synchronized localization in time and frequency domain this signal processing method have the ability to separate the fine details in a signal this method can reveal some hidden aspects of the input signal such as trends breakdown points and discontinuities wavelet transformation is a lossless process being lossless means that a signal can be decomposes using wavelet transformation and then reconstructed using the obtained coefficients to the original signal without losing any information however for fine analysis the computational cost is relatively high and determining the best mother wavelet and decomposition level can be somewhat time consuming eemd also provides a time frequency analysis of the original signal similar to wavelet transform it can decompose the signal into disparate frequency components the imfs produced in eemd method demonstrates different frequencies from high to low the last imf like the first component of wavelet transform called approximation shows the low frequency fluctuations in the signal other imfs show the components with higher frequencies the main advantage of eemd is that it does not require prearranged mathematical functions and parameters and can efficiently deal with non stationary signals part d of fig 4 shows the main plot of means for data length it demonstrated the great importance of data length in data driven models as shown in this part the worst performance belongs to 2 year data length and the best one belongs to 20 year data length it is reasonable because data driven models work based on a training process they should be trained using observed data in order to find the relations between dependent and independent variables therefore data length can significantly affect the performance of the data driven rainfall runoff models this analysis showed that a 2 year dataset cannot produce reasonable and satisfactory results the performance improvement can be seen by increasing data length there was a significant difference between 2 year and 5 year data length it shows that 2 year data length couldn t provide enough information for the data driven models to be trained it can be also determined that there was no significant difference between 15 and 20 year data length in terms of model performance it suggests that a 15 year data length can be acceptable in data scarce situations we tried to make sure that the hydro meteorological conditions are similar in different time series in order to avoid dissimilarities in the condition of different time series with different situations for shorter data lengths i e 2 and 5 year datasets more than one dataset were used and the average of the results were used as the final performance based on the main plot of means produced by taguchi analysis it can be concluded that a hybrid wavelet gmdh model with the first combination as input data and 25 year data length can provide the best results in terms of prediction accuracy fig 6 shows the results of runoff modeling using the optimized model structure for the three basins parts a b and c belongs to kasilian latian dam and bar erieh basins respectively this figure shows a great improvement in the performance comparing with all previous tests the uncertainty and reliability of the model for the three basins are 0 12 0 51 0 10 0 59 and 0 15 0 58 for kasilian basin bar erieh basin and latian dam basin respectively table 4 shows the y k f the average of the nrmse for a certain factor in the kth level after calculating y k f ssf sst and ver the weight or importance of each factor wf was calculated using equation 16 table 5 demonstrates final weights and importance of the factors table 5 shows that the most important factor affecting the accuracy of runoff prediction is input data and after that data length it means that when the input data are not satisfactory and do not include the main and most important explanatoryvariables models and preprocessing methods cannot provide acceptable results as the mentioned models are data driven models the length of data can play an important role the next important factor is preprocessing and then the model type 4 conclusion the main aim of this study was to determine the optimum input combination model type preprocessing and data length to predict daily runoff as well as assessing the weight or importance of each mentioned factor affecting the accuracy of daily runoff prediction as performing a full factorial design of experiment needs a large number of tests taguchi fractional factorial design of experiment was used in this study taguchi efficiently determined the optimum modeling structure and the weights of affecting factors using this method several factors can be optimized at the same time and more quantitative information can be obtained from fewer experimental trials this method is also straightforward and easy to apply it was shown that wavelet transform can be used efficiently to improve the performance of data driven models this study showed that the most important factor affecting the accuracy of runoff prediction is input data both from the aspect of the variables used in modeling process and the length of data however it should be taken into account that the results obtained from this method are only relative and do not indicate the absolute weight of the factors actually the lower weights of model and pre processing does not mean that these factors are not important their weights are lower in comparison with data combination and data length comparing the results of the optimized model with the simple gmdh model without preprocessing and considering combination 5 complete dataset and 20 year data length showed that wavelet processing could improve the runoff prediction accuracy from the aspect of nrmse from 20 43 29 02 and 60 82 to 20 27 13 57 and 22 91 for kasilian latian dam and bar erieh basins respectively it is a considerable improvement if the dataset and data length is satisfactory using more accurate models and preprocessing method can improve the prediction accuracy sometimes considerably the other important point is that these results are just achieved in three basins these results are coming from the results of the modeling in the three basins i e kasilian basin which is located in the northern parts of iran with cold and humid climate and mainly forest land cover which can be a representative of such areas in the world latian dam basin with a cold and semiarid climate and rough topography and bar erieh basin which is representative of arid areas however the results may change in different conditions to some extent this study can be done in more different basins with different physical and hydro climatological conditions so that the results are more generalizable credit authorship contribution statement vahid moosavi conceptualization methodology validation supervision writing original draft software zeinab gheisoori fard software resources visualization data curation mehdi vafakhah writing review editing formal analysis validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3688,permeability is one of the most important properties in subsurface flow problems which measures the ability of rocks to transmit fluid normally permeability is determined through experiments and numerical simulations both of which are time consuming in this paper we propose a new effective method based on convolutional neural networks with physical information cnnphys to rapidly evaluate rock permeability from its three dimensional 3d image in order to obtain sufficient reliable labeled data rock image reconstruction is utilized to generate sufficient samples based on the joshi quiblier adler method next the corresponding permeability is calculated using the lattice boltzmann method we compare the prediction performance of cnnphys and convolutional neural networks cnns the results demonstrate that cnnphys achieves superior performance especially in the case of a small dataset and an out of range problem moreover the performance of both cnn and cnnphys is greatly improved combined with transfer learning in the case of an out of range problem this opens novel pathways for rapidly predicting permeability in subsurface applications keywords deep learning permeability prediction physical information small dataset out of range problem 1 introduction in subsurface flow problems such as groundwater flow and oil gas production geng et al 2020 wada et al 2010 permeability plays an important role since it measures the ability of rocks to transmit fluid the permeability of rocks is usually modeled through laboratory core analysis shi et al 2017 rock permeability can be obtained through experiments using darcy s law cui et al 2009 dicker and smits 1988 feng et al 2016 tinni et al 2013 zhang et al 2000b however the experiments are both time consuming and significantly constrained by laboratory conditions experiments may cause irreversible damage to cores it is time intensive and impractical to conduct a large number of experiments alternatively numerical methods are flexible in parameter settings and easy to repeat it is convenient to calculate the permeability of different rocks using numerical simulations zhang et al 2000a zhao et al 2020 the application of numerical simulations however is also restricted by time cost and available computational resources the pore network model pnm is introduced to reduce the computational cost of numerical simulations dong and blunt 2009 in pnm the pore structure is simplified with a few assumptions the pressure drop is neglected in pore bodies and throat cross sections are considered as definitive geometrical shapes such as circles squares and triangles in pnm however this simplification might have an impact on accuracy compared with experiments and simulations semi empirical equations e g the kozeny carman equation can be applied to predict permeability more rapidly akande et al 2017 olatunji 2010 the semi empirical formulas for specific rocks are fitted according to the rock particle distribution composition and deposition process the permeability can then be predicted based on porosity and other properties through formulas empirical formulas however have a limited range of applications and it is difficult for empirical expressions to deal with strong heterogeneities in recent years some researchers have attempted to use machine learning ml methods to deal with geophysical problems niu et al 2020 sun 2018 such as formation lithology classification mohamed et al 2019 seismic reservoir characterization liu et al 2019 and fast reservoir evaluation nwachukwu et al 2018 in reservoir evaluation researchers are mainly interested in two important reservoir properties i e porosity and permeability consequently the prediction of porosity and permeability is studied by many researchers based on ml graczyk and matyka 2020 magana mora et al 2020 compared with porosity permeability prediction is more challenging because permeability reflects the complex features of pore structures while porosity only reflects the first order statistics of pore structures in addition the permeability distribution is highly heterogeneous in reservoirs which makes predicting permeability more difficult instead of predicting reservoir permeability directly many scholars focus on predicting permeability at core scale which is helpful for understanding permeability at reservoir scale graczyk and matyka 2020 rabbani and babaei 2019 tian et al 2020a tian et al 2020b wu et al 2018 there are two main ml approaches applied in permeability prediction at core scale the first approach is to combine fully connected neural networks with feature extraction approaches such as experiments al khalifah et al 2020 or pnm rabbani and babaei 2019 tian et al 2020b the process is as follows firstly the feature extraction approach is used to extract pore space features from a rock image such as coordination number average pore diameter average pore channel cross section area etc subsequently these features are taken as input of the fully connected neural network to predict the permeability the process of this method is relatively complex since it necessitates hand crafted feature design which is not general and requires professional experience moreover part of the original image information will be lost during feature extraction another way to predict permeability uses deep learning methods recently the convolutional neural network cnn has been widely employed in computer vision he et al 2016 huang et al 2017 krizhevsky et al 2017 it is convenient for cnn to deal with problems with spatial correlations the rock image contains all of the information about the pore structure and the permeability is completely determined by the pore structure therefore some researchers predict permeability based on rock image and cnn santos et al 2020 tian et al 2020a wu et al 2018 zhou et al 2020 compared with fully connected neural networks cnn is a complete end to end model without complex hand crafted feature design wu et al 2018 and yoon 2019 respectively apply cnn to predict permeability from two dimensional synthetic pore scale images kamrava et al 2019 utilize cnn to link the morphology of three dimensional porous media to their macroscopic permeability santos et al 2020 introduce the application of a widely used cnn architecture i e resnet in three dimensional permeability prediction nevertheless intensive study of permeability prediction remains to be continued due to the limitations in previous studies first most of the previous investigations focus on two dimensional or simplified three dimensional pore structures which are different from practical applications furthermore the extant approaches are data driven algorithms which are extremely dependent on high quality data which are sometimes expensive to obtain in fact in order to reduce dependency on the data some researchers attempt to incorporate physical equations or information into ml methods usually referred to as physics informed machine learning chen and zhang 2021 mo et al 2020 tang et al 2020 wang et al 2020a raissi et al first propose the physics informed neural network pinn which incorporates the residual of partial differential equations pdes into the loss function of a neural network raissi et al 2019 wang et al further introduce the theory guided neural network tgnn which can incorporate not only physical principles but also practical engineering theories wang et al 2020b based on cnn and pinn in this paper we propose a convolutional neural network incorporated with global physical information cnnphys to predict rock permeability from three dimensional rock images the remainder of this paper is organized as follows in section 2 the cnnphys structure as well as the process of data preparation is described we select the hyper parameters and validate the performance of the cnn utilized in this paper in section 3 1 the performance of cnnphys is compared with cnn and other commonly used methods i e the kozeny carman equation carman 1939 sobieski and zhang 2014 and cnnresnet he et al 2016 in section 3 2 the efficiency of cnnphys is further demonstrated in the case of a small dataset and an out of range problem in section 3 3 transfer learning is also performed to improve the performance of cnnphys in section 3 3 conclusions and directions for future work are provided in section 4 2 methodology 2 1 convolutional neural network the convolutional neural network cnn utilized in this paper is densenet proposed by huang huang et al 2017 the structure of densenet is shown in fig 1 a in fact in deep learning research the risk of gradient vanishing increases as the depth of neural networks increases pascanu et al 2013 extensive researches have attempted to solve this problem such as resnet he et al 2016 highway networks srivastava et al 2015 stochastic depth huang et al 2016 and fractalnets larsson et al 2016 despite differences of the network structure of these algorithms the core concepts are the same i e to connect the shallow layer and the deep layer directly the idea of this cross layer connection is also adopted in densenet the dense block contains several layers of the dense layer inside of each block each dense layer has the same structure composed of a batch norm layer an activation function layer and a convolution layer different from the general cnn the input of each convolutional layer in the dense block is composed of the outputs of all of the previous dense layers this direct connection design makes the transmission of features and gradients more effective which also reduces the phenomenon of gradient vanishing in other words densenet is a kind of convolutional neural network which benefits from reusing features and global connection strategy of all hidden layers in fact densenet has achieved considerable success and shown better performance than other convolutional neural networks e g alexnet and resnet in groundwater research at large scale mo et al 2019a mo et al 2020 mo et al 2019b however to the best of the authors knowledge densenet has not yet been applied in research of micro scale pore structures based on the idea of densenet we select relu as an activation function and adam kingma and ba 2014 as an optimizer to train the networks the total training epoch is 120 and the decay rate of learning is 0 2 the initial learning rate is 1 10 3 this means that when the training process iterates 60 times the learning rate decays to 2 10 4 in order to improve the stability of training similarly when the iteration reaches 90 the learning rate decays to 4 10 5 the decay rate of learning is designed to ensure the stability of the model and accurate results the loss function of training adopted in this paper is the l2 loss function 1 l 2 loss 1 n i 1 n y i y i 2 the following metrics are used to quantify the predictive performance of the model 2 mse 1 n i 1 n y i y i 2 3 r 2 i 1 n y i y y i y 2 i 1 n y i y 2 i 1 n y i y 2 where n represents the number of samples y i represents the accurate permeability of sample i y i represents the predicted permeability of the model y represents the averaged permeability of total samples and y represents the averaged predicted permeability of total samples 2 2 convolutional neural network with physical information the kozeny carman equation carman 1939 has been broadly utilized in permeability prediction although the kozeny carman equation is a semi empirical formula with a few assumptions it does reveal the global factors that affect rock permeability in the original kozeny carman equation carman 1939 there are only two variables i e porosity ϕ and specific surface area σ the kozeny carman equation was further extended to incorporate porosity ϕ and tortuosity τ sobieski and zhang 2014 4 k c 0 τ 2 σ 2 ϕ 3 1 ϕ 2 where c 0 is the kozeny constant therefore it can be concluded that porosity and tortuosity will contribute to permeability prediction globally in addition the local features of the rock image could be captured by cnn through convolution kernels even so the influential global factors i e τ and σ may not be extracted implicitly by cnns which would limit the performance of predictive accuracy therefore we attempt to inform the cnns with explicit physical information aiming to obtain better predictive performance apart from the three dimensional binary rock image with one channel we add physical information as another input the upper half of the new matrix is porosity and the lower half is tortuosity which is shown in fig 1b the definition of tortuosity can be expressed as borujeni et al 2013 chukwudozie 2011 5 τ l e l 2 where l is the length of the domain and l e is the fluid traveled distance in order to constrain the whole network the physical information is added in each dense block which is shown in fig 1b consequently the size of the physical information matrix is consistent with the feature size in each dense block specifically the size of the physical information matrix is 64 64 64 32 32 32 16 16 16 and 8 8 8 in dense block 1 dense block 2 dense block 3 and dense block 4 respectively here we take dense block 2 as an example to elaborate how to integrate the physical information in the process as illustrated in fig 1 dense block 2 is distributed after transition layer 1 the output of transition layer 1 is a tensor with the size of m 32 32 32 which is denoted as t1 m is the channel number of t1 before dense block 2 we need to adjust t1 to integrate the physical information first we generate a tensor with the size of 1 32 32 32 which includes porosity and tortuosity information and denoted as tp then we combine t1 and tp into t2 along the channel direction as a consequence t2 with the size of m 1 32 32 32 includes both the output of previous layers and physical information we adopt t2 rather than t1 as the input of dense block 2 to integrate the physical information in dense block 2 the input of the convolutional neural network also incorporates the original binary image and the physical information matrix with the size of 128 128 128 the physical information is injected in the network repeatedly at many layers to train parameters of the entire network rather than a few layers the l2 loss function is adopted in the training the other hyper parameters of cnnphys are the same as those of cnn which are given in section 2 1 2 3 data preparation the convolutional neural network cnn used in this paper is essentially a data based surrogate model which requires a large amount of reliable data the most intuitive and accurate method to obtain data is through experiments the 3d rock image can be obtained through ct scan techniques rock permeability can then be measured through experiments however if training data are prepared with ct scans and experiments the cost in both material and time are untenable therefore we adopt a numerical method i e the joshi quiblier adler jqa method adler et al 1990 joshi 1975 jude et al 2013 quiblier 1984 to reconstruct the 3d digital rock images the lattice boltzmann method lbm is then applied as the substitute of experiments to calculate the corresponding permeability in order to obtain sufficient reliable labeled data the first step is reconstruction of the three dimensional rock image the jqa method can accurately and rapidly generate pore structure images characterized by binary random fields adler et al 1990 joshi 1975 jude et al 2013 quiblier 1984 in addition the generated fields are consistent with given porosity and two point correlation functions more specifically the jqa reconstruction process can be divided into two steps the first step is to generate an intermediate gaussian field in agreement with a specific two point correlation function in which all variables conform to the standard gaussian distribution the second step is to truncate the generated intermediate gaussian field with the target porosity as the threshold the main formula of the jqa method is expressed below the geostatistical analysis method is utilized to describe the pore network zhang 2002 6 z x 1 x ω pore 0 x ω solid where x is the coordinates of the point of the field and the z value of the solid matrix and pore is defined as 0 and 1 respectively according to z we can further define porosity as equation logeswaran et al and the two point correlation function as eq 8 7 ϕ z z x 8 r z x x r ϕ z z x ϕ z z x r ϕ z ϕ z 2 the two point correlation function represents the correlation between the values of two points in space when studying the image of the pore structure the two point correlation function represents the probability that two points at different distances belong to the pore space in the reconstruction process there are two important parameters as input one is the porosity of the pore structure and the other one is the two point correlation function in this paper the matérn model matérn 2013 is utilized as the two point correlation function which can be expressed as follows 9 r v r 2 1 v γ v 2 v r λ v k v 2 v r λ where λ represents the correlation length parameter and is selected as 20 μm v represents the smoothness parameter and is selected as 0 6 and the porosity range is set as 0 20 0 30 we prepare 4 500 samples of which the permeability distribution is shown in fig 2 when the correlation length of the dataset is 20 μm the permeability is distributed between 0 and 2 darcy the generated rock images are shown in fig 3 the size of each generated image is 128 128 128 it can be inferred from the generated images that the larger is the porosity the better will be the pore connectivity the pore diameter of the rock with a larger porosity is also relatively larger than that with a smaller porosity in order to validate the generated image the generated images are quantitatively analyzed using the pore network extraction method pnm dong and blunt 2009 rabbani and babaei 2019 the pore radius the throat radius and the coordination number are calculated as geometric parameters to characterize the generated porous media as shown in fig 4 we compare the statistical results of the generated image and the real rock image alyafei et al 2015 ramstad et al 2012 the results show that the geometric parameter distributions of generated structures are highly similar to those of real rock structures in this work we adopt the lattice boltzmann method lbm to calculate rock permeability lbm is a mesoscopic numerical simulation method that is easy to implement and achieves high precision lbm regards the fluid as virtual particles moving and colliding on the lattice connection line the macroscopic properties of the fluid are studied in mathematical form by analyzing the density function of particles in lbm boltzmann equations are solved on the orthogonal lattice theoretically the permeability of porous media is determined by pore structure and is unrelated to fluid properties in order to eliminate the viscosity dependence of the simulation results as much as possible lbm with multiple relaxation times mrt lbm is utilized to carry out the numerical simulations of fluid flow in porous media in this paper pan et al 2006 moreover the d3q19 model is used in this paper to improve the accuracy and the stability of the lbm simulation fields maier et al 1998 the main formula of mrt lbm is shown as follows the d3q19 model consists of 19 discrete speed components 10 e i 0 0 0 c i 0 1 0 0 c 0 1 0 c 0 0 1 c i 1 6 1 1 0 c 1 0 1 c 0 1 1 c i 7 18 the corresponding weights of the components are 11 w i 1 3 i 0 1 18 i 1 6 1 36 i 7 18 compared with lbm with single relaxation time mrt lbm introduces the collision matrix s furthermore the collision evolution of the particle density function in all directions is related to other directions 12 f x e i δ t t δ t f i x t s ij f j eq x t f i x t f j where f j 3 w j ρ 0 e j a c 2 is the term that represents the force a action and ρ 0 is the average density of the fluid field which is usually set to 1 the collision matrix s can be diagonalized as m 1 s m where m is the transform matrix between the particle density function and the corresponding moment then the particle density function can be written in a new form 13 m mf eq 12 can also be rewritten as 14 f x e δ t t δ t f x t m 1 s m eq x t m x t f where m eq is the moment at equilibrium the elements in diagonal matrices s also provide corresponding relaxation coefficients for different moments 15 m ρ e ε j x q x j y q y 3 p xx 3 π xx p ww π ww p xy p yz p xz m x m y m z t where the density and momentum are conserved quantities and the remaining 17 moments are all non conserved quantities in simulating the fluid flow of porous media to calculate permeability it is noted that the nonlinear convection terms in the navier stokes equations can be ignored only when the reynolds number is close to 0 therefore the equilibrium states corresponding to non conserved moments are e eq 11 ρ ε eq 3 ρ 16 q x eq 2 3 j x q y eq 2 3 j y q z eq 2 3 j z p xx eq p ww eq p xy eq p yz eq p xz eq 0 π xx eq π ww eq 0 m x eq m y eq m z eq 0 the corresponding relaxation matrix s is 17 s diag 0 s e s ε 0 s q 0 s q 0 s q s v s π s v s π s v s v s v s m s m s m where s v is the relaxation coefficient related to the fluid viscosity the corresponding relation is expressed as 18 v 1 3 1 s v 1 2 in the simulation of stokes flow we adopted the empirical formulas proposed by pan pan et al 2006 to determine the value of the diagonal relaxation matrix 19 s v 1 τ 2 6 v 1 s e s ε s π s m s q 8 2 s v 8 s v 3 case studies we have designed the following three groups of experiments to investigate this problem for permeability prediction in section 3 1 selection of the hyper parameters of cnn is performed in section 3 2 we compare the performance of cnnphys and other methods in section 3 3 the small dataset problem and out of range problem are examined based on cnnphys and transfer learning 3 1 permeability prediction with pure cnn in order to optimize the structure of cnn we analyze the influence of hyper parameters on prediction performance including the number of dense layers batch size growth rate and l2 regularization coefficient we prepare 4 500 samples 80 of which serve as the training dataset and the remaining 20 are used as the testing dataset in this section we will demonstrate the effectiveness of cnn which extracts information only from rock images cnn takes the binary rock image as input and outputs the corresponding permeability the detailed architectures of cnn with different numbers of layers are shown in table a1 of the appendix in hyper parameter optimization in order to investigate the influence of the number of dense layers the number of dense layers is adjusted while other hyper parameters remain unchanged the number of dense layers is set to 20 30 40 and 50 we adopt mse and r2 as metrics to evaluate the predictive performance with respect to the number of dense layers and batch size the results of mse and r2 with different numbers of layers and growth rate are shown in fig 5 a and 5b respectively when the growth rate is fixed mse decreases and r2 increases as the number of layers increases with the number of layers less than 40 the reason for this is that the error of prediction is mainly caused by underfitting in the case of a small number of layers as the number of layers increases performance would be improved when the number of layers is larger than 40 overfitting occurs easily therefore the mse increases and r2 decreases as the number of layers increases in addition mse decreases as the growth rate increases while r2 increases as the growth rate increases there is no significant difference between the curves with a growth rate of 8 and 10 which probably indicates that the cross layer connections within the network are saturated the results of mse and r2 with different batch sizes and regularization coefficients are shown in fig 5c and 5d respectively l2 regularization can reduce overfitting to a certain extent accordingly the predictive performance improves as the l2 regularization coefficient increases from 0 to 1 10 4 when the regularization coefficient is too large however underfitting easily occurs the predictive performance decreases as the l2 regularization coefficient increases from 1 10 4 to 1 5 10 4 since batch size would have an influence on the optimization path of the model different batch sizes could lead to different final convergence results of the model we finally select the batch size as 8 to balance accuracy and efficiency the above results indicate that the hyper parameters have a significant influence on the predictive performance of cnn after selecting the optimal hyper parameters the r2 and the mse of cnn are 0 985 and 3 69 10 3 respectively which indicates that with this cnn structure the prediction of rock permeability based on rock image is acceptable the final selected hyper parameters are shown in table 1 3 2 permeability prediction with cnnphys to validate the effects of physical information for improving predictive performance we take it as additional input of the cnn and we term the framework as cnnphys in this subsection we compare the performance of cnn and cnnphys as discussed in section 2 2 the physical information consists of porosity and tortuosity the dataset is the same as that in section 3 1 for comparison permeability is also predicted using the kozeny carman equation and cnnresnet he et al 2016 which are used in many previous studies tian et al 2020a wu et al 2018 for prediction with the kozeny carman equation we calculate the porosity tortuosity and specific surface area of each sample of the training set then we adjust c0 in the kozeny carman equation to fit the permeability distribution of the training set all of these methods are tested on the same testing set as that in section 3 1 the results of these methods versus the lbm simulation are presented in fig 6 additionally predictive permeability versus porosity is shown in fig 7 the permeability predicted by the kozeny carman equation is in the same magnitude as the lbm results however the results show that the kozeny carman equation has a low prediction precision with the r2 of 0 002 which indicates that it is difficult to predict permeability when the pore structures are complex cnnresnet obtains an approximate prediction with the r2 of 0 954 the mse of the kozeny carman equation and cnnresnet is 2 45 10 1 and 1 08 10 2 respectively the r2 of cnn is 0 985 while the r2 of cnnphys is 0 994 the mse of cnnphys is 1 48 10 3 while the mse of cnn is 3 69 10 3 both cnn and cnnphys utilized in this paper achieve better performance than the kozeny carman equation and cnnresnet which indicates that the expression ability of densenet is more powerful in terms of mse and r2 adding physical information can effectively improve the prediction and generalization ability of the model fig 8 shows the average epoch loss on the testing set of each epoch during the cnn and cnnphys training process the result shows that the testing loss of cnnphys is much smaller than that of cnn from the beginning as the number of epoch increases the loss of cnn gradually approaches cnnphys at the end of training the loss of cnn is close to but still larger than cnnphys in other words the loss of cnnphys converges faster than cnn moreover the loss of cnn fluctuates more widely than cnnphys in the process of training it is noted that cnnphys is both accurate and efficient it takes 429 h to predict the permeability of 4 500 samples via lbm using an intel xeon cpu e5 2690 with 14 cores while it takes only 12 h and 13 h to train cnn and cnnphys respectively using a nvidia geforce rtx 2080ti in addition once the cnn and cnnphys are constructed it takes only several seconds to predict permeability the deep learning algorithm can greatly save computational time and improve the efficiency of predicting permeability furthermore the trained network could be directly used on new rock images that have not been processed by trained models the detailed time cost is given in table 2 3 3 permeability prediction in the case of a small dataset and an out of range problem while deep learning algorithms have advantages they also possess certain limitations in general neural networks perform better in the case of large datasets in case of small datasets it is more likely to overfit the data and obtain inaccurate predictions on the test set krizhevsky et al 2017 however it is expensive to obtain data through numerical simulations and experiments in practice which motivates us to investigate the performance of the proposed method on a small dataset in this subsection we first evaluate the performance of cnn and cnnphys in the case of a small dataset the dataset contains 1 500 samples with λ 20 μm which represents the correlation length of rock image reconstruction in the jqa method 80 of the samples are used for training while the remaining 20 are used for testing the matching performance of cnn and cnnphys predictions versus the lbm simulation is shown in fig 9 a and 9b respectively the r2 and mse of cnnphys is 0 988 and 2 95 10 3 while the r2 and mse of cnn is 0 971 and 7 2 10 3 respectively this proves that in the case of a small dataset cnnphys achieves better performance than cnn moreover the decrease of the dataset size has less of an impact on the performance of cnnphys fig 9c and 9d show the performance of cnn and cnnphys with the testing data when λ 30 μm the r2 of cnnphys is 0 976 while that of cnn is 0 962 both cnn and cnnphys exhibit slightly worse performance compared with 20 μm the reason for this is because the longer is the correlation length the better will be the pore connectivity and the more complex will be the pore structure as a consequence it is more difficult to predict rock permeability nevertheless the performance of cnn and cnnphys remains satisfactory overall the results demonstrate that when the dataset size is on the order of a thousand cnn and cnnphys can be utilized to predict permeability accurately in practice the difference between the sample distribution of the training dataset and that of the testing dataset called the out of range problem may pose challenges to predictive performance since traditional machine learning is based on an independently identical distribution assumption in this study vital differences of rock permeability exist with different porosity when we only have sufficient labeled small porosity and few labeled large porosity samples an error for predicting rock permeability with large porosity and high permeability may exist which constitutes a type of out of range problem for this reason the out of range problem is investigated based on cnn and cnnphys combined with transfer learning in this subsection in this paper we utilize model fine tuning which is proposed by yosinski et al yosinski et al 2014 and widely used in deep learning as the approach to implement transfer learning devlin et al 2018 howard and ruder 2018 logeswaran et al 2019 the model fine tuning is to not only replace and retrain the model on the new dataset but also to fine tune the weights of the pre trained network by continuing the backpropagation it is possible to fine tune all of the layers of the model or it is possible to keep some of the earlier layers fixed due to overfitting concerns and only fine tune some higher level portion of the network this is motivated by the observation that the earlier features of a model contain more generic features that should be useful to many tasks but later layers of the model become progressively more specific to the details of the classes contained in the original dataset in this paper we use a new dataset to fine tune the weights of all the layers of the pre trained cnn and cnnphys first pre trained cnn and cnnphys are utilized to deal with the out of range problem directly it is noted that the distribution of the training set is different from that of the testing set here the porosity range of the training set is between 0 20 and 0 30 while that of the testing set is between 0 30 and 0 35 in addition the corresponding permeability range of the training set is between 0 darcy to 1 5 darcy while that of the testing set is between 1 5 darcy to 3 5 darcy the performance of cnn and cnnphys with the testing data versus the lbm simulation is shown in fig 9e and 9f respectively the region is defined as the extendable region if the r2 score of prediction performance is above 0 70 in this region the extendable region of cnn is from 1 5 darcy to 1 7 darcy while the extensional region of cnnphys is from 1 5 darcy to 2 5 darcy the length of the extensional region has been increased from 0 2 to 1 0 with cnnphys it is usually difficult for neural networks to deal with the out of range problem because the features of the out of range samples might be different from the features of the training samples moreover if the label of the test samples is out of the range of training data the prediction of the neural network will be inaccurate as a consequence of the lack of information nonetheless the results indicate that cnnphys achieves better performance in dealing with out of range problems cnn and cnnphys are further combined with transfer learning based on the pre trained cnn and cnnphys the additional training is performed with 16 new samples which follow the distribution of the testing set transfer learning can learn knowledge from available data and apply it to other relevant problems yosinski et al 2014 the predictive performances are tested with 160 new samples the performance of cnn and cnnphys combined with transfer learning with the testing set versus the lbm simulation is shown in fig 9g and 9 h respectively compared with the case of no transfer learning the r2 of cnn increases from 0 947 to 0 835 and that of cnnphys increases from 0 682 to 0 900 we also analyze the effects of the size of new available labeled data and compare the results with those of the model without pre training the detailed results are presented in table 3 when the size of available data is less than 32 the model cannot even be trained without pre training the performances of both cnn and cnnphys are improved with the assistance of transfer learning and a few new labeled samples in addition to the difference of porosity the difference of λ in rock image reconstruction also leads to different characteristics of rock structure which constitutes another out of range problem in this problem λ of the training set is set as 30 μm in image reconstruction while that of the testing set is 20 μm consequently the distribution of the training set is different from that of the testing set the results show that the inconsistencies of training and testing data indeed lead to a decrease of prediction accuracy however even under this condition the new method we proposed i e cnnphys achieves much better performance than cnn the performance of cnn and cnnphys versus the lbm simulation is shown in fig 9i and 9j respectively as a result of characteristic differences the permeability of the training set is larger than that of the testing set the predicted permeability of cnn is much larger than the calculated permeability of lbm in contrast the predicted permeability of cnnphys is more accurate subsequently based on the pre trained model the additional training for transfer learning is carried out with 16 new samples following the distribution of the testing set the performance of cnn and cnnphys combined with transfer learning with the testing set is shown in fig 9k and 9l respectively the r2 of cnn increases from 0 595 to 0 930 and that of cnnphys increases from 0 821 to 0 955 compared with no transfer learning we also analyze the effects of the new available labeled dataset size and compare the results with those of the model without pre training the detailed results are given in table 4 again when the available data size is less than 32 the model cannot even be trained without pre training these results demonstrate that the inclusion of transfer learning markedly improves model generalization 4 conclusions in this paper we present a robust and computationally feasible surrogate model i e cnnphys for predicting rock permeability based on cnn and physical information cnnphys combines the local features extracted from cnn and global knowledge implied in a semi empirical formula we reconstruct the rock image based on the jqa method and calculate its corresponding permeability using lbm in order to validate the efficiency of cnnphys we compare the prediction performance of the kozeny carman equation cnnresnet cnn and cnnphys in addition we discuss the application of cnnphys combined with transfer learning in the case of a small dataset and an out of range problem according to the results the following conclusions can be drawn it is important to choose appropriate cnn hyper parameters which have significant impacts on predictive performance because the complexity of the neural network structure is determined by hyper parameters a simple network structure easily leads to underfitting while a structure that is too complicated easily leads to overfitting it is more effective and efficient to use the cnn and cnnphys proposed in this paper as surrogate models to predict rock permeability based on rock images in the case of sufficient samples compared with the kozeny carman equation and cnnresnet cnnphys could obtain more accurate predictions than cnn especially in the case of a small dataset and an out of range problem when the sample distributions of the training set and the testing set are different the predictive permeability of cnnphys is markedly more accurate than that of cnn this proves that the additional physical information as a complementary feature of cnn exerts a positive influence on predicting permeability it is demonstrated that transfer learning is highly useful for out of range problems since transfer learning can combine previous rock knowledge with new rock features in aggregate cnnphys opens new pathways for rapidly predicting permeability in subsurface applications in addition the framework developed in this work also provides a fast and accurate venue for uncertainty quantification for example assessing the permeability distribution for rock samples of given statistics however one potential challenge for the proposed method is that it may be difficult to predict permeability of rock images with a larger size e g images with a size of 1000 1000 1000 since the application is limited by training time and graphic memory moreover the present research focuses on the core scale rather than the reservoir scale future work should indeed focus on upscaling research of predicting permeability from the core scale to the reservoir scale in addition the generated samples in this paper are not as realistic as the ground truth because we only consider porosity and two point correlation in image reconstruction in the future we should train the cnnphys with samples generated with new methods e g conditional gans proposed by zheng and zhang zheng and zhang 2020 for digital rock reconstruction which reproduce samples that not only possess geologic realism but also satisfy user specified properties credit authorship contribution statement pengfei tang conceptualization methodology writing original draft writing review editing software data curation visualization investigation dongxiao zhang conceptualization methodology writing original draft writing review editing supervision heng li conceptualization methodology writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partially funded by the shenzhen key laboratory of natural gas hydrates grant no zdsys20200421111201738 and the sustech qingdao new energy technology research institute appendix architecture of cnn in this appendix we describe the detailed architecture of cnn and how the architecture assists to avoid gradient vanishing and overfitting we utilize the convolutional neural network cnn i e densenet huang et al 2017 as a surrogate model to predict permeability in this paper the detailed architectures of cnn with different numbers of layers are shown in table a1 the number of dense layers is set to 20 30 40 and 50 cnn 20 means that there is a total of 20 dense layers in the network there are four dense blocks in each cnn each dense block has different numbers of dense layers for example there are two dense layers in dense block 1 of cnn 20 each dense layer is composed of a convolution layer with the kernel size of 1 1 1 as well as a convolution layer with the kernel size of 3 3 3 to extract image features in fact in deep learning research the risk of gradient vanishing increases as the depth of the neural networks increases the idea of cross layer connections is adopted in densenet if there are l dense layers in a dense block the input of layer l consists of l k channels from layer 1 to layer l 1 however the continuous superposition of the channel will become a large number if there are many dense layers inside of a dense block furthermore the bottleneck and the transition block are introduced to further simplify the architecture of the cnn more specifically the convolution is divided into two steps in the bottleneck the feature channels are reduced from l k to 4 k by the 1 1 1 convolution kernel first then the reduced features are coded by the 3 3 3 convolution kernel after each dense block there is also a transition layer to reduce the number of feature maps using the 1 1 1 convolution kernel and adjust the size of feature maps using the pooling layer the strides of the convolution layers and pooling layers are 1 1 1 and 2 2 2 respectively in other words we use the bottleneck and the transition block to adjust the size and channel of the feature maps the computational cost is greatly reduced by using the bottleneck and the transition block as a result of the design of the cross connection layers the number of feature maps is greatly decreased in densenet consequently the parameters of the network are markedly reduced which has a certain inhibitory effect on overfitting 
3688,permeability is one of the most important properties in subsurface flow problems which measures the ability of rocks to transmit fluid normally permeability is determined through experiments and numerical simulations both of which are time consuming in this paper we propose a new effective method based on convolutional neural networks with physical information cnnphys to rapidly evaluate rock permeability from its three dimensional 3d image in order to obtain sufficient reliable labeled data rock image reconstruction is utilized to generate sufficient samples based on the joshi quiblier adler method next the corresponding permeability is calculated using the lattice boltzmann method we compare the prediction performance of cnnphys and convolutional neural networks cnns the results demonstrate that cnnphys achieves superior performance especially in the case of a small dataset and an out of range problem moreover the performance of both cnn and cnnphys is greatly improved combined with transfer learning in the case of an out of range problem this opens novel pathways for rapidly predicting permeability in subsurface applications keywords deep learning permeability prediction physical information small dataset out of range problem 1 introduction in subsurface flow problems such as groundwater flow and oil gas production geng et al 2020 wada et al 2010 permeability plays an important role since it measures the ability of rocks to transmit fluid the permeability of rocks is usually modeled through laboratory core analysis shi et al 2017 rock permeability can be obtained through experiments using darcy s law cui et al 2009 dicker and smits 1988 feng et al 2016 tinni et al 2013 zhang et al 2000b however the experiments are both time consuming and significantly constrained by laboratory conditions experiments may cause irreversible damage to cores it is time intensive and impractical to conduct a large number of experiments alternatively numerical methods are flexible in parameter settings and easy to repeat it is convenient to calculate the permeability of different rocks using numerical simulations zhang et al 2000a zhao et al 2020 the application of numerical simulations however is also restricted by time cost and available computational resources the pore network model pnm is introduced to reduce the computational cost of numerical simulations dong and blunt 2009 in pnm the pore structure is simplified with a few assumptions the pressure drop is neglected in pore bodies and throat cross sections are considered as definitive geometrical shapes such as circles squares and triangles in pnm however this simplification might have an impact on accuracy compared with experiments and simulations semi empirical equations e g the kozeny carman equation can be applied to predict permeability more rapidly akande et al 2017 olatunji 2010 the semi empirical formulas for specific rocks are fitted according to the rock particle distribution composition and deposition process the permeability can then be predicted based on porosity and other properties through formulas empirical formulas however have a limited range of applications and it is difficult for empirical expressions to deal with strong heterogeneities in recent years some researchers have attempted to use machine learning ml methods to deal with geophysical problems niu et al 2020 sun 2018 such as formation lithology classification mohamed et al 2019 seismic reservoir characterization liu et al 2019 and fast reservoir evaluation nwachukwu et al 2018 in reservoir evaluation researchers are mainly interested in two important reservoir properties i e porosity and permeability consequently the prediction of porosity and permeability is studied by many researchers based on ml graczyk and matyka 2020 magana mora et al 2020 compared with porosity permeability prediction is more challenging because permeability reflects the complex features of pore structures while porosity only reflects the first order statistics of pore structures in addition the permeability distribution is highly heterogeneous in reservoirs which makes predicting permeability more difficult instead of predicting reservoir permeability directly many scholars focus on predicting permeability at core scale which is helpful for understanding permeability at reservoir scale graczyk and matyka 2020 rabbani and babaei 2019 tian et al 2020a tian et al 2020b wu et al 2018 there are two main ml approaches applied in permeability prediction at core scale the first approach is to combine fully connected neural networks with feature extraction approaches such as experiments al khalifah et al 2020 or pnm rabbani and babaei 2019 tian et al 2020b the process is as follows firstly the feature extraction approach is used to extract pore space features from a rock image such as coordination number average pore diameter average pore channel cross section area etc subsequently these features are taken as input of the fully connected neural network to predict the permeability the process of this method is relatively complex since it necessitates hand crafted feature design which is not general and requires professional experience moreover part of the original image information will be lost during feature extraction another way to predict permeability uses deep learning methods recently the convolutional neural network cnn has been widely employed in computer vision he et al 2016 huang et al 2017 krizhevsky et al 2017 it is convenient for cnn to deal with problems with spatial correlations the rock image contains all of the information about the pore structure and the permeability is completely determined by the pore structure therefore some researchers predict permeability based on rock image and cnn santos et al 2020 tian et al 2020a wu et al 2018 zhou et al 2020 compared with fully connected neural networks cnn is a complete end to end model without complex hand crafted feature design wu et al 2018 and yoon 2019 respectively apply cnn to predict permeability from two dimensional synthetic pore scale images kamrava et al 2019 utilize cnn to link the morphology of three dimensional porous media to their macroscopic permeability santos et al 2020 introduce the application of a widely used cnn architecture i e resnet in three dimensional permeability prediction nevertheless intensive study of permeability prediction remains to be continued due to the limitations in previous studies first most of the previous investigations focus on two dimensional or simplified three dimensional pore structures which are different from practical applications furthermore the extant approaches are data driven algorithms which are extremely dependent on high quality data which are sometimes expensive to obtain in fact in order to reduce dependency on the data some researchers attempt to incorporate physical equations or information into ml methods usually referred to as physics informed machine learning chen and zhang 2021 mo et al 2020 tang et al 2020 wang et al 2020a raissi et al first propose the physics informed neural network pinn which incorporates the residual of partial differential equations pdes into the loss function of a neural network raissi et al 2019 wang et al further introduce the theory guided neural network tgnn which can incorporate not only physical principles but also practical engineering theories wang et al 2020b based on cnn and pinn in this paper we propose a convolutional neural network incorporated with global physical information cnnphys to predict rock permeability from three dimensional rock images the remainder of this paper is organized as follows in section 2 the cnnphys structure as well as the process of data preparation is described we select the hyper parameters and validate the performance of the cnn utilized in this paper in section 3 1 the performance of cnnphys is compared with cnn and other commonly used methods i e the kozeny carman equation carman 1939 sobieski and zhang 2014 and cnnresnet he et al 2016 in section 3 2 the efficiency of cnnphys is further demonstrated in the case of a small dataset and an out of range problem in section 3 3 transfer learning is also performed to improve the performance of cnnphys in section 3 3 conclusions and directions for future work are provided in section 4 2 methodology 2 1 convolutional neural network the convolutional neural network cnn utilized in this paper is densenet proposed by huang huang et al 2017 the structure of densenet is shown in fig 1 a in fact in deep learning research the risk of gradient vanishing increases as the depth of neural networks increases pascanu et al 2013 extensive researches have attempted to solve this problem such as resnet he et al 2016 highway networks srivastava et al 2015 stochastic depth huang et al 2016 and fractalnets larsson et al 2016 despite differences of the network structure of these algorithms the core concepts are the same i e to connect the shallow layer and the deep layer directly the idea of this cross layer connection is also adopted in densenet the dense block contains several layers of the dense layer inside of each block each dense layer has the same structure composed of a batch norm layer an activation function layer and a convolution layer different from the general cnn the input of each convolutional layer in the dense block is composed of the outputs of all of the previous dense layers this direct connection design makes the transmission of features and gradients more effective which also reduces the phenomenon of gradient vanishing in other words densenet is a kind of convolutional neural network which benefits from reusing features and global connection strategy of all hidden layers in fact densenet has achieved considerable success and shown better performance than other convolutional neural networks e g alexnet and resnet in groundwater research at large scale mo et al 2019a mo et al 2020 mo et al 2019b however to the best of the authors knowledge densenet has not yet been applied in research of micro scale pore structures based on the idea of densenet we select relu as an activation function and adam kingma and ba 2014 as an optimizer to train the networks the total training epoch is 120 and the decay rate of learning is 0 2 the initial learning rate is 1 10 3 this means that when the training process iterates 60 times the learning rate decays to 2 10 4 in order to improve the stability of training similarly when the iteration reaches 90 the learning rate decays to 4 10 5 the decay rate of learning is designed to ensure the stability of the model and accurate results the loss function of training adopted in this paper is the l2 loss function 1 l 2 loss 1 n i 1 n y i y i 2 the following metrics are used to quantify the predictive performance of the model 2 mse 1 n i 1 n y i y i 2 3 r 2 i 1 n y i y y i y 2 i 1 n y i y 2 i 1 n y i y 2 where n represents the number of samples y i represents the accurate permeability of sample i y i represents the predicted permeability of the model y represents the averaged permeability of total samples and y represents the averaged predicted permeability of total samples 2 2 convolutional neural network with physical information the kozeny carman equation carman 1939 has been broadly utilized in permeability prediction although the kozeny carman equation is a semi empirical formula with a few assumptions it does reveal the global factors that affect rock permeability in the original kozeny carman equation carman 1939 there are only two variables i e porosity ϕ and specific surface area σ the kozeny carman equation was further extended to incorporate porosity ϕ and tortuosity τ sobieski and zhang 2014 4 k c 0 τ 2 σ 2 ϕ 3 1 ϕ 2 where c 0 is the kozeny constant therefore it can be concluded that porosity and tortuosity will contribute to permeability prediction globally in addition the local features of the rock image could be captured by cnn through convolution kernels even so the influential global factors i e τ and σ may not be extracted implicitly by cnns which would limit the performance of predictive accuracy therefore we attempt to inform the cnns with explicit physical information aiming to obtain better predictive performance apart from the three dimensional binary rock image with one channel we add physical information as another input the upper half of the new matrix is porosity and the lower half is tortuosity which is shown in fig 1b the definition of tortuosity can be expressed as borujeni et al 2013 chukwudozie 2011 5 τ l e l 2 where l is the length of the domain and l e is the fluid traveled distance in order to constrain the whole network the physical information is added in each dense block which is shown in fig 1b consequently the size of the physical information matrix is consistent with the feature size in each dense block specifically the size of the physical information matrix is 64 64 64 32 32 32 16 16 16 and 8 8 8 in dense block 1 dense block 2 dense block 3 and dense block 4 respectively here we take dense block 2 as an example to elaborate how to integrate the physical information in the process as illustrated in fig 1 dense block 2 is distributed after transition layer 1 the output of transition layer 1 is a tensor with the size of m 32 32 32 which is denoted as t1 m is the channel number of t1 before dense block 2 we need to adjust t1 to integrate the physical information first we generate a tensor with the size of 1 32 32 32 which includes porosity and tortuosity information and denoted as tp then we combine t1 and tp into t2 along the channel direction as a consequence t2 with the size of m 1 32 32 32 includes both the output of previous layers and physical information we adopt t2 rather than t1 as the input of dense block 2 to integrate the physical information in dense block 2 the input of the convolutional neural network also incorporates the original binary image and the physical information matrix with the size of 128 128 128 the physical information is injected in the network repeatedly at many layers to train parameters of the entire network rather than a few layers the l2 loss function is adopted in the training the other hyper parameters of cnnphys are the same as those of cnn which are given in section 2 1 2 3 data preparation the convolutional neural network cnn used in this paper is essentially a data based surrogate model which requires a large amount of reliable data the most intuitive and accurate method to obtain data is through experiments the 3d rock image can be obtained through ct scan techniques rock permeability can then be measured through experiments however if training data are prepared with ct scans and experiments the cost in both material and time are untenable therefore we adopt a numerical method i e the joshi quiblier adler jqa method adler et al 1990 joshi 1975 jude et al 2013 quiblier 1984 to reconstruct the 3d digital rock images the lattice boltzmann method lbm is then applied as the substitute of experiments to calculate the corresponding permeability in order to obtain sufficient reliable labeled data the first step is reconstruction of the three dimensional rock image the jqa method can accurately and rapidly generate pore structure images characterized by binary random fields adler et al 1990 joshi 1975 jude et al 2013 quiblier 1984 in addition the generated fields are consistent with given porosity and two point correlation functions more specifically the jqa reconstruction process can be divided into two steps the first step is to generate an intermediate gaussian field in agreement with a specific two point correlation function in which all variables conform to the standard gaussian distribution the second step is to truncate the generated intermediate gaussian field with the target porosity as the threshold the main formula of the jqa method is expressed below the geostatistical analysis method is utilized to describe the pore network zhang 2002 6 z x 1 x ω pore 0 x ω solid where x is the coordinates of the point of the field and the z value of the solid matrix and pore is defined as 0 and 1 respectively according to z we can further define porosity as equation logeswaran et al and the two point correlation function as eq 8 7 ϕ z z x 8 r z x x r ϕ z z x ϕ z z x r ϕ z ϕ z 2 the two point correlation function represents the correlation between the values of two points in space when studying the image of the pore structure the two point correlation function represents the probability that two points at different distances belong to the pore space in the reconstruction process there are two important parameters as input one is the porosity of the pore structure and the other one is the two point correlation function in this paper the matérn model matérn 2013 is utilized as the two point correlation function which can be expressed as follows 9 r v r 2 1 v γ v 2 v r λ v k v 2 v r λ where λ represents the correlation length parameter and is selected as 20 μm v represents the smoothness parameter and is selected as 0 6 and the porosity range is set as 0 20 0 30 we prepare 4 500 samples of which the permeability distribution is shown in fig 2 when the correlation length of the dataset is 20 μm the permeability is distributed between 0 and 2 darcy the generated rock images are shown in fig 3 the size of each generated image is 128 128 128 it can be inferred from the generated images that the larger is the porosity the better will be the pore connectivity the pore diameter of the rock with a larger porosity is also relatively larger than that with a smaller porosity in order to validate the generated image the generated images are quantitatively analyzed using the pore network extraction method pnm dong and blunt 2009 rabbani and babaei 2019 the pore radius the throat radius and the coordination number are calculated as geometric parameters to characterize the generated porous media as shown in fig 4 we compare the statistical results of the generated image and the real rock image alyafei et al 2015 ramstad et al 2012 the results show that the geometric parameter distributions of generated structures are highly similar to those of real rock structures in this work we adopt the lattice boltzmann method lbm to calculate rock permeability lbm is a mesoscopic numerical simulation method that is easy to implement and achieves high precision lbm regards the fluid as virtual particles moving and colliding on the lattice connection line the macroscopic properties of the fluid are studied in mathematical form by analyzing the density function of particles in lbm boltzmann equations are solved on the orthogonal lattice theoretically the permeability of porous media is determined by pore structure and is unrelated to fluid properties in order to eliminate the viscosity dependence of the simulation results as much as possible lbm with multiple relaxation times mrt lbm is utilized to carry out the numerical simulations of fluid flow in porous media in this paper pan et al 2006 moreover the d3q19 model is used in this paper to improve the accuracy and the stability of the lbm simulation fields maier et al 1998 the main formula of mrt lbm is shown as follows the d3q19 model consists of 19 discrete speed components 10 e i 0 0 0 c i 0 1 0 0 c 0 1 0 c 0 0 1 c i 1 6 1 1 0 c 1 0 1 c 0 1 1 c i 7 18 the corresponding weights of the components are 11 w i 1 3 i 0 1 18 i 1 6 1 36 i 7 18 compared with lbm with single relaxation time mrt lbm introduces the collision matrix s furthermore the collision evolution of the particle density function in all directions is related to other directions 12 f x e i δ t t δ t f i x t s ij f j eq x t f i x t f j where f j 3 w j ρ 0 e j a c 2 is the term that represents the force a action and ρ 0 is the average density of the fluid field which is usually set to 1 the collision matrix s can be diagonalized as m 1 s m where m is the transform matrix between the particle density function and the corresponding moment then the particle density function can be written in a new form 13 m mf eq 12 can also be rewritten as 14 f x e δ t t δ t f x t m 1 s m eq x t m x t f where m eq is the moment at equilibrium the elements in diagonal matrices s also provide corresponding relaxation coefficients for different moments 15 m ρ e ε j x q x j y q y 3 p xx 3 π xx p ww π ww p xy p yz p xz m x m y m z t where the density and momentum are conserved quantities and the remaining 17 moments are all non conserved quantities in simulating the fluid flow of porous media to calculate permeability it is noted that the nonlinear convection terms in the navier stokes equations can be ignored only when the reynolds number is close to 0 therefore the equilibrium states corresponding to non conserved moments are e eq 11 ρ ε eq 3 ρ 16 q x eq 2 3 j x q y eq 2 3 j y q z eq 2 3 j z p xx eq p ww eq p xy eq p yz eq p xz eq 0 π xx eq π ww eq 0 m x eq m y eq m z eq 0 the corresponding relaxation matrix s is 17 s diag 0 s e s ε 0 s q 0 s q 0 s q s v s π s v s π s v s v s v s m s m s m where s v is the relaxation coefficient related to the fluid viscosity the corresponding relation is expressed as 18 v 1 3 1 s v 1 2 in the simulation of stokes flow we adopted the empirical formulas proposed by pan pan et al 2006 to determine the value of the diagonal relaxation matrix 19 s v 1 τ 2 6 v 1 s e s ε s π s m s q 8 2 s v 8 s v 3 case studies we have designed the following three groups of experiments to investigate this problem for permeability prediction in section 3 1 selection of the hyper parameters of cnn is performed in section 3 2 we compare the performance of cnnphys and other methods in section 3 3 the small dataset problem and out of range problem are examined based on cnnphys and transfer learning 3 1 permeability prediction with pure cnn in order to optimize the structure of cnn we analyze the influence of hyper parameters on prediction performance including the number of dense layers batch size growth rate and l2 regularization coefficient we prepare 4 500 samples 80 of which serve as the training dataset and the remaining 20 are used as the testing dataset in this section we will demonstrate the effectiveness of cnn which extracts information only from rock images cnn takes the binary rock image as input and outputs the corresponding permeability the detailed architectures of cnn with different numbers of layers are shown in table a1 of the appendix in hyper parameter optimization in order to investigate the influence of the number of dense layers the number of dense layers is adjusted while other hyper parameters remain unchanged the number of dense layers is set to 20 30 40 and 50 we adopt mse and r2 as metrics to evaluate the predictive performance with respect to the number of dense layers and batch size the results of mse and r2 with different numbers of layers and growth rate are shown in fig 5 a and 5b respectively when the growth rate is fixed mse decreases and r2 increases as the number of layers increases with the number of layers less than 40 the reason for this is that the error of prediction is mainly caused by underfitting in the case of a small number of layers as the number of layers increases performance would be improved when the number of layers is larger than 40 overfitting occurs easily therefore the mse increases and r2 decreases as the number of layers increases in addition mse decreases as the growth rate increases while r2 increases as the growth rate increases there is no significant difference between the curves with a growth rate of 8 and 10 which probably indicates that the cross layer connections within the network are saturated the results of mse and r2 with different batch sizes and regularization coefficients are shown in fig 5c and 5d respectively l2 regularization can reduce overfitting to a certain extent accordingly the predictive performance improves as the l2 regularization coefficient increases from 0 to 1 10 4 when the regularization coefficient is too large however underfitting easily occurs the predictive performance decreases as the l2 regularization coefficient increases from 1 10 4 to 1 5 10 4 since batch size would have an influence on the optimization path of the model different batch sizes could lead to different final convergence results of the model we finally select the batch size as 8 to balance accuracy and efficiency the above results indicate that the hyper parameters have a significant influence on the predictive performance of cnn after selecting the optimal hyper parameters the r2 and the mse of cnn are 0 985 and 3 69 10 3 respectively which indicates that with this cnn structure the prediction of rock permeability based on rock image is acceptable the final selected hyper parameters are shown in table 1 3 2 permeability prediction with cnnphys to validate the effects of physical information for improving predictive performance we take it as additional input of the cnn and we term the framework as cnnphys in this subsection we compare the performance of cnn and cnnphys as discussed in section 2 2 the physical information consists of porosity and tortuosity the dataset is the same as that in section 3 1 for comparison permeability is also predicted using the kozeny carman equation and cnnresnet he et al 2016 which are used in many previous studies tian et al 2020a wu et al 2018 for prediction with the kozeny carman equation we calculate the porosity tortuosity and specific surface area of each sample of the training set then we adjust c0 in the kozeny carman equation to fit the permeability distribution of the training set all of these methods are tested on the same testing set as that in section 3 1 the results of these methods versus the lbm simulation are presented in fig 6 additionally predictive permeability versus porosity is shown in fig 7 the permeability predicted by the kozeny carman equation is in the same magnitude as the lbm results however the results show that the kozeny carman equation has a low prediction precision with the r2 of 0 002 which indicates that it is difficult to predict permeability when the pore structures are complex cnnresnet obtains an approximate prediction with the r2 of 0 954 the mse of the kozeny carman equation and cnnresnet is 2 45 10 1 and 1 08 10 2 respectively the r2 of cnn is 0 985 while the r2 of cnnphys is 0 994 the mse of cnnphys is 1 48 10 3 while the mse of cnn is 3 69 10 3 both cnn and cnnphys utilized in this paper achieve better performance than the kozeny carman equation and cnnresnet which indicates that the expression ability of densenet is more powerful in terms of mse and r2 adding physical information can effectively improve the prediction and generalization ability of the model fig 8 shows the average epoch loss on the testing set of each epoch during the cnn and cnnphys training process the result shows that the testing loss of cnnphys is much smaller than that of cnn from the beginning as the number of epoch increases the loss of cnn gradually approaches cnnphys at the end of training the loss of cnn is close to but still larger than cnnphys in other words the loss of cnnphys converges faster than cnn moreover the loss of cnn fluctuates more widely than cnnphys in the process of training it is noted that cnnphys is both accurate and efficient it takes 429 h to predict the permeability of 4 500 samples via lbm using an intel xeon cpu e5 2690 with 14 cores while it takes only 12 h and 13 h to train cnn and cnnphys respectively using a nvidia geforce rtx 2080ti in addition once the cnn and cnnphys are constructed it takes only several seconds to predict permeability the deep learning algorithm can greatly save computational time and improve the efficiency of predicting permeability furthermore the trained network could be directly used on new rock images that have not been processed by trained models the detailed time cost is given in table 2 3 3 permeability prediction in the case of a small dataset and an out of range problem while deep learning algorithms have advantages they also possess certain limitations in general neural networks perform better in the case of large datasets in case of small datasets it is more likely to overfit the data and obtain inaccurate predictions on the test set krizhevsky et al 2017 however it is expensive to obtain data through numerical simulations and experiments in practice which motivates us to investigate the performance of the proposed method on a small dataset in this subsection we first evaluate the performance of cnn and cnnphys in the case of a small dataset the dataset contains 1 500 samples with λ 20 μm which represents the correlation length of rock image reconstruction in the jqa method 80 of the samples are used for training while the remaining 20 are used for testing the matching performance of cnn and cnnphys predictions versus the lbm simulation is shown in fig 9 a and 9b respectively the r2 and mse of cnnphys is 0 988 and 2 95 10 3 while the r2 and mse of cnn is 0 971 and 7 2 10 3 respectively this proves that in the case of a small dataset cnnphys achieves better performance than cnn moreover the decrease of the dataset size has less of an impact on the performance of cnnphys fig 9c and 9d show the performance of cnn and cnnphys with the testing data when λ 30 μm the r2 of cnnphys is 0 976 while that of cnn is 0 962 both cnn and cnnphys exhibit slightly worse performance compared with 20 μm the reason for this is because the longer is the correlation length the better will be the pore connectivity and the more complex will be the pore structure as a consequence it is more difficult to predict rock permeability nevertheless the performance of cnn and cnnphys remains satisfactory overall the results demonstrate that when the dataset size is on the order of a thousand cnn and cnnphys can be utilized to predict permeability accurately in practice the difference between the sample distribution of the training dataset and that of the testing dataset called the out of range problem may pose challenges to predictive performance since traditional machine learning is based on an independently identical distribution assumption in this study vital differences of rock permeability exist with different porosity when we only have sufficient labeled small porosity and few labeled large porosity samples an error for predicting rock permeability with large porosity and high permeability may exist which constitutes a type of out of range problem for this reason the out of range problem is investigated based on cnn and cnnphys combined with transfer learning in this subsection in this paper we utilize model fine tuning which is proposed by yosinski et al yosinski et al 2014 and widely used in deep learning as the approach to implement transfer learning devlin et al 2018 howard and ruder 2018 logeswaran et al 2019 the model fine tuning is to not only replace and retrain the model on the new dataset but also to fine tune the weights of the pre trained network by continuing the backpropagation it is possible to fine tune all of the layers of the model or it is possible to keep some of the earlier layers fixed due to overfitting concerns and only fine tune some higher level portion of the network this is motivated by the observation that the earlier features of a model contain more generic features that should be useful to many tasks but later layers of the model become progressively more specific to the details of the classes contained in the original dataset in this paper we use a new dataset to fine tune the weights of all the layers of the pre trained cnn and cnnphys first pre trained cnn and cnnphys are utilized to deal with the out of range problem directly it is noted that the distribution of the training set is different from that of the testing set here the porosity range of the training set is between 0 20 and 0 30 while that of the testing set is between 0 30 and 0 35 in addition the corresponding permeability range of the training set is between 0 darcy to 1 5 darcy while that of the testing set is between 1 5 darcy to 3 5 darcy the performance of cnn and cnnphys with the testing data versus the lbm simulation is shown in fig 9e and 9f respectively the region is defined as the extendable region if the r2 score of prediction performance is above 0 70 in this region the extendable region of cnn is from 1 5 darcy to 1 7 darcy while the extensional region of cnnphys is from 1 5 darcy to 2 5 darcy the length of the extensional region has been increased from 0 2 to 1 0 with cnnphys it is usually difficult for neural networks to deal with the out of range problem because the features of the out of range samples might be different from the features of the training samples moreover if the label of the test samples is out of the range of training data the prediction of the neural network will be inaccurate as a consequence of the lack of information nonetheless the results indicate that cnnphys achieves better performance in dealing with out of range problems cnn and cnnphys are further combined with transfer learning based on the pre trained cnn and cnnphys the additional training is performed with 16 new samples which follow the distribution of the testing set transfer learning can learn knowledge from available data and apply it to other relevant problems yosinski et al 2014 the predictive performances are tested with 160 new samples the performance of cnn and cnnphys combined with transfer learning with the testing set versus the lbm simulation is shown in fig 9g and 9 h respectively compared with the case of no transfer learning the r2 of cnn increases from 0 947 to 0 835 and that of cnnphys increases from 0 682 to 0 900 we also analyze the effects of the size of new available labeled data and compare the results with those of the model without pre training the detailed results are presented in table 3 when the size of available data is less than 32 the model cannot even be trained without pre training the performances of both cnn and cnnphys are improved with the assistance of transfer learning and a few new labeled samples in addition to the difference of porosity the difference of λ in rock image reconstruction also leads to different characteristics of rock structure which constitutes another out of range problem in this problem λ of the training set is set as 30 μm in image reconstruction while that of the testing set is 20 μm consequently the distribution of the training set is different from that of the testing set the results show that the inconsistencies of training and testing data indeed lead to a decrease of prediction accuracy however even under this condition the new method we proposed i e cnnphys achieves much better performance than cnn the performance of cnn and cnnphys versus the lbm simulation is shown in fig 9i and 9j respectively as a result of characteristic differences the permeability of the training set is larger than that of the testing set the predicted permeability of cnn is much larger than the calculated permeability of lbm in contrast the predicted permeability of cnnphys is more accurate subsequently based on the pre trained model the additional training for transfer learning is carried out with 16 new samples following the distribution of the testing set the performance of cnn and cnnphys combined with transfer learning with the testing set is shown in fig 9k and 9l respectively the r2 of cnn increases from 0 595 to 0 930 and that of cnnphys increases from 0 821 to 0 955 compared with no transfer learning we also analyze the effects of the new available labeled dataset size and compare the results with those of the model without pre training the detailed results are given in table 4 again when the available data size is less than 32 the model cannot even be trained without pre training these results demonstrate that the inclusion of transfer learning markedly improves model generalization 4 conclusions in this paper we present a robust and computationally feasible surrogate model i e cnnphys for predicting rock permeability based on cnn and physical information cnnphys combines the local features extracted from cnn and global knowledge implied in a semi empirical formula we reconstruct the rock image based on the jqa method and calculate its corresponding permeability using lbm in order to validate the efficiency of cnnphys we compare the prediction performance of the kozeny carman equation cnnresnet cnn and cnnphys in addition we discuss the application of cnnphys combined with transfer learning in the case of a small dataset and an out of range problem according to the results the following conclusions can be drawn it is important to choose appropriate cnn hyper parameters which have significant impacts on predictive performance because the complexity of the neural network structure is determined by hyper parameters a simple network structure easily leads to underfitting while a structure that is too complicated easily leads to overfitting it is more effective and efficient to use the cnn and cnnphys proposed in this paper as surrogate models to predict rock permeability based on rock images in the case of sufficient samples compared with the kozeny carman equation and cnnresnet cnnphys could obtain more accurate predictions than cnn especially in the case of a small dataset and an out of range problem when the sample distributions of the training set and the testing set are different the predictive permeability of cnnphys is markedly more accurate than that of cnn this proves that the additional physical information as a complementary feature of cnn exerts a positive influence on predicting permeability it is demonstrated that transfer learning is highly useful for out of range problems since transfer learning can combine previous rock knowledge with new rock features in aggregate cnnphys opens new pathways for rapidly predicting permeability in subsurface applications in addition the framework developed in this work also provides a fast and accurate venue for uncertainty quantification for example assessing the permeability distribution for rock samples of given statistics however one potential challenge for the proposed method is that it may be difficult to predict permeability of rock images with a larger size e g images with a size of 1000 1000 1000 since the application is limited by training time and graphic memory moreover the present research focuses on the core scale rather than the reservoir scale future work should indeed focus on upscaling research of predicting permeability from the core scale to the reservoir scale in addition the generated samples in this paper are not as realistic as the ground truth because we only consider porosity and two point correlation in image reconstruction in the future we should train the cnnphys with samples generated with new methods e g conditional gans proposed by zheng and zhang zheng and zhang 2020 for digital rock reconstruction which reproduce samples that not only possess geologic realism but also satisfy user specified properties credit authorship contribution statement pengfei tang conceptualization methodology writing original draft writing review editing software data curation visualization investigation dongxiao zhang conceptualization methodology writing original draft writing review editing supervision heng li conceptualization methodology writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partially funded by the shenzhen key laboratory of natural gas hydrates grant no zdsys20200421111201738 and the sustech qingdao new energy technology research institute appendix architecture of cnn in this appendix we describe the detailed architecture of cnn and how the architecture assists to avoid gradient vanishing and overfitting we utilize the convolutional neural network cnn i e densenet huang et al 2017 as a surrogate model to predict permeability in this paper the detailed architectures of cnn with different numbers of layers are shown in table a1 the number of dense layers is set to 20 30 40 and 50 cnn 20 means that there is a total of 20 dense layers in the network there are four dense blocks in each cnn each dense block has different numbers of dense layers for example there are two dense layers in dense block 1 of cnn 20 each dense layer is composed of a convolution layer with the kernel size of 1 1 1 as well as a convolution layer with the kernel size of 3 3 3 to extract image features in fact in deep learning research the risk of gradient vanishing increases as the depth of the neural networks increases the idea of cross layer connections is adopted in densenet if there are l dense layers in a dense block the input of layer l consists of l k channels from layer 1 to layer l 1 however the continuous superposition of the channel will become a large number if there are many dense layers inside of a dense block furthermore the bottleneck and the transition block are introduced to further simplify the architecture of the cnn more specifically the convolution is divided into two steps in the bottleneck the feature channels are reduced from l k to 4 k by the 1 1 1 convolution kernel first then the reduced features are coded by the 3 3 3 convolution kernel after each dense block there is also a transition layer to reduce the number of feature maps using the 1 1 1 convolution kernel and adjust the size of feature maps using the pooling layer the strides of the convolution layers and pooling layers are 1 1 1 and 2 2 2 respectively in other words we use the bottleneck and the transition block to adjust the size and channel of the feature maps the computational cost is greatly reduced by using the bottleneck and the transition block as a result of the design of the cross connection layers the number of feature maps is greatly decreased in densenet consequently the parameters of the network are markedly reduced which has a certain inhibitory effect on overfitting 
3689,in drylands planted trees generally develop roots through mechanisms that enhance their capacity to forage for resources particularly water in deep soils and thus survive droughts however it remains unclear how and to what extent lack of access to deep root water uptake will affect the water balance growth and production of trees in water limited ecosystems to address this gap we designed a novel experiment to partition soils and roots below 200 cm depth from higher layers in a dryland orchard on china s loess plateau to evaluate the response of apple malus pumila mill trees to the lack of deep roots and access to deep soil resource in 2019 and 2020 when a natural extreme drought occurred we found that without partitioning apple trees clearly used deep soil water thereby causing more soil desiccation than partitioned trees especially during the drought the unavailability of deep soil water resulted in much greater water stress which cause a decrease of 36 in transpiration 20 in photosynthesis rate and 32 in mid day leaf water potential relative to unpartitioned trees consequently the yield and quality of apple trees also clearly decreased 11 and 15 these findings suggest that the absence of uptake of deep soil water by roots reduced trees drought resistance however the partitioned trees had clearly higher water use efficiency wue than the unpartitioned trees although in both cases wue was strongly reduced by the extreme drought our results demonstrate that deep soil resources and roots play key roles in trees responses to drought and are likely to play crucial roles in future ecosystem dynamics keywords deep soil partitioned drought transpiration leaf water potential photosynthesis 1 introduction tree mortality caused by droughts in water limited regions has been extensively reported recently liu et al 2013 hember et al 2017 choat et al 2018 trees in drylands can generally develop deep roots that enable them to forage for soil water and or groundwater in deep soils fan et al 2017 wang et al 2021 thereby avoiding hydraulic failure during extreme droughts ding et al 2021 which are projected to intensify due to ongoing climate change huang et al 2017 a mass of evidence indicates that plants in dryland ecosystems and other systems prone to extreme drought such as sand dunes generally produce the deepest roots extending to 2 m depths in 95 of dryland locations schenk and jackson 2002 therefore understanding of the role of deep root water uptake is essential for predicting the sustainability of plantations in water limited regions uptake of deep water by trees roots is linked to functional traits during prolonged dry seasons in multiple types of ecosystems markewitz et al 2010 fan et al 2017 yang et al 2017 studies involving in situ measurements modeling and stable isotopic labeling have demonstrated that trees develop deep roots and dimorphic root systems through adaptive mechanisms that enhance their capacity to forage for resources these mechanisms enable them to alleviate drought stress by utilizing deep soil water markewitz et al 2010 stahl et al 2013 christina et al 2017 beyer et al 2018 wang et al 2020 ding et al 2021 thereby improving forests productivity and carbon fixation nepstad et al 2014 pierret et al 2016 thorup kristensen et al 2020 for instance nepstad et al 1994 found that evergreen forests in tropical amazonia maintain evapotranspiration during five month dry periods by absorbing water from soil at depths of 8 m and soil water in the 250 1150 cm layer reportedly accounts for ca 30 of total water consumption in amazonian forest christina et al 2017 also found that eucalyptus plantations in amazonia greatly increase water uptake near the water table 12 m deep in dry seasons in drylands numerous studies have shown that planted trees may intensively use water in deep 20 m soil layers e g wang et al 2011 jia et al 2017 gao et al 2018a li et al 2019 yang et al 2020 wang et al 2021 and soil water at 2 m depths deposited decades or even hundreds of years before can be exhausted in several seasons the evidence outlined above strongly highlights the importance of deep roots and soil water for ecosystems resilience to drought stress however the extent to which the ability to take up deep soil water contributes to their hydrological and physiological status has not been robustly addressed this raises the question how and to what extent will planted trees respond hydrologically and physiologically if they cannot access deep soil resources in water limited ecosystems the effects may be particularly acute for such trees which form xylem conduits when they have access to more water and thus are more vulnerable to cavitation than plants in ecosystems that never had deep water security and hence have long been adapted to water stress the largely dryland loess plateau of china is the world s main apple cultivation region with coverage of apple orchards and annual apple fruit production exceeding 1 30 million ha and 23 million tons accounting for 22 and 27 of the global totals respectively gao et al 2021a thus apple trees on the plateau provide ideal material for addressing the above question so in the present study we cut off apple trees deep roots 2 m in efforts to elucidate effects of lack of deep roots and access to deep soil resources on the ecohydrological and physiological performance of planted trees in dryland systems in term of transpiration ψleaf photosynthesis and wue we hypothesized that lack of deep roots and access to deep soil water would negatively influence ecohydrological and physiological processes and extreme drought would exacerbate these effects 2 materials and methods 2 1 site description the study was conducted in an apple plantation covering ca 56 ha at qingshuigou 37 27 n 110 20 e 989 m a s l in zizhou county which belongs to yulin city shaanxi province a hilly region of the loess plateau fig s1 the study area has a semiarid climate with mean annual precipitation of 498 mm during 2006 2020 ca 70 of which falls in july august and september in intense but short events it also has 170 frost free days and 2633 sunshine hours on average and mean annual temperature of 9 1 the area is covered by loess soil with silt loam texture over 30 m thick and groundwater is not available to plants apple malus pumila mill trees in this area must develop deep and robust root systems to utilize deep soil water therefore deep soil water 2 m deep is crucial for the sustainable growth of apple trees in this area in addition due to the high cost of irrigation apple plantations in hilly regions of the loess plateau are generally rainfed song et al 2017 precipitation solar radiation relative humidity and air temperature were measured every 30 min during 2019 and 2020 by an automated weather station close to the experimental field 2 2 experimental design apple trees in the focal orchard were planted in 2009 with 2 5 and 3 m spacing within and between rows respectively trees with similar plant height crown diameter and diameter at breast height in the orchard were assigned to either partitioning of deep soils and roots from higher layers or control continuation of normal conditions treatments fig 1 three plots were established for each treatment each hosting sixteen apple trees and three apple trees were selected for the experiment fig s2a each selected tree was isolated from surrounding trees with iron sheets down to 1 m depth to reduce effects of adjacent trees on their water balance fig s2b due to the majority of apple tree s lateral roots concentrated in the top 80 cm we hypothesize that root water uptake through lateral fine roots below 100 cm should be negligible the layout of apple trees in the experiment plot was shown in fig s2 and the basic information was given in table 1 ech20 em 50 sensors decagon devices inc usa were used to measure soil volumetric water content for this purpose a trench was vertically dug to expose the soil profile around the vertical projection of each tree in the control plots fig s3a then sensors were inserted in april 2018 into the soil profile at depths of 20 60 100 150 200 250 300 350 400 and 450 cm fig 1 the sensors were installed 20 cm horizontally from the trunk of each apple tree and recorded water content every 15 min in addition the trenches were refilled after installation of the sensors in early april 2018 a trench 250 cm long and 450 cm deep was dug around the vertical projection of the canopy of each apple tree in the partitioned plots polyethylene pe isolation board 3 0 m 2 5 m 1 cm thick was used to partition the soil supporting each tree into a shallow 0 200 cm layer and deep 200 450 cm layer fig 1 at 200 210 cm depth from the ground a horizontal isolation layer 10 cm thick and covering the entire vertical projection was excavated and filled with pe board and soil before installation of the pe board positions of sensor cables were determined holes were drilled and cables were passed through them to enable installation of sensors above and below the partitioning layer the installation positions and monitoring frequencies of sensors in these plots were the same as those in control plots fig 1 collection of these data began on april 2019 to enable the trees to recover for a year before monitoring effects of the treatment again the trenches were refilled the details of dealing with roots and the horizontal isolation have been added in supplementary file fig s3 before the experiment was conducted a 500 cm deep profile was excavated in each tree to install sensors and collect undisturbed soil cores at corresponding depths to obtain measurements of soil dry bulk density and gravimetric water content θ g g g 1 via the drying method the θ g values were then transformed to volumetric water content θ v cm3 cm 3 and calibration curves were obtained by plotting θ v measurements obtained with the em50 sensors against values obtained with the oven drying method since apple trees in the field were close to each other and the soil texture was similar we pooled data obtained for all depths in the 0 450 cm range to generate a calibration curve the coefficient of determination r2 was 0 9 by fitting the soil moisture measured by oven drying method with that measured by em50 fig s4 thus the results of em50 were acceptable 2 3 fine root sampling root samples were collected in october 2020 from each experiment trees in each control and partitioned plot at 20 cm intervals across the 0 500 cm and 0 200 cm soil profiles respectively samples of roots of each tree were collected from three directions 0 120 240 using a soil auger a cylinder with a side opening 90 mm diameter and 200 mm height 40 cm from their stems the details of dealing with root samples can be referred to yang et al 2020 and wang et al 2021 2 4 estimation of soil water consumption to calculate soil water storage we assumed that the soil water contents of the 0 20 cm layer were equal to values obtained from the calibrated sensors at 20 cm depth with corresponding assumptions for other layers daily soil water storage during 2019 and 2020 was estimated at depths of 0 100 100 200 and 200 450 cm soil water storage sws mm in each layer for a given day was determined by the following equation 1 sws 10 θ δ h here δ h is 100 cm 100 cm and 250 cm for the 0 100 cm 100 200 cm and 200 450 cm layers respectively î is the calibrated daily average soil volumetric water for the corresponding soil layers and the constant 10 is the cm to mm conversion coefficient monthly soil water storage change in month i δ s w s at 0 100 100 200 and 200 450 cm depths was estimated by the following equation 2 δ s w s sws i sws i 1 here sws i and s ws i 1 are the average soil water storage mm in month i and previous month for that soil layer respectively a negative δ s w s indicates use of soil water by the apple trees the summed negative δ s w s in each layer during each growing season was defined as the total water uptake from that layer during that growing season 2 5 sap flow measurements the daily transpiration rate of apple trees in each experimental plot was monitored during the 2019 and 2020 growing seasons as follows an flgs tdp xm1000 system dynamax co usa was used to monitor sap flux in the selected apple trees the system included 12 thermal dissipation probes tdps with two needle probes length 10 mm diameter 2 mm and a cr1000 data logger campbell co usa sapwood area as was estimated from the relationship between sapwood area and tree diameter determined by cutting 21 randomly chosen neighboring apple trees 30 cm above the ground in 2019 a significant linear correlation between the trees as and trunk diameter was obtained as shown in fig s5 the details of installation are described in ye et al 2021 2 6 leaf water potential ψleaf and photosynthesis characteristics the mature healthy fully expanded leaves in the upper canopy from each tree were collected in four directions on each sampling date predawn and midday leaf water potential ψpd ψmd were measured at 05 30 06 30 and 12 00 13 00 h respectively on sunny days we obtained four measurements of both variables on each occasion and averaged them to obtain mean ψpd and ψmd values for each tree sampled leaves were immediately enclosed in plastic bags and ψleaf determination was initiated within a minute using a model 1505d pressure chamber pms instrument co albany or usa in addition gas exchange parameters of four mature fully expanded leaves of each tree were measured from 9 00 11 00 am on sunny days using a li 6400xt portable photosynthesis system li cor lincoln nebraska usa the li 6400 xt was adjusted to a fixed co2 concentration 400 μmol mol photosynthetically active radiation par 1500 μmol m 2 s 1 leaf temperature ranged from 25 32 and relative humidity ranged between 40 and 65 during the periods of measurements vpd inside the chamber ranged from 0 5 to 2 5 kpa light saturated net photosynthetic rate pn transpiration rate tr and stomatal conductance gs were recorded we also calculated leaf water use efficiency wue expressed as pn tr both ψleaf and photosynthesis characteristics were measured four times during the covered growth period in 2020 during the blossoming fruit bearing 17 may young fruit expansion 20 june fruit expansion 30 july and fruit maturation stages 2 september respectively in addition the relative chlorophyll content and maximum electron transport rate jmax of leaves were measured using a hand held dual wave length chlorophyll meter spad 502 minolta camera co ltd japan and pulse amplitude modulated fluorometer pam 2500 waltz effeltrich germany respectively furthermore the indices of fruit quality were detected the content of soluble sugar and soluble solids in fruit were assessed by the anthrone colorimetry and hand held refractometer respectively the fruit firmness and vitamin c content were determined using the gy 1 fruit hardness tester and 2 6 dichloro indophenol titration method respectively 2 7 statistical analysis using the acquired data we compared soil water contents and transpiration rates in control and partitioned plots in both 2019 and 2020 we also compared ψleaf photosynthesis rates and wue in these plots in 2020 between treatment differences in the variables were explored using analysis of variance anova followed by tukey s standardized multiple comparisons test to identify significant differences spss software version 23 0 was used for all these analyses and differences were considered significant if p 0 05 all presented graphics were generated using origin 2021 software originlab corporation usa except for figure s1 s3 3 results 3 1 climate and root distribution the precipitation at the study site amounted to 560 and 484 mm in 2019 and 2020 respectively fig 2 the total precipitation in 2020 was similar to the multiyear 2006 2020 mean 498 mm but in 2019 it was 12 4 higher than the multiyear mean most of the precipitation was concentrated in the growing season from april to october 542 mm and 424 mm in 2019 and 2020 accounting for 96 9 and 87 7 of the total annual precipitation respectively fig 2 note that the prolonged drought in 2020 primarily occurred from april to july when the precipitation was 85 8 mm fig s6 equivalent to just 31 4 of the precipitation in the same period in 2019 distributions of fine root length density frld across the soil profile in the control and partitioned plots are shown in fig 3 overall it decreased with increasing soil depth in control plots there were fine roots in both shallow and deeper layers but most roots were in shallow layers 70 of the total fine roots were located in the 0 200 cm soil layer and 50 in the 0 100 cm layer however fine roots were more abundant in shallow layers of the partitioned plots particularly in the 0 100 cm layer where they were 42 5 more abundant than in the control plots 3 2 soil water dynamics there were clear daily and vertical variations in volumetric soil water content in the control and partitioned plots throughout the study period during 2019 and 2020 fig 4 the response to rain was gradual in subsurface layers particularly in the 100 200 cm which only responded to the highest precipitation events there was little variation in soil water content in the 200 450 cm layers during the growing seasons the average soil water content in the 0 200 cm soil layer was clearly lower in the partitioned plots 13 9 and 12 6 respectively than in the control plots 15 2 and 13 8 respectively in the dry season april to july of 2019 and 2020 by contrast in the 200 450 cm layer it was clearly lower in the control plots than in partitioned plots during these periods 11 5 vs 12 2 in 2019 and 9 5 vs 11 1 in 2020 the seasonal variation in soil water depletion at monitored depths in the control and partitioned plots are shown in fig s7 during the total growing season the soil water depletion in the 0 200 soil layers of control and partitioned plots amounted to 42 3 and 29 0 mm respectively in 2019 and 105 3 mm and 106 1 mm respectively in 2020 the soil water storage in the 0 200 cm layer of partitioned plots strongly decreased in 2020 and was 266 lower on average in these plots than in 2019 in addition there was clear deep soil water absorption in the control plots during the drought period the fractions of soil water extracted from the 200 450 cm soil layers were approximately 50 4 and 70 5 in the dry seasons of 2019 and 2020 respectively 3 3 transpiration variation in transpiration of trees in the control and partitioned plots during the growing period are presented in fig 5 the pattern of changes in the daily transpiration rate td with time were similar in both years increasing to a maximum in mid may to early june then progressively decreasing to a minimum in september fig 5a rainfall clearly affected transpiration generally the td values increased immediately after rainfall in control plots ranges in td were 0 26 2 4 mm d 1 in 2019 and 0 1 2 8 mm day 1 in 2020 corresponding ranges in the partitioned plots were 0 06 2 12 and 0 08 2 0 mm day 1 respectively at the monthly scale transpiration rates were similar in the control and partitioned plots in most months of 2019 except august and september fig 5b however they were significantly lower in partitioned plots p 0 05 than in control plots by 36 on average in 2020 fig 5b especially in the drought period from may to july of 2020 transpiration was much 43 lower in the partitioned plots at the annual scale there was no significant p 0 05 difference in transpiration rates between control and partitioned plots in 2019 but they were significantly lower in partitioned plots p 0 001 in 2020 fig 5c 3 4 seasonal variation in plant physiology throughout the entire growing period in 2020 leaf water potential ψpd and ψmd was lower in the partitioned plots than in the control plots fig 6 overall in these plots ψpd ranged from 0 08 to 0 77 mpa and 0 27 to 1 36 mpa respectively while ψmd ranged from 1 62 to 2 31 mpa and 2 58 to 3 12 mpa respectively thus in the partitioned plots ψpd was 0 14 to 0 36 mpa lower than in control plots in all four growth stages but the differences were not significant p 0 05 however ψmd was significantly lower p 0 05 in partitioned plots than in control plots in the four successive growth stages by 36 11 41 and 41 respectively the pn tr gs and wue values recorded in the control and partitioned plots during the growing season in 2020 are presented in fig 7 pn tr and gs were significantly higher p 0 05 in the control plots especially in dry periods on average pn was 20 higher in the control plots than in partitioned plots during the study period the variations in gs and tr were similar to the variations in pn in contrast the trees in partitioned plots had higher wue than those in control plots especially on the 17 may blossoming fruit bearing and 2 september fruit maturation measuring occasions when the differences in wue δwue between the control and partitioned plots were 6 1 and 1 9 μ mol m 2 s 1 respectively in addition single fruit weight yield soluble sugar soluble solid and fruit shape index were significantly p 0 05 lower in partitioned plots by 31 2 11 2 11 2 47 3 and 8 1 respectively than in control plots table 2 4 discussion the importance of deep rooting and deep soil water for trees has been repeatedly underlined in the last three decades nepstad et al 1994 markewitz et al 2010 christina et al 2017 fan et al 2017 thorup kristensen et al 2020 however few studies have quantitatively evaluated the importance of deep root water uptake for trees in drylands in the study presented here we addressed this issue through a novel experimental approach of isolating deep roots and soils 4 1 ecohydrological response to lack of deep roots and access to deep soil deep rooted apple trees can strongly affect the water balance of ecosystems li et al 2019 wu et al 2021 accordingly we found that the depth averaged water content of soil under trees in the focal dryland orchard tended to decrease substantially with time causing severe deep soil desiccation fig 4 clearly this could have important consequences that warrant rigorous attention including the effects of lack of access to deep water supplies on apple trees e g death or successful adaptation to changes in the soil moisture profile the partitioning experiment reported here was designed to address this issue as summarized in our research question how and to what extent will planted trees respond hydrologically and physiologically if they cannot access deep soil resources in water limited ecosystems there were clearly higher soil moisture deficits in the upper 200 cm in the partitioned plots than in the control plots fig 4 this could be attributed to the lack of access to deep soil stimulating trees to develop shallow root systems thereby increasing the proportion of water taken up from the upper 200 cm fig 3 fig s7 recharge of shallow soil water while deep soil water was still depleted has also reported in pinus elliottii plantations and p massoniana plantations in southeastern china yang et al 2017 a possible explanation is that the deep soil water is a more stable water source for deep rooted plants during drought periods gao et al 2018b deep roots may also be activated when severe water stress results in irreversible embolism or death of shallow roots williams and ehleringer 2000 overall deep rooting is especially important in seasonally dry environments because it enables plants to access deep soil water thereby enhancing their capacity to cope with prolonged drought and obtain potential competitive advantage in drought prone ecosystems christina et al 2017 yang et al 2017 jiang et al 2020 we also found that transpiration was higher in partitioned plots than in control plots in most months of 2019 fig 5b indicating that moderate water stress did not limit transpiration supporting findings by gao et al 2021b that moderate drought may promote transpiration however transpiration rates were much lower in partitioned plots than in control plots in 2020 fig 5 which contributed to the intense declines in shallow water availability caused by the extreme meteorological drought figs 4 and s5 the lack of availability of deep soil water increased water stress to some extent and reduced water uptake thereby leading to reduction in transpiration in contrast deep roots in the control plots enabled plants to access vast deep soil water resources thereby buffering the declines in transpiration during prolonged drought similarly broedel et al 2017 found that in amazonian primary forest deep root uptake down to 5 m resulted in stronger and longer deep soil water depletion than normal during a drought in 2005 but sustained canopy transpiration christina et al 2017 also found that 32 and 57 of water transpired by eucalyptus trees was taken up from 3 to 10 m soil layers during the dry periods in their first 2 years after planting in south east brazil 4 2 physiological responses to lack of deep roots and access to deep soil we found no significant p 0 05 difference in ψpd between control and partitioned plots but ψmd was significantly lower p 0 05 in partitioned plots than in control plots during the growing season of 2020 fig 6 this pattern is consistent with the variation in transpiration in 2020 suggesting that lack of access to deep soil water greatly increased water stress and resulted in lower ψmd clearly ψmd will become more negative during prolonged drought and a decline in ψleaf can lead to downregulation of maximum carboxylation rates vcmax and electron transport rates jmax fig s8 bhusal et al 2020 as well as leaf shedding if leaf turgor cannot be maintained brodribb et al 2002 by contrast deep roots facilitate maintenance of ψleaf during drought periods thereby increasing plants capacities to avoid hydraulic failure bucci et al 2009 ding et al 2021 the wue of the trees was higher in partitioned plots than in control plots although their photosynthesis and transpiration rates were lower during the study period fig 7 this is consistent with expectations as water stress causes closure of stomata galmes et al 2007 misson et al 2010 santos et al 2018 salmon et al 2020 but transpiration rates may decline more drastically than photosynthesis rates under moderate drought stress notably trees in partitioned plots also had lower wue on 20 june and 30 july in the young fruit expansion and fruit expansion stages respectively than on 17 may and 2 september in the blossoming fruit bearing and fruit maturation stages respectively in 2020 this can be attributed to the young fruit and fruit expansion periods coinciding with the prolonged extreme drought period fig s6 and the lack of access to deep soil water forcing the trees to absorb more shallow soil water thereby decreasing the water availability figs 4 s7 and intensifying the water stress in these conditions the relative chlorophyll content also significantly decreased fig s9 resulting in further reduction of photosynthesis rates and thus reduction of wue by contrast water absorption by deep roots may enable trees to maintain higher photosynthetic rates wue hydration and productivity thus increasing their capacity to avoid carbon deficits hydraulic failure and mortality during prolonged extreme drought unfortunately we did not obtain the data of leaf stable carbon isotope δ13c leaf δ13c can be used to assess long term variation in traits that covary with leaf gas exchange carbon uptake and water relations including intrinsit water use effieiency wuei which is the ratio of carbon fixed via photosynthesis to water vapour losses via stomatal conductance lavergne et al 2020 thus variations in leaf δ13c warrant attention in future studies to clarify its response to prolonged deep soil water deficits 4 3 the mechanism of effects of deep soil and roots on trees performance we propose a model to in depth explain the observed effects of lack of deep root water uptake on apple trees performance fig 8 first it leads to insufficient water availability in the absence of sufficient precipitation and increases in trees absorption of shallow soil water this exacerbates water stress thereby further reducing water availability and eventually uptake with consequent reductions in transpiration drought induced reductions in transpiration will impair the trees carbon and nitrogen assimilation thus reducing their primary productivity and resistance to water stress pests and pathogens mcdowell et al 2008 gessler et al 2017 in addition ψleaf becomes increasingly negative with increasing water stress and hydraulic conductance may decrease through several biophysical and physiological mechanisms including irreversible collapse of leaf veins zhang et al 2016 and regulation of aquaporins in cell membranes mcelrone et al 2007 these effects may cause embolism choat et al 2018 thus further reducing delivery of water to the canopy and pronounced reductions in canopy leaf area moreover declines in ψleaf will limit jmax and vcmax thus reducing the efficiency of solar energy utilization and carbon fixation galmes et al 2007 water stress also leads to stomatal closure with a series of negative consequences on short time scales these include cessation of photosynthetic co2 assimilation reduction of leaf cooling and increases in probability of photodamage choat et al 2018 santos et al 2018 on long time scales low photosynthetic rates lead to low translocation of sugars through the phloem and hence depletion of non structural carbohydrate pools mitchell et al 2013 sevanto et al 2014 low photosynthetic rates also reduce the production of chemical defense compounds thus increasing plants vulnerability to pests and pathogens mcdowell et al 2011 in summary lack of deep roots and access to deep soil resources is likely to have adverse effects on trees hydraulics e g by impairing ψleaf and osmotic adjustment and c balance e g by reducing transpiration and photosynthesis which may cause lower resistance to pests and pathogens e g by reducing production of chemical defense compounds mcdowell et al 2011 dietze et al 2014 these effects may lead to hydraulic failure c starvation and or increases in severity of pest and pathogens attacks low yields and quality of plantations table 2 and higher risks of tree mortality 4 4 theoretical insights and practical implications our findings have important implications for ecosystem sustainability and future vegetation dynamics under climatic drying first the findings regarding differential responses of trees functional traits to lack of access to deep water provide indications of likely changes in hydrological and physiological processes associated with climate change second acquisition of deep water is arguably the most effective drought avoidance mechanism for dryland plantations our findings illuminate trees responses to lack of deep root water uptake and help efforts to assess how and to what extent deep soil water affects trees growth when there are no natural analogs this is highly important for enhancing understanding of plants deep root functions as well as relationships between deep soil resource constraints and plant traits third use of nature based solutions including trees has been globally advocated to address sustainability issues seddon et al 2019 mori 2020 this study may provide practical insights into the dynamics of planted trees in drylands if there is no access to deep soil water in future extreme climates 5 conclusions this study highlights the importance of deep root water uptake for the maintenance of plant transpiration leaf water potential photosynthesis and water use efficiency during prolonged drought in water limited ecosystems trees without deep root systems can only access shallow water which is most easily accessible but its content rapidly fluctuates so they will be at high risk of drought induced mortality with increases in drought frequency and or duration specifically the absence of deep soil water would decrease 36 in transpiration 20 in photosynthesis rate and 32 in mid day leaf water potential consequently the yield and quality of apple trees decreased 11 and 15 respectively overall the study enhances understanding of effects of lack of deep roots and access to deep soil resources on plants responses to drought and provide insights into their roles in hydrological processes and plantation dynamics in drought vulnerable ecosystems with significant implication for the long term sustainability of plantation in a changing climate credit authorship contribution statement min yang data curation formal analysis investigation methodology validation visualization writing original draft xiaodong gao conceptualization funding acquisition validation project administration resources data curation writing review editing shaofei wang investigation data curation methodology visualization xining zhao conceptualization funding acquisition validation methodology visualization resources data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thanks prof ying fan reinfelder for her constructive suggestions and miaotai ye and bo yang for their help in sampling this work was jointly supported by the national key research and development program of china grant no 2021yfd1900700 national natural science foundation of china grant no 41771316 42125705 the shaanxi key research and development program grant no 2020zdlny07 04 natural science basic research program of shaanxi grant no 2021jc 19 the cyrus tang foundation and the 111 project grant no b12007 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127471 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3689,in drylands planted trees generally develop roots through mechanisms that enhance their capacity to forage for resources particularly water in deep soils and thus survive droughts however it remains unclear how and to what extent lack of access to deep root water uptake will affect the water balance growth and production of trees in water limited ecosystems to address this gap we designed a novel experiment to partition soils and roots below 200 cm depth from higher layers in a dryland orchard on china s loess plateau to evaluate the response of apple malus pumila mill trees to the lack of deep roots and access to deep soil resource in 2019 and 2020 when a natural extreme drought occurred we found that without partitioning apple trees clearly used deep soil water thereby causing more soil desiccation than partitioned trees especially during the drought the unavailability of deep soil water resulted in much greater water stress which cause a decrease of 36 in transpiration 20 in photosynthesis rate and 32 in mid day leaf water potential relative to unpartitioned trees consequently the yield and quality of apple trees also clearly decreased 11 and 15 these findings suggest that the absence of uptake of deep soil water by roots reduced trees drought resistance however the partitioned trees had clearly higher water use efficiency wue than the unpartitioned trees although in both cases wue was strongly reduced by the extreme drought our results demonstrate that deep soil resources and roots play key roles in trees responses to drought and are likely to play crucial roles in future ecosystem dynamics keywords deep soil partitioned drought transpiration leaf water potential photosynthesis 1 introduction tree mortality caused by droughts in water limited regions has been extensively reported recently liu et al 2013 hember et al 2017 choat et al 2018 trees in drylands can generally develop deep roots that enable them to forage for soil water and or groundwater in deep soils fan et al 2017 wang et al 2021 thereby avoiding hydraulic failure during extreme droughts ding et al 2021 which are projected to intensify due to ongoing climate change huang et al 2017 a mass of evidence indicates that plants in dryland ecosystems and other systems prone to extreme drought such as sand dunes generally produce the deepest roots extending to 2 m depths in 95 of dryland locations schenk and jackson 2002 therefore understanding of the role of deep root water uptake is essential for predicting the sustainability of plantations in water limited regions uptake of deep water by trees roots is linked to functional traits during prolonged dry seasons in multiple types of ecosystems markewitz et al 2010 fan et al 2017 yang et al 2017 studies involving in situ measurements modeling and stable isotopic labeling have demonstrated that trees develop deep roots and dimorphic root systems through adaptive mechanisms that enhance their capacity to forage for resources these mechanisms enable them to alleviate drought stress by utilizing deep soil water markewitz et al 2010 stahl et al 2013 christina et al 2017 beyer et al 2018 wang et al 2020 ding et al 2021 thereby improving forests productivity and carbon fixation nepstad et al 2014 pierret et al 2016 thorup kristensen et al 2020 for instance nepstad et al 1994 found that evergreen forests in tropical amazonia maintain evapotranspiration during five month dry periods by absorbing water from soil at depths of 8 m and soil water in the 250 1150 cm layer reportedly accounts for ca 30 of total water consumption in amazonian forest christina et al 2017 also found that eucalyptus plantations in amazonia greatly increase water uptake near the water table 12 m deep in dry seasons in drylands numerous studies have shown that planted trees may intensively use water in deep 20 m soil layers e g wang et al 2011 jia et al 2017 gao et al 2018a li et al 2019 yang et al 2020 wang et al 2021 and soil water at 2 m depths deposited decades or even hundreds of years before can be exhausted in several seasons the evidence outlined above strongly highlights the importance of deep roots and soil water for ecosystems resilience to drought stress however the extent to which the ability to take up deep soil water contributes to their hydrological and physiological status has not been robustly addressed this raises the question how and to what extent will planted trees respond hydrologically and physiologically if they cannot access deep soil resources in water limited ecosystems the effects may be particularly acute for such trees which form xylem conduits when they have access to more water and thus are more vulnerable to cavitation than plants in ecosystems that never had deep water security and hence have long been adapted to water stress the largely dryland loess plateau of china is the world s main apple cultivation region with coverage of apple orchards and annual apple fruit production exceeding 1 30 million ha and 23 million tons accounting for 22 and 27 of the global totals respectively gao et al 2021a thus apple trees on the plateau provide ideal material for addressing the above question so in the present study we cut off apple trees deep roots 2 m in efforts to elucidate effects of lack of deep roots and access to deep soil resources on the ecohydrological and physiological performance of planted trees in dryland systems in term of transpiration ψleaf photosynthesis and wue we hypothesized that lack of deep roots and access to deep soil water would negatively influence ecohydrological and physiological processes and extreme drought would exacerbate these effects 2 materials and methods 2 1 site description the study was conducted in an apple plantation covering ca 56 ha at qingshuigou 37 27 n 110 20 e 989 m a s l in zizhou county which belongs to yulin city shaanxi province a hilly region of the loess plateau fig s1 the study area has a semiarid climate with mean annual precipitation of 498 mm during 2006 2020 ca 70 of which falls in july august and september in intense but short events it also has 170 frost free days and 2633 sunshine hours on average and mean annual temperature of 9 1 the area is covered by loess soil with silt loam texture over 30 m thick and groundwater is not available to plants apple malus pumila mill trees in this area must develop deep and robust root systems to utilize deep soil water therefore deep soil water 2 m deep is crucial for the sustainable growth of apple trees in this area in addition due to the high cost of irrigation apple plantations in hilly regions of the loess plateau are generally rainfed song et al 2017 precipitation solar radiation relative humidity and air temperature were measured every 30 min during 2019 and 2020 by an automated weather station close to the experimental field 2 2 experimental design apple trees in the focal orchard were planted in 2009 with 2 5 and 3 m spacing within and between rows respectively trees with similar plant height crown diameter and diameter at breast height in the orchard were assigned to either partitioning of deep soils and roots from higher layers or control continuation of normal conditions treatments fig 1 three plots were established for each treatment each hosting sixteen apple trees and three apple trees were selected for the experiment fig s2a each selected tree was isolated from surrounding trees with iron sheets down to 1 m depth to reduce effects of adjacent trees on their water balance fig s2b due to the majority of apple tree s lateral roots concentrated in the top 80 cm we hypothesize that root water uptake through lateral fine roots below 100 cm should be negligible the layout of apple trees in the experiment plot was shown in fig s2 and the basic information was given in table 1 ech20 em 50 sensors decagon devices inc usa were used to measure soil volumetric water content for this purpose a trench was vertically dug to expose the soil profile around the vertical projection of each tree in the control plots fig s3a then sensors were inserted in april 2018 into the soil profile at depths of 20 60 100 150 200 250 300 350 400 and 450 cm fig 1 the sensors were installed 20 cm horizontally from the trunk of each apple tree and recorded water content every 15 min in addition the trenches were refilled after installation of the sensors in early april 2018 a trench 250 cm long and 450 cm deep was dug around the vertical projection of the canopy of each apple tree in the partitioned plots polyethylene pe isolation board 3 0 m 2 5 m 1 cm thick was used to partition the soil supporting each tree into a shallow 0 200 cm layer and deep 200 450 cm layer fig 1 at 200 210 cm depth from the ground a horizontal isolation layer 10 cm thick and covering the entire vertical projection was excavated and filled with pe board and soil before installation of the pe board positions of sensor cables were determined holes were drilled and cables were passed through them to enable installation of sensors above and below the partitioning layer the installation positions and monitoring frequencies of sensors in these plots were the same as those in control plots fig 1 collection of these data began on april 2019 to enable the trees to recover for a year before monitoring effects of the treatment again the trenches were refilled the details of dealing with roots and the horizontal isolation have been added in supplementary file fig s3 before the experiment was conducted a 500 cm deep profile was excavated in each tree to install sensors and collect undisturbed soil cores at corresponding depths to obtain measurements of soil dry bulk density and gravimetric water content θ g g g 1 via the drying method the θ g values were then transformed to volumetric water content θ v cm3 cm 3 and calibration curves were obtained by plotting θ v measurements obtained with the em50 sensors against values obtained with the oven drying method since apple trees in the field were close to each other and the soil texture was similar we pooled data obtained for all depths in the 0 450 cm range to generate a calibration curve the coefficient of determination r2 was 0 9 by fitting the soil moisture measured by oven drying method with that measured by em50 fig s4 thus the results of em50 were acceptable 2 3 fine root sampling root samples were collected in october 2020 from each experiment trees in each control and partitioned plot at 20 cm intervals across the 0 500 cm and 0 200 cm soil profiles respectively samples of roots of each tree were collected from three directions 0 120 240 using a soil auger a cylinder with a side opening 90 mm diameter and 200 mm height 40 cm from their stems the details of dealing with root samples can be referred to yang et al 2020 and wang et al 2021 2 4 estimation of soil water consumption to calculate soil water storage we assumed that the soil water contents of the 0 20 cm layer were equal to values obtained from the calibrated sensors at 20 cm depth with corresponding assumptions for other layers daily soil water storage during 2019 and 2020 was estimated at depths of 0 100 100 200 and 200 450 cm soil water storage sws mm in each layer for a given day was determined by the following equation 1 sws 10 θ δ h here δ h is 100 cm 100 cm and 250 cm for the 0 100 cm 100 200 cm and 200 450 cm layers respectively î is the calibrated daily average soil volumetric water for the corresponding soil layers and the constant 10 is the cm to mm conversion coefficient monthly soil water storage change in month i δ s w s at 0 100 100 200 and 200 450 cm depths was estimated by the following equation 2 δ s w s sws i sws i 1 here sws i and s ws i 1 are the average soil water storage mm in month i and previous month for that soil layer respectively a negative δ s w s indicates use of soil water by the apple trees the summed negative δ s w s in each layer during each growing season was defined as the total water uptake from that layer during that growing season 2 5 sap flow measurements the daily transpiration rate of apple trees in each experimental plot was monitored during the 2019 and 2020 growing seasons as follows an flgs tdp xm1000 system dynamax co usa was used to monitor sap flux in the selected apple trees the system included 12 thermal dissipation probes tdps with two needle probes length 10 mm diameter 2 mm and a cr1000 data logger campbell co usa sapwood area as was estimated from the relationship between sapwood area and tree diameter determined by cutting 21 randomly chosen neighboring apple trees 30 cm above the ground in 2019 a significant linear correlation between the trees as and trunk diameter was obtained as shown in fig s5 the details of installation are described in ye et al 2021 2 6 leaf water potential ψleaf and photosynthesis characteristics the mature healthy fully expanded leaves in the upper canopy from each tree were collected in four directions on each sampling date predawn and midday leaf water potential ψpd ψmd were measured at 05 30 06 30 and 12 00 13 00 h respectively on sunny days we obtained four measurements of both variables on each occasion and averaged them to obtain mean ψpd and ψmd values for each tree sampled leaves were immediately enclosed in plastic bags and ψleaf determination was initiated within a minute using a model 1505d pressure chamber pms instrument co albany or usa in addition gas exchange parameters of four mature fully expanded leaves of each tree were measured from 9 00 11 00 am on sunny days using a li 6400xt portable photosynthesis system li cor lincoln nebraska usa the li 6400 xt was adjusted to a fixed co2 concentration 400 μmol mol photosynthetically active radiation par 1500 μmol m 2 s 1 leaf temperature ranged from 25 32 and relative humidity ranged between 40 and 65 during the periods of measurements vpd inside the chamber ranged from 0 5 to 2 5 kpa light saturated net photosynthetic rate pn transpiration rate tr and stomatal conductance gs were recorded we also calculated leaf water use efficiency wue expressed as pn tr both ψleaf and photosynthesis characteristics were measured four times during the covered growth period in 2020 during the blossoming fruit bearing 17 may young fruit expansion 20 june fruit expansion 30 july and fruit maturation stages 2 september respectively in addition the relative chlorophyll content and maximum electron transport rate jmax of leaves were measured using a hand held dual wave length chlorophyll meter spad 502 minolta camera co ltd japan and pulse amplitude modulated fluorometer pam 2500 waltz effeltrich germany respectively furthermore the indices of fruit quality were detected the content of soluble sugar and soluble solids in fruit were assessed by the anthrone colorimetry and hand held refractometer respectively the fruit firmness and vitamin c content were determined using the gy 1 fruit hardness tester and 2 6 dichloro indophenol titration method respectively 2 7 statistical analysis using the acquired data we compared soil water contents and transpiration rates in control and partitioned plots in both 2019 and 2020 we also compared ψleaf photosynthesis rates and wue in these plots in 2020 between treatment differences in the variables were explored using analysis of variance anova followed by tukey s standardized multiple comparisons test to identify significant differences spss software version 23 0 was used for all these analyses and differences were considered significant if p 0 05 all presented graphics were generated using origin 2021 software originlab corporation usa except for figure s1 s3 3 results 3 1 climate and root distribution the precipitation at the study site amounted to 560 and 484 mm in 2019 and 2020 respectively fig 2 the total precipitation in 2020 was similar to the multiyear 2006 2020 mean 498 mm but in 2019 it was 12 4 higher than the multiyear mean most of the precipitation was concentrated in the growing season from april to october 542 mm and 424 mm in 2019 and 2020 accounting for 96 9 and 87 7 of the total annual precipitation respectively fig 2 note that the prolonged drought in 2020 primarily occurred from april to july when the precipitation was 85 8 mm fig s6 equivalent to just 31 4 of the precipitation in the same period in 2019 distributions of fine root length density frld across the soil profile in the control and partitioned plots are shown in fig 3 overall it decreased with increasing soil depth in control plots there were fine roots in both shallow and deeper layers but most roots were in shallow layers 70 of the total fine roots were located in the 0 200 cm soil layer and 50 in the 0 100 cm layer however fine roots were more abundant in shallow layers of the partitioned plots particularly in the 0 100 cm layer where they were 42 5 more abundant than in the control plots 3 2 soil water dynamics there were clear daily and vertical variations in volumetric soil water content in the control and partitioned plots throughout the study period during 2019 and 2020 fig 4 the response to rain was gradual in subsurface layers particularly in the 100 200 cm which only responded to the highest precipitation events there was little variation in soil water content in the 200 450 cm layers during the growing seasons the average soil water content in the 0 200 cm soil layer was clearly lower in the partitioned plots 13 9 and 12 6 respectively than in the control plots 15 2 and 13 8 respectively in the dry season april to july of 2019 and 2020 by contrast in the 200 450 cm layer it was clearly lower in the control plots than in partitioned plots during these periods 11 5 vs 12 2 in 2019 and 9 5 vs 11 1 in 2020 the seasonal variation in soil water depletion at monitored depths in the control and partitioned plots are shown in fig s7 during the total growing season the soil water depletion in the 0 200 soil layers of control and partitioned plots amounted to 42 3 and 29 0 mm respectively in 2019 and 105 3 mm and 106 1 mm respectively in 2020 the soil water storage in the 0 200 cm layer of partitioned plots strongly decreased in 2020 and was 266 lower on average in these plots than in 2019 in addition there was clear deep soil water absorption in the control plots during the drought period the fractions of soil water extracted from the 200 450 cm soil layers were approximately 50 4 and 70 5 in the dry seasons of 2019 and 2020 respectively 3 3 transpiration variation in transpiration of trees in the control and partitioned plots during the growing period are presented in fig 5 the pattern of changes in the daily transpiration rate td with time were similar in both years increasing to a maximum in mid may to early june then progressively decreasing to a minimum in september fig 5a rainfall clearly affected transpiration generally the td values increased immediately after rainfall in control plots ranges in td were 0 26 2 4 mm d 1 in 2019 and 0 1 2 8 mm day 1 in 2020 corresponding ranges in the partitioned plots were 0 06 2 12 and 0 08 2 0 mm day 1 respectively at the monthly scale transpiration rates were similar in the control and partitioned plots in most months of 2019 except august and september fig 5b however they were significantly lower in partitioned plots p 0 05 than in control plots by 36 on average in 2020 fig 5b especially in the drought period from may to july of 2020 transpiration was much 43 lower in the partitioned plots at the annual scale there was no significant p 0 05 difference in transpiration rates between control and partitioned plots in 2019 but they were significantly lower in partitioned plots p 0 001 in 2020 fig 5c 3 4 seasonal variation in plant physiology throughout the entire growing period in 2020 leaf water potential ψpd and ψmd was lower in the partitioned plots than in the control plots fig 6 overall in these plots ψpd ranged from 0 08 to 0 77 mpa and 0 27 to 1 36 mpa respectively while ψmd ranged from 1 62 to 2 31 mpa and 2 58 to 3 12 mpa respectively thus in the partitioned plots ψpd was 0 14 to 0 36 mpa lower than in control plots in all four growth stages but the differences were not significant p 0 05 however ψmd was significantly lower p 0 05 in partitioned plots than in control plots in the four successive growth stages by 36 11 41 and 41 respectively the pn tr gs and wue values recorded in the control and partitioned plots during the growing season in 2020 are presented in fig 7 pn tr and gs were significantly higher p 0 05 in the control plots especially in dry periods on average pn was 20 higher in the control plots than in partitioned plots during the study period the variations in gs and tr were similar to the variations in pn in contrast the trees in partitioned plots had higher wue than those in control plots especially on the 17 may blossoming fruit bearing and 2 september fruit maturation measuring occasions when the differences in wue δwue between the control and partitioned plots were 6 1 and 1 9 μ mol m 2 s 1 respectively in addition single fruit weight yield soluble sugar soluble solid and fruit shape index were significantly p 0 05 lower in partitioned plots by 31 2 11 2 11 2 47 3 and 8 1 respectively than in control plots table 2 4 discussion the importance of deep rooting and deep soil water for trees has been repeatedly underlined in the last three decades nepstad et al 1994 markewitz et al 2010 christina et al 2017 fan et al 2017 thorup kristensen et al 2020 however few studies have quantitatively evaluated the importance of deep root water uptake for trees in drylands in the study presented here we addressed this issue through a novel experimental approach of isolating deep roots and soils 4 1 ecohydrological response to lack of deep roots and access to deep soil deep rooted apple trees can strongly affect the water balance of ecosystems li et al 2019 wu et al 2021 accordingly we found that the depth averaged water content of soil under trees in the focal dryland orchard tended to decrease substantially with time causing severe deep soil desiccation fig 4 clearly this could have important consequences that warrant rigorous attention including the effects of lack of access to deep water supplies on apple trees e g death or successful adaptation to changes in the soil moisture profile the partitioning experiment reported here was designed to address this issue as summarized in our research question how and to what extent will planted trees respond hydrologically and physiologically if they cannot access deep soil resources in water limited ecosystems there were clearly higher soil moisture deficits in the upper 200 cm in the partitioned plots than in the control plots fig 4 this could be attributed to the lack of access to deep soil stimulating trees to develop shallow root systems thereby increasing the proportion of water taken up from the upper 200 cm fig 3 fig s7 recharge of shallow soil water while deep soil water was still depleted has also reported in pinus elliottii plantations and p massoniana plantations in southeastern china yang et al 2017 a possible explanation is that the deep soil water is a more stable water source for deep rooted plants during drought periods gao et al 2018b deep roots may also be activated when severe water stress results in irreversible embolism or death of shallow roots williams and ehleringer 2000 overall deep rooting is especially important in seasonally dry environments because it enables plants to access deep soil water thereby enhancing their capacity to cope with prolonged drought and obtain potential competitive advantage in drought prone ecosystems christina et al 2017 yang et al 2017 jiang et al 2020 we also found that transpiration was higher in partitioned plots than in control plots in most months of 2019 fig 5b indicating that moderate water stress did not limit transpiration supporting findings by gao et al 2021b that moderate drought may promote transpiration however transpiration rates were much lower in partitioned plots than in control plots in 2020 fig 5 which contributed to the intense declines in shallow water availability caused by the extreme meteorological drought figs 4 and s5 the lack of availability of deep soil water increased water stress to some extent and reduced water uptake thereby leading to reduction in transpiration in contrast deep roots in the control plots enabled plants to access vast deep soil water resources thereby buffering the declines in transpiration during prolonged drought similarly broedel et al 2017 found that in amazonian primary forest deep root uptake down to 5 m resulted in stronger and longer deep soil water depletion than normal during a drought in 2005 but sustained canopy transpiration christina et al 2017 also found that 32 and 57 of water transpired by eucalyptus trees was taken up from 3 to 10 m soil layers during the dry periods in their first 2 years after planting in south east brazil 4 2 physiological responses to lack of deep roots and access to deep soil we found no significant p 0 05 difference in ψpd between control and partitioned plots but ψmd was significantly lower p 0 05 in partitioned plots than in control plots during the growing season of 2020 fig 6 this pattern is consistent with the variation in transpiration in 2020 suggesting that lack of access to deep soil water greatly increased water stress and resulted in lower ψmd clearly ψmd will become more negative during prolonged drought and a decline in ψleaf can lead to downregulation of maximum carboxylation rates vcmax and electron transport rates jmax fig s8 bhusal et al 2020 as well as leaf shedding if leaf turgor cannot be maintained brodribb et al 2002 by contrast deep roots facilitate maintenance of ψleaf during drought periods thereby increasing plants capacities to avoid hydraulic failure bucci et al 2009 ding et al 2021 the wue of the trees was higher in partitioned plots than in control plots although their photosynthesis and transpiration rates were lower during the study period fig 7 this is consistent with expectations as water stress causes closure of stomata galmes et al 2007 misson et al 2010 santos et al 2018 salmon et al 2020 but transpiration rates may decline more drastically than photosynthesis rates under moderate drought stress notably trees in partitioned plots also had lower wue on 20 june and 30 july in the young fruit expansion and fruit expansion stages respectively than on 17 may and 2 september in the blossoming fruit bearing and fruit maturation stages respectively in 2020 this can be attributed to the young fruit and fruit expansion periods coinciding with the prolonged extreme drought period fig s6 and the lack of access to deep soil water forcing the trees to absorb more shallow soil water thereby decreasing the water availability figs 4 s7 and intensifying the water stress in these conditions the relative chlorophyll content also significantly decreased fig s9 resulting in further reduction of photosynthesis rates and thus reduction of wue by contrast water absorption by deep roots may enable trees to maintain higher photosynthetic rates wue hydration and productivity thus increasing their capacity to avoid carbon deficits hydraulic failure and mortality during prolonged extreme drought unfortunately we did not obtain the data of leaf stable carbon isotope δ13c leaf δ13c can be used to assess long term variation in traits that covary with leaf gas exchange carbon uptake and water relations including intrinsit water use effieiency wuei which is the ratio of carbon fixed via photosynthesis to water vapour losses via stomatal conductance lavergne et al 2020 thus variations in leaf δ13c warrant attention in future studies to clarify its response to prolonged deep soil water deficits 4 3 the mechanism of effects of deep soil and roots on trees performance we propose a model to in depth explain the observed effects of lack of deep root water uptake on apple trees performance fig 8 first it leads to insufficient water availability in the absence of sufficient precipitation and increases in trees absorption of shallow soil water this exacerbates water stress thereby further reducing water availability and eventually uptake with consequent reductions in transpiration drought induced reductions in transpiration will impair the trees carbon and nitrogen assimilation thus reducing their primary productivity and resistance to water stress pests and pathogens mcdowell et al 2008 gessler et al 2017 in addition ψleaf becomes increasingly negative with increasing water stress and hydraulic conductance may decrease through several biophysical and physiological mechanisms including irreversible collapse of leaf veins zhang et al 2016 and regulation of aquaporins in cell membranes mcelrone et al 2007 these effects may cause embolism choat et al 2018 thus further reducing delivery of water to the canopy and pronounced reductions in canopy leaf area moreover declines in ψleaf will limit jmax and vcmax thus reducing the efficiency of solar energy utilization and carbon fixation galmes et al 2007 water stress also leads to stomatal closure with a series of negative consequences on short time scales these include cessation of photosynthetic co2 assimilation reduction of leaf cooling and increases in probability of photodamage choat et al 2018 santos et al 2018 on long time scales low photosynthetic rates lead to low translocation of sugars through the phloem and hence depletion of non structural carbohydrate pools mitchell et al 2013 sevanto et al 2014 low photosynthetic rates also reduce the production of chemical defense compounds thus increasing plants vulnerability to pests and pathogens mcdowell et al 2011 in summary lack of deep roots and access to deep soil resources is likely to have adverse effects on trees hydraulics e g by impairing ψleaf and osmotic adjustment and c balance e g by reducing transpiration and photosynthesis which may cause lower resistance to pests and pathogens e g by reducing production of chemical defense compounds mcdowell et al 2011 dietze et al 2014 these effects may lead to hydraulic failure c starvation and or increases in severity of pest and pathogens attacks low yields and quality of plantations table 2 and higher risks of tree mortality 4 4 theoretical insights and practical implications our findings have important implications for ecosystem sustainability and future vegetation dynamics under climatic drying first the findings regarding differential responses of trees functional traits to lack of access to deep water provide indications of likely changes in hydrological and physiological processes associated with climate change second acquisition of deep water is arguably the most effective drought avoidance mechanism for dryland plantations our findings illuminate trees responses to lack of deep root water uptake and help efforts to assess how and to what extent deep soil water affects trees growth when there are no natural analogs this is highly important for enhancing understanding of plants deep root functions as well as relationships between deep soil resource constraints and plant traits third use of nature based solutions including trees has been globally advocated to address sustainability issues seddon et al 2019 mori 2020 this study may provide practical insights into the dynamics of planted trees in drylands if there is no access to deep soil water in future extreme climates 5 conclusions this study highlights the importance of deep root water uptake for the maintenance of plant transpiration leaf water potential photosynthesis and water use efficiency during prolonged drought in water limited ecosystems trees without deep root systems can only access shallow water which is most easily accessible but its content rapidly fluctuates so they will be at high risk of drought induced mortality with increases in drought frequency and or duration specifically the absence of deep soil water would decrease 36 in transpiration 20 in photosynthesis rate and 32 in mid day leaf water potential consequently the yield and quality of apple trees decreased 11 and 15 respectively overall the study enhances understanding of effects of lack of deep roots and access to deep soil resources on plants responses to drought and provide insights into their roles in hydrological processes and plantation dynamics in drought vulnerable ecosystems with significant implication for the long term sustainability of plantation in a changing climate credit authorship contribution statement min yang data curation formal analysis investigation methodology validation visualization writing original draft xiaodong gao conceptualization funding acquisition validation project administration resources data curation writing review editing shaofei wang investigation data curation methodology visualization xining zhao conceptualization funding acquisition validation methodology visualization resources data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thanks prof ying fan reinfelder for her constructive suggestions and miaotai ye and bo yang for their help in sampling this work was jointly supported by the national key research and development program of china grant no 2021yfd1900700 national natural science foundation of china grant no 41771316 42125705 the shaanxi key research and development program grant no 2020zdlny07 04 natural science basic research program of shaanxi grant no 2021jc 19 the cyrus tang foundation and the 111 project grant no b12007 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127471 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
