index,text
26005,the swat model is a well documented hydrologic model however some studies report that the existing swat auto irrigation methods are unable to represent actual irrigation management particularly in intensively irrigated regions in the u s great plains the swat model does not reproduce the management allowed depletion mad irrigation scheduling commonly used by researchers and producers to this purpose the swat source code has been modified to include the mad auto irrigation function this study evaluated the performance of the soil water content swc corrected soil water content cswc plant water demand pwd and mad auto irrigation methods in streamflow and irrigation simulations by comparison with observed data the cswc and mad methods performed the best in streamflow simulations with nse 0 75 and pbias within 11 comparisons of simulated irrigation with the field irrigation also indicated the cswc and mad methods outperformed other methods with the nse 0 75 and pbias within 5 keywords auto irrigation method management allowed depletion soil water depletion plant water demand streamflow crop yield 1 introduction hydrologic models are commonly used as an alternative to the experimental method in order to estimate water balance at various scales chen et al 2018a uniyal and dietrich 2019 wang et al 2018 water balance is based on the principle that for a given period the total water inputs in a system must be equal to the total water outputs plus the net change in system storage a more accurate determination of the water balance is crucial for water resources planning and management in semi arid or arid regions water is not only treated as a limited resource but also as a commodity dechmi et al 2012 irrigation is the most important water input to overcome inadequate rainfall for sustainable crop production irrigation contributes significantly to water balance in many agricultural regions therefore a more accurate and representative estimation of irrigation scheduling irrigation amount and frequency can lead to improved water balance simulations and enhanced confidence for further what if scenarios in water limited environments the soil and water assessment tool swat arnold et al 1998 is one of the most popular hydrologic models which is used worldwide for water resources management at watershed scale brauer et al 2015 chen et al 2016 cibin et al 2016 uniyal et al 2019 wu et al 2019 the majority of swat model calibrations have relied on water outputs such as streamflow actual crop evapotranspiration etc marek et al 2016a b and or soil moisture qi et al 2018 however a very limited number of studies have considered water input elements such as irrigation data as an important factor for swat calibration or evaluation brauer and gitz 2012 chen et al 2018a recently several studies highlighted that the existing auto irrigation functions in swat were unable to adequately represent actual irrigation management in the intensively irrigated watersheds or fields akhavan et al 2010 chen et al 2017 2019 dechmi et al 2012 mcinerney et al 2018 uniyal et al 2019 xie and cui 2011 specifically uniyal et al 2019 and uniyal and dietrich 2019 found that the irrigation amounts simulated using the soil water content swc method in swat were 2 3 times greater than the amounts using the plant water demand pwd method and indicated a large deviation from the actual field irrigation amounts in hamerstorf lower saxony germany consequently uniyal and dietrich 2019 modified the pwd auto irrigation method by replacing the existing root density distribution function with a new function proposed by li et al 1999 and incorporating a dynamic estimation of the plant water uptake compensation factor epco into a modified swat model their findings revealed that the modified swat model was able to simulate the irrigation amounts with an acceptable bias for corn zea mays l winter wheat triticum aestivum l potato solanum tuberosum l and sugar beet beta vulgaris l furthermore the modified swat model was used to simulate the irrigation amounts for three different agro climatic watersheds in germany vietnam and india results indicated improved irrigation simulations in terms of long term and annual irrigation amounts compared to the existing swat pwd auto irrigation method the swat manual irrigation method allows users to manually input the irrigation amount for specific dates arnold et al 2012 however if the amount of water specified exceeds the amount of water required to fill the soil profile to the field capacity the excess water is returned to the irrigation source and is not considered in the calculation of daily soil water balance dechmi et al 2012 in addition due to the dearth of measured irrigation data on a watershed scale the manual irrigation method has rarely been used in the spatially distributed swat model for watershed scale studies mcinerney et al 2018 introduced a spatially variable event based irrigation schedule model which was designed to represent the spatial and temporal variation of the irrigation input irrigation depths at the daily time scale this approach generated irrigation schedules prior to the swat model run and hence fell into the category of manual irrigation method their study found that this new irrigation schedule model led to improved watershed streamflow simulation using swat and enabled uncertainty decomposition in the murray darling basin australia as for the existing swat swc auto irrigation method chen et al 2017 provided a summary of the shortcomings they demonstrated that the method provided a satisfactory simulation of etc after calibration but the simulated irrigation was overestimated compared to actual field irrigation the major reason for the overirrigation is due to the continuance of irrigation scheduling after crop harvest marek et al 2017 chen et al 2017 therefore chen et al 2018a developed a new and representative management allowed depletion mad auto irrigation method using a swat single hydrologic response unit hru method at field level by correcting the identified deficiencies of the initial swc auto irrigation method and including the maximum crop rooting depth for the mad threshold calculation subsequently the mad auto irrigation function was extensively evaluated using data from six corn study sites using the swat single hru method across five states in the u s great plains including texas new mexico kansas colorado and nebraska chen et al 2019a evaluation results indicated that the existing swc and pwd auto irrigation methods in swat clearly overestimated and underestimated the irrigation amount when compared to the observed field irrigation amount respectively however the mad auto irrigation method performed reasonably chen et al 2019a this research is a follow up study to evaluate the performance of the mad auto irrigation method in simulating streamflow irrigation and crop yield at the watershed scale with respect to both terrestrial and river processes the objectives of this study were to 1 evaluate the performance of the corrected swc mad and existing auto irrigation methods in swat for streamflow simulation in the intensively irrigated palo duro watershed and 2 compare the simulated irrigation and crop yield using the corrected swc mad and two existing auto irrigation methods in swat with measured data at moore county and from a field study within the palo duro watershed 2 materials and methods 2 1 study watershed the palo duro watershed huc 11100104 in the texas high plains thp has a total delineated area of approximately 2600 km2 the watershed is located within the counties of hartley moore sherman hansford and hutchinson fig 1 the long term 1995 2014 average annual precipitation across the watershed varies from 297 to 417 mm and the long term average annual maximum and minimum temperatures are approximately 22 c and 5 c respectively the topography of the watershed is relatively flat with elevations from 905 to 1208 m there is a long history of irrigated corn and dual purpose winter wheat production in this watershed fig 2 the primary soil types are sherm silty clay loam soil fine mixed mesic torrertic paleustolls 60 of the total watershed area sunray loam soil fine silty mixed superactive mesic calcidic paleustolls 11 and gruver clay loam soil fine mixed superactive mesic aridic paleustolls 7 soil survey staff 2014 fig 3 three agro meteorological stations operated by the texas high plains evapotranspiration txhpet network marek et al 2010 measured daily precipitation minimum and maximum temperatures solar radiation wind speed and relative humidity from 1995 to 2014 these weather data were used for the watershed study fig 1 available streamflow data from the u s geological survey usgs gage 07233500 from 2000 to 2014 were used to calibrate the swat model in the semi arid and relatively flat thp there are only a limited number of usgs gages and they have recorded many zero streamflow values particularly in dry years this posed some challenges for the swat model calibration using streamflow therefore the measured county level irrigation which is a dominant water input component and crop yield were also used for verifying the credibility of the swat model the palo duro watershed has experienced a significant land use change from corn to cotton gossypium hirsutum l production since 2014 national agricultural statistics service cropland data layer nass cdl therefore it is both timely and meaningful to set up and calibrate a swat watershed study using a more representative mad irrigation function to further evaluate the impacts of emerging land use change on hydrological processes 2 2 swat model description the swat model is a continuous time process based semi distributed and watershed scale hydrological model arnold et al 1998 the primary model components include hydrology crop growth and water quality and the major model inputs are elevation land use land cover soil weather and management practices srinivasan et al 2010 arcswat version 2012 10 2 19 and revision 664 for arcgis 10 2 2 platform was used in this study the swat calibration and uncertainty procedures 2012 swat cup 2012 with the sequential uncertainty fitting version 2 sufi 2 abbaspour et al 2007 was used for the parameter sensitivity analysis model calibration and validation in view of streamflow 2 3 swat model setup 2 3 1 digital elevation model dem land use soil and slope the dem of the watershed with a horizontal resolution of 30 30 m was downloaded from the usgs https viewer nationalmap gov basic and used for estimation of watershed terrain related parameters the 2008 nass cdl http nassgeodata gmu edu cropscape was used for acreage and land use distribution the dominant agricultural land uses in the watershed in 2008 were grain corn winter wheat and grain sorghum sorghum bicolor l which occupied 15 34 and 6 of the watershed area fig 2 about 35 of the watershed area was covered in grassland fine scale soil data were obtained from the soil survey geographic database ssurgo soil survey staff 2014 the watershed was classified into four slope groups 1 1 3 3 5 and 5 2 3 2 hydrologic response unit hru the hru is the basic building block of swat which consists of homogeneous land use soil properties and slope swat computations are performed at the hru level and are then aggregated to the subbasin outlet which is routed through the river system the size threshold used for the subbasin creation is 5000 ha in this watershed for the hru definition thresholds of 1 3 and 5 were used for land use soil and slope respectively the number of subbasins and hrus identified for the study watershed were 34 and 3 151 respectively 2 3 3 meteorological data daily meteorological data were obtained from the research grade agro meteorological stations rather than from a publicly accessible source such as the national oceanic and atmospheric administration national centers for environmental information noaa ncei in a previous study the research grade meteorological stations were used to evaluate the quality of the existing noaa ncei climate data the results found the quality of the publicly accessible noaa ncei climate data is generally poor in the thp marek et al 2010 in addition there was an apparent lack of quality assurance quality control qa qc with the available noaa ncei datasets holman et al 2014 consequently it was necessary to use the collected climate data from the research grade meteorological stations climate data of three meteorological stations for the period 1995 2014 were used in this study these meteorological stations were maintained in accordance with the american society of civil engineers environmental and water resources institute asce ewri specifications asce 2005 with a qa qc program experienced research personnel were responsible for routine station maintenance and data quality assurance 2 3 4 management practices of different land uses land use management practices for three dominant crops grain corn grain sorghum and winter wheat were assigned based on the local field experiments at etter tx fig 1 and nearby bushland large weighing lysimeter data chen et al 2018b detailed swat management practices for the three dominant crops can be found in chen et al 2018b the grassland was simulated as southwestern u s range and the most commonly adopted heavy continuous grazing management was simulated on the grassland park et al 2017 detailed management practices for the grassland were set up according to park et al 2017 swat study in clear creek watershed in north central texas in this study management practices for the major land uses in the swat model were scheduled by the specific date the management parameters for other small areas of land uses were mostly maintained using default values according to nass county wise corn and sorghum acreage estimates over the period from 1995 to 2014 nass 2019 nearly all the grain corn and sorghum areas were managed under irrigated conditions in the palo duro watershed as for the winter wheat land use approximately 11 of the watershed area was managed under irrigated conditions according to the nass report center pivot irrigation is the dominant irrigation system in the region making the irrigated circular fields easy to identify from remote sensing images therefore subbasins containing a large number of circular patterns under the winter wheat land use which represent center pivot irrigated areas were considered as irrigated winter wheat subbasins fig 4 it was also verified that the total irrigated winter wheat area was 33 of the entire winter wheat growing areas in the watershed in this study auto irrigation functions were used to schedule irrigation for 11 of the winter wheat and all corn and sorghum acreages in the palo duro watershed 2 3 5 auto irrigation scheduling two existing swat auto irrigation functions a corrected swc method and a newly developed mad auto irrigation method chen et al 2018a were used to schedule irrigation for the irrigated land uses the swat default auto irrigation functions are pwd and swc methods the descriptions and explanations regarding the swat default auto irrigation methods are provided in neitsch et al 2011 it is worth noting that the swat default swc auto irrigation method applies irrigation strictly according to the user defined trigger threshold of soil water deficit and does not suspend irrigation prior to planting or after crop harvest therefore whenever the trigger threshold of soil water deficit is reached an irrigation event will be scheduled irrespective of the crop growing season to overcome this shortcoming a trigger value of 9999 mm of soil water deficit was included on the same date of crop harvesting for the corrected swc method 2 4 observed streamflow irrigation and crop yield data observed monthly streamflow data recorded at the usgs gage 07233500 for the time period from 2000 to 2014 were obtained from the usgs national water information system http waterdata usgs gov nwis sw and used for swat calibration field agronomic data for corn were available from 2011 to 2014 at etter texas in the palo duro watershed all the irrigation events were recorded for the study years at etter chen et al 2019b bi weekly corn growth data including leaf area index lai and aboveground biomass were collected from 2012 to 2014 final corn yields were also collected for four study years and these field data were used for the swat calibration the recorded irrigation data at etter were used to evaluate the swat model performance using different auto irrigation methods in this study the measured county level irrigation and crop yields at moore county in the palo duro watershed were used to provide auxiliary data to calibrate the swat model specifically the county level observed irrigation survey data in 2000 2003 2008 and 2013 nass irrigation and water management survey 2020 and crop yield data nass 2019 for irrigated corn and irrigated and dryland winter wheat from 2000 to 2014 at moore county fig 1 were used for swat calibration the calibrated crop growth parameters from the nearby bushland large weighing lysimeter fields were used initially chen et al 2018b the measured county level irrigation and crop yield data were used to further calibrate the adopted crop growth parameters in this study the final calibrated crop growth parameters in the palo duro watershed are listed in table 1 2 5 swat model calibration and validation the first five years 1995 1999 of swat simulations were used as the swat warmup period the streamflow data from 2000 to 2014 were divided into two parts and the data for the 2000 2006 and 2007 2014 periods were used for swat calibration and validation respectively in this study a decision of which auto irrigation method to use for calibration of the swat model had to be made since the mad auto irrigation is a more representative auto irrigation method in the study region callison 2012 usda nrcs 2005 u s department of the interior bureau of reclamation 2020 chen et al 2018a chen et al 2019a it was selected for calibration the soil evaporation compensation factor esco epco and crop growth parameters were then calibrated manually to match the county level annual irrigation amounts and crop yields at moore county table 1 in addition the sensitive hydrologic parameters were identified by performing sensitivity analysis using the swat cup sufi 2 for 1000 simulations using each auto irrigation method with the goal of maximizing nash sutcliffe efficiency nse abbaspour et al 2007 mehan et al 2017 tejaswini and sathian 2018 the identified sensitive parameters and the calibrated values for the streamflow simulation are listed in table 2 and table 3 respectively finally the calibrated parameters for streamflow were adjusted manually to achieve the best calibrated swat models according to model performance statistics using different auto irrigation methods in this study table 3 after calibration the swat model was validated based on the streamflow data for the period 2007 2014 after achieving a satisfactory streamflow calibration the swat model was used for predictions of irrigation at the etter field site with four auto irrigation methods the swat model performance in streamflow prediction during calibration and validation periods was evaluated using four different statistical measures coefficient of determination r 2 legates and mccabe 1999 nse nash and sutcliffe 1970 kling gupta efficiency kge gupta et al 2009 kling et al 2012 and percent bias pbias the r 2 represents the proportion of total variance in the observed data that can be explained by the model the r 2 ranges from 0 to 1 with higher values denoting better model performance the nse indicates how well the plot of observed vs simulated values fits on the 1 1 line it ranges from to 1 and the nse values closer to 1 indicate the better model performance the kge combines the nse components correlation bias and coefficients of variation in a more balanced way it corrects the underestimation of variability and provides a direct assessment of four aspects of streamflow time series namely shape timing water balance and variability the kge values also range between to 1 and the kge values closer to 1 represent the better model performance the pbias varies between 100 and with smaller absolute values closer to 0 indicating better agreement in this study the goal of the model calibration is to achieve nse r 2 kge and pbias of 0 75 0 75 0 55 and within 15 respectively during both the calibration and validation periods moriasi et al 2007 2 6 scenario analysis for the scenario analysis four auto irrigation strategies swc corrected soil water content cswc pwd and mad auto irrigation were compared in simulating streamflow county level irrigation and county level crop yields in the palo duro watershed and in predicting monthly irrigation at the etter study field specifically field scale measured irrigation data from the etter station were used to evaluate the swat model performance at the hru level using four auto irrigation methods at the regional scale in addition to the streamflow comparison the observed county level irrigation and crop yields at moore county in the palo duro watershed were compared to the swat simulated values from the different auto irrigation methods the newly developed mad auto irrigation method a cswc method and two swat default auto irrigation methods of pwd and swc were used in the comparison the irrigation trigger values used for the mad method were 0 4 0 6 which represents the commonly used values in actual irrigation management for commodity crops under full irrigation conditions in the thp callison 2012 usda nrcs 2005 u s department of the interior bureau of reclamation 2020 as for the trigger value of the swc method the value of 100 mm of soil water depletion was used which was an approximation according to the mad value of 0 5 for comparison purposes a suggested plant water stress threshold value of 0 95 was used for the pwd auto irrigation method according to the swat input output file documentation arnold et al 2012 since the irrigation can be applied during the non crop growing season when using the default swc method chen et al 2018a the non growing season irrigation was suspended by switching the irrigation trigger value to 9999 mm of soil water depletion on the same date of the harvest and kill function therefore this auto irrigation strategy was referred to as the corrected swc auto irrigation method in this study 3 results and discussion 3 1 swat sensitivity analysis and performance statistics for streamflow simulation using different auto irrigation methods the sensitive hydrologic parameters for streamflow calibration were identified based on the global sensitivity analysis using swat cup sufi 2 algorithm for 1000 simulations to maximize nse table 2 the initial ranges and the calibrated values of the hydrologic parameters are presented in table 3 results suggested that soil conservation service scs curve number for moisture condition ii cn2 groundwater revap coefficient gw revap and baseflow recession constant alpha bf were the most sensitive parameters to influence streamflow in the study watershed p 0 05 in addition groundwater delay gw delay manning s n value for overland flow ov n available soil water capacity sol awc threshold depth of water in the shallow aquifer required for return flow to occur gwqmn minimum melt rate for snow during the year smfmn and manning s n value for the main channel ch n2 were moderately sensitive parameters the relatively less sensitive parameters such as maximum melt rate for snow during the year smfmx snowfall temperature sftmp epco etc were also used for the streamflow calibration table 2 for 1000 simulations using each auto irrigation method total four iterations the best simulation number 885 was manually identified in this study to improve the model performance statistic of pbias and the model performance during the validation period the most sensitive parameter of cn2 mehan et al 2017 opere and okello 2011 was further manually adjusted slightly for different auto irrigation methods table 3 after calibrations the newly developed mad and cswc auto irrigation methods achieved the targeted goal of the model calibration during the calibration period 2000 2006 for streamflow simulations with the nse r 2 kge and pbias of 0 77 0 91 0 57 and 9 4 fig 5 a and 0 79 0 90 0 60 and 9 6 respectively the default swc auto irrigation method in swat indicated the smallest kge of 0 52 during the calibration period the pwd auto irrigation method denoted the lowest of nse and r 2 values of 0 74 and 0 89 among the four auto irrigation methods table 4 the four auto irrigation methods all worked very well for streamflow simulation during the validation period 2007 2014 with the nse r 2 kge and pbias values 0 90 0 90 0 85 and within 15 according to the criteria of moriasi et al 2007 and the targeted goal in this study the swat model performance statistics using mad fig 5 and cswc auto irrigation methods in this study demonstrate a good agreement between the simulated and observed streamflow for both calibration and validation periods harmel et al 2014 suggested considering the purpose of the model use while evaluating their performance since this study primarily focused on assessing the performance of a newly developed mad a cswc and two existing swat auto irrigation methods on monthly irrigation and streamflow simulations achieving a good model performance on a monthly time step was treated as appropriate to use the calibrated models 3 2 comparisons of county level swat simulated annual irrigation amounts and crop yields with observed data at moore county the simulated annual irrigation amounts for corn at moore county in 2003 and 2008 were underestimated compared to the observed data using four auto irrigation methods with the pwd method indicating a more than 20 underestimation fig 6 and fig s1 however the corn yield comparisons from 2000 to 2014 using four auto irrigation methods showed a reasonable match with an r 2 of 0 32 and overall pbias within 2 fig 7 and fig s2 the clear overestimations of the irrigation amounts for winter wheat at moore county were found using the swc cswc and pwd auto irrigation methods fig 8 and fig s3 the commercial fields particularly the winter wheat fields in the thp are usually not well managed compared to the research farms in fact winter wheat is generally not fully irrigated as marginal increases in yield do not offset input costs marek et al 2018 production data from producers revealed a more than 30 reduction in irrigation water for limited irrigation winter wheat compared to corn production in the thp texas water development board 2011 therefore a mad value of 0 75 representing a typically limited irrigation management condition was used for winter wheat in this case chen et al 2019a results showed a good match between simulated and observed irrigation using this deficit mad value fig 8d simulated irrigated winter wheat yield from 2000 to 2012 at moore county had a large interannual bias compared to the measured yield with the low r 2 values lower than 0 03 for the four auto irrigation methods however the overall pbias values were 7 7 and 12 for mad and the other three auto irrigation methods fig 9 and fig s4 the simulated county level dryland winter wheat yield matched well with the observed data with the r 2 and overall pbias values of 0 79 and 2 1 respectively for all auto irrigation methods fig s5 overall model predictions of winter wheat yields were better under dryland conditions than under irrigated conditions using swat mittelstet et al 2015 also found that the relationship between swat simulated county level dryland wheat yields versus the nass observed wheat yields had an r 2 of 0 61 in the north fork river basin of the southwestern oklahoma and the texas panhandle 3 3 comparison of field scale swat simulated monthly irrigation using different auto irrigation methods to actual irrigation at etter using a single hru in a previous study considering actual irrigation data as input for the corn hru at the etter study site very good agreement of lai nse r 2 and pbias 0 70 0 70 and 3 1 and aboveground biomass 0 95 0 95 and 4 4 were found after calibration during the corn growing periods of 2011 2014 chen et al 2019b the pbias values for simulating corn yield of each year from 2011 to 2014 were within 10 indicating a good agreement chen et al 2019b in this study the calibrated hydrologic parameters in the palo duro watershed were used for the corn hru at etter for further irrigation comparisons using the four auto irrigation methods monthly irrigation comparisons at the etter study site indicated the mad and cswc auto irrigation methods nse 0 75 and pbias within 5 outperformed the other two swat default auto irrigation strategies nse values 0 73 when compared to the actual irrigation fig 10 the simulated irrigation amount using the swat swc auto irrigation method was larger pbias 10 3 than that of the actual irrigation value fig 10a this was in part due to a deficiency in the swat swc auto irrigation method that applied irrigation during the crop non growing season chen et al 2017 for instance simulated irrigation events could occur from january to april of 2012 2014 fig 10a by using the cswc auto irrigation method the model performance statistics of nse r 2 and pbias values in predicting irrigation improved from 0 71 0 72 and 10 3 to 0 77 0 77 and 1 6 fig 10b in most cases the real world field irrigation is scheduled according to measured estimated soil water depletion or management allowed depletion of plant available water rather than the pwd method the cswc method can be used to represent the actual irrigation trigger and management in the field conditions which may partially explain the good irrigation simulation the pbias was 14 8 when using the pwd auto irrigation method fig 7c the pwd auto irrigation method clearly underestimated the irrigation amount this is due to the method that triggers an irrigation once a given reduction in plant growth occurs due to water stress in this case the plant needs to experience water stress before the irrigation event is scheduled therefore the simulated irrigation amount tends to be lower compared to the actual irrigation with corn management for high yield potential and with minimal or no water stress chen et al 2019a uniyal and dietrich 2019 also found the irrigation amounts simulated using the pwd method in swat were 2 3 times less than the simulated values using the swc method at the basin scale and had a large deviation from the actual field irrigation amounts in germany 4 conclusions and remarks in this study four auto irrigation methods swc cswc pwd and mad were evaluated on a watershed scale using observed streamflow and county level irrigation and crop yield data results found that the cswc and mad methods outperformed the other two auto irrigation methods in simulations of streamflow at the watershed outlet and county level irrigation specifically the cswc and mad auto irrigation methods achieved a good model performance for streamflow simulation with nse 0 75 kge 0 55 and pbias within 11 during both the calibration and validation periods generally cswc and mad methods achieved the best simulations of county level irrigation and crop yields of corn and winter wheat as for the comparison of irrigation at the etter study site using a single hru the existing swc auto irrigation method tended to overestimate irrigation with irrigation applications during the non crop growing season nse and pbias of 0 71 and 10 3 however the cswc auto irrigation method clearly improved irrigation simulation with nse and pbias values of 0 77 and 1 6 the existing pwd auto irrigation method underestimated the irrigation amount pbias of 14 8 finally the mad auto irrigation method achieved the best performance in view of an nse of 0 78 experimental fields are usually managed by the researchers who maintain detailed records of agronomic information for specific research studies management practices and production goals may vary for surrounding agricultural area this in part may explain the good agreement in simulating irrigation at the field scale as compared to the county level comparison however the county level yield comparison is necessary for evaluating the swat performance in predicting crop growth and components of the regional hydrological cycle recently swat is increasingly used to simulate field scale studies using the single hru method chen et al 2019a cibin et al 2015 marek et al 2016a moloney et al 2015 the more accurate simulation of the irrigation scheduling can provide applied value for researchers and producers to manage irrigation using simulation models for instance thorp et al 2017 compared field irrigation treatment determined by the calibrated dssat csm cropgro cotton simulation model to the commonly used spreadsheet field irrigation scheduling at maricopa agricultural center arizona they concluded that cotton irrigation scheduling with calibrated dssat led to similar irrigation amounts but with a slightly different seasonal irrigation distribution and equal or higher cotton yield through a field experiment in ira oasis china from 2016 to 2018 chen et al 2020 also reported the irrigation scheduling simulated by the root zone water quality model rzwqm2 resulted in significant increases in seed cotton yield 32 and water productivity 20 compared to the soil water sensor based irrigation scheduling method from this view the calibrated swat mad model may also be used to assist in actual field irrigation scheduling the newly developed mad auto irrigation method is not free from limitations for example a general limitation of auto irrigation approaches is the assumption that daily irrigation occurs whenever the plant soil needs water mcinerney et al 2018 this assumption may not hold true in practice due to daily irrigation schedules varying from producer to producer on a watershed scale and may be constrained by water delivery systems and water restrictions such as well capacity in addition after the setup the auto irrigation methods allow a uniform irrigation depth such as 25 4 mm 1 in per irrigation event as used in this study neitsch et al 2011 however the spatial variation of the irrigation depths in the thp can range from 12 7 mm 0 5 in to 38 1 mm 1 5 in for the same crop according to the well capacities crop water demand and soil water holding capacity marek et al 2020 since these limitations also apply to the mad auto irrigation method it may not be appropriate to compare simulated irrigation using the mad method to the actual field irrigation on a daily scale or according to individual irrigation events recently mcinerney et al 2018 developed a spatially variable event based irrigation scheduling method this method can represent the spatial and temporal variations of the irrigation depths on a daily basis the integration of this method with the mad auto irrigation method for the watershed scale swat study should be the right direction for future work the mad auto irrigation method was developed according to ten year large weighing lysimeter measured data at the usda ars cprl bushland and was further successfully evaluated at the field scale across multiple corn production locations with different climatic conditions and soil types in the u s great plains in this study this method was further tested in an intensively irrigated watershed in the thp with respect to the hydrologic cycle for both terrestrial and aquatic processes these results and evaluations demonstrate the robust nature of the mad auto irrigation method and reinforce its potential as an effective irrigation scheduling tool in assessing scenarios related to irrigation management strategies which should be useful in evaluating alternative irrigation management strategies and subsequent water management and planning efforts software availability name of software swat mad auto irrigation description the swat mad auto irrigation function simulates irrigation scheduling during crop growing season by incorporating an allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and site specific soil properties developers yong chen and gary w marek year available 2018 availability contact the developers cost free language fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by the ogallala aquifer program a consortium between usda agricultural research service kansas state university texas a m agrilife research texas a m agrilife extension service texas tech university and west texas a m university the txhpet data sets used were acquired with funding support from the ogallala aquifer program and grants from state and local water conservation agencies and commodity organizations the txhpet data supported more than 20 hatch projects we gratefully thank the three anonymous reviewers for their valuable suggestions for improving this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104789 
26005,the swat model is a well documented hydrologic model however some studies report that the existing swat auto irrigation methods are unable to represent actual irrigation management particularly in intensively irrigated regions in the u s great plains the swat model does not reproduce the management allowed depletion mad irrigation scheduling commonly used by researchers and producers to this purpose the swat source code has been modified to include the mad auto irrigation function this study evaluated the performance of the soil water content swc corrected soil water content cswc plant water demand pwd and mad auto irrigation methods in streamflow and irrigation simulations by comparison with observed data the cswc and mad methods performed the best in streamflow simulations with nse 0 75 and pbias within 11 comparisons of simulated irrigation with the field irrigation also indicated the cswc and mad methods outperformed other methods with the nse 0 75 and pbias within 5 keywords auto irrigation method management allowed depletion soil water depletion plant water demand streamflow crop yield 1 introduction hydrologic models are commonly used as an alternative to the experimental method in order to estimate water balance at various scales chen et al 2018a uniyal and dietrich 2019 wang et al 2018 water balance is based on the principle that for a given period the total water inputs in a system must be equal to the total water outputs plus the net change in system storage a more accurate determination of the water balance is crucial for water resources planning and management in semi arid or arid regions water is not only treated as a limited resource but also as a commodity dechmi et al 2012 irrigation is the most important water input to overcome inadequate rainfall for sustainable crop production irrigation contributes significantly to water balance in many agricultural regions therefore a more accurate and representative estimation of irrigation scheduling irrigation amount and frequency can lead to improved water balance simulations and enhanced confidence for further what if scenarios in water limited environments the soil and water assessment tool swat arnold et al 1998 is one of the most popular hydrologic models which is used worldwide for water resources management at watershed scale brauer et al 2015 chen et al 2016 cibin et al 2016 uniyal et al 2019 wu et al 2019 the majority of swat model calibrations have relied on water outputs such as streamflow actual crop evapotranspiration etc marek et al 2016a b and or soil moisture qi et al 2018 however a very limited number of studies have considered water input elements such as irrigation data as an important factor for swat calibration or evaluation brauer and gitz 2012 chen et al 2018a recently several studies highlighted that the existing auto irrigation functions in swat were unable to adequately represent actual irrigation management in the intensively irrigated watersheds or fields akhavan et al 2010 chen et al 2017 2019 dechmi et al 2012 mcinerney et al 2018 uniyal et al 2019 xie and cui 2011 specifically uniyal et al 2019 and uniyal and dietrich 2019 found that the irrigation amounts simulated using the soil water content swc method in swat were 2 3 times greater than the amounts using the plant water demand pwd method and indicated a large deviation from the actual field irrigation amounts in hamerstorf lower saxony germany consequently uniyal and dietrich 2019 modified the pwd auto irrigation method by replacing the existing root density distribution function with a new function proposed by li et al 1999 and incorporating a dynamic estimation of the plant water uptake compensation factor epco into a modified swat model their findings revealed that the modified swat model was able to simulate the irrigation amounts with an acceptable bias for corn zea mays l winter wheat triticum aestivum l potato solanum tuberosum l and sugar beet beta vulgaris l furthermore the modified swat model was used to simulate the irrigation amounts for three different agro climatic watersheds in germany vietnam and india results indicated improved irrigation simulations in terms of long term and annual irrigation amounts compared to the existing swat pwd auto irrigation method the swat manual irrigation method allows users to manually input the irrigation amount for specific dates arnold et al 2012 however if the amount of water specified exceeds the amount of water required to fill the soil profile to the field capacity the excess water is returned to the irrigation source and is not considered in the calculation of daily soil water balance dechmi et al 2012 in addition due to the dearth of measured irrigation data on a watershed scale the manual irrigation method has rarely been used in the spatially distributed swat model for watershed scale studies mcinerney et al 2018 introduced a spatially variable event based irrigation schedule model which was designed to represent the spatial and temporal variation of the irrigation input irrigation depths at the daily time scale this approach generated irrigation schedules prior to the swat model run and hence fell into the category of manual irrigation method their study found that this new irrigation schedule model led to improved watershed streamflow simulation using swat and enabled uncertainty decomposition in the murray darling basin australia as for the existing swat swc auto irrigation method chen et al 2017 provided a summary of the shortcomings they demonstrated that the method provided a satisfactory simulation of etc after calibration but the simulated irrigation was overestimated compared to actual field irrigation the major reason for the overirrigation is due to the continuance of irrigation scheduling after crop harvest marek et al 2017 chen et al 2017 therefore chen et al 2018a developed a new and representative management allowed depletion mad auto irrigation method using a swat single hydrologic response unit hru method at field level by correcting the identified deficiencies of the initial swc auto irrigation method and including the maximum crop rooting depth for the mad threshold calculation subsequently the mad auto irrigation function was extensively evaluated using data from six corn study sites using the swat single hru method across five states in the u s great plains including texas new mexico kansas colorado and nebraska chen et al 2019a evaluation results indicated that the existing swc and pwd auto irrigation methods in swat clearly overestimated and underestimated the irrigation amount when compared to the observed field irrigation amount respectively however the mad auto irrigation method performed reasonably chen et al 2019a this research is a follow up study to evaluate the performance of the mad auto irrigation method in simulating streamflow irrigation and crop yield at the watershed scale with respect to both terrestrial and river processes the objectives of this study were to 1 evaluate the performance of the corrected swc mad and existing auto irrigation methods in swat for streamflow simulation in the intensively irrigated palo duro watershed and 2 compare the simulated irrigation and crop yield using the corrected swc mad and two existing auto irrigation methods in swat with measured data at moore county and from a field study within the palo duro watershed 2 materials and methods 2 1 study watershed the palo duro watershed huc 11100104 in the texas high plains thp has a total delineated area of approximately 2600 km2 the watershed is located within the counties of hartley moore sherman hansford and hutchinson fig 1 the long term 1995 2014 average annual precipitation across the watershed varies from 297 to 417 mm and the long term average annual maximum and minimum temperatures are approximately 22 c and 5 c respectively the topography of the watershed is relatively flat with elevations from 905 to 1208 m there is a long history of irrigated corn and dual purpose winter wheat production in this watershed fig 2 the primary soil types are sherm silty clay loam soil fine mixed mesic torrertic paleustolls 60 of the total watershed area sunray loam soil fine silty mixed superactive mesic calcidic paleustolls 11 and gruver clay loam soil fine mixed superactive mesic aridic paleustolls 7 soil survey staff 2014 fig 3 three agro meteorological stations operated by the texas high plains evapotranspiration txhpet network marek et al 2010 measured daily precipitation minimum and maximum temperatures solar radiation wind speed and relative humidity from 1995 to 2014 these weather data were used for the watershed study fig 1 available streamflow data from the u s geological survey usgs gage 07233500 from 2000 to 2014 were used to calibrate the swat model in the semi arid and relatively flat thp there are only a limited number of usgs gages and they have recorded many zero streamflow values particularly in dry years this posed some challenges for the swat model calibration using streamflow therefore the measured county level irrigation which is a dominant water input component and crop yield were also used for verifying the credibility of the swat model the palo duro watershed has experienced a significant land use change from corn to cotton gossypium hirsutum l production since 2014 national agricultural statistics service cropland data layer nass cdl therefore it is both timely and meaningful to set up and calibrate a swat watershed study using a more representative mad irrigation function to further evaluate the impacts of emerging land use change on hydrological processes 2 2 swat model description the swat model is a continuous time process based semi distributed and watershed scale hydrological model arnold et al 1998 the primary model components include hydrology crop growth and water quality and the major model inputs are elevation land use land cover soil weather and management practices srinivasan et al 2010 arcswat version 2012 10 2 19 and revision 664 for arcgis 10 2 2 platform was used in this study the swat calibration and uncertainty procedures 2012 swat cup 2012 with the sequential uncertainty fitting version 2 sufi 2 abbaspour et al 2007 was used for the parameter sensitivity analysis model calibration and validation in view of streamflow 2 3 swat model setup 2 3 1 digital elevation model dem land use soil and slope the dem of the watershed with a horizontal resolution of 30 30 m was downloaded from the usgs https viewer nationalmap gov basic and used for estimation of watershed terrain related parameters the 2008 nass cdl http nassgeodata gmu edu cropscape was used for acreage and land use distribution the dominant agricultural land uses in the watershed in 2008 were grain corn winter wheat and grain sorghum sorghum bicolor l which occupied 15 34 and 6 of the watershed area fig 2 about 35 of the watershed area was covered in grassland fine scale soil data were obtained from the soil survey geographic database ssurgo soil survey staff 2014 the watershed was classified into four slope groups 1 1 3 3 5 and 5 2 3 2 hydrologic response unit hru the hru is the basic building block of swat which consists of homogeneous land use soil properties and slope swat computations are performed at the hru level and are then aggregated to the subbasin outlet which is routed through the river system the size threshold used for the subbasin creation is 5000 ha in this watershed for the hru definition thresholds of 1 3 and 5 were used for land use soil and slope respectively the number of subbasins and hrus identified for the study watershed were 34 and 3 151 respectively 2 3 3 meteorological data daily meteorological data were obtained from the research grade agro meteorological stations rather than from a publicly accessible source such as the national oceanic and atmospheric administration national centers for environmental information noaa ncei in a previous study the research grade meteorological stations were used to evaluate the quality of the existing noaa ncei climate data the results found the quality of the publicly accessible noaa ncei climate data is generally poor in the thp marek et al 2010 in addition there was an apparent lack of quality assurance quality control qa qc with the available noaa ncei datasets holman et al 2014 consequently it was necessary to use the collected climate data from the research grade meteorological stations climate data of three meteorological stations for the period 1995 2014 were used in this study these meteorological stations were maintained in accordance with the american society of civil engineers environmental and water resources institute asce ewri specifications asce 2005 with a qa qc program experienced research personnel were responsible for routine station maintenance and data quality assurance 2 3 4 management practices of different land uses land use management practices for three dominant crops grain corn grain sorghum and winter wheat were assigned based on the local field experiments at etter tx fig 1 and nearby bushland large weighing lysimeter data chen et al 2018b detailed swat management practices for the three dominant crops can be found in chen et al 2018b the grassland was simulated as southwestern u s range and the most commonly adopted heavy continuous grazing management was simulated on the grassland park et al 2017 detailed management practices for the grassland were set up according to park et al 2017 swat study in clear creek watershed in north central texas in this study management practices for the major land uses in the swat model were scheduled by the specific date the management parameters for other small areas of land uses were mostly maintained using default values according to nass county wise corn and sorghum acreage estimates over the period from 1995 to 2014 nass 2019 nearly all the grain corn and sorghum areas were managed under irrigated conditions in the palo duro watershed as for the winter wheat land use approximately 11 of the watershed area was managed under irrigated conditions according to the nass report center pivot irrigation is the dominant irrigation system in the region making the irrigated circular fields easy to identify from remote sensing images therefore subbasins containing a large number of circular patterns under the winter wheat land use which represent center pivot irrigated areas were considered as irrigated winter wheat subbasins fig 4 it was also verified that the total irrigated winter wheat area was 33 of the entire winter wheat growing areas in the watershed in this study auto irrigation functions were used to schedule irrigation for 11 of the winter wheat and all corn and sorghum acreages in the palo duro watershed 2 3 5 auto irrigation scheduling two existing swat auto irrigation functions a corrected swc method and a newly developed mad auto irrigation method chen et al 2018a were used to schedule irrigation for the irrigated land uses the swat default auto irrigation functions are pwd and swc methods the descriptions and explanations regarding the swat default auto irrigation methods are provided in neitsch et al 2011 it is worth noting that the swat default swc auto irrigation method applies irrigation strictly according to the user defined trigger threshold of soil water deficit and does not suspend irrigation prior to planting or after crop harvest therefore whenever the trigger threshold of soil water deficit is reached an irrigation event will be scheduled irrespective of the crop growing season to overcome this shortcoming a trigger value of 9999 mm of soil water deficit was included on the same date of crop harvesting for the corrected swc method 2 4 observed streamflow irrigation and crop yield data observed monthly streamflow data recorded at the usgs gage 07233500 for the time period from 2000 to 2014 were obtained from the usgs national water information system http waterdata usgs gov nwis sw and used for swat calibration field agronomic data for corn were available from 2011 to 2014 at etter texas in the palo duro watershed all the irrigation events were recorded for the study years at etter chen et al 2019b bi weekly corn growth data including leaf area index lai and aboveground biomass were collected from 2012 to 2014 final corn yields were also collected for four study years and these field data were used for the swat calibration the recorded irrigation data at etter were used to evaluate the swat model performance using different auto irrigation methods in this study the measured county level irrigation and crop yields at moore county in the palo duro watershed were used to provide auxiliary data to calibrate the swat model specifically the county level observed irrigation survey data in 2000 2003 2008 and 2013 nass irrigation and water management survey 2020 and crop yield data nass 2019 for irrigated corn and irrigated and dryland winter wheat from 2000 to 2014 at moore county fig 1 were used for swat calibration the calibrated crop growth parameters from the nearby bushland large weighing lysimeter fields were used initially chen et al 2018b the measured county level irrigation and crop yield data were used to further calibrate the adopted crop growth parameters in this study the final calibrated crop growth parameters in the palo duro watershed are listed in table 1 2 5 swat model calibration and validation the first five years 1995 1999 of swat simulations were used as the swat warmup period the streamflow data from 2000 to 2014 were divided into two parts and the data for the 2000 2006 and 2007 2014 periods were used for swat calibration and validation respectively in this study a decision of which auto irrigation method to use for calibration of the swat model had to be made since the mad auto irrigation is a more representative auto irrigation method in the study region callison 2012 usda nrcs 2005 u s department of the interior bureau of reclamation 2020 chen et al 2018a chen et al 2019a it was selected for calibration the soil evaporation compensation factor esco epco and crop growth parameters were then calibrated manually to match the county level annual irrigation amounts and crop yields at moore county table 1 in addition the sensitive hydrologic parameters were identified by performing sensitivity analysis using the swat cup sufi 2 for 1000 simulations using each auto irrigation method with the goal of maximizing nash sutcliffe efficiency nse abbaspour et al 2007 mehan et al 2017 tejaswini and sathian 2018 the identified sensitive parameters and the calibrated values for the streamflow simulation are listed in table 2 and table 3 respectively finally the calibrated parameters for streamflow were adjusted manually to achieve the best calibrated swat models according to model performance statistics using different auto irrigation methods in this study table 3 after calibration the swat model was validated based on the streamflow data for the period 2007 2014 after achieving a satisfactory streamflow calibration the swat model was used for predictions of irrigation at the etter field site with four auto irrigation methods the swat model performance in streamflow prediction during calibration and validation periods was evaluated using four different statistical measures coefficient of determination r 2 legates and mccabe 1999 nse nash and sutcliffe 1970 kling gupta efficiency kge gupta et al 2009 kling et al 2012 and percent bias pbias the r 2 represents the proportion of total variance in the observed data that can be explained by the model the r 2 ranges from 0 to 1 with higher values denoting better model performance the nse indicates how well the plot of observed vs simulated values fits on the 1 1 line it ranges from to 1 and the nse values closer to 1 indicate the better model performance the kge combines the nse components correlation bias and coefficients of variation in a more balanced way it corrects the underestimation of variability and provides a direct assessment of four aspects of streamflow time series namely shape timing water balance and variability the kge values also range between to 1 and the kge values closer to 1 represent the better model performance the pbias varies between 100 and with smaller absolute values closer to 0 indicating better agreement in this study the goal of the model calibration is to achieve nse r 2 kge and pbias of 0 75 0 75 0 55 and within 15 respectively during both the calibration and validation periods moriasi et al 2007 2 6 scenario analysis for the scenario analysis four auto irrigation strategies swc corrected soil water content cswc pwd and mad auto irrigation were compared in simulating streamflow county level irrigation and county level crop yields in the palo duro watershed and in predicting monthly irrigation at the etter study field specifically field scale measured irrigation data from the etter station were used to evaluate the swat model performance at the hru level using four auto irrigation methods at the regional scale in addition to the streamflow comparison the observed county level irrigation and crop yields at moore county in the palo duro watershed were compared to the swat simulated values from the different auto irrigation methods the newly developed mad auto irrigation method a cswc method and two swat default auto irrigation methods of pwd and swc were used in the comparison the irrigation trigger values used for the mad method were 0 4 0 6 which represents the commonly used values in actual irrigation management for commodity crops under full irrigation conditions in the thp callison 2012 usda nrcs 2005 u s department of the interior bureau of reclamation 2020 as for the trigger value of the swc method the value of 100 mm of soil water depletion was used which was an approximation according to the mad value of 0 5 for comparison purposes a suggested plant water stress threshold value of 0 95 was used for the pwd auto irrigation method according to the swat input output file documentation arnold et al 2012 since the irrigation can be applied during the non crop growing season when using the default swc method chen et al 2018a the non growing season irrigation was suspended by switching the irrigation trigger value to 9999 mm of soil water depletion on the same date of the harvest and kill function therefore this auto irrigation strategy was referred to as the corrected swc auto irrigation method in this study 3 results and discussion 3 1 swat sensitivity analysis and performance statistics for streamflow simulation using different auto irrigation methods the sensitive hydrologic parameters for streamflow calibration were identified based on the global sensitivity analysis using swat cup sufi 2 algorithm for 1000 simulations to maximize nse table 2 the initial ranges and the calibrated values of the hydrologic parameters are presented in table 3 results suggested that soil conservation service scs curve number for moisture condition ii cn2 groundwater revap coefficient gw revap and baseflow recession constant alpha bf were the most sensitive parameters to influence streamflow in the study watershed p 0 05 in addition groundwater delay gw delay manning s n value for overland flow ov n available soil water capacity sol awc threshold depth of water in the shallow aquifer required for return flow to occur gwqmn minimum melt rate for snow during the year smfmn and manning s n value for the main channel ch n2 were moderately sensitive parameters the relatively less sensitive parameters such as maximum melt rate for snow during the year smfmx snowfall temperature sftmp epco etc were also used for the streamflow calibration table 2 for 1000 simulations using each auto irrigation method total four iterations the best simulation number 885 was manually identified in this study to improve the model performance statistic of pbias and the model performance during the validation period the most sensitive parameter of cn2 mehan et al 2017 opere and okello 2011 was further manually adjusted slightly for different auto irrigation methods table 3 after calibrations the newly developed mad and cswc auto irrigation methods achieved the targeted goal of the model calibration during the calibration period 2000 2006 for streamflow simulations with the nse r 2 kge and pbias of 0 77 0 91 0 57 and 9 4 fig 5 a and 0 79 0 90 0 60 and 9 6 respectively the default swc auto irrigation method in swat indicated the smallest kge of 0 52 during the calibration period the pwd auto irrigation method denoted the lowest of nse and r 2 values of 0 74 and 0 89 among the four auto irrigation methods table 4 the four auto irrigation methods all worked very well for streamflow simulation during the validation period 2007 2014 with the nse r 2 kge and pbias values 0 90 0 90 0 85 and within 15 according to the criteria of moriasi et al 2007 and the targeted goal in this study the swat model performance statistics using mad fig 5 and cswc auto irrigation methods in this study demonstrate a good agreement between the simulated and observed streamflow for both calibration and validation periods harmel et al 2014 suggested considering the purpose of the model use while evaluating their performance since this study primarily focused on assessing the performance of a newly developed mad a cswc and two existing swat auto irrigation methods on monthly irrigation and streamflow simulations achieving a good model performance on a monthly time step was treated as appropriate to use the calibrated models 3 2 comparisons of county level swat simulated annual irrigation amounts and crop yields with observed data at moore county the simulated annual irrigation amounts for corn at moore county in 2003 and 2008 were underestimated compared to the observed data using four auto irrigation methods with the pwd method indicating a more than 20 underestimation fig 6 and fig s1 however the corn yield comparisons from 2000 to 2014 using four auto irrigation methods showed a reasonable match with an r 2 of 0 32 and overall pbias within 2 fig 7 and fig s2 the clear overestimations of the irrigation amounts for winter wheat at moore county were found using the swc cswc and pwd auto irrigation methods fig 8 and fig s3 the commercial fields particularly the winter wheat fields in the thp are usually not well managed compared to the research farms in fact winter wheat is generally not fully irrigated as marginal increases in yield do not offset input costs marek et al 2018 production data from producers revealed a more than 30 reduction in irrigation water for limited irrigation winter wheat compared to corn production in the thp texas water development board 2011 therefore a mad value of 0 75 representing a typically limited irrigation management condition was used for winter wheat in this case chen et al 2019a results showed a good match between simulated and observed irrigation using this deficit mad value fig 8d simulated irrigated winter wheat yield from 2000 to 2012 at moore county had a large interannual bias compared to the measured yield with the low r 2 values lower than 0 03 for the four auto irrigation methods however the overall pbias values were 7 7 and 12 for mad and the other three auto irrigation methods fig 9 and fig s4 the simulated county level dryland winter wheat yield matched well with the observed data with the r 2 and overall pbias values of 0 79 and 2 1 respectively for all auto irrigation methods fig s5 overall model predictions of winter wheat yields were better under dryland conditions than under irrigated conditions using swat mittelstet et al 2015 also found that the relationship between swat simulated county level dryland wheat yields versus the nass observed wheat yields had an r 2 of 0 61 in the north fork river basin of the southwestern oklahoma and the texas panhandle 3 3 comparison of field scale swat simulated monthly irrigation using different auto irrigation methods to actual irrigation at etter using a single hru in a previous study considering actual irrigation data as input for the corn hru at the etter study site very good agreement of lai nse r 2 and pbias 0 70 0 70 and 3 1 and aboveground biomass 0 95 0 95 and 4 4 were found after calibration during the corn growing periods of 2011 2014 chen et al 2019b the pbias values for simulating corn yield of each year from 2011 to 2014 were within 10 indicating a good agreement chen et al 2019b in this study the calibrated hydrologic parameters in the palo duro watershed were used for the corn hru at etter for further irrigation comparisons using the four auto irrigation methods monthly irrigation comparisons at the etter study site indicated the mad and cswc auto irrigation methods nse 0 75 and pbias within 5 outperformed the other two swat default auto irrigation strategies nse values 0 73 when compared to the actual irrigation fig 10 the simulated irrigation amount using the swat swc auto irrigation method was larger pbias 10 3 than that of the actual irrigation value fig 10a this was in part due to a deficiency in the swat swc auto irrigation method that applied irrigation during the crop non growing season chen et al 2017 for instance simulated irrigation events could occur from january to april of 2012 2014 fig 10a by using the cswc auto irrigation method the model performance statistics of nse r 2 and pbias values in predicting irrigation improved from 0 71 0 72 and 10 3 to 0 77 0 77 and 1 6 fig 10b in most cases the real world field irrigation is scheduled according to measured estimated soil water depletion or management allowed depletion of plant available water rather than the pwd method the cswc method can be used to represent the actual irrigation trigger and management in the field conditions which may partially explain the good irrigation simulation the pbias was 14 8 when using the pwd auto irrigation method fig 7c the pwd auto irrigation method clearly underestimated the irrigation amount this is due to the method that triggers an irrigation once a given reduction in plant growth occurs due to water stress in this case the plant needs to experience water stress before the irrigation event is scheduled therefore the simulated irrigation amount tends to be lower compared to the actual irrigation with corn management for high yield potential and with minimal or no water stress chen et al 2019a uniyal and dietrich 2019 also found the irrigation amounts simulated using the pwd method in swat were 2 3 times less than the simulated values using the swc method at the basin scale and had a large deviation from the actual field irrigation amounts in germany 4 conclusions and remarks in this study four auto irrigation methods swc cswc pwd and mad were evaluated on a watershed scale using observed streamflow and county level irrigation and crop yield data results found that the cswc and mad methods outperformed the other two auto irrigation methods in simulations of streamflow at the watershed outlet and county level irrigation specifically the cswc and mad auto irrigation methods achieved a good model performance for streamflow simulation with nse 0 75 kge 0 55 and pbias within 11 during both the calibration and validation periods generally cswc and mad methods achieved the best simulations of county level irrigation and crop yields of corn and winter wheat as for the comparison of irrigation at the etter study site using a single hru the existing swc auto irrigation method tended to overestimate irrigation with irrigation applications during the non crop growing season nse and pbias of 0 71 and 10 3 however the cswc auto irrigation method clearly improved irrigation simulation with nse and pbias values of 0 77 and 1 6 the existing pwd auto irrigation method underestimated the irrigation amount pbias of 14 8 finally the mad auto irrigation method achieved the best performance in view of an nse of 0 78 experimental fields are usually managed by the researchers who maintain detailed records of agronomic information for specific research studies management practices and production goals may vary for surrounding agricultural area this in part may explain the good agreement in simulating irrigation at the field scale as compared to the county level comparison however the county level yield comparison is necessary for evaluating the swat performance in predicting crop growth and components of the regional hydrological cycle recently swat is increasingly used to simulate field scale studies using the single hru method chen et al 2019a cibin et al 2015 marek et al 2016a moloney et al 2015 the more accurate simulation of the irrigation scheduling can provide applied value for researchers and producers to manage irrigation using simulation models for instance thorp et al 2017 compared field irrigation treatment determined by the calibrated dssat csm cropgro cotton simulation model to the commonly used spreadsheet field irrigation scheduling at maricopa agricultural center arizona they concluded that cotton irrigation scheduling with calibrated dssat led to similar irrigation amounts but with a slightly different seasonal irrigation distribution and equal or higher cotton yield through a field experiment in ira oasis china from 2016 to 2018 chen et al 2020 also reported the irrigation scheduling simulated by the root zone water quality model rzwqm2 resulted in significant increases in seed cotton yield 32 and water productivity 20 compared to the soil water sensor based irrigation scheduling method from this view the calibrated swat mad model may also be used to assist in actual field irrigation scheduling the newly developed mad auto irrigation method is not free from limitations for example a general limitation of auto irrigation approaches is the assumption that daily irrigation occurs whenever the plant soil needs water mcinerney et al 2018 this assumption may not hold true in practice due to daily irrigation schedules varying from producer to producer on a watershed scale and may be constrained by water delivery systems and water restrictions such as well capacity in addition after the setup the auto irrigation methods allow a uniform irrigation depth such as 25 4 mm 1 in per irrigation event as used in this study neitsch et al 2011 however the spatial variation of the irrigation depths in the thp can range from 12 7 mm 0 5 in to 38 1 mm 1 5 in for the same crop according to the well capacities crop water demand and soil water holding capacity marek et al 2020 since these limitations also apply to the mad auto irrigation method it may not be appropriate to compare simulated irrigation using the mad method to the actual field irrigation on a daily scale or according to individual irrigation events recently mcinerney et al 2018 developed a spatially variable event based irrigation scheduling method this method can represent the spatial and temporal variations of the irrigation depths on a daily basis the integration of this method with the mad auto irrigation method for the watershed scale swat study should be the right direction for future work the mad auto irrigation method was developed according to ten year large weighing lysimeter measured data at the usda ars cprl bushland and was further successfully evaluated at the field scale across multiple corn production locations with different climatic conditions and soil types in the u s great plains in this study this method was further tested in an intensively irrigated watershed in the thp with respect to the hydrologic cycle for both terrestrial and aquatic processes these results and evaluations demonstrate the robust nature of the mad auto irrigation method and reinforce its potential as an effective irrigation scheduling tool in assessing scenarios related to irrigation management strategies which should be useful in evaluating alternative irrigation management strategies and subsequent water management and planning efforts software availability name of software swat mad auto irrigation description the swat mad auto irrigation function simulates irrigation scheduling during crop growing season by incorporating an allowable depletion percentage of plant available soil water determined by crop specific maximum rooting depth and site specific soil properties developers yong chen and gary w marek year available 2018 availability contact the developers cost free language fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by the ogallala aquifer program a consortium between usda agricultural research service kansas state university texas a m agrilife research texas a m agrilife extension service texas tech university and west texas a m university the txhpet data sets used were acquired with funding support from the ogallala aquifer program and grants from state and local water conservation agencies and commodity organizations the txhpet data supported more than 20 hatch projects we gratefully thank the three anonymous reviewers for their valuable suggestions for improving this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104789 
26006,the longest flow path is widely used for studying hydrology traditionally both or either of upstream and downstream flow length rasters are required to calculate the longest flow path when processing multiple subwatersheds this approach requires separate calculations of the downstream flow length raster for all the subwatersheds however raster computation involves a lot of disk input output and can be slow by defining the longest flow path recursively and introducing a branching strategy based on hack s law this study proposes a new longest flow path algorithm that computes as few rasters as possible to reduce computational time and improve efficiency to avoid stack overflows by excessive recursion its iterative counterpart algorithm was also proposed the proposed algorithms were implemented as a grass gis module benchmark experiments proved that the new module outperforms an existing tool for a commercial gis keywords longest flow path watershed hydrology open source gis grass gis 1 introduction this paper proposes a new recursive algorithm and its iterative implementation for calculating the longest flow path the longest flow path is one of major watershed parameters huang and lee 2016 used by many hydrologic models including the hydrologic engineering center s hydrologic modeling system hec hms feldman 2000 the soil and water assessment tool swat arnold et al 1998 the storm water management model swmm rossman and huber 2016 and the topography model topmodel beven and kirkby 1979 to name a few it is mainly used to calculate the time of concentration and the lag time for hydrologic analysis maidment and djokic 2000 feldman 2000 olivera 2001 the united states geological survey usgs uses it to analyze annual peak flow data and create regression equations that estimate the magnitude and frequency of floods gotvald et al 2009 feaster et al 2014 williams sether 2015 a flow path is the hydrologic path or watercourse from one point to another in the watershed the longest flow path represents the flow path from a headwater typically the watershed divide to the outlet that is longer than all other flow paths in the watershed it can set theoretically be defined as 1 lfp fp i fp i fp j i j where lfp is a longest flow path and i and j are indices for flow paths fp in the watershed there can be more than one longest flow path in some case depending on the topography a typical procedure to determine the longest flow path requires the digital elevation model dem and involves a geographic information system gis smith 1995 to the best of the author s knowledge and based on an extensive literature review smith 1995 introduced the original longest flow path algorithm for gis this algorithm calculates two flow length rasters for an outlet cell using the dem and adds both rasters to determine the longest flow path raster smith 1995 olivera and maidment 1998 the arc hydro toolbox maidment 2002 for arcgis pro esri 2020a provides a longest flow path tool that does not require the calculation of an upstream flow length raster however in spite of its important role in hydrologic studies not much attention has been paid to the efficiency of the existing approaches this study improves upon the decades old algorithm and introduces a new recursive algorithm and its iterative implementation that is more memory efficient section 2 reviews smith s 1995 work open source implementations of his algorithm for the geographic resources analysis support system grass gis neteler et al 2012 and the algorithm of the arc hydro longest flow path tool for arcgis pro section 3 elaborates on the new algorithms and designs benchmark experiments for comparing the performance of the new module and the arc hydro tool benchmark results are analyzed and discussed in sections 4 and 5 respectively and section 6 summarizes findings 2 background 2 1 smith s flow length based approach the length of the longest flow path for an outlet cell can be obtained by calculating the upstream flow length raster smith 1995 however finding the longest flow path itself also requires the calculation of the downstream flow length raster and the summation of both flow length rasters smith 1995 the downstream flow length dfl is the flow length starting from the outlet it can be written as 2 dfl i 0 i 0 at the outlet fl i 1 i dfl i 1 i 1 where dfl i is the downstream flow length from the outlet to cell i fl i 1 i is the flow length between cells i 1 and i and i is 0 at the outlet and increases as we traverse in the upstream direction the upstream flow length ufl is the flow length starting from a headwater similar to the dfl the ufl can be written as 3 ufl i 0 i 0 at a headwater fl i 1 i max ufl i 1 i 1 where ufl i is the upstream flow length from a headwater to cell i fl i 1 i is the flow length between cells i 1 and i and i is 0 at a headwater and increases as we traverse in the downstream direction the dfl and ufl are maximum at the headwater on the longest flow path and the outlet respectively and both maximums are the same and referred to as the longest flow length lfl the longest flow path lfp is the path defined by all cells with a value equal to the lfl defined by 4 lfl max d f l u f l where d f l and u f l represent the downstream and upstream flow length rasters respectively boldface notations for sets of cells and features such as raster and vector maps however it would not be possible to simply find cells with the exact lfl value because of rounding errors and in practice a small error tolerance is needed to find all cells as follows 5 lfl ε c i lfl ε where c i is the value of cell i and ε is a positive error tolerance which should be close to 0 finally the longest flow path can be determined by connecting these cells fig 1 shows an example of the dfl and the longest flow path in fig 1a the dfl is 0 black at the outlet and high bright at headwater cells the ufl raster is not shown because most cells outside the longest flow path have a low value dark and the cells along the longest flow path bright are hard to see because of the cell size adding the d f l and u f l rasters yields a summation raster d f l u f l which is used to derive the longest flow path shown in fig 1b 2 2 grass gis modules historically grass gis neteler et al 2012 has supported a raster based approach for calculating the longest flow path since the release of the r lfp and v lfp modules in 2014 table 1 summarizes raster based modules that the author developed for grass gis a critical problem in the raster based approach can occur during a raster to vector conversion because the conversion process does not understand hydrology and simply reshapes a series of cells into a linear feature without considering flow directions depending on the conversion algorithm a cluster of cells may be simplified and some cells can be missing from the vector output this conversion issue is noted as a raster to vector issue in table 1 fig 2 shows an example of this raster to vector conversion issue the vector longest flow path initially follows flow directions faithfully but it takes a short cut skipping one cell in the middle this conversion issue violates the hydrologic validity of the vector result the deprecated r lfp was responsible for computing the raster longest flow path while v lfp would convert it to vector only one outlet was supported at a time which made these modules inefficient for processing multiple outlets because of heavy disk input output i o these modules were deprecated by the new r lfp the new r lfp module addressed the raster to vector conversion issue by tracing the flow direction raster from headwater cells this module directly generates the vector longest flow path and eliminated need for v lfp it also supports multiple outlets in one run and can save substantial computational time the major difference between this module and the other modules is a lack of the upstream flow length raster u f l in the algorithm only the downstream flow length raster d u f l is computed for each outlet and the longest flow path is traced following the flow direction raster f d r starting from headwater cells the lfp sh module was developed to calculate longest flow paths for a lot of watersheds this module tries to save computational time by calculating the upstream and downstream flow length rasters only once for the entire computational region encompassing all watersheds the module can handle multiple outlets on the same stream network but its algorithm is rather complicated because of heavy vector handling however it still converts raster to vector which means that it inherits the same hydrologic issue from its deprecated siblings the lfp2 sh module was an attempt to get rid of complicated vector handling from lfp sh these two modules were not released as part of grass gis because they were written in bash a non official scripting language for grass gis free software foundation 2014 both modules calculate and sum the downstream and upstream flow length rasters at different stages and extract cells whose value is equal to the longest flow length the lfp sh module creates the summation raster d u f l only once outside the main loop to minimize disk i o while the lfp2 sh module creates the upstream flow length raster u f l once outside the loop but it calculates the downstream flow length raster d f l for each outlet to reduce vector edits all of these existing modules only support accumulated watersheds for example if there is one outlet upstream of another the longest flow path for the downstream outlet will be delineated for the entire watershed not for the downstream subwatershed leaving an overlap between the two longest flow paths this way of calculating multiple longest flow paths is not typically used for hydrologic analysis so these modules were not used for this study however based on an independent test these modules underperformed the new module introduced in section 3 and were found to be inefficient in terms of computational time 2 3 arc hydro tool the arc hydro toolbox for arcgis pro provides the longest flow path tool written in python van rossum and drake 2009 algorithm 1 shows its pseudocode this tool only calculates the downstream flow length raster d f l for each subwatershed using the built in flow length tool and finds headwater cells with the longest flow length from the boundary cells it then traces down the flow direction raster starting from the headwater cells the tool implicitly assumes that longest flow paths always start from the subwatershed divide because it searches only the boundary zone for headwater cells algorithm 1 pseudocode for the longest flow path tool in arc hydro for arcgis pro created based on python code analysis image 1 3 methods 3 1 recursive definition of the longest flow path the i o of raster data often takes relatively a long computational time compared to actual raster manipulation and becomes a bottleneck qin et al 2014 it would be ideal to not use raster data at all but the most basic input is the flow direction raster map and it is not feasible to implement an algorithm that is purely vector based in this section we approach the problem of calculating the longest flow path from a different perspective and try to avoid calculating the upstream and downstream flow length rasters and summing these two rasters all together in fact the longest flow path defined by eq 1 can recursively be redefined as 6 lfp i lfp j l j i lfp j l j i lfp k l k i j k u p j k if u p 0 0 otherwise where lfp i is a longest flow path for cell i l j i is the flow path from cells j to i and u p is the set of up to eight upstream neighbor cells flowing into cell i in other words if there are no upstream neighbor cells we have reached a headwater cell and the longest flow path at the headwater cell will be a zero vector 0 if there are upstream neighbor cells a longest flow path for the current cell can be determined by connecting its upstream neighbor cell s longest flow path and the path from the neighbor to the current cell since there can be maximum eight upstream neighbor cells one condition has to be satisfied that the combined path of the upstream cell s longest flow path and the path from the neighbor to the current cell has to be longer than or equal to the length of the combined path of any other neighbor cells this recursive definition of the longest flow path looks simple but it is not straightforward to implement it in a computationally efficient way because the number of possible upstream cells can grow exponentially depending on the size topography and shape of the watershed assuming that each cell receives flow from n 8 upstream neighbor cells on average we can estimate the number of cells to visit v starting from one outlet towards upstream in s steps using the following equation 7 v i 0 s n i where s 0 means that we are at the outlet by plugging the number of cells on an m n map into v and backcalculating s for v m n we can estimate how many steps we can traverse the stream upwards before we have to visit all the cells on the map for example on a 1000 1000 map with an average upstream neighbor cells n 2 we would have to visit all the cells a million while being able to traverse the stream only 19 steps upwards for n 3 it would take 13 steps upwards until we have to visit all the million cells in the worse case when n 8 it would take only seven steps of course not all cells have the same number of upstream neighbors and calculating the actual number of steps would be more complicated however from these examples we learned that it would not be any efficient computationally compared to the flow length based approach to address the problem of an exponentially growing number of cells to visit in each step we need to devise some constraints that help us filter out non candidate neighbor cells as early as possible in the following sections we will discuss the relationship between the longest flow path and area of watersheds and develop a strategy that can filter out neighbor cells whose potential longest flow length is shorter than others 3 2 branching strategy hack 1957 proposed a power law relationship between the main channel length and area of watersheds this empirical relationship is usually referred to as hack s law rigon et al 1996 and can be written as 8 l c a h where l is the distance from the outlet to the drainage divide along the stream channel a is the watershed area and c and h are hack s coefficient and exponent respectively many researchers have studied the significance of h sassolas serrayet et al 2018 and it is generally believed to be slightly below 0 6 and greater than 0 5 rigon et al 1996 mesa and gupta 1987 derived a theoretical equation of h as a function of the watershed s magnitude n as follows 9 h n 1 2 π π n 1 2 π 1 n for n 1 and 1000 h is 1 147 and 0 509 eq 9 implies that h tends to converge to 0 5 as n approaches infinity and the minimum h value is 0 5 sassolas serrayet et al 2018 analyzed hack s coefficient c using around 22 000 watersheds in bhutan and found c between 1 5 and 2 5 for h 0 5 which is consistent with previous studies sassolas serrayet et al 2018 they proposed an equation for c using the gravelius compactness coefficient gc gravelius 1914 as follows 10 c 1 2 gc π 1 4 gc 2 π 4 where gc is the ratio of the watershed perimeter to the circumference of the circle whose area is equivalent to the watershed area gc can be written as 11 gc p 2 π a where p and a are the watershed perimeter and area respectively gc becomes 1 for perfectly circular watersheds imaginary of course and starts growing as a watershed is being elongated sassolas serrayet et al 2018 the minimum gc allowed in eq 10 is 4 π yielding 1 as the minimum c the coefficient c is not unitless and has a dimension of l 1 2 h where l denotes a length unit however as h approaches 0 5 c tends to be l 1 2 0 5 1 and c becomes practically unitless one consideration we have to take into account for this research is that the longest flow path does not always follow the main channel for upstream most subwatersheds the definition of the longest flow path may agree with that of the main channel in hack s law in some cases however for downstream subwatersheds the longest flow path starts from non stream land surface as shallow surface runoff and enters the stream later by definition the longest flow path is almost always longer than the stream channel flowing through a subwatershed for this reason we cannot directly use eq 8 with typical c and h values since the recursive definition in eq 6 traverses the flow path from downstream towards upstream we want to eliminate inflowing neighbor cells that cannot possibly generate a longest flow path by comparing them with their peers at each branching cell this paper proposes that just like the main channel length l in hack s law the longest flow length lfl be also estimated using eq 8 with a slightly different interpretation since hack s law involves the watershed area the flow direction raster cannot be used alone in the new recursive algorithm instead flow accumulation is computed using the flow direction raster and is used to estimate the subwatershed area at each cell the basic idea for branching and eliminating neighbor cells is simple let s say there are two inflowing neighbor cells we take their flow accumulation values and calculate their potential minimum and maximum lfls if the maximum lfl of one cell is shorter than the minimum lfl of the other cell the former cannot generate a longest flow path because its theoretically longest lfl is still shorter than the other cell s theoretically shortest lfl the former is discarded from further recursion and the latter survives and is further being traversed and evaluated 3 3 longest and shortest longest flow lengths we will address the longest longest flow length lfl max first because estimating this distance is more intuitive and easier given the flow accumulation value the number of upstream cells at a cell in what configuration would these upstream cells yield lfl max travelling all these cells diagonally only without any horizontal or vertical moves will result in lfl max therefore lfl max can be written as 12 lfl max s fac 2 where s is the width or height mostly the same in any gis of one cell and fac is the flow accumulation value at a neighbor cell now we will use eq 8 to estimate the shortest longest flow length lfl min since we compare one cell s lfl max with another cell s lfl min and decide whether or not to eliminate one of these cells it is important to be conservative in estimating lfl min because we do not want to discard those cells that would have yielded a longest flow path if they were not discarded accidentally if lfl min is estimated to be too short the recursive algorithm will not be able to filter out many non candidate cells or not at all in the worse case as if there were no branching strategy in place in contrast if it is estimated to be too long the algorithm will start getting rid of good candidate cells and can generate non longest flow paths considering that the longest flow path is usually longer than the main channel and it is safer to underestimate lfl min not to miss cells that are perfectly fine the paper proposes the following equation for lfl min 13 lfl min s fac eq 13 can be obtained by plugging c 1 h 0 5 and a s 2 fac into eq 8 a value of 1 for c is less than its minimum value 4 π for eq 10 and a value of 0 5 for h is its minimum value for eq 9 it means that eq 13 is always shorter than eq 8 with the minimum c and h for eqs 10 and 9 respectively and being conservative also the unit of c is unitless because h is 0 5 fig 3 shows three different simplified shapes of watersheds and their shortest longest flow path lfp with its lfl assuming that headwater points are located on the watershed boundary in these simplified watersheds the shortest lfp must be a straight line connecting the remotest point blue dots and the outlet red dots any other straight paths will not be able to drain the furthest point and any non straight paths will be longer than the shortest lfp for hypothetical circular watersheds lfl 2 π a for isosceles triangular and rectangular watersheds by taking the derivative of the lfl with respect to w we can obtain 14 dlfl d ω lfl 1 w 4 4 a 2 w 3 and 15 dlfl d ω lfl 1 w 4 a 2 w 3 respectively by setting eqs 14 and 15 to 0 we found that w 2 a and w 2 a minimize the lfl of isosceles triangular and rectangular watersheds respectively plugging these w values into the lfl equations in fig 3 yields the shortest lfls of 2 a and a respectively for both types of watersheds since we use the fac raster to calculate the watershed area we can substitute s 2 fac for a in the final lfl equations therefore all three lfls for the simplified watersheds can be rewritten as 2 s π fac s 2 fac and s fac and we confirmed that all these lengths are longer than or equal to the shortest lfl defined in eq 13 3 4 new grass gis module the recursive algorithm and its iterative variant take a flow direction raster and outlet points as input parameters it was a design decision not to provide subwatershed polygons because delineating them is an additional step however since subwatershed polygons are not provided these algorithms have no knowledge about the shape of subwatersheds in addition fac values read from the flow accumulation raster are only accumulated without flow subaccumulation introduced in algorithm 2 these algorithms would only generate longest flow paths for accumulated watersheds the flow subaccumulation process takes each of input outlet points and subtracts its fac value from the fac value of its downstream cells the final result of this process is a raster where individual subwatershed regions have their own flow accumulation computed independent of their upstream flow contribution if the flow direction raster is clipped to each subwatershed flow accumulation is computed and resulting flow accumulation rasters are patched together for all subwatersheds the patched raster will be identical to the flow subaccumulation raster this process is computationally very light because it only traces down not up along the downstream portion of lfps both recursive and iterative algorithms perform flow subaccumulation before starting the up tracing process algorithm 2 pseudocode for the subaccumulate function for both recursive and iterative algorithms image 2 algorithm 3 is used to find inflowing neighbor cells and calculate their attributes including the accumulation value downstream length and theoretical minimum and maximum lfls which will be used later in the main logic for up tracing in both recursive and iterative algorithms algorithm 3 pseudocode for the findup function for both recursive and iterative algorithms image 3 algorithms 4 and 5 present pseudocode for the main procedure and its subroutine respectively for implementing recursive lfp tracing algorithms 6 and 7 present pseudocode for the main procedure and its subroutine respectively for implementing lfp tracing iteratively using a stack as its primary data structure for managing neighbor cells the r accumulate grass gis module implements both recursive with the r flag and iterative default without the r flag lfp algorithms this module is written in the c language kernighan and ritchie 1988 to differentiate between the two algorithms in the same grass gis module the recursive and iterative implementations will be referred to as r accumulate recursive and r accumulate iterative respectively the reason why the author implemented the iterative version is for stack safety in deep recursion any function or procedure calls use the stack memory to store data about the calls until those routines return to the caller reese 2013 however the size of this memory is limited and invoking a function recursively too many times can cause a stack overflow when the system runs out of stack memory reese 2013 in fact this stack overflow condition occurred in one of the experiments that will be discussed in section 4 by converting recursive calls into a last in first out lifo data structure using the heap memory a stack in the heap memory the iterative version avoids stack overflows that can be caused by deep recursion in the recursive version one lfp is searched for first all the way to the upstream most headwater cell of one neighbor cell and move to the next branch depth first search while in the iterative version all neighbor cells are pushed to the stack first move upstream by one cell and repeat these two steps until we find an upstream most headwater cell breadth first search this difference in search strategies becomes important when the size of a subwatershed is large because depth first search would require a lot of stack memory for excessive recursion algorithm 4 pseudocode for r accumulate recursive image 4 algorithm 5 pseudocode for the traceup function for r accumulate recursive image 5 algorithm 6 pseudocode for r accumulate iterative image 6 algorithm 7 pseudocode for the traceup function for r accumulate iterative image 7 3 5 benchmark experiments the states of georgia and texas in the united states were selected for benchmark experiments the 1arcsecond approximately 30 m national elevation dataset ned tiles were downloaded from usgs 2020 patched and clipped to the state boundaries of georgia and texas a hundred random outlet points were generated for each state once and the same set of outlets was used for all the experiments for the same state there are many different factors affecting the performance of software including system architectures central processing units cpus clock speeds the sizes of cache and random access memory ram drive types operating systems oss compilers software implementations etc lee et al 2016 micheloni and olivo 2017 wang et al 2019 however because of limited computational resources the main purpose of the benchmark experiments in this study is to see how the disk type and memory size impact the performance of the recursive r accumulate recursive and iterative r accumulate iterative versions of the grass gis module the author used grass gis on two linux systems with different configurations including one with a lower clock speed a hard disk drive hdd and more memory and the other with a higher clock speed a solid state drive ssd and less memory he also used the longest flow path tool from the arc hydro toolbox for arcgis pro on a windows system for benchmarking with commercial software the windows system is equipped with the most recent cpu with the highest clock speed among the three systems and an ssd since the windows system did not have access to grass gis administrative rights are required to run the grass gis installer only the arcgis pro tool was run on this system similarly arcgis pro is not available for linux so only grass gis was used on the linux systems table 2 shows the system specifications used for experiments including oss cpus the amounts of ram the sizes of swap partitions and software versions since the longest flow path tool in arc hydro pro accepts subwatersheds instead of outlets the watershed tool built in arcgis pro was used to delineate 100 subwatershed raster maps first which were then converted to polygon vector maps using the raster to polygon tool before these experiments were conducted the longest flow path tool in arc hydro pro will be referred to as the arc hydro tool or simply arc hydro hereafter because its name longest flow path is too generic two different experiments were conducted including batch and non batch mode experiments the batch mode experiment assumes a scenario where we want to compute longest flow paths for multiple outlets at once in one run of each program it is expected to increase disk i o compared to the non batch mode experiment explained later because a program needs to read in a flow direction raster and calculate flow accumulation and subaccumulation for the entire state not just for the area of interest this experiment tests how fast and efficient each program is at computing multiple longest flow paths once after reading input data each program is run 30 times independently for each state the results are used to calculate the average and standard deviation of run times in contrast the non batch mode experiment runs programs for each outlet grass gis module or subwatershed arc hydro tool at a time independently in a separate process since the arc hydro tool uses subwatershed polygons as a mask the grass gis module also used the same polygons to restrict the computational region to be fair in this experiment the burden of disk i o is expected to be lower than in the batch mode experiment because programs do not have to read in any data outside the area of interest this experiment allows to analyze the performance sensitivity of the program to the subwatershed area each program is run once for each outlet independently hence there are 100 runs for each state because there are 100 outlets or subwatersheds for the arc hydro tool 4 results 4 1 validation of longest flow paths first we need to validate eqs 12 and 13 proposed for the longest and shortest longest flow lengths respectively for both states the lengths of all calculated longest flow paths were strictly between lfl min and lfl max estimated using the equations the gravelius compactness coefficient gc calculated using eq 11 varied from 1 64 to 5 49 plugging the gc into eq 11 yielded hack s coefficient c of 1 99 7 25 which is greater than the assumed c 1 in eq 13 for the shortest longest flow length for the state of georgia all the three programs have successfully completed 30 batch runs and 100 non batch runs however for the state of texas r accumulate recursive failed to complete all 30 batch runs and one non batch run on both linux ssd and hdd systems in fact the 30 batch runs failed because of the same subwatershed for which the one failed non batch run was not successful at calculating the longest flow path with a segmentation fault error except for these failed cases r accumulate recursive and r accumulate iterative generated identical longest flow paths with a maximum error tolerance of 1 10 8 m due to round off errors in both experiments fig 4 shows the longest flow paths generated by r accumulate iterative figs 5 and 6 highlight major differences between the results from r accumulate iterative and arc hydro because of the assumption made in the arc hydro tool where any longest flow paths must start from the drainage divide of a subwatershed and cannot start from its interior it is clear from fig 5 that the arc hydro tool can generate inferior longest flow paths that are not truly the longest in some cases thirteen and six longest flow paths in georgia and texas respectively started from the interior of their subwatersheds and the arc hydro tool generated not so longest flow paths that are shorter than those generated by r accumulate iterative the probabilities of generating invalid longest flow paths are 13 and 6 out of 100 longest flow paths for georgia and texas respectively fig 5a shows a relative error in length of 1 3 but fig 5b presents a more extreme case where the error is 27 4 another feature of the arc hydro tool is an extension of the end node of the longest flow path but it simply extends the last node into the same direction as the last segment not in the flow direction at the outlet cell as shown in fig 6a last from an independent run using a different dataset neteler and mitasova 2008 the arc hydro tool generated a shorter longest flow path for a subwatershed where the true longest flow path actually starts from a boundary cell which means that this subwatershed satisfies the tool s drainage divide assumption but the tool still failed to find the longest flow path 4 2 comparisons of elapsed times table 3 summarizes 30 elapsed times for processing all the 100 outlets in each state in the batch mode experiment overall the arc hydro tool was about 22 49 times slower than the grass gis module for georgia the linux ssd system was about two times faster than the linux hdd system on the same linux ssd system the worst run of r accumulate recursive outperformed the best run of r accumulate iterative however on the linux hdd system one r accumulate recursive run was the worst of both versions and the range of r accumulate recursive except the worst outlier was within the range of r accumulate iterative for texas the linux ssd system was about 1 4 times slower than the linux hdd system however r accumulate recursive failed to complete on both systems so comparisons between the two versions were not made table 4 summarizes 100 elapsed times for each state in the non batch mode experiment the grass gis module on both linux ssd and hdd systems was about 34 97 times faster than the arc hydro tool on the windows system with an ssd drive fig 7 shows the performance sensitivity of each program to the subwatershed area for both states the elapsed time of the arc hydro tool grew exponentially with increasing subwatershed areas its log linear regression analysis yielded adjusted r 2 values r adj 2 s of 0 9194 and 0 8681 for georgia and texas one outlier excluded from texas respectively with a p value less than 2 2 10 16 in contrast the grass gis module exhibited a linear growth r adj 2 0 9503 with a p value 2 2 10 16 for both states 5 discussion the grass gis module on the linux systems outperformed the arc hydro tool on windows in both batch and non batch mode experiments even though the windows system has the best hardware specifications except the size of ram less than linux hdd but more than linux ssd the performance of the arc hydro tool deteriorated exponentially as the subwatershed size grew as opposed to the linear performance deterioration of the grass gis module this inefficiency of the arc hydro tool may be attributed to esri s decision to implement the arc hydro toolbox using their python application programming interface api instead of the net microsoft corporation 2020 api which is a more advanced approach to extending the capabilities of arcgis pro esri 2020b however it is still to be investigated if the lower performance was actually due to the additional python layer between the frontend and its core engine or simply the tool s algorithm without a net implementation of the arc hydro tool it would not be feasible to determine which was the case the arc hydro tool not only was slower but also had three issues as follows 1 longest flow paths starting from the interior of subwatersheds were not correctly calculated because of its boundary assumption where the tool assumes that longest flow paths only start from boundary cells fig 5 2 for one non benchmark run it was not able to calculate the correct longest flow path even though the boundary assumption was not violated fig 6b and 3 its end node extension feature does not consider the flow direction at the outlet cell fig 6a in the batch mode experiment the linux ssd system was faster than its hdd variant except for the case of texas since the linux hdd system has a slower cpu and hdd with more ram we can see that the amount of memory played a more important role in processing the texas subwatersheds texas has about total 2 million integer 4 bytes cells in the flow direction raster which when loaded into memory occupies 8 gb the grass gis module additionally allocates 8 gb to calculate a flow accumulation raster holding these two rasters in memory alone requires 16 gb leaving only on disk swap space for other tasks such as recursion and storing outputs on the linux ssd system which has 16 gb system memory the faster cpu did not help when memory ran out and the slower swap space started kicking in on the other hand the linux hdd system still has 32 gb of ram for calculating longest flow paths however even with the remaining 32 gb memory recursively traversing the largest 51st subwatershed in texas consumed up the 8 mib stack memory allocated by the os kernel to r accumulate recursive the area of this subwatershed is 47 077 62 km2 given the 30 m resolution of the elevation data this area translates to total 52 million cells assuming that only 5 which is conservative compared to the size of the subwatershed of the 52 million cells need to be recursively visited to find the longest flow path the average stack size per visit or per recursive call would be only 3 bytes 8 1024 2 0 05 52 1000 2 which is not enough to store state information about each recursive function call including variables this lack of stack space caused a stack overflow since all the 100 subwatersheds were being processed together in one process the stack overflow resulted in no outputs at all in contrast with no recursive calls r accumulate iterative was able to leverage the swap space on the linux ssd system or the remaining 32 gb memory on the linux hdd system still the linux ssd system was slower because a disk based swap partition is slower than real memory in the non batch mode experiment the linux ssd system consistently outperformed the linux hdd system the stack overflow issue did occur again with the largest texas subwatershed but unlike in the batch mode experiment r accumulate recursive calculated the other 99 longest flow paths successfully because each subwatershed was processed separately also for the same reason the linux ssd system ran out of memory only with this subwatershed and handled the other subwatersheds smoothly with enough memory therefore no performance hit was observed on this system both linux systems showed consistent performance per subwatershed area across the states regardless of the total area 0 00034 s km2 to 0 00039 s km2 for linux ssd and 0 000 60 s km2 to 0 00076 s km2 for linux hdd as shown in table 4 we can use this consistency in performance to predict computational times when running the grass gis module in terms of the total computing time of the grass gis module between the batch and non batch mode experiments the former was faster than the latter for georgia and vice versa for texas for georgia it took more time to process individual subwatersheds separately this added computational time in the non batch mode experiment can be explained by an overhead expense of creating 100 separate output vector maps and repeating basic managerial tasks such as reading header files adjusting computational regions etc however for texas the non batch mode experiment was faster because processing one subwatershed at a time is much more memory efficient than processing all the subwatersheds in one process especially when there are many large subwatersheds in the state in both experiments r accumulate iterative r accumulate without the r flag turned out to be more robust with no failures and only slightly slower than its recursive sibling r accumulate with the r flag for this reason it is recommended to use the r accumulate module without the r flag which is the default currently administrative rights are required to install grass gis on microsoft windows which is why the r accumulate module was only tested on the linux systems however grass gis and the new module also support microsoft windows xp or newer and macos 10 4 10 or newer future research includes implementation of the proposed algorithms for arcgis pro and comparison of their performance with that of the grass gis module 6 conclusions in this paper we studied the original longest flow path approach and briefly reviewed existing longest flow path programs it was shown how hydrologically invalid longest flow paths could be generated by naively converting raster results to vector and a recursive definition of the longest flow path was introduced naturally a new recursive algorithm was proposed based on the recursive definition for calculating the longest flow path using depth first search and its iterative counterpart algorithm was developed using breadth first search these algorithms use equations for the longest and shorted longest flow lengths that were derived from hack s law a branching strategy was devised to help the proposed algorithms filter out inferior neighbor cells and speed up traversal using randomly generated 100 outlets and subwatersheds each in georgia and texas performance comparisons were made between the recursive and iterative versions of the r accumulate grass gis module that implement the proposed algorithms and the longest flow path tool in the arc hydro toolbox for arcgis pro an implicit assumption in the arc hydro tool was tested and shown to not work well in some case with increasing subwatershed areas the performance of the arc hydro tool exponentially deteriorated while that of the grass gis module linearly declined the grass gis module outperformed the arc hydro tool but the recursive implementation failed to process the largest subwatershed in texas because of a stack overflow resulting from deep recursion the iterative implementation was more robust in terms of memory usage and successfully calculated all the longest flow paths for both states since the iterative implementation was only slightly slower than its recursive counterpart the iterative version is highly recommended over the other version grass gis and the new module support microsoft windows xp or newer with administrative rights macos 10 4 10 or newer recent gnu linux or a unix variant future work includes implementation of the proposed algorithms for arcgis pro software availability grass gis software free under the gnu gpl license grass gis https grass osgeo org r accumulate https github com osgeo grass addons tree master grass7 raster r accumulate recursive version with the r flag r accumulate recursive iterative version by default without the r flag r accumulate iterative operating system requirements microsoft windows xp or newer macos 10 4 10 or newer recent gnu linux or a unix variant declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the author thanks dr daniel p ames for handling this manuscript and an anonymous reviewer for providing constructive comments he also thanks the grass gis community for their continued support for improving the historical modules and stimulating the development of the new software 
26006,the longest flow path is widely used for studying hydrology traditionally both or either of upstream and downstream flow length rasters are required to calculate the longest flow path when processing multiple subwatersheds this approach requires separate calculations of the downstream flow length raster for all the subwatersheds however raster computation involves a lot of disk input output and can be slow by defining the longest flow path recursively and introducing a branching strategy based on hack s law this study proposes a new longest flow path algorithm that computes as few rasters as possible to reduce computational time and improve efficiency to avoid stack overflows by excessive recursion its iterative counterpart algorithm was also proposed the proposed algorithms were implemented as a grass gis module benchmark experiments proved that the new module outperforms an existing tool for a commercial gis keywords longest flow path watershed hydrology open source gis grass gis 1 introduction this paper proposes a new recursive algorithm and its iterative implementation for calculating the longest flow path the longest flow path is one of major watershed parameters huang and lee 2016 used by many hydrologic models including the hydrologic engineering center s hydrologic modeling system hec hms feldman 2000 the soil and water assessment tool swat arnold et al 1998 the storm water management model swmm rossman and huber 2016 and the topography model topmodel beven and kirkby 1979 to name a few it is mainly used to calculate the time of concentration and the lag time for hydrologic analysis maidment and djokic 2000 feldman 2000 olivera 2001 the united states geological survey usgs uses it to analyze annual peak flow data and create regression equations that estimate the magnitude and frequency of floods gotvald et al 2009 feaster et al 2014 williams sether 2015 a flow path is the hydrologic path or watercourse from one point to another in the watershed the longest flow path represents the flow path from a headwater typically the watershed divide to the outlet that is longer than all other flow paths in the watershed it can set theoretically be defined as 1 lfp fp i fp i fp j i j where lfp is a longest flow path and i and j are indices for flow paths fp in the watershed there can be more than one longest flow path in some case depending on the topography a typical procedure to determine the longest flow path requires the digital elevation model dem and involves a geographic information system gis smith 1995 to the best of the author s knowledge and based on an extensive literature review smith 1995 introduced the original longest flow path algorithm for gis this algorithm calculates two flow length rasters for an outlet cell using the dem and adds both rasters to determine the longest flow path raster smith 1995 olivera and maidment 1998 the arc hydro toolbox maidment 2002 for arcgis pro esri 2020a provides a longest flow path tool that does not require the calculation of an upstream flow length raster however in spite of its important role in hydrologic studies not much attention has been paid to the efficiency of the existing approaches this study improves upon the decades old algorithm and introduces a new recursive algorithm and its iterative implementation that is more memory efficient section 2 reviews smith s 1995 work open source implementations of his algorithm for the geographic resources analysis support system grass gis neteler et al 2012 and the algorithm of the arc hydro longest flow path tool for arcgis pro section 3 elaborates on the new algorithms and designs benchmark experiments for comparing the performance of the new module and the arc hydro tool benchmark results are analyzed and discussed in sections 4 and 5 respectively and section 6 summarizes findings 2 background 2 1 smith s flow length based approach the length of the longest flow path for an outlet cell can be obtained by calculating the upstream flow length raster smith 1995 however finding the longest flow path itself also requires the calculation of the downstream flow length raster and the summation of both flow length rasters smith 1995 the downstream flow length dfl is the flow length starting from the outlet it can be written as 2 dfl i 0 i 0 at the outlet fl i 1 i dfl i 1 i 1 where dfl i is the downstream flow length from the outlet to cell i fl i 1 i is the flow length between cells i 1 and i and i is 0 at the outlet and increases as we traverse in the upstream direction the upstream flow length ufl is the flow length starting from a headwater similar to the dfl the ufl can be written as 3 ufl i 0 i 0 at a headwater fl i 1 i max ufl i 1 i 1 where ufl i is the upstream flow length from a headwater to cell i fl i 1 i is the flow length between cells i 1 and i and i is 0 at a headwater and increases as we traverse in the downstream direction the dfl and ufl are maximum at the headwater on the longest flow path and the outlet respectively and both maximums are the same and referred to as the longest flow length lfl the longest flow path lfp is the path defined by all cells with a value equal to the lfl defined by 4 lfl max d f l u f l where d f l and u f l represent the downstream and upstream flow length rasters respectively boldface notations for sets of cells and features such as raster and vector maps however it would not be possible to simply find cells with the exact lfl value because of rounding errors and in practice a small error tolerance is needed to find all cells as follows 5 lfl ε c i lfl ε where c i is the value of cell i and ε is a positive error tolerance which should be close to 0 finally the longest flow path can be determined by connecting these cells fig 1 shows an example of the dfl and the longest flow path in fig 1a the dfl is 0 black at the outlet and high bright at headwater cells the ufl raster is not shown because most cells outside the longest flow path have a low value dark and the cells along the longest flow path bright are hard to see because of the cell size adding the d f l and u f l rasters yields a summation raster d f l u f l which is used to derive the longest flow path shown in fig 1b 2 2 grass gis modules historically grass gis neteler et al 2012 has supported a raster based approach for calculating the longest flow path since the release of the r lfp and v lfp modules in 2014 table 1 summarizes raster based modules that the author developed for grass gis a critical problem in the raster based approach can occur during a raster to vector conversion because the conversion process does not understand hydrology and simply reshapes a series of cells into a linear feature without considering flow directions depending on the conversion algorithm a cluster of cells may be simplified and some cells can be missing from the vector output this conversion issue is noted as a raster to vector issue in table 1 fig 2 shows an example of this raster to vector conversion issue the vector longest flow path initially follows flow directions faithfully but it takes a short cut skipping one cell in the middle this conversion issue violates the hydrologic validity of the vector result the deprecated r lfp was responsible for computing the raster longest flow path while v lfp would convert it to vector only one outlet was supported at a time which made these modules inefficient for processing multiple outlets because of heavy disk input output i o these modules were deprecated by the new r lfp the new r lfp module addressed the raster to vector conversion issue by tracing the flow direction raster from headwater cells this module directly generates the vector longest flow path and eliminated need for v lfp it also supports multiple outlets in one run and can save substantial computational time the major difference between this module and the other modules is a lack of the upstream flow length raster u f l in the algorithm only the downstream flow length raster d u f l is computed for each outlet and the longest flow path is traced following the flow direction raster f d r starting from headwater cells the lfp sh module was developed to calculate longest flow paths for a lot of watersheds this module tries to save computational time by calculating the upstream and downstream flow length rasters only once for the entire computational region encompassing all watersheds the module can handle multiple outlets on the same stream network but its algorithm is rather complicated because of heavy vector handling however it still converts raster to vector which means that it inherits the same hydrologic issue from its deprecated siblings the lfp2 sh module was an attempt to get rid of complicated vector handling from lfp sh these two modules were not released as part of grass gis because they were written in bash a non official scripting language for grass gis free software foundation 2014 both modules calculate and sum the downstream and upstream flow length rasters at different stages and extract cells whose value is equal to the longest flow length the lfp sh module creates the summation raster d u f l only once outside the main loop to minimize disk i o while the lfp2 sh module creates the upstream flow length raster u f l once outside the loop but it calculates the downstream flow length raster d f l for each outlet to reduce vector edits all of these existing modules only support accumulated watersheds for example if there is one outlet upstream of another the longest flow path for the downstream outlet will be delineated for the entire watershed not for the downstream subwatershed leaving an overlap between the two longest flow paths this way of calculating multiple longest flow paths is not typically used for hydrologic analysis so these modules were not used for this study however based on an independent test these modules underperformed the new module introduced in section 3 and were found to be inefficient in terms of computational time 2 3 arc hydro tool the arc hydro toolbox for arcgis pro provides the longest flow path tool written in python van rossum and drake 2009 algorithm 1 shows its pseudocode this tool only calculates the downstream flow length raster d f l for each subwatershed using the built in flow length tool and finds headwater cells with the longest flow length from the boundary cells it then traces down the flow direction raster starting from the headwater cells the tool implicitly assumes that longest flow paths always start from the subwatershed divide because it searches only the boundary zone for headwater cells algorithm 1 pseudocode for the longest flow path tool in arc hydro for arcgis pro created based on python code analysis image 1 3 methods 3 1 recursive definition of the longest flow path the i o of raster data often takes relatively a long computational time compared to actual raster manipulation and becomes a bottleneck qin et al 2014 it would be ideal to not use raster data at all but the most basic input is the flow direction raster map and it is not feasible to implement an algorithm that is purely vector based in this section we approach the problem of calculating the longest flow path from a different perspective and try to avoid calculating the upstream and downstream flow length rasters and summing these two rasters all together in fact the longest flow path defined by eq 1 can recursively be redefined as 6 lfp i lfp j l j i lfp j l j i lfp k l k i j k u p j k if u p 0 0 otherwise where lfp i is a longest flow path for cell i l j i is the flow path from cells j to i and u p is the set of up to eight upstream neighbor cells flowing into cell i in other words if there are no upstream neighbor cells we have reached a headwater cell and the longest flow path at the headwater cell will be a zero vector 0 if there are upstream neighbor cells a longest flow path for the current cell can be determined by connecting its upstream neighbor cell s longest flow path and the path from the neighbor to the current cell since there can be maximum eight upstream neighbor cells one condition has to be satisfied that the combined path of the upstream cell s longest flow path and the path from the neighbor to the current cell has to be longer than or equal to the length of the combined path of any other neighbor cells this recursive definition of the longest flow path looks simple but it is not straightforward to implement it in a computationally efficient way because the number of possible upstream cells can grow exponentially depending on the size topography and shape of the watershed assuming that each cell receives flow from n 8 upstream neighbor cells on average we can estimate the number of cells to visit v starting from one outlet towards upstream in s steps using the following equation 7 v i 0 s n i where s 0 means that we are at the outlet by plugging the number of cells on an m n map into v and backcalculating s for v m n we can estimate how many steps we can traverse the stream upwards before we have to visit all the cells on the map for example on a 1000 1000 map with an average upstream neighbor cells n 2 we would have to visit all the cells a million while being able to traverse the stream only 19 steps upwards for n 3 it would take 13 steps upwards until we have to visit all the million cells in the worse case when n 8 it would take only seven steps of course not all cells have the same number of upstream neighbors and calculating the actual number of steps would be more complicated however from these examples we learned that it would not be any efficient computationally compared to the flow length based approach to address the problem of an exponentially growing number of cells to visit in each step we need to devise some constraints that help us filter out non candidate neighbor cells as early as possible in the following sections we will discuss the relationship between the longest flow path and area of watersheds and develop a strategy that can filter out neighbor cells whose potential longest flow length is shorter than others 3 2 branching strategy hack 1957 proposed a power law relationship between the main channel length and area of watersheds this empirical relationship is usually referred to as hack s law rigon et al 1996 and can be written as 8 l c a h where l is the distance from the outlet to the drainage divide along the stream channel a is the watershed area and c and h are hack s coefficient and exponent respectively many researchers have studied the significance of h sassolas serrayet et al 2018 and it is generally believed to be slightly below 0 6 and greater than 0 5 rigon et al 1996 mesa and gupta 1987 derived a theoretical equation of h as a function of the watershed s magnitude n as follows 9 h n 1 2 π π n 1 2 π 1 n for n 1 and 1000 h is 1 147 and 0 509 eq 9 implies that h tends to converge to 0 5 as n approaches infinity and the minimum h value is 0 5 sassolas serrayet et al 2018 analyzed hack s coefficient c using around 22 000 watersheds in bhutan and found c between 1 5 and 2 5 for h 0 5 which is consistent with previous studies sassolas serrayet et al 2018 they proposed an equation for c using the gravelius compactness coefficient gc gravelius 1914 as follows 10 c 1 2 gc π 1 4 gc 2 π 4 where gc is the ratio of the watershed perimeter to the circumference of the circle whose area is equivalent to the watershed area gc can be written as 11 gc p 2 π a where p and a are the watershed perimeter and area respectively gc becomes 1 for perfectly circular watersheds imaginary of course and starts growing as a watershed is being elongated sassolas serrayet et al 2018 the minimum gc allowed in eq 10 is 4 π yielding 1 as the minimum c the coefficient c is not unitless and has a dimension of l 1 2 h where l denotes a length unit however as h approaches 0 5 c tends to be l 1 2 0 5 1 and c becomes practically unitless one consideration we have to take into account for this research is that the longest flow path does not always follow the main channel for upstream most subwatersheds the definition of the longest flow path may agree with that of the main channel in hack s law in some cases however for downstream subwatersheds the longest flow path starts from non stream land surface as shallow surface runoff and enters the stream later by definition the longest flow path is almost always longer than the stream channel flowing through a subwatershed for this reason we cannot directly use eq 8 with typical c and h values since the recursive definition in eq 6 traverses the flow path from downstream towards upstream we want to eliminate inflowing neighbor cells that cannot possibly generate a longest flow path by comparing them with their peers at each branching cell this paper proposes that just like the main channel length l in hack s law the longest flow length lfl be also estimated using eq 8 with a slightly different interpretation since hack s law involves the watershed area the flow direction raster cannot be used alone in the new recursive algorithm instead flow accumulation is computed using the flow direction raster and is used to estimate the subwatershed area at each cell the basic idea for branching and eliminating neighbor cells is simple let s say there are two inflowing neighbor cells we take their flow accumulation values and calculate their potential minimum and maximum lfls if the maximum lfl of one cell is shorter than the minimum lfl of the other cell the former cannot generate a longest flow path because its theoretically longest lfl is still shorter than the other cell s theoretically shortest lfl the former is discarded from further recursion and the latter survives and is further being traversed and evaluated 3 3 longest and shortest longest flow lengths we will address the longest longest flow length lfl max first because estimating this distance is more intuitive and easier given the flow accumulation value the number of upstream cells at a cell in what configuration would these upstream cells yield lfl max travelling all these cells diagonally only without any horizontal or vertical moves will result in lfl max therefore lfl max can be written as 12 lfl max s fac 2 where s is the width or height mostly the same in any gis of one cell and fac is the flow accumulation value at a neighbor cell now we will use eq 8 to estimate the shortest longest flow length lfl min since we compare one cell s lfl max with another cell s lfl min and decide whether or not to eliminate one of these cells it is important to be conservative in estimating lfl min because we do not want to discard those cells that would have yielded a longest flow path if they were not discarded accidentally if lfl min is estimated to be too short the recursive algorithm will not be able to filter out many non candidate cells or not at all in the worse case as if there were no branching strategy in place in contrast if it is estimated to be too long the algorithm will start getting rid of good candidate cells and can generate non longest flow paths considering that the longest flow path is usually longer than the main channel and it is safer to underestimate lfl min not to miss cells that are perfectly fine the paper proposes the following equation for lfl min 13 lfl min s fac eq 13 can be obtained by plugging c 1 h 0 5 and a s 2 fac into eq 8 a value of 1 for c is less than its minimum value 4 π for eq 10 and a value of 0 5 for h is its minimum value for eq 9 it means that eq 13 is always shorter than eq 8 with the minimum c and h for eqs 10 and 9 respectively and being conservative also the unit of c is unitless because h is 0 5 fig 3 shows three different simplified shapes of watersheds and their shortest longest flow path lfp with its lfl assuming that headwater points are located on the watershed boundary in these simplified watersheds the shortest lfp must be a straight line connecting the remotest point blue dots and the outlet red dots any other straight paths will not be able to drain the furthest point and any non straight paths will be longer than the shortest lfp for hypothetical circular watersheds lfl 2 π a for isosceles triangular and rectangular watersheds by taking the derivative of the lfl with respect to w we can obtain 14 dlfl d ω lfl 1 w 4 4 a 2 w 3 and 15 dlfl d ω lfl 1 w 4 a 2 w 3 respectively by setting eqs 14 and 15 to 0 we found that w 2 a and w 2 a minimize the lfl of isosceles triangular and rectangular watersheds respectively plugging these w values into the lfl equations in fig 3 yields the shortest lfls of 2 a and a respectively for both types of watersheds since we use the fac raster to calculate the watershed area we can substitute s 2 fac for a in the final lfl equations therefore all three lfls for the simplified watersheds can be rewritten as 2 s π fac s 2 fac and s fac and we confirmed that all these lengths are longer than or equal to the shortest lfl defined in eq 13 3 4 new grass gis module the recursive algorithm and its iterative variant take a flow direction raster and outlet points as input parameters it was a design decision not to provide subwatershed polygons because delineating them is an additional step however since subwatershed polygons are not provided these algorithms have no knowledge about the shape of subwatersheds in addition fac values read from the flow accumulation raster are only accumulated without flow subaccumulation introduced in algorithm 2 these algorithms would only generate longest flow paths for accumulated watersheds the flow subaccumulation process takes each of input outlet points and subtracts its fac value from the fac value of its downstream cells the final result of this process is a raster where individual subwatershed regions have their own flow accumulation computed independent of their upstream flow contribution if the flow direction raster is clipped to each subwatershed flow accumulation is computed and resulting flow accumulation rasters are patched together for all subwatersheds the patched raster will be identical to the flow subaccumulation raster this process is computationally very light because it only traces down not up along the downstream portion of lfps both recursive and iterative algorithms perform flow subaccumulation before starting the up tracing process algorithm 2 pseudocode for the subaccumulate function for both recursive and iterative algorithms image 2 algorithm 3 is used to find inflowing neighbor cells and calculate their attributes including the accumulation value downstream length and theoretical minimum and maximum lfls which will be used later in the main logic for up tracing in both recursive and iterative algorithms algorithm 3 pseudocode for the findup function for both recursive and iterative algorithms image 3 algorithms 4 and 5 present pseudocode for the main procedure and its subroutine respectively for implementing recursive lfp tracing algorithms 6 and 7 present pseudocode for the main procedure and its subroutine respectively for implementing lfp tracing iteratively using a stack as its primary data structure for managing neighbor cells the r accumulate grass gis module implements both recursive with the r flag and iterative default without the r flag lfp algorithms this module is written in the c language kernighan and ritchie 1988 to differentiate between the two algorithms in the same grass gis module the recursive and iterative implementations will be referred to as r accumulate recursive and r accumulate iterative respectively the reason why the author implemented the iterative version is for stack safety in deep recursion any function or procedure calls use the stack memory to store data about the calls until those routines return to the caller reese 2013 however the size of this memory is limited and invoking a function recursively too many times can cause a stack overflow when the system runs out of stack memory reese 2013 in fact this stack overflow condition occurred in one of the experiments that will be discussed in section 4 by converting recursive calls into a last in first out lifo data structure using the heap memory a stack in the heap memory the iterative version avoids stack overflows that can be caused by deep recursion in the recursive version one lfp is searched for first all the way to the upstream most headwater cell of one neighbor cell and move to the next branch depth first search while in the iterative version all neighbor cells are pushed to the stack first move upstream by one cell and repeat these two steps until we find an upstream most headwater cell breadth first search this difference in search strategies becomes important when the size of a subwatershed is large because depth first search would require a lot of stack memory for excessive recursion algorithm 4 pseudocode for r accumulate recursive image 4 algorithm 5 pseudocode for the traceup function for r accumulate recursive image 5 algorithm 6 pseudocode for r accumulate iterative image 6 algorithm 7 pseudocode for the traceup function for r accumulate iterative image 7 3 5 benchmark experiments the states of georgia and texas in the united states were selected for benchmark experiments the 1arcsecond approximately 30 m national elevation dataset ned tiles were downloaded from usgs 2020 patched and clipped to the state boundaries of georgia and texas a hundred random outlet points were generated for each state once and the same set of outlets was used for all the experiments for the same state there are many different factors affecting the performance of software including system architectures central processing units cpus clock speeds the sizes of cache and random access memory ram drive types operating systems oss compilers software implementations etc lee et al 2016 micheloni and olivo 2017 wang et al 2019 however because of limited computational resources the main purpose of the benchmark experiments in this study is to see how the disk type and memory size impact the performance of the recursive r accumulate recursive and iterative r accumulate iterative versions of the grass gis module the author used grass gis on two linux systems with different configurations including one with a lower clock speed a hard disk drive hdd and more memory and the other with a higher clock speed a solid state drive ssd and less memory he also used the longest flow path tool from the arc hydro toolbox for arcgis pro on a windows system for benchmarking with commercial software the windows system is equipped with the most recent cpu with the highest clock speed among the three systems and an ssd since the windows system did not have access to grass gis administrative rights are required to run the grass gis installer only the arcgis pro tool was run on this system similarly arcgis pro is not available for linux so only grass gis was used on the linux systems table 2 shows the system specifications used for experiments including oss cpus the amounts of ram the sizes of swap partitions and software versions since the longest flow path tool in arc hydro pro accepts subwatersheds instead of outlets the watershed tool built in arcgis pro was used to delineate 100 subwatershed raster maps first which were then converted to polygon vector maps using the raster to polygon tool before these experiments were conducted the longest flow path tool in arc hydro pro will be referred to as the arc hydro tool or simply arc hydro hereafter because its name longest flow path is too generic two different experiments were conducted including batch and non batch mode experiments the batch mode experiment assumes a scenario where we want to compute longest flow paths for multiple outlets at once in one run of each program it is expected to increase disk i o compared to the non batch mode experiment explained later because a program needs to read in a flow direction raster and calculate flow accumulation and subaccumulation for the entire state not just for the area of interest this experiment tests how fast and efficient each program is at computing multiple longest flow paths once after reading input data each program is run 30 times independently for each state the results are used to calculate the average and standard deviation of run times in contrast the non batch mode experiment runs programs for each outlet grass gis module or subwatershed arc hydro tool at a time independently in a separate process since the arc hydro tool uses subwatershed polygons as a mask the grass gis module also used the same polygons to restrict the computational region to be fair in this experiment the burden of disk i o is expected to be lower than in the batch mode experiment because programs do not have to read in any data outside the area of interest this experiment allows to analyze the performance sensitivity of the program to the subwatershed area each program is run once for each outlet independently hence there are 100 runs for each state because there are 100 outlets or subwatersheds for the arc hydro tool 4 results 4 1 validation of longest flow paths first we need to validate eqs 12 and 13 proposed for the longest and shortest longest flow lengths respectively for both states the lengths of all calculated longest flow paths were strictly between lfl min and lfl max estimated using the equations the gravelius compactness coefficient gc calculated using eq 11 varied from 1 64 to 5 49 plugging the gc into eq 11 yielded hack s coefficient c of 1 99 7 25 which is greater than the assumed c 1 in eq 13 for the shortest longest flow length for the state of georgia all the three programs have successfully completed 30 batch runs and 100 non batch runs however for the state of texas r accumulate recursive failed to complete all 30 batch runs and one non batch run on both linux ssd and hdd systems in fact the 30 batch runs failed because of the same subwatershed for which the one failed non batch run was not successful at calculating the longest flow path with a segmentation fault error except for these failed cases r accumulate recursive and r accumulate iterative generated identical longest flow paths with a maximum error tolerance of 1 10 8 m due to round off errors in both experiments fig 4 shows the longest flow paths generated by r accumulate iterative figs 5 and 6 highlight major differences between the results from r accumulate iterative and arc hydro because of the assumption made in the arc hydro tool where any longest flow paths must start from the drainage divide of a subwatershed and cannot start from its interior it is clear from fig 5 that the arc hydro tool can generate inferior longest flow paths that are not truly the longest in some cases thirteen and six longest flow paths in georgia and texas respectively started from the interior of their subwatersheds and the arc hydro tool generated not so longest flow paths that are shorter than those generated by r accumulate iterative the probabilities of generating invalid longest flow paths are 13 and 6 out of 100 longest flow paths for georgia and texas respectively fig 5a shows a relative error in length of 1 3 but fig 5b presents a more extreme case where the error is 27 4 another feature of the arc hydro tool is an extension of the end node of the longest flow path but it simply extends the last node into the same direction as the last segment not in the flow direction at the outlet cell as shown in fig 6a last from an independent run using a different dataset neteler and mitasova 2008 the arc hydro tool generated a shorter longest flow path for a subwatershed where the true longest flow path actually starts from a boundary cell which means that this subwatershed satisfies the tool s drainage divide assumption but the tool still failed to find the longest flow path 4 2 comparisons of elapsed times table 3 summarizes 30 elapsed times for processing all the 100 outlets in each state in the batch mode experiment overall the arc hydro tool was about 22 49 times slower than the grass gis module for georgia the linux ssd system was about two times faster than the linux hdd system on the same linux ssd system the worst run of r accumulate recursive outperformed the best run of r accumulate iterative however on the linux hdd system one r accumulate recursive run was the worst of both versions and the range of r accumulate recursive except the worst outlier was within the range of r accumulate iterative for texas the linux ssd system was about 1 4 times slower than the linux hdd system however r accumulate recursive failed to complete on both systems so comparisons between the two versions were not made table 4 summarizes 100 elapsed times for each state in the non batch mode experiment the grass gis module on both linux ssd and hdd systems was about 34 97 times faster than the arc hydro tool on the windows system with an ssd drive fig 7 shows the performance sensitivity of each program to the subwatershed area for both states the elapsed time of the arc hydro tool grew exponentially with increasing subwatershed areas its log linear regression analysis yielded adjusted r 2 values r adj 2 s of 0 9194 and 0 8681 for georgia and texas one outlier excluded from texas respectively with a p value less than 2 2 10 16 in contrast the grass gis module exhibited a linear growth r adj 2 0 9503 with a p value 2 2 10 16 for both states 5 discussion the grass gis module on the linux systems outperformed the arc hydro tool on windows in both batch and non batch mode experiments even though the windows system has the best hardware specifications except the size of ram less than linux hdd but more than linux ssd the performance of the arc hydro tool deteriorated exponentially as the subwatershed size grew as opposed to the linear performance deterioration of the grass gis module this inefficiency of the arc hydro tool may be attributed to esri s decision to implement the arc hydro toolbox using their python application programming interface api instead of the net microsoft corporation 2020 api which is a more advanced approach to extending the capabilities of arcgis pro esri 2020b however it is still to be investigated if the lower performance was actually due to the additional python layer between the frontend and its core engine or simply the tool s algorithm without a net implementation of the arc hydro tool it would not be feasible to determine which was the case the arc hydro tool not only was slower but also had three issues as follows 1 longest flow paths starting from the interior of subwatersheds were not correctly calculated because of its boundary assumption where the tool assumes that longest flow paths only start from boundary cells fig 5 2 for one non benchmark run it was not able to calculate the correct longest flow path even though the boundary assumption was not violated fig 6b and 3 its end node extension feature does not consider the flow direction at the outlet cell fig 6a in the batch mode experiment the linux ssd system was faster than its hdd variant except for the case of texas since the linux hdd system has a slower cpu and hdd with more ram we can see that the amount of memory played a more important role in processing the texas subwatersheds texas has about total 2 million integer 4 bytes cells in the flow direction raster which when loaded into memory occupies 8 gb the grass gis module additionally allocates 8 gb to calculate a flow accumulation raster holding these two rasters in memory alone requires 16 gb leaving only on disk swap space for other tasks such as recursion and storing outputs on the linux ssd system which has 16 gb system memory the faster cpu did not help when memory ran out and the slower swap space started kicking in on the other hand the linux hdd system still has 32 gb of ram for calculating longest flow paths however even with the remaining 32 gb memory recursively traversing the largest 51st subwatershed in texas consumed up the 8 mib stack memory allocated by the os kernel to r accumulate recursive the area of this subwatershed is 47 077 62 km2 given the 30 m resolution of the elevation data this area translates to total 52 million cells assuming that only 5 which is conservative compared to the size of the subwatershed of the 52 million cells need to be recursively visited to find the longest flow path the average stack size per visit or per recursive call would be only 3 bytes 8 1024 2 0 05 52 1000 2 which is not enough to store state information about each recursive function call including variables this lack of stack space caused a stack overflow since all the 100 subwatersheds were being processed together in one process the stack overflow resulted in no outputs at all in contrast with no recursive calls r accumulate iterative was able to leverage the swap space on the linux ssd system or the remaining 32 gb memory on the linux hdd system still the linux ssd system was slower because a disk based swap partition is slower than real memory in the non batch mode experiment the linux ssd system consistently outperformed the linux hdd system the stack overflow issue did occur again with the largest texas subwatershed but unlike in the batch mode experiment r accumulate recursive calculated the other 99 longest flow paths successfully because each subwatershed was processed separately also for the same reason the linux ssd system ran out of memory only with this subwatershed and handled the other subwatersheds smoothly with enough memory therefore no performance hit was observed on this system both linux systems showed consistent performance per subwatershed area across the states regardless of the total area 0 00034 s km2 to 0 00039 s km2 for linux ssd and 0 000 60 s km2 to 0 00076 s km2 for linux hdd as shown in table 4 we can use this consistency in performance to predict computational times when running the grass gis module in terms of the total computing time of the grass gis module between the batch and non batch mode experiments the former was faster than the latter for georgia and vice versa for texas for georgia it took more time to process individual subwatersheds separately this added computational time in the non batch mode experiment can be explained by an overhead expense of creating 100 separate output vector maps and repeating basic managerial tasks such as reading header files adjusting computational regions etc however for texas the non batch mode experiment was faster because processing one subwatershed at a time is much more memory efficient than processing all the subwatersheds in one process especially when there are many large subwatersheds in the state in both experiments r accumulate iterative r accumulate without the r flag turned out to be more robust with no failures and only slightly slower than its recursive sibling r accumulate with the r flag for this reason it is recommended to use the r accumulate module without the r flag which is the default currently administrative rights are required to install grass gis on microsoft windows which is why the r accumulate module was only tested on the linux systems however grass gis and the new module also support microsoft windows xp or newer and macos 10 4 10 or newer future research includes implementation of the proposed algorithms for arcgis pro and comparison of their performance with that of the grass gis module 6 conclusions in this paper we studied the original longest flow path approach and briefly reviewed existing longest flow path programs it was shown how hydrologically invalid longest flow paths could be generated by naively converting raster results to vector and a recursive definition of the longest flow path was introduced naturally a new recursive algorithm was proposed based on the recursive definition for calculating the longest flow path using depth first search and its iterative counterpart algorithm was developed using breadth first search these algorithms use equations for the longest and shorted longest flow lengths that were derived from hack s law a branching strategy was devised to help the proposed algorithms filter out inferior neighbor cells and speed up traversal using randomly generated 100 outlets and subwatersheds each in georgia and texas performance comparisons were made between the recursive and iterative versions of the r accumulate grass gis module that implement the proposed algorithms and the longest flow path tool in the arc hydro toolbox for arcgis pro an implicit assumption in the arc hydro tool was tested and shown to not work well in some case with increasing subwatershed areas the performance of the arc hydro tool exponentially deteriorated while that of the grass gis module linearly declined the grass gis module outperformed the arc hydro tool but the recursive implementation failed to process the largest subwatershed in texas because of a stack overflow resulting from deep recursion the iterative implementation was more robust in terms of memory usage and successfully calculated all the longest flow paths for both states since the iterative implementation was only slightly slower than its recursive counterpart the iterative version is highly recommended over the other version grass gis and the new module support microsoft windows xp or newer with administrative rights macos 10 4 10 or newer recent gnu linux or a unix variant future work includes implementation of the proposed algorithms for arcgis pro software availability grass gis software free under the gnu gpl license grass gis https grass osgeo org r accumulate https github com osgeo grass addons tree master grass7 raster r accumulate recursive version with the r flag r accumulate recursive iterative version by default without the r flag r accumulate iterative operating system requirements microsoft windows xp or newer macos 10 4 10 or newer recent gnu linux or a unix variant declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the author thanks dr daniel p ames for handling this manuscript and an anonymous reviewer for providing constructive comments he also thanks the grass gis community for their continued support for improving the historical modules and stimulating the development of the new software 
26007,water level monitoring and forecasting are essential tasks in flood emergency response this study proposes an edge computing based sensory network ecomsnet an innovative decentralized early warning system ews for water level monitoring and prediction a sensor embedded algorithm integrates the direct step method dsm with a microgenetic algorithm mga this algorithm predicts the water surface profile and corrects it once water level observations are available it also meets efficiency requirements to accommodate sensor computation limitations the errors in the predicted water surface profiles in channels with gradually varied flows are 5 in a laboratory flume experiment and below 10 in a field experiment the ecomsnet is an achievement of edge computing based internet of things it shows potential to increase emergency response efficiency however the system requires further refinement and testing if it is to adequately address rapidly varied unsteady flow in a scaled up implementation keywords early warning system edge computing iot microgenetic algorithm water level prediction software and or data availability software name mudspv4 developer tsun hua yang year first official release 2020 hardware requirements pc system requirements windows linux mac program language python program size 1 mb availability https github com joshyang1977 ecomsnet license gpl 3 0 1 introduction flooding is a major global problem that affects millions of people annually acosta coll et al 2018 it ranks first in terms of occurrence among natural disasters and is estimated to have affected approximately 107 million people from 2008 to 2018 cred 2018 structural and nonstructural measures have been devised and developed to prevent or mitigate losses of life and property among all the measures early warning systems ewss have shown their capability to reduce the possibility of personal injury loss of life and damage to property and the environment acosta coll et al 2018 angermann et al 2010 ewss for rivers can provide forecasts of the magnitude of peak flow and its occurrence time with an extended lead time decision makers can then take necessary measures to mitigate the flood damage accordingly observation devices in the ews serve to monitor onsite and real time information and the observed records provide inputs to flood models to produce forecasts for example water level gauges measure time varied river stages and transmit the data to a remote server the flood model in the server uses the received data as initial or boundary conditions to simulate future river stages this is the most common framework for ewss termed the centralized ews framework e g chang et al 2018 gourley and clark 2018 the effectiveness of the centralized framework relies on tight integration of geographically distributed system components such as sensors and central computing sources communication structure failure is always a concern especially during extreme events angermann et al 2010 in addition the decision making time for emergency response comprises data monitoring transmitting processing and decision making the shorter the processing time the higher the efficiency is of the emergency response for example du et al 2019 proposed a sensor web to provide flood event process detection and instant service however previous studies are still based on the centralized framework this approach cannot avoid a system delay if any failure occurs during the data transmission process the efficiency cannot be improved because the processes of data monitoring transmission processing and decision making in a centralized ews are all substantially time consuming when data are mainly produced at the edges of a network i e river stages and rainfall it is more efficient to process the data at these locations than at other locations in other words the sensor not only collects but also processes the observed data to produce informative results consequently a decentralized framework named edge computing is adopted in this study to allow computation to be performed at the sensor the definition of edge computing is a distributed computing framework that executes computation and data storage tasks close to the locations where the data are needed emergency response activities usually take place at the location where a disaster occurs edge computing has the potential to address the concerns of response time requirements increasingly more services are removed from the cloud centralized to the edge decentralized of the network because data processing at the edge can ensure shorter response time and better reliability shi et al 2016 thanks to the rapid development of the techniques of the internet of things iot and associated hardware the technique of edge computing has pushed the horizon of a new emergency response paradigm through integration with iot technology such as on site microcomputer based sensors the edge computing based iot or eciot omoniwa et al 2019 can collect and process data and then generate decision supporting information simultaneously without delay few studies in the development of ewss have considered the eciot talat et al 2019 presented a decentralized flood detection system using image data this system used on site embedded cameras in sensors to capture and process images these sensors generate warning alarms based on their received information however it is not clear in that paper how the warning alarm is generated vunabandi et al 2015 proposed a framework that covers both arduino based technology and other inexpensive wireless sensor network components to detect floods and send an alert to the local community in rural rwanda the sensors in that study continuously transmitted the data to base stations for information processing the base station generated an alarm based on its decision to notify the residents about the flood the performance was not characterized in that study perumal et al 2015 deployed a water level sensor locally to detect the desired parameter if the water level reached the parameter threshold the signal was fed in real time to a social network such as twitter a cloud server was configured as a data repository mhaisen et al 2018 proposed a self powered water monitoring system that included a water monitoring device centralized cloud receiving and processing data and a mobile application to display results studies such as perumal et al 2015 and mhaisen et al 2018 relied on a centralized cloud system to process and analyze data therefore their sensors involved the data consumer instead of the data producer which might incur longer response times and add a heavy load to networks this study implements the concept of eciot and develops an edge computing based sensory network named ecomsnet for real time water surface profile predictions starting with a simple algorithm for data quality control qc of observations the ecomsnet applies the direct step method dsm to rapidly predict water levels given a few parameters such as the discharge channel bottom slope and roughness coefficient the sensory network monitors the variation in water stages with ultrasonic sensors and then provides water level predictions with the dsm even with limited computing power the ecomsnet can still rapidly provide predictions by itself it is not necessary to transmit the observations from local sites to central computing devices or cloud services to provide further processed data to improve the accuracy of the predictions three real time correction techniques were developed and embedded in the ecomsnet the sensors update the predictions in real time by using the three correction techniques once the observations become available further decision supporting information such as flood warning or evacuation messages can also be directly issued to other iot devices from the ecomsnet its innovative decentralized ews framework avoids accidents arising from communication failure between local sensors and central servers and decreases the data processing time therefore increasing the effectiveness of emergency response this study tested the ecomsnet in the lab and field experiments with gradually varied flows gvf and proved its capability to predict the water levels however further testing and refinement for unsteady flow conditions are necessary if the ecomsnet is going to be implemented in a scaled up river flood warning system as the first attempt of the application of eciot in the ews this study lays out a system structure preliminary results and findings based on the structure can inform further applications the remainder of this paper is organized as follows section 2 details the ecomsnet system structure and its major components the ecomsnet concept was verified with a laboratory flume and a field experiment details regarding the experiments are discussed in section 3 finally sections 4 and 5 present the results discussion and conclusions 2 ecomsnet system structure the purpose of this study is to develop a sensory network system that can provide real time water level observations predictions and correction based on the eciot concept the system structure is illustrated in fig 1 the system named the ecomsnet is an embedded system that consists of raspberry pi rpi based monitoring devices with dedicated functions the monitoring devices use ultrasonic sensors to measure the water levels and the recorded data are preprocessed for qc before further calculation the processed data are exchanged through wireless techniques e g wi fi 4g the long range spectrum lora and nb iot between upstream and downstream sensors thereafter the water level predictions and the corrections are performed using developed algorithms the abovementioned system architecture is illustrated in fig 1 the system consists of four components 1 water monitoring sensors an ultrasonic sensor mounted on the rpi board can measure the water level at a frequency of 100 hz 2 a qc algorithm is developed to filter the measurement noise and ensure the quality of the input data used by the prediction model 3 a water level prediction algorithm a scheme is chosen based on the dsm and designed to provide predictions to meet the needs of rapid prediction efficiency 4 a correction algorithm is developed to improve the performance of predictions using real time observations the details of each component are described in the following subsections 2 1 raspberry pi rpi based sensor low cost open source and low energy consumption sensors are always of interest for environmental monitoring kwok 2017 prince et al 2019 an automatic and wireless water level sensor usually incurs a high purchase price us 1000 5000 per unit therefore financial considerations become an obstacle to a dense monitoring network for the deployment of any ews this study therefore considered the rpi as a platform embedded with an ultrasonic sensor to process and analyze the monitored data at a local site the detailed sensor setup is shown in fig 2 the rpi is a reliable low cost approx us 35 microcomputer which was developed in 2006 by the university of cambridge s computer department and produced by the raspberry pi foundation since 2012 as a tool to encourage students to learn programming languages prince et al 2019 it allows users to use the python programming language in the unix debian based raspian operating system a free operating system optimized for rpi hardware there are a few models of rpi this study applied the rpi 3 model b which was released in february 2018 with a 1 4 ghz 64 bit quad core processor onboard 802 11ac wi fi 100 mbits sec bluetooth and usb boot capabilities the rpi 3 model b is powered by a micro usb cable and uses an sd card for booting and long term data storage two usb ports are provided for additional usb components in this study the rpi connects to the internet using onboard wi fi if available or a wireless 4g usb internet adapter if necessary lora a spread spectrum modulation technique for data transmission was also tested in this study during a field experiment it was found that lora is well suited for the data i e water level transmitted in this study the details are not discussed here finally the ultrasonic sensor is connected and powered through the general purpose input output gpio pins included in the rpi design an ultrasonic sensor mounted on the rpi measures the distance to the water surface the sensor emits sound waves at a specific frequency and detects the sound waves that bounce back the distance can be calibrated by the elapsed time between the generated and returning sound waves environment conditions such as temperature and ambient noise impact the performance of ultrasonic sensors the sensor used in this study is the high performance ultrasonic distance sensor ks103 with an i2c bus from shenzhen dauxi technologies co ltd 2014 it works in the range of 1 cm 800 cm with functions of temperature compensation and noise cancelation the sensor detects frequencies of up to 500 hz sensor calibration was performed in static water before the laboratory and field experiments table 1 shows the calibrated results all the coefficients of determination r2 for the three sensors were close to 1 0 the ultrasonic observations were almost the same as the ground truth observations the ultrasonic sensor observations were calibrated by using the trendline equations in table 1 2 2 depth difference data quality control qc scheme monitored data serves as major input to the ecomsnet fig 1 the data quality dq and accuracy are critical requirements for a high prediction performance if observed data are of poor quality the decision making process is likely to be unsound the observation accuracy is modified using the trendline equations in the previous session data outliers are one of major manifestations of data uncertainty karkouch et al 2016 in this study the data sampling frequency of the ultrasonic sensor can be high because it takes less than a second to measure the distance between the sensor and the water surface the surrounding environment where the sensor is deployed which may involve abnormal sound wave reflection because of sudden moving objects e g birds or irregular vibrations e g truck vibrations has a significant impact on the measuring results considering the data life cycle in the iot qin et al 2014 the sampling interval δ t in this study as calculated as eq 1 is highly related to the distance d and surrounding environment an interval of 2 s is chosen and there are at most 300 observations in an observation period of 10 min if there is no interference 1 δ t 2 d v s where δ t is the sampling interval and d is the distance between the ultrasonic ssor and the water surface vs 343 m s is the speed of sound at 20 c since outliers can be considered events with extremely small probabilities of occurrence they are also seen as points in a data set that are highly unlikely to occur given a model othe data otey et al 2006 therefore a simple real time data qc scheme is applied in the study before serving as input to the prediction scheme fig 3 shows an example of data observation from the ultrasonic sensor data exchange is performed between the upstream and downstream sensors the depth upstream is assumed to be lower than the depth downstream because of the positive slope and gvf conditions in this study the observations within the red square in fig 3 are those that violate this assumption a therefore are excluded from the inputs to the following prediction process by implementing this rule the dq is assured to meet the assumptions of this study prediction can then proceed and the predicted results are considered to be reasonable 2 3 direct step method dsm water surface profile calculations are necessary for disaster response for a specified discharge in the eciot based ews an efficient and accurate flood prediction scheme is necessary to meet the requirement of a high data sampling frequency because of the limited computing power of the microcomputer an efficient and robust water surface profile prediction scheme is vital the dsm proposed by chow 1959 is characterized by dividing the channel into short reaches that are computationally analyzed step by step from one end of the reach to the other this simple step method is applicable to prismatic channels and gvf given the channel cross sections channel roughness n and rate of discharge q the flow depth and velocity can be rapidly computed the dsm method has been tested for its capability to predict the water level theoretically it was also used in this study to prove its capability for the computation of gvf conditions in the lab and field however if the application is in rapidly varied unsteady flow rvf conditions the dsm may need to be modified to account for flow unsteadiness behaviors the rpi sensors in the ecomsnet provide high frequency observations of water depths the dsm which is embedded in the rpi sensor can rapidly compute unknown depths at a section adjacent to the observations the dsm is derived from the energy equation below and illustrated in fig 4 for a short reach of length δ x x i 1 x i it is assumed that the total head energy at the two end sections i and i 1 is equal 2 s 0 δ x y i α i v i 2 2 g y i 1 α i 1 v i 1 2 2 g h f where s 0 z i z i 1 δ x is the bottom slope and y i i 1 and v i i 1 are respectively the depths of flow and mean velocities at sections i and i 1 the total head loss h f due to friction is assumed as follows 3 h f 1 2 s f x i 1 x i wre s f is the friction slope at the short reach δ x shown in fig 4 the u s army corps of engineers united states army corps of engineers 1982 proposed three approaches tdetermine the friction slope average friction slope arithmetic mean friction slope and harmonic mean friction slope chaudhry 2007 suggested that the average slope is proven to yield satisfacry results and is the simplest of the above three methods therefore the average friction slope s f 1 2 s f i s f i 1 is used in this study and δ x is solved as shown below 4 δ x e i 1 e i s 0 s f w e y α v 2 2 g is the specific energy and the energy coefficients a aumed to satisfy α i 1 α i α the friction slope s f is obtained from manning s equation as follows 5 v 1 n r 2 3 s f 1 2 ere n is manning s roughness coefficient r a p is the hydraulic radius p is the wetted perimeter and v q a where q is known eq 4 is used to determine δ x when the flow depths at sections i and i 1 are known for a specified discharge q in this study the dsm calculation starts from either the upstream or the downstream end where the water depths are observed the water depth y i 1 athe other end of the reach is assumed to be y i δ h and δ h is assumed to be 0 01 cm in this study to obtain the smallest δ x the smallest δ x can meet the need for the dsm assumption the flow depth at any location can be estimated by iterating the abovementioned steps and accumulating δ x to the specific location the obtained location of the predetermined water depth may not be exactly the same as that at the desired location this condition is still acceptable since the difference in the water depth is only 0 01 cm 2 4 real time correction techniques the water surface profile was obtained from the method proposed in the previous section however in the ews the input data uncertainty and the flood model structure and its associated parameter uncertainties decrease the forecast performance liu and gupta 2007 incorporating observations into the forecasting process can efficiently reduce the uncertainties and increase the prediction accuracy shu et al 2005 a few studies have shown the effectiveness of postprocessing and statistical correction techniques to reduce forecast errors e g pagano et al 2011 and shen et al 2015 this study developed three postprocessing methods to obtain the best performance 1 arithmetic average 2 constant error assumption between observations and predictions and 3 application of a microgenetic algorithm mga for an optimized channel roughness n the details of these real time correction techniques are discussed below 2 4 1 upstream downstream adjustment method uda a simple arithmetic adjustment method of predictions is proposed eq 4 shows that the distance δ x is obtained from water depths from y i to y i 1 upstream to downstream direction the opposite calculation can also be performed to obtain δ x from water depths y i 1 to y i downstream to upstream direction the only difference is the sign of δ x the assumption is made that a constant prediction error ε in water depth predictions exists if both y i and y i 1 are known at locations x i 1 and x i respectively the water surface profile between these two observations e g the middle point x i 1 2 can be obtained as follows 6 y i 1 2 i f y i ε e y i 1 2 i and y i 1 2 i 1 are the water depth predictions from upstream i or dnstream i 1 and ε is the prediction error the subscript i 1 2 means that the predicted location is at location x i 1 2 and the superscripts i and i 1 mean that the prediction is based on an observation at either location x i 1 or location x i finally y i 1 2 is the arithmetic average for prediction at x i 1 2 between y i 1 2 i and y i 1 2 i 1 and serves as the corrected final result chaudhry 2007 mentioned that it is not necessary to compute the water surface profile in the upstream or downstream direction according to its flow condition super or subcritical flow while using the dsm the computations do not become unstable or yield incorrect results ithe abovementioned convention is not followed chaudhry 2007 noted that if the flow depth is known at a control section there is no reason why computation in either the upstream or downstream direction would bdifferent this is because the system either numerically solves a differential equation for the specified itial condition or solves a nonlinear algebraic equation therefore this study developed the uda correction approach which is based on the averaged results from the upstream and downstream sides 2 4 2 constant error assumption method cem the dsm predicts the water depth y i 1 i at location x i 1 based on the observed water depth y i o at location x i fig 4 the distance between x i and x i 1 is divided into several short reaches and the computation is carried step by step from one end of the reach to the other the observation y i 1 o is also available when the predicted y i 1 i is provided because sensors are installed at x i and x i 1 this study assumes that there is a constant error or so called residual r between the observation and the prediction at the same location for simplicity many studies have assumed constant error in the data between observation and prediction especially for linear functions tellinghuisen 2001 the plot of predictions y i 1 i vs observations y i 1 o at location x i 1 in shown in fig 5 the residual r i 1 is shown as follows 7 r i 1 y i 1 o y i 1 i a constant residual r is assumed along the stream so the residual r i 1 2 at location x i 1 2 can be obtained as follows 8 r i 1 2 r i 1 finally the corrected water depth at location x i 1 2 can be obtained by 9 y i 1 2 c o r r e c t e d y i 1 2 i r i 1 2 2 4 3 optimized manning s roughness n using a microgenetic algorithm mga the previous two correction techniques are based on manning s roughness n in eq 5 being known if the material of the experimental flume or on site channel is known manning s roughness n can be approximately estimated chow 1959 however this parameter cannot be easily estimated in practice in the case of concrete or grass lined channels none of their roughness coefficients are the same or known therefore a self identified algorithm is developed to find the optimized n according to best available observations this study applied an mga to find the optimized manning s roughness n for the best water depth predictions the mga has proven its capability to identify this parameter in river flow modeling yang et al 2014 the details of the mga were presented previously krishnakumar 1990 yang and tsai 2019 and will not be discussed here this study applied the assumptions of the mga described by krishnakumar 1990 which include a population size of five a crossover rate of one and a mutation rate of zero the following steps shown in fig 6 are used to obtain manning s roughness n during real time operations step 1 generate a group of five chromosomes at random the binary string is used to code n for each station into a chromosome in this application there are 8 genes in a chromosome step 2 calculate the predicted water depth when there is an observation available at every sampling interval δt to obtain the optimized manning s roughness n the minimum fitness value f is the objective function which is defined as follows 10 f y i t o y i t where y i t and y i t o represent the predicted and measured depths at a given location x i respectively and t represents the moment when the observation becomes available step 3 keep the fittest chromosome for the next generation carry out tournament selection for the remaining four chromosomes with a crossover rate of one and a mutation rate of zero until the new generation is achieved step 4 calculate the fitness value from eq 10 thereafter nominal convergence which means that all the individuals in the population have either identical or very similar genotypes is verified in this study nominal convergence occurs when more than 7 out of 8 genes in each chromosome in the entire population share the same value when nominal convergence is achieved the calculation is stopped and the best chromosome as well as the optimized manning s roughness n is obtained for this generation finally steps 1 to 4 are repeated 50 times to avoid obtaining local optimum solutions 3 details of laboratory and field experiments a laboratory flume and a field open channel were used to test the performance of the ecomsnet the laboratory experiment was conducted at the laboratory of sediment transport department of civil engineering national chiao tung university taiwan and the field experiment location was selected at an irrigation channel in miaoli county taiwan the experimental details are discussed in the following subsections and shown in table 2 3 1 laboratory experiment the experimental flume was a closed loop open surface recirculating rectangular flume with dimensions of 10 m in length 0 4 m in width and 0 5 m in height the bed slope of the flume was 1 4 a schematic sketch of the experimental flume is shown in fig 7 the water was pumped by a water pump and the flow rate was controlled by a valve the experimental section was 5 m long and was 4 m downstream from the entrance to establish a fully developed flow regime a honeycomb structure was set after the entrance to stabilize the inflow condition this study also adjusted the water depth using a tail gate at the outlet even though the flow discharge remained the same two rpi based ultrasonic sensors were installed upstream and downstream the sensors measured the water depths and computed the water surface profile automatically observed information and calculated results were exchanged through a wi fi network a third sensor was installed at the middle location for performance verification 3 2 field experiment a field experiment was conducted in an open channel located in miaoli county taiwan fig 8 it is an irrigation channel red line and water is diverted from the east river blue line next to the channel fig 8 a the length for the test reach between the upstream and downstream measuring points is 30 m long and the channel is 50 cm wide fig 8 b the channel slope is assumed to be 0 005 or 0 286 the flow in the channel is not purposely designed this study applied the flowtracker2 ft2 handheld acoustic doppler velocimeter by sontek xylem inc shown in fig 8 c to measure the flow discharge in advance the measured discharge was used as a known variable to the dsm in the ecomsnet to evaluate the performance this study also used the ft2 to measure water depths at locations ref 1 and ref 2 in fig 8 a in comparison with the predictions the locations ref 1 and ref 2 are 14 5 and 22 1 m from the upstream location respectively since it is an irrigation channel the water quality is in good condition and it is assumed that the water quality has no impact on the measurement in this study in comparison with river flow conditions the flow in the channel is still a low velocity quasi steady flow condition 4 results and discussion this study develops the ecomsnet to provide almost real time water depth forecasts the ecomsnet first processed the observations with the depth difference qc approach mentioned in section 2 2 and the outliers due to inaccurate signal reflection were excluded in the prediction process the backwater effect a typical flow condition for subcritical flow in alluvial rivers and open channels brandimarte and woldeyes 2013 was implemented by setting up a tail gate at the end of the experimental flume fig 9 this configuration was used to evaluate the performance of the ecomsnet finally the predicted results were corrected by three correction algorithms all abovementioned steps were performed in the sensors of the ecomsnet which is an implementation of the eciot it is also important to investigate the mean of the observations and its impact on the forecast results therefore the water depth predictions from the mean observations taken as the input in comparison with the mean water depth prediction by averaging the forecasts of all predictions at different times are discussed in this session finally an irrigation channel in the field was chosen to test the performance of the ecomsnet in practice 4 1 sensitivity analysis to different sizes of δh there are two parameters manning s n and δh used in the dsm the first parameter manning s n can be identified based on the condition of the flume this study used an mga technique to search for the optimized manning s n and therefore to obtain the best performance the dsm starts calculating from the upstream or downstream end where the water depth y i is observed and uses the water depth y i 1 at the other end to estimate the distance δx of the reach the difference between depths y i and y i 1 is the second parameter δh whose size also has an impact on the predictions a greater δh results in a larger δx during the step by step process the assumption for the dsm is that δx is small enough that the equations remain valid in other words the bias increases when δx is large therefore sensitivity analysis of different sizes of δh resulting in different δx values must be conducted before further application the lab i scenario in table 2 was selected to test the sensitivity to δh manning s n in this case is assumed to be 0 01 and five δh values are implemented in the dsm model fig 10 shows that different δh values produce different results when δh is smaller than 0 01 cm the results essentially converge therefore δh 0 01 cm was selected for the prediction of water depth in the following sections for efficiency and accuracy 4 2 performance evaluation this study evaluates the prediction performance using the following indexes 11 mean absolute error mae 1 t n 1 t y i t y i t o 12 root mean squared error rmse t 1 t y i t y i t o 2 t where y i t and y i t o respectively indicate the predicted and observed water depth at location i at time t the mae and rmse are the two most common indexes used to describe the overall performance of predictions in different applications e g nguyen et al 2015 and yang and tsai 2019 the rmse is the standard deviation of the prediction errors in other words the rmse is a measure of how spread out these errors are the mae was suggested to replace the rmse since the former is a more natural definition of an average error and is unambiguous willmott and matsuura 2005 4 2 1 comparisons of water depth predictions in the laboratory the performance of the ecomsnet was evaluated using the three laboratory experiment scenarios listed in table 2 the flow conditions and slopes were limited by the design of the experimental flume these discharges were 0 0087 and 0 0014 m3 s lab ii and iii have the same discharge but the water depth can be altered by adjusting the angle of the tail gate the ecomsnet was activated and produced predictions when the gvf condition was achieved the observation duration for the three scenarios was 10 min the water depth production at any given location in the flume was provided using observations confirmed by qc the three real time correction techniques uda cem and mga were then used to improve the predictions to provide modified results finally the performance of the ecomsnet was evaluated by comparing the modified results at the middle of the flume with observations there were 210 178 and 175 available observations after the qc process for lab i ii and iii respectively table 3 lists the performance of the water depth predictions from the three real time correction schemes within the ecomsnet the uda approach yields better mae and rmse results than the other approaches for all three scenarios from a comparison of the lab ii and lab iii results the mae and rmse values decrease when the water depth increases at the same flow rate this is because a higher water depth leads to a slower flow velocity as a result the variation of velocity along the flow direction δ v δ x is small and it is consistent with the assumption of the application of dsm model for gvf conditions the mga does not produce the best results among the three correction techniques its mae and rmse values are all below 1 0 cm which are on the same order of magnitude as the uda s values among all three techniques the cem produces the worst scores in terms of the mae and the rmse fig 11 shows the comparisons of predictions with observations at the middle location in the experimental flume the vertical axis in the figures is defined as follows 13 difference y i t y i t o y i t o 100 the box and whisker plots show the minimum first quartile median third quartile and maximum of the difference in all predictions within the observation period the cross sign corresponds to the average value in terms of the differences of all predictions eq 13 given any approach higher water depths or lower velocities lead to better performance in terms of the difference distribution for example the mga s average differences are 4 50 4 41 and 2 62 for the three scenarios the score decreases when the initial downstream water depth increases the variation in the difference distribution i e distance between the first and third quartiles for the same discharge but different water depths i e lab ii and lab iii is also smaller this finding indicates that the performance of the predictions is stable and converges when the water depth is higher the same results are found with the cem and uda a deeper water depth produces a more stable water surface condition resulting in better observations and prediction performance for the comparison between lab ii and lab iii a deeper water depth indicates a slower water velocity at the same discharge this leads to a more gvf condition that allows the dsm model to show better performance regardless of the comparison of the performance the ecomsnet with the uda and mga real time correction schemes can provide less than a 5 difference for all three flow conditions 4 2 2 detailed discussion of the performance of the uda and mga models a further discussion of the difference in the performances of the uda and mga models is warranted the uda approach assumes that manning s roughness n is known so the prediction and correction can be performed in the laboratory experiment all components that come into contact with water are made of stainless steel and glass reinforced plastic manning s roughness n is reasonably assumed to be 0 01 chow 1959 fig 12 compares the performance in terms of the difference eq 13 with various n values using the uda method the results show that the performance of the uda method is very sensitive to the assigned roughness n the performance varies from 2 3 to 9 4 for the lab i scenario 2 7 8 5 for the lab ii scenario and 0 42 1 9 for the lab iii scenario this indicates that the choice of an appropriate manning s n is essential for the prediction performance when the uda approach is applied however in reality the channel roughness is difficult to identify thus the uda approach is not practical de perez et al 2015 concluded that decision makers would hesitate to take any response measure if the performance of ews is not considered acceptable the meaning of unacceptable corresponds to too many false alarms or a large difference between observation and prediction because of the uncertainty of the observations the average performance of the mga for practical applications is discussed here with a total of 10 min for each flow scenario in table 2 an interval of 1 min was required to calculate the average water depths upstream and downstream the ecomsnet then used the average depths as inputs to provide the water depth predictions and corrections at the middle location this is a practical application to evaluate the ews s performance by considering the average observation and using it to provide overall prediction table 4 shows the optimized manning s roughness n values for each interval the consistency of the optimized n values implies the stability of the flow scenario lab iii shows the most consistent n value only the value of 0 03322 during the 5th to 6th minute is different from the others the more stable the flow condition is the better the performance of the ecomsnet is the lab iii flow condition exhibits the most gradual variation among the three scenarios fig 13 shows the average performance avg in comparison with observations at each interval using the associated optimized n above the dark and light gray areas represent the threshold values of 1 5 avg and 1 10 avg respectively the available sample sizes are 210 178 175 for the three flow scenarios as plotted in the figures to evaluate the performance the coverage rate for the vertical axis in the figures is defined as follows 14 coverage rate cr h i t s n where hits is the number of observations within the threshold values and n is the total available observations during the observation time 10 min in this study the cr values for the threshold values of 1 5 and 1 10 are 0 727 and 0 995 respectively this means that the average predictions from the ecomsnet plus 5 uncertainty can provide greater than 72 coverage in comparison with all observations this system can reach 96 coverage if a 10 uncertainty is added to the average predictions from the ecomsnet the 5 and 10 uncertainties are reasonable for engineering applications in practice the worst cr values of the 1 5 avg threshold were found at the 7th and 3rd minutes for the lab i and lab ii scenarios respectively the predicted water levels at the middle location were below most observations however the observations were still below the 1 10 avg threshold 4 2 3 application of the ecomsnet in a field experiment finally the ecomsnet was implemented in an irrigation channel to test the performance the details of the channel were described in section 3 2 the flow conditions were assumed to gradually vary because there was no in outflow during the experimental period for the reach two reference points at different locations were selected to test the prediction performance in comparison with the observations the times of water depth measurements at reference points 1 and 2 were 10 05 and 10 09 a m respectively the measured discharges were 0 056 m3 s and 0 030 m3 s and the measured water depths were 12 5 cm and 14 0 cm the flow conditions were not purposely designed and assumed to be quasi steady within that minute at the observation time above the discharges were used as input and the water depth predictions at associated locations using the mga approach were 11 68 cm and 12 75 cm the corresponding differences eq 13 were 6 5 and 8 9 the differences are slightly higher than in the laboratory results the roughness of the irrigation channel is not as uniform as the flume in the lab the slope of the channel is assumed to be constant in the ecomsnet but may not be constant along the test reach the flow condition in the field is also not as stable as in the lab differences arose over only 4 min but the discharge varied from 0 056 m3 s to 0 030 m3 s all the reasons above may influence the performance note that the differences were all within 10 the results are still consistent with the finding that the observations fell within the 1 10 avg range and are reasonable for engineering applications 4 2 4 discussion of the application of the ecomsnet this study proposed the ecomsnet and tested its performance and availability in the lab and field experiments the overall performance in the field was not as good as that in the laboratory experiment the prediction accuracy rates were 4 50 4 41 and 2 62 eq 13 for the three scenarios in the lab and 6 5 and 8 9 for the two scenarios in the field a few reasons were considered and explained above it is found that the environmental uncertainties such as the bottom slope flow discharge and channel roughness have a considerable impact on the performance for example the bottom slope along the flow direction one of the parameters in the dsm model is uniform in the lab channel but not in the field experiment the ambient conditions such as cross wind speeds and temperature have significant impacts on the performance of ultrasonic sensors jeon et al 2011 finally the dsm model used in this ecomsnet was efficient for gvf flow conditions the observations and predictions were performed when the gvf was reached the variation in terms of discharge in the field is faster than that in the lab therefore the observation and prediction did not fully meet the requirements of gvf conditions and the accuracy rate dropped from 2 62 to 8 9 the dsm method has been tested for its capability to predict the water level theoretically it was also tested in this study to prove its capability in the lab and field for gvf conditions based on the results its performance drops when the flow condition change rapidly while the average prediction error is still within 10 it is expected to increase for a higher flow rate to implement the ecomsnet for a scaled up project such as in the river for flood warning there are admittedly some issues such as the unsteadiness of flow that will need to be addressed different models can be tested in the ecomsnet but the limiting computing power of the edge sensors needs to be considered comparison of performance in terms of edge computing vs centralized computing framework cannot be rigorously performed in this study the scale of the tests in this study was small it cannot quantify the difference between these two frameworks however to the authors knowledge the fundamental algorithms models and sensors between edge computing and centralized computing are the same the biggest differences are the efficiency of information communication and responses the edge computing performs the observations and calculations within the on site sensors but centralized computing needs to transmit the observations to the central server to finish the computation the uncertainty and time consumed in this transmission process could be issues for emergency response in addition observations and calculations are done at the edge of the system this means that real time correction can be done seamlessly to improve the performance while centralized computing cannot accomplish this 5 conclusions 5 1 contributions of this study an innovative decentralized ews named ecomsnet was proposed and tested in an experimental flume and an irrigation channel the ecomsnet was developed according to the eciot concept including a modular microcomputer an rpi to execute edge computing an ultrasonic sensor for water level monitoring and an embedded dsm model for real time water surface profile predictions and corrections the ecomsnet can transmit observations among sensors with multiple wireless communication techniques such as wi fi 4g lora and nb iot in comparison with traditional ewss the edge computing based structure can shorten the transmission times of the observations and decrease the system costs as a result the efficiency of decision making for emergency response may also be improved the overall uncertainty in the measurement of water levels with an ultrasonic sensor is also studied in this paper a straightforward quality control technique was implemented to decrease the measurement uncertainty of the water level since the microcomputer based sensor has limited computing power a quick and efficient dsm model was applied to produce water level forecasts in gvf conditions finally three data assimilation techniques were developed to seamlessly improve the prediction results while limiting computational resource consumption the results from the lab and field experiments had an error margin of no more than 10 the experimental results confirm that the ecomsnet has the advantages of high efficiency accurate prediction low cost low energy and simple installation the preliminary analysis shows that the ecomsnet shows potential for further flood warning applications such as large scale inundation monitoring network in urban areas and in irrigation and drainage systems 5 2 limitations and future work the results show issues that appear to need further investigation before scaled up applications first is the reliable data exchange among sensors the lora technique was used in the field experiment because of its energy efficiency advantages however the loss rate in terms of information exchange among sensors was high the forecasts were delayed because of data loss other approaches such as nb iot and 4g are considered as alternatives the nb iot has advantages of low communication cost and energy consumption however it is not applicable for a large amount of frequently changing information the 4g is stable and capable of high speed transmission of large volumes of data the economic cost of 4g is an issue the second issue to be addressed is a stable way to continuously monitor the flood depth in the laboratory experiment the ecomsnet was shown to be capable of capturing the variability in water depth however the ambient environment such as temperature fluctuation affects the speed of the ultrasonic sensor s sound waves air currents also disturb the path of the ultrasonic wave this condition creates errors in readings and therefore generates inaccurate water depth predictions this issue can be improved by switching to a better quality of ultrasonic sensor with temperature compensation function different sensors such as lidar microwave radar infrared sensors should be further investigated for its performance in comparison with the ultrasonic sensor the measuring distance was limited to 4 m because of the limitation of the ultrasonic sensor for a further application such as for river monitoring the mounting height of the sensor should be considered and other types of sensor can improve this deficiency the third issue is the power supply if the ecomsnet is going implemented in s scaled up project a stable and sustainable power source is necessary in this study a 20 000 mah portable power charger was used it meets the need in this study for 72 h non stop monitoring task it will need more power if more devices or longer monitoring time are needed besides stability of voltage is important to ensure accuracy of ultrasonic measurements fourth the dsm model used in this study was developed to simulate gvf conditions if the current model is used in rvf conditions or unsteady flow conditions the performance needs to be further investigated modifications to the current dsm model are expected or different models should be developed to meet the need of specified flow conditions finally the ecomsnet was evaluated for a given discharge and known cross sections in practice those are usually unknown for the users sensors in the ecomsnet must have the capability to automatically estimate the flow discharge so the system can fully operate on site without any assumptions in addition further studies to evaluate the ecomsnet in terms of robustness lifetime and environmental conditions are needed to obtain better prediction results in practical applications such as natural rivers and open channels declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to acknowledge that the support of the laboratory of sediment transport department of civil engineering national chiao tung university taiwan for the experimental flume appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104771 funding this work was supported by the higher education sprout project of national chiao tung university and the ministry of education moe taiwan this work is also supported by the ministry of science and technology most of taiwan under research grant most 108 2625 m 009 004 role of the funding source this funding source had no role in the design of this study and will not have any role during its execution analyses interpretation of the data or decision to submit results 
26007,water level monitoring and forecasting are essential tasks in flood emergency response this study proposes an edge computing based sensory network ecomsnet an innovative decentralized early warning system ews for water level monitoring and prediction a sensor embedded algorithm integrates the direct step method dsm with a microgenetic algorithm mga this algorithm predicts the water surface profile and corrects it once water level observations are available it also meets efficiency requirements to accommodate sensor computation limitations the errors in the predicted water surface profiles in channels with gradually varied flows are 5 in a laboratory flume experiment and below 10 in a field experiment the ecomsnet is an achievement of edge computing based internet of things it shows potential to increase emergency response efficiency however the system requires further refinement and testing if it is to adequately address rapidly varied unsteady flow in a scaled up implementation keywords early warning system edge computing iot microgenetic algorithm water level prediction software and or data availability software name mudspv4 developer tsun hua yang year first official release 2020 hardware requirements pc system requirements windows linux mac program language python program size 1 mb availability https github com joshyang1977 ecomsnet license gpl 3 0 1 introduction flooding is a major global problem that affects millions of people annually acosta coll et al 2018 it ranks first in terms of occurrence among natural disasters and is estimated to have affected approximately 107 million people from 2008 to 2018 cred 2018 structural and nonstructural measures have been devised and developed to prevent or mitigate losses of life and property among all the measures early warning systems ewss have shown their capability to reduce the possibility of personal injury loss of life and damage to property and the environment acosta coll et al 2018 angermann et al 2010 ewss for rivers can provide forecasts of the magnitude of peak flow and its occurrence time with an extended lead time decision makers can then take necessary measures to mitigate the flood damage accordingly observation devices in the ews serve to monitor onsite and real time information and the observed records provide inputs to flood models to produce forecasts for example water level gauges measure time varied river stages and transmit the data to a remote server the flood model in the server uses the received data as initial or boundary conditions to simulate future river stages this is the most common framework for ewss termed the centralized ews framework e g chang et al 2018 gourley and clark 2018 the effectiveness of the centralized framework relies on tight integration of geographically distributed system components such as sensors and central computing sources communication structure failure is always a concern especially during extreme events angermann et al 2010 in addition the decision making time for emergency response comprises data monitoring transmitting processing and decision making the shorter the processing time the higher the efficiency is of the emergency response for example du et al 2019 proposed a sensor web to provide flood event process detection and instant service however previous studies are still based on the centralized framework this approach cannot avoid a system delay if any failure occurs during the data transmission process the efficiency cannot be improved because the processes of data monitoring transmission processing and decision making in a centralized ews are all substantially time consuming when data are mainly produced at the edges of a network i e river stages and rainfall it is more efficient to process the data at these locations than at other locations in other words the sensor not only collects but also processes the observed data to produce informative results consequently a decentralized framework named edge computing is adopted in this study to allow computation to be performed at the sensor the definition of edge computing is a distributed computing framework that executes computation and data storage tasks close to the locations where the data are needed emergency response activities usually take place at the location where a disaster occurs edge computing has the potential to address the concerns of response time requirements increasingly more services are removed from the cloud centralized to the edge decentralized of the network because data processing at the edge can ensure shorter response time and better reliability shi et al 2016 thanks to the rapid development of the techniques of the internet of things iot and associated hardware the technique of edge computing has pushed the horizon of a new emergency response paradigm through integration with iot technology such as on site microcomputer based sensors the edge computing based iot or eciot omoniwa et al 2019 can collect and process data and then generate decision supporting information simultaneously without delay few studies in the development of ewss have considered the eciot talat et al 2019 presented a decentralized flood detection system using image data this system used on site embedded cameras in sensors to capture and process images these sensors generate warning alarms based on their received information however it is not clear in that paper how the warning alarm is generated vunabandi et al 2015 proposed a framework that covers both arduino based technology and other inexpensive wireless sensor network components to detect floods and send an alert to the local community in rural rwanda the sensors in that study continuously transmitted the data to base stations for information processing the base station generated an alarm based on its decision to notify the residents about the flood the performance was not characterized in that study perumal et al 2015 deployed a water level sensor locally to detect the desired parameter if the water level reached the parameter threshold the signal was fed in real time to a social network such as twitter a cloud server was configured as a data repository mhaisen et al 2018 proposed a self powered water monitoring system that included a water monitoring device centralized cloud receiving and processing data and a mobile application to display results studies such as perumal et al 2015 and mhaisen et al 2018 relied on a centralized cloud system to process and analyze data therefore their sensors involved the data consumer instead of the data producer which might incur longer response times and add a heavy load to networks this study implements the concept of eciot and develops an edge computing based sensory network named ecomsnet for real time water surface profile predictions starting with a simple algorithm for data quality control qc of observations the ecomsnet applies the direct step method dsm to rapidly predict water levels given a few parameters such as the discharge channel bottom slope and roughness coefficient the sensory network monitors the variation in water stages with ultrasonic sensors and then provides water level predictions with the dsm even with limited computing power the ecomsnet can still rapidly provide predictions by itself it is not necessary to transmit the observations from local sites to central computing devices or cloud services to provide further processed data to improve the accuracy of the predictions three real time correction techniques were developed and embedded in the ecomsnet the sensors update the predictions in real time by using the three correction techniques once the observations become available further decision supporting information such as flood warning or evacuation messages can also be directly issued to other iot devices from the ecomsnet its innovative decentralized ews framework avoids accidents arising from communication failure between local sensors and central servers and decreases the data processing time therefore increasing the effectiveness of emergency response this study tested the ecomsnet in the lab and field experiments with gradually varied flows gvf and proved its capability to predict the water levels however further testing and refinement for unsteady flow conditions are necessary if the ecomsnet is going to be implemented in a scaled up river flood warning system as the first attempt of the application of eciot in the ews this study lays out a system structure preliminary results and findings based on the structure can inform further applications the remainder of this paper is organized as follows section 2 details the ecomsnet system structure and its major components the ecomsnet concept was verified with a laboratory flume and a field experiment details regarding the experiments are discussed in section 3 finally sections 4 and 5 present the results discussion and conclusions 2 ecomsnet system structure the purpose of this study is to develop a sensory network system that can provide real time water level observations predictions and correction based on the eciot concept the system structure is illustrated in fig 1 the system named the ecomsnet is an embedded system that consists of raspberry pi rpi based monitoring devices with dedicated functions the monitoring devices use ultrasonic sensors to measure the water levels and the recorded data are preprocessed for qc before further calculation the processed data are exchanged through wireless techniques e g wi fi 4g the long range spectrum lora and nb iot between upstream and downstream sensors thereafter the water level predictions and the corrections are performed using developed algorithms the abovementioned system architecture is illustrated in fig 1 the system consists of four components 1 water monitoring sensors an ultrasonic sensor mounted on the rpi board can measure the water level at a frequency of 100 hz 2 a qc algorithm is developed to filter the measurement noise and ensure the quality of the input data used by the prediction model 3 a water level prediction algorithm a scheme is chosen based on the dsm and designed to provide predictions to meet the needs of rapid prediction efficiency 4 a correction algorithm is developed to improve the performance of predictions using real time observations the details of each component are described in the following subsections 2 1 raspberry pi rpi based sensor low cost open source and low energy consumption sensors are always of interest for environmental monitoring kwok 2017 prince et al 2019 an automatic and wireless water level sensor usually incurs a high purchase price us 1000 5000 per unit therefore financial considerations become an obstacle to a dense monitoring network for the deployment of any ews this study therefore considered the rpi as a platform embedded with an ultrasonic sensor to process and analyze the monitored data at a local site the detailed sensor setup is shown in fig 2 the rpi is a reliable low cost approx us 35 microcomputer which was developed in 2006 by the university of cambridge s computer department and produced by the raspberry pi foundation since 2012 as a tool to encourage students to learn programming languages prince et al 2019 it allows users to use the python programming language in the unix debian based raspian operating system a free operating system optimized for rpi hardware there are a few models of rpi this study applied the rpi 3 model b which was released in february 2018 with a 1 4 ghz 64 bit quad core processor onboard 802 11ac wi fi 100 mbits sec bluetooth and usb boot capabilities the rpi 3 model b is powered by a micro usb cable and uses an sd card for booting and long term data storage two usb ports are provided for additional usb components in this study the rpi connects to the internet using onboard wi fi if available or a wireless 4g usb internet adapter if necessary lora a spread spectrum modulation technique for data transmission was also tested in this study during a field experiment it was found that lora is well suited for the data i e water level transmitted in this study the details are not discussed here finally the ultrasonic sensor is connected and powered through the general purpose input output gpio pins included in the rpi design an ultrasonic sensor mounted on the rpi measures the distance to the water surface the sensor emits sound waves at a specific frequency and detects the sound waves that bounce back the distance can be calibrated by the elapsed time between the generated and returning sound waves environment conditions such as temperature and ambient noise impact the performance of ultrasonic sensors the sensor used in this study is the high performance ultrasonic distance sensor ks103 with an i2c bus from shenzhen dauxi technologies co ltd 2014 it works in the range of 1 cm 800 cm with functions of temperature compensation and noise cancelation the sensor detects frequencies of up to 500 hz sensor calibration was performed in static water before the laboratory and field experiments table 1 shows the calibrated results all the coefficients of determination r2 for the three sensors were close to 1 0 the ultrasonic observations were almost the same as the ground truth observations the ultrasonic sensor observations were calibrated by using the trendline equations in table 1 2 2 depth difference data quality control qc scheme monitored data serves as major input to the ecomsnet fig 1 the data quality dq and accuracy are critical requirements for a high prediction performance if observed data are of poor quality the decision making process is likely to be unsound the observation accuracy is modified using the trendline equations in the previous session data outliers are one of major manifestations of data uncertainty karkouch et al 2016 in this study the data sampling frequency of the ultrasonic sensor can be high because it takes less than a second to measure the distance between the sensor and the water surface the surrounding environment where the sensor is deployed which may involve abnormal sound wave reflection because of sudden moving objects e g birds or irregular vibrations e g truck vibrations has a significant impact on the measuring results considering the data life cycle in the iot qin et al 2014 the sampling interval δ t in this study as calculated as eq 1 is highly related to the distance d and surrounding environment an interval of 2 s is chosen and there are at most 300 observations in an observation period of 10 min if there is no interference 1 δ t 2 d v s where δ t is the sampling interval and d is the distance between the ultrasonic ssor and the water surface vs 343 m s is the speed of sound at 20 c since outliers can be considered events with extremely small probabilities of occurrence they are also seen as points in a data set that are highly unlikely to occur given a model othe data otey et al 2006 therefore a simple real time data qc scheme is applied in the study before serving as input to the prediction scheme fig 3 shows an example of data observation from the ultrasonic sensor data exchange is performed between the upstream and downstream sensors the depth upstream is assumed to be lower than the depth downstream because of the positive slope and gvf conditions in this study the observations within the red square in fig 3 are those that violate this assumption a therefore are excluded from the inputs to the following prediction process by implementing this rule the dq is assured to meet the assumptions of this study prediction can then proceed and the predicted results are considered to be reasonable 2 3 direct step method dsm water surface profile calculations are necessary for disaster response for a specified discharge in the eciot based ews an efficient and accurate flood prediction scheme is necessary to meet the requirement of a high data sampling frequency because of the limited computing power of the microcomputer an efficient and robust water surface profile prediction scheme is vital the dsm proposed by chow 1959 is characterized by dividing the channel into short reaches that are computationally analyzed step by step from one end of the reach to the other this simple step method is applicable to prismatic channels and gvf given the channel cross sections channel roughness n and rate of discharge q the flow depth and velocity can be rapidly computed the dsm method has been tested for its capability to predict the water level theoretically it was also used in this study to prove its capability for the computation of gvf conditions in the lab and field however if the application is in rapidly varied unsteady flow rvf conditions the dsm may need to be modified to account for flow unsteadiness behaviors the rpi sensors in the ecomsnet provide high frequency observations of water depths the dsm which is embedded in the rpi sensor can rapidly compute unknown depths at a section adjacent to the observations the dsm is derived from the energy equation below and illustrated in fig 4 for a short reach of length δ x x i 1 x i it is assumed that the total head energy at the two end sections i and i 1 is equal 2 s 0 δ x y i α i v i 2 2 g y i 1 α i 1 v i 1 2 2 g h f where s 0 z i z i 1 δ x is the bottom slope and y i i 1 and v i i 1 are respectively the depths of flow and mean velocities at sections i and i 1 the total head loss h f due to friction is assumed as follows 3 h f 1 2 s f x i 1 x i wre s f is the friction slope at the short reach δ x shown in fig 4 the u s army corps of engineers united states army corps of engineers 1982 proposed three approaches tdetermine the friction slope average friction slope arithmetic mean friction slope and harmonic mean friction slope chaudhry 2007 suggested that the average slope is proven to yield satisfacry results and is the simplest of the above three methods therefore the average friction slope s f 1 2 s f i s f i 1 is used in this study and δ x is solved as shown below 4 δ x e i 1 e i s 0 s f w e y α v 2 2 g is the specific energy and the energy coefficients a aumed to satisfy α i 1 α i α the friction slope s f is obtained from manning s equation as follows 5 v 1 n r 2 3 s f 1 2 ere n is manning s roughness coefficient r a p is the hydraulic radius p is the wetted perimeter and v q a where q is known eq 4 is used to determine δ x when the flow depths at sections i and i 1 are known for a specified discharge q in this study the dsm calculation starts from either the upstream or the downstream end where the water depths are observed the water depth y i 1 athe other end of the reach is assumed to be y i δ h and δ h is assumed to be 0 01 cm in this study to obtain the smallest δ x the smallest δ x can meet the need for the dsm assumption the flow depth at any location can be estimated by iterating the abovementioned steps and accumulating δ x to the specific location the obtained location of the predetermined water depth may not be exactly the same as that at the desired location this condition is still acceptable since the difference in the water depth is only 0 01 cm 2 4 real time correction techniques the water surface profile was obtained from the method proposed in the previous section however in the ews the input data uncertainty and the flood model structure and its associated parameter uncertainties decrease the forecast performance liu and gupta 2007 incorporating observations into the forecasting process can efficiently reduce the uncertainties and increase the prediction accuracy shu et al 2005 a few studies have shown the effectiveness of postprocessing and statistical correction techniques to reduce forecast errors e g pagano et al 2011 and shen et al 2015 this study developed three postprocessing methods to obtain the best performance 1 arithmetic average 2 constant error assumption between observations and predictions and 3 application of a microgenetic algorithm mga for an optimized channel roughness n the details of these real time correction techniques are discussed below 2 4 1 upstream downstream adjustment method uda a simple arithmetic adjustment method of predictions is proposed eq 4 shows that the distance δ x is obtained from water depths from y i to y i 1 upstream to downstream direction the opposite calculation can also be performed to obtain δ x from water depths y i 1 to y i downstream to upstream direction the only difference is the sign of δ x the assumption is made that a constant prediction error ε in water depth predictions exists if both y i and y i 1 are known at locations x i 1 and x i respectively the water surface profile between these two observations e g the middle point x i 1 2 can be obtained as follows 6 y i 1 2 i f y i ε e y i 1 2 i and y i 1 2 i 1 are the water depth predictions from upstream i or dnstream i 1 and ε is the prediction error the subscript i 1 2 means that the predicted location is at location x i 1 2 and the superscripts i and i 1 mean that the prediction is based on an observation at either location x i 1 or location x i finally y i 1 2 is the arithmetic average for prediction at x i 1 2 between y i 1 2 i and y i 1 2 i 1 and serves as the corrected final result chaudhry 2007 mentioned that it is not necessary to compute the water surface profile in the upstream or downstream direction according to its flow condition super or subcritical flow while using the dsm the computations do not become unstable or yield incorrect results ithe abovementioned convention is not followed chaudhry 2007 noted that if the flow depth is known at a control section there is no reason why computation in either the upstream or downstream direction would bdifferent this is because the system either numerically solves a differential equation for the specified itial condition or solves a nonlinear algebraic equation therefore this study developed the uda correction approach which is based on the averaged results from the upstream and downstream sides 2 4 2 constant error assumption method cem the dsm predicts the water depth y i 1 i at location x i 1 based on the observed water depth y i o at location x i fig 4 the distance between x i and x i 1 is divided into several short reaches and the computation is carried step by step from one end of the reach to the other the observation y i 1 o is also available when the predicted y i 1 i is provided because sensors are installed at x i and x i 1 this study assumes that there is a constant error or so called residual r between the observation and the prediction at the same location for simplicity many studies have assumed constant error in the data between observation and prediction especially for linear functions tellinghuisen 2001 the plot of predictions y i 1 i vs observations y i 1 o at location x i 1 in shown in fig 5 the residual r i 1 is shown as follows 7 r i 1 y i 1 o y i 1 i a constant residual r is assumed along the stream so the residual r i 1 2 at location x i 1 2 can be obtained as follows 8 r i 1 2 r i 1 finally the corrected water depth at location x i 1 2 can be obtained by 9 y i 1 2 c o r r e c t e d y i 1 2 i r i 1 2 2 4 3 optimized manning s roughness n using a microgenetic algorithm mga the previous two correction techniques are based on manning s roughness n in eq 5 being known if the material of the experimental flume or on site channel is known manning s roughness n can be approximately estimated chow 1959 however this parameter cannot be easily estimated in practice in the case of concrete or grass lined channels none of their roughness coefficients are the same or known therefore a self identified algorithm is developed to find the optimized n according to best available observations this study applied an mga to find the optimized manning s roughness n for the best water depth predictions the mga has proven its capability to identify this parameter in river flow modeling yang et al 2014 the details of the mga were presented previously krishnakumar 1990 yang and tsai 2019 and will not be discussed here this study applied the assumptions of the mga described by krishnakumar 1990 which include a population size of five a crossover rate of one and a mutation rate of zero the following steps shown in fig 6 are used to obtain manning s roughness n during real time operations step 1 generate a group of five chromosomes at random the binary string is used to code n for each station into a chromosome in this application there are 8 genes in a chromosome step 2 calculate the predicted water depth when there is an observation available at every sampling interval δt to obtain the optimized manning s roughness n the minimum fitness value f is the objective function which is defined as follows 10 f y i t o y i t where y i t and y i t o represent the predicted and measured depths at a given location x i respectively and t represents the moment when the observation becomes available step 3 keep the fittest chromosome for the next generation carry out tournament selection for the remaining four chromosomes with a crossover rate of one and a mutation rate of zero until the new generation is achieved step 4 calculate the fitness value from eq 10 thereafter nominal convergence which means that all the individuals in the population have either identical or very similar genotypes is verified in this study nominal convergence occurs when more than 7 out of 8 genes in each chromosome in the entire population share the same value when nominal convergence is achieved the calculation is stopped and the best chromosome as well as the optimized manning s roughness n is obtained for this generation finally steps 1 to 4 are repeated 50 times to avoid obtaining local optimum solutions 3 details of laboratory and field experiments a laboratory flume and a field open channel were used to test the performance of the ecomsnet the laboratory experiment was conducted at the laboratory of sediment transport department of civil engineering national chiao tung university taiwan and the field experiment location was selected at an irrigation channel in miaoli county taiwan the experimental details are discussed in the following subsections and shown in table 2 3 1 laboratory experiment the experimental flume was a closed loop open surface recirculating rectangular flume with dimensions of 10 m in length 0 4 m in width and 0 5 m in height the bed slope of the flume was 1 4 a schematic sketch of the experimental flume is shown in fig 7 the water was pumped by a water pump and the flow rate was controlled by a valve the experimental section was 5 m long and was 4 m downstream from the entrance to establish a fully developed flow regime a honeycomb structure was set after the entrance to stabilize the inflow condition this study also adjusted the water depth using a tail gate at the outlet even though the flow discharge remained the same two rpi based ultrasonic sensors were installed upstream and downstream the sensors measured the water depths and computed the water surface profile automatically observed information and calculated results were exchanged through a wi fi network a third sensor was installed at the middle location for performance verification 3 2 field experiment a field experiment was conducted in an open channel located in miaoli county taiwan fig 8 it is an irrigation channel red line and water is diverted from the east river blue line next to the channel fig 8 a the length for the test reach between the upstream and downstream measuring points is 30 m long and the channel is 50 cm wide fig 8 b the channel slope is assumed to be 0 005 or 0 286 the flow in the channel is not purposely designed this study applied the flowtracker2 ft2 handheld acoustic doppler velocimeter by sontek xylem inc shown in fig 8 c to measure the flow discharge in advance the measured discharge was used as a known variable to the dsm in the ecomsnet to evaluate the performance this study also used the ft2 to measure water depths at locations ref 1 and ref 2 in fig 8 a in comparison with the predictions the locations ref 1 and ref 2 are 14 5 and 22 1 m from the upstream location respectively since it is an irrigation channel the water quality is in good condition and it is assumed that the water quality has no impact on the measurement in this study in comparison with river flow conditions the flow in the channel is still a low velocity quasi steady flow condition 4 results and discussion this study develops the ecomsnet to provide almost real time water depth forecasts the ecomsnet first processed the observations with the depth difference qc approach mentioned in section 2 2 and the outliers due to inaccurate signal reflection were excluded in the prediction process the backwater effect a typical flow condition for subcritical flow in alluvial rivers and open channels brandimarte and woldeyes 2013 was implemented by setting up a tail gate at the end of the experimental flume fig 9 this configuration was used to evaluate the performance of the ecomsnet finally the predicted results were corrected by three correction algorithms all abovementioned steps were performed in the sensors of the ecomsnet which is an implementation of the eciot it is also important to investigate the mean of the observations and its impact on the forecast results therefore the water depth predictions from the mean observations taken as the input in comparison with the mean water depth prediction by averaging the forecasts of all predictions at different times are discussed in this session finally an irrigation channel in the field was chosen to test the performance of the ecomsnet in practice 4 1 sensitivity analysis to different sizes of δh there are two parameters manning s n and δh used in the dsm the first parameter manning s n can be identified based on the condition of the flume this study used an mga technique to search for the optimized manning s n and therefore to obtain the best performance the dsm starts calculating from the upstream or downstream end where the water depth y i is observed and uses the water depth y i 1 at the other end to estimate the distance δx of the reach the difference between depths y i and y i 1 is the second parameter δh whose size also has an impact on the predictions a greater δh results in a larger δx during the step by step process the assumption for the dsm is that δx is small enough that the equations remain valid in other words the bias increases when δx is large therefore sensitivity analysis of different sizes of δh resulting in different δx values must be conducted before further application the lab i scenario in table 2 was selected to test the sensitivity to δh manning s n in this case is assumed to be 0 01 and five δh values are implemented in the dsm model fig 10 shows that different δh values produce different results when δh is smaller than 0 01 cm the results essentially converge therefore δh 0 01 cm was selected for the prediction of water depth in the following sections for efficiency and accuracy 4 2 performance evaluation this study evaluates the prediction performance using the following indexes 11 mean absolute error mae 1 t n 1 t y i t y i t o 12 root mean squared error rmse t 1 t y i t y i t o 2 t where y i t and y i t o respectively indicate the predicted and observed water depth at location i at time t the mae and rmse are the two most common indexes used to describe the overall performance of predictions in different applications e g nguyen et al 2015 and yang and tsai 2019 the rmse is the standard deviation of the prediction errors in other words the rmse is a measure of how spread out these errors are the mae was suggested to replace the rmse since the former is a more natural definition of an average error and is unambiguous willmott and matsuura 2005 4 2 1 comparisons of water depth predictions in the laboratory the performance of the ecomsnet was evaluated using the three laboratory experiment scenarios listed in table 2 the flow conditions and slopes were limited by the design of the experimental flume these discharges were 0 0087 and 0 0014 m3 s lab ii and iii have the same discharge but the water depth can be altered by adjusting the angle of the tail gate the ecomsnet was activated and produced predictions when the gvf condition was achieved the observation duration for the three scenarios was 10 min the water depth production at any given location in the flume was provided using observations confirmed by qc the three real time correction techniques uda cem and mga were then used to improve the predictions to provide modified results finally the performance of the ecomsnet was evaluated by comparing the modified results at the middle of the flume with observations there were 210 178 and 175 available observations after the qc process for lab i ii and iii respectively table 3 lists the performance of the water depth predictions from the three real time correction schemes within the ecomsnet the uda approach yields better mae and rmse results than the other approaches for all three scenarios from a comparison of the lab ii and lab iii results the mae and rmse values decrease when the water depth increases at the same flow rate this is because a higher water depth leads to a slower flow velocity as a result the variation of velocity along the flow direction δ v δ x is small and it is consistent with the assumption of the application of dsm model for gvf conditions the mga does not produce the best results among the three correction techniques its mae and rmse values are all below 1 0 cm which are on the same order of magnitude as the uda s values among all three techniques the cem produces the worst scores in terms of the mae and the rmse fig 11 shows the comparisons of predictions with observations at the middle location in the experimental flume the vertical axis in the figures is defined as follows 13 difference y i t y i t o y i t o 100 the box and whisker plots show the minimum first quartile median third quartile and maximum of the difference in all predictions within the observation period the cross sign corresponds to the average value in terms of the differences of all predictions eq 13 given any approach higher water depths or lower velocities lead to better performance in terms of the difference distribution for example the mga s average differences are 4 50 4 41 and 2 62 for the three scenarios the score decreases when the initial downstream water depth increases the variation in the difference distribution i e distance between the first and third quartiles for the same discharge but different water depths i e lab ii and lab iii is also smaller this finding indicates that the performance of the predictions is stable and converges when the water depth is higher the same results are found with the cem and uda a deeper water depth produces a more stable water surface condition resulting in better observations and prediction performance for the comparison between lab ii and lab iii a deeper water depth indicates a slower water velocity at the same discharge this leads to a more gvf condition that allows the dsm model to show better performance regardless of the comparison of the performance the ecomsnet with the uda and mga real time correction schemes can provide less than a 5 difference for all three flow conditions 4 2 2 detailed discussion of the performance of the uda and mga models a further discussion of the difference in the performances of the uda and mga models is warranted the uda approach assumes that manning s roughness n is known so the prediction and correction can be performed in the laboratory experiment all components that come into contact with water are made of stainless steel and glass reinforced plastic manning s roughness n is reasonably assumed to be 0 01 chow 1959 fig 12 compares the performance in terms of the difference eq 13 with various n values using the uda method the results show that the performance of the uda method is very sensitive to the assigned roughness n the performance varies from 2 3 to 9 4 for the lab i scenario 2 7 8 5 for the lab ii scenario and 0 42 1 9 for the lab iii scenario this indicates that the choice of an appropriate manning s n is essential for the prediction performance when the uda approach is applied however in reality the channel roughness is difficult to identify thus the uda approach is not practical de perez et al 2015 concluded that decision makers would hesitate to take any response measure if the performance of ews is not considered acceptable the meaning of unacceptable corresponds to too many false alarms or a large difference between observation and prediction because of the uncertainty of the observations the average performance of the mga for practical applications is discussed here with a total of 10 min for each flow scenario in table 2 an interval of 1 min was required to calculate the average water depths upstream and downstream the ecomsnet then used the average depths as inputs to provide the water depth predictions and corrections at the middle location this is a practical application to evaluate the ews s performance by considering the average observation and using it to provide overall prediction table 4 shows the optimized manning s roughness n values for each interval the consistency of the optimized n values implies the stability of the flow scenario lab iii shows the most consistent n value only the value of 0 03322 during the 5th to 6th minute is different from the others the more stable the flow condition is the better the performance of the ecomsnet is the lab iii flow condition exhibits the most gradual variation among the three scenarios fig 13 shows the average performance avg in comparison with observations at each interval using the associated optimized n above the dark and light gray areas represent the threshold values of 1 5 avg and 1 10 avg respectively the available sample sizes are 210 178 175 for the three flow scenarios as plotted in the figures to evaluate the performance the coverage rate for the vertical axis in the figures is defined as follows 14 coverage rate cr h i t s n where hits is the number of observations within the threshold values and n is the total available observations during the observation time 10 min in this study the cr values for the threshold values of 1 5 and 1 10 are 0 727 and 0 995 respectively this means that the average predictions from the ecomsnet plus 5 uncertainty can provide greater than 72 coverage in comparison with all observations this system can reach 96 coverage if a 10 uncertainty is added to the average predictions from the ecomsnet the 5 and 10 uncertainties are reasonable for engineering applications in practice the worst cr values of the 1 5 avg threshold were found at the 7th and 3rd minutes for the lab i and lab ii scenarios respectively the predicted water levels at the middle location were below most observations however the observations were still below the 1 10 avg threshold 4 2 3 application of the ecomsnet in a field experiment finally the ecomsnet was implemented in an irrigation channel to test the performance the details of the channel were described in section 3 2 the flow conditions were assumed to gradually vary because there was no in outflow during the experimental period for the reach two reference points at different locations were selected to test the prediction performance in comparison with the observations the times of water depth measurements at reference points 1 and 2 were 10 05 and 10 09 a m respectively the measured discharges were 0 056 m3 s and 0 030 m3 s and the measured water depths were 12 5 cm and 14 0 cm the flow conditions were not purposely designed and assumed to be quasi steady within that minute at the observation time above the discharges were used as input and the water depth predictions at associated locations using the mga approach were 11 68 cm and 12 75 cm the corresponding differences eq 13 were 6 5 and 8 9 the differences are slightly higher than in the laboratory results the roughness of the irrigation channel is not as uniform as the flume in the lab the slope of the channel is assumed to be constant in the ecomsnet but may not be constant along the test reach the flow condition in the field is also not as stable as in the lab differences arose over only 4 min but the discharge varied from 0 056 m3 s to 0 030 m3 s all the reasons above may influence the performance note that the differences were all within 10 the results are still consistent with the finding that the observations fell within the 1 10 avg range and are reasonable for engineering applications 4 2 4 discussion of the application of the ecomsnet this study proposed the ecomsnet and tested its performance and availability in the lab and field experiments the overall performance in the field was not as good as that in the laboratory experiment the prediction accuracy rates were 4 50 4 41 and 2 62 eq 13 for the three scenarios in the lab and 6 5 and 8 9 for the two scenarios in the field a few reasons were considered and explained above it is found that the environmental uncertainties such as the bottom slope flow discharge and channel roughness have a considerable impact on the performance for example the bottom slope along the flow direction one of the parameters in the dsm model is uniform in the lab channel but not in the field experiment the ambient conditions such as cross wind speeds and temperature have significant impacts on the performance of ultrasonic sensors jeon et al 2011 finally the dsm model used in this ecomsnet was efficient for gvf flow conditions the observations and predictions were performed when the gvf was reached the variation in terms of discharge in the field is faster than that in the lab therefore the observation and prediction did not fully meet the requirements of gvf conditions and the accuracy rate dropped from 2 62 to 8 9 the dsm method has been tested for its capability to predict the water level theoretically it was also tested in this study to prove its capability in the lab and field for gvf conditions based on the results its performance drops when the flow condition change rapidly while the average prediction error is still within 10 it is expected to increase for a higher flow rate to implement the ecomsnet for a scaled up project such as in the river for flood warning there are admittedly some issues such as the unsteadiness of flow that will need to be addressed different models can be tested in the ecomsnet but the limiting computing power of the edge sensors needs to be considered comparison of performance in terms of edge computing vs centralized computing framework cannot be rigorously performed in this study the scale of the tests in this study was small it cannot quantify the difference between these two frameworks however to the authors knowledge the fundamental algorithms models and sensors between edge computing and centralized computing are the same the biggest differences are the efficiency of information communication and responses the edge computing performs the observations and calculations within the on site sensors but centralized computing needs to transmit the observations to the central server to finish the computation the uncertainty and time consumed in this transmission process could be issues for emergency response in addition observations and calculations are done at the edge of the system this means that real time correction can be done seamlessly to improve the performance while centralized computing cannot accomplish this 5 conclusions 5 1 contributions of this study an innovative decentralized ews named ecomsnet was proposed and tested in an experimental flume and an irrigation channel the ecomsnet was developed according to the eciot concept including a modular microcomputer an rpi to execute edge computing an ultrasonic sensor for water level monitoring and an embedded dsm model for real time water surface profile predictions and corrections the ecomsnet can transmit observations among sensors with multiple wireless communication techniques such as wi fi 4g lora and nb iot in comparison with traditional ewss the edge computing based structure can shorten the transmission times of the observations and decrease the system costs as a result the efficiency of decision making for emergency response may also be improved the overall uncertainty in the measurement of water levels with an ultrasonic sensor is also studied in this paper a straightforward quality control technique was implemented to decrease the measurement uncertainty of the water level since the microcomputer based sensor has limited computing power a quick and efficient dsm model was applied to produce water level forecasts in gvf conditions finally three data assimilation techniques were developed to seamlessly improve the prediction results while limiting computational resource consumption the results from the lab and field experiments had an error margin of no more than 10 the experimental results confirm that the ecomsnet has the advantages of high efficiency accurate prediction low cost low energy and simple installation the preliminary analysis shows that the ecomsnet shows potential for further flood warning applications such as large scale inundation monitoring network in urban areas and in irrigation and drainage systems 5 2 limitations and future work the results show issues that appear to need further investigation before scaled up applications first is the reliable data exchange among sensors the lora technique was used in the field experiment because of its energy efficiency advantages however the loss rate in terms of information exchange among sensors was high the forecasts were delayed because of data loss other approaches such as nb iot and 4g are considered as alternatives the nb iot has advantages of low communication cost and energy consumption however it is not applicable for a large amount of frequently changing information the 4g is stable and capable of high speed transmission of large volumes of data the economic cost of 4g is an issue the second issue to be addressed is a stable way to continuously monitor the flood depth in the laboratory experiment the ecomsnet was shown to be capable of capturing the variability in water depth however the ambient environment such as temperature fluctuation affects the speed of the ultrasonic sensor s sound waves air currents also disturb the path of the ultrasonic wave this condition creates errors in readings and therefore generates inaccurate water depth predictions this issue can be improved by switching to a better quality of ultrasonic sensor with temperature compensation function different sensors such as lidar microwave radar infrared sensors should be further investigated for its performance in comparison with the ultrasonic sensor the measuring distance was limited to 4 m because of the limitation of the ultrasonic sensor for a further application such as for river monitoring the mounting height of the sensor should be considered and other types of sensor can improve this deficiency the third issue is the power supply if the ecomsnet is going implemented in s scaled up project a stable and sustainable power source is necessary in this study a 20 000 mah portable power charger was used it meets the need in this study for 72 h non stop monitoring task it will need more power if more devices or longer monitoring time are needed besides stability of voltage is important to ensure accuracy of ultrasonic measurements fourth the dsm model used in this study was developed to simulate gvf conditions if the current model is used in rvf conditions or unsteady flow conditions the performance needs to be further investigated modifications to the current dsm model are expected or different models should be developed to meet the need of specified flow conditions finally the ecomsnet was evaluated for a given discharge and known cross sections in practice those are usually unknown for the users sensors in the ecomsnet must have the capability to automatically estimate the flow discharge so the system can fully operate on site without any assumptions in addition further studies to evaluate the ecomsnet in terms of robustness lifetime and environmental conditions are needed to obtain better prediction results in practical applications such as natural rivers and open channels declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to acknowledge that the support of the laboratory of sediment transport department of civil engineering national chiao tung university taiwan for the experimental flume appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104771 funding this work was supported by the higher education sprout project of national chiao tung university and the ministry of education moe taiwan this work is also supported by the ministry of science and technology most of taiwan under research grant most 108 2625 m 009 004 role of the funding source this funding source had no role in the design of this study and will not have any role during its execution analyses interpretation of the data or decision to submit results 
26008,this research introduces a two level integration of climate economy modelling and portfolio analysis to simulate technological subsidisation with implications for multiple sustainable development goals sdgs across socioeconomic trajectories and considering different levels of uncertainties we use integrated assessment modelling outputs relevant for progress across three sdgs namely air pollution related mortality sdg3 access to clean energy sdg7 and greenhouse gas emissions sdg13 calculated with the global change assessment model gcam for different subsidy levels for six sustainable technologies across three shared socioeconomic pathways ssps feeding them into a portfolio analysis model optimal portfolios that are robust in the individual socioeconomic scenarios as well as across the socioeconomic scenarios are identified by means of an ssp robustness score a second link between the two models is established by feeding portfolio analysis results back into gcam application in a case study for eastern africa confirms that most ssp robust portfolios show smaller output ranges among scenarios keywords integrated assessment modeling global change assessment model shared socioeconomic pathways sustainable development goals stochastic uncertainty scenario analysis software and data availability software name global change assessment model gcam developers joint global change research institute jgcri http www globalchange umd edu contact address 5825 university research court suite 3500 college park md 20740 united states year first available 1982 required software java program language c xml availability download available at https github com jgcri gcam core releases tag gcam v4 4 1 cost free software name aucmecon 2 developers mavrotas g florios k 2013 an improved version of the augmented ε constraint method augmecon2 for finding the exact pareto set in multi objective integer programming problems applied mathematics and computation 219 18 9652 9669 contact address laboratory of industrial and energy economics school of chemical engineering national technical university of athens zographou campus athens 15780 greece e mail mavrotas chemeng ntua gr year first available 2013 required software gam program language gams language availability download available at https sites google com site kflorios augmecon2 cost free 1 introduction integrated assessment models iams are a core element of the scientific processes that comprise the best available science peters 2016 when it comes to analysing energy system transitions within the context of climate change mitigation and sustainable socioeconomic development nikas et al 2019 pietzcker et al 2017 schwanitz 2013 janssen et al 2009 these tools are applied to analyse adaptive energy environment economy systems in the global scientific and policy arena ewert et al 2015 gidden et al 2018 estrada et al 2019 advance scientific understanding of the potential to combat climate change and underlying dynamics towards robust and sustainable development huppmann et al 2019 warren et al 2019 and evaluate the various technologies initiatives and policy options that ensure clean and sustainable energy transition wyrwa 2015 shi et al 2017 liu et al 2019 in particular these models constitute a well established scientific tool aimed at understanding feedbacks and influences between different system components including the social economic and ecological implications of different natural or anthropogenic factors especially with regard to interlinkages between the human and the natural system calvin and bond lamberty 2018 gidden et al 2018 the core advantage of these complex models is that they provide an integrated system perspective to study the dauntingly complex interactions between energy economy land use water and climate systems scott et al 1999 weyant 2017 through such an integration iams combine multiple and diverse components across their social organisational and conceptual boundaries to provide a comprehensive analysis of the problem collins et al 2015 jakeman and letcher 2003 for this purpose different modules or components are coupled with one other usually including but not limited to the economy the environment the energy system and the climate feedbacks or economic impacts of changes among them giupponi et al 2013 modelling results are widely used to inter alia directly influence decisions and taken stock of towards advising policymakers as is the case of the assessment reports of the intergovernmental panel on climate change ipcc recent examples of publications where iams contribute to providing background information on possible energy and climate futures and to scientifically underpinning international climate policy negotiations are the ipcc s special report on the impacts of global warming of 1 5 c above pre industrial levels ipcc 2018 the world energy outlook 2018 international energy agency 2018 or the european union eu energy roadmap 2050 ec com 2016 in this respect decision makers are based on iam driven policy prescriptions to develop policies that contribute to managing environmental resources and assets in a way that delivers acceptable environmental and socioeconomic outcomes more details on iams can be found in krey 2014 weyant 2017 and nikas et al 2019 which review energy economic models including or focusing on iams and provide a categorisation of them based on parameters like their degree of integration and mathematical underpinnings as well as highlight the challenges associated with these modelling frameworks given their strengths and weaknesses hamilton et al 2015 however analyses based exclusively on these formalised frameworks alone are usually not sufficient to address the broad spectrum of challenges associated with climate change and policy assessment doukas et al 2018 and recent advances and paradigms call for and or apply complementing them with other methods and tools turnheim et al 2015 geels et al 2016 in this direction iams have recently been coupled with a diversity of tools towards enhancing scientific processes and leading to more pragmatic policy prescriptions including but not limited to life cycle analyses arvesen et al 2018 fuzzy cognitive mapping nikas et al 2020 antosiewicz et al 2020 and multiple criteria decision aid frameworks baležentis and streimikiene 2017 shmelev and van den bergh 2016 one of these tools which has been established in the climate policy domain doukas and nikas 2020 in diverse applications allan et al 2011 bistline 2016 odeh et al 2018 zhang et al 2018 and long been coupled with iams e g baker and solak 2011 pugh et al 2011 forouli et al 2019a forouli et al 2019b del granado et al 2019 is portfolio theory in fact energy planning decisions are often portfolio building problems in which the task is to find a viable mix of actions to meet the overall objectives targets and constraints therefore today as many energy related decisions fall into this category portfolio decision analysis methods and tools are seen as the next step in energy decision support marinoni et al 2011 vilkkumaa et al 2014 through such tools decision makers are able to consider a set of actions and create policy incorporating relevant concerns and interests in a balanced way typically decision makers have to consider the overall performance of a portfolio across many relevant dimensions or criteria such as techno economic socio political and environmental impacts huang and wu 2008 muñoz et al 2009 portfolio analysis pa addresses the need to consider multiple objectives and constraints and further contributes to identifying promising candidate actions and examining interactions among them portfolio decision models were first applied on risk diversification in financial investments and have their roots to the work of markowitz 1952 markowitz proposed a mean variance model to support investment decisions in light of uncertainty associated with the future returns of financial assets today there is a range of portfolio modelling approaches which offer modelling and optimisation support to find the most preferred portfolio of actions and which are applicable to energy and environmental modelling lahtinen et al 2017 provide a detailed comparative description of portfolio modelling approaches among the most common ones are the value cost or benefit cost approach where actions are prioritised according to the value cost ratio until a budget cap is reached hajkowicz et al 2008 marinoni et al 2011 the disadvantage of this method is that in case of synergies or interactions between the actions optimality is not guaranteed as interactions play a critical role in energy and or climate economy problems this approach is often not sufficient an approach that incorporates the risk parameter into the evaluation is the modern portfolio theory approach where the optimal resource allocation for each risk level is identified crowe and parker 2008 paydar and qureshi 2012 from the above we understand that pa and iams are widely used in policy analysis and evaluation of pathways for the transformation of the human and earth systems the interconnectedness of our world is broadly acknowledged to require integrated rather than piecemeal approaches to resolving complex environmental issues particularly in view of the increasing speed and pervasiveness of connections associated with globalisation with the interaction of sustainable development goals sdgs with climate change and action gaining increasing prominence at the interface of science and policy developing computational tools and models that operate across academic disciplines and methodologies becomes ever more important in this paper we use a multi objective optimisation approach where the result is a set of non dominated portfolios through this approach interactions among the set of actions and portfolio constraints can be considered the goal is to generate non dominated combinations of actions in terms of comparing between the evaluation criteria as required by portfolio modelling and in order to generate the non dominated portfolios all candidate actions are simultaneously considered and optimised in the same portfolio optimisation model the goal is to identify optimal portfolios of actions or a set of non dominated portfolios that best meet multiple objectives while satisfying the problem constraints decision makers can then select a portfolio among the non dominated ones tailored to their needs and preferences to understand the term of portfolio dominance a portfolio is said to be dominated if there exists another portfolio of actions that performs better in some attribute criterion and at least equally good in all other attributes the model based portfolio generation process proposed here supports the consideration of multiple objectives and constraints and interactions among the actions while acknowledging the vital role of uncertainty in particular the first goal of this paper is to create an efficient scientific workflow and a two way technical integration of integrated assessment modelling and portfolio optimisation outcomes to this end at first we simulate future policy under policy relevant socioeconomic scenarios such as the shared socioeconomic pathways ssps o neill et al 2014 the global change assessment model gcam is used as the implementation integrated assessment model 1 1 τhe updated gcam documentation website includes a specific section describing the ssp implementation throughout the model https github com jgcri gcam doc blob gh pages ssp md the outputs from each policy scenario are translated into progress parameters relevant to three sdgs of the united nations 2030 agenda for sustainable development and fed into a pa model these parameters include air pollution related mortality sdg3 access to clean energy sdg7 and greenhouse gas emissions sdg13 the optimisation problem formulation is run for selecting the optimal combinations of subsidy levels for six technologies which simultaneously maximise progress in each of the selected sdgs this is the first step of iam pa integration moreover acknowledging that uncertainty is widely accepted to be pervasive in any attempt to manage and understand environmental problems uusitalo et al 2015 a robustness analysis is incorporated in the proposed framework depending on the discipline and context of application uncertainty of data or model components can be interpreted in different ways varying from measures of performance bounds alternative scenarios fuss et al 2012 trachanas et al 2018 or probability distributions lin and beck 2012 in this approach the propagation of uncertainties through the integrated models involves determining the effect on the output of changes in the inputs and is expressed stochastically by means of a probability distribution and deterministically with the use of scenarios probabilistic uncertainty is incorporated in the portfolio analysis model to find robust pareto optimal portfolios of technologies in each of ssps deterministic uncertainty referring to specific scenarios with clearly determined datasets nikas et al 2019 is used to assess the robustness of the modelling results across different socioeconomic pathways and timescales van groenendaal and kleijnen 2002 this is done primarily by using different ssps which represent epistemic uncertainty hanger kopp et al 2019 but constitute reference single futures of deterministic nature on which modelling exercises anchor to cover a broad spectrum of possible future socioeconomic states of the world van ruijven et al 2014 the second goal of this paper is to simulate an ssp robustness scenario by defining ssp based uncertainty bounds as boundaries for robustness and simulate probabilistic uncertainty among the socioeconomic pathways results of the ssp robustness scenario are compared with results of the distinct socioeconomic pathways analysis the second step of iam pa integration is achieved by feeding the pa results back to gcam the ssp robust subsidy portfolios are re run in the gcam model with each ssp to check whether portfolios that are found to be robust to ssp based uncertainty are also translated to more homogeneity between the ssps with respect to the portfolio s impact on sdg progress the identification of technological portfolios that are robust among the different ssps can be helpful for stakeholders to make decisions and formulate policies that will be optimal independently of the realisation of different ssps in the future providing a useful tool to handle ssp based uncertainty validation of the methodological framework which is outlined in fig 1 is achieved by means of a case study in eastern africa in section 3 2 methods 2 1 the global change assessment model gcam is a dynamic recursive partial equilibrium model connecting socioeconomics energy land use and climate systems and can be used to investigate the consequences of climate change mitigation policies including carbon taxes carbon trading regulations and accelerated deployment of energy technology jgcri 2017 gcam and its predecessors have been used in applications investigating future emission scenarios and energy technology pathways edmonds et al 1994 rao 2017 gcam is one of the four models chosen to develop the representative concentration pathways of the ipcc s 5th assessment report pachauri 2015 and has been included in almost all major climate energy assessments over the last few decades the model covers the entire world dividing it into 32 regions and runs in 5 year time steps from 1990 to 2100 simulating future emission paths for 24 greenhouse gases and short lived species including co2 from fossil fuel combustion and land use change ch4 n2o nox so2 bc oc co and nmvoc for the purposes of this study gcam version 4 4 is used as a base within this model the case study region in the model eastern africa see section 3 1 has been adjusted for a more informed reflection of modern real world conditions van de ven et al 2019 in particular urban energy demand has been separated from rural energy demand yu et al 2014 and specific residential energy demands such as cooking lighting refrigeration and tvs separated from other residential energy uses especially demand for cooking has been modelled in more detail improving realistic projections into future cooking energy use and its impacts on indoor and outdoor air quality these impacts on air quality are quantified by measuring the premature deaths that air pollution both indoor and outdoor is projected to cause in future scenarios indoor mortality is estimated by extrapolating a historical causal relationship between indoor pm2 5 and mortality measured by the global burden of disease forouzanfar et al 2016 outdoor mortality is measured through the air quality model tm5 fasst van dingenen et al 2018 furthermore additional costs have been added to the provision of centrally generated electricity to rural areas representing the required extensions in transmission and distribution networks while mini grids have been added as an alternative for rural energy demand this novelty allows for projecting more realistic future scenarios in terms of household energy access which has been measured on a household level using the tier framework world bank 2015 see van de ven et al 2019 for all details on how gcam outputs are translated to sdg progress indicators the inputs used to run the gcam model and the outputs retrieved to be utilised as input in the portfolio analysis pa model are presented in fig 2 inputs to gcam include socioeconomic data for different ssps such as population gross domestic product gdp rate of urbanisation energy demand food demand household discount rates agricultural yields energy resource productivity and emission factors outputs to be used in the pa model include energy access tier changes avoided premature deaths and ghg emission reductions as a result of different subsidy levels for a number of sustainable technologies per ssp and time point 2020 2030 and 2040 2 2 multi objective optimisation and portfolio analysis multi objective optimisation refers to the simultaneous optimisation i e minimisation or maximisation of multiple usually conflicting objective functions once such a problem is posed it is of the practitioner s interest to obtain approximate and view the set of all trade off or compromise solutions of the problem the set of trade off solutions is referred to in the current article as the pareto front of the problem as environmental problems are driven by multiple objectives and criteria a single optimal solution very rarely exists rather a pareto set of solutions can be identified within which no single solution is strictly better than any other and a trade off is required between the competing objectives if the variables of a multi objective optimisation problem take values from a continuous set then we refer to that problem as a multi objective continuous optimisation problem on the other hand if the variables take values from a set of integers then the problem is referred to as a multi objective integer programming moip problem in this paper we model and solve a moip problem the most widely used methods concerning the identification of pareto optimal solutions are the weighting and ε constraint methods especially in cases of integer programming the ε constraint method has better performance and certain advantages over the weighting generation methods steuer 1989 in the principle of the ε constraint method lies the optimisation of one of the objective functions p using the other objective functions p 1 as constraints only portfolios that are non dominated i e when none of the objective functions can be improved in performance without degrading one or more of the other objective function values can be considered as portfolios that represent the optimal trade off between objectives for the purpose of identifying the non dominated or pareto optimal solutions to the mathematical optimisation formulation here the use of an extension of the ε constraint method namely the augmented ε constraint augmecon 2 mavrotas and florios 2013 algorithm is suggested the augmecon 2 method guarantees the generation of all pareto optimal solutions while avoiding the generation of other non optimal solutions the augmecon 2 method can deal with multiple objectives simultaneously and has been successful in recent optimisation studies in a variety of fields concerning municipal solid waste management mavrotas et al 2013 energy efficiency policies evaluation forouli et al 2019b power generation technology portfolio optimisation forouli et al 2019a equity portfolio construction and selection xidonas et al 2010 biopharmaceutical processes vieira et al 2017 surface mounting devices machines component allocation torabi et al 2013 etc in the augmecon 2 method the problem to be solved is of the following form 1 max f 1 x e p s s 2 r 2 10 1 s 3 r 3 10 p 2 s p r p with the following constraints subject to 2 x f f k x s k e k k 2 p where f k x is the objective function to be maximized f is the feasible region e p s 10 6 10 3 e k is the right hand side of the corresponding constraint for the objective function k r k is the range of the objective function k s k is a surplus variable for objective function k the optimisation process is driven by the parametrical variation in the right hand side of the constrained objective functions e k at first the range r k of objective functions 2 p that will be used as constraints is calculated from the payoff table the table with the results from the individual optimisation of the p objective functions the augmecon 2 method proposes the use of lexicographic optimisation for every objective function in order to construct the payoff table with only pareto optimal solutions the range of the k t h objective function is divided to g k intervals using g k 1 intermediate equidistant grid points thus we have in total g k 1 grid points that are used to vary parametrically the right hand side e k of the k th objective function the step for the variation of e k for objective function k will be 3 s t e p k r k g k and the right hand side of the corresponding constraint in the i th iteration for objective function k will be 4 e k f m i n k i k s t e p k f m i n k is the minimum from the payoff table of objective function k the optimisation process is solved iteratively for the different e k which correspond to the different grid points and so the number of runs are g 2 1 g 3 1 g p 1 supposing we first begin to optimise by adding s t e p 2 to e 2 in each iteration we compare the surplus variable s 2 of objective function f 2 that corresponds to the innermost objective function i e the first of the p objective functions from which we begin with s t e p 2 when the surplus variable s 2 is larger than s t e p 2 it is implied that in the next iteration the same solution will be obtained with the only difference being the surplus variable which will now have the value s 2 s t e p 2 this makes the iteration redundant and therefore we can bypass it as no new pareto optimal solution is generated we then calculate the new e 2 by moving forward by s t e p 2 until all grid points of f 2 are either assessed or bypassed then we repeat the same procedure by varying the right hand side of f 3 namely e 3 and the iterations are repeated for the p objecting functions following the above calculation procedure we obtain the exact pareto set of optimal solutions for a more in depth description on finding the exact pareto set in multi objective integer programming problems with the use of the augmented ε constraint the reader is referred to mavrotas and florios 2013 the portfolio optimisation problem is solved in the general algebraic modelling system gams the portfolio analysis parameters coming from gcam section 2 1 include the three parameters relevant to progress across three different sdgs energy access tier change ghg emission cuts and avoided premature deaths due to air pollution fig 2 2 3 a cross scenario framework to understand the impact of technology subsidies the gcam modelling exercise considers subsidies for different energy technology packages in combination with three different socioeconomic pathways in the context of the region of the case study focus i e eastern africa ssps 3 and 5 can be seen as extreme scenarios of respectively low and high development and are expected to represent the margins of uncertainty for policy implementation drawing from both the narratives associated with the ssp framework o neill et al 2017 and the gcam outputs in a few situations however the average conditions as represented in ssp 2 which reflects a possible future following historic patterns translate to the highest or lowest depending on the technology cost effectiveness of technological subsidisation results for ssp 1 sustainability and ssp 4 inequality are assumed to lie in most cases within the margins of the three modelled ssps and therefore neither of these two scenarios have been modelled explicitly the optimisation portfolio analysis problem fig 3 is thus run separately for each of the three ssps 2 3 and 5 we identify three evaluation criteria or objectives to optimise and thus we formulate a tri objective optimisation problem the evaluation criteria include the maximisation of ghg emission reductions the maximisation of energy access tier improvement and the maximisation of avoided premature deaths corresponding to sdg13 climate action sdg7 affordable and clean energy and sdg 3 good health and well being respectively the three different time points are reflected as differences in the values of the portfolio analysis input data last but not least the model considers a subsidy budget constraint in order to ensure that the overall cost of the approved applications does not exceed a predefined value ultimately nine portfolio optimisation problems are solved three for each of the different timescales 2020 2030 and 2040 for each of the three ssps ssp2 ssp3 and spp5 2 4 stochastic uncertainty analysis the proposed approach examines the effects of both deterministic and stochastic non deterministic uncertainty in order to effectively assess the robustness of the resulting optimal portfolios deterministic uncertainty is expressed by means of the above described scenario analysis we consider different scenarios in terms of technology performance in each of the three time frames but more importantly the optimality of solutions is stress tested across the three socioeconomic scenarios regarding stochastic uncertainty which is inherent in these parameters this is incorporated into the model by running a monte carlo simulation the uncertain model parameters namely the performance of the assumed technologies in terms of maximising emission reductions energy access and health benefits are treated as of stochastic nature by sampling their values using a uniform distribution at first we run the model using deterministic values for all the uncertain parameters and the no uncertainty pareto front is determined then monte carlo simulation is performed iteratively to sample random values for the uncertain parameters from the uniform distributions and the model is solved to generate the set of pareto optimal portfolios eventually the execution of multiple monte carlo iterations results in many differentiated pareto fronts which are analysed to draw conclusions over the robustness of the portfolios shaping the pareto front when no uncertainty is considered we perform 1000 monte carlo iterations as discussed above the gcam model is run for the three ssp scenarios separately for every subsidy level the ssps are seen here as an uncertain set of conditions that affect the performance of every technological subsidy policy for the three major optimisation problems run to identify the optimal portfolios separately for each of the three considered ssps the mean value for the uniform distributions is set equal to the estimated values as obtained from the runs of the gcam model and the deviation of the monte carlo iterations is set equal to 5 as in forouli et al 2019a 2 5 a validation framework in order to perform a robustness analysis on the modelling results of the individual ssps we introduce the ssp robustness scenario the ssp robustness scenario is not run in the gcam model as a new scenario and there is no intention to introduce a new scenario to replace or simulate any of the ssps as already mentioned gcam is run for the individual ssps and generates results per ssp regarding the impact of each technology subsidisation option on the three parameters energy access change ghg emissions reductions and avoided deaths associated to air pollution this impact is different per ssp our purpose is to define robust subsidy portfolios regardless of what ssp our world will resemble in the future the ssp robustness scenario uses the mid point of the ssp scenario outcomes on the cost effectiveness for progress along these parameters corresponding to the three sdgs and uses the range along the three ssp outcomes to perform a robustness analysis uncertainty over which ssp is expected to be realised in the future is therefore incorporated in the portfolio analysis as a range for the cost effectiveness of the technologies in achieving progress to each of the sdgs van de ven et al 2019 in this way the range of the gcam ssp simulation outcomes which are different for each technology define the ranges of the uniform distribution which is used for monte carlo simulation uncertainty ranges differ among the considered technologies the higher the range of the ssp outcomes is the broader the range of uncertainty in the uniform distribution is considered a portfolio analysis problem in which technologies with narrow uncertainty boundaries are optimised is thus expected to be more robust among the different ssps compared to one with a larger uncertainty range depicting higher vulnerability to the ssp simulation outcomes to better clarify this in the extreme scenario where the resulting performance of a technology is identical among the different ssps portfolios resulting from the optimisation will be completely robust when uncertainty is examined in terms of different ssp realisation the ranges of the uniform distributions ssp based uncertainty boundaries and an example of calculating the ssp based uncertainty boundaries based on the mid point of the performances across the three ssps are presented in tables 1 and 2 in order to verify if the robustness of ssp uncertainty bounds leads to more robust solutions among the different ssps optimal portfolios of the ssp robustness scenario differing in their robustness score are selected and reiterated in the gcam model the reiteration is applied across the three different ssps new results on the contribution of the technologies to each of the sdgs are retrieved and the goal is to test whether the results of a more robust portfolio are indeed more homogeneous between the different ssps compared to a less robust one this is quantified by measuring the ranges of performances across the three sdgs among the ssps and verifying that they are smaller in case of a more robust portfolio to summarise the information flow into between and out of the two models fig 2 the gcam model is initially fed with socioeconomic data from three ssps and its outputs are used to calculate avoided premature mortality due to air pollution ghg emissions cuts and energy access levels associated with different subsidisation levels for a number of sustainable technologies the latter along with a given total budget are fed into the pa model which calculates the most robust near optimal technology subsidisation portfolios for three timescales 2020 2030 and 2040 per ssp carrying out monte carlo analysis within a 5 uncertainty range for each of the three sdg relevant parameters at the same time an additional subsidy dataset is developed based on the midpoint of the extreme gcam resulting outcomes in terms of cost effectiveness of technology subsidies for sdg progress separately for each technology and sdg impact monte carlo analysis for this extra scenario is performed in a different uncertainty range that is defined by the extreme values of each parameter for each subsidy level and technology finally optimal subsidy levels for each technology from selected portfolios of this scenario of various robustness scores are fed back into gcam in order to validate whether portfolios with higher robustness scores are indeed more robust to the impact of ssp related modelling inputs on outputs in terms of the cost effectiveness of sdg progress 3 validation and discussion 3 1 context of the case study three different ssp datasets o neill et al 2014 have been modelled in gcam for the purposes of a case study focusing on twelve eastern african countries burundi comoros djibouti eritrea ethiopia kenya madagascar rwanda somalia sudan south sudan and uganda aggregated and assessed as one region these datasets include ssp 2 middle of the road ssp 3 regional rivalry and ssp 5 fossil fuelled development specifically the gcam inputs that have been adapted to each of these ssps are global population gdp rate of urbanisation energy and food demand household discount rates agricultural yields energy resource productivity and emission factors 2 2 https github com jgcri gcam doc blob gh pages ssp md riahi et al 2017 table 3 shows the assumed evolution from 2010 to 2040 of the ssp inputs regionally for eastern africa that have most influence on the gcam outputs population gdp and urbanisation the candidate actions are six technological subsidisation pathways revolving around liquefied petroleum gas lpg photovoltaics pv biogas ethanol charcoal and fuelwood i e technologies likely to be adopted in the twelve developing countries of eastern africa based on their action pledges as reflected in their nationally determined contributions while contributing to the three predefined sdgs the gcam generated parameters for the three sdgs showcase the contribution of each technology pathway to each of the objective functions under twenty subsidy levels for each of the four major optimisation problems different time frames are applied and results for the years 2020 2030 and 2040 are extracted the budget constraint starts from 3 5 billion usd at 2015 values in 2020 and increases by 5 per year until 2030 and 2040 3 2 cost effectiveness of technology subsidies and sdg progress the first step in the proposed methodological framework is to simulate future socioeconomic scenarios through the gcam model and translate outputs from each policy scenario into progress parameters relevant to sdgs this is done by applying six different pathways of technology subsidies up to 2040 and then measuring the impact of these subsidies on progress towards each of the three sdgs results on the cost effectiveness of technology subsidies for sdg progress figs 4 6 show that subsidies for biogas systems are the most cost effective for each of the indicators scenarios and years on the contrary subsidies for fuelwood pathways are only reasonably cost effective in the short term for charcoal pathways we observe that cost effectiveness is highly dependent on the invested subsidies when examining progress on the different timescales we see that more subsidies are required for achieving the same impact in the long term at least for energy access and premature mortality indicators in addition in the medium and long term technologies like fuelwood charcoal and ethanol show an even negative impact for the examined subsidies between the different ssps cost effectiveness remains in the same levels for each technology with some differences observed in the short term for biogas and the ghg emissions indicator as well as on charcoal for reducing ghg emissions in the medium and long term overall more subsidies are required to achieve a positive impact in the three sdgs when ssp 5 is realised generally we notice that depending on the scenario and the point in time some technology pathways are more cost effective than others for a specific sdg and some result in negative outcomes and thus not incorporated in figs 4 6 thus the need to identify technological portfolios that are both pareto optimal in terms of contributing simultaneously to the three sdgs and most importantly robust among the different scenarios is more than prominent 3 3 robust subsidy portfolios in each individual ssp the second goal of this research is to identify optimal technology portfolios that are robust in each of the different socioeconomic pathways when probabilistic uncertainty in the model parameters is imposed the results of the portfolio optimisation incorporating the robustness information produced by the monte carlo runs are shown per policy scenario in figs 7 9 where we can see the set of solutions that represent the best possible trade offs between the three sdgs a comparison of results among the different ssps can be easily retrieved in each figure differences on technological performance among the ssps are mainly observed in ssp 5 for the years 2030 and 2040 in more detail ssp 2 can prove more progress friendly in achieving the three sdgs in the short term among all different socioeconomic pathways in the medium and long term ssp 3 leads to better results for the energy access and health criteria while for the goal of reducing emissions ssp 2 performs better ssp 5 features the lowest contribution to the optimisation objectives for all considered time scales which is fairly consistent with its intended narrative ssp5 is characterised by higher incomes and urbanisation which increase access to high quality energy sources such as lpg even without subsidisation due to this more positive counterfactual technology subsidisation in ssp5 is found less cost effective the reasoning on how policy implications affect the adoption of technologies can be found in van de ven et al 2019 3 4 robust subsidy portfolios across all three ssps in this section the goal is to define robust subsidy portfolios for any of the three ssps 2 3 and 5 to evaluate the robustness of the results an analysis that applies the ranges of the gcam simulation outcomes between ssps as its boundaries for robustness is introduced and carried out fig 10 illustrates the pareto fronts of the optimal solutions while giving information on the robustness of the results which is represented by the size of dots the bigger the dots the higher the robustness the behaviour of the optimal solutions across the different timescales shows homogeneity with the analysis provided for the different ssps what is important is to additionally verify if the robustness of ssp uncertainty bounds leads to more robust solutions among the different ssps to achieve that we select two optimal portfolios for each of the six pareto curves of fig 10 one with a higher robustness score and one with a lower robustness score the robustness score indicates the number of monte carlo iterations within which a portfolio remains optimal we re iterate these portfolios in the gcam model to test whether the results of a more robust portfolio are indeed more homogeneous between the different ssps i e that the ranges of sdg performances across the ssps are smaller in case of a more robust portfolio the results shown in table 4 suggest that in the majority of the scenarios we can confirm a smaller output range between ssps if a portfolio with a higher robustness score is chosen the range in outcomes decreases by up to 16 for the baseline scenario in 2020 3 5 empirical findings discussion and results table 5 shows how the realisation of the different ssps affects the total impact and contributions per technology for the most robust pareto optimal subsidy portfolios the robustness of the optimisation process is examined for each of the ssps separately assuming that the performance of the assumed technologies in terms of maximising emission reductions energy access and health benefits is stochastically uncertain consistent with the analysis on the cost effectiveness of the technology subsidies biogas is the technology with the higher participation in the robust portfolios this is evident across the different ssps and timescales the use of lpg and pv systems also have a high contribution to progress in the three sdgs charcoal kilns and ethanol technologies reach their maximum potential which though corresponds to a much lower subsidy and impact level compared to lpg pv and biogas fuelwood is the least attractive technology the policy context on how the different policy scenarios affect subsidisation and effectiveness of the technologies is provided in van de ven et al 2019 here the ssps are assumed as an uncertain set of conditions that affect the performance of every technological subsidy policy and we focus on how the realisation of the different ssps will ultimately affect the participation of technologies in the robust portfolios for the year 2020 technologies show a stable share of participation in the robust portfolios in 2030 a high increase in the contribution of ethanol is observed for ssp 5 where ethanol is subsidised up to 11 in contrast to ssps 2 and 3 where subsidisation for ethanol is less than 2 the realisation of different ssps has an overall bigger effect on sdg progress in 2040 4 conclusions this research presents a two level integration of an integrated assessment model namely gcam and a portfolio analysis model based on augmecon 2 and monte carlo simulations with the aim to provide policymakers with a comprehensive tool to address environmental and energy related issues facilitating the exchange of input data and model results across different methodologies the integration is applied to a case study focusing on technological portfolio optimisation among different plausible socioeconomic futures in eastern africa initially the gcam model is run to simulate future socioeconomic scenarios for six relatively sustainable technologies outputs from each scenario are translated into progress parameters relevant to sdgs and are fed into a pa model the portfolio optimisation model leads to the identification of pareto fronts of optimal portfolios and allows comparison among different ssps the results show how resource allocation must be shared among the technologies to achieve optimal trade offs on the simultaneous achievement of three goals increase of energy access sdg7 reduced exposure to air pollution and avoidance of related mortality sdg3 and mitigation of global warming sdg13 biogas is the technology with the higher participation in the robust portfolios across all ssps and timescales while fuelwood is the least attractive technology a comparison between the ssps shows that differences in the technological performances among the ssps are mainly observed in ssp 5 for the years 2030 and 2040 in order to hedge uncertainty concerning the realisation of different ssps an analysis that applies the ranges of the gcam simulation outcomes between ssps as its boundaries for robustness is introduced the second link between the pa and gcam models is achieved by feeding the gcam model with the results of the portfolio optimisation analysis to verify if the robustness of ssp uncertainty boundaries leads to more robust solutions across the different ssps we selected for each pareto front a portfolio of high robustness and a portfolio with lower robustness score and reiterated these portfolios in the gcam model to retrieve results for the technologies impact for each ssp this is to test whether the resulting ranges of sdg performances between the ssps are smaller in case of a more ssp robust portfolio the results confirm that all portfolios show a smaller output range between ssps if a portfolio with a higher robustness score is chosen showcasing the advantage of the proposed methodology a limitation of our proposed methodology related to the ssp robustness scenario lies on the initial choice of the mid point between the range of ssp results in terms of the impact of each technology on sdg progress for identifying the pareto front of the optimal portfolio the proposed idea is not to imply that the ssps have the same probability of occurrence as no rational assessment of probabilities of various representative scenarios can conclude equal likelihood kinzig and starrett 2003 hence justifying the use of a mean value but rather to assess the uncertainty of the results across the entire spectrum of the resulting values as defined by the individual ssp results even though we do apply the full ssp related uncertainty range when calculating the robustness of the individual portfolios on the pareto front it could be that the election of a different point within the ssp outcome range would yield a slightly different pareto front and hence alter the portfolio outcome however we think that the difficulty to select a justifiable mid value within an ssp related outcome range poses a limitation that is a necessary evil which enables the stochastic identification of optimal technology portfolios against all or most potential outcomes grübler and nakicenovic 2001 of technological subsidisation regardless of our capacity to envisage a future world state that leads to these outcomes and especially since these outcomes are forecasts of a single model allen 2003 nevertheless acknowledging this limitation and understanding the knowledge gaps reflected in this broad spectrum against which the resulting technological subsidisation portfolios are assessed may allow science both to reduce uncertainties in a systematic manner and to convey to policymakers the need to manage and integrate uncertainty into the policymaking process schneider 2003 along those lines it is also important to note that although the policy assumptions of the paper are carefully selected based on published research van de ven et al 2019 policy relevance of results is highly dependent on the assumptions applied when modelling the policy scenarios and these must be carefully considered when interpreting the results without overlooking this limitation the novel methodology introduced in this research can be useful for stakeholders to manage the uncertainty prominent in the future states of the world according to different adaptation and mitigation challenges both core elements of the methodology namely the gcam model and the portfolio analysis model can be extended to include more parameters i e technologies than the ones represented in the current application however we must consider the limitations in terms of time and processing requirements that the solution of more complex problems i e of numerous objective functions more monte carlo simulations may require this limitation can in the future be overcome by using an enhanced algorithm for solving the portfolio analysis model like augmecon r which is a more advanced version of augmecon 2 with significantly faster resolution performance nikas et al 2020 further prospects towards enriching the proposed methodological framework include the selection of variables with implications for a broader set of sdgs and the integration of the models with participatory tools to actively involve stakeholders in the case study participation of stakeholders in policy analysis has been found to improve system understanding and scoping risks associated with climate policy and technologies van vliet et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the h2020 european commission project paris reinforce under grant agreement no 820846 and by the spanish ministry of economy and competitiveness mineco through bc3 maría de maeztu excellence accreditation mdm 2017 0714 the sole responsibility for the content of this paper lies with the authors the paper does not necessarily reflect the opinions of the european commission or the spanish government 
26008,this research introduces a two level integration of climate economy modelling and portfolio analysis to simulate technological subsidisation with implications for multiple sustainable development goals sdgs across socioeconomic trajectories and considering different levels of uncertainties we use integrated assessment modelling outputs relevant for progress across three sdgs namely air pollution related mortality sdg3 access to clean energy sdg7 and greenhouse gas emissions sdg13 calculated with the global change assessment model gcam for different subsidy levels for six sustainable technologies across three shared socioeconomic pathways ssps feeding them into a portfolio analysis model optimal portfolios that are robust in the individual socioeconomic scenarios as well as across the socioeconomic scenarios are identified by means of an ssp robustness score a second link between the two models is established by feeding portfolio analysis results back into gcam application in a case study for eastern africa confirms that most ssp robust portfolios show smaller output ranges among scenarios keywords integrated assessment modeling global change assessment model shared socioeconomic pathways sustainable development goals stochastic uncertainty scenario analysis software and data availability software name global change assessment model gcam developers joint global change research institute jgcri http www globalchange umd edu contact address 5825 university research court suite 3500 college park md 20740 united states year first available 1982 required software java program language c xml availability download available at https github com jgcri gcam core releases tag gcam v4 4 1 cost free software name aucmecon 2 developers mavrotas g florios k 2013 an improved version of the augmented ε constraint method augmecon2 for finding the exact pareto set in multi objective integer programming problems applied mathematics and computation 219 18 9652 9669 contact address laboratory of industrial and energy economics school of chemical engineering national technical university of athens zographou campus athens 15780 greece e mail mavrotas chemeng ntua gr year first available 2013 required software gam program language gams language availability download available at https sites google com site kflorios augmecon2 cost free 1 introduction integrated assessment models iams are a core element of the scientific processes that comprise the best available science peters 2016 when it comes to analysing energy system transitions within the context of climate change mitigation and sustainable socioeconomic development nikas et al 2019 pietzcker et al 2017 schwanitz 2013 janssen et al 2009 these tools are applied to analyse adaptive energy environment economy systems in the global scientific and policy arena ewert et al 2015 gidden et al 2018 estrada et al 2019 advance scientific understanding of the potential to combat climate change and underlying dynamics towards robust and sustainable development huppmann et al 2019 warren et al 2019 and evaluate the various technologies initiatives and policy options that ensure clean and sustainable energy transition wyrwa 2015 shi et al 2017 liu et al 2019 in particular these models constitute a well established scientific tool aimed at understanding feedbacks and influences between different system components including the social economic and ecological implications of different natural or anthropogenic factors especially with regard to interlinkages between the human and the natural system calvin and bond lamberty 2018 gidden et al 2018 the core advantage of these complex models is that they provide an integrated system perspective to study the dauntingly complex interactions between energy economy land use water and climate systems scott et al 1999 weyant 2017 through such an integration iams combine multiple and diverse components across their social organisational and conceptual boundaries to provide a comprehensive analysis of the problem collins et al 2015 jakeman and letcher 2003 for this purpose different modules or components are coupled with one other usually including but not limited to the economy the environment the energy system and the climate feedbacks or economic impacts of changes among them giupponi et al 2013 modelling results are widely used to inter alia directly influence decisions and taken stock of towards advising policymakers as is the case of the assessment reports of the intergovernmental panel on climate change ipcc recent examples of publications where iams contribute to providing background information on possible energy and climate futures and to scientifically underpinning international climate policy negotiations are the ipcc s special report on the impacts of global warming of 1 5 c above pre industrial levels ipcc 2018 the world energy outlook 2018 international energy agency 2018 or the european union eu energy roadmap 2050 ec com 2016 in this respect decision makers are based on iam driven policy prescriptions to develop policies that contribute to managing environmental resources and assets in a way that delivers acceptable environmental and socioeconomic outcomes more details on iams can be found in krey 2014 weyant 2017 and nikas et al 2019 which review energy economic models including or focusing on iams and provide a categorisation of them based on parameters like their degree of integration and mathematical underpinnings as well as highlight the challenges associated with these modelling frameworks given their strengths and weaknesses hamilton et al 2015 however analyses based exclusively on these formalised frameworks alone are usually not sufficient to address the broad spectrum of challenges associated with climate change and policy assessment doukas et al 2018 and recent advances and paradigms call for and or apply complementing them with other methods and tools turnheim et al 2015 geels et al 2016 in this direction iams have recently been coupled with a diversity of tools towards enhancing scientific processes and leading to more pragmatic policy prescriptions including but not limited to life cycle analyses arvesen et al 2018 fuzzy cognitive mapping nikas et al 2020 antosiewicz et al 2020 and multiple criteria decision aid frameworks baležentis and streimikiene 2017 shmelev and van den bergh 2016 one of these tools which has been established in the climate policy domain doukas and nikas 2020 in diverse applications allan et al 2011 bistline 2016 odeh et al 2018 zhang et al 2018 and long been coupled with iams e g baker and solak 2011 pugh et al 2011 forouli et al 2019a forouli et al 2019b del granado et al 2019 is portfolio theory in fact energy planning decisions are often portfolio building problems in which the task is to find a viable mix of actions to meet the overall objectives targets and constraints therefore today as many energy related decisions fall into this category portfolio decision analysis methods and tools are seen as the next step in energy decision support marinoni et al 2011 vilkkumaa et al 2014 through such tools decision makers are able to consider a set of actions and create policy incorporating relevant concerns and interests in a balanced way typically decision makers have to consider the overall performance of a portfolio across many relevant dimensions or criteria such as techno economic socio political and environmental impacts huang and wu 2008 muñoz et al 2009 portfolio analysis pa addresses the need to consider multiple objectives and constraints and further contributes to identifying promising candidate actions and examining interactions among them portfolio decision models were first applied on risk diversification in financial investments and have their roots to the work of markowitz 1952 markowitz proposed a mean variance model to support investment decisions in light of uncertainty associated with the future returns of financial assets today there is a range of portfolio modelling approaches which offer modelling and optimisation support to find the most preferred portfolio of actions and which are applicable to energy and environmental modelling lahtinen et al 2017 provide a detailed comparative description of portfolio modelling approaches among the most common ones are the value cost or benefit cost approach where actions are prioritised according to the value cost ratio until a budget cap is reached hajkowicz et al 2008 marinoni et al 2011 the disadvantage of this method is that in case of synergies or interactions between the actions optimality is not guaranteed as interactions play a critical role in energy and or climate economy problems this approach is often not sufficient an approach that incorporates the risk parameter into the evaluation is the modern portfolio theory approach where the optimal resource allocation for each risk level is identified crowe and parker 2008 paydar and qureshi 2012 from the above we understand that pa and iams are widely used in policy analysis and evaluation of pathways for the transformation of the human and earth systems the interconnectedness of our world is broadly acknowledged to require integrated rather than piecemeal approaches to resolving complex environmental issues particularly in view of the increasing speed and pervasiveness of connections associated with globalisation with the interaction of sustainable development goals sdgs with climate change and action gaining increasing prominence at the interface of science and policy developing computational tools and models that operate across academic disciplines and methodologies becomes ever more important in this paper we use a multi objective optimisation approach where the result is a set of non dominated portfolios through this approach interactions among the set of actions and portfolio constraints can be considered the goal is to generate non dominated combinations of actions in terms of comparing between the evaluation criteria as required by portfolio modelling and in order to generate the non dominated portfolios all candidate actions are simultaneously considered and optimised in the same portfolio optimisation model the goal is to identify optimal portfolios of actions or a set of non dominated portfolios that best meet multiple objectives while satisfying the problem constraints decision makers can then select a portfolio among the non dominated ones tailored to their needs and preferences to understand the term of portfolio dominance a portfolio is said to be dominated if there exists another portfolio of actions that performs better in some attribute criterion and at least equally good in all other attributes the model based portfolio generation process proposed here supports the consideration of multiple objectives and constraints and interactions among the actions while acknowledging the vital role of uncertainty in particular the first goal of this paper is to create an efficient scientific workflow and a two way technical integration of integrated assessment modelling and portfolio optimisation outcomes to this end at first we simulate future policy under policy relevant socioeconomic scenarios such as the shared socioeconomic pathways ssps o neill et al 2014 the global change assessment model gcam is used as the implementation integrated assessment model 1 1 τhe updated gcam documentation website includes a specific section describing the ssp implementation throughout the model https github com jgcri gcam doc blob gh pages ssp md the outputs from each policy scenario are translated into progress parameters relevant to three sdgs of the united nations 2030 agenda for sustainable development and fed into a pa model these parameters include air pollution related mortality sdg3 access to clean energy sdg7 and greenhouse gas emissions sdg13 the optimisation problem formulation is run for selecting the optimal combinations of subsidy levels for six technologies which simultaneously maximise progress in each of the selected sdgs this is the first step of iam pa integration moreover acknowledging that uncertainty is widely accepted to be pervasive in any attempt to manage and understand environmental problems uusitalo et al 2015 a robustness analysis is incorporated in the proposed framework depending on the discipline and context of application uncertainty of data or model components can be interpreted in different ways varying from measures of performance bounds alternative scenarios fuss et al 2012 trachanas et al 2018 or probability distributions lin and beck 2012 in this approach the propagation of uncertainties through the integrated models involves determining the effect on the output of changes in the inputs and is expressed stochastically by means of a probability distribution and deterministically with the use of scenarios probabilistic uncertainty is incorporated in the portfolio analysis model to find robust pareto optimal portfolios of technologies in each of ssps deterministic uncertainty referring to specific scenarios with clearly determined datasets nikas et al 2019 is used to assess the robustness of the modelling results across different socioeconomic pathways and timescales van groenendaal and kleijnen 2002 this is done primarily by using different ssps which represent epistemic uncertainty hanger kopp et al 2019 but constitute reference single futures of deterministic nature on which modelling exercises anchor to cover a broad spectrum of possible future socioeconomic states of the world van ruijven et al 2014 the second goal of this paper is to simulate an ssp robustness scenario by defining ssp based uncertainty bounds as boundaries for robustness and simulate probabilistic uncertainty among the socioeconomic pathways results of the ssp robustness scenario are compared with results of the distinct socioeconomic pathways analysis the second step of iam pa integration is achieved by feeding the pa results back to gcam the ssp robust subsidy portfolios are re run in the gcam model with each ssp to check whether portfolios that are found to be robust to ssp based uncertainty are also translated to more homogeneity between the ssps with respect to the portfolio s impact on sdg progress the identification of technological portfolios that are robust among the different ssps can be helpful for stakeholders to make decisions and formulate policies that will be optimal independently of the realisation of different ssps in the future providing a useful tool to handle ssp based uncertainty validation of the methodological framework which is outlined in fig 1 is achieved by means of a case study in eastern africa in section 3 2 methods 2 1 the global change assessment model gcam is a dynamic recursive partial equilibrium model connecting socioeconomics energy land use and climate systems and can be used to investigate the consequences of climate change mitigation policies including carbon taxes carbon trading regulations and accelerated deployment of energy technology jgcri 2017 gcam and its predecessors have been used in applications investigating future emission scenarios and energy technology pathways edmonds et al 1994 rao 2017 gcam is one of the four models chosen to develop the representative concentration pathways of the ipcc s 5th assessment report pachauri 2015 and has been included in almost all major climate energy assessments over the last few decades the model covers the entire world dividing it into 32 regions and runs in 5 year time steps from 1990 to 2100 simulating future emission paths for 24 greenhouse gases and short lived species including co2 from fossil fuel combustion and land use change ch4 n2o nox so2 bc oc co and nmvoc for the purposes of this study gcam version 4 4 is used as a base within this model the case study region in the model eastern africa see section 3 1 has been adjusted for a more informed reflection of modern real world conditions van de ven et al 2019 in particular urban energy demand has been separated from rural energy demand yu et al 2014 and specific residential energy demands such as cooking lighting refrigeration and tvs separated from other residential energy uses especially demand for cooking has been modelled in more detail improving realistic projections into future cooking energy use and its impacts on indoor and outdoor air quality these impacts on air quality are quantified by measuring the premature deaths that air pollution both indoor and outdoor is projected to cause in future scenarios indoor mortality is estimated by extrapolating a historical causal relationship between indoor pm2 5 and mortality measured by the global burden of disease forouzanfar et al 2016 outdoor mortality is measured through the air quality model tm5 fasst van dingenen et al 2018 furthermore additional costs have been added to the provision of centrally generated electricity to rural areas representing the required extensions in transmission and distribution networks while mini grids have been added as an alternative for rural energy demand this novelty allows for projecting more realistic future scenarios in terms of household energy access which has been measured on a household level using the tier framework world bank 2015 see van de ven et al 2019 for all details on how gcam outputs are translated to sdg progress indicators the inputs used to run the gcam model and the outputs retrieved to be utilised as input in the portfolio analysis pa model are presented in fig 2 inputs to gcam include socioeconomic data for different ssps such as population gross domestic product gdp rate of urbanisation energy demand food demand household discount rates agricultural yields energy resource productivity and emission factors outputs to be used in the pa model include energy access tier changes avoided premature deaths and ghg emission reductions as a result of different subsidy levels for a number of sustainable technologies per ssp and time point 2020 2030 and 2040 2 2 multi objective optimisation and portfolio analysis multi objective optimisation refers to the simultaneous optimisation i e minimisation or maximisation of multiple usually conflicting objective functions once such a problem is posed it is of the practitioner s interest to obtain approximate and view the set of all trade off or compromise solutions of the problem the set of trade off solutions is referred to in the current article as the pareto front of the problem as environmental problems are driven by multiple objectives and criteria a single optimal solution very rarely exists rather a pareto set of solutions can be identified within which no single solution is strictly better than any other and a trade off is required between the competing objectives if the variables of a multi objective optimisation problem take values from a continuous set then we refer to that problem as a multi objective continuous optimisation problem on the other hand if the variables take values from a set of integers then the problem is referred to as a multi objective integer programming moip problem in this paper we model and solve a moip problem the most widely used methods concerning the identification of pareto optimal solutions are the weighting and ε constraint methods especially in cases of integer programming the ε constraint method has better performance and certain advantages over the weighting generation methods steuer 1989 in the principle of the ε constraint method lies the optimisation of one of the objective functions p using the other objective functions p 1 as constraints only portfolios that are non dominated i e when none of the objective functions can be improved in performance without degrading one or more of the other objective function values can be considered as portfolios that represent the optimal trade off between objectives for the purpose of identifying the non dominated or pareto optimal solutions to the mathematical optimisation formulation here the use of an extension of the ε constraint method namely the augmented ε constraint augmecon 2 mavrotas and florios 2013 algorithm is suggested the augmecon 2 method guarantees the generation of all pareto optimal solutions while avoiding the generation of other non optimal solutions the augmecon 2 method can deal with multiple objectives simultaneously and has been successful in recent optimisation studies in a variety of fields concerning municipal solid waste management mavrotas et al 2013 energy efficiency policies evaluation forouli et al 2019b power generation technology portfolio optimisation forouli et al 2019a equity portfolio construction and selection xidonas et al 2010 biopharmaceutical processes vieira et al 2017 surface mounting devices machines component allocation torabi et al 2013 etc in the augmecon 2 method the problem to be solved is of the following form 1 max f 1 x e p s s 2 r 2 10 1 s 3 r 3 10 p 2 s p r p with the following constraints subject to 2 x f f k x s k e k k 2 p where f k x is the objective function to be maximized f is the feasible region e p s 10 6 10 3 e k is the right hand side of the corresponding constraint for the objective function k r k is the range of the objective function k s k is a surplus variable for objective function k the optimisation process is driven by the parametrical variation in the right hand side of the constrained objective functions e k at first the range r k of objective functions 2 p that will be used as constraints is calculated from the payoff table the table with the results from the individual optimisation of the p objective functions the augmecon 2 method proposes the use of lexicographic optimisation for every objective function in order to construct the payoff table with only pareto optimal solutions the range of the k t h objective function is divided to g k intervals using g k 1 intermediate equidistant grid points thus we have in total g k 1 grid points that are used to vary parametrically the right hand side e k of the k th objective function the step for the variation of e k for objective function k will be 3 s t e p k r k g k and the right hand side of the corresponding constraint in the i th iteration for objective function k will be 4 e k f m i n k i k s t e p k f m i n k is the minimum from the payoff table of objective function k the optimisation process is solved iteratively for the different e k which correspond to the different grid points and so the number of runs are g 2 1 g 3 1 g p 1 supposing we first begin to optimise by adding s t e p 2 to e 2 in each iteration we compare the surplus variable s 2 of objective function f 2 that corresponds to the innermost objective function i e the first of the p objective functions from which we begin with s t e p 2 when the surplus variable s 2 is larger than s t e p 2 it is implied that in the next iteration the same solution will be obtained with the only difference being the surplus variable which will now have the value s 2 s t e p 2 this makes the iteration redundant and therefore we can bypass it as no new pareto optimal solution is generated we then calculate the new e 2 by moving forward by s t e p 2 until all grid points of f 2 are either assessed or bypassed then we repeat the same procedure by varying the right hand side of f 3 namely e 3 and the iterations are repeated for the p objecting functions following the above calculation procedure we obtain the exact pareto set of optimal solutions for a more in depth description on finding the exact pareto set in multi objective integer programming problems with the use of the augmented ε constraint the reader is referred to mavrotas and florios 2013 the portfolio optimisation problem is solved in the general algebraic modelling system gams the portfolio analysis parameters coming from gcam section 2 1 include the three parameters relevant to progress across three different sdgs energy access tier change ghg emission cuts and avoided premature deaths due to air pollution fig 2 2 3 a cross scenario framework to understand the impact of technology subsidies the gcam modelling exercise considers subsidies for different energy technology packages in combination with three different socioeconomic pathways in the context of the region of the case study focus i e eastern africa ssps 3 and 5 can be seen as extreme scenarios of respectively low and high development and are expected to represent the margins of uncertainty for policy implementation drawing from both the narratives associated with the ssp framework o neill et al 2017 and the gcam outputs in a few situations however the average conditions as represented in ssp 2 which reflects a possible future following historic patterns translate to the highest or lowest depending on the technology cost effectiveness of technological subsidisation results for ssp 1 sustainability and ssp 4 inequality are assumed to lie in most cases within the margins of the three modelled ssps and therefore neither of these two scenarios have been modelled explicitly the optimisation portfolio analysis problem fig 3 is thus run separately for each of the three ssps 2 3 and 5 we identify three evaluation criteria or objectives to optimise and thus we formulate a tri objective optimisation problem the evaluation criteria include the maximisation of ghg emission reductions the maximisation of energy access tier improvement and the maximisation of avoided premature deaths corresponding to sdg13 climate action sdg7 affordable and clean energy and sdg 3 good health and well being respectively the three different time points are reflected as differences in the values of the portfolio analysis input data last but not least the model considers a subsidy budget constraint in order to ensure that the overall cost of the approved applications does not exceed a predefined value ultimately nine portfolio optimisation problems are solved three for each of the different timescales 2020 2030 and 2040 for each of the three ssps ssp2 ssp3 and spp5 2 4 stochastic uncertainty analysis the proposed approach examines the effects of both deterministic and stochastic non deterministic uncertainty in order to effectively assess the robustness of the resulting optimal portfolios deterministic uncertainty is expressed by means of the above described scenario analysis we consider different scenarios in terms of technology performance in each of the three time frames but more importantly the optimality of solutions is stress tested across the three socioeconomic scenarios regarding stochastic uncertainty which is inherent in these parameters this is incorporated into the model by running a monte carlo simulation the uncertain model parameters namely the performance of the assumed technologies in terms of maximising emission reductions energy access and health benefits are treated as of stochastic nature by sampling their values using a uniform distribution at first we run the model using deterministic values for all the uncertain parameters and the no uncertainty pareto front is determined then monte carlo simulation is performed iteratively to sample random values for the uncertain parameters from the uniform distributions and the model is solved to generate the set of pareto optimal portfolios eventually the execution of multiple monte carlo iterations results in many differentiated pareto fronts which are analysed to draw conclusions over the robustness of the portfolios shaping the pareto front when no uncertainty is considered we perform 1000 monte carlo iterations as discussed above the gcam model is run for the three ssp scenarios separately for every subsidy level the ssps are seen here as an uncertain set of conditions that affect the performance of every technological subsidy policy for the three major optimisation problems run to identify the optimal portfolios separately for each of the three considered ssps the mean value for the uniform distributions is set equal to the estimated values as obtained from the runs of the gcam model and the deviation of the monte carlo iterations is set equal to 5 as in forouli et al 2019a 2 5 a validation framework in order to perform a robustness analysis on the modelling results of the individual ssps we introduce the ssp robustness scenario the ssp robustness scenario is not run in the gcam model as a new scenario and there is no intention to introduce a new scenario to replace or simulate any of the ssps as already mentioned gcam is run for the individual ssps and generates results per ssp regarding the impact of each technology subsidisation option on the three parameters energy access change ghg emissions reductions and avoided deaths associated to air pollution this impact is different per ssp our purpose is to define robust subsidy portfolios regardless of what ssp our world will resemble in the future the ssp robustness scenario uses the mid point of the ssp scenario outcomes on the cost effectiveness for progress along these parameters corresponding to the three sdgs and uses the range along the three ssp outcomes to perform a robustness analysis uncertainty over which ssp is expected to be realised in the future is therefore incorporated in the portfolio analysis as a range for the cost effectiveness of the technologies in achieving progress to each of the sdgs van de ven et al 2019 in this way the range of the gcam ssp simulation outcomes which are different for each technology define the ranges of the uniform distribution which is used for monte carlo simulation uncertainty ranges differ among the considered technologies the higher the range of the ssp outcomes is the broader the range of uncertainty in the uniform distribution is considered a portfolio analysis problem in which technologies with narrow uncertainty boundaries are optimised is thus expected to be more robust among the different ssps compared to one with a larger uncertainty range depicting higher vulnerability to the ssp simulation outcomes to better clarify this in the extreme scenario where the resulting performance of a technology is identical among the different ssps portfolios resulting from the optimisation will be completely robust when uncertainty is examined in terms of different ssp realisation the ranges of the uniform distributions ssp based uncertainty boundaries and an example of calculating the ssp based uncertainty boundaries based on the mid point of the performances across the three ssps are presented in tables 1 and 2 in order to verify if the robustness of ssp uncertainty bounds leads to more robust solutions among the different ssps optimal portfolios of the ssp robustness scenario differing in their robustness score are selected and reiterated in the gcam model the reiteration is applied across the three different ssps new results on the contribution of the technologies to each of the sdgs are retrieved and the goal is to test whether the results of a more robust portfolio are indeed more homogeneous between the different ssps compared to a less robust one this is quantified by measuring the ranges of performances across the three sdgs among the ssps and verifying that they are smaller in case of a more robust portfolio to summarise the information flow into between and out of the two models fig 2 the gcam model is initially fed with socioeconomic data from three ssps and its outputs are used to calculate avoided premature mortality due to air pollution ghg emissions cuts and energy access levels associated with different subsidisation levels for a number of sustainable technologies the latter along with a given total budget are fed into the pa model which calculates the most robust near optimal technology subsidisation portfolios for three timescales 2020 2030 and 2040 per ssp carrying out monte carlo analysis within a 5 uncertainty range for each of the three sdg relevant parameters at the same time an additional subsidy dataset is developed based on the midpoint of the extreme gcam resulting outcomes in terms of cost effectiveness of technology subsidies for sdg progress separately for each technology and sdg impact monte carlo analysis for this extra scenario is performed in a different uncertainty range that is defined by the extreme values of each parameter for each subsidy level and technology finally optimal subsidy levels for each technology from selected portfolios of this scenario of various robustness scores are fed back into gcam in order to validate whether portfolios with higher robustness scores are indeed more robust to the impact of ssp related modelling inputs on outputs in terms of the cost effectiveness of sdg progress 3 validation and discussion 3 1 context of the case study three different ssp datasets o neill et al 2014 have been modelled in gcam for the purposes of a case study focusing on twelve eastern african countries burundi comoros djibouti eritrea ethiopia kenya madagascar rwanda somalia sudan south sudan and uganda aggregated and assessed as one region these datasets include ssp 2 middle of the road ssp 3 regional rivalry and ssp 5 fossil fuelled development specifically the gcam inputs that have been adapted to each of these ssps are global population gdp rate of urbanisation energy and food demand household discount rates agricultural yields energy resource productivity and emission factors 2 2 https github com jgcri gcam doc blob gh pages ssp md riahi et al 2017 table 3 shows the assumed evolution from 2010 to 2040 of the ssp inputs regionally for eastern africa that have most influence on the gcam outputs population gdp and urbanisation the candidate actions are six technological subsidisation pathways revolving around liquefied petroleum gas lpg photovoltaics pv biogas ethanol charcoal and fuelwood i e technologies likely to be adopted in the twelve developing countries of eastern africa based on their action pledges as reflected in their nationally determined contributions while contributing to the three predefined sdgs the gcam generated parameters for the three sdgs showcase the contribution of each technology pathway to each of the objective functions under twenty subsidy levels for each of the four major optimisation problems different time frames are applied and results for the years 2020 2030 and 2040 are extracted the budget constraint starts from 3 5 billion usd at 2015 values in 2020 and increases by 5 per year until 2030 and 2040 3 2 cost effectiveness of technology subsidies and sdg progress the first step in the proposed methodological framework is to simulate future socioeconomic scenarios through the gcam model and translate outputs from each policy scenario into progress parameters relevant to sdgs this is done by applying six different pathways of technology subsidies up to 2040 and then measuring the impact of these subsidies on progress towards each of the three sdgs results on the cost effectiveness of technology subsidies for sdg progress figs 4 6 show that subsidies for biogas systems are the most cost effective for each of the indicators scenarios and years on the contrary subsidies for fuelwood pathways are only reasonably cost effective in the short term for charcoal pathways we observe that cost effectiveness is highly dependent on the invested subsidies when examining progress on the different timescales we see that more subsidies are required for achieving the same impact in the long term at least for energy access and premature mortality indicators in addition in the medium and long term technologies like fuelwood charcoal and ethanol show an even negative impact for the examined subsidies between the different ssps cost effectiveness remains in the same levels for each technology with some differences observed in the short term for biogas and the ghg emissions indicator as well as on charcoal for reducing ghg emissions in the medium and long term overall more subsidies are required to achieve a positive impact in the three sdgs when ssp 5 is realised generally we notice that depending on the scenario and the point in time some technology pathways are more cost effective than others for a specific sdg and some result in negative outcomes and thus not incorporated in figs 4 6 thus the need to identify technological portfolios that are both pareto optimal in terms of contributing simultaneously to the three sdgs and most importantly robust among the different scenarios is more than prominent 3 3 robust subsidy portfolios in each individual ssp the second goal of this research is to identify optimal technology portfolios that are robust in each of the different socioeconomic pathways when probabilistic uncertainty in the model parameters is imposed the results of the portfolio optimisation incorporating the robustness information produced by the monte carlo runs are shown per policy scenario in figs 7 9 where we can see the set of solutions that represent the best possible trade offs between the three sdgs a comparison of results among the different ssps can be easily retrieved in each figure differences on technological performance among the ssps are mainly observed in ssp 5 for the years 2030 and 2040 in more detail ssp 2 can prove more progress friendly in achieving the three sdgs in the short term among all different socioeconomic pathways in the medium and long term ssp 3 leads to better results for the energy access and health criteria while for the goal of reducing emissions ssp 2 performs better ssp 5 features the lowest contribution to the optimisation objectives for all considered time scales which is fairly consistent with its intended narrative ssp5 is characterised by higher incomes and urbanisation which increase access to high quality energy sources such as lpg even without subsidisation due to this more positive counterfactual technology subsidisation in ssp5 is found less cost effective the reasoning on how policy implications affect the adoption of technologies can be found in van de ven et al 2019 3 4 robust subsidy portfolios across all three ssps in this section the goal is to define robust subsidy portfolios for any of the three ssps 2 3 and 5 to evaluate the robustness of the results an analysis that applies the ranges of the gcam simulation outcomes between ssps as its boundaries for robustness is introduced and carried out fig 10 illustrates the pareto fronts of the optimal solutions while giving information on the robustness of the results which is represented by the size of dots the bigger the dots the higher the robustness the behaviour of the optimal solutions across the different timescales shows homogeneity with the analysis provided for the different ssps what is important is to additionally verify if the robustness of ssp uncertainty bounds leads to more robust solutions among the different ssps to achieve that we select two optimal portfolios for each of the six pareto curves of fig 10 one with a higher robustness score and one with a lower robustness score the robustness score indicates the number of monte carlo iterations within which a portfolio remains optimal we re iterate these portfolios in the gcam model to test whether the results of a more robust portfolio are indeed more homogeneous between the different ssps i e that the ranges of sdg performances across the ssps are smaller in case of a more robust portfolio the results shown in table 4 suggest that in the majority of the scenarios we can confirm a smaller output range between ssps if a portfolio with a higher robustness score is chosen the range in outcomes decreases by up to 16 for the baseline scenario in 2020 3 5 empirical findings discussion and results table 5 shows how the realisation of the different ssps affects the total impact and contributions per technology for the most robust pareto optimal subsidy portfolios the robustness of the optimisation process is examined for each of the ssps separately assuming that the performance of the assumed technologies in terms of maximising emission reductions energy access and health benefits is stochastically uncertain consistent with the analysis on the cost effectiveness of the technology subsidies biogas is the technology with the higher participation in the robust portfolios this is evident across the different ssps and timescales the use of lpg and pv systems also have a high contribution to progress in the three sdgs charcoal kilns and ethanol technologies reach their maximum potential which though corresponds to a much lower subsidy and impact level compared to lpg pv and biogas fuelwood is the least attractive technology the policy context on how the different policy scenarios affect subsidisation and effectiveness of the technologies is provided in van de ven et al 2019 here the ssps are assumed as an uncertain set of conditions that affect the performance of every technological subsidy policy and we focus on how the realisation of the different ssps will ultimately affect the participation of technologies in the robust portfolios for the year 2020 technologies show a stable share of participation in the robust portfolios in 2030 a high increase in the contribution of ethanol is observed for ssp 5 where ethanol is subsidised up to 11 in contrast to ssps 2 and 3 where subsidisation for ethanol is less than 2 the realisation of different ssps has an overall bigger effect on sdg progress in 2040 4 conclusions this research presents a two level integration of an integrated assessment model namely gcam and a portfolio analysis model based on augmecon 2 and monte carlo simulations with the aim to provide policymakers with a comprehensive tool to address environmental and energy related issues facilitating the exchange of input data and model results across different methodologies the integration is applied to a case study focusing on technological portfolio optimisation among different plausible socioeconomic futures in eastern africa initially the gcam model is run to simulate future socioeconomic scenarios for six relatively sustainable technologies outputs from each scenario are translated into progress parameters relevant to sdgs and are fed into a pa model the portfolio optimisation model leads to the identification of pareto fronts of optimal portfolios and allows comparison among different ssps the results show how resource allocation must be shared among the technologies to achieve optimal trade offs on the simultaneous achievement of three goals increase of energy access sdg7 reduced exposure to air pollution and avoidance of related mortality sdg3 and mitigation of global warming sdg13 biogas is the technology with the higher participation in the robust portfolios across all ssps and timescales while fuelwood is the least attractive technology a comparison between the ssps shows that differences in the technological performances among the ssps are mainly observed in ssp 5 for the years 2030 and 2040 in order to hedge uncertainty concerning the realisation of different ssps an analysis that applies the ranges of the gcam simulation outcomes between ssps as its boundaries for robustness is introduced the second link between the pa and gcam models is achieved by feeding the gcam model with the results of the portfolio optimisation analysis to verify if the robustness of ssp uncertainty boundaries leads to more robust solutions across the different ssps we selected for each pareto front a portfolio of high robustness and a portfolio with lower robustness score and reiterated these portfolios in the gcam model to retrieve results for the technologies impact for each ssp this is to test whether the resulting ranges of sdg performances between the ssps are smaller in case of a more ssp robust portfolio the results confirm that all portfolios show a smaller output range between ssps if a portfolio with a higher robustness score is chosen showcasing the advantage of the proposed methodology a limitation of our proposed methodology related to the ssp robustness scenario lies on the initial choice of the mid point between the range of ssp results in terms of the impact of each technology on sdg progress for identifying the pareto front of the optimal portfolio the proposed idea is not to imply that the ssps have the same probability of occurrence as no rational assessment of probabilities of various representative scenarios can conclude equal likelihood kinzig and starrett 2003 hence justifying the use of a mean value but rather to assess the uncertainty of the results across the entire spectrum of the resulting values as defined by the individual ssp results even though we do apply the full ssp related uncertainty range when calculating the robustness of the individual portfolios on the pareto front it could be that the election of a different point within the ssp outcome range would yield a slightly different pareto front and hence alter the portfolio outcome however we think that the difficulty to select a justifiable mid value within an ssp related outcome range poses a limitation that is a necessary evil which enables the stochastic identification of optimal technology portfolios against all or most potential outcomes grübler and nakicenovic 2001 of technological subsidisation regardless of our capacity to envisage a future world state that leads to these outcomes and especially since these outcomes are forecasts of a single model allen 2003 nevertheless acknowledging this limitation and understanding the knowledge gaps reflected in this broad spectrum against which the resulting technological subsidisation portfolios are assessed may allow science both to reduce uncertainties in a systematic manner and to convey to policymakers the need to manage and integrate uncertainty into the policymaking process schneider 2003 along those lines it is also important to note that although the policy assumptions of the paper are carefully selected based on published research van de ven et al 2019 policy relevance of results is highly dependent on the assumptions applied when modelling the policy scenarios and these must be carefully considered when interpreting the results without overlooking this limitation the novel methodology introduced in this research can be useful for stakeholders to manage the uncertainty prominent in the future states of the world according to different adaptation and mitigation challenges both core elements of the methodology namely the gcam model and the portfolio analysis model can be extended to include more parameters i e technologies than the ones represented in the current application however we must consider the limitations in terms of time and processing requirements that the solution of more complex problems i e of numerous objective functions more monte carlo simulations may require this limitation can in the future be overcome by using an enhanced algorithm for solving the portfolio analysis model like augmecon r which is a more advanced version of augmecon 2 with significantly faster resolution performance nikas et al 2020 further prospects towards enriching the proposed methodological framework include the selection of variables with implications for a broader set of sdgs and the integration of the models with participatory tools to actively involve stakeholders in the case study participation of stakeholders in policy analysis has been found to improve system understanding and scoping risks associated with climate policy and technologies van vliet et al 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the h2020 european commission project paris reinforce under grant agreement no 820846 and by the spanish ministry of economy and competitiveness mineco through bc3 maría de maeztu excellence accreditation mdm 2017 0714 the sole responsibility for the content of this paper lies with the authors the paper does not necessarily reflect the opinions of the european commission or the spanish government 
26009,recently a stochastic data driven framework was introduced for forecasting uncertain multiscale hydrological and water resources processes e g streamflow urban water demand uwd that uses wavelet decomposition of input data to address multiscale change and stochastics to account for input variable selection parameter and model output uncertainty quilty et al 2019 the former study considered all sources of uncertainty together in contrast this study explores how input variable selection uncertainty and wavelet decomposition impact probabilistic forecasting performance by considering eight variations of this framework that either include ignore wavelet decomposition and varying levels of uncertainty 1 none 2 parameter 3 parameter and model output and 4 input variable selection parameter and model output for a daily uwd forecasting case study in montreal canada substantial improvements in forecasting performance e g 16 30 improvement in the mean interval score was achieved when input variable selection uncertainty and wavelet decomposition were included within the framework keywords uncertainty stochastic data driven models input variable selection wavelet decomposition forecasting list of abbreviations aw average width camels catchment attributes and meteorology for large sample studies canopex canadian model parameter experiment crps continuous ranked probability score ddff data driven forecasting framework ea edgeworth approixmations based conditional mutual information ew sddff ensemble wavelet stochastic data driven forecasting framework is mean interval score mae mean absolute error mlr multiple linear regression nash nash sutcliffe efficiency index pcis partial correlation input selection picp prediction interval coverage probability rmse root mean square error sddff stochastic data driven forecasting framework sov second order volterra series model swddff stochastic wavelet data driven forecasting framework uwd urban water demand 1 introduction accounting for the uncertainty in hydrological and water resources forecasts is recognized as a crucial task in the management planning and operation of hydrological and water resources systems krzysztofowicz 2001 the literature abounds with studies seeking to estimate uncertainties related to input data parameters model structure model output initial conditions etc and their impact on the resulting forecasts see gong et al 2013 beven 2015 and nearing et al 2016 and references therein for many different examples often the goal is to incorporate one or many of these different uncertainty sources into a reliable probabilistic forecasting framework since probabilistic forecasts overcome a major limitation of deterministic points forecasts in that a range of potential outcomes are computed tyralis and koutsoyiannis 2017 2014 rather than a single value they are often adopted for risk assessment and decision making purposes in hydrology and water resources benke et al 2008 ramos et al 2013 vogel 2017 therefore much effort is spent on refining and innovating various probability based forecasting methods to serve this end e g foresti et al 2019 fortin et al 2006 papacharalampous et al 2019c 2019a pappenberger et al 2015 raftery 2016 sharma et al 2019 sivillo et al 1997 thiboult et al 2017 wani et al 2017 xu et al 2018 one such innovative probabilistic forecasting framework which will be explored in detail in this research was very recently introduced in quilty et al 2019 see also quilty 2018 the authors developed a stochastic wavelet based data driven forecasting framework to address three key issues in hydrological and water resources forecasting namely nonlinearity multiscale change and uncertainty their framework uses data driven statistical methods e g artificial neural networks to model nonlinear relationships between input explanatory and target response variables wavelet decomposition to account for multiscale changes in input and or target variables see for instance sivakumar 2017 sec 4 6 for a practical summary of wavelet decomposition and labat 2010 2008 as well nourani et al 2014 for its application in hydrology and water resources and a stochastic approach montanari and koutsoyiannis 2012 to account for uncertainty in each stage of the model design e g input data input variable selection parameters model output the main advantages of the framework introduced in quilty et al 2019 include 1 the holistic consideration of nonlinearity multiscale change and uncertainty within a single framework 2 the ability to generate probabilistic forecasts that account for varying sources of uncertainty in the modelling chain and 3 its data driven nature that allows it to be used in single or multi model ensemble settings since input data can include either raw or transformed time series single model or the output from different forecasting models multi model one feature of the framework from quilty et al 2019 that is particularly novel and that has not been explored in much detail in the literature with the exception of three earlier studies karakaya et al 2016 quilty et al 2016 taormina et al 2016 is that of input variable selection uncertainty input variable selection uncertainty arises from the fact that an input variable selection algorithm may select different input variables when presented with only a potentially randomly drawn subset of the original data set i e target and input variable pairs in the case where a particular process based i e physically based and or conceptual model is used for a forecasting task the required model inputs are pre determined however for a data driven forecasting model input variable selection is of particular importance in guiding the development of accurate and useful models for real world applications since the most useful inputs to use in the model are not known in advance and the selection of redundant or irrelevant inputs can deteriorate model performance creaco et al 2016 galelli et al 2014 hejazi and cai 2009 recent large scale experiments have provided substantial empirical evidence showing that careful input variable selection both improves model performance more often than when it is not used and that its usefulness is problem dependent papacharalampous et al 2018a 2018b 2019b tyralis and papacharalampous 2017 furthermore the selection of which input variables to use in a data driven model is inherently uncertain especially when the input candidate pool is very large in hydrological and water resources forecasting wavelet decomposition is often used to decompose input data time series into a number of sub components i e additional time series that capture multiscale behaviour within the input data these wavelet decomposed time series are then considered as candidate inputs in a data driven model and when used wavelet decomposed inputs have been shown to improve forecast performance fahimi et al 2017 nourani et al 2014 however the number of wavelet decomposed inputs grows linearly with the decomposition level a scale based parameter therefore when wavelet decomposition is considered the candidate input pool can grow very quickly and potentially increase input variable selection uncertainty to date the impact of wavelet decomposition on input variable selection uncertainty has not been studied while input variable selection uncertainty is just beginning to be considered within and outside of hydrological and water resources forecasting karakaya et al 2016 quilty et al 2019 2016 taormina et al 2016 its impact on probabilistic forecasting performance in conjunction with other sources of uncertainty e g parameters and or model output has also not yet been explored therefore the purpose of this study is to use the novel framework from quilty et al 2019 to assess how varying the sources of uncertainty considered within the framework i e input variable selection parameter and model output uncertainty in conjunction with the use of wavelet decomposed input data impacts forecasting performance in this study forecasting performance is evaluated in terms of accuracy and reliability the goal of this research is to explore and answer two questions within the context of the quilty et al 2019 framework 1 can probabilistic forecasting performance be improved by considering input variable selection uncertainty in addition to parameter and model output uncertainty 2 while wavelet decomposition of input data may increase input variable selection uncertainty can it still be used to improve forecasting performance by seeking to answer these two questions this study makes two main contributions to the literature including 1 the first attempt to explore and quantify the impact of input variable selection uncertainty on probabilistic forecasting performance 2 the first attempt to explore and quantify the impact of wavelet decomposition on input variable selection uncertainty and probabilistic forecasting performance these two questions are explored through a real world urban water demand forecasting case study adopted in earlier related studies quilty et al 2019 quilty and adamowski 2018 the hope is that in answering these two questions the importance of input variable selection uncertainty may be better appreciated and its adoption more often considered in hydrological and water resources forecasting the framework and experimental design discussed herein can be used by other researchers or practitioners to explore the added value of adopting input variable selection uncertainty and wavelet decomposed input data in the context of probabilistic data driven forecasting for a variety of hydrological and water resources processes e g streamflow rainfall evaporation the remainder of this study is outlined as follows section 2 places this study in the context of earlier related research in section 3 the main uncertainty sources and methods used in this study are described section 4 provides experimental details concerning the case study section 5 highlights the main results of this study and includes a discussion of their significance and section 6 concludes with a summary of this work as well as future research directions 2 context of current study this study explores the impact of wavelet decomposition and input variable selection uncertainty on forecasting model performance and includes a comparative analysis of the forecasting performance achieved by the frameworks proposed in the authors earlier works quilty et al 2019 quilty and adamowski 2018 by examining their results for the same case study connections between this study and earlier research is highlighted below quilty et al 2019 proposed the stochastic data driven forecasting framework sddff which used a combination of input variable selection methods quilty et al 2016 deterministic data driven models and a stochastic resampling procedure introduced in montanari and koutsoyiannis 2012 for the purpose of generating probabilistic forecasts of any hydrological or water resources process e g streamflow rainfall urban water demand the stochastic resampling procedure referred to as the blueprint by the original authors montanari and koutsoyiannis 2012 can be used to convert any deterministic model process based data driven etc into a stochastic model by quantifying uncertainty at each step in the model design e g input data parameters model output the original authors focussed on process based models and considered uncertainty in input data parameters and model output montanari and koutsoyiannis 2012 in contrast quilty et al 2019 focussed on data driven models and in addition to input data parameters and model output included input variable selection uncertainty due to its importance in data driven modelling quilty et al 2019 further augmented the sddff by considering two different approaches for capturing multiscale behaviour common amongst most hydrological and water resources processes through the use of wavelet decomposition 1 by using wavelet decomposed raw time series as model inputs creating the stochastic wavelet data driven forecasting framework swddff and 2 using wavelet based forecasts generated by the wavelet data driven forecasting framework wddff quilty and adamowski 2018 as model inputs creating the ensemble wavelet stochastic data driven forecasting framework ew sddff the authors considered ew sddff as an ensemble model since it used wavelet based i e wddff forecasts as model inputs while sddff and swddf were considered single models since they used raw or wavelet decomposed raw time series as model inputs the sddff swddff and ew sddff are all based on the same core principle i e input variable selection data driven modelling and a stochastic resampling procedure but differ according to what is used as input data in the respective frameworks see fig 1 in quilty et al 2019 the swddff and ew sddff models that were developped in quilty et al 2019 included three sources of uncertainty input variable selection parameters and model output however the authors did not explore the impact of varying levels of uncertainty on forecast performance to address this gap and to further highlight the importance of including input variable selection uncertainty in the development of probabilistic forecasting models this study in contrast to quilty et al 2019 is concerned with exploring the impact of varying levels of uncertainty on forecasting performance for four different cases 1 none 2 parameters 3 parameters and model output and 4 input variable selection parameters and model output additionally the role of wavelet decomposition is also explored to determine whether wavelet decomposed input data can be used to improve forecast performance even though it may contribute to a higher level of input variable selection uncertainty the swddff is explored in this work instead of the ew sddff as the focus of this study is on single models rather than ensemble models similar to quilty et al 2019 input data uncertainty is ignored in this study as suitable information required for its estimation was not available for the data used in the case study earlier research by the authors quilty et al 2019 quilty and adamowski 2018 focussed on urban water demand forecasting a topic of considerable interest and importance in urban hydrology and water resources management chen et al 2017 donkor et al 2014 herrera et al 2010 house peters and chang 2011 pacchin et al 2019 preciado et al 2019 and especially valuable given the lack of process based models that can be used for this task which in large part is the reason why the literature has focussed on data driven models for addressing this problem this research also adopts the same case study used in quilty and adamowski 2018 and quilty et al 2019 for two main reasons 1 the deterministic models developped in quilty and adamowski 2018 were used to generate their stochastic counterparts i e sddff and swddff in this study and 2 to evaluate the results of this study through a comparison with those found in these earlier studies the next section begins with a brief discussion of the different uncertainty sources discussed throughout this study before moving into the different methods used in this research 3 methods in this section the stochastic wavelet data driven forecasting framework and its relation to its non wavelet based counterpart sddff is presented alongside the various uncertainty sources considered within the framework afterwards details are given on how to estimate these different uncertainty sources as well as how to draw from their related probability distributions when applying the framework in practice 3 1 description of uncertainty sources one of the most important requirements of the swddff and sddff is specifying the sources of uncertainty accounted for in the framework the swddff considers input data input variable selection parameter and model output uncertainty although input data is ignored in this study see also section 3 2 and 3 4 these uncertainty sources are defined according to montanari and koutsoyiannis 2012 and quilty et al 2019 in the next sub section it will be shown how these different uncertainty sources are represented by probability distributions within the swddff input data uncertainty refers to possible measurement errors in the input data that is later used as input to the input variable selection algorithm for consideration in the data driven model input variable selection uncertainty refers to the fact that the input variables selected by a particular input variable selection algorithm is not certain by randomly sampling the target and input variable pairs then performing input variable selection for each random sample one can estimate input variable selection uncertainty according to the different input variable sets generated by this process e g by quantifying uncertainty using the entropy measure shannon 1948 parameter uncertainty is defined as the uncertainty present in the model parameter vector as related to the model structure or calibration method model output uncertainty is related to the input data uncertainty input variable selection uncertainty and model error the latter representing the data driven model s failure to reproduce behaviours of the actual process being forecasted and in most cases is mainly related to model structural error montanari and koutsoyiannis 2012 it can be argued that input variable selection uncertainty may account for model structure uncertainty as different input variable sets automatically lead to different model structures when considering a particular data driven model however while we agree to a certain extent that input variable selection influences model structure when working with data driven models such as artificial neural networks the input variables used within the artificial neural network do not solely define the model structure as the number of hidden neurons and their arrangement activation functions and so on still need to be decided upon therefore in this study we differentiate between input variable selection uncertainty and model structure uncertainty the former accounts for uncertainty inherent in the selection of input variables within a given input variable selection algorithm i e independent of the data driven model while the latter is associated with the specific settings of a particular data driven model s structural design such as the number of hidden neurons in an artificial neural network the type of kernel function in support vector regression the number of rules in artificial neuro fuzzy inference systems the polynomial order in polynomial regression models e g volterra series models wu and kareem 2014 model structural uncertainty is not explicitly estimated in this study but is assumed to be implicitly contained in the model output uncertainty montanari and koutsoyiannis 2012 quilty et al 2019 the next sub section describes the stochastic frameworks explored in this study 3 2 stochastic wavelet data driven forecasting framework the stochastic data driven forecasting framework is the foundation from which the swddff is built while the blueprint by montanari and koutsoyiannis 2012 motivated the sddff since the sddff and the blueprint have been collectively described in sufficient detail elsewhere montanari and koutsoyiannis 2012 quilty 2018 quilty et al 2019 sikorska et al 2015 a brief description of the sddff and its relation to the blueprint is given in section a of the supplementary material since the sddff and the swddff share the same general equation 1 with the exception that swddff uses wavelet decomposed input data only the swddff is described below as wavelet decomposition is an integral component of the swddff it is briefly described before presenting the main swddff equation wavelet decomposition is applied to the input data x via the mapping w x x w where x w represents the wavelet decomposed inputs the wavelet decomposition of the input data maps a vector input x to a matrix of dimension j 1 i e w x r x w r j 1 or an input matrix x of dimension d to a matrix of dimension d j 1 i e w x r d x w r d j 1 where each j j 1 represents a scale for each input data vector and j is the decomposition level the wavelet decomposed inputs at scales j 1 j usually referred to as wavelet coefficients represent changes in averages over a scale τ j 2 j 1 while the remaining wavelet decomposed inputs are referred to as scaling coefficients and represent variations at scales λ j 2 j and higher percival and walden 2000 sec 5 0 the major benefit of using wavelet decomposed inputs instead of the original inputs is that different scales within the input data can be extracted and used in an input variable selection algorithm to identify only the relevant scale based information that is necessary to forecast the target variable which has been shown to be a key factor in improving model performance when forecasting multiscale processes in hydrology and water resources nourani et al 2014 rathinasamy et al 2014 sang 2013 we adopted the best practices discussed in quilty and adamowski 2018 to perform wavelet decomposition of the input data and refer the interested reader to their work for a detailed theoretical background including a discussion of the key features of wavelet decomposition of model inputs and their use in forecasting applications by considering wavelet decomposed input data the swddff can be written as 1 f q q θ x w ω ω f e q s θ x w ω θ x w ω f θ θ x w ω f ω ω x w f x w x w d θ d x w where q is the true variable to be forecasted f q q is the probability density function of the true value of the target variable to be forecasted which quantifies uncertainty in the forecast of the true target variable e q s θ x w ω is the model error which incorporates all uncertainties not explicitly accounted for in eq 1 s is a deterministic data driven model e g artificial neural network with parameter vector θ wavelet decomposed model inputs x w and the binary vector ω that spans the columns x w defining which of the wavelet decomposed inputs were selected during input variable selection and used in the data driven model f e q s θ x w ω θ x w ω is the conditional probability density function of the model error e conditioned on the input data and parameters which quantifies uncertainties not explicitly accounted for in the model f θ θ x w ω is the conditional probability density function of the parameters θ conditioned on the wavelet decomposed input data and selected input variables which quantifies parametric uncertainty f ω ω x w is the conditional probability distribution of the selected input variables conditioned on the wavelet decomposed model inputs which quantifies input variable selection uncertainty ω is the set of all selected input variable sets for a particular input variable selection algorithm see section b in the supplementary material for details and f x w x w is the probability density function of the wavelet decomposed input data which quantifies the uncertainty in the wavelet decomposed input data the sddff is realized by a simple adjustment to eq 1 i e modifying the swddff by using the original raw input data x instead of the wavelet decomposed input data x w the estimation of the integrals in eq 1 is carried out through monte carlo sampling the main focus of the next sub section which describes how the swddff can be applied in practice 3 3 applying the stochastic wavelet data driven forecasting framework here we give the workflow required for using the swddff and sddff in practice which amounts to performing monte carlo sampling to estimate the integrals in eq 1 the workflow builds directly from that given by the blueprint authors montanari and koutsoyiannis 2012 sikorska et al 2015 by considering the addition of input variable selection uncertainty and wavelet decomposition another source of difference between swddff sddff and the original blueprint is the fact that we avoid assuming independence between the model error parameters input variable selection and input data due to the similarity between the workflow of swddff and sddff we only provide details for the former the workflow for the swddff as summarized in fig 1 can be stated as follows 1 a random sample is drawn from the probability density f x x 2 wavelet decomposition is performed on the drawn sample input vector from 1 i e w x x w obtaining an equivalent realization from f x w x w 3 a random sample is drawn from the conditional probability density f ω ω x w 4 a random sample is drawn from the conditional probability density f θ θ x w ω 5 using the sampled information θ x w ω a model output is computed via s θ x w ω 6 for the model output from 5 a random error is picked up from the conditional probability density f e q s θ x w ω θ x w ω and added to s θ x w ω 7 steps 1 to 6 are repeated n times giving n different forecasts of q 8 the probability density f q q is realized by the n forecasts of q if one wished to forgo the wavelet decomposition step and instead adopt the sddff then one solely needs to remove step 2 above substituting x for x w in the remaining steps furthermore if one wanted to estimate uncertainty related to only specific components of swddff or sddff e g parameter uncertainty perhaps to compare the contribution of the different sources of uncertainty as in this study then only those steps in the workflow need to be carried out montanari and koutsoyiannis 2012 again we do not consider input data uncertainty i e step 1 in the swddff workflow primarily to focus on input variable selection uncertainty but also because suitable information on input data uncertainty was not available however future work could consider input data uncertainty next we discuss the details necessary to implement the swddff and sddff by demonstrating how one can estimate section 3 4 and sample section 3 5 from the probability distributions in eq 1 3 4 estimation of the different probability distributions as noted in montanari and koutsoyiannis 2012 a key ingredient to using the blueprint and therefore swddff is the specification of the probability distributions we used the bootstrap efron and tibshirani 1993 henderson 2005 to estimate the different probability distributions i e input variable selection parameters and model error section b 1 of the supplementary material provides information on how the bootstrap is used to estimate the probability distribution for a statistic of interest we chose the bootstrap as it is a popular method that can be used for empirically estimating the probability distribution of a random variable leading to its use in a wide array of hydrological and water resources applications involving resampling and uncertainty estimation erkyihun et al 2016 faghih et al 2017 gupta 2010 hirsch et al 2015 lall and sharma 1996 rustomji and wilkinson 2008 sharma and tiwari 2009 srinivas and srinivasan 2005 it was also suggested as a means to estimate parameter uncertainty in montanari and koutsoyiannis 2012 and was used in sikorska et al 2015 to estimate the conditional probability density function of the model error the key benefits of the bootstrap is that it is non parametric likelihood free and simple to implement as it solely relies on random sampling from a given dataset srivastav et al 2007 wani et al 2017 for those interested in non bootstrap based probability density function estimation methods we recommend that the reader reviews section 3 4 and 6 in montanari and koutsoyiannis 2012 and the references mentioned therein taormina et al 2016 can be reviewed for an interesting information theoretic approach to exploring input variable selection uncertainty for building the bootstrap based probability distributions we rely on quilty et al 2016 for estimating the input variable selection probability distribution the paired bootstrap approach in wan et al 2014 for estimating the parameter probability density functions and the k nearest neighbours bootstrap approach for estimating the model error probability density function sikorska et al 2015 although it is not covered in this study the bootstrap can also be used for estimating the input data probability density function barton et al 2014 freschi et al 2017 xie et al 2016 3 4 1 input variable selection the input variable selection probability distribution f ω ω x w was estimated via the bootstrap by resampling the calibration data set pairs y x w several times where y represents past observations of q i e in the calibration dataset evaluating a given input variable selection algorithm for each resample and storing its result i e the selected input variables in the binary variable ω ω the different ω for each resample were used to infer the empirical input variable selection probability distribution note that the empirical input variable selection probability distribution could have multiple instances where the same input variables are selected for different bootstrap resamples if the input variable selection uncertainty is minimal or null this could lead to the same input variables being selected for each bootstrap resample or in the other extreme each bootstrap resample may lead to completely different selected input variable sets resulting in a flat uniform probability distribution we note that for cases where each input variable selection set i e for a given bootstrap resample is unique it is not to say that each selected input variable in that set is unique when compared to the remaining selected input variable sets in other words even for an input variable selection probability distribution that is flat there may be a single or group of input variable s that are included in each input variable set see section b 2 of the supplementary material for further details on how the bootstrap was used to infer the input variable selection probability distribution 3 4 2 parameter since the parameter uncertainty is conditioned on the input variable selection uncertainty for each unique input variable selection set ω ω the parameter probability density function is estimated through the bootstrap by resampling the calibration data set pairs y x w ω several times where x w ω simply represents that only variables ω 1 have been selected in x w and optimizing the parameter vector for each resample with respect to a deterministic model s note that y x w ω is a simplification of the triple y x w ω where ω ω is held constant for each bootstrap resample when inferring the parameter uncertainty for that particular unique selected input variable set therefore each unique input variable selection set has multiple parameter vectors associated with it see section b 3 of the supplementary material for a schematic that shows the relationship between unique input variable selection sets parameters and model errors the different parameter sets for each ω ω make up the conditional probability density function for the parameters i e f θ θ x w ω note that this conditional probability density function can be computationally intensive to estimate as it requires a sufficient number of bootstrap resamples for each selected input variable set which directly depends on the number of unique selected input variable sets in ω and the number of required bootstrap resamples 3 4 3 model error as the model error is conditioned on both the parameter and input variable selection uncertainty the conditional probability density function for the model error f e q s θ x w ω θ x w ω was obtained by estimating the error e q s θ x w ω on the validation set for the model s optimal parameter vector which was determined by the nash sutcliffe efficiency index nash moriasi et al 2007 associated with each unique input variable set montanari and koutsoyiannis 2012 sikorska et al 2015 therefore a set distribution of model errors was associated with each model attached to the different unique selected input variable sets as noted in montanari and koutsoyiannis 2012 the optimal parameter vector could be exchanged for the parameter vector that provided average or median performance or in the extreme case one could use the model error for each parameter set we did not follow these approaches as earlier experiments demonstrated that the performance using our discussed method was high and reflective of performance that would generally satisfy most real world applications relevant to our case study the caveat of estimating the model error distribution using this approach is that the model error in the validation set should be reflective of the model error which could be expected when running the model in simulation forecast mode which can easily be verified by a test set as done in this study montanari and koutsoyiannis 2012 sikorska et al 2015 it is important to note the distinction between the use of y for the calibration dataset and the use of q in validation mode in keeping with the assumptions of the blueprint during validation mode q is not yet observed at the time of issuing the forecast using s θ x w ω for some new input x while in calibration mode past realizations of q i e y are available since it is on the basis of past realizations that input variables are selected and model parameters estimated below we briefly discuss how we sample from the input variable selection parameter and model error probability distributions in the swddff workflow for the sddff workflow wavelet decomposition of model inputs is excluded and the remaining steps are the same 3 5 sampling from the different probability distributions in order to use the swddff in a forecasting application i e to obtain f q q for a given input x which may be a realization from a test set or newly received information in a real time setting one needs to pass x through the swddff workflow described in fig 1 section 3 3 by sampling from the probability distributions mentioned above section 3 4 1 3 4 2 and 3 4 3 if input data uncertainty is not being considered as in this study then x can be converted to its wavelet decomposed counterpart x w before proceeding with the steps mentioned below after obtaining x w a selected input variable set ω is picked up at random from f ω ω x w second a parameter vector from θ associated with the selected input variable set ω is picked up at random from f θ θ x w ω third s θ x w ω is evaluated resulting in a model output q fourth q is then compared against the validation model outputs for the selected input variable set ω and using the k nearest neighbour bootstrap see sikorska et al 2015 for details a model error is randomly sampled from f e q s θ x w ω θ x w ω and added to q this process is repeated a sufficient number of times and results in f q q for a given input x 4 experimental details we now give details concerning our case study experiment settings i e in terms of estimating and sampling from the different probability distributions in eq 1 along with evaluation metrics used to judge the quality accuracy an reliability of the forecasts produced by swddff 4 1 case study the main objective of this case study is to explore how accurately and reliably the swddff and sddff can forecast a multiscale water resources process e g urban water demand rainfall to demonstrate the potential usefulness of the swddff for obtaining accurate and reliable hydrological and water resources forecasts of a multiscale process we chose a real world case study based on a daily urban water demand uwd dataset from montreal canada that was recently studied by the authors quilty et al 2019 quilty and adamowski 2018 fig 2 shows the wavelet decomposition of montreal s average daily uwd whose multiscale nature manifests in prominent weekly seasonal and annual cycles we explored the swddff for forecasting the average daily uwd at lead times of 1 7 and 14 day s ahead using historical uwd maximum air temperature rainfall and the antecedent precipitation index as model inputs other variables such as day of the week or holiday indicators could be useful for forecasting uwd but were not explored in order to maintain consistency with earlier work quilty et al 2019 quilty and adamowski 2018 since montreal s urban water supply system is the second largest in canada supplying water to around 1 9 million persons statistics canada 2018 it is important that short term changes in uwd be known in advance for optimizing system operations e g pump scheduling reservoir operations planning e g water main maintenance hydrant flow testing and construction e g chlorination temporary servicing for which 1 7 and 14 day ahead forecasts of average daily uwd are useful the authors in quilty and adamowski 2018 developed a number of wavelet wddff and non wavelet data driven forecasts for this dataset for the purpose of demonstrating 1 best practices for wavelet based forecasting and 2 the improvement in accuracy that can be achieved by wavelet based forecasts in comparison to their non wavelet based counterparts we used the wddff and non wavelet based models which we term data driven forecasting framework ddff from this earlier study as benchmarks for the swddff and sddff models developed in this paper the swddff and sddff models were developed by modifying the respective wddff and ddff models from quilty and adamowski 2018 through application of the workflow described in section 3 3 and given in fig 1 in table 1 we list the different models and their properties that were used in quilty and adamowski 2018 along with their swddff and sddff counterparts for details on the original models wddff and ddff we refer the reader to quilty and adamowski 2018 briefly the models shown in table 1 can be described by the following information model i e model name ddff sddff wddff and swddff method i e data driven model used in the framework second order volterra sov and multiple linear regression mlr input variable selection ivs i e input variable selection algorithm used to select input variables for the model edgeworth approximations based conditional mutual information ea and partial correlation input selection pcis wavelet filter i e wavelet filter used in the wddff models least asymetric with 14 coefficients la14 and best localized with 14 coefficients bl14 and decomposition level i e the number of scales j considered in the wavelet decomposition of the inputs for the wddff models it is important to note that while other data driven models e g artificial neural networks support vector regression random forests could have been used we chose to focus only on the data driven models mlr and sov that were used in our earlier related research quilty et al 2019 quilty and adamowski 2018 to enable comparisons between the results of the present study and these earlier studies the advantage of adopting mlr and sov is that each method s parameters can be solved using linear least squares quilty and adamowski 2018 although uwd forecasting is a nonlinear problem house peters and chang 2011 mlr is often used as a linear benchmark adamowski et al 2012 de souza groppo et al 2019 oyebode and ighravwe 2019 mlr has been shown to provide competitive performance with nonlinear data driven models for hydrological and water resources forecasting problems latt and wittenberg 2014 papacharalampous et al 2019d papacharalampous and tyralis 2018 especially when combined with wavelet decomposition mouatadid et al 2019 quilty and adamowski 2018 sov was used as a polynomial regression alternative to mlr while mlr is very popular in hydrology and water resources sov is not used as often despite its high potential for hydrological and water resources forecasting labat et al 1999 prasad et al 2018 which has been especially demonstrated when coupled with wavelet decomposition maheswaran and khosa 2012 2013 2014 section c of our supplementary material provides information on sov the dataset i e consisting of average daily historical uwd daily maximimum air temperature daily rainfall and daily antecendent precipitation index extends over the period february 1999 to decemeber 2010 section d of our supplementary material includes time series plots for the target and input variables as well as scatter plots showing the nonlinear relationships between the different variables a total of 2395 2389 and 2382 calibration 583 validation and 366 test records were included in the 1 7 and 14 day lead time forecasts of the target variable i e the same validation and test records were used for each lead time forecast the non wavelet based models ddff and sddff had a total of 56 inputs that were considered during input variable selection which included 14 days of time lagged time series for each explanatory variable historical uwd daily maximum air temperature daily rainfall and daily antecedent precipitation index the wavelet based models wddff and swddff had a total of 448 inputs that were considered during input variable selection comprised of the same inputs as the non wavelet based models in addition to their wavelet decomposed counterparts at a decomposition level of six j 6 56 6 1 56 448 section d of the supplementary material shows how a lag time of 14 days was selected for the explanatory variables using the informational correlation coefficient linfoot 1957 while section e describes how the candidate pool of model inputs were generated as well as the inputs that were selected for use in ddff and wddff according to quilty and adamowski 2018 all forecasts for lead times 1 7 and 14 days ahead only use explanatory variables measured at the forecast trigger date i e on the date which the forecast is issued or further in the past as far back as 14 days 4 2 model settings it is important to re iterate that the calibration records were used for building the input variable selection and parameter probability distributions thus characterizing their respective sources of uncertainty while the validation set was used for building the model error probability distribution the test set was used to assess the accuracy and reliability of the forecasts since it was not involved in the estimation of the previously mentioned probability distributions allowing one to infer the generalization abilities of the swddff and sddff models and their potential for real world problems this approach is consistent with what was followed by the blueprint authors montanari and koutsoyiannis 2012 sikorska et al 2015 and quilty et al 2019 in our experiments we used a total of 500 bootstrap resamples to develop the different probability distributions input variable selection parameter and model error and drew from the respective probability distributions 500 times n 500 to generate the probability distribution of the target forecast probability density function f q q ten k 10 nearest neighbours were considered in the k nearest neighbour bootstrap for drawing from the model error probability density function during earlier tests we found that these settings provided generally good performance that would be suitable for real world applications relevant to our case study to demonstrate the role and importance of the different sources of uncertainty i e input variable selection parameters and model output on the forecast results we explored four different settings for the swddff and sddff models the four different settings that were explored explicitly considered the following sources of uncertainty 1 none wddff and ddff 2 parameter swddff 1 and sddff 1 3 parameter and model output swddff 2 and sddff 2 and 4 input variable selection parameter and model output swddff 3 and sddff 3 we chose these four settings to compare the performance achieved by the swddff and sddff models when different sources of uncertainty were explicitly accounted for allowing us to discern the relative importance of each source of uncertainty with a focus on the role of input variable selection uncertainty and its impact on forecast accuracy and reliability we note that these sources of uncertainty should not be considered additive montanari and koutsoyiannis 2012 similarly differences between swddff and sddff are useful for assessing the impact of wavelet decomposition on forecasting performance it is relevant to note earlier studies in hydrology and water resources whose models are special cases of the different swddff and sddff models described above sddff 1 is comparable to the bootstrap data driven forecasting models that focussed on estimating parameter uncertainty jia and culver 2006 tiwari and chatterjee 2010 zaier et al 2010 sddff 2 is similar to the bootstrap artificial neural network forecasting models in wang et al 2013 that considered parameter and model output uncertainty swddff 1 is akin to the bootstrap wavelet data driven forecasting models that considered parameter uncertainty belayneh et al 2016 kasiviswanathan et al 2016 tiwari and adamowski 2017 swddff 2 is most alike the bootstrap wavelet artificial neural network forecasting model in wang et al 2013 however the authors used a wavelet based activation function in the hidden layer of the neural network instead of applying wavelet decomposition directly to the input data swddff 3 is the first wavelet based forecasting method that considers input variable selection parameter and model output uncertainty quilty et al 2019 in the next sub section we briefly describe the evaluation metrics used to judge the accuracy and reliability of the swddff and sddff forecasts in our case study 4 3 forecast evaluation metrics to assess the accuracy and reliability of the swddff and sddff models we applied both deterministic and probabilistic performance evaluation metrics commonly adopted in hydrology and water resources studies to measure the accuracy of the different forecasts we used the nash krause and boyle 2005 sutcliffe 1970 root mean square error rmse and mean absolute error mae hauduc et al 2015 legates and mccabe jr 1999 moriasi et al 2007 we considered the continuous ranked probability score crps prediction interval coverage probability picp average prediction interval width aw and mean interval score is boucher et al 2011 del giudice et al 2015 gneiting and raftery 2007 kasiviswanathan et al 2013 solomatine and shrestha 2009 to assess the swddff and sddff forecasts from a probabilistic perspective to supplement these performance evaluation metrics we also used scatter plots and time series plots for a graphical assessment of the different models the nash rmse and mae measure the deviation of the forecast from the observations both nash and rmse are based on the square of errors and are more sensitive to large errors while the mae treats small and large errors equally the crps a useful measure reflecting both forecast sharpness and reliability compares the distribution of the forecast with that of the observations and reduces to the mean absolute error for point forecasts allowing for the direct comparison between point and probabilistic forecasts fortin et al 2006 gneiting and katzfuss 2014 hersbach 2000 pappenberger et al 2015 the picp measures the coverage of the observations within the forecasts prediction intervals which should ideally match that of the prediction interval nominal confidence of 100 1 α we set α at 0 05 in our experiments and is related to the reliability of the forecast wan et al 2014 the optimal value of the picp at this confidence level is 95 the aw simply measures the average width of the prediction intervals and is related to the forecasts sharpness gneiting et al 2007 the is combines both sharpness and reliability into a single metric by considering the width of the uncertainty bound difference between upper and lower prediction intervals in combination with the position of the observation relative to the uncertainty bound bourgin et al 2015 gneiting and raftery 2007 for the swddff and sddff models the deterministic measures nash rmse and mae were calculated by using the mean of the n realizations defining f q q enabling comparisons with the wddff and ddff models while the probabilistic measures crps picp aw and is consider the entire distribution i e all n realizations defining f q q since the swddff 1 and sddff 1 models only account for parameter uncertainty their n forecasts provide an estimate of the variance or uncertainty of only the parameters and not the true value of the target variable however swddff 2 sddff 2 swddff 3 and sddff 3 through their n forecasts include the variance of not only the parameters but also of the future forecasted target variable therefore the former results in confidence intervals while the latter results in prediction intervals which are always wider and encompass the confidence intervals faraway 2014 sec 4 1 since prediction intervals are required to estimate picp aw and is gneiting and raftery 2007 we assess only the swddff 2 sddff 2 swddff 3 and sddff 3 using these measures however the crps can be used more generally to assess an ensemble of forecasts e g whether generated by drawing samples from a stochastic model such as in this study making perturbations to initial conditions tweaking hyper parameters or combining forecasts from different models boucher et al 2015 zamo and naveau 2017 and is therefore used for assessing how well the n realizations from the swddff and sddff models forecast the target variable therefore the crps acts as a useful measure for assessing the impact that the different sources of uncertainty input variable selection parameter and model output have on swddff and sddff accuracy and reliability at the same time since crps reduces to the mae for deterministic forecasts it is a useful metric for comparing the swddff and sddff against the wddff and ddff forecasts in terms of the probabilistic assessment of the swddff and sddff models we prefer reliable forecasts to sharp forecasts since users of forecasts in the form of a probability distribution are often interested in the uncertainty of the next outcome and unreliable forecasts are usually of little use regardless of their sharpness boucher et al 2010 therefore we are most interested in the picp score and then the crps is and aw scores in other words if a forecasting model has the best picp score and relatively close crps is aw and deterministic nash rmse and mae scores when compared to its competitors the model with the best picp score is deemed superior for the task of probabilistic forecasting all models are applied to the test period once and each performance metrics is calculated across the entire test period i e models are not re calibrated for each forecast 5 results and discussion we now turn to the evaluation of the swddff and sddff and their benchmarks wddff and ddff table 2 records the deterministic nash rmse and mae and probabilistic crps picp aw and is forecast evaluation metrics for our uwd forecasting case study in montreal canada our focus is on comparing the swddff and sddff that account for various sources of uncertainty in their forecasts the results in table 2 support our thesis at least for this case study that input variable selection uncertainty and wavelet decomposition are two important factors in improving forecast accuracy and reliability in comparison to forecasts that do not consider either or only one of these tools compare swddff 3 and sddff 3 forecasts against the others in table 2 to support this claim we summarize our main findings below remember that only the test set performances are discussed in this section as we are solely concerned with how the frameworks performed out of sample 1 for each lead time the swddff 3 model exhibited the most reliable forecasts in terms of the picp score ideally the picp should match the prediction interval nominal confidence of 95 therefore in a probabilistic sense the inclusion of input variable selection uncertainty and wavelet decomposition of model inputs were important factors that resulted in the most reliable forecasts for our case study 2 for each lead time the swddff 3 model provided either the best lowest crps or is in conjunction with the best picp score since the crps reduces to the mae for a point forecast swddff and sddff models provided better forecast performance when used probabilistically rather than deterministically since in all cases the crps was lower than the mae scores 3 for each lead time and most performance measures wavelet decomposition of model inputs improved forecast performance over the non wavelet based models i swddff 3 showed a 2 12 13 13 and 33 19 improvement in nash rmse over the sddff 3 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 12 14 13 16 and 18 25 improvement in crps is over the sddff 3 at 1 7 and 14 day lead times respectively iii however there were exceptions for the 7 day lead time a the crps for the swddff 1 model was inferior to that of the sddff 1 however the swddff 3 which accounted for input variable selection uncertainty had the lowest crps at the 7 day lead time b the picp for the swddff 2 model indicated that it was less reliable than the sddff 2 model although it had better nash rmse mae crps is and aw scores regardless the swddff 3 exhibited the best reliability picp 4 input variable selection uncertainty appears to become more important in improving model performance both in terms of deterministic and probabilistic measures at the 7 and 14 day lead times in contrast to the 1 day lead time i swddff 3 showed a 0 1 2 5 and 4 3 improvement in nash rmse over the swddff 2 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 2 0 8 15 and 4 15 improvement in crps is over the swddff 2 at 1 7 and 14 day lead times respectively 5 the combined impact of including both input variable selection uncertainty and wavelet decomposition of model inputs can be gauged by comparing the deterministic and probabilistic forecasting performance of swddff 3 and sddff 2 i swddff 3 showed a 3 13 13 13 and 34 19 improvement in nash rmse over the sddff 2 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 12 16 12 16 and 19 30 improvement in crps is over the sddff 2 at 1 7 and 14 day lead times respectively to assess the impact of wavelet decomposition on input variable selection uncertainty we utilized the entropy measure from information theory shannon 1948 singh 2013 to quantify the uncertainty in the input variable selection probability distribution the higher the entropy the higher the uncertainty if the probability distribution defining input variable selection uncertainty were flat i e a uniform distribution then it would have maximum entropy the entropy of the input variable selection probability distributions for swddff 1 swddff 2 and swddff 3 are 6 215 5 801 and 4 911 while for sddff 1 sddff 2 and sddff 3 they are 0 755 1 797 and 1 557 all entropy measurements are in nats given that a total of 500 random samples were used to estimate input variable selection uncertainty for swddff and sddff the maximum entropy is 6 215 therefore one can conclude by the entropy and performance measures reported in table 2 that wavelet decomposition increases input variable selection uncertainty but in all cases leads to an increase in forecast accuracy and reliability further information on calculating input variable selection uncertainty via entropy including additional discussion on the input variable selection of the swddff and sddff variants can be found in the supplementary material section b 2 and e respectively one may observe that the wddff and ddff models often provided better deterministic performance than their respective swddff and sddff counterparts e g see the 1 day lead time results in table 2 this is easily explained by the fact that these models wddff and ddff did not take into account any form of uncertainty assessment and therefore were biased with their forecasts relying on a single set of parameters and selected input variables likewise there exist cases where the swddff 1 and sddff 1 models performed better than or as nearly good as in a deterministic sense their respective swddff 2 and swddff 3 and sddff 2 and sddff 3 counterparts e g refer again to the 1 day lead time results in table 2 this is met with similar reasoning as above the swddff 1 and sddff 1 forecasts do not explain the variance in their predictions only in their parameters and therefore the mean of their n forecasts are biased as evidenced by the crps furthermore a comparison between the swddff 2 and sddff 2 with their respective swddff 3 and sddff 3 counterparts reveals that the forecasts of the former are also biased since they do not consider the source of error stemming from forecasts produced using different input variable sets which resulted in lower reliability for all cases i e for the swddff 2 and sddff 2 models the last point is worth re iterating in each case that input variable selection uncertainty was considered whether wavelets were used or not forecast reliability improved as well as either or both the crps and is this performance was further increased by incorporating wavelet decomposition of model inputs we believe that this is evidence supporting the importance of including input variable selection uncertainty and wavelet decomposed model inputs in stochastic data driven forecasting of multiscale processes such as those commonly encountered in hydrology and water resources furthermore by including input variable selection uncertainty alongside parameter and model output uncertainty swddff swddff 3 in particular improved upon some of the most advanced wavelet based forecasting models e g those comparable to swddff 1 and swddf 2 see the end of section 4 2 in the hydrological and water resources literature that have either included an assessment of only parameter uncertainty belayneh et al 2016 kasiviswanathan et al 2016 sehgal et al 2014 tiwari and adamowski 2017 or parameter and model output uncertainty bachour et al 2016 maslova et al 2016 wang et al 2013 while there have been some wavelet based forecasting models that have focussed on combining multiple wavelet based forecasts to help address forecasting uncertainty barzegar et al 2018 2017 rathinasamy et al 2013 they were not concerned with generating and evaluating probabilistic forecasts but rather an average pooled ensemble forecast however performance of the swddff 3 in this study did not outperform the ew sddff from quilty et al 2019 as explained below due to the use of the same case study the results of the swddff can also be compared to the ew sddff framework from quilty et al 2019 the best ew sddff outperformed the swddff across all performance metrics rmse nash mae crps picp aw and is however this is to be expected as the ew sddff is based on multiple wddff models i e it is a multi wavelet approach while the swddff is based on only a single wddff therefore the ew sddff benefits from increased variability present in the model inputs due to its multi wavelet nature i e being based on multiple wavelet filters decomposition levels etc that the swddff does not include similarly an earlier study by rathinasamy et al 2013 demonstrated that a multi wavelet approach led to improved streamflow forecasting performance at multiple temporal scales daily weekly and monthly finding that coupling multiple forecasting models incorporating different decomposition levels and wavelet filters were better able to capture different behaviours of the target time series e g low medium and high flows that were missed by single wavelet models given that the swddff 3 models provided the best overall performance we now compare its performance against sddff 3 using different graphical tools note that in these figures swddff and sddff are equivalent to swddff 3 and sddff 3 respectively we notice in fig 3 which compares the crps versus the number of resamples n included in f q q that the crps steadily decreases as the number of resamples n is increased there is a noticeable drop in crps around n 50 but the crps continues to descend after this point this is an indication that n 500 appears to be more than sufficient for stabilizing performance of the swddff and sddff models for this case study the scatter plots for the observed versus each of the n forecasts in f q q in fig 4 show that the swddff forecasts are more tightly centered on the bisector line indicating a better fit than the sddff forecasts the sddff forecasts tend to have larger outliers for medium and higher flows than the swddff time series plots for the mean as well as for the 0 025 and 0 975 quantiles of the f q q forecasts versus the observations in figs 5 and 6 complement fig 4 by demonstrating that the swddff and sddff forecasts are not only accurate but successful at matching the different scales in the uwd time series that change through time e g at the weekly seasonal and annual scales for each lead time the swddff and sddff captured the weekly cycle during lower flows october 2009 may 2010 reasonably well in terms of its mean forecast and its prediction intervals i e 0 025 and 0 975 quantiles during the seasonal drop in uwd august october 2009 the swddff and sddff had lower accuracy but were still reliable across all lead times when the seasonal demand picked up may 2010 onwards the 1 day lead time forecasts were both accurate and reliable however the accuracy and reliability of the mean forecast and its prediction intervals were noticeably worse for 7 and 14 day lead times in general the prediction intervals for the swddff can be deemed superior to those for the sddff visually and statistically as there is not only an improvement in sharpness see the aw scores but also in reliability see picp cprs and is however while both forecasting models tended to exhibit strong reliability their upper prediction interval tended to overestimate lower flows especially at longer lead times 7 and 14 days ahead in general the high level of accuracy and reliability of the swddff and sddff indicate that both methods could be particularly useful for optimizing pumping operations within urban water supply systems while the improved accuracy of the swddff over the sddff at weekly 7 day and bi weekly 14 day lead times could be especially useful for evaluating water restrictions during drought or emergency situations in addition to planning construction and maintenance activities within water supply systems since the forecasts generated by both swddff and sddff can be interpreted probabilistically they may also prove to be useful in estimating risk reliability resilience and vulnerability of water supply systems yung et al 2011 finally to demonstrate the generalization capabilities of the swddff and sddff for other forecasting applications important to hydrology and water resources e g rainfall streamflow both frameworks should be explored using large scale experimental data sets such as those adopted in earlier studies e g kratzert et al 2018 papacharalampous et al 2019b tyralis and koutsoyiannis 2017 tyralis and papacharalampous 2017 2018 xu et al 2018 in particular the catchment attributes and meteorology for large sample studies camels addor et al 2017 and canadian model parameter experiment canopex arsenault et al 2016 are open source data sets that could be used for these purposes 6 summary and conclusions this study explored the recently introduced stochastic wavelet data driven forecasting framework swddff quilty et al 2019 for forecasting uncertain multiscale hydrological and water resources processes e g urban water demand the swddff uses wavelet decomposition to extract important multiscale information from the model inputs passes this information to a potentially nonlinear data driven model artificial neural networks support vector regression etc to generate forecasts of the target variable and accounts for several important uncertainty sources input data input variable selection parameter model output etc resulting in a probabilistic forecast the goal of this study was to answer two important questions often neglected in data driven hydrological and water resources forecasting can input variable selection uncertainty improve forecast performance i e accuracy and reliability when included alongside parameter and model output uncertainty despite wavelet decomposition of input data increasing input variable selection uncertainty can it still be used to improve forecast performance these two questions were explored by varying the level of uncertainty none parameter parameter and model output and input variable selection parameter and model output included in the swddff and its non wavelet based counterpart sddff results from our 1 7 and 14 day ahead urban water demand forecasting case study in montreal canada answered both questions in the affirmative by including input variable selection alongside parameter and model output uncertainties the swddff demonstrated an improvement of 3 in the nash sutcliffe efficiency index and 15 in the mean interval score at the 14 day lead time over the case when swddff only considered parameter and model output uncertainties although wavelet decomposition of model inputs was shown to increase input variable selection uncertainty according to the entropy measure 4 911 nats for swddff vs 1 557 nats for sddff it resulted in an increase of 33 in the nash sutcliffe efficiency index and 25 in the mean interval score at the 14 day lead time the main contributions of this research include 1 the first study to explore and quantify the impact of input variable selection uncertainty on probabilistic forecasting performance and 2 the first study to demonstrate that the increase in input variable selection uncertainty due to wavelet decomposition of input data may be a reasonable expense to incur in order to gain a significant increase in probabilistic forecasting performance our hope is that the swddff may serve as a useful tool for those interested in building probabilistic wavelet based data driven forecasting models to address the important issues of nonlinearity multiscale change and uncertainty in hydrological and water resources forecasting we note that our study was limited in a number of ways 1 we only used a single case study 2 we did not consider a wide variety of wavelet filters and decomposition levels during wavelet decomposition 3 only a small number of data driven models and input variable selection methods were studied and 4 we did not consider input data uncertainty in order to focus on input variable selection uncertainty we believe that these limitations are reasonable for the present study as our main goal was to determine whether including input variable selection uncertainty in addition to parameter and model output uncertainty in swddff and sddff as well as using wavelet decomposed input data i e using swddff instead of sddff led to improvements in forecast accuracy and reliability by evaluating the two frameworks in a case study explored earlier by the authors quilty et al 2019 quilty and adamowski 2018 however different research avenues can be explored in future studies such as 1 testing swddff and sddff on a larger number of hydrological and water resources processes e g streamflow drought evaporation such as those included in recent open source large scale data sets e g camels addor et al 2017 and canopex arsenault et al 2016 as well as time series stemming from different domains 2 the evaluation of a number of different wavelet families and decomposition levels 3 exploring a wider range of data driven models and input variable selection methods 4 including input data uncertainty alongside input variable selection parameter and model uncertainty 5 comparing the performance of swddff and sddff against process based models such as those used in the original blueprint for different hydrological and water resources processes 6 testing the suitability of different methods such as meta heuristic optimization approaches for generating the various probability density functions input variable selection parameters etc 7 comparing swddff against sddff coupled with different pre processing methods e g principal component analysis independent component analysis auto encoders wetzel 2017 declaration of competing interest none acknowledgements we thank the editor and the three reviewers for their many suggestions that helped to improve the present paper we extend our gratitude to the city of montreal for providing the urban water demand data used in our case study urban water demand data may be requested from the city of montreal via http donnees ville montreal qc ca the meteorological data used in this study can be obtained from environment canada at climate weather gc ca funding for this research was provided by natural sciences and engineering research council nserc discovery and accelerator grants grant numbers g219643 nserc rgpin 2015 05554 g241149 nserc rgpas 477886 15 held by jan adamowski the funding agency had no involvement in this study whatsoever study design in the collection analysis or interpretation of data in the writing of this paper and in the decision to submit the paper for publication appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104718 
26009,recently a stochastic data driven framework was introduced for forecasting uncertain multiscale hydrological and water resources processes e g streamflow urban water demand uwd that uses wavelet decomposition of input data to address multiscale change and stochastics to account for input variable selection parameter and model output uncertainty quilty et al 2019 the former study considered all sources of uncertainty together in contrast this study explores how input variable selection uncertainty and wavelet decomposition impact probabilistic forecasting performance by considering eight variations of this framework that either include ignore wavelet decomposition and varying levels of uncertainty 1 none 2 parameter 3 parameter and model output and 4 input variable selection parameter and model output for a daily uwd forecasting case study in montreal canada substantial improvements in forecasting performance e g 16 30 improvement in the mean interval score was achieved when input variable selection uncertainty and wavelet decomposition were included within the framework keywords uncertainty stochastic data driven models input variable selection wavelet decomposition forecasting list of abbreviations aw average width camels catchment attributes and meteorology for large sample studies canopex canadian model parameter experiment crps continuous ranked probability score ddff data driven forecasting framework ea edgeworth approixmations based conditional mutual information ew sddff ensemble wavelet stochastic data driven forecasting framework is mean interval score mae mean absolute error mlr multiple linear regression nash nash sutcliffe efficiency index pcis partial correlation input selection picp prediction interval coverage probability rmse root mean square error sddff stochastic data driven forecasting framework sov second order volterra series model swddff stochastic wavelet data driven forecasting framework uwd urban water demand 1 introduction accounting for the uncertainty in hydrological and water resources forecasts is recognized as a crucial task in the management planning and operation of hydrological and water resources systems krzysztofowicz 2001 the literature abounds with studies seeking to estimate uncertainties related to input data parameters model structure model output initial conditions etc and their impact on the resulting forecasts see gong et al 2013 beven 2015 and nearing et al 2016 and references therein for many different examples often the goal is to incorporate one or many of these different uncertainty sources into a reliable probabilistic forecasting framework since probabilistic forecasts overcome a major limitation of deterministic points forecasts in that a range of potential outcomes are computed tyralis and koutsoyiannis 2017 2014 rather than a single value they are often adopted for risk assessment and decision making purposes in hydrology and water resources benke et al 2008 ramos et al 2013 vogel 2017 therefore much effort is spent on refining and innovating various probability based forecasting methods to serve this end e g foresti et al 2019 fortin et al 2006 papacharalampous et al 2019c 2019a pappenberger et al 2015 raftery 2016 sharma et al 2019 sivillo et al 1997 thiboult et al 2017 wani et al 2017 xu et al 2018 one such innovative probabilistic forecasting framework which will be explored in detail in this research was very recently introduced in quilty et al 2019 see also quilty 2018 the authors developed a stochastic wavelet based data driven forecasting framework to address three key issues in hydrological and water resources forecasting namely nonlinearity multiscale change and uncertainty their framework uses data driven statistical methods e g artificial neural networks to model nonlinear relationships between input explanatory and target response variables wavelet decomposition to account for multiscale changes in input and or target variables see for instance sivakumar 2017 sec 4 6 for a practical summary of wavelet decomposition and labat 2010 2008 as well nourani et al 2014 for its application in hydrology and water resources and a stochastic approach montanari and koutsoyiannis 2012 to account for uncertainty in each stage of the model design e g input data input variable selection parameters model output the main advantages of the framework introduced in quilty et al 2019 include 1 the holistic consideration of nonlinearity multiscale change and uncertainty within a single framework 2 the ability to generate probabilistic forecasts that account for varying sources of uncertainty in the modelling chain and 3 its data driven nature that allows it to be used in single or multi model ensemble settings since input data can include either raw or transformed time series single model or the output from different forecasting models multi model one feature of the framework from quilty et al 2019 that is particularly novel and that has not been explored in much detail in the literature with the exception of three earlier studies karakaya et al 2016 quilty et al 2016 taormina et al 2016 is that of input variable selection uncertainty input variable selection uncertainty arises from the fact that an input variable selection algorithm may select different input variables when presented with only a potentially randomly drawn subset of the original data set i e target and input variable pairs in the case where a particular process based i e physically based and or conceptual model is used for a forecasting task the required model inputs are pre determined however for a data driven forecasting model input variable selection is of particular importance in guiding the development of accurate and useful models for real world applications since the most useful inputs to use in the model are not known in advance and the selection of redundant or irrelevant inputs can deteriorate model performance creaco et al 2016 galelli et al 2014 hejazi and cai 2009 recent large scale experiments have provided substantial empirical evidence showing that careful input variable selection both improves model performance more often than when it is not used and that its usefulness is problem dependent papacharalampous et al 2018a 2018b 2019b tyralis and papacharalampous 2017 furthermore the selection of which input variables to use in a data driven model is inherently uncertain especially when the input candidate pool is very large in hydrological and water resources forecasting wavelet decomposition is often used to decompose input data time series into a number of sub components i e additional time series that capture multiscale behaviour within the input data these wavelet decomposed time series are then considered as candidate inputs in a data driven model and when used wavelet decomposed inputs have been shown to improve forecast performance fahimi et al 2017 nourani et al 2014 however the number of wavelet decomposed inputs grows linearly with the decomposition level a scale based parameter therefore when wavelet decomposition is considered the candidate input pool can grow very quickly and potentially increase input variable selection uncertainty to date the impact of wavelet decomposition on input variable selection uncertainty has not been studied while input variable selection uncertainty is just beginning to be considered within and outside of hydrological and water resources forecasting karakaya et al 2016 quilty et al 2019 2016 taormina et al 2016 its impact on probabilistic forecasting performance in conjunction with other sources of uncertainty e g parameters and or model output has also not yet been explored therefore the purpose of this study is to use the novel framework from quilty et al 2019 to assess how varying the sources of uncertainty considered within the framework i e input variable selection parameter and model output uncertainty in conjunction with the use of wavelet decomposed input data impacts forecasting performance in this study forecasting performance is evaluated in terms of accuracy and reliability the goal of this research is to explore and answer two questions within the context of the quilty et al 2019 framework 1 can probabilistic forecasting performance be improved by considering input variable selection uncertainty in addition to parameter and model output uncertainty 2 while wavelet decomposition of input data may increase input variable selection uncertainty can it still be used to improve forecasting performance by seeking to answer these two questions this study makes two main contributions to the literature including 1 the first attempt to explore and quantify the impact of input variable selection uncertainty on probabilistic forecasting performance 2 the first attempt to explore and quantify the impact of wavelet decomposition on input variable selection uncertainty and probabilistic forecasting performance these two questions are explored through a real world urban water demand forecasting case study adopted in earlier related studies quilty et al 2019 quilty and adamowski 2018 the hope is that in answering these two questions the importance of input variable selection uncertainty may be better appreciated and its adoption more often considered in hydrological and water resources forecasting the framework and experimental design discussed herein can be used by other researchers or practitioners to explore the added value of adopting input variable selection uncertainty and wavelet decomposed input data in the context of probabilistic data driven forecasting for a variety of hydrological and water resources processes e g streamflow rainfall evaporation the remainder of this study is outlined as follows section 2 places this study in the context of earlier related research in section 3 the main uncertainty sources and methods used in this study are described section 4 provides experimental details concerning the case study section 5 highlights the main results of this study and includes a discussion of their significance and section 6 concludes with a summary of this work as well as future research directions 2 context of current study this study explores the impact of wavelet decomposition and input variable selection uncertainty on forecasting model performance and includes a comparative analysis of the forecasting performance achieved by the frameworks proposed in the authors earlier works quilty et al 2019 quilty and adamowski 2018 by examining their results for the same case study connections between this study and earlier research is highlighted below quilty et al 2019 proposed the stochastic data driven forecasting framework sddff which used a combination of input variable selection methods quilty et al 2016 deterministic data driven models and a stochastic resampling procedure introduced in montanari and koutsoyiannis 2012 for the purpose of generating probabilistic forecasts of any hydrological or water resources process e g streamflow rainfall urban water demand the stochastic resampling procedure referred to as the blueprint by the original authors montanari and koutsoyiannis 2012 can be used to convert any deterministic model process based data driven etc into a stochastic model by quantifying uncertainty at each step in the model design e g input data parameters model output the original authors focussed on process based models and considered uncertainty in input data parameters and model output montanari and koutsoyiannis 2012 in contrast quilty et al 2019 focussed on data driven models and in addition to input data parameters and model output included input variable selection uncertainty due to its importance in data driven modelling quilty et al 2019 further augmented the sddff by considering two different approaches for capturing multiscale behaviour common amongst most hydrological and water resources processes through the use of wavelet decomposition 1 by using wavelet decomposed raw time series as model inputs creating the stochastic wavelet data driven forecasting framework swddff and 2 using wavelet based forecasts generated by the wavelet data driven forecasting framework wddff quilty and adamowski 2018 as model inputs creating the ensemble wavelet stochastic data driven forecasting framework ew sddff the authors considered ew sddff as an ensemble model since it used wavelet based i e wddff forecasts as model inputs while sddff and swddf were considered single models since they used raw or wavelet decomposed raw time series as model inputs the sddff swddff and ew sddff are all based on the same core principle i e input variable selection data driven modelling and a stochastic resampling procedure but differ according to what is used as input data in the respective frameworks see fig 1 in quilty et al 2019 the swddff and ew sddff models that were developped in quilty et al 2019 included three sources of uncertainty input variable selection parameters and model output however the authors did not explore the impact of varying levels of uncertainty on forecast performance to address this gap and to further highlight the importance of including input variable selection uncertainty in the development of probabilistic forecasting models this study in contrast to quilty et al 2019 is concerned with exploring the impact of varying levels of uncertainty on forecasting performance for four different cases 1 none 2 parameters 3 parameters and model output and 4 input variable selection parameters and model output additionally the role of wavelet decomposition is also explored to determine whether wavelet decomposed input data can be used to improve forecast performance even though it may contribute to a higher level of input variable selection uncertainty the swddff is explored in this work instead of the ew sddff as the focus of this study is on single models rather than ensemble models similar to quilty et al 2019 input data uncertainty is ignored in this study as suitable information required for its estimation was not available for the data used in the case study earlier research by the authors quilty et al 2019 quilty and adamowski 2018 focussed on urban water demand forecasting a topic of considerable interest and importance in urban hydrology and water resources management chen et al 2017 donkor et al 2014 herrera et al 2010 house peters and chang 2011 pacchin et al 2019 preciado et al 2019 and especially valuable given the lack of process based models that can be used for this task which in large part is the reason why the literature has focussed on data driven models for addressing this problem this research also adopts the same case study used in quilty and adamowski 2018 and quilty et al 2019 for two main reasons 1 the deterministic models developped in quilty and adamowski 2018 were used to generate their stochastic counterparts i e sddff and swddff in this study and 2 to evaluate the results of this study through a comparison with those found in these earlier studies the next section begins with a brief discussion of the different uncertainty sources discussed throughout this study before moving into the different methods used in this research 3 methods in this section the stochastic wavelet data driven forecasting framework and its relation to its non wavelet based counterpart sddff is presented alongside the various uncertainty sources considered within the framework afterwards details are given on how to estimate these different uncertainty sources as well as how to draw from their related probability distributions when applying the framework in practice 3 1 description of uncertainty sources one of the most important requirements of the swddff and sddff is specifying the sources of uncertainty accounted for in the framework the swddff considers input data input variable selection parameter and model output uncertainty although input data is ignored in this study see also section 3 2 and 3 4 these uncertainty sources are defined according to montanari and koutsoyiannis 2012 and quilty et al 2019 in the next sub section it will be shown how these different uncertainty sources are represented by probability distributions within the swddff input data uncertainty refers to possible measurement errors in the input data that is later used as input to the input variable selection algorithm for consideration in the data driven model input variable selection uncertainty refers to the fact that the input variables selected by a particular input variable selection algorithm is not certain by randomly sampling the target and input variable pairs then performing input variable selection for each random sample one can estimate input variable selection uncertainty according to the different input variable sets generated by this process e g by quantifying uncertainty using the entropy measure shannon 1948 parameter uncertainty is defined as the uncertainty present in the model parameter vector as related to the model structure or calibration method model output uncertainty is related to the input data uncertainty input variable selection uncertainty and model error the latter representing the data driven model s failure to reproduce behaviours of the actual process being forecasted and in most cases is mainly related to model structural error montanari and koutsoyiannis 2012 it can be argued that input variable selection uncertainty may account for model structure uncertainty as different input variable sets automatically lead to different model structures when considering a particular data driven model however while we agree to a certain extent that input variable selection influences model structure when working with data driven models such as artificial neural networks the input variables used within the artificial neural network do not solely define the model structure as the number of hidden neurons and their arrangement activation functions and so on still need to be decided upon therefore in this study we differentiate between input variable selection uncertainty and model structure uncertainty the former accounts for uncertainty inherent in the selection of input variables within a given input variable selection algorithm i e independent of the data driven model while the latter is associated with the specific settings of a particular data driven model s structural design such as the number of hidden neurons in an artificial neural network the type of kernel function in support vector regression the number of rules in artificial neuro fuzzy inference systems the polynomial order in polynomial regression models e g volterra series models wu and kareem 2014 model structural uncertainty is not explicitly estimated in this study but is assumed to be implicitly contained in the model output uncertainty montanari and koutsoyiannis 2012 quilty et al 2019 the next sub section describes the stochastic frameworks explored in this study 3 2 stochastic wavelet data driven forecasting framework the stochastic data driven forecasting framework is the foundation from which the swddff is built while the blueprint by montanari and koutsoyiannis 2012 motivated the sddff since the sddff and the blueprint have been collectively described in sufficient detail elsewhere montanari and koutsoyiannis 2012 quilty 2018 quilty et al 2019 sikorska et al 2015 a brief description of the sddff and its relation to the blueprint is given in section a of the supplementary material since the sddff and the swddff share the same general equation 1 with the exception that swddff uses wavelet decomposed input data only the swddff is described below as wavelet decomposition is an integral component of the swddff it is briefly described before presenting the main swddff equation wavelet decomposition is applied to the input data x via the mapping w x x w where x w represents the wavelet decomposed inputs the wavelet decomposition of the input data maps a vector input x to a matrix of dimension j 1 i e w x r x w r j 1 or an input matrix x of dimension d to a matrix of dimension d j 1 i e w x r d x w r d j 1 where each j j 1 represents a scale for each input data vector and j is the decomposition level the wavelet decomposed inputs at scales j 1 j usually referred to as wavelet coefficients represent changes in averages over a scale τ j 2 j 1 while the remaining wavelet decomposed inputs are referred to as scaling coefficients and represent variations at scales λ j 2 j and higher percival and walden 2000 sec 5 0 the major benefit of using wavelet decomposed inputs instead of the original inputs is that different scales within the input data can be extracted and used in an input variable selection algorithm to identify only the relevant scale based information that is necessary to forecast the target variable which has been shown to be a key factor in improving model performance when forecasting multiscale processes in hydrology and water resources nourani et al 2014 rathinasamy et al 2014 sang 2013 we adopted the best practices discussed in quilty and adamowski 2018 to perform wavelet decomposition of the input data and refer the interested reader to their work for a detailed theoretical background including a discussion of the key features of wavelet decomposition of model inputs and their use in forecasting applications by considering wavelet decomposed input data the swddff can be written as 1 f q q θ x w ω ω f e q s θ x w ω θ x w ω f θ θ x w ω f ω ω x w f x w x w d θ d x w where q is the true variable to be forecasted f q q is the probability density function of the true value of the target variable to be forecasted which quantifies uncertainty in the forecast of the true target variable e q s θ x w ω is the model error which incorporates all uncertainties not explicitly accounted for in eq 1 s is a deterministic data driven model e g artificial neural network with parameter vector θ wavelet decomposed model inputs x w and the binary vector ω that spans the columns x w defining which of the wavelet decomposed inputs were selected during input variable selection and used in the data driven model f e q s θ x w ω θ x w ω is the conditional probability density function of the model error e conditioned on the input data and parameters which quantifies uncertainties not explicitly accounted for in the model f θ θ x w ω is the conditional probability density function of the parameters θ conditioned on the wavelet decomposed input data and selected input variables which quantifies parametric uncertainty f ω ω x w is the conditional probability distribution of the selected input variables conditioned on the wavelet decomposed model inputs which quantifies input variable selection uncertainty ω is the set of all selected input variable sets for a particular input variable selection algorithm see section b in the supplementary material for details and f x w x w is the probability density function of the wavelet decomposed input data which quantifies the uncertainty in the wavelet decomposed input data the sddff is realized by a simple adjustment to eq 1 i e modifying the swddff by using the original raw input data x instead of the wavelet decomposed input data x w the estimation of the integrals in eq 1 is carried out through monte carlo sampling the main focus of the next sub section which describes how the swddff can be applied in practice 3 3 applying the stochastic wavelet data driven forecasting framework here we give the workflow required for using the swddff and sddff in practice which amounts to performing monte carlo sampling to estimate the integrals in eq 1 the workflow builds directly from that given by the blueprint authors montanari and koutsoyiannis 2012 sikorska et al 2015 by considering the addition of input variable selection uncertainty and wavelet decomposition another source of difference between swddff sddff and the original blueprint is the fact that we avoid assuming independence between the model error parameters input variable selection and input data due to the similarity between the workflow of swddff and sddff we only provide details for the former the workflow for the swddff as summarized in fig 1 can be stated as follows 1 a random sample is drawn from the probability density f x x 2 wavelet decomposition is performed on the drawn sample input vector from 1 i e w x x w obtaining an equivalent realization from f x w x w 3 a random sample is drawn from the conditional probability density f ω ω x w 4 a random sample is drawn from the conditional probability density f θ θ x w ω 5 using the sampled information θ x w ω a model output is computed via s θ x w ω 6 for the model output from 5 a random error is picked up from the conditional probability density f e q s θ x w ω θ x w ω and added to s θ x w ω 7 steps 1 to 6 are repeated n times giving n different forecasts of q 8 the probability density f q q is realized by the n forecasts of q if one wished to forgo the wavelet decomposition step and instead adopt the sddff then one solely needs to remove step 2 above substituting x for x w in the remaining steps furthermore if one wanted to estimate uncertainty related to only specific components of swddff or sddff e g parameter uncertainty perhaps to compare the contribution of the different sources of uncertainty as in this study then only those steps in the workflow need to be carried out montanari and koutsoyiannis 2012 again we do not consider input data uncertainty i e step 1 in the swddff workflow primarily to focus on input variable selection uncertainty but also because suitable information on input data uncertainty was not available however future work could consider input data uncertainty next we discuss the details necessary to implement the swddff and sddff by demonstrating how one can estimate section 3 4 and sample section 3 5 from the probability distributions in eq 1 3 4 estimation of the different probability distributions as noted in montanari and koutsoyiannis 2012 a key ingredient to using the blueprint and therefore swddff is the specification of the probability distributions we used the bootstrap efron and tibshirani 1993 henderson 2005 to estimate the different probability distributions i e input variable selection parameters and model error section b 1 of the supplementary material provides information on how the bootstrap is used to estimate the probability distribution for a statistic of interest we chose the bootstrap as it is a popular method that can be used for empirically estimating the probability distribution of a random variable leading to its use in a wide array of hydrological and water resources applications involving resampling and uncertainty estimation erkyihun et al 2016 faghih et al 2017 gupta 2010 hirsch et al 2015 lall and sharma 1996 rustomji and wilkinson 2008 sharma and tiwari 2009 srinivas and srinivasan 2005 it was also suggested as a means to estimate parameter uncertainty in montanari and koutsoyiannis 2012 and was used in sikorska et al 2015 to estimate the conditional probability density function of the model error the key benefits of the bootstrap is that it is non parametric likelihood free and simple to implement as it solely relies on random sampling from a given dataset srivastav et al 2007 wani et al 2017 for those interested in non bootstrap based probability density function estimation methods we recommend that the reader reviews section 3 4 and 6 in montanari and koutsoyiannis 2012 and the references mentioned therein taormina et al 2016 can be reviewed for an interesting information theoretic approach to exploring input variable selection uncertainty for building the bootstrap based probability distributions we rely on quilty et al 2016 for estimating the input variable selection probability distribution the paired bootstrap approach in wan et al 2014 for estimating the parameter probability density functions and the k nearest neighbours bootstrap approach for estimating the model error probability density function sikorska et al 2015 although it is not covered in this study the bootstrap can also be used for estimating the input data probability density function barton et al 2014 freschi et al 2017 xie et al 2016 3 4 1 input variable selection the input variable selection probability distribution f ω ω x w was estimated via the bootstrap by resampling the calibration data set pairs y x w several times where y represents past observations of q i e in the calibration dataset evaluating a given input variable selection algorithm for each resample and storing its result i e the selected input variables in the binary variable ω ω the different ω for each resample were used to infer the empirical input variable selection probability distribution note that the empirical input variable selection probability distribution could have multiple instances where the same input variables are selected for different bootstrap resamples if the input variable selection uncertainty is minimal or null this could lead to the same input variables being selected for each bootstrap resample or in the other extreme each bootstrap resample may lead to completely different selected input variable sets resulting in a flat uniform probability distribution we note that for cases where each input variable selection set i e for a given bootstrap resample is unique it is not to say that each selected input variable in that set is unique when compared to the remaining selected input variable sets in other words even for an input variable selection probability distribution that is flat there may be a single or group of input variable s that are included in each input variable set see section b 2 of the supplementary material for further details on how the bootstrap was used to infer the input variable selection probability distribution 3 4 2 parameter since the parameter uncertainty is conditioned on the input variable selection uncertainty for each unique input variable selection set ω ω the parameter probability density function is estimated through the bootstrap by resampling the calibration data set pairs y x w ω several times where x w ω simply represents that only variables ω 1 have been selected in x w and optimizing the parameter vector for each resample with respect to a deterministic model s note that y x w ω is a simplification of the triple y x w ω where ω ω is held constant for each bootstrap resample when inferring the parameter uncertainty for that particular unique selected input variable set therefore each unique input variable selection set has multiple parameter vectors associated with it see section b 3 of the supplementary material for a schematic that shows the relationship between unique input variable selection sets parameters and model errors the different parameter sets for each ω ω make up the conditional probability density function for the parameters i e f θ θ x w ω note that this conditional probability density function can be computationally intensive to estimate as it requires a sufficient number of bootstrap resamples for each selected input variable set which directly depends on the number of unique selected input variable sets in ω and the number of required bootstrap resamples 3 4 3 model error as the model error is conditioned on both the parameter and input variable selection uncertainty the conditional probability density function for the model error f e q s θ x w ω θ x w ω was obtained by estimating the error e q s θ x w ω on the validation set for the model s optimal parameter vector which was determined by the nash sutcliffe efficiency index nash moriasi et al 2007 associated with each unique input variable set montanari and koutsoyiannis 2012 sikorska et al 2015 therefore a set distribution of model errors was associated with each model attached to the different unique selected input variable sets as noted in montanari and koutsoyiannis 2012 the optimal parameter vector could be exchanged for the parameter vector that provided average or median performance or in the extreme case one could use the model error for each parameter set we did not follow these approaches as earlier experiments demonstrated that the performance using our discussed method was high and reflective of performance that would generally satisfy most real world applications relevant to our case study the caveat of estimating the model error distribution using this approach is that the model error in the validation set should be reflective of the model error which could be expected when running the model in simulation forecast mode which can easily be verified by a test set as done in this study montanari and koutsoyiannis 2012 sikorska et al 2015 it is important to note the distinction between the use of y for the calibration dataset and the use of q in validation mode in keeping with the assumptions of the blueprint during validation mode q is not yet observed at the time of issuing the forecast using s θ x w ω for some new input x while in calibration mode past realizations of q i e y are available since it is on the basis of past realizations that input variables are selected and model parameters estimated below we briefly discuss how we sample from the input variable selection parameter and model error probability distributions in the swddff workflow for the sddff workflow wavelet decomposition of model inputs is excluded and the remaining steps are the same 3 5 sampling from the different probability distributions in order to use the swddff in a forecasting application i e to obtain f q q for a given input x which may be a realization from a test set or newly received information in a real time setting one needs to pass x through the swddff workflow described in fig 1 section 3 3 by sampling from the probability distributions mentioned above section 3 4 1 3 4 2 and 3 4 3 if input data uncertainty is not being considered as in this study then x can be converted to its wavelet decomposed counterpart x w before proceeding with the steps mentioned below after obtaining x w a selected input variable set ω is picked up at random from f ω ω x w second a parameter vector from θ associated with the selected input variable set ω is picked up at random from f θ θ x w ω third s θ x w ω is evaluated resulting in a model output q fourth q is then compared against the validation model outputs for the selected input variable set ω and using the k nearest neighbour bootstrap see sikorska et al 2015 for details a model error is randomly sampled from f e q s θ x w ω θ x w ω and added to q this process is repeated a sufficient number of times and results in f q q for a given input x 4 experimental details we now give details concerning our case study experiment settings i e in terms of estimating and sampling from the different probability distributions in eq 1 along with evaluation metrics used to judge the quality accuracy an reliability of the forecasts produced by swddff 4 1 case study the main objective of this case study is to explore how accurately and reliably the swddff and sddff can forecast a multiscale water resources process e g urban water demand rainfall to demonstrate the potential usefulness of the swddff for obtaining accurate and reliable hydrological and water resources forecasts of a multiscale process we chose a real world case study based on a daily urban water demand uwd dataset from montreal canada that was recently studied by the authors quilty et al 2019 quilty and adamowski 2018 fig 2 shows the wavelet decomposition of montreal s average daily uwd whose multiscale nature manifests in prominent weekly seasonal and annual cycles we explored the swddff for forecasting the average daily uwd at lead times of 1 7 and 14 day s ahead using historical uwd maximum air temperature rainfall and the antecedent precipitation index as model inputs other variables such as day of the week or holiday indicators could be useful for forecasting uwd but were not explored in order to maintain consistency with earlier work quilty et al 2019 quilty and adamowski 2018 since montreal s urban water supply system is the second largest in canada supplying water to around 1 9 million persons statistics canada 2018 it is important that short term changes in uwd be known in advance for optimizing system operations e g pump scheduling reservoir operations planning e g water main maintenance hydrant flow testing and construction e g chlorination temporary servicing for which 1 7 and 14 day ahead forecasts of average daily uwd are useful the authors in quilty and adamowski 2018 developed a number of wavelet wddff and non wavelet data driven forecasts for this dataset for the purpose of demonstrating 1 best practices for wavelet based forecasting and 2 the improvement in accuracy that can be achieved by wavelet based forecasts in comparison to their non wavelet based counterparts we used the wddff and non wavelet based models which we term data driven forecasting framework ddff from this earlier study as benchmarks for the swddff and sddff models developed in this paper the swddff and sddff models were developed by modifying the respective wddff and ddff models from quilty and adamowski 2018 through application of the workflow described in section 3 3 and given in fig 1 in table 1 we list the different models and their properties that were used in quilty and adamowski 2018 along with their swddff and sddff counterparts for details on the original models wddff and ddff we refer the reader to quilty and adamowski 2018 briefly the models shown in table 1 can be described by the following information model i e model name ddff sddff wddff and swddff method i e data driven model used in the framework second order volterra sov and multiple linear regression mlr input variable selection ivs i e input variable selection algorithm used to select input variables for the model edgeworth approximations based conditional mutual information ea and partial correlation input selection pcis wavelet filter i e wavelet filter used in the wddff models least asymetric with 14 coefficients la14 and best localized with 14 coefficients bl14 and decomposition level i e the number of scales j considered in the wavelet decomposition of the inputs for the wddff models it is important to note that while other data driven models e g artificial neural networks support vector regression random forests could have been used we chose to focus only on the data driven models mlr and sov that were used in our earlier related research quilty et al 2019 quilty and adamowski 2018 to enable comparisons between the results of the present study and these earlier studies the advantage of adopting mlr and sov is that each method s parameters can be solved using linear least squares quilty and adamowski 2018 although uwd forecasting is a nonlinear problem house peters and chang 2011 mlr is often used as a linear benchmark adamowski et al 2012 de souza groppo et al 2019 oyebode and ighravwe 2019 mlr has been shown to provide competitive performance with nonlinear data driven models for hydrological and water resources forecasting problems latt and wittenberg 2014 papacharalampous et al 2019d papacharalampous and tyralis 2018 especially when combined with wavelet decomposition mouatadid et al 2019 quilty and adamowski 2018 sov was used as a polynomial regression alternative to mlr while mlr is very popular in hydrology and water resources sov is not used as often despite its high potential for hydrological and water resources forecasting labat et al 1999 prasad et al 2018 which has been especially demonstrated when coupled with wavelet decomposition maheswaran and khosa 2012 2013 2014 section c of our supplementary material provides information on sov the dataset i e consisting of average daily historical uwd daily maximimum air temperature daily rainfall and daily antecendent precipitation index extends over the period february 1999 to decemeber 2010 section d of our supplementary material includes time series plots for the target and input variables as well as scatter plots showing the nonlinear relationships between the different variables a total of 2395 2389 and 2382 calibration 583 validation and 366 test records were included in the 1 7 and 14 day lead time forecasts of the target variable i e the same validation and test records were used for each lead time forecast the non wavelet based models ddff and sddff had a total of 56 inputs that were considered during input variable selection which included 14 days of time lagged time series for each explanatory variable historical uwd daily maximum air temperature daily rainfall and daily antecedent precipitation index the wavelet based models wddff and swddff had a total of 448 inputs that were considered during input variable selection comprised of the same inputs as the non wavelet based models in addition to their wavelet decomposed counterparts at a decomposition level of six j 6 56 6 1 56 448 section d of the supplementary material shows how a lag time of 14 days was selected for the explanatory variables using the informational correlation coefficient linfoot 1957 while section e describes how the candidate pool of model inputs were generated as well as the inputs that were selected for use in ddff and wddff according to quilty and adamowski 2018 all forecasts for lead times 1 7 and 14 days ahead only use explanatory variables measured at the forecast trigger date i e on the date which the forecast is issued or further in the past as far back as 14 days 4 2 model settings it is important to re iterate that the calibration records were used for building the input variable selection and parameter probability distributions thus characterizing their respective sources of uncertainty while the validation set was used for building the model error probability distribution the test set was used to assess the accuracy and reliability of the forecasts since it was not involved in the estimation of the previously mentioned probability distributions allowing one to infer the generalization abilities of the swddff and sddff models and their potential for real world problems this approach is consistent with what was followed by the blueprint authors montanari and koutsoyiannis 2012 sikorska et al 2015 and quilty et al 2019 in our experiments we used a total of 500 bootstrap resamples to develop the different probability distributions input variable selection parameter and model error and drew from the respective probability distributions 500 times n 500 to generate the probability distribution of the target forecast probability density function f q q ten k 10 nearest neighbours were considered in the k nearest neighbour bootstrap for drawing from the model error probability density function during earlier tests we found that these settings provided generally good performance that would be suitable for real world applications relevant to our case study to demonstrate the role and importance of the different sources of uncertainty i e input variable selection parameters and model output on the forecast results we explored four different settings for the swddff and sddff models the four different settings that were explored explicitly considered the following sources of uncertainty 1 none wddff and ddff 2 parameter swddff 1 and sddff 1 3 parameter and model output swddff 2 and sddff 2 and 4 input variable selection parameter and model output swddff 3 and sddff 3 we chose these four settings to compare the performance achieved by the swddff and sddff models when different sources of uncertainty were explicitly accounted for allowing us to discern the relative importance of each source of uncertainty with a focus on the role of input variable selection uncertainty and its impact on forecast accuracy and reliability we note that these sources of uncertainty should not be considered additive montanari and koutsoyiannis 2012 similarly differences between swddff and sddff are useful for assessing the impact of wavelet decomposition on forecasting performance it is relevant to note earlier studies in hydrology and water resources whose models are special cases of the different swddff and sddff models described above sddff 1 is comparable to the bootstrap data driven forecasting models that focussed on estimating parameter uncertainty jia and culver 2006 tiwari and chatterjee 2010 zaier et al 2010 sddff 2 is similar to the bootstrap artificial neural network forecasting models in wang et al 2013 that considered parameter and model output uncertainty swddff 1 is akin to the bootstrap wavelet data driven forecasting models that considered parameter uncertainty belayneh et al 2016 kasiviswanathan et al 2016 tiwari and adamowski 2017 swddff 2 is most alike the bootstrap wavelet artificial neural network forecasting model in wang et al 2013 however the authors used a wavelet based activation function in the hidden layer of the neural network instead of applying wavelet decomposition directly to the input data swddff 3 is the first wavelet based forecasting method that considers input variable selection parameter and model output uncertainty quilty et al 2019 in the next sub section we briefly describe the evaluation metrics used to judge the accuracy and reliability of the swddff and sddff forecasts in our case study 4 3 forecast evaluation metrics to assess the accuracy and reliability of the swddff and sddff models we applied both deterministic and probabilistic performance evaluation metrics commonly adopted in hydrology and water resources studies to measure the accuracy of the different forecasts we used the nash krause and boyle 2005 sutcliffe 1970 root mean square error rmse and mean absolute error mae hauduc et al 2015 legates and mccabe jr 1999 moriasi et al 2007 we considered the continuous ranked probability score crps prediction interval coverage probability picp average prediction interval width aw and mean interval score is boucher et al 2011 del giudice et al 2015 gneiting and raftery 2007 kasiviswanathan et al 2013 solomatine and shrestha 2009 to assess the swddff and sddff forecasts from a probabilistic perspective to supplement these performance evaluation metrics we also used scatter plots and time series plots for a graphical assessment of the different models the nash rmse and mae measure the deviation of the forecast from the observations both nash and rmse are based on the square of errors and are more sensitive to large errors while the mae treats small and large errors equally the crps a useful measure reflecting both forecast sharpness and reliability compares the distribution of the forecast with that of the observations and reduces to the mean absolute error for point forecasts allowing for the direct comparison between point and probabilistic forecasts fortin et al 2006 gneiting and katzfuss 2014 hersbach 2000 pappenberger et al 2015 the picp measures the coverage of the observations within the forecasts prediction intervals which should ideally match that of the prediction interval nominal confidence of 100 1 α we set α at 0 05 in our experiments and is related to the reliability of the forecast wan et al 2014 the optimal value of the picp at this confidence level is 95 the aw simply measures the average width of the prediction intervals and is related to the forecasts sharpness gneiting et al 2007 the is combines both sharpness and reliability into a single metric by considering the width of the uncertainty bound difference between upper and lower prediction intervals in combination with the position of the observation relative to the uncertainty bound bourgin et al 2015 gneiting and raftery 2007 for the swddff and sddff models the deterministic measures nash rmse and mae were calculated by using the mean of the n realizations defining f q q enabling comparisons with the wddff and ddff models while the probabilistic measures crps picp aw and is consider the entire distribution i e all n realizations defining f q q since the swddff 1 and sddff 1 models only account for parameter uncertainty their n forecasts provide an estimate of the variance or uncertainty of only the parameters and not the true value of the target variable however swddff 2 sddff 2 swddff 3 and sddff 3 through their n forecasts include the variance of not only the parameters but also of the future forecasted target variable therefore the former results in confidence intervals while the latter results in prediction intervals which are always wider and encompass the confidence intervals faraway 2014 sec 4 1 since prediction intervals are required to estimate picp aw and is gneiting and raftery 2007 we assess only the swddff 2 sddff 2 swddff 3 and sddff 3 using these measures however the crps can be used more generally to assess an ensemble of forecasts e g whether generated by drawing samples from a stochastic model such as in this study making perturbations to initial conditions tweaking hyper parameters or combining forecasts from different models boucher et al 2015 zamo and naveau 2017 and is therefore used for assessing how well the n realizations from the swddff and sddff models forecast the target variable therefore the crps acts as a useful measure for assessing the impact that the different sources of uncertainty input variable selection parameter and model output have on swddff and sddff accuracy and reliability at the same time since crps reduces to the mae for deterministic forecasts it is a useful metric for comparing the swddff and sddff against the wddff and ddff forecasts in terms of the probabilistic assessment of the swddff and sddff models we prefer reliable forecasts to sharp forecasts since users of forecasts in the form of a probability distribution are often interested in the uncertainty of the next outcome and unreliable forecasts are usually of little use regardless of their sharpness boucher et al 2010 therefore we are most interested in the picp score and then the crps is and aw scores in other words if a forecasting model has the best picp score and relatively close crps is aw and deterministic nash rmse and mae scores when compared to its competitors the model with the best picp score is deemed superior for the task of probabilistic forecasting all models are applied to the test period once and each performance metrics is calculated across the entire test period i e models are not re calibrated for each forecast 5 results and discussion we now turn to the evaluation of the swddff and sddff and their benchmarks wddff and ddff table 2 records the deterministic nash rmse and mae and probabilistic crps picp aw and is forecast evaluation metrics for our uwd forecasting case study in montreal canada our focus is on comparing the swddff and sddff that account for various sources of uncertainty in their forecasts the results in table 2 support our thesis at least for this case study that input variable selection uncertainty and wavelet decomposition are two important factors in improving forecast accuracy and reliability in comparison to forecasts that do not consider either or only one of these tools compare swddff 3 and sddff 3 forecasts against the others in table 2 to support this claim we summarize our main findings below remember that only the test set performances are discussed in this section as we are solely concerned with how the frameworks performed out of sample 1 for each lead time the swddff 3 model exhibited the most reliable forecasts in terms of the picp score ideally the picp should match the prediction interval nominal confidence of 95 therefore in a probabilistic sense the inclusion of input variable selection uncertainty and wavelet decomposition of model inputs were important factors that resulted in the most reliable forecasts for our case study 2 for each lead time the swddff 3 model provided either the best lowest crps or is in conjunction with the best picp score since the crps reduces to the mae for a point forecast swddff and sddff models provided better forecast performance when used probabilistically rather than deterministically since in all cases the crps was lower than the mae scores 3 for each lead time and most performance measures wavelet decomposition of model inputs improved forecast performance over the non wavelet based models i swddff 3 showed a 2 12 13 13 and 33 19 improvement in nash rmse over the sddff 3 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 12 14 13 16 and 18 25 improvement in crps is over the sddff 3 at 1 7 and 14 day lead times respectively iii however there were exceptions for the 7 day lead time a the crps for the swddff 1 model was inferior to that of the sddff 1 however the swddff 3 which accounted for input variable selection uncertainty had the lowest crps at the 7 day lead time b the picp for the swddff 2 model indicated that it was less reliable than the sddff 2 model although it had better nash rmse mae crps is and aw scores regardless the swddff 3 exhibited the best reliability picp 4 input variable selection uncertainty appears to become more important in improving model performance both in terms of deterministic and probabilistic measures at the 7 and 14 day lead times in contrast to the 1 day lead time i swddff 3 showed a 0 1 2 5 and 4 3 improvement in nash rmse over the swddff 2 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 2 0 8 15 and 4 15 improvement in crps is over the swddff 2 at 1 7 and 14 day lead times respectively 5 the combined impact of including both input variable selection uncertainty and wavelet decomposition of model inputs can be gauged by comparing the deterministic and probabilistic forecasting performance of swddff 3 and sddff 2 i swddff 3 showed a 3 13 13 13 and 34 19 improvement in nash rmse over the sddff 2 at 1 7 and 14 day lead times respectively ii swddff 3 showed a 12 16 12 16 and 19 30 improvement in crps is over the sddff 2 at 1 7 and 14 day lead times respectively to assess the impact of wavelet decomposition on input variable selection uncertainty we utilized the entropy measure from information theory shannon 1948 singh 2013 to quantify the uncertainty in the input variable selection probability distribution the higher the entropy the higher the uncertainty if the probability distribution defining input variable selection uncertainty were flat i e a uniform distribution then it would have maximum entropy the entropy of the input variable selection probability distributions for swddff 1 swddff 2 and swddff 3 are 6 215 5 801 and 4 911 while for sddff 1 sddff 2 and sddff 3 they are 0 755 1 797 and 1 557 all entropy measurements are in nats given that a total of 500 random samples were used to estimate input variable selection uncertainty for swddff and sddff the maximum entropy is 6 215 therefore one can conclude by the entropy and performance measures reported in table 2 that wavelet decomposition increases input variable selection uncertainty but in all cases leads to an increase in forecast accuracy and reliability further information on calculating input variable selection uncertainty via entropy including additional discussion on the input variable selection of the swddff and sddff variants can be found in the supplementary material section b 2 and e respectively one may observe that the wddff and ddff models often provided better deterministic performance than their respective swddff and sddff counterparts e g see the 1 day lead time results in table 2 this is easily explained by the fact that these models wddff and ddff did not take into account any form of uncertainty assessment and therefore were biased with their forecasts relying on a single set of parameters and selected input variables likewise there exist cases where the swddff 1 and sddff 1 models performed better than or as nearly good as in a deterministic sense their respective swddff 2 and swddff 3 and sddff 2 and sddff 3 counterparts e g refer again to the 1 day lead time results in table 2 this is met with similar reasoning as above the swddff 1 and sddff 1 forecasts do not explain the variance in their predictions only in their parameters and therefore the mean of their n forecasts are biased as evidenced by the crps furthermore a comparison between the swddff 2 and sddff 2 with their respective swddff 3 and sddff 3 counterparts reveals that the forecasts of the former are also biased since they do not consider the source of error stemming from forecasts produced using different input variable sets which resulted in lower reliability for all cases i e for the swddff 2 and sddff 2 models the last point is worth re iterating in each case that input variable selection uncertainty was considered whether wavelets were used or not forecast reliability improved as well as either or both the crps and is this performance was further increased by incorporating wavelet decomposition of model inputs we believe that this is evidence supporting the importance of including input variable selection uncertainty and wavelet decomposed model inputs in stochastic data driven forecasting of multiscale processes such as those commonly encountered in hydrology and water resources furthermore by including input variable selection uncertainty alongside parameter and model output uncertainty swddff swddff 3 in particular improved upon some of the most advanced wavelet based forecasting models e g those comparable to swddff 1 and swddf 2 see the end of section 4 2 in the hydrological and water resources literature that have either included an assessment of only parameter uncertainty belayneh et al 2016 kasiviswanathan et al 2016 sehgal et al 2014 tiwari and adamowski 2017 or parameter and model output uncertainty bachour et al 2016 maslova et al 2016 wang et al 2013 while there have been some wavelet based forecasting models that have focussed on combining multiple wavelet based forecasts to help address forecasting uncertainty barzegar et al 2018 2017 rathinasamy et al 2013 they were not concerned with generating and evaluating probabilistic forecasts but rather an average pooled ensemble forecast however performance of the swddff 3 in this study did not outperform the ew sddff from quilty et al 2019 as explained below due to the use of the same case study the results of the swddff can also be compared to the ew sddff framework from quilty et al 2019 the best ew sddff outperformed the swddff across all performance metrics rmse nash mae crps picp aw and is however this is to be expected as the ew sddff is based on multiple wddff models i e it is a multi wavelet approach while the swddff is based on only a single wddff therefore the ew sddff benefits from increased variability present in the model inputs due to its multi wavelet nature i e being based on multiple wavelet filters decomposition levels etc that the swddff does not include similarly an earlier study by rathinasamy et al 2013 demonstrated that a multi wavelet approach led to improved streamflow forecasting performance at multiple temporal scales daily weekly and monthly finding that coupling multiple forecasting models incorporating different decomposition levels and wavelet filters were better able to capture different behaviours of the target time series e g low medium and high flows that were missed by single wavelet models given that the swddff 3 models provided the best overall performance we now compare its performance against sddff 3 using different graphical tools note that in these figures swddff and sddff are equivalent to swddff 3 and sddff 3 respectively we notice in fig 3 which compares the crps versus the number of resamples n included in f q q that the crps steadily decreases as the number of resamples n is increased there is a noticeable drop in crps around n 50 but the crps continues to descend after this point this is an indication that n 500 appears to be more than sufficient for stabilizing performance of the swddff and sddff models for this case study the scatter plots for the observed versus each of the n forecasts in f q q in fig 4 show that the swddff forecasts are more tightly centered on the bisector line indicating a better fit than the sddff forecasts the sddff forecasts tend to have larger outliers for medium and higher flows than the swddff time series plots for the mean as well as for the 0 025 and 0 975 quantiles of the f q q forecasts versus the observations in figs 5 and 6 complement fig 4 by demonstrating that the swddff and sddff forecasts are not only accurate but successful at matching the different scales in the uwd time series that change through time e g at the weekly seasonal and annual scales for each lead time the swddff and sddff captured the weekly cycle during lower flows october 2009 may 2010 reasonably well in terms of its mean forecast and its prediction intervals i e 0 025 and 0 975 quantiles during the seasonal drop in uwd august october 2009 the swddff and sddff had lower accuracy but were still reliable across all lead times when the seasonal demand picked up may 2010 onwards the 1 day lead time forecasts were both accurate and reliable however the accuracy and reliability of the mean forecast and its prediction intervals were noticeably worse for 7 and 14 day lead times in general the prediction intervals for the swddff can be deemed superior to those for the sddff visually and statistically as there is not only an improvement in sharpness see the aw scores but also in reliability see picp cprs and is however while both forecasting models tended to exhibit strong reliability their upper prediction interval tended to overestimate lower flows especially at longer lead times 7 and 14 days ahead in general the high level of accuracy and reliability of the swddff and sddff indicate that both methods could be particularly useful for optimizing pumping operations within urban water supply systems while the improved accuracy of the swddff over the sddff at weekly 7 day and bi weekly 14 day lead times could be especially useful for evaluating water restrictions during drought or emergency situations in addition to planning construction and maintenance activities within water supply systems since the forecasts generated by both swddff and sddff can be interpreted probabilistically they may also prove to be useful in estimating risk reliability resilience and vulnerability of water supply systems yung et al 2011 finally to demonstrate the generalization capabilities of the swddff and sddff for other forecasting applications important to hydrology and water resources e g rainfall streamflow both frameworks should be explored using large scale experimental data sets such as those adopted in earlier studies e g kratzert et al 2018 papacharalampous et al 2019b tyralis and koutsoyiannis 2017 tyralis and papacharalampous 2017 2018 xu et al 2018 in particular the catchment attributes and meteorology for large sample studies camels addor et al 2017 and canadian model parameter experiment canopex arsenault et al 2016 are open source data sets that could be used for these purposes 6 summary and conclusions this study explored the recently introduced stochastic wavelet data driven forecasting framework swddff quilty et al 2019 for forecasting uncertain multiscale hydrological and water resources processes e g urban water demand the swddff uses wavelet decomposition to extract important multiscale information from the model inputs passes this information to a potentially nonlinear data driven model artificial neural networks support vector regression etc to generate forecasts of the target variable and accounts for several important uncertainty sources input data input variable selection parameter model output etc resulting in a probabilistic forecast the goal of this study was to answer two important questions often neglected in data driven hydrological and water resources forecasting can input variable selection uncertainty improve forecast performance i e accuracy and reliability when included alongside parameter and model output uncertainty despite wavelet decomposition of input data increasing input variable selection uncertainty can it still be used to improve forecast performance these two questions were explored by varying the level of uncertainty none parameter parameter and model output and input variable selection parameter and model output included in the swddff and its non wavelet based counterpart sddff results from our 1 7 and 14 day ahead urban water demand forecasting case study in montreal canada answered both questions in the affirmative by including input variable selection alongside parameter and model output uncertainties the swddff demonstrated an improvement of 3 in the nash sutcliffe efficiency index and 15 in the mean interval score at the 14 day lead time over the case when swddff only considered parameter and model output uncertainties although wavelet decomposition of model inputs was shown to increase input variable selection uncertainty according to the entropy measure 4 911 nats for swddff vs 1 557 nats for sddff it resulted in an increase of 33 in the nash sutcliffe efficiency index and 25 in the mean interval score at the 14 day lead time the main contributions of this research include 1 the first study to explore and quantify the impact of input variable selection uncertainty on probabilistic forecasting performance and 2 the first study to demonstrate that the increase in input variable selection uncertainty due to wavelet decomposition of input data may be a reasonable expense to incur in order to gain a significant increase in probabilistic forecasting performance our hope is that the swddff may serve as a useful tool for those interested in building probabilistic wavelet based data driven forecasting models to address the important issues of nonlinearity multiscale change and uncertainty in hydrological and water resources forecasting we note that our study was limited in a number of ways 1 we only used a single case study 2 we did not consider a wide variety of wavelet filters and decomposition levels during wavelet decomposition 3 only a small number of data driven models and input variable selection methods were studied and 4 we did not consider input data uncertainty in order to focus on input variable selection uncertainty we believe that these limitations are reasonable for the present study as our main goal was to determine whether including input variable selection uncertainty in addition to parameter and model output uncertainty in swddff and sddff as well as using wavelet decomposed input data i e using swddff instead of sddff led to improvements in forecast accuracy and reliability by evaluating the two frameworks in a case study explored earlier by the authors quilty et al 2019 quilty and adamowski 2018 however different research avenues can be explored in future studies such as 1 testing swddff and sddff on a larger number of hydrological and water resources processes e g streamflow drought evaporation such as those included in recent open source large scale data sets e g camels addor et al 2017 and canopex arsenault et al 2016 as well as time series stemming from different domains 2 the evaluation of a number of different wavelet families and decomposition levels 3 exploring a wider range of data driven models and input variable selection methods 4 including input data uncertainty alongside input variable selection parameter and model uncertainty 5 comparing the performance of swddff and sddff against process based models such as those used in the original blueprint for different hydrological and water resources processes 6 testing the suitability of different methods such as meta heuristic optimization approaches for generating the various probability density functions input variable selection parameters etc 7 comparing swddff against sddff coupled with different pre processing methods e g principal component analysis independent component analysis auto encoders wetzel 2017 declaration of competing interest none acknowledgements we thank the editor and the three reviewers for their many suggestions that helped to improve the present paper we extend our gratitude to the city of montreal for providing the urban water demand data used in our case study urban water demand data may be requested from the city of montreal via http donnees ville montreal qc ca the meteorological data used in this study can be obtained from environment canada at climate weather gc ca funding for this research was provided by natural sciences and engineering research council nserc discovery and accelerator grants grant numbers g219643 nserc rgpin 2015 05554 g241149 nserc rgpas 477886 15 held by jan adamowski the funding agency had no involvement in this study whatsoever study design in the collection analysis or interpretation of data in the writing of this paper and in the decision to submit the paper for publication appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104718 
