index,text
25765,recent breakthroughs in artificial intelligence ai and particularly in deep learning dl have created tremendous excitement and opportunities in the earth and environmental sciences communities to leverage these new data driven technologies however one needs to understand the fundamental concepts that give rise to dl and how they differ from process based mechanistic modelling this paper revisits those fundamentals and addresses 10 questions that might be posed by earth and environmental scientists and with the aid of a real world modelling experiment it explains some critical but often ignored issues dl may face in practice the overarching objective is to contribute to a future of ai assisted earth and environmental sciences where ai models can 1 embrace the typically ignored knowledge base available 2 function credibly in true out of sample prediction and 3 handle non stationarity in earth and environmental systems comparing and contrasting earth and environmental problems with prominent ai applications such as playing chess and trading in stock markets provides critical insights for better directing future research in this field keywords artificial intelligence machine learning deep learning artificial neural networks process based modelling earth systems hydrology plain language summary the recent unprecedented performance of deep learning dl in image and language processing has accelerated applications in non native areas such as earth and environmental sciences where knowledge driven process based modelling has dominated to date a major challenge however is dl and process based modelling are rooted in different worldviews towards problem solving this paper explains the whats and whys of dl from first principles and how they are different from those of process based modelling a hydrologic modelling experiment is presented to illustrate the fundamental differences between the two worldviews and to shed light on some major issues dl has to deal with in the modelling of earth and environmental systems these issues largely arise from the fact that such systems are complex with behaviors that can change in ways that are physically explainable but not seen in the period of record due to factors such as climate change and human interventions such issues must be addressed at the heart of the endeavor to extend dl techniques that embrace the knowledge base available in anticipation of breakthroughs in an age of big data and computational power 1 the rise of deep learning the last decade has witnessed a tremendous rise in techniques called deep learning dl under the umbrella of artificial intelligence ai and machine learning ml and their unprecedented performance in areas such as computer vision krizhevsky et al 2012 natural language processing young et al 2018 and gaming silver et al 2018 these successes have motivated the application of dl across a wide range of disciplines including medicine hosny et al 2018 earth sciences reichstein et al 2019 robotics torresen 2018 engineering panchal et al 2019 and finance lee et al 2019 dl owes its exemplary success to the boom in computational power and the emergence of big data sources and associated data storage and sharing technologies earth and environmental sciences appear to be positioned to benefit from dl as big data sources on a range of in situ and remotely sensed variables are becoming increasingly available with the advances in sensing technologies reichstein et al 2019 mao et al 2020 the storage volume of remote sensing data for earth observations is already well beyond dozens of petabytes with transmission rates exceeding hundreds of terabytes per day datasets based on model outputs are rising for example the climate assessment dataset provided by the coupled model intercomparison project phase 6 may reach 40 petabytes eyring et al 2016 reanalysis climatic datasets have also grown for example nasa s modern era retrospective analysis for research and applications version 2 merra 2 is 400 terabytes gelaro et al 2017 in addition datasets generated via tens of thousands of citizen science projects are providing large and rich sources of ground based data this potential is shifting the attention of earth and environmental scientists and relevant funding agencies towards ml as evidenced for example by the shift in research work presented at the american geophysical union agu s fall meetings the largest assembly of earth and environmental scientists with more than 27 000 people in attendance and 25 000 presentations in 2019 the number of ml related presentations has risen consistently from 0 2 of total presentations in 2015 to 4 2 in 2020 in particular this shift has been astonishing in the agu s non linear geophysics earth and space science informatics natural hazards hydrology and seismology sections where 28 2 1 18 5 1 9 1 3 7 5 1 4 and 6 7 0 9 of total presentations respectively were related to ml in 2020 2015 recent successful applications of dl techniques to earth and environmental sciences include weather nowcasting and forecasting shi et al 2015 2017 satellite precipitation bias reduction tao et al 2016 rainfall runoff modelling kratzert et al 2018 feng et al 2020 ma et al 2021 rain and snow retrieval from spaceborne sensors tang et al 2018 downscaling hydroclimatic variables ducournau and fablet 2016 precipitation estimation tao et al 2018 pan et al 2019 and surrogate modelling gu et al 2020 yu et al 2020 unsuccessful applications perhaps similar to many other areas remain largely unreported in the peer reviewed scientific literature but occasionally appear in other media e g wexler 2017 kolakowski 2018 rudin 2019 notably dl is a trendy term that refers to a specific class of artificial neural networks anns which have been around and widely applied in earth and environmental sciences since the early 1990s with the birth of domains such as hydroinformatics abbott 1991 these applications are documented in reviews by gardner and dorling 1998 maier and dandy 2000 krasnopolsky 2007 maier et al 2010 abrahart et al 2012 razavi et al 2012 shen 2018 bergen et al 2019 and reichstein et al 2019 arguably however the uptake of anns in general to facilitate and advance earth and environmental sciences has not kept pace with data availability and computational power over the past three decades but why the challenges impeding the widespread application of anns to earth and environmental problems to date may be rooted in the fact that convincingly casting those problems for which an extensive knowledge base is usually available within the ann framework is often not straightforward moreover the lack of interpretability and explainability of anns has been a major hindrance as model developers need to be able to make sense of why a model functions the way it does and to explain that to model users these challenges can be further complicated in the absence of a solid understanding of the fundamentals of anns and how they differ from theory driven mechanistic modelling and prediction mechanistic modelling also called process based or knowledge based modelling in this paper has traditionally been the cornerstone of scientific advancement and policy support so why this paper motivated by the recent breakthroughs by dl in its original areas of application namely computer vision and natural language processing this paper aims to address the persistent challenges facing dl applications in non native areas related to earth and environmental sciences with this overarching aim this paper addresses but not necessarily answers 10 questions regarding the fundamentals of dl and its explainability and bridgeability to earth and environmental systems modelling 1 what is dl and how did it evolve from anns 2 how can we interpret the internal functioning of dl 3 does dl abandon occam s razor or the principle of parsimony 4 why is dl considered superior to other types of ml 5 how can dl account for memory and time dependency 6 how do dl and process based models compare in calibration and validation and which one may be more trustworthy 7 what is the value of the often ignored domain knowledge in dl and why may dl suffer from this ignorance 8 why is dl essentially different from process based modelling 9 what are the existing approaches to bridging dl and process based modelling 10 what can we learn from prominent dl applications such as gaming and the stock market the structure of this paper is such that it best serves the reader when all sections are followed sequentially however an advanced reader could directly refer to a section that is more relevant to a question of interest sections 2 through 7 are about questions 1 through 6 and sections 8 1 through 8 4 are about questions 7 through 10 respectively a simple hydrological modelling problem and multiple synthetic functions are used to explain complex concepts via simple examples the contents of this paper are intended to be accessible to a wide audience from various fields under the umbrella of earth and environmental sciences however the views presented mainly arise from the author s data and theory driven research background in hydrology and water resources this paper concludes in section 9 by revisiting some ancient issues pertaining to modelling in general around the mathematistry trap raised by the late prominent scientists george box 1976 and vit klemeš 1986b those issues can be just as fresh today as they once were particularly in the absence of a solid understanding of what dl can and cannot do in a project and in the presence of possible hype or over excitement around it further section 10 is a postscript where the author reflects on some significant review comments received on this paper before its final publication 2 back to fundamentals 2 1 why ml and dl ml as a subclass of ai is nowadays concerned with developing machines that improve their own performance in carrying out a given task over time by learning from examples with minimal human efforts to instruct the machines how to do so jordan and mitchell 2015 according to goodfellow et al 2016 however the early efforts to generate ai were based on a knowledge base paradigm to instruct machines with a formal set of step by step mathematical and if then rules those efforts focused on carrying out tasks that were intellectually difficult for humans but straightforward for computers goodfellow et al 2016 argue such efforts led to no major successes and the ai of today is about enabling machines to perform tasks that humans perform intuitively and rather easily but have difficulty formally describing how they do so examples of such tasks include recognizing faces in a photo or comprehending spoken words not only did state of the art ai divorce from the knowledge base but it also completely separated from classic data driven modelling rooted in statistics such as regression this separation was a response to the need for models that are not constrained by the many assumptions typical statistical models hold for example traditional statistical modelling requires a formalization of relationships between variables and assumptions about functional shapes distributions of variables and their inter dependencies which enables hypothesis testing and the generation of confidence bounds conversely in the ml context the underlying relationships in data may have any complex form which is typically unknown a priori and the data used may have any size and distributional properties see dangeti 2017 p 10 11 because of these characteristics ml is deemed suitable to pursue the longstanding ambition to build machines that work with minimal or no human supervision and imposed assumptions ml techniques nowadays and in particular dl provide such flexible tools that can adapt to a wide range of data and applications 2 2 evolution of dl and major milestones it was 1957 when frank rosenblatt invented the first algorithm termed perceptron rosenblatt 1957 which today forms the smallest computational unit of dl a perceptron alternatively termed a neuron because of its resemblance to the basic working unit of the brain is shown in fig 1 a and formulated as eq 1 y f i 1 d w i x i b where d is the dimension of input space x is the input vector w is a set of weights corresponding to the input vector b is bias and f is an activation function a perceptron has d 1 tunable parameters i e d weights and one bias and is basically nothing but a multiple linear regression augmented by an output function f which is non linear the form of the activation function was originally a step function but now a range of monotonic functional forms such as sigmoidal are used the invention of perceptrons created significant excitement in the ai community and beyond but it soon became clear that a perceptron would not be able to map input spaces that are not linearly separable such as the xor problem minsky and papert 1969 rendering perceptrons of limited use in real world applications the reason for this inability is that the core of the perceptron is a linear regression efforts to overcome this barrier could have followed two different avenues perhaps the most intuitive avenue was to employ non linear regression by allowing the terms inside the parentheses in eq 1 to be of other algebraic forms such as quadratic however this was not a viable option in part because the user then would need to specify the form of non linearity which is not typically known a priori requiring possibly extensive trial and error the second avenue that led to today s dl was to combine perceptrons both in parallel and in series to create so called multi layer perceptrons mlps as shown in fig 1b with the hope this more complex system could overcome the barrier an mlp would then have many more tunable parameters than the perceptron the first layer of neurons also called the first hidden layer would have d n1 weights and n1 biases where n1 is the number of neurons in this layer similarly the second hidden layer would have n1 n2 weights and n2 biases and the last layer called the output layer would have nd 1 nd weights and nd biases where nd 1 and nd are the numbers of neurons in the second to last and last layers respectively and d is the total number of layers the total number of layers in an mlp and the number of neurons in each layer are hyper parameters to be specified by users also important is the choice of activation functions in each layer note that a linear activation function is typically only suitable for the last layer and in general any stack of linear layers is effectively equivalent to a single linear layer but mlps on their own did not go far and the field stagnated for many years because of the absence of an algorithm that could automatically derive from data the network weights and biases a process referred to as training in the ai community it took until the mid 1980s when the first back propagation bp algorithm was invented to enable the training of mlps with any network structure rumelhart et al 1986 this invention marked the beginning of the second wave of popularity of anns bp is essentially an optimization algorithm based on non linear programing that minimizes a loss function f representing the goodness of fit of predictions to observations such as the sum of squared errors as follows eq 2 f k 1 m j 1 n d t j k y j k 2 where y j k is the output of neuron j in the output layer when the network is forced with input data sample k and t j k is the respective desired target also m is the size of training data and nd is the number of neurons in the output layer different variations of bp rooted in first e g gradient descent or second order e g newton s method optimization or a combination thereof now exist see e g the levenberg marquardt algorithm as implemented by hagan and menhaj 1994 these algorithms are fundamentally the same as optimization algorithms used nowadays for calibration of process based models the only difference is that in the case of anns and unlike most process based models the partial derivatives of the loss function with respect to weights and biases are analytically available through the chain rule of differentiation in parallel derivative free and metaheuristic optimization algorithms have shown promise in training e g dengiz et al 2009 rakitianskaia and engelbrecht 2009 razavi and tolson 2011 but have yet to become mainstream the training of anns is an iterative optimization process where the network parameters are updated after each iteration called an epoch in the ann context to minimize the loss function this process can be via batch training where at each epoch the entire batch of training data i e all m input output sets are used alternatively each epoch can follow mini batch training based on a subset of training or incremental online training based on a single training data sample chosen randomly or otherwise hagan et al 1996 these two approaches also commonly referred to as stochastic gradient descent are useful when the size of training data is large bottou 1998 2010 in the late 1980s after the invention of bp mlps were proven to be universal approximators hornik et al 1989 this proof indicated mlps with only one hidden layer that possesses a sigmoidal activation function and a linear output layer would be able to approximate any function with any desired level of accuracy provided the number of hidden neurons is sufficient since then the universal function approximation theorem has been the fundamental driver of interest in mlps across a variety of disciplines and applications mlps gradually formed a prominent class of artificial neural networks or simply neural networks a name reflecting their perceived resemblance to biological neural networks mlps which are also sometimes called feedforward neural networks fnns are the building blocks of a range of other anns developed later on including autoencoders bourlard and kamp 1988 recurrent neural networks rnns elman 1990 and its popular variation long short term memory lstm hochreiter and schmidhuber 1997 convolutional neural networks cnns lawrence et al 1997 and generative adversarial networks gans goodfellow et al 2014 anns started receiving much attention in earth and environmental sciences in the early 1990s the pioneering applications of anns include benediktsson et al 1990 badran et al 1991 stogryn et al 1994 bankert 1994 and cabrera mercader and staelin 1995 in the context of remote sensing of the environment mccann 1992 boznar et al 1993 and navone and ceccatto 1994 in the context of atmospheric forecasting and kang et al 1993 hsu et al 1995 and minns and hall 1996 in the context of hydrologic modelling perhaps the most prominent and widely used application of anns in these fields has been related to the development of persiann or precipitation estimation from remotely sensed information using artificial neural networks hsu et al 1997 sorooshian et al 2000 ashouri et al 2015 which has been maintained and updated for two decades accessible at https chrsdata eng uci edu despite all of these advances including the development of the now highly successful anns such as lstm hochreiter and schmidhuber 1997 and cnn lawrence et al 1997 investments in anns and therefore their popularity saw a decline in the ai community beginning in the mid 1990s this was reportedly triggered by failures to fulfill overly ambitious or unrealistic promises by prominent ai scientists goodfellow et al 2016 that brought about somewhat a negative reputation for anns duerr et al 2020 as historically observed in ai winters hendler 2008 anns in earth and environmental sciences however remained fairly popular arguably until the mid 2000s the focus of researchers in these fields was to find novel applications of anns across different earth and environmental problems it took until early 2010s before the third wave of popularity and interest in anns hit when the field was revived and renamed deep learning depth is a recently popularized term and loosely refers to the number of hidden layers in anns a related term is width which loosely refers to the number of neurons in hidden layers now a dl model typically refers to an ann with more than a few hidden layers the recent excitement around anns is despite the fact that the structure formulation and other properties of mlps and some of their offsprings such as lstm that provides mlps with a memory see section 6 have remained essentially unchanged since their inception except for some minor modifications so one might ask is dl merely a repackaging and rebranding of what existed before the next section attempts to answer this question while reviewing the recent milestones 2 3 latest developments and rebranding the field to better understand the recent developments in the field of anns one first needs to know the history around the depth concept anns since their inception have been used with various numbers of hidden layers that is with various depths most applications however remained limited to networks with only one hidden layer until very recently for example razavi et al 2012 surveyed ann applications for surrogate modelling in water resources literature and reported that more than 90 of those had only one hidden layer there was and perhaps still is no consensus about a proper network depth because identifying the optimal network configuration for a given problem and dataset is challenging historically before the rise of dl some researchers favored anns with more than one hidden layer arguing that they require fewer hidden neurons to approximate the same function see e g tamura and tateishi 1997 on the other hand others asserted that single hidden layer anns are superior to those with more than one hidden layer with the same level of complexity see e g de villiers and barnard 1993 three general reasons historically drove interests towards anns with a single hidden layer 1 the universal function approximation theorem hornik et al 1989 as it provided a compelling argument that such anns are fully capable of learning any function 2 the principle of parsimony as anns with fewer hidden layers are generally deemed less complex and more understandable and 3 difficulty of training as anns with more hidden layers are more complex to train see e g de villiers and barnard 1993 so what recently shifted the status quo towards anns with multiple typically many hidden layers goodfellow et al 2016 attribute the beginning of this shift to the work of hinton et al 2006 where unsupervised learning was used to pre train deep anns they showed unsupervised learning could effectively initialize the network s parameters such that the subsequent training efforts through bp would become more successful in ai unsupervised learning refers to a process where a model learns from unlabeled examples which are technically inputs with no associated output this is as opposed to supervised learning where examples i e data points are labeled meaning the output associated with each input is available this process is called model calibration in the context of process based modelling now one might ask how unsupervised learning can be of any help in supervised learning a common method for this purpose uses autoencoders which are a class of anns historically used for dimensionality reduction and feature learning bourlard and kamp 1988 an autoencoder is an mlp typically trained by bp with one or more hidden layers that receives input and aims to produce the same input as its output in a typical autoencoder the middle layer has fewer neurons than the dimension of input thereby acting as a bottleneck that encodes the input data in a lower dimensional space the signals in the middle layer preserve the information contained in the inputs which will be decoded back to the original space in the following layers autoencoders can pre train some layers of a deep ann such that the weights of those layers capture the main features in input data before passing them to the next layers after the pre training phase by unsupervised learning the ann needs to be further trained in the conventional supervised manner using the actual output data and algorithms such as bp while the third wave of ann popularity began by leveraging unsupervised learning to train deep anns goodfellow et al 2016 argue the interest has gradually shifted back to the classic learning algorithms such as bp even for training deep anns those classic learning algorithms are now believed to work quite well in the dl context perhaps due to the boom in computational power in this regard a game changer was the introduction of graphics processing units gpus to the ann community as a powerful tool to massively parallelize and thus expedite training algorithms raina et al 2009 such computational power has enabled the development of large anns in terms of both depth and width as such anns with hundreds of millions e g devlin et al 2018 or even a trillion parameters e g rajbhandari et al 2019 are becoming common such a tremendous revival of the field of anns might seem at first surprising to those earth and environmental scientists who have known the field for a long time this might be due in part to the fact that many ann applications to earth and environmental problems nowadays are fundamentally similar to those performed in the 1990s and 2000s more broadly the basic forms of many anns used today have not changed much other than becoming larger differences if any in an application are often in the details for example following glorot et al 2011 the tendency now is to use the rectified linear unit relu which is an unbounded function instead of the standard sigmoidal activation functions see eq 1 this revival may be explained by the huge successes of some anns in their original areas of application such as image processing krizhevsky et al 2012 and speech recognition young et al 2018 their commercial potential and the resulting major investments by mega companies such as google in this field perhaps recent rebranding of the field under the title of deep learning might have been in part a marketing strategy duerr et al 2020 while as cited in schmidhuber 2015a this term was first introduced by dechter 1986 to ml and by aizenberg et al 2000 to anns 3 geometrical interpretation of dl anns have always struggled with explainability and interpretability extensive research efforts have endeavored to peer inside the black box of anns via various heuristics rooted in sensitivity analysis of input output mappings created by anns e g bach 2015 toms 2020 or see section 3 4 of razavi et al 2021 for a review or techniques that attempt to explain the internals of ann models e g benítez et al 1997 tickle et al 1998 castro et al 2002 wilby et al 2003 xiang et al 2005 see et al 2008 samek and müller 2019 despite all these advances the issues around explainability and interpretability of anns and of many ml techniques in general are as relevant today as ever see rudin 2019 this section utilizes a geometrical interpretation to illustrate the internal functioning of anns and explain why deeper anns can be more powerful than shallower anns in learning representations in data this interpretation is adopted in part from the work of razavi and tolson 2011 in which they recast anns with respect to a new set of variables that are interpretable based on the network functional geometry 3 1 a perceptron an ann is in principle made of a number of perceptrons see section 2 2 consider a basic ann with a single hidden layer with a sigmoidal activation function as shown fig 2 a each hidden neuron e g the r th neuron is a perceptron whose output y r 1 is multiplied by the weight w 1 r 2 before entering the output neuron this hidden neuron when only having one input x 1 forms a functional relationship such as that shown in fig 2b this sigmoidal unit can be characterized by three variables slope location and height there is one to one mapping between these variables and the original network variables w r 1 1 b r 1 and w 1 r 2 as shown in the figure as such one can directly control the shape of the sigmoidal unit through slope location and height and where needed map them onto the network s original variables the benefit of doing so is that unlike the original variables the new variables are geometrically interpretable and therefore more intuitive fig 2c shows the geometry of a perceptron with two inputs x 1 and x 2 in this case the resulting sigmoidal unit forms a plane that can be characterized by slope location and height plus an additional variable called angle that specifies the direction toward which the sigmoidal unit is facing this geometry can be extended to perceptrons with three or more say d inputs where the sigmoidal unit becomes a hyperplane characterized by a slope location and height and d 1 angles full details of this geometrical interpretation and how it works in practice are available in razavi and tolson 2011 now let us see in the following how anns can approximate any function by putting together a large number of such sigmoidal units 3 2 anns with one hidden layer single hidden layer anns are capable of approximating any function by combining in parallel as many sigmoidal units as required for example suppose the underlying function to approximate is the sine function shown in fig 3 a three sigmoidal units with almost equal heights equal absolute slopes and different locations are required in parallel to represent the features of the function these three units can be produced by the hidden layer of an ann and fed into a linear output layer where they are summed superposed to approximate the sine function as shown in fig 3b for problems with two or more inputs the function approximation is not as straightforward for example suppose the objective in a two input problem is to approximate the dome like feature shown in fig 4 a a single hidden layer ann with four sigmoidal hidden neurons and one linear output neuron would be able to approximate the dome part of the space as shown in fig 4b this ann would basically superpose four sigmoidal units with equal heights equal slopes equal locations but different angles 90 apart the performance of this ann however is unacceptable as it creates erroneous features on the tails but can we rectify this issue by using more hidden neurons fig 4c shows the performance of a network with eight sigmoidal units all having the same heights slopes and locations but different angles 45 apart with more sigmoidal units at work the performance at the tails is improved producing a smoother surface almost 40 hidden neurons are required as shown in fig 4d to generate almost completely smooth tails similar to the original function shown in fig 4a this example provides a geometrical proof for the universal function approximation theorem of hornik et al 1989 because in principle any complex function could be approximated by superposing a large number of such dome like i e basis functions the challenge however is that many possibly an excessively large number of hidden neurons may be required for a given problem to attain a desired level of approximation accuracy 3 3 so why more than one hidden layer as proven by hornik et al 1989 and geometrically shown in the example above anns with a sigmoidal hidden layer and a linear output layer are capable of approximating any function with any desired level of accuracy so one may wonder about the need to have deeper anns this section attempts to answer this question via an example let us look back at the original function we aimed to approximate in fig 4a only four sigmoidal units were required as seen in fig 4b to reproduce the dome like feature at the center one might ask can we stick to these four sigmoidal units and somehow smooth the tails yes all that is needed is a second layer with a nonlinear activation function e g sigmoidal to deactivate any feature that is under a threshold in other words in this process the geometry formed by the sigmoidal units in the first layer filters through another sigmoidal unit that bounds that geometry fig 4e shows how adding the second nonlinear layer enables the network to reproduce the original function with only four neurons in the first hidden layer similar to single hidden layer anns those with two hidden layers can approximate any function by putting a number of the dome like functions side by side in general and as shown in the example deeper anns can provide more flexibility while they may require fewer hidden neurons across the network for representation learning however the training of deeper anns has been historically much more difficult because of the now well known problem of vanishing and exploding gradients this problem relates to the fact that the partial derivatives of a loss function eq 2 with respect to weights and biases in first layers obtained via the chain rule of differentiation tend to become very small i e close to zero or very large i e exponentially growing or fluctuating improved algorithms along with higher computational power have now eased that difficulty and made possible the training of very deep anns schmidhuber 2015b lastly a related consideration about the proper number of hidden layers is about the fact that in many problems only a small part of the input space is active in other words some combinations of input variables might not occur in reality and therefore the accuracy of the ann might not matter much in the regions of input space containing those combinations for example consider a case similar to one shown in fig 4b where the corners on the input space do not show up in the data available a hydrological example is where snowfall and temperature are two inputs to anns because snowfall would never occur along with a high temperature the respective part of the input space always remains inactive 4 relevance of occam s razor and equifinality 4 1 issues with the complexity of anns anns are known for their hyper flexibility in fitting data owing to their enormous degrees of freedom for example consider a problem with five inputs and one output a single hidden layer ann with 10 hidden neurons would have 71 tunable parameters 60 weights and 11 biases and adding a second 10 neuron hidden layer would result in a network with 181 parameters 160 weights and 21 biases compare that with linear or quadratic regression models for the same problem which would have six or 21 tunable parameters respectively such large degrees of freedom manifest in large numbers of parameters encountered in the field of anns do not seem consistent with a basic principle in statistical modelling occam s razor occam s razor or principle of parsimony indicates that simpler hypotheses or models should be preferred over more complex ones in other words those models that serve the purpose with as few parameters as possible should be chosen however many data driven modellers in particular in the field of ml have arguably abandoned occam s razor for example ann users typically do not try simpler model types such as regression for the problem at hand and when using anns they do not necessarily look for the most parsimonious network note that some literature proposes systematic approaches to choose a network structure based on growing pruning or other strategies e g reed 1993 teoh et al 2006 xu et al 2006 in practice however such approaches have been of limited use and most ann users choose the network structure on an ad hoc basis or by trial and error see a survey by wu et al 2014 recently giant anns with hundreds of millions of parameters or more have become widespread devlin et al 2018 rajbhandari et al 2019 in addition equifinality a common and widely discussed issue in process based modelling beven and freer 2001 khatami et al 2019 is not generally discussed or considered an issue in the context of anns equifinality concerns the fact that in most cases different model structures and parameter values can lead to similar modelling results in other words model structure and parameters are not uniquely identifiable from data guillaume et al 2019 this is despite the fact that loosely speaking the level of equifinality of anns is much larger than other types of models because of their massively parallel nature in producing model outputs so how does dl handle the above issues the answer is indirectly by trying to avoid their undesired implications which are overfitting and lack of generalizability the former refers to a situation where a model fits the noise in the data rather than the underlying function the latter refers to a case where the model does poorly in out of sample prediction that is predicting situations unseen in the data used for model training various techniques are available in the ann literature to address these issues as outlined in the following 4 2 leashing the hyper flexibility of anns most techniques to control the hyper flexibility of anns and to avoid overfitting fall under two general strategies namely early stopping and regularization before reviewing these strategies in this section let us revisit the common data splitting approach for calibration and validation of models anns and traditional mechanistic models have major differences in terms of calibration and validation in traditional modelling practices the available data are commonly divided into calibration and validation datasets the former is used to identify the model structure and parameters while the latter is used to test the model performance in out of sample prediction in ann practices however the available data are typically divided into three sets commonly referred to as training validation and testing datasets any data chosen for training and testing in the ann context are respectively treated like calibration and validation datasets in the traditional modelling context the third validation dataset in the ann context is needed to leash the hyper flexibility of the network while training the simultaneous use of training and validation datasets during ann training may be best described with the early stopping strategy as follows in the early stopping strategy the quality of fit to the validation dataset is evaluated after each epoch that is an optimization iteration trying to minimize the loss function on the training data see section 2 2 empirically speaking as the training error decreases over time the validation error decreases as well for a while however at some particular epoch the validation error may begin to increase while the training error may keep decreasing see fig 5 this epoch is deemed to mark the beginning of overfitting thus the user stops the training process this strategy is therefore called early stopping in the sense that the training stops early before it can further improve the fit to the training dataset for a review see prechelt 1998 when the training process stops the generalizability of the trained network is assessed via out of sample prediction on the testing dataset regularization is another commonly used strategy to put a leash on the hyper flexibility of anns unlike early stopping this strategy tries to minimize a regularization function during training to control the ann flexibility and tailor it to the problem at hand this strategy has roots in the theory of tikhonov regularization and typically views a more regularized model as one with a smoother response surface tikhonov and arsenin 1977 johansen 1997 a traditional regularization function in the ann context is the sum of the square of all network parameters krogh and hertz 1991 based on the notion that in general the smaller the parameters of a neuron the less activated it is for example in an extreme case where all parameters of a neuron are zero that neuron becomes fully inactive and does not contribute a feature to the overall network response razavi and tolson 2011 provide a more efficient regularization function based on the geometry presented in section 3 where the regularization function is the sum of squares of all of the slopes this regularization function only targets and removes the unnecessary features which are unsupported by data from the overall network response but how can one balance the goodness of fit and smoothness of the network response in practice this is a bi objective optimization problem where one objective is to minimize the error function and the other is to minimize the regularization function these two objective functions are commonly integrated into one loss function via weighting schemes fig 6 shows how the two objectives compete in a real example ideally one may wish to achieve a performance such as that shown in fig 6e doing so is not trivial however because in practice the underlying function is unknown available data are limited and response surfaces are multi dimensional and cannot be easily visualized the bayesian regulation method developed by mackay 1992 and extended by foresee and hagan 1997 has proven useful to adaptively assign the weights associated with each function during training a more advanced and recently developed regularization strategy is called dropout hinton et al 2012 srivastava et al 2014 dropout is a heuristic particularly designed for deep anns that randomly deactivates and then activates different neurons or groups of neurons at each epoch in the course of training when a part of an ann is inactivated in this process the resulting network is called a thinned network the ultimate prediction after training with dropout is viewed as an approximation of the ensemble average of predictions by many independent anns basically the many different thinned networks created throughout the process are assumed to represent anns with different configurations and parameters this heuristic discourages neurons to co adapt too much and as such is believed to avoid overfitting one may find parallels between the ensemble average philosophy of dropout and that of the more traditional bagging strategy in ml originally developed by breiman 1996 that bootstraps available data to develop an ensemble of models and average their outputs 5 fundamental differences from other ml methods 5 1 local versus distributed representations most ml methods such as those based on kernel functions are based on local representations these methods while forming connectionist networks like anns represent each entity e g a training sample point in the input space via an independent processing unit for example radial basis functions broomhead and lowe 1988 gaussian emulator machines kennedy and o hagan 2000 and support vector machines vapnik 1998 cherkassky and ma 2004 may use as many kernels as the number of training samples each kernel typically has a limited radius of influence in the input space and therefore only responds to inputs located in their local neighborhood kernel based methods may face major difficulties when training data include identical or similar samples conversely a unique feature of anns is their ability to learn through distributed representations hinton et al 1986 they typically represent an entity via collective efforts distributed among multiple processing units e g sigmoidal units unlike kernel functions the sigmoidal units typically have large regions of influence see e g fig 2c that overlap each other in the input space see e g fig 4b the former figure shows that a sigmoidal unit influences the entire input space by dividing it into three zones lower tail upper tail and slope the latter figure shows how the influences of four such sigmoidal units are superposed to generate the network response 5 2 implications for users the use of distributed representations has several practical implications to the author s knowledge these include transparency the internal functioning of methods based on local representations is more transparent local representations are the most straightforward and easy to interpret way of learning whereas distributed representations can be complex often leading to emergent properties that cannot be easily explained by local representations hinton et al 1986 learning difficulty distributed representations are more difficult and time consuming to learn in local representations the role of each processing unit may be assigned independently of the other units but in distributed representations many processing units may be configured together in complex ways to represent a feature in the data network size distributed representations often need much smaller network sizes in general the size of the networks based on local representations is directly proportional to the size of the dataset in most cases with a proportionality constant of one that is the number of processing units mirrors the number of training data samples the size of networks based on distributed representations however depends on the complexity of features in the dataset not its size this ability enables anns to scale relatively easily to big data sizes whereas many kernel based methods can become computationally intractable in the presence of large data sets for example ratto et al 2007 report a limit of only 400 sample points for the use with gaussian emulator machines inexact interpolation or emulation networks based on distributed representations are generally inexact emulators this means they do not exactly fit the training samples to represent the features and patterns in the data this is unlike some other ml methods such as radial basis functions broomhead and lowe 1988 and gaussian emulator machines kennedy and o hagan 2000 that are exact emulators perfectly interpolating the training samples other inexact emulators include support vector machines vapnik 1998 cherkassky and ma 2004 and multivariate adaptive regression splines mars friedman 1991 in addition anns are essentially multi output models because they can have as many output neurons as required for a given problem this means a single ann can simultaneously predict different variables while accounting for their possible cross correlations many other ml methods are single output models for example in the case of support vector machines one needs to develop two independent models to be able to predict two variables in the same system 6 how to introduce order time dependency and memory basic anns provide static mapping from inputs to outputs however many applications require mappings with a formal representation of time evolution and memory to enable anns to do so different approaches have been developed in the literature using operators such as delay boxes recurrent connections and information gates as explained in the following 6 1 tapped delay lines a tapped delay line tdl consists of a certain number of time delay operators arranged in an incremental order fig 7 a tdls can be installed on any parts of anns to represent time explicitly the resulting ann shown in fig 7b commonly referred to as a time delay neural network tdnn waibel et al 1989 has been widely used in a range of time series processing applications as such tdnns possess a static memory with an adjustable length the length of a tdl can be viewed as a hyper parameter to be tuned during training along with network structural properties such as the numbers of layers and neurons in each layer adding tdls to an ann significantly increases the number of tunable parameters for example a standard ann with three inputs and 10 neurons in the first hidden layer would have 30 weights in that layer while adding tdls with a length of five to the inputs would result in an additional 150 weights 180 in total to be trained 6 2 recurrent connections tdls as described in section 6 1 explicitly represent time with memory units of limited length unlike tdls recurrent connections first introduced by jordan 1986 enable anns to account for time evolution based on an implicit memory concept which is theoretically of unlimited length and is highly context dependent elman 1990 recurrent connections receive the outputs of a layer at every time step and feed them back to the same or some other layer in the next time step technically they do so via a context unit that stores those outputs in a set of delay boxes fig 7c recurrent connections can be installed on one or more layers e g jordan 1986 elman 1990 or locally on some select neurons e g frasconi et al 1992 an ann enabled with recurrent connections is commonly called a recurrent neural network rnn an rnn can possess many more tunable parameters compared to a standard ann with the same number of layers and neurons using the example given in section 6 1 an ann with three inputs and 10 neurons in the first hidden layer would have 30 weights in that layer whereas adding recurrent connections to that layer similar to fig 7c would add 100 more weights 130 in total to that layer unlike tdnns that possess only a short term memory rnns in theory can represent long term dependencies as well in practice however the implicit memory created by recurrent connections can easily be dominated by short term dependencies in other words even very small features arising from short term dependencies tend to mask features arising from long term dependencies in addition rnns are prone to the problem of exploding and vanishing gradients in their training explained in section 3 3 bengio et al 1994 this is because rnns even with a single hidden layer are in principle deep networks implicitly possessing an infinite number of recursive layers 6 3 gate layers to preserve or forget information over time to balance and explicitly account for both short and long term dependencies hochreiter and schmidhuber 1997 introduced a new type of rnns called long short term memory lstm they extended and further parametrized the context unit also called cell such that the network can more explicitly control what information to hold over time and what to forget the lstm s context unit modulates not only the outputs in the previous time step but also the inputs to the network in the current time step it typically does so via three new independent layers of neurons arranged in the so called forget gate input gate and output gate layers the neurons of each gate layer receive recurrent connections as well as the new input to the network and generate their response between zero and one via using a logistic function see e g fig 7d these responses are then multiplied by their respective signals flowing through the context which means a value of zero would kill a signal whereas a value of one would fully preserve it due to the additional weights and biases in the gate layers an lstm typically has many more tunable parameters than a conventional rnn lstms are now perhaps the most popular and widely used type of anns with memory however lstms took a long time more than a decade to become known and mainstream particularly beyond their core computer science community their widespread application nowadays owes in part to recently developed software tools such as those in python s tensorflow that efficiently implement variations of lstms for a range of problems 6 4 training considerations when the order of data matters the training of memory enabled anns such as tdnns rnns and lstms is different from that of standard anns in terms of the way time ordered data are presented to the network to train standard anns the data entries can be presented in any order even randomly for example through stochastic gradient descent bottou 2010 in memory enabled anns however the data entries should be presented in order of occurrence so that the structure of the time dependency is preserved while this point might seem trivial it requires careful attention in practical applications another point to consider in the training of memory enabled anns is that all data entries are typically viewed to have equal importance regardless of their location in the sequence when used in an online operational forecast however the forgetting factor approach can be used to discount older samples this approach allows the network to adapt to non stationary environments where more recent data are more representative of the underlying processes than older data razavi and araghinejad 2009 lastly the above operators can be combined in a variety of ways a well known combination is time delay recurrent neural networks developed by kim 1998 and used in various applications such as long term precipitation forecasting in karamouz et al 2008 while such combinations may show improved modelling power compared to other ml or statistical methods the attribution of memory gains to the different elements can arguably be challenging if possible at all 7 ml versus process based modelling an experiment ml has been extensively used to model systems for which process based models are also available process based models are based on the physics governing the underlying processes and are therefore typically evaluated based on both their physical realism and goodness of fit to data ml however does not do much if anything with the underlying physics while reportedly doing a superior job in fitting data even in out of sample prediction a fairly large body of literature benchmarks ml techniques particularly anns against process based models examples of such comparisons directly or indirectly in the context of hydrologic modelling include hsu et al 1995 tokar and markus 2000 wilby et al 2003 kratzert et al 2018 kratzert et al 2019 feng et al 2020 and ma et al 2021 some studies such as wilby et al 2003 also detected correlations between the weights of an ann and state variables of a process based hydrologic model as a way to verify that their ann can capture the underlying processes in a hydrologic system this section provides an experiment that runs and compares an ann and a process based model for the same problem and walks the reader through all of the steps involved in particular the processes around calibration and validation role of physics and interpretations of out of sample prediction are discussed this experiment is performed in the context of hydrologic modelling which has seen tremendous progress over the years with respect to both ml and process based modelling 7 1 data and models the case study used aims to model the hydrologic system of the oldman river watershed in alberta canada this watershed has an area of 1434 73 km2 at waldron s corner with a long term average temperature of 2 2 c on average this watershed receives 611 mm of precipitation rainfall snowfall annually and generates 11 7 m3 s of river flow fig 8 shows the 30 year long daily time series data used the first 22 years were used for model calibration i e the seen data in model development and the last eight years for model validation i e the unseen data in model development the first three months of the calibration period were used for model spin up for the ann training the calibration period was further broken into training 17 years and testing 5 years periods the latter for early stopping of the training process to avoid overfitting note that as explained in section 4 2 the naming convention in the ann context for the validation and testing periods is often the other way around to model this system an lstm configuration was chosen here as a state of the art model that accounts for time dependency and memory the inputs to the lstm model are daily precipitation and temperature fig 8a and d and the output is the concurrent daily flow fig 8e the lstm structure was rather arbitrarily chosen to have one hidden layer with five neurons resulting in 166 calibration parameters for benchmarking purposes a classic hydrologic model called hbv lindström et al 1997 as implemented in hbv sask razavi et al 2019 was used with the same data hbv sask is based on a conceptualization of physical principles governing the water movement in a watershed using 12 calibration parameters each of these parameters has a physical interpretation and a physically justified feasible range see fig 9 and table 2 of razavi et al 2019 full detail including data of this oldman river watershed case study which has been developed for educational purposes is available with the source code 7 2 model performance in calibration the model calibration problem was cast as an optimization problem that tries to maximize the goodness of fit to data by tuning the model parameters with the nash sutcliffe efficiency nse nash and sutcliffe 1970 as the objective function nse is essentially a normalized version of mean squared errors computed as 1 var errors var observations as such an nse of one indicates a perfect fit and an nse of zero indicates the model prediction is not any better than the average of observations as a rule of thumb hydrologists often call an nse of 0 7 and higher an acceptable fit the lstm model was calibrated using bp with the early stopping strategy in each epoch the training period data were used to update the network parameters while the testing period data were used to detect possible overfitting five independent replicates of lstm calibration with different initial random seeds were conducted to account for possible variability of model performance fig 9a shows the training results of the five replicates compared to a case where the training would not have stopped as expected the lstm performance keeps improving in training whereas in testing it begins to degrade at some point the objective function in training came very close to one after many more epochs but with very poor performance in testing not shown the hbv model was calibrated by a multi start newton type optimization algorithm similar to lstm five independent replicates of hbv calibration were run fig 9b compares the performance of hbv with that of lstm in calibration at this point only check the performance of the standard lstm model in calibration the figure shows all five replicates of lstm outperform those of hbv note that the calibration performance of hbv shown herein is almost the best the author has achieved so far for this watershed based on these results the superiority of lstm over hbv in calibration is quite significant in regards to the goodness of fit to streamflow the performance of the two models in validation is discussed in section 7 4 but before that the next section discusses what information the two contained prior to calibration 7 3 what about a priori information encoded in the models at this point let us step back and investigate what we have achieved in terms of learning from data for both the lstm and hbv models the development of the lstm model was not based on any a priori knowledge of how a watershed system works and the governing physical principles as such the model learned everything from scratch merely using examples from data basically the model started with a fully randomized internal configuration controlled by a large number i e 166 of parameters and then tuned those parameters to adapt the internal functioning of lstm to the underlying real world system represented in the data fig 10 a shows the lstm performance of arbitrarily chosen replicates before and after calibration the model response to inputs before calibration seems to be completely random but after calibration the model response has learned to closely follow the underlying system response unlike lstm hbv encodes the expert knowledge available in the field of hydrology this model is a collection of conservation of mass equations and process parametrizations that represent how hydrologists conceptualize the way a watershed works this physically based modelling structure is presumably able to emulate the behavior of any watershed by tuning only 12 parameters fig 10b shows how the hbv model performs before calibration with parameter values chosen to be at the midpoint of their ranges and after calibration the figure shows the uncalibrated model responds reasonably to the inputs it generally captures the timing of flows and emulates the low flow segments well but is overly responsive to large precipitation events generating spurious spikes in flows calibration either manual by expert knowledge or automatic as done here via optimization can fix the discrepancies and fit the model output to observations so a fundamental difference between the two approaches is now clearer using a process based model is about directly using the expert knowledge available in a scientific field and tuning it to the case study of interest while using anns is about learning everything from scratch directly from data this difference is manifest in the number of parameters that need to be tuned to achieve a reasonable performance notably the lstm model achieved a better performance in emulating observations after calibration as evident in a comparison of fig 10a and b however in any modelling exercise one needs to ensure the model gives the right answer for the right reasons kirchner 2006 that is why proper model evaluation in out of sample prediction is critically important as discussed in the next section 7 4 model validation standard versus true out of sample prediction in general validation and verification of mathematical models are very challenging in some scientific disciplines if possible at all oreskes et al 1994 the standard practice however is to test the performance of the model under investigation in terms of reproducing some historical record not seen during model calibration klemeš 1986a a process called out of sample prediction in this paper fig 9b shows the results of such practice in the validation period set in fig 8 in this case both lstm standard and hbv models do reasonably well in regards to goodness of fit to streamflow with lstm outperforming hbv across all replicates in addition and as expected both models produced slightly lower nse values in validation compared to those in calibration a point is as oreskes et al 1994 articulated the above so called model validation is inherently partial while the performance of lstm appears to be better than that of hbv in a relative sense one needs to take extra care before making such a conclusion as argued by klemeš 1986a more than three decades ago a strong assumption in this type of validation is that the conditions under which the model will be used will be similar to the conditions under which the model has been developed and calibrated it is now well recognized that such an assumption may not hold as many natural systems are essentially non stationary milly et al 2008 despite such recognition this standard model validation practice has arguably remained unchanged humphrey et al 2017 beven 2018 here let us take a stress test approach via a what if scenario question to test and compare the performance of both models in a true out of sample prediction basically under conditions that have not truly been seen in the process of model development and calibration the question is how the system would behave if the average temperature warmed by 2 c while everything else remained the same to assess this hypothetical scenario both calibrated models were fed a new temperature time series obtained by adding 2 c to all daily temperature values of fig 8d these new synthetic inputs roughly provide a picture of what might happen in this watershed under global warming the modelling results under such scenarios are typically used to inform policy making for climate change adaptation now let us use the two different models to evaluate the possible changes in the watershed behavior in response to a 2 c warming here instead of looking at individual simulated time series the possible change in the average seasonality of flows is of interest first look at fig 11 a to check the consistency of simulated flows for the historical period both models generally follow the observed seasonality but the range provided by the lstm model is generally narrower and better encapsulates observations in both low and high flows under the new conditions however the two models show the two distinct behaviors shown in fig 11b according to lstm peak summer flows would decline by about 25 on average and the time of the peak would shift backward by about a week from the beginning of june to a time in the fourth week of may according to hbv however the changes would be more pronounced the peak flows would decline by about 35 on average and the flows might show two modes the higher one at the beginning of may and the other at the beginning of june at about the same time as the peak in the historical observations are such differences not sufficiently large so as to make the user skeptical about the modelling process 7 5 injecting some physics into ml at this point one may wonder about the possibility of ensuring that lstm results be physically consistent particularly under new conditions let us give it a try by recasting the modelling problem based on some understanding of the governing physics in hydrology for example physics tells us that the freezing point of water is around 0 c and therefore this threshold could be used as an approximation to differentiate rainfall from snowfall on a daily basis i e if the temperature on a day is above below 0 c the precipitation on that day if any is considered to be rainfall snowfall see fig 8b and c this differentiation is actually a part of process parameterization in hbv similar to many other hydrologic models via a parameter called temperature threshold tt for freezing thawing and separating rain and snow with a feasible range from 4 to 4 c the warming of a watershed would naturally change the rainfall to snowfall ratio and so integrating this domain knowledge with the lstm model makes sense perhaps the most straightforward way of introducing the tt concept to lstm is via pre processing of the inputs therefore a new lstm model was developed and calibrated called process informed lstm in this paper with three inputs rainfall snowfall and temperature as shown in fig 8b c and d similar to the original the new lstm model has one hidden layer with five neurons resulting in 186 calibration parameters the procedure for the calibration and validation of the process informed lstm was the same as for the standard lstm already explained in sections 7 2 and 7 4 fig 9b compares the performance of the process informed lstm with hbv and the standard lstm the figure shows the two lstm models perform comparably well process informed lstm results in a slightly lower average nse in validation but with only five replicates this small difference should be interpreted with caution fig 11b demonstrates the performance of the process informed lstm model in the true out of sample prediction according to this model the summer peak flows would decline by 20 on average and the time of peak would appear about two weeks earlier than in the historical record in the third week of may in general accounting for some known physics shifted the timing of the lstm s rising limb to the left to become more consistent with that of hbv such a shift is expected because now the model accounts for the new conditions under which there would be more rain and less snow and rain tends to run off more quickly whereas snow is stored in the mountain for longer times before it melts and appears in streamflows downstream incorporating any further physical knowledge to the lstm will likely further shift the rising limb to the left for example one may attempt to inform the model about the known direct relationship between temperature and snowmelt rate and the thawing freezing threshold doing so however is less straightforward than the basic approach implemented above and requires modifying the internals of the model a review and discussion on more advanced strategies to incorporate the knowledge available into anns come later in the paper in section 7 3 7 6 so what model should we trust the ml or process based model now the question is which one of the three models produced the most credible picture of possible watershed behavior under the new conditions in practice this question is very difficult to answer if possible at all in general the prediction of such changes can be debated and might vary from one study to another depending on the models and data used and disciplinary views perhaps a definite answer would need to wait until the future has come and shown such possible changes and from a bigger picture point of view models of natural systems cannot be verified or validated in true out of sample prediction because those systems are never closed and not everything can be represented in a model as argued by oreskes et al 1994 nearly three decades ago but as scientists we have our own perceptions and intuitions these might be biased but still useful to provide a ground for building confidence in the credibility of a model in the context of the case study given previous research on the canadian rocky mountains has indicated that warming alone will result in a considerable reduction in flows and earlier peaks in watersheds similar to the oldman river watershed a synthesis of research efforts under the changing cold regions network ccrn debeer et al 2021 on the cold interior of western canada indicates a shift in timing of the spring hydrograph rise and peak flows of nearly two weeks earlier by mid 21st century and as much as one month by the late 21st century these projections which themselves are based on rigorous atmosphere landsurface modelling are consistent with the modelling results presented in the previous sections but cannot pinpoint the most accurate model what is worrisome is the large divergence in behavior of models in response to expected but yet to be seen perturbations whereas those models produce comparable results in standard out of sample prediction broadly speaking one might say any known consistency of a model with the known underlying physics can improve model s explainability and interpretability thereby helping us better explain the model behavior in response to such perturbations explainability and interpretability are fundamental assets in building trust in a model and of course physically based models are advantaged in that respect related to the notion of explainability and interpretability one way to boost confidence in a model is to go beyond checking only for the intended predictand streamflow in this example and to analyze or even constrain the model performance in terms of other fluxes such as evapotranspiration e g vervoort et al 2014 or state variables such as soil moisture e g yassin et al 2017 ml models however are typicality not developed to allow such analyses while doing so is relatively straightforward in the case of process based models enabling ml to do so requires an in depth understanding and appreciation of the value of domain knowledge as discussed in the next section 8 discussion 8 1 what is the typically ignored value of domain knowledge in dl true out of sample prediction is nothing but extrapolation beyond the observed data and behaviors used in model development and calibration extrapolation is a reality that many predictive models nowadays must face because of non stationarity in climate and the environment milly et al 2008 any purely regression type model including those arising from dl would be disadvantaged in extrapolation as by definition extrapolating would require working in parts of the problem space for which they have not received any information conversely mechanistic models may be salvaged in extrapolation by the domain knowledge encoded within them but what does domain knowledge offer when it comes to extrapolation the answer is a set of principles modulated via conservation laws e g mass energy and momentum and process parametrizations which represent our perceptions of how two or more variables might be related gupta et al 2012 such principles have been developed and evolved over time based on extensive observation and research by scientists and practitioners the limits of validity of such principles are typically known in the following the importance of taking advantage of those principles in modelling and prediction is discussed with respect to three aspects conservation laws monotonicity and rates and feedback mechanisms conservation laws in physics a conservation law states that a specific measurable property does not change within an isolated system with time such a law is usually expressed as a continuity equation that is a differential equation equates the rate of change in storage within a control volume with the difference between what comes in and what goes out of the control volume in land surface modelling for example conservation laws are built into mechanistic models to ensure water and energy balance is preserved in simulations over time ml models however do not automatically account for such laws and as a result water or energy can be falsely introduced or lost in the course of simulation monotonicity and rates the knowledge base includes the general characteristics of some causal relationships between various physical variables for example we know from basic thermodynamics that the relationship between melt rate and available heat is monotonic that is more heat causes a higher melt rate furthermore we have some rough estimate of the feasible range of the rate of change in one with respect to the other similarly from basic hydrology we know the causal relationships governing the way a hillslope stores and releases water are generally such that a positive correlation exists between water available in the soil and its contribution to flows more water means more flows due to gravitational forces mechanistic models directly account for such knowledge on causal relationships this knowledge is encoded in process parametrizations typically in the form of deterministic monotonic functions or rarely in hysteretic forms with a limited number of parameters to be calibrated to the specific case study in hand gupta et al 2012 however in the case of hyper flexible models such as anns such functions need to be entirely derived from data all from scratch and ignoring the knowledge base related to those monotonic relationships therefore extrapolation runs the risk that such relationships become non monotonic and or have unrealistic rates producing erroneous behaviors this risk is exacerbated by the fact that identifying and diagnosing such errors are very difficult if possible at all feedback mechanisms a real world physical system is a combination of variables that interact over time typically via a range of feedback mechanisms such feedback mechanisms control the internal dynamics of the system and are key to its evolution over time for example consider a coupled water vegetation system in which precipitation available soil moisture and plant biomass interact in complex time dependent ways even at times creating positive feedbacks that destabilize the system s behavior rodriguez iturbe et al 1991 scheffer et al 2001 the knowledge base available about these feedback mechanisms is often built into mechanistic models using differential equations ordinary or partial to describe the system dynamics the representation of such dynamics in the making of models is important particularly for long term predictions and over long time scales dl models are often unable to account explicitly for such long term dynamics if a particular dynamical behavior is present in training data then dl can capture that behavior in its mapping from input onto output dl however has no explicit mechanism to represent that dynamic under perturbed conditions beyond what has been recorded in the training data is mechanistic modelling immune to issues with extrapolation certainly not while a discussion on the limitations and prospects of mechanistic modelling is beyond this paper one solution to improve extrapolability of mechanistic modelling over time which is also relevant to ml is space for time substitution assuming spatial and temporal variations are equivalent this strategy is to investigate multiple or many sites simultaneously instead of one to infer a temporal trend for a site based on information from other sites that have different properties and or experienced different conditions for example refer to pickett 1989 and blois et al 2013 in the context of ecology and singh et al 2011 in the context of hydrology in the era of big data ml can benefit explicitly or implicitly from such strategies when spatio temporal data across large domains are available for example kratzert et al 2019 feng et al 2020 and ma et al 2021 utilize the camels dataset which includes catchment attributes and hydrometeorological data across many different sites newman et al 2014 addor et al 2017 to improve the performance of dl in hydrological modelling applications in addition both modelling paradigms ml and mechanistic modelling can benefit from knowledge based patterns and behaviors derived from data around a real world system for example the notions of limits of acceptability beven 2006 hydrological signatures gupta et al 2008 and pattern oriented modeling grimm et al 2005 grimm and railsback 2012 can potentially inform the process of developing and validating models of any type such knowledge may be directly incorporated into the loss functions used to train dl models as described in section 8 3 the bottom line is that mechanistic models are generally expected to be less prone to generating spurious behaviors in true out of sample prediction therefore many domain experts may be inclined to trust physically based models as their behavior is constrained by physical laws that are perceived as unchanging with time the points made in this section will become clearer in the next section where the essential differences between dl and mechanistic modelling are discussed 8 2 why is dl essentially different from process based modelling in the author s view the first principles of anns are rooted in connectionism hyper flexibility and vigorous optimization these characteristics are fundamentally different from the guiding principles of developing and calibrating mechanistic models as described in the following connectionism is an approach that orchestrates a set of simple algebraic operations in a massively parallel manner to create a model that is able to carry out complicated tasks following this approach anns represent the response of a system under consideration to an input by summing the collective efforts of many neurons whose roles cannot be easily attributed to individual processes involved in that system this is unlike mechanistic modelling where each part of a model is designed to be responsible for a specific process hyper flexibility is a characteristic of a model with excessive degrees of freedom which can literally fit any dataset and is not constrained by the many assumptions held by typical statistical models anns are known to be hyper flexible mechanistic models however have limited degrees of freedom depending on the knowledge base available about the processes being modelled ideally mechanistic models tend to have just as many degrees of freedom as can be supported and constrained by available knowledge and data vigorous optimization here refers to the practice of manipulating model parameters at any cost to maximize the goodness of fit to calibration data the training of anns is all about minimizing an error function that is among two competing anns the one producing smaller errors in calibration and validation is the winner optimization is also often an essential part of mechanistic modelling to calibrate model parameters however in mechanistic modelling minimizing the errors is not the goal but a means to improve the realism of the model in other words unlike the case of anns physical feasibility of a parameter its identifiability and equifinality are typical considerations in mechanistic modelling the recognition of these fundamental differences is critically important when one aims to choose the right modelling paradigm for a purpose compare the two paradigms in a case study or attempt to bridge the two paradigms possibly for improved modelling performance the following section outlines the status quo for bridging the two paradigms and some emerging trends 8 3 how can we bridge dl and process based modelling the history of research on reconciling and bridging anns with mechanistic modelling in earth and environmental sciences dates back to the early 2000s or perhaps earlier these efforts have generally had the objective of simultaneously leveraging the strengths of the two modelling paradigms to further our knowledge and predictive ability abrahart et al 2012 reviewed such research in the context of hydrology and refer to it as hybridization they introduced three possible approaches for this purpose which herein are referred to as surrogate modelling one way coupling and modular coupling seven years later reichstein et al 2019 in an influential article in nature re introduced the notion of hybrid modelling and the above three approaches as the next steps in earth science in the following these three approaches are explained and then more modern existing approaches arising from research fields beyond earth and environmental sciences are discussed surrogate modelling alternatively called metamodelling or model emulation refers to the process of developing and applying a simpler cheap to run model in lieu of a more complex computationally intensive model in this process a data driven surrogate such as an ann is trained on samples of a limited number of original model runs to approximate the model response surface the developed surrogate model can then be used in different frameworks in conjunction with the original model in multi query applications such as optimization and uncertainty quantification example applications of anns as surrogates of mechanistic models include johnson and rogers 2000 broad et al 2005 behzadian et al 2009 and vali et al 2021 the reader may refer to a review by asher et al 2015 for a range of approaches used for surrogate modelling one way coupling refers to the process combining a mechanistic model with an ml model such that the output of the former feeds into the latter as input a general rationale for such a combination is that a mechanistic model may not be able to fully explain the observed data and therefore an ml model could be of help in extracting any information left in the residuals of the mechanistic model for example consider a case where a mechanistic hydrologic model is used for streamflow forecasting and as expected some errors in model outputs are present an ann can be used to model such errors over a historical period to provide some predictive ability on the distribution of errors for a time step into the future then running these two models in sequence may provide higher forecasting skills example applications of such one way coupling include shamseldin and o connor 2001 anctil et al 2003 solomatine and shrestha 2009 wani et al 2017 and li et al in review modular coupling refers to cases where an ml model is used as a module sub model of a larger mechanistic model or vice versa the rationale for this type of coupling may be that a particular model might have proven skills in representing a particular process and is therefore preferred while other processes are better represented by another model modular coupling is intrinsically an ad hoc process which can be done in a variety of ways depending on the problem at hand and models and data available for example chen and adams 2006 and corzo et al 2009 used anns as the routing module within a distributed hydrological model chua and wong 2010 developed an ann based hydrologic model using the output of a kinematic wave model as one of its inputs mekonnen et al 2015 developed and coupled a process based model and an ann model for simulating hydrology in contributing and non contributing areas of prairie regions respectively humphrey et al 2016 developed an ann model of monthly streamflow prediction that receives simulated soil moisture as an input from a process based hydrologic model hunter et al 2018 coupled a process based in stream salt transport model an ann based saline groundwater accession model and linear regression models of floodplain storage bennett and nijssen 2021 developed an ann model for the simulation of turbulent heat fluxes and built it into a process based hydrologic model beyond the earth and environmental sciences community the notion of bridging the knowledge base and ml has a long history e g see the knowledge based artificial neural networks by towell and shavlik 1994 but it has received significantly more attention recently different approaches mostly arising from mathematics and computer science have been proposed under titles such as theory guided data science karpatne et al 2017a informed machine learning von rueden et al 2019 and physics informed neural networks raissi et al 2019 to name a few providing a full coverage of such approaches is beyond the scope of this paper and many of them have been developed for specific application areas with limited relevance to earth and environmental problems in the following three relevant approaches are explained regularizing anns via knowledge based loss terms a new regularization function can be developed based on the available knowledge surrounding a given problem and be added to the loss function used in training for example any violation of the conservation laws or monotonicity of relationships as described in section 8 1 can be quantified and penalized during training refer to stewart and ermon 2017 and karpatne et al 2017b to see how this approach can work in two different application areas the former in image processing and the latter in lake temperature modeling using mechanistic model runs to augment ann training data a mechanistic model can be used to simulate the system under investigation under a range of conditions to generate synthetic data to augment the available training data this approach may be particularly useful in guiding anns in extrapolation beyond conditions seen in the original training data see the discussion in section 8 1 this approach is based on the assumption that the mechanistic model used is sufficiently accurate an assumption that needs to be treated with caution for an example of this approach in the field of systems biology see deist et al 2019 integrating differential equations into anns this approach is a very recent and perhaps the most mathematically elaborate in terms of integrating the knowledge base into anns primarily developed by raissi et al 2019 it parametrizes the known differential equations describing a system and integrates them into the body of anns the integrated model is then trained to the available data simultaneously inferring the parameters of the differential equations and network weights one could view this approach as an extension to the knowledge based loss terms described above where the new loss term penalizes the network for deviations from those known differential equations this approach still seems embryonic but perhaps with great potential for scientific breakthroughs 8 4 what can we learn from prominent ml applications ml has already been used across a wide range of disciplines and applications but with varying degrees of success here and for context consider two special and well known applications playing chess and predicting the stock market ml has achieved incredible superhuman level performance in chess and similar games silver et al 2018 while its performance in stock market prediction has been criticized despite its widespread application e g pearlstein 2018 these opposing outcomes may be explained by the following reasons chess does not possess any properties of complex systems bar yam 1997 whereas financial systems are essentially complex with a wide range of agents interacting at a wide range of scales giving rise to emergent behaviors and even black swans any ai based financial services themselves would also be agents influencing the stock market even possibly inducing vicious cycles chess can be viewed as a closed system as no exogenous factors influence any properties or dynamics of the board and players whereas stock markets are open systems and for any analyses the assumed boundary conditions depend on the analyst s judgement chess is a fully observable system as the entire board pieces rules and moves are seen by the players but stock markets are only partially observable and some controlling elements in the market might be hidden to the analysts chess is stationary as the properties and governing rules of the game remain constant over time whereas stock markets are non stationary and their long term dynamics and behaviors may change in unpredictable ways driven by political social economic or natural events so what earth and environmental systems arguably fall somewhere in between these two specific applications with respect to their four fundamental and inter related characteristics such systems are complex open partially observable and non stationary loosely speaking understanding and predicting earth and environmental systems face similar challenges to those of the stock markets in terms of those four characteristics however unlike stock market systems that are conceived to be partially predictable at best fama 1970 malkiel 2003 the behaviors of earth and environmental systems are generally believed to be predictable with limits of predictability that have been improving as more knowledge and data become available the comparisons above try to convey two points first the revolutionary success of dl in one field of application cannot necessarily be extended directly to another field of application the context matters and success depends on the characteristics of the problem at hand second different disciplines may cross fertilize dl applications and learn from one another however cross fertilization is non trivial and requires more direct communications between experts in different disciplines about existing methods common issues and ways forward 9 concluding remarks dl has perhaps by now served every researcher and practitioner in earth and environmental sciences communities in tasks such as image and language processing at least through their smart phones such astonishing and within reach technologies have boosted interest in dl and in ai in general within these communities evidenced by the significant growth in the number of their research papers on dl many including the author of this paper believe the combination of ai with unprecedented data sources and increased computational power will offer exciting new opportunities for expanding our knowledge about various earth and environmental systems unsurprisingly similar to many other innovations ai and particularly dl techniques are facing different and sometimes contrasting views towards their future for example in the hydrology context nearing et al 2020 suggest a divorce from the current hydrological theories while beven 2020 advocates for the fundamental role of knowledge base in dl interpretation it is certainly an exciting time for earth and environmental sciences to benefit from dl tools shen et al 2018 picture a bright future but articulate some important technical and cultural challenges to overcome in the years to come by more targeted educational and organizational efforts we need also to be mindful of any possible risk of hype and over excitement around these new tools arguably still many applications of dl in earth and environmental sciences have primarily focused on off the shelf applications of methods largely developed by mathematicians and computer scientists to problems in a new domain with no or limited considerations of the available domain s knowledge base the immediate risk of such practices is that the popularity of ai tools in earth and environmental sciences would then follow the ups and downs of those tools in the areas from which they originate there is also a greater risk in the author s view as follows let us flash back to more than three decades ago when the prominent statistician george box 1976 p 797 798 warned about the mathematistry trap characterized by development of theory for theory s sake which since it seldom touches down with practice has a tendency to redefine the problem rather than solve it he argued that there is unhappy evidence that mathematistry is not harmless in such areas as sociology psychology education and even i sadly say engineering investigators who are not themselves statisticians sometimes take mathematistry seriously overawed by what they do not understand they mistakenly distrust their own common sense and adopt inappropriate procedures devised by mathematicians with no scientific experience this sentiment was then echoed by the prominent hydrologist vit klemeš 1986b p 177 and p 185 who said the danger increases with the proliferation of computerized hydrologic models whose cheaply arranged ability to fit data is presented as proof of their soundness and as a justification for using them for user attractive but hydrologically indefensible extrapolations he continued the danger to hydrology from extrapolations based on mathematistry is that they lead it on the path of bad science the point here is that the risk of mathematistry seems to be just as fresh as it must have been back then particularly when it comes to the application of ai tools in earth and environmental sciences due to the very nature of such tools this risk may even well extend to their original areas of application party because of their lack of explainability and interpretability to a point that such practice has been referred to as a form of modern alchemy see rahimi and recht 2017 for the sentiment lecun 2017 for a rebuttal and hutson 2018 for a summary at a more fundamental level rudin 2019 argues that trying to explain black box models rather than creating models that are interpretable in the first place is likely to perpetuate bad practice and can potentially cause great harm to society this point is not to undermine the benefits of ai technology particularly for earth and environmental applications instead it calls for improved rigor and better appreciation of possible issues after all it has been long known even in environmental sciences that complex models can be made to produce virtually any desired behavior fallaciously given their large degrees of freedom as articulated by hornberger and spear 1981 three decades ago having such risks in mind the new potential afforded by ai for earth and environmental sciences is great to realize this potential we need to reconcile data driven ai techniques and the theory driven knowledge base the knowledge base is at the heart of traditional programming which is still a major building block of process based mechanistic modelling in earth and environmental sciences clearly the traditional knowledge based programming and ai are made up of two fundamentally different worldviews for problem solving and therefore their reconciliation will not be straightforward this paper tried to address some critical questions in this regard and provide some perspective for this important endeavor in anticipation of new breakthroughs in earth and environmental sciences in an age of big data and computational power 10 postscript i had the privilege to receive relatively extensive feedback after this paper was posted on an open access venue until the formal peer review process was completed i attribute this interest and sometimes perhaps disinterest in the paper to the extensive excitement that currently exists around ai and ml in earth and environmental sciences community among the review comments that i received there were two extreme but contrasting views on the message and sentiments of this paper one reviewer believed that this work is an attempt to ignore or minimize the benefits of the recent developments in dl for environmental modelling which are essentially the applications of deeper anns that have been made possible by the growth in computational power and data availability conversely another reviewer believed that this paper exaggerates such benefits and that the classic shallower anns that have been subject to extensive research in the past three decades can work at least equally well in environmental modelling i was intrigued by these two contrasting views and thought i would reflect on them in this postscript section this paper was not intended to support any of these contrasting views instead the intention was to provide a rather balanced overview of how far we have come and the long standing challenges in an attempt to better inform and shape future ai initiates in our community away from any possible hype or over excitement one way or the other my take is that either of those contrasting views may be subject to personal biases arising from factors such as research background and career stage the former view seems to resonate primarily with the newer generation of researchers who are currently championing dl in earth and environmental sciences the latter view however seems to be shared by more senior colleagues whose research portfolios include a long history of work on anns my personal communications suggest that this group often feels frustrated by the recent avalanche of papers branded under deep learning many of which might be considered by the group as reinventing the wheel ignoring similar previous successful work in the field of environmental modelling and beyond being mindful of this fact the present paper included numerous older papers with ideas that still seem fresh today with respect to branding i would like to include a quote from a recent book by duerr et al 2020 that in the earlier days of machine learning ml neural networks nns were already around but it was technically impossible to train deep nns with many layers mainly because of a lack of computer power and training data why do we talk about dl instead of artificial nns dl sells better than artificial nns this might sound disrespectful but such rebranding was probably a smart move especially because nns haven t delivered what was promised during the last decades and therefore gained a somewhat bad reputation and a quote from the famous book by goodfellow et al 2016 that at this point the authors refer to the early 2000s deep networks were generally believed to be very difficult to train we now know that algorithms that have existed since the 1980s work quite well but this was not apparent circa 2006 the issue is perhaps simply that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time any interpretation of the above statements should of course consider their context which is computer science and areas of applications which primarily are computer vision and natural language processing also deep anns in those applications typically have thousands of layers while in earth and environmental modelling deep anns may have significantly fewer layers perhaps in the order of tens or less however the above statements suggest that our community might need to be more careful in branding recent projects and initiatives around dl with stronger connection with earlier similar work performed in our fields a reviewer asked for a direct comparison of deep versus shallow anns wondering if deeper ones have really any superiority for environmental modelling i think running such comparisons would require extensive numerical experiments across several case studies and may not be straightforward instead this paper tried to explain an old theorem in simple language how anns either shallow or deep are universal function approximators and can basically approximate any function if set up properly therefore for typical environmental modelling i am of the mind that one can make either one work but the modelers need to be aware of nuanced issues as described in this paper beyond any debate around deep versus shallow anns a primary motivation of writing this paper was to address a long standing issue that there is some mistrust in ml in part of our community particularly among process hydrologists and theory driven modellers i have extensively dealt with that issue over the years as a researcher who started his research career with ml and anns in hydrology back in 2003 but gradually delved into process based modelling my observations suggest that ml and process based modelling have largely evolved in isolated research camps among which the co creation or exchange of knowledge has been limited the present paper might serve in bringing the two worldviews together and promote the dialogue between the champions of process based modelling and those of ml this dialogue might be much needed at this time of excitement and increased funding around ml in earth and environmental communities such dialogues can help us ensure proper training of the current students so they retain curiosity about physical understanding and expand our knowledge base while being equipped with the emerging data science technologies and the ever growing computational power declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper was shaped by discussions with david hannah amin elshorbagy hoshin gupta grey nearing steven weijs dimitri solomatine sujay kumar lucy marshall amin dezfuli thorsten wagener yashar mehdad facebook ai and many others over the last year who directly or indirectly provided feedback on different aspects covered also i am thankful to dan ames tony jakeman joseph guillaume and holger maier from environmental modelling and software for providing constructive comments that helped me improve the paper this work was a part of my sabbatical plan that was scheduled to take place during 2020 21 in australian national university university of birmingham as a ias vanguard fellow and university of bristol as a benjamin meaker distinguished visiting professor although these visits were cancelled due to the covid 19 pandemic i am in indebted to tony jakeman david hannah and thorsten wagener for their gracious and generous offers to host me and my family at their respective institutions and my special thanks to nasim my wife and the kids kian and nikan who made my stay at home sabbatical time memorable full of love and fun lastly funding supports from the natural sciences and engineering research council of canada discovery grants and integrated modeling program for canada impc under the framework of global water futures gwf are acknowledged 
25765,recent breakthroughs in artificial intelligence ai and particularly in deep learning dl have created tremendous excitement and opportunities in the earth and environmental sciences communities to leverage these new data driven technologies however one needs to understand the fundamental concepts that give rise to dl and how they differ from process based mechanistic modelling this paper revisits those fundamentals and addresses 10 questions that might be posed by earth and environmental scientists and with the aid of a real world modelling experiment it explains some critical but often ignored issues dl may face in practice the overarching objective is to contribute to a future of ai assisted earth and environmental sciences where ai models can 1 embrace the typically ignored knowledge base available 2 function credibly in true out of sample prediction and 3 handle non stationarity in earth and environmental systems comparing and contrasting earth and environmental problems with prominent ai applications such as playing chess and trading in stock markets provides critical insights for better directing future research in this field keywords artificial intelligence machine learning deep learning artificial neural networks process based modelling earth systems hydrology plain language summary the recent unprecedented performance of deep learning dl in image and language processing has accelerated applications in non native areas such as earth and environmental sciences where knowledge driven process based modelling has dominated to date a major challenge however is dl and process based modelling are rooted in different worldviews towards problem solving this paper explains the whats and whys of dl from first principles and how they are different from those of process based modelling a hydrologic modelling experiment is presented to illustrate the fundamental differences between the two worldviews and to shed light on some major issues dl has to deal with in the modelling of earth and environmental systems these issues largely arise from the fact that such systems are complex with behaviors that can change in ways that are physically explainable but not seen in the period of record due to factors such as climate change and human interventions such issues must be addressed at the heart of the endeavor to extend dl techniques that embrace the knowledge base available in anticipation of breakthroughs in an age of big data and computational power 1 the rise of deep learning the last decade has witnessed a tremendous rise in techniques called deep learning dl under the umbrella of artificial intelligence ai and machine learning ml and their unprecedented performance in areas such as computer vision krizhevsky et al 2012 natural language processing young et al 2018 and gaming silver et al 2018 these successes have motivated the application of dl across a wide range of disciplines including medicine hosny et al 2018 earth sciences reichstein et al 2019 robotics torresen 2018 engineering panchal et al 2019 and finance lee et al 2019 dl owes its exemplary success to the boom in computational power and the emergence of big data sources and associated data storage and sharing technologies earth and environmental sciences appear to be positioned to benefit from dl as big data sources on a range of in situ and remotely sensed variables are becoming increasingly available with the advances in sensing technologies reichstein et al 2019 mao et al 2020 the storage volume of remote sensing data for earth observations is already well beyond dozens of petabytes with transmission rates exceeding hundreds of terabytes per day datasets based on model outputs are rising for example the climate assessment dataset provided by the coupled model intercomparison project phase 6 may reach 40 petabytes eyring et al 2016 reanalysis climatic datasets have also grown for example nasa s modern era retrospective analysis for research and applications version 2 merra 2 is 400 terabytes gelaro et al 2017 in addition datasets generated via tens of thousands of citizen science projects are providing large and rich sources of ground based data this potential is shifting the attention of earth and environmental scientists and relevant funding agencies towards ml as evidenced for example by the shift in research work presented at the american geophysical union agu s fall meetings the largest assembly of earth and environmental scientists with more than 27 000 people in attendance and 25 000 presentations in 2019 the number of ml related presentations has risen consistently from 0 2 of total presentations in 2015 to 4 2 in 2020 in particular this shift has been astonishing in the agu s non linear geophysics earth and space science informatics natural hazards hydrology and seismology sections where 28 2 1 18 5 1 9 1 3 7 5 1 4 and 6 7 0 9 of total presentations respectively were related to ml in 2020 2015 recent successful applications of dl techniques to earth and environmental sciences include weather nowcasting and forecasting shi et al 2015 2017 satellite precipitation bias reduction tao et al 2016 rainfall runoff modelling kratzert et al 2018 feng et al 2020 ma et al 2021 rain and snow retrieval from spaceborne sensors tang et al 2018 downscaling hydroclimatic variables ducournau and fablet 2016 precipitation estimation tao et al 2018 pan et al 2019 and surrogate modelling gu et al 2020 yu et al 2020 unsuccessful applications perhaps similar to many other areas remain largely unreported in the peer reviewed scientific literature but occasionally appear in other media e g wexler 2017 kolakowski 2018 rudin 2019 notably dl is a trendy term that refers to a specific class of artificial neural networks anns which have been around and widely applied in earth and environmental sciences since the early 1990s with the birth of domains such as hydroinformatics abbott 1991 these applications are documented in reviews by gardner and dorling 1998 maier and dandy 2000 krasnopolsky 2007 maier et al 2010 abrahart et al 2012 razavi et al 2012 shen 2018 bergen et al 2019 and reichstein et al 2019 arguably however the uptake of anns in general to facilitate and advance earth and environmental sciences has not kept pace with data availability and computational power over the past three decades but why the challenges impeding the widespread application of anns to earth and environmental problems to date may be rooted in the fact that convincingly casting those problems for which an extensive knowledge base is usually available within the ann framework is often not straightforward moreover the lack of interpretability and explainability of anns has been a major hindrance as model developers need to be able to make sense of why a model functions the way it does and to explain that to model users these challenges can be further complicated in the absence of a solid understanding of the fundamentals of anns and how they differ from theory driven mechanistic modelling and prediction mechanistic modelling also called process based or knowledge based modelling in this paper has traditionally been the cornerstone of scientific advancement and policy support so why this paper motivated by the recent breakthroughs by dl in its original areas of application namely computer vision and natural language processing this paper aims to address the persistent challenges facing dl applications in non native areas related to earth and environmental sciences with this overarching aim this paper addresses but not necessarily answers 10 questions regarding the fundamentals of dl and its explainability and bridgeability to earth and environmental systems modelling 1 what is dl and how did it evolve from anns 2 how can we interpret the internal functioning of dl 3 does dl abandon occam s razor or the principle of parsimony 4 why is dl considered superior to other types of ml 5 how can dl account for memory and time dependency 6 how do dl and process based models compare in calibration and validation and which one may be more trustworthy 7 what is the value of the often ignored domain knowledge in dl and why may dl suffer from this ignorance 8 why is dl essentially different from process based modelling 9 what are the existing approaches to bridging dl and process based modelling 10 what can we learn from prominent dl applications such as gaming and the stock market the structure of this paper is such that it best serves the reader when all sections are followed sequentially however an advanced reader could directly refer to a section that is more relevant to a question of interest sections 2 through 7 are about questions 1 through 6 and sections 8 1 through 8 4 are about questions 7 through 10 respectively a simple hydrological modelling problem and multiple synthetic functions are used to explain complex concepts via simple examples the contents of this paper are intended to be accessible to a wide audience from various fields under the umbrella of earth and environmental sciences however the views presented mainly arise from the author s data and theory driven research background in hydrology and water resources this paper concludes in section 9 by revisiting some ancient issues pertaining to modelling in general around the mathematistry trap raised by the late prominent scientists george box 1976 and vit klemeš 1986b those issues can be just as fresh today as they once were particularly in the absence of a solid understanding of what dl can and cannot do in a project and in the presence of possible hype or over excitement around it further section 10 is a postscript where the author reflects on some significant review comments received on this paper before its final publication 2 back to fundamentals 2 1 why ml and dl ml as a subclass of ai is nowadays concerned with developing machines that improve their own performance in carrying out a given task over time by learning from examples with minimal human efforts to instruct the machines how to do so jordan and mitchell 2015 according to goodfellow et al 2016 however the early efforts to generate ai were based on a knowledge base paradigm to instruct machines with a formal set of step by step mathematical and if then rules those efforts focused on carrying out tasks that were intellectually difficult for humans but straightforward for computers goodfellow et al 2016 argue such efforts led to no major successes and the ai of today is about enabling machines to perform tasks that humans perform intuitively and rather easily but have difficulty formally describing how they do so examples of such tasks include recognizing faces in a photo or comprehending spoken words not only did state of the art ai divorce from the knowledge base but it also completely separated from classic data driven modelling rooted in statistics such as regression this separation was a response to the need for models that are not constrained by the many assumptions typical statistical models hold for example traditional statistical modelling requires a formalization of relationships between variables and assumptions about functional shapes distributions of variables and their inter dependencies which enables hypothesis testing and the generation of confidence bounds conversely in the ml context the underlying relationships in data may have any complex form which is typically unknown a priori and the data used may have any size and distributional properties see dangeti 2017 p 10 11 because of these characteristics ml is deemed suitable to pursue the longstanding ambition to build machines that work with minimal or no human supervision and imposed assumptions ml techniques nowadays and in particular dl provide such flexible tools that can adapt to a wide range of data and applications 2 2 evolution of dl and major milestones it was 1957 when frank rosenblatt invented the first algorithm termed perceptron rosenblatt 1957 which today forms the smallest computational unit of dl a perceptron alternatively termed a neuron because of its resemblance to the basic working unit of the brain is shown in fig 1 a and formulated as eq 1 y f i 1 d w i x i b where d is the dimension of input space x is the input vector w is a set of weights corresponding to the input vector b is bias and f is an activation function a perceptron has d 1 tunable parameters i e d weights and one bias and is basically nothing but a multiple linear regression augmented by an output function f which is non linear the form of the activation function was originally a step function but now a range of monotonic functional forms such as sigmoidal are used the invention of perceptrons created significant excitement in the ai community and beyond but it soon became clear that a perceptron would not be able to map input spaces that are not linearly separable such as the xor problem minsky and papert 1969 rendering perceptrons of limited use in real world applications the reason for this inability is that the core of the perceptron is a linear regression efforts to overcome this barrier could have followed two different avenues perhaps the most intuitive avenue was to employ non linear regression by allowing the terms inside the parentheses in eq 1 to be of other algebraic forms such as quadratic however this was not a viable option in part because the user then would need to specify the form of non linearity which is not typically known a priori requiring possibly extensive trial and error the second avenue that led to today s dl was to combine perceptrons both in parallel and in series to create so called multi layer perceptrons mlps as shown in fig 1b with the hope this more complex system could overcome the barrier an mlp would then have many more tunable parameters than the perceptron the first layer of neurons also called the first hidden layer would have d n1 weights and n1 biases where n1 is the number of neurons in this layer similarly the second hidden layer would have n1 n2 weights and n2 biases and the last layer called the output layer would have nd 1 nd weights and nd biases where nd 1 and nd are the numbers of neurons in the second to last and last layers respectively and d is the total number of layers the total number of layers in an mlp and the number of neurons in each layer are hyper parameters to be specified by users also important is the choice of activation functions in each layer note that a linear activation function is typically only suitable for the last layer and in general any stack of linear layers is effectively equivalent to a single linear layer but mlps on their own did not go far and the field stagnated for many years because of the absence of an algorithm that could automatically derive from data the network weights and biases a process referred to as training in the ai community it took until the mid 1980s when the first back propagation bp algorithm was invented to enable the training of mlps with any network structure rumelhart et al 1986 this invention marked the beginning of the second wave of popularity of anns bp is essentially an optimization algorithm based on non linear programing that minimizes a loss function f representing the goodness of fit of predictions to observations such as the sum of squared errors as follows eq 2 f k 1 m j 1 n d t j k y j k 2 where y j k is the output of neuron j in the output layer when the network is forced with input data sample k and t j k is the respective desired target also m is the size of training data and nd is the number of neurons in the output layer different variations of bp rooted in first e g gradient descent or second order e g newton s method optimization or a combination thereof now exist see e g the levenberg marquardt algorithm as implemented by hagan and menhaj 1994 these algorithms are fundamentally the same as optimization algorithms used nowadays for calibration of process based models the only difference is that in the case of anns and unlike most process based models the partial derivatives of the loss function with respect to weights and biases are analytically available through the chain rule of differentiation in parallel derivative free and metaheuristic optimization algorithms have shown promise in training e g dengiz et al 2009 rakitianskaia and engelbrecht 2009 razavi and tolson 2011 but have yet to become mainstream the training of anns is an iterative optimization process where the network parameters are updated after each iteration called an epoch in the ann context to minimize the loss function this process can be via batch training where at each epoch the entire batch of training data i e all m input output sets are used alternatively each epoch can follow mini batch training based on a subset of training or incremental online training based on a single training data sample chosen randomly or otherwise hagan et al 1996 these two approaches also commonly referred to as stochastic gradient descent are useful when the size of training data is large bottou 1998 2010 in the late 1980s after the invention of bp mlps were proven to be universal approximators hornik et al 1989 this proof indicated mlps with only one hidden layer that possesses a sigmoidal activation function and a linear output layer would be able to approximate any function with any desired level of accuracy provided the number of hidden neurons is sufficient since then the universal function approximation theorem has been the fundamental driver of interest in mlps across a variety of disciplines and applications mlps gradually formed a prominent class of artificial neural networks or simply neural networks a name reflecting their perceived resemblance to biological neural networks mlps which are also sometimes called feedforward neural networks fnns are the building blocks of a range of other anns developed later on including autoencoders bourlard and kamp 1988 recurrent neural networks rnns elman 1990 and its popular variation long short term memory lstm hochreiter and schmidhuber 1997 convolutional neural networks cnns lawrence et al 1997 and generative adversarial networks gans goodfellow et al 2014 anns started receiving much attention in earth and environmental sciences in the early 1990s the pioneering applications of anns include benediktsson et al 1990 badran et al 1991 stogryn et al 1994 bankert 1994 and cabrera mercader and staelin 1995 in the context of remote sensing of the environment mccann 1992 boznar et al 1993 and navone and ceccatto 1994 in the context of atmospheric forecasting and kang et al 1993 hsu et al 1995 and minns and hall 1996 in the context of hydrologic modelling perhaps the most prominent and widely used application of anns in these fields has been related to the development of persiann or precipitation estimation from remotely sensed information using artificial neural networks hsu et al 1997 sorooshian et al 2000 ashouri et al 2015 which has been maintained and updated for two decades accessible at https chrsdata eng uci edu despite all of these advances including the development of the now highly successful anns such as lstm hochreiter and schmidhuber 1997 and cnn lawrence et al 1997 investments in anns and therefore their popularity saw a decline in the ai community beginning in the mid 1990s this was reportedly triggered by failures to fulfill overly ambitious or unrealistic promises by prominent ai scientists goodfellow et al 2016 that brought about somewhat a negative reputation for anns duerr et al 2020 as historically observed in ai winters hendler 2008 anns in earth and environmental sciences however remained fairly popular arguably until the mid 2000s the focus of researchers in these fields was to find novel applications of anns across different earth and environmental problems it took until early 2010s before the third wave of popularity and interest in anns hit when the field was revived and renamed deep learning depth is a recently popularized term and loosely refers to the number of hidden layers in anns a related term is width which loosely refers to the number of neurons in hidden layers now a dl model typically refers to an ann with more than a few hidden layers the recent excitement around anns is despite the fact that the structure formulation and other properties of mlps and some of their offsprings such as lstm that provides mlps with a memory see section 6 have remained essentially unchanged since their inception except for some minor modifications so one might ask is dl merely a repackaging and rebranding of what existed before the next section attempts to answer this question while reviewing the recent milestones 2 3 latest developments and rebranding the field to better understand the recent developments in the field of anns one first needs to know the history around the depth concept anns since their inception have been used with various numbers of hidden layers that is with various depths most applications however remained limited to networks with only one hidden layer until very recently for example razavi et al 2012 surveyed ann applications for surrogate modelling in water resources literature and reported that more than 90 of those had only one hidden layer there was and perhaps still is no consensus about a proper network depth because identifying the optimal network configuration for a given problem and dataset is challenging historically before the rise of dl some researchers favored anns with more than one hidden layer arguing that they require fewer hidden neurons to approximate the same function see e g tamura and tateishi 1997 on the other hand others asserted that single hidden layer anns are superior to those with more than one hidden layer with the same level of complexity see e g de villiers and barnard 1993 three general reasons historically drove interests towards anns with a single hidden layer 1 the universal function approximation theorem hornik et al 1989 as it provided a compelling argument that such anns are fully capable of learning any function 2 the principle of parsimony as anns with fewer hidden layers are generally deemed less complex and more understandable and 3 difficulty of training as anns with more hidden layers are more complex to train see e g de villiers and barnard 1993 so what recently shifted the status quo towards anns with multiple typically many hidden layers goodfellow et al 2016 attribute the beginning of this shift to the work of hinton et al 2006 where unsupervised learning was used to pre train deep anns they showed unsupervised learning could effectively initialize the network s parameters such that the subsequent training efforts through bp would become more successful in ai unsupervised learning refers to a process where a model learns from unlabeled examples which are technically inputs with no associated output this is as opposed to supervised learning where examples i e data points are labeled meaning the output associated with each input is available this process is called model calibration in the context of process based modelling now one might ask how unsupervised learning can be of any help in supervised learning a common method for this purpose uses autoencoders which are a class of anns historically used for dimensionality reduction and feature learning bourlard and kamp 1988 an autoencoder is an mlp typically trained by bp with one or more hidden layers that receives input and aims to produce the same input as its output in a typical autoencoder the middle layer has fewer neurons than the dimension of input thereby acting as a bottleneck that encodes the input data in a lower dimensional space the signals in the middle layer preserve the information contained in the inputs which will be decoded back to the original space in the following layers autoencoders can pre train some layers of a deep ann such that the weights of those layers capture the main features in input data before passing them to the next layers after the pre training phase by unsupervised learning the ann needs to be further trained in the conventional supervised manner using the actual output data and algorithms such as bp while the third wave of ann popularity began by leveraging unsupervised learning to train deep anns goodfellow et al 2016 argue the interest has gradually shifted back to the classic learning algorithms such as bp even for training deep anns those classic learning algorithms are now believed to work quite well in the dl context perhaps due to the boom in computational power in this regard a game changer was the introduction of graphics processing units gpus to the ann community as a powerful tool to massively parallelize and thus expedite training algorithms raina et al 2009 such computational power has enabled the development of large anns in terms of both depth and width as such anns with hundreds of millions e g devlin et al 2018 or even a trillion parameters e g rajbhandari et al 2019 are becoming common such a tremendous revival of the field of anns might seem at first surprising to those earth and environmental scientists who have known the field for a long time this might be due in part to the fact that many ann applications to earth and environmental problems nowadays are fundamentally similar to those performed in the 1990s and 2000s more broadly the basic forms of many anns used today have not changed much other than becoming larger differences if any in an application are often in the details for example following glorot et al 2011 the tendency now is to use the rectified linear unit relu which is an unbounded function instead of the standard sigmoidal activation functions see eq 1 this revival may be explained by the huge successes of some anns in their original areas of application such as image processing krizhevsky et al 2012 and speech recognition young et al 2018 their commercial potential and the resulting major investments by mega companies such as google in this field perhaps recent rebranding of the field under the title of deep learning might have been in part a marketing strategy duerr et al 2020 while as cited in schmidhuber 2015a this term was first introduced by dechter 1986 to ml and by aizenberg et al 2000 to anns 3 geometrical interpretation of dl anns have always struggled with explainability and interpretability extensive research efforts have endeavored to peer inside the black box of anns via various heuristics rooted in sensitivity analysis of input output mappings created by anns e g bach 2015 toms 2020 or see section 3 4 of razavi et al 2021 for a review or techniques that attempt to explain the internals of ann models e g benítez et al 1997 tickle et al 1998 castro et al 2002 wilby et al 2003 xiang et al 2005 see et al 2008 samek and müller 2019 despite all these advances the issues around explainability and interpretability of anns and of many ml techniques in general are as relevant today as ever see rudin 2019 this section utilizes a geometrical interpretation to illustrate the internal functioning of anns and explain why deeper anns can be more powerful than shallower anns in learning representations in data this interpretation is adopted in part from the work of razavi and tolson 2011 in which they recast anns with respect to a new set of variables that are interpretable based on the network functional geometry 3 1 a perceptron an ann is in principle made of a number of perceptrons see section 2 2 consider a basic ann with a single hidden layer with a sigmoidal activation function as shown fig 2 a each hidden neuron e g the r th neuron is a perceptron whose output y r 1 is multiplied by the weight w 1 r 2 before entering the output neuron this hidden neuron when only having one input x 1 forms a functional relationship such as that shown in fig 2b this sigmoidal unit can be characterized by three variables slope location and height there is one to one mapping between these variables and the original network variables w r 1 1 b r 1 and w 1 r 2 as shown in the figure as such one can directly control the shape of the sigmoidal unit through slope location and height and where needed map them onto the network s original variables the benefit of doing so is that unlike the original variables the new variables are geometrically interpretable and therefore more intuitive fig 2c shows the geometry of a perceptron with two inputs x 1 and x 2 in this case the resulting sigmoidal unit forms a plane that can be characterized by slope location and height plus an additional variable called angle that specifies the direction toward which the sigmoidal unit is facing this geometry can be extended to perceptrons with three or more say d inputs where the sigmoidal unit becomes a hyperplane characterized by a slope location and height and d 1 angles full details of this geometrical interpretation and how it works in practice are available in razavi and tolson 2011 now let us see in the following how anns can approximate any function by putting together a large number of such sigmoidal units 3 2 anns with one hidden layer single hidden layer anns are capable of approximating any function by combining in parallel as many sigmoidal units as required for example suppose the underlying function to approximate is the sine function shown in fig 3 a three sigmoidal units with almost equal heights equal absolute slopes and different locations are required in parallel to represent the features of the function these three units can be produced by the hidden layer of an ann and fed into a linear output layer where they are summed superposed to approximate the sine function as shown in fig 3b for problems with two or more inputs the function approximation is not as straightforward for example suppose the objective in a two input problem is to approximate the dome like feature shown in fig 4 a a single hidden layer ann with four sigmoidal hidden neurons and one linear output neuron would be able to approximate the dome part of the space as shown in fig 4b this ann would basically superpose four sigmoidal units with equal heights equal slopes equal locations but different angles 90 apart the performance of this ann however is unacceptable as it creates erroneous features on the tails but can we rectify this issue by using more hidden neurons fig 4c shows the performance of a network with eight sigmoidal units all having the same heights slopes and locations but different angles 45 apart with more sigmoidal units at work the performance at the tails is improved producing a smoother surface almost 40 hidden neurons are required as shown in fig 4d to generate almost completely smooth tails similar to the original function shown in fig 4a this example provides a geometrical proof for the universal function approximation theorem of hornik et al 1989 because in principle any complex function could be approximated by superposing a large number of such dome like i e basis functions the challenge however is that many possibly an excessively large number of hidden neurons may be required for a given problem to attain a desired level of approximation accuracy 3 3 so why more than one hidden layer as proven by hornik et al 1989 and geometrically shown in the example above anns with a sigmoidal hidden layer and a linear output layer are capable of approximating any function with any desired level of accuracy so one may wonder about the need to have deeper anns this section attempts to answer this question via an example let us look back at the original function we aimed to approximate in fig 4a only four sigmoidal units were required as seen in fig 4b to reproduce the dome like feature at the center one might ask can we stick to these four sigmoidal units and somehow smooth the tails yes all that is needed is a second layer with a nonlinear activation function e g sigmoidal to deactivate any feature that is under a threshold in other words in this process the geometry formed by the sigmoidal units in the first layer filters through another sigmoidal unit that bounds that geometry fig 4e shows how adding the second nonlinear layer enables the network to reproduce the original function with only four neurons in the first hidden layer similar to single hidden layer anns those with two hidden layers can approximate any function by putting a number of the dome like functions side by side in general and as shown in the example deeper anns can provide more flexibility while they may require fewer hidden neurons across the network for representation learning however the training of deeper anns has been historically much more difficult because of the now well known problem of vanishing and exploding gradients this problem relates to the fact that the partial derivatives of a loss function eq 2 with respect to weights and biases in first layers obtained via the chain rule of differentiation tend to become very small i e close to zero or very large i e exponentially growing or fluctuating improved algorithms along with higher computational power have now eased that difficulty and made possible the training of very deep anns schmidhuber 2015b lastly a related consideration about the proper number of hidden layers is about the fact that in many problems only a small part of the input space is active in other words some combinations of input variables might not occur in reality and therefore the accuracy of the ann might not matter much in the regions of input space containing those combinations for example consider a case similar to one shown in fig 4b where the corners on the input space do not show up in the data available a hydrological example is where snowfall and temperature are two inputs to anns because snowfall would never occur along with a high temperature the respective part of the input space always remains inactive 4 relevance of occam s razor and equifinality 4 1 issues with the complexity of anns anns are known for their hyper flexibility in fitting data owing to their enormous degrees of freedom for example consider a problem with five inputs and one output a single hidden layer ann with 10 hidden neurons would have 71 tunable parameters 60 weights and 11 biases and adding a second 10 neuron hidden layer would result in a network with 181 parameters 160 weights and 21 biases compare that with linear or quadratic regression models for the same problem which would have six or 21 tunable parameters respectively such large degrees of freedom manifest in large numbers of parameters encountered in the field of anns do not seem consistent with a basic principle in statistical modelling occam s razor occam s razor or principle of parsimony indicates that simpler hypotheses or models should be preferred over more complex ones in other words those models that serve the purpose with as few parameters as possible should be chosen however many data driven modellers in particular in the field of ml have arguably abandoned occam s razor for example ann users typically do not try simpler model types such as regression for the problem at hand and when using anns they do not necessarily look for the most parsimonious network note that some literature proposes systematic approaches to choose a network structure based on growing pruning or other strategies e g reed 1993 teoh et al 2006 xu et al 2006 in practice however such approaches have been of limited use and most ann users choose the network structure on an ad hoc basis or by trial and error see a survey by wu et al 2014 recently giant anns with hundreds of millions of parameters or more have become widespread devlin et al 2018 rajbhandari et al 2019 in addition equifinality a common and widely discussed issue in process based modelling beven and freer 2001 khatami et al 2019 is not generally discussed or considered an issue in the context of anns equifinality concerns the fact that in most cases different model structures and parameter values can lead to similar modelling results in other words model structure and parameters are not uniquely identifiable from data guillaume et al 2019 this is despite the fact that loosely speaking the level of equifinality of anns is much larger than other types of models because of their massively parallel nature in producing model outputs so how does dl handle the above issues the answer is indirectly by trying to avoid their undesired implications which are overfitting and lack of generalizability the former refers to a situation where a model fits the noise in the data rather than the underlying function the latter refers to a case where the model does poorly in out of sample prediction that is predicting situations unseen in the data used for model training various techniques are available in the ann literature to address these issues as outlined in the following 4 2 leashing the hyper flexibility of anns most techniques to control the hyper flexibility of anns and to avoid overfitting fall under two general strategies namely early stopping and regularization before reviewing these strategies in this section let us revisit the common data splitting approach for calibration and validation of models anns and traditional mechanistic models have major differences in terms of calibration and validation in traditional modelling practices the available data are commonly divided into calibration and validation datasets the former is used to identify the model structure and parameters while the latter is used to test the model performance in out of sample prediction in ann practices however the available data are typically divided into three sets commonly referred to as training validation and testing datasets any data chosen for training and testing in the ann context are respectively treated like calibration and validation datasets in the traditional modelling context the third validation dataset in the ann context is needed to leash the hyper flexibility of the network while training the simultaneous use of training and validation datasets during ann training may be best described with the early stopping strategy as follows in the early stopping strategy the quality of fit to the validation dataset is evaluated after each epoch that is an optimization iteration trying to minimize the loss function on the training data see section 2 2 empirically speaking as the training error decreases over time the validation error decreases as well for a while however at some particular epoch the validation error may begin to increase while the training error may keep decreasing see fig 5 this epoch is deemed to mark the beginning of overfitting thus the user stops the training process this strategy is therefore called early stopping in the sense that the training stops early before it can further improve the fit to the training dataset for a review see prechelt 1998 when the training process stops the generalizability of the trained network is assessed via out of sample prediction on the testing dataset regularization is another commonly used strategy to put a leash on the hyper flexibility of anns unlike early stopping this strategy tries to minimize a regularization function during training to control the ann flexibility and tailor it to the problem at hand this strategy has roots in the theory of tikhonov regularization and typically views a more regularized model as one with a smoother response surface tikhonov and arsenin 1977 johansen 1997 a traditional regularization function in the ann context is the sum of the square of all network parameters krogh and hertz 1991 based on the notion that in general the smaller the parameters of a neuron the less activated it is for example in an extreme case where all parameters of a neuron are zero that neuron becomes fully inactive and does not contribute a feature to the overall network response razavi and tolson 2011 provide a more efficient regularization function based on the geometry presented in section 3 where the regularization function is the sum of squares of all of the slopes this regularization function only targets and removes the unnecessary features which are unsupported by data from the overall network response but how can one balance the goodness of fit and smoothness of the network response in practice this is a bi objective optimization problem where one objective is to minimize the error function and the other is to minimize the regularization function these two objective functions are commonly integrated into one loss function via weighting schemes fig 6 shows how the two objectives compete in a real example ideally one may wish to achieve a performance such as that shown in fig 6e doing so is not trivial however because in practice the underlying function is unknown available data are limited and response surfaces are multi dimensional and cannot be easily visualized the bayesian regulation method developed by mackay 1992 and extended by foresee and hagan 1997 has proven useful to adaptively assign the weights associated with each function during training a more advanced and recently developed regularization strategy is called dropout hinton et al 2012 srivastava et al 2014 dropout is a heuristic particularly designed for deep anns that randomly deactivates and then activates different neurons or groups of neurons at each epoch in the course of training when a part of an ann is inactivated in this process the resulting network is called a thinned network the ultimate prediction after training with dropout is viewed as an approximation of the ensemble average of predictions by many independent anns basically the many different thinned networks created throughout the process are assumed to represent anns with different configurations and parameters this heuristic discourages neurons to co adapt too much and as such is believed to avoid overfitting one may find parallels between the ensemble average philosophy of dropout and that of the more traditional bagging strategy in ml originally developed by breiman 1996 that bootstraps available data to develop an ensemble of models and average their outputs 5 fundamental differences from other ml methods 5 1 local versus distributed representations most ml methods such as those based on kernel functions are based on local representations these methods while forming connectionist networks like anns represent each entity e g a training sample point in the input space via an independent processing unit for example radial basis functions broomhead and lowe 1988 gaussian emulator machines kennedy and o hagan 2000 and support vector machines vapnik 1998 cherkassky and ma 2004 may use as many kernels as the number of training samples each kernel typically has a limited radius of influence in the input space and therefore only responds to inputs located in their local neighborhood kernel based methods may face major difficulties when training data include identical or similar samples conversely a unique feature of anns is their ability to learn through distributed representations hinton et al 1986 they typically represent an entity via collective efforts distributed among multiple processing units e g sigmoidal units unlike kernel functions the sigmoidal units typically have large regions of influence see e g fig 2c that overlap each other in the input space see e g fig 4b the former figure shows that a sigmoidal unit influences the entire input space by dividing it into three zones lower tail upper tail and slope the latter figure shows how the influences of four such sigmoidal units are superposed to generate the network response 5 2 implications for users the use of distributed representations has several practical implications to the author s knowledge these include transparency the internal functioning of methods based on local representations is more transparent local representations are the most straightforward and easy to interpret way of learning whereas distributed representations can be complex often leading to emergent properties that cannot be easily explained by local representations hinton et al 1986 learning difficulty distributed representations are more difficult and time consuming to learn in local representations the role of each processing unit may be assigned independently of the other units but in distributed representations many processing units may be configured together in complex ways to represent a feature in the data network size distributed representations often need much smaller network sizes in general the size of the networks based on local representations is directly proportional to the size of the dataset in most cases with a proportionality constant of one that is the number of processing units mirrors the number of training data samples the size of networks based on distributed representations however depends on the complexity of features in the dataset not its size this ability enables anns to scale relatively easily to big data sizes whereas many kernel based methods can become computationally intractable in the presence of large data sets for example ratto et al 2007 report a limit of only 400 sample points for the use with gaussian emulator machines inexact interpolation or emulation networks based on distributed representations are generally inexact emulators this means they do not exactly fit the training samples to represent the features and patterns in the data this is unlike some other ml methods such as radial basis functions broomhead and lowe 1988 and gaussian emulator machines kennedy and o hagan 2000 that are exact emulators perfectly interpolating the training samples other inexact emulators include support vector machines vapnik 1998 cherkassky and ma 2004 and multivariate adaptive regression splines mars friedman 1991 in addition anns are essentially multi output models because they can have as many output neurons as required for a given problem this means a single ann can simultaneously predict different variables while accounting for their possible cross correlations many other ml methods are single output models for example in the case of support vector machines one needs to develop two independent models to be able to predict two variables in the same system 6 how to introduce order time dependency and memory basic anns provide static mapping from inputs to outputs however many applications require mappings with a formal representation of time evolution and memory to enable anns to do so different approaches have been developed in the literature using operators such as delay boxes recurrent connections and information gates as explained in the following 6 1 tapped delay lines a tapped delay line tdl consists of a certain number of time delay operators arranged in an incremental order fig 7 a tdls can be installed on any parts of anns to represent time explicitly the resulting ann shown in fig 7b commonly referred to as a time delay neural network tdnn waibel et al 1989 has been widely used in a range of time series processing applications as such tdnns possess a static memory with an adjustable length the length of a tdl can be viewed as a hyper parameter to be tuned during training along with network structural properties such as the numbers of layers and neurons in each layer adding tdls to an ann significantly increases the number of tunable parameters for example a standard ann with three inputs and 10 neurons in the first hidden layer would have 30 weights in that layer while adding tdls with a length of five to the inputs would result in an additional 150 weights 180 in total to be trained 6 2 recurrent connections tdls as described in section 6 1 explicitly represent time with memory units of limited length unlike tdls recurrent connections first introduced by jordan 1986 enable anns to account for time evolution based on an implicit memory concept which is theoretically of unlimited length and is highly context dependent elman 1990 recurrent connections receive the outputs of a layer at every time step and feed them back to the same or some other layer in the next time step technically they do so via a context unit that stores those outputs in a set of delay boxes fig 7c recurrent connections can be installed on one or more layers e g jordan 1986 elman 1990 or locally on some select neurons e g frasconi et al 1992 an ann enabled with recurrent connections is commonly called a recurrent neural network rnn an rnn can possess many more tunable parameters compared to a standard ann with the same number of layers and neurons using the example given in section 6 1 an ann with three inputs and 10 neurons in the first hidden layer would have 30 weights in that layer whereas adding recurrent connections to that layer similar to fig 7c would add 100 more weights 130 in total to that layer unlike tdnns that possess only a short term memory rnns in theory can represent long term dependencies as well in practice however the implicit memory created by recurrent connections can easily be dominated by short term dependencies in other words even very small features arising from short term dependencies tend to mask features arising from long term dependencies in addition rnns are prone to the problem of exploding and vanishing gradients in their training explained in section 3 3 bengio et al 1994 this is because rnns even with a single hidden layer are in principle deep networks implicitly possessing an infinite number of recursive layers 6 3 gate layers to preserve or forget information over time to balance and explicitly account for both short and long term dependencies hochreiter and schmidhuber 1997 introduced a new type of rnns called long short term memory lstm they extended and further parametrized the context unit also called cell such that the network can more explicitly control what information to hold over time and what to forget the lstm s context unit modulates not only the outputs in the previous time step but also the inputs to the network in the current time step it typically does so via three new independent layers of neurons arranged in the so called forget gate input gate and output gate layers the neurons of each gate layer receive recurrent connections as well as the new input to the network and generate their response between zero and one via using a logistic function see e g fig 7d these responses are then multiplied by their respective signals flowing through the context which means a value of zero would kill a signal whereas a value of one would fully preserve it due to the additional weights and biases in the gate layers an lstm typically has many more tunable parameters than a conventional rnn lstms are now perhaps the most popular and widely used type of anns with memory however lstms took a long time more than a decade to become known and mainstream particularly beyond their core computer science community their widespread application nowadays owes in part to recently developed software tools such as those in python s tensorflow that efficiently implement variations of lstms for a range of problems 6 4 training considerations when the order of data matters the training of memory enabled anns such as tdnns rnns and lstms is different from that of standard anns in terms of the way time ordered data are presented to the network to train standard anns the data entries can be presented in any order even randomly for example through stochastic gradient descent bottou 2010 in memory enabled anns however the data entries should be presented in order of occurrence so that the structure of the time dependency is preserved while this point might seem trivial it requires careful attention in practical applications another point to consider in the training of memory enabled anns is that all data entries are typically viewed to have equal importance regardless of their location in the sequence when used in an online operational forecast however the forgetting factor approach can be used to discount older samples this approach allows the network to adapt to non stationary environments where more recent data are more representative of the underlying processes than older data razavi and araghinejad 2009 lastly the above operators can be combined in a variety of ways a well known combination is time delay recurrent neural networks developed by kim 1998 and used in various applications such as long term precipitation forecasting in karamouz et al 2008 while such combinations may show improved modelling power compared to other ml or statistical methods the attribution of memory gains to the different elements can arguably be challenging if possible at all 7 ml versus process based modelling an experiment ml has been extensively used to model systems for which process based models are also available process based models are based on the physics governing the underlying processes and are therefore typically evaluated based on both their physical realism and goodness of fit to data ml however does not do much if anything with the underlying physics while reportedly doing a superior job in fitting data even in out of sample prediction a fairly large body of literature benchmarks ml techniques particularly anns against process based models examples of such comparisons directly or indirectly in the context of hydrologic modelling include hsu et al 1995 tokar and markus 2000 wilby et al 2003 kratzert et al 2018 kratzert et al 2019 feng et al 2020 and ma et al 2021 some studies such as wilby et al 2003 also detected correlations between the weights of an ann and state variables of a process based hydrologic model as a way to verify that their ann can capture the underlying processes in a hydrologic system this section provides an experiment that runs and compares an ann and a process based model for the same problem and walks the reader through all of the steps involved in particular the processes around calibration and validation role of physics and interpretations of out of sample prediction are discussed this experiment is performed in the context of hydrologic modelling which has seen tremendous progress over the years with respect to both ml and process based modelling 7 1 data and models the case study used aims to model the hydrologic system of the oldman river watershed in alberta canada this watershed has an area of 1434 73 km2 at waldron s corner with a long term average temperature of 2 2 c on average this watershed receives 611 mm of precipitation rainfall snowfall annually and generates 11 7 m3 s of river flow fig 8 shows the 30 year long daily time series data used the first 22 years were used for model calibration i e the seen data in model development and the last eight years for model validation i e the unseen data in model development the first three months of the calibration period were used for model spin up for the ann training the calibration period was further broken into training 17 years and testing 5 years periods the latter for early stopping of the training process to avoid overfitting note that as explained in section 4 2 the naming convention in the ann context for the validation and testing periods is often the other way around to model this system an lstm configuration was chosen here as a state of the art model that accounts for time dependency and memory the inputs to the lstm model are daily precipitation and temperature fig 8a and d and the output is the concurrent daily flow fig 8e the lstm structure was rather arbitrarily chosen to have one hidden layer with five neurons resulting in 166 calibration parameters for benchmarking purposes a classic hydrologic model called hbv lindström et al 1997 as implemented in hbv sask razavi et al 2019 was used with the same data hbv sask is based on a conceptualization of physical principles governing the water movement in a watershed using 12 calibration parameters each of these parameters has a physical interpretation and a physically justified feasible range see fig 9 and table 2 of razavi et al 2019 full detail including data of this oldman river watershed case study which has been developed for educational purposes is available with the source code 7 2 model performance in calibration the model calibration problem was cast as an optimization problem that tries to maximize the goodness of fit to data by tuning the model parameters with the nash sutcliffe efficiency nse nash and sutcliffe 1970 as the objective function nse is essentially a normalized version of mean squared errors computed as 1 var errors var observations as such an nse of one indicates a perfect fit and an nse of zero indicates the model prediction is not any better than the average of observations as a rule of thumb hydrologists often call an nse of 0 7 and higher an acceptable fit the lstm model was calibrated using bp with the early stopping strategy in each epoch the training period data were used to update the network parameters while the testing period data were used to detect possible overfitting five independent replicates of lstm calibration with different initial random seeds were conducted to account for possible variability of model performance fig 9a shows the training results of the five replicates compared to a case where the training would not have stopped as expected the lstm performance keeps improving in training whereas in testing it begins to degrade at some point the objective function in training came very close to one after many more epochs but with very poor performance in testing not shown the hbv model was calibrated by a multi start newton type optimization algorithm similar to lstm five independent replicates of hbv calibration were run fig 9b compares the performance of hbv with that of lstm in calibration at this point only check the performance of the standard lstm model in calibration the figure shows all five replicates of lstm outperform those of hbv note that the calibration performance of hbv shown herein is almost the best the author has achieved so far for this watershed based on these results the superiority of lstm over hbv in calibration is quite significant in regards to the goodness of fit to streamflow the performance of the two models in validation is discussed in section 7 4 but before that the next section discusses what information the two contained prior to calibration 7 3 what about a priori information encoded in the models at this point let us step back and investigate what we have achieved in terms of learning from data for both the lstm and hbv models the development of the lstm model was not based on any a priori knowledge of how a watershed system works and the governing physical principles as such the model learned everything from scratch merely using examples from data basically the model started with a fully randomized internal configuration controlled by a large number i e 166 of parameters and then tuned those parameters to adapt the internal functioning of lstm to the underlying real world system represented in the data fig 10 a shows the lstm performance of arbitrarily chosen replicates before and after calibration the model response to inputs before calibration seems to be completely random but after calibration the model response has learned to closely follow the underlying system response unlike lstm hbv encodes the expert knowledge available in the field of hydrology this model is a collection of conservation of mass equations and process parametrizations that represent how hydrologists conceptualize the way a watershed works this physically based modelling structure is presumably able to emulate the behavior of any watershed by tuning only 12 parameters fig 10b shows how the hbv model performs before calibration with parameter values chosen to be at the midpoint of their ranges and after calibration the figure shows the uncalibrated model responds reasonably to the inputs it generally captures the timing of flows and emulates the low flow segments well but is overly responsive to large precipitation events generating spurious spikes in flows calibration either manual by expert knowledge or automatic as done here via optimization can fix the discrepancies and fit the model output to observations so a fundamental difference between the two approaches is now clearer using a process based model is about directly using the expert knowledge available in a scientific field and tuning it to the case study of interest while using anns is about learning everything from scratch directly from data this difference is manifest in the number of parameters that need to be tuned to achieve a reasonable performance notably the lstm model achieved a better performance in emulating observations after calibration as evident in a comparison of fig 10a and b however in any modelling exercise one needs to ensure the model gives the right answer for the right reasons kirchner 2006 that is why proper model evaluation in out of sample prediction is critically important as discussed in the next section 7 4 model validation standard versus true out of sample prediction in general validation and verification of mathematical models are very challenging in some scientific disciplines if possible at all oreskes et al 1994 the standard practice however is to test the performance of the model under investigation in terms of reproducing some historical record not seen during model calibration klemeš 1986a a process called out of sample prediction in this paper fig 9b shows the results of such practice in the validation period set in fig 8 in this case both lstm standard and hbv models do reasonably well in regards to goodness of fit to streamflow with lstm outperforming hbv across all replicates in addition and as expected both models produced slightly lower nse values in validation compared to those in calibration a point is as oreskes et al 1994 articulated the above so called model validation is inherently partial while the performance of lstm appears to be better than that of hbv in a relative sense one needs to take extra care before making such a conclusion as argued by klemeš 1986a more than three decades ago a strong assumption in this type of validation is that the conditions under which the model will be used will be similar to the conditions under which the model has been developed and calibrated it is now well recognized that such an assumption may not hold as many natural systems are essentially non stationary milly et al 2008 despite such recognition this standard model validation practice has arguably remained unchanged humphrey et al 2017 beven 2018 here let us take a stress test approach via a what if scenario question to test and compare the performance of both models in a true out of sample prediction basically under conditions that have not truly been seen in the process of model development and calibration the question is how the system would behave if the average temperature warmed by 2 c while everything else remained the same to assess this hypothetical scenario both calibrated models were fed a new temperature time series obtained by adding 2 c to all daily temperature values of fig 8d these new synthetic inputs roughly provide a picture of what might happen in this watershed under global warming the modelling results under such scenarios are typically used to inform policy making for climate change adaptation now let us use the two different models to evaluate the possible changes in the watershed behavior in response to a 2 c warming here instead of looking at individual simulated time series the possible change in the average seasonality of flows is of interest first look at fig 11 a to check the consistency of simulated flows for the historical period both models generally follow the observed seasonality but the range provided by the lstm model is generally narrower and better encapsulates observations in both low and high flows under the new conditions however the two models show the two distinct behaviors shown in fig 11b according to lstm peak summer flows would decline by about 25 on average and the time of the peak would shift backward by about a week from the beginning of june to a time in the fourth week of may according to hbv however the changes would be more pronounced the peak flows would decline by about 35 on average and the flows might show two modes the higher one at the beginning of may and the other at the beginning of june at about the same time as the peak in the historical observations are such differences not sufficiently large so as to make the user skeptical about the modelling process 7 5 injecting some physics into ml at this point one may wonder about the possibility of ensuring that lstm results be physically consistent particularly under new conditions let us give it a try by recasting the modelling problem based on some understanding of the governing physics in hydrology for example physics tells us that the freezing point of water is around 0 c and therefore this threshold could be used as an approximation to differentiate rainfall from snowfall on a daily basis i e if the temperature on a day is above below 0 c the precipitation on that day if any is considered to be rainfall snowfall see fig 8b and c this differentiation is actually a part of process parameterization in hbv similar to many other hydrologic models via a parameter called temperature threshold tt for freezing thawing and separating rain and snow with a feasible range from 4 to 4 c the warming of a watershed would naturally change the rainfall to snowfall ratio and so integrating this domain knowledge with the lstm model makes sense perhaps the most straightforward way of introducing the tt concept to lstm is via pre processing of the inputs therefore a new lstm model was developed and calibrated called process informed lstm in this paper with three inputs rainfall snowfall and temperature as shown in fig 8b c and d similar to the original the new lstm model has one hidden layer with five neurons resulting in 186 calibration parameters the procedure for the calibration and validation of the process informed lstm was the same as for the standard lstm already explained in sections 7 2 and 7 4 fig 9b compares the performance of the process informed lstm with hbv and the standard lstm the figure shows the two lstm models perform comparably well process informed lstm results in a slightly lower average nse in validation but with only five replicates this small difference should be interpreted with caution fig 11b demonstrates the performance of the process informed lstm model in the true out of sample prediction according to this model the summer peak flows would decline by 20 on average and the time of peak would appear about two weeks earlier than in the historical record in the third week of may in general accounting for some known physics shifted the timing of the lstm s rising limb to the left to become more consistent with that of hbv such a shift is expected because now the model accounts for the new conditions under which there would be more rain and less snow and rain tends to run off more quickly whereas snow is stored in the mountain for longer times before it melts and appears in streamflows downstream incorporating any further physical knowledge to the lstm will likely further shift the rising limb to the left for example one may attempt to inform the model about the known direct relationship between temperature and snowmelt rate and the thawing freezing threshold doing so however is less straightforward than the basic approach implemented above and requires modifying the internals of the model a review and discussion on more advanced strategies to incorporate the knowledge available into anns come later in the paper in section 7 3 7 6 so what model should we trust the ml or process based model now the question is which one of the three models produced the most credible picture of possible watershed behavior under the new conditions in practice this question is very difficult to answer if possible at all in general the prediction of such changes can be debated and might vary from one study to another depending on the models and data used and disciplinary views perhaps a definite answer would need to wait until the future has come and shown such possible changes and from a bigger picture point of view models of natural systems cannot be verified or validated in true out of sample prediction because those systems are never closed and not everything can be represented in a model as argued by oreskes et al 1994 nearly three decades ago but as scientists we have our own perceptions and intuitions these might be biased but still useful to provide a ground for building confidence in the credibility of a model in the context of the case study given previous research on the canadian rocky mountains has indicated that warming alone will result in a considerable reduction in flows and earlier peaks in watersheds similar to the oldman river watershed a synthesis of research efforts under the changing cold regions network ccrn debeer et al 2021 on the cold interior of western canada indicates a shift in timing of the spring hydrograph rise and peak flows of nearly two weeks earlier by mid 21st century and as much as one month by the late 21st century these projections which themselves are based on rigorous atmosphere landsurface modelling are consistent with the modelling results presented in the previous sections but cannot pinpoint the most accurate model what is worrisome is the large divergence in behavior of models in response to expected but yet to be seen perturbations whereas those models produce comparable results in standard out of sample prediction broadly speaking one might say any known consistency of a model with the known underlying physics can improve model s explainability and interpretability thereby helping us better explain the model behavior in response to such perturbations explainability and interpretability are fundamental assets in building trust in a model and of course physically based models are advantaged in that respect related to the notion of explainability and interpretability one way to boost confidence in a model is to go beyond checking only for the intended predictand streamflow in this example and to analyze or even constrain the model performance in terms of other fluxes such as evapotranspiration e g vervoort et al 2014 or state variables such as soil moisture e g yassin et al 2017 ml models however are typicality not developed to allow such analyses while doing so is relatively straightforward in the case of process based models enabling ml to do so requires an in depth understanding and appreciation of the value of domain knowledge as discussed in the next section 8 discussion 8 1 what is the typically ignored value of domain knowledge in dl true out of sample prediction is nothing but extrapolation beyond the observed data and behaviors used in model development and calibration extrapolation is a reality that many predictive models nowadays must face because of non stationarity in climate and the environment milly et al 2008 any purely regression type model including those arising from dl would be disadvantaged in extrapolation as by definition extrapolating would require working in parts of the problem space for which they have not received any information conversely mechanistic models may be salvaged in extrapolation by the domain knowledge encoded within them but what does domain knowledge offer when it comes to extrapolation the answer is a set of principles modulated via conservation laws e g mass energy and momentum and process parametrizations which represent our perceptions of how two or more variables might be related gupta et al 2012 such principles have been developed and evolved over time based on extensive observation and research by scientists and practitioners the limits of validity of such principles are typically known in the following the importance of taking advantage of those principles in modelling and prediction is discussed with respect to three aspects conservation laws monotonicity and rates and feedback mechanisms conservation laws in physics a conservation law states that a specific measurable property does not change within an isolated system with time such a law is usually expressed as a continuity equation that is a differential equation equates the rate of change in storage within a control volume with the difference between what comes in and what goes out of the control volume in land surface modelling for example conservation laws are built into mechanistic models to ensure water and energy balance is preserved in simulations over time ml models however do not automatically account for such laws and as a result water or energy can be falsely introduced or lost in the course of simulation monotonicity and rates the knowledge base includes the general characteristics of some causal relationships between various physical variables for example we know from basic thermodynamics that the relationship between melt rate and available heat is monotonic that is more heat causes a higher melt rate furthermore we have some rough estimate of the feasible range of the rate of change in one with respect to the other similarly from basic hydrology we know the causal relationships governing the way a hillslope stores and releases water are generally such that a positive correlation exists between water available in the soil and its contribution to flows more water means more flows due to gravitational forces mechanistic models directly account for such knowledge on causal relationships this knowledge is encoded in process parametrizations typically in the form of deterministic monotonic functions or rarely in hysteretic forms with a limited number of parameters to be calibrated to the specific case study in hand gupta et al 2012 however in the case of hyper flexible models such as anns such functions need to be entirely derived from data all from scratch and ignoring the knowledge base related to those monotonic relationships therefore extrapolation runs the risk that such relationships become non monotonic and or have unrealistic rates producing erroneous behaviors this risk is exacerbated by the fact that identifying and diagnosing such errors are very difficult if possible at all feedback mechanisms a real world physical system is a combination of variables that interact over time typically via a range of feedback mechanisms such feedback mechanisms control the internal dynamics of the system and are key to its evolution over time for example consider a coupled water vegetation system in which precipitation available soil moisture and plant biomass interact in complex time dependent ways even at times creating positive feedbacks that destabilize the system s behavior rodriguez iturbe et al 1991 scheffer et al 2001 the knowledge base available about these feedback mechanisms is often built into mechanistic models using differential equations ordinary or partial to describe the system dynamics the representation of such dynamics in the making of models is important particularly for long term predictions and over long time scales dl models are often unable to account explicitly for such long term dynamics if a particular dynamical behavior is present in training data then dl can capture that behavior in its mapping from input onto output dl however has no explicit mechanism to represent that dynamic under perturbed conditions beyond what has been recorded in the training data is mechanistic modelling immune to issues with extrapolation certainly not while a discussion on the limitations and prospects of mechanistic modelling is beyond this paper one solution to improve extrapolability of mechanistic modelling over time which is also relevant to ml is space for time substitution assuming spatial and temporal variations are equivalent this strategy is to investigate multiple or many sites simultaneously instead of one to infer a temporal trend for a site based on information from other sites that have different properties and or experienced different conditions for example refer to pickett 1989 and blois et al 2013 in the context of ecology and singh et al 2011 in the context of hydrology in the era of big data ml can benefit explicitly or implicitly from such strategies when spatio temporal data across large domains are available for example kratzert et al 2019 feng et al 2020 and ma et al 2021 utilize the camels dataset which includes catchment attributes and hydrometeorological data across many different sites newman et al 2014 addor et al 2017 to improve the performance of dl in hydrological modelling applications in addition both modelling paradigms ml and mechanistic modelling can benefit from knowledge based patterns and behaviors derived from data around a real world system for example the notions of limits of acceptability beven 2006 hydrological signatures gupta et al 2008 and pattern oriented modeling grimm et al 2005 grimm and railsback 2012 can potentially inform the process of developing and validating models of any type such knowledge may be directly incorporated into the loss functions used to train dl models as described in section 8 3 the bottom line is that mechanistic models are generally expected to be less prone to generating spurious behaviors in true out of sample prediction therefore many domain experts may be inclined to trust physically based models as their behavior is constrained by physical laws that are perceived as unchanging with time the points made in this section will become clearer in the next section where the essential differences between dl and mechanistic modelling are discussed 8 2 why is dl essentially different from process based modelling in the author s view the first principles of anns are rooted in connectionism hyper flexibility and vigorous optimization these characteristics are fundamentally different from the guiding principles of developing and calibrating mechanistic models as described in the following connectionism is an approach that orchestrates a set of simple algebraic operations in a massively parallel manner to create a model that is able to carry out complicated tasks following this approach anns represent the response of a system under consideration to an input by summing the collective efforts of many neurons whose roles cannot be easily attributed to individual processes involved in that system this is unlike mechanistic modelling where each part of a model is designed to be responsible for a specific process hyper flexibility is a characteristic of a model with excessive degrees of freedom which can literally fit any dataset and is not constrained by the many assumptions held by typical statistical models anns are known to be hyper flexible mechanistic models however have limited degrees of freedom depending on the knowledge base available about the processes being modelled ideally mechanistic models tend to have just as many degrees of freedom as can be supported and constrained by available knowledge and data vigorous optimization here refers to the practice of manipulating model parameters at any cost to maximize the goodness of fit to calibration data the training of anns is all about minimizing an error function that is among two competing anns the one producing smaller errors in calibration and validation is the winner optimization is also often an essential part of mechanistic modelling to calibrate model parameters however in mechanistic modelling minimizing the errors is not the goal but a means to improve the realism of the model in other words unlike the case of anns physical feasibility of a parameter its identifiability and equifinality are typical considerations in mechanistic modelling the recognition of these fundamental differences is critically important when one aims to choose the right modelling paradigm for a purpose compare the two paradigms in a case study or attempt to bridge the two paradigms possibly for improved modelling performance the following section outlines the status quo for bridging the two paradigms and some emerging trends 8 3 how can we bridge dl and process based modelling the history of research on reconciling and bridging anns with mechanistic modelling in earth and environmental sciences dates back to the early 2000s or perhaps earlier these efforts have generally had the objective of simultaneously leveraging the strengths of the two modelling paradigms to further our knowledge and predictive ability abrahart et al 2012 reviewed such research in the context of hydrology and refer to it as hybridization they introduced three possible approaches for this purpose which herein are referred to as surrogate modelling one way coupling and modular coupling seven years later reichstein et al 2019 in an influential article in nature re introduced the notion of hybrid modelling and the above three approaches as the next steps in earth science in the following these three approaches are explained and then more modern existing approaches arising from research fields beyond earth and environmental sciences are discussed surrogate modelling alternatively called metamodelling or model emulation refers to the process of developing and applying a simpler cheap to run model in lieu of a more complex computationally intensive model in this process a data driven surrogate such as an ann is trained on samples of a limited number of original model runs to approximate the model response surface the developed surrogate model can then be used in different frameworks in conjunction with the original model in multi query applications such as optimization and uncertainty quantification example applications of anns as surrogates of mechanistic models include johnson and rogers 2000 broad et al 2005 behzadian et al 2009 and vali et al 2021 the reader may refer to a review by asher et al 2015 for a range of approaches used for surrogate modelling one way coupling refers to the process combining a mechanistic model with an ml model such that the output of the former feeds into the latter as input a general rationale for such a combination is that a mechanistic model may not be able to fully explain the observed data and therefore an ml model could be of help in extracting any information left in the residuals of the mechanistic model for example consider a case where a mechanistic hydrologic model is used for streamflow forecasting and as expected some errors in model outputs are present an ann can be used to model such errors over a historical period to provide some predictive ability on the distribution of errors for a time step into the future then running these two models in sequence may provide higher forecasting skills example applications of such one way coupling include shamseldin and o connor 2001 anctil et al 2003 solomatine and shrestha 2009 wani et al 2017 and li et al in review modular coupling refers to cases where an ml model is used as a module sub model of a larger mechanistic model or vice versa the rationale for this type of coupling may be that a particular model might have proven skills in representing a particular process and is therefore preferred while other processes are better represented by another model modular coupling is intrinsically an ad hoc process which can be done in a variety of ways depending on the problem at hand and models and data available for example chen and adams 2006 and corzo et al 2009 used anns as the routing module within a distributed hydrological model chua and wong 2010 developed an ann based hydrologic model using the output of a kinematic wave model as one of its inputs mekonnen et al 2015 developed and coupled a process based model and an ann model for simulating hydrology in contributing and non contributing areas of prairie regions respectively humphrey et al 2016 developed an ann model of monthly streamflow prediction that receives simulated soil moisture as an input from a process based hydrologic model hunter et al 2018 coupled a process based in stream salt transport model an ann based saline groundwater accession model and linear regression models of floodplain storage bennett and nijssen 2021 developed an ann model for the simulation of turbulent heat fluxes and built it into a process based hydrologic model beyond the earth and environmental sciences community the notion of bridging the knowledge base and ml has a long history e g see the knowledge based artificial neural networks by towell and shavlik 1994 but it has received significantly more attention recently different approaches mostly arising from mathematics and computer science have been proposed under titles such as theory guided data science karpatne et al 2017a informed machine learning von rueden et al 2019 and physics informed neural networks raissi et al 2019 to name a few providing a full coverage of such approaches is beyond the scope of this paper and many of them have been developed for specific application areas with limited relevance to earth and environmental problems in the following three relevant approaches are explained regularizing anns via knowledge based loss terms a new regularization function can be developed based on the available knowledge surrounding a given problem and be added to the loss function used in training for example any violation of the conservation laws or monotonicity of relationships as described in section 8 1 can be quantified and penalized during training refer to stewart and ermon 2017 and karpatne et al 2017b to see how this approach can work in two different application areas the former in image processing and the latter in lake temperature modeling using mechanistic model runs to augment ann training data a mechanistic model can be used to simulate the system under investigation under a range of conditions to generate synthetic data to augment the available training data this approach may be particularly useful in guiding anns in extrapolation beyond conditions seen in the original training data see the discussion in section 8 1 this approach is based on the assumption that the mechanistic model used is sufficiently accurate an assumption that needs to be treated with caution for an example of this approach in the field of systems biology see deist et al 2019 integrating differential equations into anns this approach is a very recent and perhaps the most mathematically elaborate in terms of integrating the knowledge base into anns primarily developed by raissi et al 2019 it parametrizes the known differential equations describing a system and integrates them into the body of anns the integrated model is then trained to the available data simultaneously inferring the parameters of the differential equations and network weights one could view this approach as an extension to the knowledge based loss terms described above where the new loss term penalizes the network for deviations from those known differential equations this approach still seems embryonic but perhaps with great potential for scientific breakthroughs 8 4 what can we learn from prominent ml applications ml has already been used across a wide range of disciplines and applications but with varying degrees of success here and for context consider two special and well known applications playing chess and predicting the stock market ml has achieved incredible superhuman level performance in chess and similar games silver et al 2018 while its performance in stock market prediction has been criticized despite its widespread application e g pearlstein 2018 these opposing outcomes may be explained by the following reasons chess does not possess any properties of complex systems bar yam 1997 whereas financial systems are essentially complex with a wide range of agents interacting at a wide range of scales giving rise to emergent behaviors and even black swans any ai based financial services themselves would also be agents influencing the stock market even possibly inducing vicious cycles chess can be viewed as a closed system as no exogenous factors influence any properties or dynamics of the board and players whereas stock markets are open systems and for any analyses the assumed boundary conditions depend on the analyst s judgement chess is a fully observable system as the entire board pieces rules and moves are seen by the players but stock markets are only partially observable and some controlling elements in the market might be hidden to the analysts chess is stationary as the properties and governing rules of the game remain constant over time whereas stock markets are non stationary and their long term dynamics and behaviors may change in unpredictable ways driven by political social economic or natural events so what earth and environmental systems arguably fall somewhere in between these two specific applications with respect to their four fundamental and inter related characteristics such systems are complex open partially observable and non stationary loosely speaking understanding and predicting earth and environmental systems face similar challenges to those of the stock markets in terms of those four characteristics however unlike stock market systems that are conceived to be partially predictable at best fama 1970 malkiel 2003 the behaviors of earth and environmental systems are generally believed to be predictable with limits of predictability that have been improving as more knowledge and data become available the comparisons above try to convey two points first the revolutionary success of dl in one field of application cannot necessarily be extended directly to another field of application the context matters and success depends on the characteristics of the problem at hand second different disciplines may cross fertilize dl applications and learn from one another however cross fertilization is non trivial and requires more direct communications between experts in different disciplines about existing methods common issues and ways forward 9 concluding remarks dl has perhaps by now served every researcher and practitioner in earth and environmental sciences communities in tasks such as image and language processing at least through their smart phones such astonishing and within reach technologies have boosted interest in dl and in ai in general within these communities evidenced by the significant growth in the number of their research papers on dl many including the author of this paper believe the combination of ai with unprecedented data sources and increased computational power will offer exciting new opportunities for expanding our knowledge about various earth and environmental systems unsurprisingly similar to many other innovations ai and particularly dl techniques are facing different and sometimes contrasting views towards their future for example in the hydrology context nearing et al 2020 suggest a divorce from the current hydrological theories while beven 2020 advocates for the fundamental role of knowledge base in dl interpretation it is certainly an exciting time for earth and environmental sciences to benefit from dl tools shen et al 2018 picture a bright future but articulate some important technical and cultural challenges to overcome in the years to come by more targeted educational and organizational efforts we need also to be mindful of any possible risk of hype and over excitement around these new tools arguably still many applications of dl in earth and environmental sciences have primarily focused on off the shelf applications of methods largely developed by mathematicians and computer scientists to problems in a new domain with no or limited considerations of the available domain s knowledge base the immediate risk of such practices is that the popularity of ai tools in earth and environmental sciences would then follow the ups and downs of those tools in the areas from which they originate there is also a greater risk in the author s view as follows let us flash back to more than three decades ago when the prominent statistician george box 1976 p 797 798 warned about the mathematistry trap characterized by development of theory for theory s sake which since it seldom touches down with practice has a tendency to redefine the problem rather than solve it he argued that there is unhappy evidence that mathematistry is not harmless in such areas as sociology psychology education and even i sadly say engineering investigators who are not themselves statisticians sometimes take mathematistry seriously overawed by what they do not understand they mistakenly distrust their own common sense and adopt inappropriate procedures devised by mathematicians with no scientific experience this sentiment was then echoed by the prominent hydrologist vit klemeš 1986b p 177 and p 185 who said the danger increases with the proliferation of computerized hydrologic models whose cheaply arranged ability to fit data is presented as proof of their soundness and as a justification for using them for user attractive but hydrologically indefensible extrapolations he continued the danger to hydrology from extrapolations based on mathematistry is that they lead it on the path of bad science the point here is that the risk of mathematistry seems to be just as fresh as it must have been back then particularly when it comes to the application of ai tools in earth and environmental sciences due to the very nature of such tools this risk may even well extend to their original areas of application party because of their lack of explainability and interpretability to a point that such practice has been referred to as a form of modern alchemy see rahimi and recht 2017 for the sentiment lecun 2017 for a rebuttal and hutson 2018 for a summary at a more fundamental level rudin 2019 argues that trying to explain black box models rather than creating models that are interpretable in the first place is likely to perpetuate bad practice and can potentially cause great harm to society this point is not to undermine the benefits of ai technology particularly for earth and environmental applications instead it calls for improved rigor and better appreciation of possible issues after all it has been long known even in environmental sciences that complex models can be made to produce virtually any desired behavior fallaciously given their large degrees of freedom as articulated by hornberger and spear 1981 three decades ago having such risks in mind the new potential afforded by ai for earth and environmental sciences is great to realize this potential we need to reconcile data driven ai techniques and the theory driven knowledge base the knowledge base is at the heart of traditional programming which is still a major building block of process based mechanistic modelling in earth and environmental sciences clearly the traditional knowledge based programming and ai are made up of two fundamentally different worldviews for problem solving and therefore their reconciliation will not be straightforward this paper tried to address some critical questions in this regard and provide some perspective for this important endeavor in anticipation of new breakthroughs in earth and environmental sciences in an age of big data and computational power 10 postscript i had the privilege to receive relatively extensive feedback after this paper was posted on an open access venue until the formal peer review process was completed i attribute this interest and sometimes perhaps disinterest in the paper to the extensive excitement that currently exists around ai and ml in earth and environmental sciences community among the review comments that i received there were two extreme but contrasting views on the message and sentiments of this paper one reviewer believed that this work is an attempt to ignore or minimize the benefits of the recent developments in dl for environmental modelling which are essentially the applications of deeper anns that have been made possible by the growth in computational power and data availability conversely another reviewer believed that this paper exaggerates such benefits and that the classic shallower anns that have been subject to extensive research in the past three decades can work at least equally well in environmental modelling i was intrigued by these two contrasting views and thought i would reflect on them in this postscript section this paper was not intended to support any of these contrasting views instead the intention was to provide a rather balanced overview of how far we have come and the long standing challenges in an attempt to better inform and shape future ai initiates in our community away from any possible hype or over excitement one way or the other my take is that either of those contrasting views may be subject to personal biases arising from factors such as research background and career stage the former view seems to resonate primarily with the newer generation of researchers who are currently championing dl in earth and environmental sciences the latter view however seems to be shared by more senior colleagues whose research portfolios include a long history of work on anns my personal communications suggest that this group often feels frustrated by the recent avalanche of papers branded under deep learning many of which might be considered by the group as reinventing the wheel ignoring similar previous successful work in the field of environmental modelling and beyond being mindful of this fact the present paper included numerous older papers with ideas that still seem fresh today with respect to branding i would like to include a quote from a recent book by duerr et al 2020 that in the earlier days of machine learning ml neural networks nns were already around but it was technically impossible to train deep nns with many layers mainly because of a lack of computer power and training data why do we talk about dl instead of artificial nns dl sells better than artificial nns this might sound disrespectful but such rebranding was probably a smart move especially because nns haven t delivered what was promised during the last decades and therefore gained a somewhat bad reputation and a quote from the famous book by goodfellow et al 2016 that at this point the authors refer to the early 2000s deep networks were generally believed to be very difficult to train we now know that algorithms that have existed since the 1980s work quite well but this was not apparent circa 2006 the issue is perhaps simply that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time any interpretation of the above statements should of course consider their context which is computer science and areas of applications which primarily are computer vision and natural language processing also deep anns in those applications typically have thousands of layers while in earth and environmental modelling deep anns may have significantly fewer layers perhaps in the order of tens or less however the above statements suggest that our community might need to be more careful in branding recent projects and initiatives around dl with stronger connection with earlier similar work performed in our fields a reviewer asked for a direct comparison of deep versus shallow anns wondering if deeper ones have really any superiority for environmental modelling i think running such comparisons would require extensive numerical experiments across several case studies and may not be straightforward instead this paper tried to explain an old theorem in simple language how anns either shallow or deep are universal function approximators and can basically approximate any function if set up properly therefore for typical environmental modelling i am of the mind that one can make either one work but the modelers need to be aware of nuanced issues as described in this paper beyond any debate around deep versus shallow anns a primary motivation of writing this paper was to address a long standing issue that there is some mistrust in ml in part of our community particularly among process hydrologists and theory driven modellers i have extensively dealt with that issue over the years as a researcher who started his research career with ml and anns in hydrology back in 2003 but gradually delved into process based modelling my observations suggest that ml and process based modelling have largely evolved in isolated research camps among which the co creation or exchange of knowledge has been limited the present paper might serve in bringing the two worldviews together and promote the dialogue between the champions of process based modelling and those of ml this dialogue might be much needed at this time of excitement and increased funding around ml in earth and environmental communities such dialogues can help us ensure proper training of the current students so they retain curiosity about physical understanding and expand our knowledge base while being equipped with the emerging data science technologies and the ever growing computational power declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper was shaped by discussions with david hannah amin elshorbagy hoshin gupta grey nearing steven weijs dimitri solomatine sujay kumar lucy marshall amin dezfuli thorsten wagener yashar mehdad facebook ai and many others over the last year who directly or indirectly provided feedback on different aspects covered also i am thankful to dan ames tony jakeman joseph guillaume and holger maier from environmental modelling and software for providing constructive comments that helped me improve the paper this work was a part of my sabbatical plan that was scheduled to take place during 2020 21 in australian national university university of birmingham as a ias vanguard fellow and university of bristol as a benjamin meaker distinguished visiting professor although these visits were cancelled due to the covid 19 pandemic i am in indebted to tony jakeman david hannah and thorsten wagener for their gracious and generous offers to host me and my family at their respective institutions and my special thanks to nasim my wife and the kids kian and nikan who made my stay at home sabbatical time memorable full of love and fun lastly funding supports from the natural sciences and engineering research council of canada discovery grants and integrated modeling program for canada impc under the framework of global water futures gwf are acknowledged 
25766,propensity score matching psm and distance adjusted psm enable estimation of causal effects from observational data by selecting controls that are similar to treated observations in terms of environmental covariates and spatial locations quantifying effects of natural disturbances such as wildfires often encounters limited availability of observational data due to the scarcity of ecological events or cost of data collection over large areas using empirical data of national forest inventory plot measurements we conducted a sensitivity analysis of distance adjusted psm on data availability for wildfire effect quantification using monte carlo simulations we assessed the influence of sample size and covariates on the balance of propensity score distributions and the performance of effect estimates the inclusion of the distance measure in matching compensated for the omission of key covariates this study provides a practical guide of determining sample size and covariates for matching with spatial information to analyze ecological data keywords data availability sample size environmental covariates natural disturbance quasi experimental method monte carlo simulation 1 introduction natural disturbances such as wildfires disease and insect outbreaks drive changes in the forest environment on a large scale wildfires one of the major natural disturbance agents cause the instant removal and redistribution of forest carbon dixon et al 1994 keith et al 2014 as well as long term effects such as delayed tree mortality carlson et al 2012 and carbon losses through decomposition of dead trees eskelson et al 2016 however the effects of natural disturbances such as wildfire on forest dynamics are difficult to estimate due to the lack of an experimental setting for natural disturbances quantification of the impacts of natural disturbances and other ecological processes has been largely based on statistical modelling using observational data liu et al 2011 instead of randomized experiments that control for confounding factors recent studies suggest quasi experimental approaches greenstone and gayer 2009 for impact quantification of wildfires butsic et al 2017a larsen et al 2019 woo et al 2021 to allow causal inference from observational studies quasi experimental methods attribute causal relationships between a treatment e g natural disturbance and outcome e g change in forest environment from observational data by specifying a control group that is as similar as possible to the treatment group based on potential confounding factors campbell and stanley 2015 propensity score matching psm is one of the quasi experimental methods that enables cause and effect analysis from observational data rosenbaum and rubin 1983 a set of observations that were affected by a natural disturbance i e treated group is matched to observations that were not affected i e untreated or raw control group based on propensity scores the propensity score denotes the probability of experiencing the natural disturbance conditional on the environmental covariates rosenbaum and rubin 1983 balancing the propensity score distributions between treated and control groups removes the possible selection bias introduced by the environmental covariates leading to a similar setting as that of a randomized experiment which assigns a known probability of treatment to the treated and control groups stuart and rubin 2004 the matched observations i e control group with balanced propensity score distributions allow unbiased estimation of the outcome of interest stuart 2010 austin 2009 ecological data are often associated with geographical locations and the spatial information of the data can reflect some of the confounding variables that were unmeasured dray et al 2006 because psm including geographical coordinates as covariates did not ensure the spatial proximity between treated and untreated groups distance adjusted propensity score matching dapsm has been suggested by papadogeorgou et al 2019 dapsm incorporates spatial distance measures in addition to the estimated propensity scores to account for unobserved covariates the application of psm and dapsm to ecological data however was introduced only recently e g butsic et al 2017a woo et al 2021 so the performance of these matching methods in terms of balance between treated and control groups and unbiasedness of the estimates under different data availability scenarios is still unknown unlike large survey data from health science and econometrics e g 30 000 treated and 60 000 untreated from smeeth et al 2009 3000 treated and 100 000 untreated from huber et al 2013 ecological data sets associated with natural disturbances are often limited in size due to the scarcity of the natural disturbance events e g wildfire disease or insect outbreak or the cost of data collection over large spatial and temporal scales kremens et al 2010 in matching availability of data can be identified based on the number of treated observations raw control observations and environmental covariates to estimate the propensity scores limited data on any of these may affect the reliability of the causal effect quantification stuart 2010 the number of treated observations is directly related to the variance of the estimated treatment effect brookhart et al 2006 and the number of raw control observations relative to the treated observations is associated with the bias in propensity scores heinrich et al 2010 the availability of covariates influences the exclusion of important confounding variables when estimating the propensity scores heckman et al 1997 given that matching based on limited data may impose more severe bias on the effect estimation than regression adjustments stuart 2010 it is critical to understand the performance of matching methods under different data availability scenarios and identify a marginal size of data and covariate set required to implement matching methods this study provides a sensitivity analysis of matching on data availability in terms of the balance achieved in propensity score distributions and of the estimated effect using a case study of wildfire effect estimation woo et al 2021 quantified the effect of wildfires on aboveground live woody forest carbon using dapsm and psm based on an empirical dataset of 23 150 forest inventory plot measurements from washington and oregon united states of america usa taking advantage of the large number of forest inventory plot measurements used in woo et al 2021 we performed a monte carlo simulation to generate subsets of the full dataset that contained 611 burned plots i e treated and 22 539 unburned plots i e raw control the propensity scores of the burned and unburned plots were estimated based on sixteen covariates that describe the different environment of plots with and without distance adjustment we examined three scenarios of data availability 1 number of burned plots 2 ratio of unburned plots relative to burned plots and 3 availability of environmental covariates for 3 we considered availability of the covariates based on data sources to define four groups topography climate land cover and remote sensing variables we first assessed how well the propensity score distributions were balanced between the burned and the control plots through matching we then observed how the estimated wildfire effect changed under the different scenarios the effects of wildfire were quantified as the difference in live woody carbon mass between the burned and the control plots we presented the balance achieved and effect estimates under dapsm and psm to assess the bias and variability expected within each matching method and data availability scenario by providing a sensitivity analysis of dapsm and psm based on real world data the results of this study suggest practical guidelines of data availability to be considered for wildfire effect quantification and by extension estimation of other natural disturbance e g arovaara et al 1984 and management effects e g butsic et al 2017b utilization of national forest inventory data will broaden the opportunity to conduct effect analyses over large spatial and temporal scales with spatially balanced samples of forest conditions the diagnostics of balance achievement by matching presented in this study are applicable for designing future quasi experiments where the spatial distances as well as the environmental covariates play an important role in distinguishing treated and raw control observations by implementing matching methods as quasi experimental approach causal effects can be estimated from observational data to inform ecological problems for which experimental settings are impractical 2 data 2 1 forest inventory data we used national forest inventory data of washington and oregon usa from the united states department of agriculture forest service forest inventory and analysis fia program smith 2002 since 2000 the fia program has collected field measurements of forested lands i e at least 10 percent potential cover by live trees by measuring permanent plots 4047 square meter in size that represent a spatially balanced sample of one plot for every 2400 ha bechtold and patterson 2005 a plot consists of four circular subplots and the measurements of trees and environmental attributes are made at both plot and subplot level ten percent of the plot locations are visited annually resulting on a 10 year remeasurement cycle bechtold and patterson 2005 between 2001 and 2016 there were a total of 23 150 inventory plot measurements in washington and oregon forests including 7994 remeasurements we classified the plot measurements into burned i e treated t and unburned i e raw control rc based on the monitoring trends in burn severity mtbs http www mtbs gov map burned plots were identified by spatially overlaying the plot locations on the fire maps every fia plot measurement that fell within the perimeter of a wildfire that occurred less than five years prior to the measurement was labeled as burned meanwhile plots that had not burned at least ten years prior to measurement were labeled as unburned here we refer to the burned measurements as burned plots and unburned measurements as unburned plots for simplicity thus unburned plots may have repeated measurements at the same location to remove dependencies among burned and unburned plots previous measurements of burned plots were excluded from the pool of raw control plots similarly if a plot burned twice the measurements after the first wildfire were also excluded to preclude the effect of reburn woo et al 2021 a total of 611 burned plots and 22 539 unburned plot measurements were used for the analysis more details on the compilation of the data are available in woo et al 2021 2 2 environmental covariates for each forest inventory plot we retrieved sixteen environmental covariates that are associated with the probability of wildfire occurrence these environmental covariates were grouped into four categories 1 topography topo 2 climate clim 3 land cover land and 4 remotely sensed variables remo table 1 additional details in woo et al 2021 topography and land cover variables of the forest inventory plots were measured in the field and obtained from the fia database topography covariates included elevation slope and aspect elevation was measured at the center of the plot and slope and aspect were measured at subplot level according to the land classification within the subplot e g forested non forested land we used the values of slope and aspect that accounted for the largest percentage of the forested land classification within the subplots land cover covariates including land ownership physiographic condition and proportion of forested area table 1 were measured at subplot level for the class variables we used two categories of land ownership public and private and three of physiographic class mesic xeric and hydric woo et al 2021 we assigned the land cover categories that accounted for the largest percentage within the four subplots climate covariates consisted of six prism derived variables related to precipitation temperature vapor pressure and moisture stress daly et al 2008 the climate covariates were derived from thirty year normal data 1981 2010 with 30 m resolution the remote sensing data included four spectral and vegetation indices including the normalized difference vegetation index ndvi and tasseled cap brightness greenness and wetness in the middle of growing season all derived from annual landsat satellite images with 30 m resolution unburned plots were associated with the remotely sensed variables in the year that the plot was measured and burned plots were associated with the variables measured in a year before the wildfire to account for stand characteristics that may be related to the risk of burning to assign the climate and remotely sensed covariates to each of the plot measurements the values of 9 pixels around the plot center location were averaged the climate and remotely sensed metrics were obtained from the landscape ecology modeling mapping and analysis lemma team https lemma forestry oregonstate edu 2 3 aboveground live woody carbon we obtained aboveground live woody carbon mass per plot mg ha 1 for both burned and unburned plots based on all live trees over 2 54 cm diameter at breast height dbh measured within the forest inventory plot by the fia protocol bechtold and patterson 2005 aboveground woody biomass including stem bark and live branches was estimated by the pacific northwest research station usda forest service woodall et al 2012 we converted the aboveground live woody biomass into carbon mass using a carbon ratio multiplier of 0 5 ipcc 2006 the obtained carbon mass was summarized as per ha values mg ha 1 for each plot and used as the outcome variable in propensity score matching to assess the effect of wildfires the estimated wildfire effect is the difference in the outcome variable between burned and unburned plots 3 matching methods 3 1 psm and dapsm for assessing wildfire effects on aboveground live woody carbon propensity scores in psm denote the probability of receiving the treatment conditional on the environmental covariates the probability assigned to each observation in this study the forest inventory plot is generally estimated using logistic regression distance adjusted propensity scores in dapsm consider spatial distances between burned and unburned plot locations in matching they are a weighted average between the propensity scores estimated from environmental covariates and the normalized euclidean distances between plot locations papadogeorgou et al 2019 because the spatial distance may take into account unobserved covariates that were not included in the covariate set to estimate the propensity scores dapsm should be preferred to psm or spatial matching sm that considers only the distances for matching woo et al 2021 the quantification of wildfire effects on aboveground live woody carbon using the matching methods followed three steps first we estimated propensity scores of burned and unburned plots using logistic regression with the sixteen environmental covariates then we estimated the distance adjusted propensity scores as a linear combination of the propensity scores and spatial distances between pairs of burned and unburned plots we used a weight of 0 5 to equally emphasize the observed environmental covariates and the spatial distance given an earlier study that found other weights i e 0 25 0 5 and 0 75 did not result in any difference in the effect estimation woo et al 2021 finally we matched the burned plots and unburned plots using psm and dapsm to obtain a set of control plots that share similar covariates and geographical locations we employed matching without replacement with a greedy algorithm austin 2011 that sequentially finds the smallest difference in the distance adjusted propensity scores between the burned and the unburned plots this algorithm is the most commonly used in propensity score matching implementation austin and cafri 2020 the matched unburned plots were identified as control plots the estimates of the wildfire effects were then quantified as the difference in aboveground live woody carbon mass between the burned and control plots 3 2 sensitivity analysis of dapsm under different data availability scenarios we ran monte carlo simulations to perform a sensitivity analysis on the balance of propensity score distributions as well as effect estimates when reducing the number of treated and raw control observations and covariate sets we created random subsets of forest inventory plot data from the full data consisting of 611 burned plots n t 611 and 22 539 unburned plots n r c 22 539 the full data was set as our reference population for the monte carlo simulations i e simulation population and the propensity scores estimated based on all available environmental covariates using the simulation population were considered to be the true propensity scores in our simulation because the number of possible combinations of all covariates is very large we grouped the environmental covariates sharing similar attributes and sources as stated in section 2 2 based on the idea that the attainability of variables is dependent on the accessibility to the data source for example topo including elevation slope and aspect are easily available for researchers through open sources such as digital elevation models dem or geographical information systems gis for most regions bungartz et al 2018 the order of accessibility to the four groups of covariates in our study was identified as topo clim remo land from the least to the most costly to obtain we considered four scenarios of environmental covariate sets for propensity score estimation 1 full set of covariates with all four groups topo clim remo land 2 topo clim remo 3 topo clim and 4 topo remo table 2 we examined three simulation scenarios of data availability number of burned plots n t number of unburned plots relative to burned plots n r c n t and environmental covariate set we varied numbers of burned plots n t and unburned plots relative to burned plots n r c n t for the random subsets of data from the simulation population table 2 under each of the four covariate set scenarios from each combination of subsets and covariate sets the plots were matched using different matching methods dapsm based on both the propensity scores and plot locations and psm based only on the propensity scores additionally we implemented sm by matching the plots based only on the distances among the plot locations the process was iterated 100 times for each combination of simulation values and conditions under different matching methods we observed the propensity score distributions and the effect estimates under the data availability scenarios we also computed mean distance to the control plots from burned plots under each matching method dapsm psm and sm 3 3 evaluation of results 3 3 1 descriptive statistics for evaluation of balance in propensity scores the quality of the matching was assessed by the balance in propensity scores between treated and control groups mccaffrey et al 2013 stuart 2010 here balance means that the distribution of propensity scores of the matched pairs were similar thus the matching reduced the potential selection bias between treated and raw control groups caused by the environmental covariates included in the model rubin 2001 suggested three statistics to assess balance 1 the difference in the means of the propensity scores between the treated and control groups 2 the ratio of the variances of the propensity scores and 3 the ratio of the variances of the residuals of the covariates after matching table 3 1 and 2 represent the first and second moment of the overall propensity score distribution respectively the ratio of the variances of the residuals of the covariates is obtained by regressing each of the covariates on the estimated propensity scores if the matching was successful and the control group is similar to the treated group the means of difference and the variance ratios of the propensity scores should be close to zero and one respectively ratio of variance between 0 8 and 1 2 is considered adequate while values below 0 5 or over 2 are too extreme rubin 2001 for the ratio of the variances of the residuals of the covariates we computed the percentage of covariates that are within each of the variance ratio ranges i e 0 8 and 1 2 0 5 or 2 for the 16 variables considered even if the model only included a subset of covariates in addition to the statistics suggested by rubin 2001 we compared the absolute standardized mean difference asmd in the sixteen covariates between the unmatched and matched burned and unburned plots to examine the balance at the individual covariate level nolte et al 2017 austin 2009 asmd was computed as the average difference standardized by the standard deviation of the variable for continuous covariates and using the average difference in proportions for categorical variables stuart 2010 asmd values less than 0 2 indicate adequate balance in the covariates while asmd values between 0 2 and 0 4 indicate moderate imbalance and asmd values over 0 4 indicate large imbalance cohen 2013 before matching the bias and variance ratio of the estimated propensity scores using the full population of burned and unburned forest inventory plots n t 611 and n r c n t 36 7 and the full covariate set 16 variables topo clim remo land was very large table 3 after dapsm the bias decreased by approximately 97 from 0 031 to 0 001 the variance ratio decreased from 3 11 to a value very close to 1 thus dapsm with all 16 covariates resulted in a control group similar to the burned plots in terms of propensity scores rubin 2001 regarding the ratio of the variance of the residuals of the covariates 50 of the covariates 8 out of 16 showed relatively large variance ratios e g 0 8 or 1 2 between treated and raw control meaning that there was imbalance between burned and unburned plots for those covariates table 3 when dapsm was applied the variance ratios ranged within 0 8 1 2 for all covariates 100 16 out of 16 table 3 these three statistics obtained from dapsm and the full data set were the baseline to compare bias and variance ratios from the subsets of data and covariate set using dapsm and psm we graphically presented the comparisons of the descriptive statistics along with the combinations of the three simulation scenarios for effective visualization pianosi et al 2016 3 3 2 performance of matching estimates the bias reduction and variability of matching estimates was examined using the distributions of the simulated wildfire effect estimates computed under varying sample sizes and covariate sets the wildfire effects were calculated as the difference in the aboveground live woody carbon mass between the burned and control plots we obtained the baseline estimate of wildfire effects on aboveground live woody carbon mass based on the simulation population to benchmark the effect estimation based on the simulation population the difference in the mean aboveground live woody carbon mass between the burned and the control plots was estimated as 30 95 mg ha 1 in absolute value the difference in the mean carbon mass between burned and raw control plots was 50 82 mg ha 1 so that the wildfire effect on aboveground live woody carbon mass was overestimated by 19 87 mg ha 1 on average due to the selection bias introduced by the different environmental conditions between plots that burn and those that do not we presented the simulated wildfire effect estimates from the subsets of data against the baseline estimate to assess the mean bias and the variability of the matching estimates also we computed relative bias of the simulated wildfire effect estimates compared to the simulation population under different data availability scenarios the simulated estimates and their relative bias were displayed as boxplots and matrix plots respectively 4 results 4 1 balance in propensity scores under different data availability scenarios 4 1 1 smaller mean bias in propensity scores under larger n t and covariate set with clim the three scenarios of data availability in matching i e n t n r c n t and covariate set jointly affected the mean bias in propensity scores under dapsm a notable change in the mean bias in propensity scores was found among different covariate sets while there was little deviation from the baseline bias under dapsm with the full covariate set including topo clim remo and land 0 003 fig 1 a dropping land variables fig 1b and clim fig 1c resulted in larger bias on average 0 005 0 007 and 0 007 0 019 respectively excluding remo from the covariate set also increased the bias but not much e g 0 002 on average fig s1 in supplementary materials the mean bias in propensity scores was relatively constant across n t with a minimal increase in bias as n r c n t decreased to 10 fig 1a and b when the covariate set excluded clim variables fig 1c however the increase of bias was remarkable compared to the covariate sets including clim especially when n r c n t was less than 10 in contrast to the covariate sets including clim fig 1a and b the bias increased with decreasing n t for the covariate set that excluded clim variables fig 1c comparing dapsm to psm shows that including distance in the matching process did not result in large differences in mean bias in propensity scores as long as clim variables were included in the covariate set fig 1a vs 1d and fig 1b vs 1e without clim variables psm produced much larger bias than dapsm regardless of n r c n t fig 1f without any environmental covariates included in the matching process i e sm with distance only fig 1e the mean bias showed a similar pattern to dapsm without clim fig 1c small increase in mean bias on average and a sharp increase in mean bias when n r c n t 10 4 1 2 smaller variance ratio in propensity scores under large n t and covariate set with clim the variance ratio in propensity scores between the burned and the control plots generally followed similar patterns as the mean bias in propensity scores matching based on covariate sets that excluded clim variables led to a large increase in the mean variance ratio with decreasing n t and n r c n t fig 2 when using the full covariate set the variance ratio fell within the range of 0 8 1 2 i e close to 1 for most combinations of n t and n r c n t except for n t less than 25 and n r c n t less than 5 fig 2a dropping the land variables resulted in variance ratios larger than 1 2 for the majority of combinations of n t and n r c n t but not in extreme values exceeding 2 fig 2b excluding clim variables from the covariate set introduced large variability in the variance ratio among different n t and n r c n t fig 2c small n t i e 75 and n r c n t i e 5 exhibited extreme variance ratios over 2 indicating imbalance in the distributions of propensity scores between burned and control plots psm produced slightly smaller variance ratios than dapsm did when clim was included in the covariate set fig 2a vs 2d fig 2b vs 2e psm without clim however did not reduce the variance in propensity scores between burned and unburned plots red lines in fig 2f once distance was included in the matching process however the variance ratio substantially dropped especially for large n t 100 and n r c n t 15 values even if no other covariates were included fig 2g 4 1 3 smaller variance ratio of the residuals of the covariates under dapsm comparing raw control to controls obtained through dapsm and psm the improvement in the variance ratio of residuals of individual covariates through matching was obvious in all covariate sets showing increase in the percentages of variance ratio between 0 8 and 1 2 fig 3 a d dapsm exhibited larger percentages of variance ratio between 0 8 and 1 2 than psm did indicating the method was more effective achieving balance in terms of individual covariates sm based on distance only was also more balance in individual covariates than psm especially when n r c n t 10 fig 3d as n t decreased increasingly extreme variance ratios of the residuals were observed for the matched plots regardless of n r c n t fig 3a d n t 25 barely reduced the variance ratio for any n r c n t through matching when climate variables clim were included in the covariate set the distribution of variance ratios obtained through both dapsm and psm did not differ much among the covariate sets fig 3a c topo and remo variables however showed little initial imbalance in the variance ratio of residuals between burned and raw control plots fig 3d compared to other covariate sets in that case psm introduced more severe imbalance in variance ratio than the raw controls fig 3d 4 1 4 dapsm balances some covariates that were not included in propensity score model the average asmd computed for each covariate showed major differences between dapsm and psm under the same covariate set table 4 fig s4 comparing the raw controls of unburned plots to the burned plots i e unmatched 11 out of 16 covariates were imbalanced i e asmd 0 2 matching based on the full covariate set achieved balance in all covariates under both dapsm and psm when the covariate sets were reduced psm resulted in imbalance in the omitted covariates e g physcl temp tempdf svpd and gsms under psm with topo remo table 4 on the other hand dapsm accomplished balance in climate variables even though the clim covariates were excluded in the propensity score estimation sm generally achieved balance in most of the covariates except for slope and physcl despite the withdrawal of all covariates in the matching process 4 2 performance of wildfire effect estimates under different data availability scenarios again the three data availability scenarios n t n r c n t and covariate set affected the bias and variance of wildfire effect estimates collectively under dapsm the number of burned plots in the simulation subsets n t was related to the bias of the estimates of average wildfire effect whereas the ratio of unburned to burned plots n r c n t influenced the variability of the estimates fig 4 dropping clim from the covariate set increased the bias and variability under small values of n t and n r c n t fig 4c when n t 500 the simulated mean effect estimate obtained from dapsm was 30 84 mg ha 1 similar to the baseline estimate 30 95 mg ha 1 the estimates of wildfire effect across all n t and n r c n t values under dapsm deviated on average by 25 30 from the baseline estimate when the covariate set included clim variables fig 4a and b whereas the average deviation was 43 for the covariate sets without clim variables fig 4c n t less than 100 tended to show large variability in the wildfire effect estimates resulting in positive effect estimates for a few subsets the bias of the effect estimates tended to increase with decreases in n t and n r c n t under dapsm fig 4a c while the bias of psm derived estimates was mostly dependent on covariate sets only not on n t or n r c n t fig 4d f the matrix plots fig 5 suggested that the bias was susceptible to small n r c n t than small n t when clim was included in the covariate set controls from dapsm with n t 100 and n r c n t 20 were found to show 20 bias compared to the simulation population fig 5a and c for small numbers of n t and n r c n t e g n t 100 and n r c n t 10 psm exhibited smaller bias than dapsm once clim was included in the covariate set fig 5 4 3 distance between matched pairs under different matching methods the mean distance between the burned plots and the matched control plots under psm ranged between 340 km and 360 km regardless of the number of burned and unburned plots it did not change substantially regardless of n t and n r c n t the mean distance under psm was similar to the mean distance of all pairwise burned plots and raw control plots in the full data 363 1 km the maximum pairwise distance between burned and raw control plots in the full data was 945 8 km under dapsm the mean distance varied from approximately 7 km 80 km increasing with decreasing number of observations fig 6 the mean distance changed little among the covariate sets under the same matching method sm showed the smallest mean distance ranging from 4 4 to 65 km fig 6 5 discussion 5 1 covariates and distance affect the balance achievement in ps distributions when matching on the propensity scores alone psm the covariates included in the models were the key driver determining the balance of the propensity score distributions between treated and controls neither the sample size nor the ratio of the number of treated plots to controls significantly affected the performance of matching clim variables were the key covariates to estimate the propensity scores for matching so that covariate sets without clim failed to fully reduce the selection bias between the burned and the unburned plots our identification of clim as the key covariates was expected as climate is one of the most critical factors determining wildfire occurrence mckenzie et al 2004 weisberg and swanson 2003 as well as carbon mass in forests stegen et al 2011 at a regional scale the results imply that to achieve balance in propensity scores it is more effective to identify and include key covariates than to increase the sample size once distance was included in the matching process dapsm the importance of clim variables subsided except when the sample size was very small large bias and variance ratio in propensity scores under psm without clim compared to dapsm suggests that the spatial distance partially incorporated the effect of clim variables in estimating the probability of wildfire the results of average asmd also specifically confirm that the distance measure contributed to balancing the environmental covariates that were not included in the propensity score estimation model while psm obtained balance only on the covariates that were included in the model closer plots have higher chance to share similar climate conditions di cecco and gouhier 2018 thus the spatial proximity may have compensated for the exclusion of clim variables even sm based solely on spatial distance outperformed psm without clim variables the results emphasize the importance of incorporating spatial measures to account for both observed and unobserved confounding factors woo et al 2021 papadogeorgou et al 2019 however incorporating spatial proximity was no longer effective as the observations became sparser over the study area the bias and variance ratio in propensity scores were remarkably large under sample sizes n t around 150 or less and n r c n t 10 this finding indicates that the benefit of spatial distance operates at a particular scale where the geographic variation lines up with the heterogeneity of important covariates the data of national forest inventory plot measurements utilized in our study were distributed systematically over the states of the pacific northwest at an approximate distance of 5 km between plots bechtold and patterson 2005 hence the spatial distance among the plots captured relatively large scale variation of climate rather than local geography such as aspect and slope in our data with the full sample the average distance between matched pairs under dapsm was about 7 km when the distance between the pairs of treated and control units became extreme due to the small sample size over the region spatial distance did not account for the variation of climate when the sample size decreased below n t 150 and n r c n t 10 the mean distance was approximately 20 km thus the ability of spatial distance to account for the effect of covariates may be linked with the patterns of landscape variables depending on the spatial resolution wang et al 2014 turner et al 1989 the three data availability scenarios the number of treated number of raw control relative to treated and covariate set interplay with each other to achieve the balance in ps distribution between the treated and the controls as the performance of the models estimating propensity scores degrades due to the omission of key covariates spatial information will compensate the lack of data unless the observations are too sparse likewise when less observations are available and the geographic locations do not provide additional information covariates will become relatively more important this is especially true when the heterogeneity of the key covariates does not align with the spatial scale of the data 5 2 sample size suggestion based on the effect estimates using dapsm and forest inventory data the effect of spatial distance in the performance of the matching was also evident from the estimated effect of wildfire defined as the difference in live carbon between burned and control plots dapsm was superior to psm showing less bias most likely because the spatial distance accounted for additional information from unobserved covariates these results agree with a recent clinical study that shows that spatial propensity score matching outperformed non spatial matching davis et al 2021 however our study further showed that including the distance measure was less effective or counterproductive when the sample size was very small and the distance between possible matches too large then the spatial distance provided no additional information resulting in an even larger bias in the estimates while the critical sample size may be scale dependent based on the results of simulated estimates we suggest a sample size of at least n t 100 and n r c n t 10 as the threshold to apply dapsm over washington and oregon this threshold of number of treated and raw control observations can serve as a practical guideline when implementing dapsm for further research using forest inventory at regional scales further research may not be limited to wildfire effects on carbon mass but on various outcomes of interest that are closely related with climate variables such as vegetation structure duguy et al 2013 biodiversity gill et al 2013 or land use duguy et al 2012 the bias in the estimates of wildfire effects observed from our study was generally larger than the results from pirracchio et al 2012 who obtained less than 10 of relative bias when decreasing the sample size from 1000 to 40 the relative bias from pirracchio et al 2012 was obtained from monte carlo simulations using an artificial dataset for which the data generation process was known it is common to use an artificial dataset for sensitivity analyses of psm e g coffman et al 2020 rubin and thomas 1996 based on the true propensity score model or true estimates our simulation on the other hand was conducted with empirical data of forest inventory measurements that involves additional uncertainty from sampling and measurement errors rudolph and stuart 2018 bennett et al 2013 the treatment in our study was a binary factor of either presence or absence of wildfire because the effect of different severities of wildfires was beyond the research interests of this study large variability across fire severities in burned plots may have increased the bias of our estimated wildfire effects nevertheless our study reflects actual relationships among the environmental variables under the presence of wildfires contrary to artificial data lechner and wunsch 2013 further application of various extended matching methods including propensity score estimation using generalized boosted models mccaffrey et al 2013 will help to assess multiple treatment effects from observational data 5 3 future work using forest inventory data dapsm can be utilized to examine any post disturbance effects based on the current forest inventory data as long as the number of disturbed plot measurements is sufficient i e 100 if the number of treated plots for a given disturbance is less than the desired threshold over a spatial region one could possibly increase the spatial and temporal intensity of the inventory plot measurements in the database such temporal or spatial intensifications are already effective as supplemental studies under the fia program the fire effects and recovery study fers used to assess fire effects and post fire carbon dynamics e g eskelson et al 2016 and the sudden oak death syndrome assessment jain and fried 2010 extension of matching methods such as matching with replacement e g nolte et al 2013 can also resolve the small sample size problem austin and cafri 2020 while maintaining high quality of matching abadie and imbens 2006 taking advantage of the well defined population and unified protocol of measurements the national forest inventory dataset combined with a variety of quasi experimental methods will provide opportunities of continuous causal effect analysis for ecological data 6 conclusion this study presented a sensitivity analysis of distance adjusted propensity score matching dapsm in relation to data availability we examined three data availability scenarios in matching number of available burned plots number of unburned plots relative to burned plots and environmental covariate set to estimate the propensity scores for the quantification of wildfire effects using national forest inventory plot measurements in washington and oregon we conducted monte carlo simulations of random subsets of burned and unburned plots to quantify the wildfire effects under different covariate sets for each simulation performance of dapsm was evaluated as the bias and variance ratio of propensity scores as well as the bias and variability of the estimates of wildfire effect we studied the influence of distance measure in matching by comparing the performance of dapsm to that of matching without distance adjustment we found that the three data availability scenarios interacted to affect the performance of matching among the three scenarios whether the covariate set included climate variables or not dominated the degree of balance in propensity score distributions between burned and control plots climate variables turned out to be the key covariates to analyze the wildfire effect on forest carbon mass the sample size especially the number of unburned plots relative to the burned plots was closely related with the distance between the matched pairs large distance among the plots due to sparseness of observations over the region resulted in severe bias and variability in the effect estimates because the key covariates were not accounted for by the distance measure based on our results the importance of key covariates and the inclusion of the spatial measure were emphasized for applying propensity score matching methods to account for both observed and unobserved confounding variables we also suggest a marginal sample size with at least a hundred treated observations and a tenfold larger number of raw controls to implement dapsm over washington and oregon to achieve balance in propensity score distributions and low bias in effect estimates future studies implementing matching approaches for effect analyses using forest inventory plot data will find this research useful to determine the sample size and covariate set in relation with spatial distances among the observations national forest inventory data facilitate monitoring and the assessment of comprehensive forest resources over large spatial and temporal scales utilization of dapsm with forest inventory data enables rigorous research on effect analysis not limited to wildfires but applicable to other disturbances and ecological processes where a randomized experiment is infeasible declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported partly by 4 year doctoral fellowship from the faculty of forestry provided by the university of british columbia canada we thank matthew gregory from the landscape ecology modeling mapping and analysis lemma team for his assistance with the prism and landsat data retrieval and compilation we also thank the many individuals involved in the design field data collection quality assurance and processing of the u s forest service forest inventory and analysis program appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105163 
25766,propensity score matching psm and distance adjusted psm enable estimation of causal effects from observational data by selecting controls that are similar to treated observations in terms of environmental covariates and spatial locations quantifying effects of natural disturbances such as wildfires often encounters limited availability of observational data due to the scarcity of ecological events or cost of data collection over large areas using empirical data of national forest inventory plot measurements we conducted a sensitivity analysis of distance adjusted psm on data availability for wildfire effect quantification using monte carlo simulations we assessed the influence of sample size and covariates on the balance of propensity score distributions and the performance of effect estimates the inclusion of the distance measure in matching compensated for the omission of key covariates this study provides a practical guide of determining sample size and covariates for matching with spatial information to analyze ecological data keywords data availability sample size environmental covariates natural disturbance quasi experimental method monte carlo simulation 1 introduction natural disturbances such as wildfires disease and insect outbreaks drive changes in the forest environment on a large scale wildfires one of the major natural disturbance agents cause the instant removal and redistribution of forest carbon dixon et al 1994 keith et al 2014 as well as long term effects such as delayed tree mortality carlson et al 2012 and carbon losses through decomposition of dead trees eskelson et al 2016 however the effects of natural disturbances such as wildfire on forest dynamics are difficult to estimate due to the lack of an experimental setting for natural disturbances quantification of the impacts of natural disturbances and other ecological processes has been largely based on statistical modelling using observational data liu et al 2011 instead of randomized experiments that control for confounding factors recent studies suggest quasi experimental approaches greenstone and gayer 2009 for impact quantification of wildfires butsic et al 2017a larsen et al 2019 woo et al 2021 to allow causal inference from observational studies quasi experimental methods attribute causal relationships between a treatment e g natural disturbance and outcome e g change in forest environment from observational data by specifying a control group that is as similar as possible to the treatment group based on potential confounding factors campbell and stanley 2015 propensity score matching psm is one of the quasi experimental methods that enables cause and effect analysis from observational data rosenbaum and rubin 1983 a set of observations that were affected by a natural disturbance i e treated group is matched to observations that were not affected i e untreated or raw control group based on propensity scores the propensity score denotes the probability of experiencing the natural disturbance conditional on the environmental covariates rosenbaum and rubin 1983 balancing the propensity score distributions between treated and control groups removes the possible selection bias introduced by the environmental covariates leading to a similar setting as that of a randomized experiment which assigns a known probability of treatment to the treated and control groups stuart and rubin 2004 the matched observations i e control group with balanced propensity score distributions allow unbiased estimation of the outcome of interest stuart 2010 austin 2009 ecological data are often associated with geographical locations and the spatial information of the data can reflect some of the confounding variables that were unmeasured dray et al 2006 because psm including geographical coordinates as covariates did not ensure the spatial proximity between treated and untreated groups distance adjusted propensity score matching dapsm has been suggested by papadogeorgou et al 2019 dapsm incorporates spatial distance measures in addition to the estimated propensity scores to account for unobserved covariates the application of psm and dapsm to ecological data however was introduced only recently e g butsic et al 2017a woo et al 2021 so the performance of these matching methods in terms of balance between treated and control groups and unbiasedness of the estimates under different data availability scenarios is still unknown unlike large survey data from health science and econometrics e g 30 000 treated and 60 000 untreated from smeeth et al 2009 3000 treated and 100 000 untreated from huber et al 2013 ecological data sets associated with natural disturbances are often limited in size due to the scarcity of the natural disturbance events e g wildfire disease or insect outbreak or the cost of data collection over large spatial and temporal scales kremens et al 2010 in matching availability of data can be identified based on the number of treated observations raw control observations and environmental covariates to estimate the propensity scores limited data on any of these may affect the reliability of the causal effect quantification stuart 2010 the number of treated observations is directly related to the variance of the estimated treatment effect brookhart et al 2006 and the number of raw control observations relative to the treated observations is associated with the bias in propensity scores heinrich et al 2010 the availability of covariates influences the exclusion of important confounding variables when estimating the propensity scores heckman et al 1997 given that matching based on limited data may impose more severe bias on the effect estimation than regression adjustments stuart 2010 it is critical to understand the performance of matching methods under different data availability scenarios and identify a marginal size of data and covariate set required to implement matching methods this study provides a sensitivity analysis of matching on data availability in terms of the balance achieved in propensity score distributions and of the estimated effect using a case study of wildfire effect estimation woo et al 2021 quantified the effect of wildfires on aboveground live woody forest carbon using dapsm and psm based on an empirical dataset of 23 150 forest inventory plot measurements from washington and oregon united states of america usa taking advantage of the large number of forest inventory plot measurements used in woo et al 2021 we performed a monte carlo simulation to generate subsets of the full dataset that contained 611 burned plots i e treated and 22 539 unburned plots i e raw control the propensity scores of the burned and unburned plots were estimated based on sixteen covariates that describe the different environment of plots with and without distance adjustment we examined three scenarios of data availability 1 number of burned plots 2 ratio of unburned plots relative to burned plots and 3 availability of environmental covariates for 3 we considered availability of the covariates based on data sources to define four groups topography climate land cover and remote sensing variables we first assessed how well the propensity score distributions were balanced between the burned and the control plots through matching we then observed how the estimated wildfire effect changed under the different scenarios the effects of wildfire were quantified as the difference in live woody carbon mass between the burned and the control plots we presented the balance achieved and effect estimates under dapsm and psm to assess the bias and variability expected within each matching method and data availability scenario by providing a sensitivity analysis of dapsm and psm based on real world data the results of this study suggest practical guidelines of data availability to be considered for wildfire effect quantification and by extension estimation of other natural disturbance e g arovaara et al 1984 and management effects e g butsic et al 2017b utilization of national forest inventory data will broaden the opportunity to conduct effect analyses over large spatial and temporal scales with spatially balanced samples of forest conditions the diagnostics of balance achievement by matching presented in this study are applicable for designing future quasi experiments where the spatial distances as well as the environmental covariates play an important role in distinguishing treated and raw control observations by implementing matching methods as quasi experimental approach causal effects can be estimated from observational data to inform ecological problems for which experimental settings are impractical 2 data 2 1 forest inventory data we used national forest inventory data of washington and oregon usa from the united states department of agriculture forest service forest inventory and analysis fia program smith 2002 since 2000 the fia program has collected field measurements of forested lands i e at least 10 percent potential cover by live trees by measuring permanent plots 4047 square meter in size that represent a spatially balanced sample of one plot for every 2400 ha bechtold and patterson 2005 a plot consists of four circular subplots and the measurements of trees and environmental attributes are made at both plot and subplot level ten percent of the plot locations are visited annually resulting on a 10 year remeasurement cycle bechtold and patterson 2005 between 2001 and 2016 there were a total of 23 150 inventory plot measurements in washington and oregon forests including 7994 remeasurements we classified the plot measurements into burned i e treated t and unburned i e raw control rc based on the monitoring trends in burn severity mtbs http www mtbs gov map burned plots were identified by spatially overlaying the plot locations on the fire maps every fia plot measurement that fell within the perimeter of a wildfire that occurred less than five years prior to the measurement was labeled as burned meanwhile plots that had not burned at least ten years prior to measurement were labeled as unburned here we refer to the burned measurements as burned plots and unburned measurements as unburned plots for simplicity thus unburned plots may have repeated measurements at the same location to remove dependencies among burned and unburned plots previous measurements of burned plots were excluded from the pool of raw control plots similarly if a plot burned twice the measurements after the first wildfire were also excluded to preclude the effect of reburn woo et al 2021 a total of 611 burned plots and 22 539 unburned plot measurements were used for the analysis more details on the compilation of the data are available in woo et al 2021 2 2 environmental covariates for each forest inventory plot we retrieved sixteen environmental covariates that are associated with the probability of wildfire occurrence these environmental covariates were grouped into four categories 1 topography topo 2 climate clim 3 land cover land and 4 remotely sensed variables remo table 1 additional details in woo et al 2021 topography and land cover variables of the forest inventory plots were measured in the field and obtained from the fia database topography covariates included elevation slope and aspect elevation was measured at the center of the plot and slope and aspect were measured at subplot level according to the land classification within the subplot e g forested non forested land we used the values of slope and aspect that accounted for the largest percentage of the forested land classification within the subplots land cover covariates including land ownership physiographic condition and proportion of forested area table 1 were measured at subplot level for the class variables we used two categories of land ownership public and private and three of physiographic class mesic xeric and hydric woo et al 2021 we assigned the land cover categories that accounted for the largest percentage within the four subplots climate covariates consisted of six prism derived variables related to precipitation temperature vapor pressure and moisture stress daly et al 2008 the climate covariates were derived from thirty year normal data 1981 2010 with 30 m resolution the remote sensing data included four spectral and vegetation indices including the normalized difference vegetation index ndvi and tasseled cap brightness greenness and wetness in the middle of growing season all derived from annual landsat satellite images with 30 m resolution unburned plots were associated with the remotely sensed variables in the year that the plot was measured and burned plots were associated with the variables measured in a year before the wildfire to account for stand characteristics that may be related to the risk of burning to assign the climate and remotely sensed covariates to each of the plot measurements the values of 9 pixels around the plot center location were averaged the climate and remotely sensed metrics were obtained from the landscape ecology modeling mapping and analysis lemma team https lemma forestry oregonstate edu 2 3 aboveground live woody carbon we obtained aboveground live woody carbon mass per plot mg ha 1 for both burned and unburned plots based on all live trees over 2 54 cm diameter at breast height dbh measured within the forest inventory plot by the fia protocol bechtold and patterson 2005 aboveground woody biomass including stem bark and live branches was estimated by the pacific northwest research station usda forest service woodall et al 2012 we converted the aboveground live woody biomass into carbon mass using a carbon ratio multiplier of 0 5 ipcc 2006 the obtained carbon mass was summarized as per ha values mg ha 1 for each plot and used as the outcome variable in propensity score matching to assess the effect of wildfires the estimated wildfire effect is the difference in the outcome variable between burned and unburned plots 3 matching methods 3 1 psm and dapsm for assessing wildfire effects on aboveground live woody carbon propensity scores in psm denote the probability of receiving the treatment conditional on the environmental covariates the probability assigned to each observation in this study the forest inventory plot is generally estimated using logistic regression distance adjusted propensity scores in dapsm consider spatial distances between burned and unburned plot locations in matching they are a weighted average between the propensity scores estimated from environmental covariates and the normalized euclidean distances between plot locations papadogeorgou et al 2019 because the spatial distance may take into account unobserved covariates that were not included in the covariate set to estimate the propensity scores dapsm should be preferred to psm or spatial matching sm that considers only the distances for matching woo et al 2021 the quantification of wildfire effects on aboveground live woody carbon using the matching methods followed three steps first we estimated propensity scores of burned and unburned plots using logistic regression with the sixteen environmental covariates then we estimated the distance adjusted propensity scores as a linear combination of the propensity scores and spatial distances between pairs of burned and unburned plots we used a weight of 0 5 to equally emphasize the observed environmental covariates and the spatial distance given an earlier study that found other weights i e 0 25 0 5 and 0 75 did not result in any difference in the effect estimation woo et al 2021 finally we matched the burned plots and unburned plots using psm and dapsm to obtain a set of control plots that share similar covariates and geographical locations we employed matching without replacement with a greedy algorithm austin 2011 that sequentially finds the smallest difference in the distance adjusted propensity scores between the burned and the unburned plots this algorithm is the most commonly used in propensity score matching implementation austin and cafri 2020 the matched unburned plots were identified as control plots the estimates of the wildfire effects were then quantified as the difference in aboveground live woody carbon mass between the burned and control plots 3 2 sensitivity analysis of dapsm under different data availability scenarios we ran monte carlo simulations to perform a sensitivity analysis on the balance of propensity score distributions as well as effect estimates when reducing the number of treated and raw control observations and covariate sets we created random subsets of forest inventory plot data from the full data consisting of 611 burned plots n t 611 and 22 539 unburned plots n r c 22 539 the full data was set as our reference population for the monte carlo simulations i e simulation population and the propensity scores estimated based on all available environmental covariates using the simulation population were considered to be the true propensity scores in our simulation because the number of possible combinations of all covariates is very large we grouped the environmental covariates sharing similar attributes and sources as stated in section 2 2 based on the idea that the attainability of variables is dependent on the accessibility to the data source for example topo including elevation slope and aspect are easily available for researchers through open sources such as digital elevation models dem or geographical information systems gis for most regions bungartz et al 2018 the order of accessibility to the four groups of covariates in our study was identified as topo clim remo land from the least to the most costly to obtain we considered four scenarios of environmental covariate sets for propensity score estimation 1 full set of covariates with all four groups topo clim remo land 2 topo clim remo 3 topo clim and 4 topo remo table 2 we examined three simulation scenarios of data availability number of burned plots n t number of unburned plots relative to burned plots n r c n t and environmental covariate set we varied numbers of burned plots n t and unburned plots relative to burned plots n r c n t for the random subsets of data from the simulation population table 2 under each of the four covariate set scenarios from each combination of subsets and covariate sets the plots were matched using different matching methods dapsm based on both the propensity scores and plot locations and psm based only on the propensity scores additionally we implemented sm by matching the plots based only on the distances among the plot locations the process was iterated 100 times for each combination of simulation values and conditions under different matching methods we observed the propensity score distributions and the effect estimates under the data availability scenarios we also computed mean distance to the control plots from burned plots under each matching method dapsm psm and sm 3 3 evaluation of results 3 3 1 descriptive statistics for evaluation of balance in propensity scores the quality of the matching was assessed by the balance in propensity scores between treated and control groups mccaffrey et al 2013 stuart 2010 here balance means that the distribution of propensity scores of the matched pairs were similar thus the matching reduced the potential selection bias between treated and raw control groups caused by the environmental covariates included in the model rubin 2001 suggested three statistics to assess balance 1 the difference in the means of the propensity scores between the treated and control groups 2 the ratio of the variances of the propensity scores and 3 the ratio of the variances of the residuals of the covariates after matching table 3 1 and 2 represent the first and second moment of the overall propensity score distribution respectively the ratio of the variances of the residuals of the covariates is obtained by regressing each of the covariates on the estimated propensity scores if the matching was successful and the control group is similar to the treated group the means of difference and the variance ratios of the propensity scores should be close to zero and one respectively ratio of variance between 0 8 and 1 2 is considered adequate while values below 0 5 or over 2 are too extreme rubin 2001 for the ratio of the variances of the residuals of the covariates we computed the percentage of covariates that are within each of the variance ratio ranges i e 0 8 and 1 2 0 5 or 2 for the 16 variables considered even if the model only included a subset of covariates in addition to the statistics suggested by rubin 2001 we compared the absolute standardized mean difference asmd in the sixteen covariates between the unmatched and matched burned and unburned plots to examine the balance at the individual covariate level nolte et al 2017 austin 2009 asmd was computed as the average difference standardized by the standard deviation of the variable for continuous covariates and using the average difference in proportions for categorical variables stuart 2010 asmd values less than 0 2 indicate adequate balance in the covariates while asmd values between 0 2 and 0 4 indicate moderate imbalance and asmd values over 0 4 indicate large imbalance cohen 2013 before matching the bias and variance ratio of the estimated propensity scores using the full population of burned and unburned forest inventory plots n t 611 and n r c n t 36 7 and the full covariate set 16 variables topo clim remo land was very large table 3 after dapsm the bias decreased by approximately 97 from 0 031 to 0 001 the variance ratio decreased from 3 11 to a value very close to 1 thus dapsm with all 16 covariates resulted in a control group similar to the burned plots in terms of propensity scores rubin 2001 regarding the ratio of the variance of the residuals of the covariates 50 of the covariates 8 out of 16 showed relatively large variance ratios e g 0 8 or 1 2 between treated and raw control meaning that there was imbalance between burned and unburned plots for those covariates table 3 when dapsm was applied the variance ratios ranged within 0 8 1 2 for all covariates 100 16 out of 16 table 3 these three statistics obtained from dapsm and the full data set were the baseline to compare bias and variance ratios from the subsets of data and covariate set using dapsm and psm we graphically presented the comparisons of the descriptive statistics along with the combinations of the three simulation scenarios for effective visualization pianosi et al 2016 3 3 2 performance of matching estimates the bias reduction and variability of matching estimates was examined using the distributions of the simulated wildfire effect estimates computed under varying sample sizes and covariate sets the wildfire effects were calculated as the difference in the aboveground live woody carbon mass between the burned and control plots we obtained the baseline estimate of wildfire effects on aboveground live woody carbon mass based on the simulation population to benchmark the effect estimation based on the simulation population the difference in the mean aboveground live woody carbon mass between the burned and the control plots was estimated as 30 95 mg ha 1 in absolute value the difference in the mean carbon mass between burned and raw control plots was 50 82 mg ha 1 so that the wildfire effect on aboveground live woody carbon mass was overestimated by 19 87 mg ha 1 on average due to the selection bias introduced by the different environmental conditions between plots that burn and those that do not we presented the simulated wildfire effect estimates from the subsets of data against the baseline estimate to assess the mean bias and the variability of the matching estimates also we computed relative bias of the simulated wildfire effect estimates compared to the simulation population under different data availability scenarios the simulated estimates and their relative bias were displayed as boxplots and matrix plots respectively 4 results 4 1 balance in propensity scores under different data availability scenarios 4 1 1 smaller mean bias in propensity scores under larger n t and covariate set with clim the three scenarios of data availability in matching i e n t n r c n t and covariate set jointly affected the mean bias in propensity scores under dapsm a notable change in the mean bias in propensity scores was found among different covariate sets while there was little deviation from the baseline bias under dapsm with the full covariate set including topo clim remo and land 0 003 fig 1 a dropping land variables fig 1b and clim fig 1c resulted in larger bias on average 0 005 0 007 and 0 007 0 019 respectively excluding remo from the covariate set also increased the bias but not much e g 0 002 on average fig s1 in supplementary materials the mean bias in propensity scores was relatively constant across n t with a minimal increase in bias as n r c n t decreased to 10 fig 1a and b when the covariate set excluded clim variables fig 1c however the increase of bias was remarkable compared to the covariate sets including clim especially when n r c n t was less than 10 in contrast to the covariate sets including clim fig 1a and b the bias increased with decreasing n t for the covariate set that excluded clim variables fig 1c comparing dapsm to psm shows that including distance in the matching process did not result in large differences in mean bias in propensity scores as long as clim variables were included in the covariate set fig 1a vs 1d and fig 1b vs 1e without clim variables psm produced much larger bias than dapsm regardless of n r c n t fig 1f without any environmental covariates included in the matching process i e sm with distance only fig 1e the mean bias showed a similar pattern to dapsm without clim fig 1c small increase in mean bias on average and a sharp increase in mean bias when n r c n t 10 4 1 2 smaller variance ratio in propensity scores under large n t and covariate set with clim the variance ratio in propensity scores between the burned and the control plots generally followed similar patterns as the mean bias in propensity scores matching based on covariate sets that excluded clim variables led to a large increase in the mean variance ratio with decreasing n t and n r c n t fig 2 when using the full covariate set the variance ratio fell within the range of 0 8 1 2 i e close to 1 for most combinations of n t and n r c n t except for n t less than 25 and n r c n t less than 5 fig 2a dropping the land variables resulted in variance ratios larger than 1 2 for the majority of combinations of n t and n r c n t but not in extreme values exceeding 2 fig 2b excluding clim variables from the covariate set introduced large variability in the variance ratio among different n t and n r c n t fig 2c small n t i e 75 and n r c n t i e 5 exhibited extreme variance ratios over 2 indicating imbalance in the distributions of propensity scores between burned and control plots psm produced slightly smaller variance ratios than dapsm did when clim was included in the covariate set fig 2a vs 2d fig 2b vs 2e psm without clim however did not reduce the variance in propensity scores between burned and unburned plots red lines in fig 2f once distance was included in the matching process however the variance ratio substantially dropped especially for large n t 100 and n r c n t 15 values even if no other covariates were included fig 2g 4 1 3 smaller variance ratio of the residuals of the covariates under dapsm comparing raw control to controls obtained through dapsm and psm the improvement in the variance ratio of residuals of individual covariates through matching was obvious in all covariate sets showing increase in the percentages of variance ratio between 0 8 and 1 2 fig 3 a d dapsm exhibited larger percentages of variance ratio between 0 8 and 1 2 than psm did indicating the method was more effective achieving balance in terms of individual covariates sm based on distance only was also more balance in individual covariates than psm especially when n r c n t 10 fig 3d as n t decreased increasingly extreme variance ratios of the residuals were observed for the matched plots regardless of n r c n t fig 3a d n t 25 barely reduced the variance ratio for any n r c n t through matching when climate variables clim were included in the covariate set the distribution of variance ratios obtained through both dapsm and psm did not differ much among the covariate sets fig 3a c topo and remo variables however showed little initial imbalance in the variance ratio of residuals between burned and raw control plots fig 3d compared to other covariate sets in that case psm introduced more severe imbalance in variance ratio than the raw controls fig 3d 4 1 4 dapsm balances some covariates that were not included in propensity score model the average asmd computed for each covariate showed major differences between dapsm and psm under the same covariate set table 4 fig s4 comparing the raw controls of unburned plots to the burned plots i e unmatched 11 out of 16 covariates were imbalanced i e asmd 0 2 matching based on the full covariate set achieved balance in all covariates under both dapsm and psm when the covariate sets were reduced psm resulted in imbalance in the omitted covariates e g physcl temp tempdf svpd and gsms under psm with topo remo table 4 on the other hand dapsm accomplished balance in climate variables even though the clim covariates were excluded in the propensity score estimation sm generally achieved balance in most of the covariates except for slope and physcl despite the withdrawal of all covariates in the matching process 4 2 performance of wildfire effect estimates under different data availability scenarios again the three data availability scenarios n t n r c n t and covariate set affected the bias and variance of wildfire effect estimates collectively under dapsm the number of burned plots in the simulation subsets n t was related to the bias of the estimates of average wildfire effect whereas the ratio of unburned to burned plots n r c n t influenced the variability of the estimates fig 4 dropping clim from the covariate set increased the bias and variability under small values of n t and n r c n t fig 4c when n t 500 the simulated mean effect estimate obtained from dapsm was 30 84 mg ha 1 similar to the baseline estimate 30 95 mg ha 1 the estimates of wildfire effect across all n t and n r c n t values under dapsm deviated on average by 25 30 from the baseline estimate when the covariate set included clim variables fig 4a and b whereas the average deviation was 43 for the covariate sets without clim variables fig 4c n t less than 100 tended to show large variability in the wildfire effect estimates resulting in positive effect estimates for a few subsets the bias of the effect estimates tended to increase with decreases in n t and n r c n t under dapsm fig 4a c while the bias of psm derived estimates was mostly dependent on covariate sets only not on n t or n r c n t fig 4d f the matrix plots fig 5 suggested that the bias was susceptible to small n r c n t than small n t when clim was included in the covariate set controls from dapsm with n t 100 and n r c n t 20 were found to show 20 bias compared to the simulation population fig 5a and c for small numbers of n t and n r c n t e g n t 100 and n r c n t 10 psm exhibited smaller bias than dapsm once clim was included in the covariate set fig 5 4 3 distance between matched pairs under different matching methods the mean distance between the burned plots and the matched control plots under psm ranged between 340 km and 360 km regardless of the number of burned and unburned plots it did not change substantially regardless of n t and n r c n t the mean distance under psm was similar to the mean distance of all pairwise burned plots and raw control plots in the full data 363 1 km the maximum pairwise distance between burned and raw control plots in the full data was 945 8 km under dapsm the mean distance varied from approximately 7 km 80 km increasing with decreasing number of observations fig 6 the mean distance changed little among the covariate sets under the same matching method sm showed the smallest mean distance ranging from 4 4 to 65 km fig 6 5 discussion 5 1 covariates and distance affect the balance achievement in ps distributions when matching on the propensity scores alone psm the covariates included in the models were the key driver determining the balance of the propensity score distributions between treated and controls neither the sample size nor the ratio of the number of treated plots to controls significantly affected the performance of matching clim variables were the key covariates to estimate the propensity scores for matching so that covariate sets without clim failed to fully reduce the selection bias between the burned and the unburned plots our identification of clim as the key covariates was expected as climate is one of the most critical factors determining wildfire occurrence mckenzie et al 2004 weisberg and swanson 2003 as well as carbon mass in forests stegen et al 2011 at a regional scale the results imply that to achieve balance in propensity scores it is more effective to identify and include key covariates than to increase the sample size once distance was included in the matching process dapsm the importance of clim variables subsided except when the sample size was very small large bias and variance ratio in propensity scores under psm without clim compared to dapsm suggests that the spatial distance partially incorporated the effect of clim variables in estimating the probability of wildfire the results of average asmd also specifically confirm that the distance measure contributed to balancing the environmental covariates that were not included in the propensity score estimation model while psm obtained balance only on the covariates that were included in the model closer plots have higher chance to share similar climate conditions di cecco and gouhier 2018 thus the spatial proximity may have compensated for the exclusion of clim variables even sm based solely on spatial distance outperformed psm without clim variables the results emphasize the importance of incorporating spatial measures to account for both observed and unobserved confounding factors woo et al 2021 papadogeorgou et al 2019 however incorporating spatial proximity was no longer effective as the observations became sparser over the study area the bias and variance ratio in propensity scores were remarkably large under sample sizes n t around 150 or less and n r c n t 10 this finding indicates that the benefit of spatial distance operates at a particular scale where the geographic variation lines up with the heterogeneity of important covariates the data of national forest inventory plot measurements utilized in our study were distributed systematically over the states of the pacific northwest at an approximate distance of 5 km between plots bechtold and patterson 2005 hence the spatial distance among the plots captured relatively large scale variation of climate rather than local geography such as aspect and slope in our data with the full sample the average distance between matched pairs under dapsm was about 7 km when the distance between the pairs of treated and control units became extreme due to the small sample size over the region spatial distance did not account for the variation of climate when the sample size decreased below n t 150 and n r c n t 10 the mean distance was approximately 20 km thus the ability of spatial distance to account for the effect of covariates may be linked with the patterns of landscape variables depending on the spatial resolution wang et al 2014 turner et al 1989 the three data availability scenarios the number of treated number of raw control relative to treated and covariate set interplay with each other to achieve the balance in ps distribution between the treated and the controls as the performance of the models estimating propensity scores degrades due to the omission of key covariates spatial information will compensate the lack of data unless the observations are too sparse likewise when less observations are available and the geographic locations do not provide additional information covariates will become relatively more important this is especially true when the heterogeneity of the key covariates does not align with the spatial scale of the data 5 2 sample size suggestion based on the effect estimates using dapsm and forest inventory data the effect of spatial distance in the performance of the matching was also evident from the estimated effect of wildfire defined as the difference in live carbon between burned and control plots dapsm was superior to psm showing less bias most likely because the spatial distance accounted for additional information from unobserved covariates these results agree with a recent clinical study that shows that spatial propensity score matching outperformed non spatial matching davis et al 2021 however our study further showed that including the distance measure was less effective or counterproductive when the sample size was very small and the distance between possible matches too large then the spatial distance provided no additional information resulting in an even larger bias in the estimates while the critical sample size may be scale dependent based on the results of simulated estimates we suggest a sample size of at least n t 100 and n r c n t 10 as the threshold to apply dapsm over washington and oregon this threshold of number of treated and raw control observations can serve as a practical guideline when implementing dapsm for further research using forest inventory at regional scales further research may not be limited to wildfire effects on carbon mass but on various outcomes of interest that are closely related with climate variables such as vegetation structure duguy et al 2013 biodiversity gill et al 2013 or land use duguy et al 2012 the bias in the estimates of wildfire effects observed from our study was generally larger than the results from pirracchio et al 2012 who obtained less than 10 of relative bias when decreasing the sample size from 1000 to 40 the relative bias from pirracchio et al 2012 was obtained from monte carlo simulations using an artificial dataset for which the data generation process was known it is common to use an artificial dataset for sensitivity analyses of psm e g coffman et al 2020 rubin and thomas 1996 based on the true propensity score model or true estimates our simulation on the other hand was conducted with empirical data of forest inventory measurements that involves additional uncertainty from sampling and measurement errors rudolph and stuart 2018 bennett et al 2013 the treatment in our study was a binary factor of either presence or absence of wildfire because the effect of different severities of wildfires was beyond the research interests of this study large variability across fire severities in burned plots may have increased the bias of our estimated wildfire effects nevertheless our study reflects actual relationships among the environmental variables under the presence of wildfires contrary to artificial data lechner and wunsch 2013 further application of various extended matching methods including propensity score estimation using generalized boosted models mccaffrey et al 2013 will help to assess multiple treatment effects from observational data 5 3 future work using forest inventory data dapsm can be utilized to examine any post disturbance effects based on the current forest inventory data as long as the number of disturbed plot measurements is sufficient i e 100 if the number of treated plots for a given disturbance is less than the desired threshold over a spatial region one could possibly increase the spatial and temporal intensity of the inventory plot measurements in the database such temporal or spatial intensifications are already effective as supplemental studies under the fia program the fire effects and recovery study fers used to assess fire effects and post fire carbon dynamics e g eskelson et al 2016 and the sudden oak death syndrome assessment jain and fried 2010 extension of matching methods such as matching with replacement e g nolte et al 2013 can also resolve the small sample size problem austin and cafri 2020 while maintaining high quality of matching abadie and imbens 2006 taking advantage of the well defined population and unified protocol of measurements the national forest inventory dataset combined with a variety of quasi experimental methods will provide opportunities of continuous causal effect analysis for ecological data 6 conclusion this study presented a sensitivity analysis of distance adjusted propensity score matching dapsm in relation to data availability we examined three data availability scenarios in matching number of available burned plots number of unburned plots relative to burned plots and environmental covariate set to estimate the propensity scores for the quantification of wildfire effects using national forest inventory plot measurements in washington and oregon we conducted monte carlo simulations of random subsets of burned and unburned plots to quantify the wildfire effects under different covariate sets for each simulation performance of dapsm was evaluated as the bias and variance ratio of propensity scores as well as the bias and variability of the estimates of wildfire effect we studied the influence of distance measure in matching by comparing the performance of dapsm to that of matching without distance adjustment we found that the three data availability scenarios interacted to affect the performance of matching among the three scenarios whether the covariate set included climate variables or not dominated the degree of balance in propensity score distributions between burned and control plots climate variables turned out to be the key covariates to analyze the wildfire effect on forest carbon mass the sample size especially the number of unburned plots relative to the burned plots was closely related with the distance between the matched pairs large distance among the plots due to sparseness of observations over the region resulted in severe bias and variability in the effect estimates because the key covariates were not accounted for by the distance measure based on our results the importance of key covariates and the inclusion of the spatial measure were emphasized for applying propensity score matching methods to account for both observed and unobserved confounding variables we also suggest a marginal sample size with at least a hundred treated observations and a tenfold larger number of raw controls to implement dapsm over washington and oregon to achieve balance in propensity score distributions and low bias in effect estimates future studies implementing matching approaches for effect analyses using forest inventory plot data will find this research useful to determine the sample size and covariate set in relation with spatial distances among the observations national forest inventory data facilitate monitoring and the assessment of comprehensive forest resources over large spatial and temporal scales utilization of dapsm with forest inventory data enables rigorous research on effect analysis not limited to wildfires but applicable to other disturbances and ecological processes where a randomized experiment is infeasible declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported partly by 4 year doctoral fellowship from the faculty of forestry provided by the university of british columbia canada we thank matthew gregory from the landscape ecology modeling mapping and analysis lemma team for his assistance with the prism and landsat data retrieval and compilation we also thank the many individuals involved in the design field data collection quality assurance and processing of the u s forest service forest inventory and analysis program appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105163 
25767,entropy measures are standard tools in environmental and ecological sciences to describe the heterogeneity of data this paper reviews a selection of spatial entropy indices some of which are very recent suitable to deal with spatial data on variables presenting a finite number of categories a special focus is given on biodiversity data but methods can be applied to any other environmental phenomena the new r package spatentropy is here introduced to compute spatial entropy measures in practice the extension from traditional entropy measures to their spatial version is a unique feature of the package which is able to work with both areal and point data a practical part is also presented where two types of environmental data are considered regarding trees biodiversity and urban expansion respectively the package spatentropy is run over these dataset and shown to represent a user friendly and helpful tool for new package users keywords spatial entropy shannon s entropy biodiversity tree species urban expansion r package software and data availability name of package software spatentropy package of r software title of package spatial entropy measures date first available 2018 02 28 version 0 1 0 date updated version 2021 04 07 version 2 0 1 developers linda altieri daniela cocchi giulia roli maintainer linda altieri contact linda altieri unibo it required software r 3 5 0 a free software environment for statistical computing and graphics available at https cran r project org required package spatstat 2 0 0 available at https cran r project org package spatstat automatically imported packages spatstat geom spatstat core spatstat linnet package availability the spatentropy package is free and downloadable from the cran repository at https cran r project org package spatentropy package reference manual available for version 2 0 1 at https cran r project org web packages spatentropy spatentropy pdf package description the heterogeneity of spatial data presenting a finite number of categories can be measured via computation of spatial entropy functions are available for the computation of the main entropy and spatial entropy measures in the literature they include the traditional version of shannon s entropy shannon 1948 batty s spatial entropy batty 1974 batty s lisa entropy o neill et al 1988 li and reynolds contagion index li and reynolds 1993 karlstrom and ceccato s entropy karlström and ceccato 2002 leibovici s entropy leibovici 2009 parresol and edwards entropy parresol and edwards 2014 and altieri s entropy altieri et al 2018a full references for all measures can be found under the topic spatentropy the package is able to work with lattice and point data the updated version works with the updated spatstat package 2 0 0 it also provides a more intuitive framework for all functions including improved examples and new data the speed of most functions has also been substantially increased data availability the spatentropy package includes two datasets the first one raintrees is a marked point pattern dataset about four rainforest tree species available at http www ctfs si edu it is a ppp object see package spatstat with 7251 points containing window an object of type owin see package spatstat i e the 1000 500 m observation area x y the numeric vectors with points coordinates marks a character vector matching the tree species to the data points this dataset documents the presence of tree species over barro colorado island panama barro colorado island has been the focus of intensive research on lowland tropical rainforest since 1923 http www ctfs si edu research identified several tree species over a rectangular observation window of size 1000 500 m the tree species constitute the point data categorical mark this dataset presents 4 species with different spatial configurations acalypha diversifolia chamguava schippii inga pezizifera and rinorea sylvatica the second dataset included in the package turin is a lattice dataset with turin urban morphological zones values are either 0 non urban or 1 urban pixels outside the administrative borders are classified as na this raster pixel lattice dataset comes from the eu corine land cover project eea 2011 and is dated 2011 it is the result of classifying the original land cover data into urbanized and nonurbanised zones known as urban morphological zones umz umz data are useful to identify shapes and patterns of urban areas and thus to detect what is known as urban sprawl turin s metropolitan area is extracted from the european dataset and is composed by the municipality of turin and the surrounding municipalities beinasco venaria reale san mauro torinese grugliasco borgaro torinese collegno pecetto torinese pino torinese moncalieri nichelino settimo torinese baldissero torinese rivoli orbassano the dataset is made of 111 113 pixels of size 250 250 m many other data examples can be found in the just mentioned package spatstat as well as in other r packages devoted to biodiversity and environmental studies e g vegan 1 introduction 1 1 entropy as a biodiversity and environmental measure measuring biodiversity with appropriate syntheses is a challenging task see e g hoskins et al 2020 and drechsler 2020 the proposals of indicators range from the simplest indices denoting species richness see e g fisher et al 2012 towards more articulated proposals related to the abundance of species in ecological communities see e g scarnati et al 2009 under this perspective entropy indices certainly represent an important and efficient tool widely used for properly measuring biological diversity leinster and cobbold 2012 shannon s entropy shannon 1948 is the most common and successful formulation as it is able to synthesize several concepts in a single number entropy information heterogeneity surprise contagion the flexibility of such index and its ability to describe any kind of data including categorical variables motivate its diffusion across several applied fields for data description and interpretation frosini 2004 over the last decades the concept of entropy has been further developed and generalized in engineering and statistics cover and thomas 2006 also extending to the case of continuous variables firstly dealt by rényi 1961 and batty 1974 1976 2010 2014 in the wide context of environmental studies such as geography ecology biology and landscape analyses a large use of entropy measures has been made in such cases the researcher often deals with spatial data i e data that are georeferenced as points or areas and as a consequence also entropy measures need to be revised to include this spatial information at this regard a number of works are available in the literature aiming at building a spatial entropy index they can be ascribed to three main approaches the first starts with batty 1974 1976 2010 2014 who defined a spatial entropy measure which evaluates the distribution of an event over an area allowing for the unequal space partition into sub areas later karlström and ceccato 2002 modified the initial proposal in order to satisfy the property of additivity in terms of decomposition of the global index into local components firstly introduced by theil 1972 and then defined by local indices of spatial association lisa criteria anselin 1995 the second approach to spatial entropy includes space based on a suitable transformation of the study variable to account for the distance between realizations co occurrences the first proposal was made by o neill et al 1988 for contiguous couples of realizations then extended by leibovici 2009 and leibovici et al 2014 to further distances and general degrees of co occurrences contagion indices li and reynolds 1993 parresol and edwards 2014 are also based on this view spatial contagion is the opposite of entropy as for the third approach a set of spatial entropy measures has been presented by altieri et al 2018a 2019a b starting from the co occurrence approach but overcoming some undesirable features of the previous measures according to this framework shannon s entropy of the transformed variable is decomposed into the information due to space and the remaining one once space is considered the proposal solves the problem of preserving additivity and disaggregating results allowing for partial and global syntheses 1 2 available software packages for entropy the r software r core team 2017 is certainly one of the most flexible options for performing statistical analysis in particular when spatial data have to be managed r is employed by a wide part of the global community of statisticians and has the great advantage of being open source i e it can be downloaded freely and anyone can contribute to the software by submitting packages of functions for additional features packages undergo severe checks by the r team which ensure the quality of the available material the software main drawback is that its traditional interface is not very user friendly this can be solved by installing an interface such as rstudio https www rstudio com as far as entropy measures are concerned several r packages on cran https cran r project org are available the most common ones are entropart entropy and entropyestimation they all allow traditional entropy computation and decomposition into its two terms known as mutual information and conditional entropy generally referred to in information theory applications cover and thomas 2006 in particular entropart provides functions to calculate the alpha beta and gamma diversity indices of communities typical of ecological studies together with simpson s evenness index the package also offers an example dataset about tree species but its data format is very specific of the package and not easy to understand for r beginners the further package entropy is more focused on various estimators of entropy a topic which does not constitute the core of the present work nor of many biodiversity studies in particular it allows to compute the hausser and strimmer shrinkage estimator the maximum likelihood estimator and its miller madow correction various bayesian estimators including the nsb estimator and the chao shen estimator it also provides functions for estimating the kullback leibler divergence and for the partition of entropy between mutual information and residual entropy its functions start from the observed counts of species without any considerations about the spatial location of data the package does not offer any data example lastly entropyestimation partially overlaps with both the aforementioned packages as it includes functions for the estimation of shannon s entropy variants of renyi s entropy mutual information kullback leibler divergence and generalized simpson s indices no data example is available further r packages though containing the word entropy in the description all deal with different data contexts or different fields of study e g sampling entropy see altieri and cocchi 2021a despite a wide literature on spatial entropy measures as illustrated in section 1 1 to our knowledge there is no r package offering an automatic computation of these indices given the potential and the crucial role of spatial entropy measures in real applications this lack needs to be overcome by providing a practical and intuitive support for new users in managing spatial information in form of point or areal data and obtaining fast and interpretable results in the contexts of biodiversity and environmental studies 1 3 spatentropy a tool for easy computation of spatial entropy measures this work aims at introducing the new r package spatentropy an improved version of the original altieri et al 2018b which collects functions for the computation of all the spatial entropy indices mentioned in section 1 1 they include the traditional version of shannon s entropy and shannon s entropy of a transformation of the study variable x known in the literature as z which considers pairs of observations at a given distance over space moreover if offers computation of batty s spatial entropy and its modified lisa version following the observation pairs approach the package provides functions for computing o neill s entropy for adjacent couples and its transformations li and reynolds relative contagion index and parresol and edwards entropy in addition the generalized version of leibovici s entropy can be computed together with the further generalization to a set of spatial entropy indices that we call altieri s entropies the package is able to work with both grid and point data and makes extensive use of the widely known spatstat package version 2 0 0 functions and data structures baddeley et al 2015 spatentropy allows usage by non statisticians provided they have basic knowledge of r the minimum effort is requested from the user and all functions are sided by data examples additionally real datasets are provided for performing as examples the studies introduced in the present paper many other data examples can be found in the just mentioned package spatstat as well as in other r packages devoted to biodiversity and environmental studies e g vegan the present paper illustrates how to make use of spatentropy focusing on a main example concerning ecological data and witnessing the need to assess biodiversity via an entropy index that includes information about the data spatial structure this example consisting of a marked point pattern dataset about four rainforest tree species is available in the spatentropy package under the name raintrees this dataset documents the presence of tropical tree species over barro colorado island panama barro colorado island has been the focus of intensive research on lowland tropical rainforest since 1923 http www ctfs si edu research identified several tree species over a rectangular observation window of size 1000 500 m the tree species constitute the point data categorical mark i e a label attached to each point of the pattern the raintrees dataset presents 4 species with different spatial configurations acalypha diversifolia acaldi chamguava schippii cha2sc inga pezizifera ingape and rinorea sylvatica rinosy the overall dataset has a total number of 7251 points and is shown in fig 1 one further example which is synthetically presented along the paper highlights both the ability of the measures proposed in this paper to deal with different types of spatial data and the wide potential of using properly calibrated entropy measures the package spatentropy offers the binary areal dataset turin which comes from the eu corine land cover project eea 2011 turin s metropolitan area is composed by the main municipality of turin and its 15 surrounding municipalities that constitute its commuting belt the dataset is a rectangular matrix of 111 113 observations each corresponding to a pixel of size 250 250 m pixels within the area administrative borders are classified as 1 urban or 0 non urban while pixels outside the border are missing values the dataset is shown in fig 2 where a dark pixel is urban and a light gray pixel is non urban in the package the dataset is accompanied by the objects turinw i e the enclosing rectangular window and turintess which contains the names and administrative borders of all municipalities the paper develops as follows in section 2 the principles for introducing space in traditional entropy measures are presented to review the leading spatial entropy indices available in the literature the application of the new r package spatentropy on the main running example of trees biodiversity is carried on in section 3 section 4 uses spatentropy and illustrates the results on the second example i e areal data of urban sprawl finally section 5 concludes the work 2 from traditional to spatial entropy measures the traditional concept of entropy dates back to shannon s formula shannon 1948 which in information theory cover and thomas 2006 quantifies the average amount of information brought by a discrete random variable x taking values x i in a set of i outcomes according to the probability mass function pmf p x in biodiversity applications x represents the variety of life considered i e the levels of biodiversity commonly discussed genetic species and ecosystem diversity as an example in the rainforest tree data introduced above x classifies the trees into the i 4 different species x 1 to x 4 i e acalypha diversifolia chamguava schippii inga pezizifera and rinorea sylvatica shannon s entropy of x is then defined as 1 h x i 1 i p x i log 1 p x i and represents the expected value of the so called information function i p x i log 1 p x i entropy measures the information or in other words the surprise coming from observing realizations intuitively outcomes with a very low probability of occurrence i e hardly observable species increase the entropy value while outcomes very likely to occur i e more observable species give a small contribution to entropy thus entropy or analogously information and surprise will be larger when the observed outcomes are not likely to occur entropy ranges in 0 log i and its maximum value is achieved when x is uniformly distributed in many situations entropy is seen as a descriptive measure and the pmf of x is built using the relative frequencies of the observed categories when the goal is to make inference on the underlying process determining the observed outcomes entropy can also be seen as an estimator h x in its standard use the pmf is estimated by the so called plug in estimator paninski 2003 in such case the descriptive measure and the estimator coincide p x i n i n substitutes the probabilities p x i with the observed relative frequencies over n realizations in particular referring to the rainforest tree example the frequencies are the ratios of the count of each species over the total number of observed trees in the pllied part of the paper all entropy measures are computed by substituting the elements of the unknown probability distribution with the observed relative frequencies a major drawback of shannon s entropy is that it does not account for the spatial location of occurrences so that datasets with identical pmf but very different spatial configurations share the same entropy value in biodiversity this point is crucial as the location of the variety of life considered is available often as point coordinates as well as strongly related to the research questions the main aim is to quantify the spread of biodiversity over a certain space indeed in the rainforest tree data example the interest lies in measuring the diversity across species but also in quantifying the spread of species according to the spatial location of the trees the following sections present the mainstream approaches to build spatial entropy measures most indices imply the formal definition of a neighbourhood cressie 1993 the simplest way of representing a neighbourhood system over n spatial units is via an adjacency matrix a anselin 1995 i e a square matrix whose elements indicate whether pairs of units are neighbours a uu 1 if u n u that is the neighbourhood of area u a uu 0 otherwise and a uu 0 by definition spatial units may be points defined via precise coordinate pairs or alternatively areas identified via common representative coordinate pairs such as the area centroids in biodiversity the concept of spatial neighbourhood generalizes to an idea of similarity so that some species are closer e g in a biological taxonomic or logical sense than others leinster and cobbold 2012 once the adjacency matrix is suitably specified computations proceed the same way for any similarity system 2 1 batty s entropy batty s spatial entropy batty 1974 1976 2010 2014 is useful for evaluating the heterogeneity in the distribution of a phenomenon over an area it is particularly appropriate when the observation area is exogenously partitioned into sub areas such as municipality administrative boundaries for a region the use of a meaningful exogenous area partition is very important since conclusions are heavily affected by the partition itself let a phenomenon of interest f occur over an observation window of size t partitioned into g areas of size t g this defines g dummy variables identifying the occurrence of f over a generic area g with g 1 g note that the phenomenon is denoted by the symbol f a presence absence variable different from x a generic categorical variable examples of f are the presence of a single species in biodiversity studies like in the case of point data on rainforest trees or the observation of urban versus non urban patches in city expansion studies like in areal data on binary turin s dataset given that f occurs over the whole window its occurrence in area g takes place with probability p g where 1 p g g g p g and g p g 1 the phenomenon intensity is obtained as λ g p g t g where t g is the area size and is assumed constant within each area batty s spatial entropy is then defined with respect to these quantities as 2 h b f g 1 g p g log t g p g it expresses the average amount of information brought by the occurrence of f in any area in the observation window accounting for unequal space partition through the multiplicative component t g note that p g p x i as the latter is the probability of occurrence of a category i e a species anywhere over the window while p g expresses the probability of occurrence of the phenomenon under study over a sub area analogously to shannon s entropy which is high when the i categories of x are equally represented over a non spatial data collection batty s entropy is high when f is equally intense over the g areas partitioning the observation window i e when λ g λ for all g batty s entropy h b f reaches a minimum value equal to log t g when p g 1 and p g 0 for all g g with g denoting the area with the smallest size the maximum value of batty s entropy is log t reached when the intensity of f is the same over all areas i e λ g 1 t for all g in the case of point data batty s spatial entropy aggregates information over each sub area into a single number p g thus neglecting the detailed information about the points coordinates this represents an unpleasant pitfall for the rainforest tree data batty s entropy may be computed if the aim lies on measuring the entropy observed across sub areas of interests controlling for their dimensions in such case entropy is computed either on the overall dataset discarding the information about the different kinds of species see e g fig 3 where g 10 or by considering each category of species separately in the latter case the starting variable x covering i 4 species is transformed into a dummy variable indicating the presence absence of one species of interest over the sub areas if the interest lies in the entropy levels of each species over the sub areas 4 dummy variables are needed and 4 batty s entropies are obtained and compared without the possibility to have a unique measure 2 2 batty s lisa entropy karlström and ceccato 2002 made a challenging proposal to exploit the contribution of neighbourhood in batty s entropy index following the local indices of spatial association lisa theory anselin 1995 they consider an adjacency matrix for each of the g sub areas introduced in section 2 1 i e a a g g g g 1 g the elements on the diagonal of the adjacency matrix a are a gg 1 i e each area neighbours itself then the elements a gg of this matrix are included to weight the probability of occurrence of f in a given spatial unit g 3 p g g 1 g a g g p g mimicking batty s entropy batty s lisa entropy index is then defined as 4 h lisa f g 1 g p g log 1 p g i e it fixes t g 1 and defines the information function as i p g log 1 p g the main purpose of batty s lisa index lies again on measuring and comparing entropies at different levels of biodiversity e g separately by species across the sub areas of interests but instead of controlling for their dimensions a sub area neighbourhood is included in the computation where again neighbourhood can be extended to a more general meaning than purely spatial adjacency this approach further allows to obtain local entropies 5 l g p g log 1 p g which represent the specific contribution of each sub area and preserve the additive properties of lisa theory in deriving the global index h lisa f the maximum of h lisa f does not depend on the choice of the neighbourhood and is log g as a tends to the identity matrix h lisa f tends to batty s spatial entropy 2 with equality in the case of t g 1 for all g 2 3 o neill s entropy and contagion indices another way to build a spatial entropy measure relies on defining a new categorical variable z where each realization identifies ordered couples x i x j of occurrences of x over space order preservation within couples regards considering the relative spatial location of the observations conventionally if order is preserved the couple x i x j implies that the observation carrying the j th category occurs at the right or below the observation carrying the i th category under this criterion such couple is different from x j x i for i categories of x the new variable z has r i 2 categories the attention moves from the computation of 1 namely h x to an index of the same form i e shannon s entropy of z h z based on the pmf p z in biodiversity entropy measures based on z are useful when the variable of interest has two or more categories e g species and when the goal is to understand how the presence of a species at one location affects neighbouring outcomes i e the adjacent presence of the same or a different species intuitively when the variable is strongly spatially associated neighbouring outcomes are closely related and the surprise and thus the entropy in observing data decreases defining the type of neighbourhood to consider is crucial so that different proposals based on this choice are available in the literature o neill et al 1988 proposed an early spatial entropy index by defining a neighbourhood based on contiguity i e areal units sharing a border shannon s entropy 1 is computed in this case for the subset of values of the variable z made of contiguous couples such couples are identified by non zero elements in a suitable adjacency matrix the contiguity matrix c the subset of couples of contiguous realizations is denoted by z c with pmf p z c then the associated shannon s entropy is 6 h o z c r 1 i 2 p z r c log 1 p z r c this measure ranges from 0 to log i 2 and quantifies the residual amount of entropy associated to the variable of interest once the influence of the spatial configuration in terms of contiguity has been taken into account when it is close to 0 the heterogeneity of data mostly depends on its spatial structure and the residual entropy is low when it is close to its maximum data do not present a strong positive or negative spatial correlation and are more similar to a randomly scattered scheme a direct derivation of this approach is based on the concept of contagion which is the opposite of entropy indeed the relative contagion index rc li and reynolds 1993 is 7 h r c z c 1 h o norm z c 1 1 log i 2 r 1 i 2 p z r c log 1 p z r c where the second term is the normalized entropy of z c obtained via the multiplication of 6 by 1 log i 2 its complement to 1 measures relative contagion the higher the spatial contagion between categories of z c the lower the spatial entropy normalization has the advantage of allowing comparison across datasets presenting a different number of categories giusto non andare a capo if one wants to account for the number of categories of x when computing the contagion index non normalized measures should be computed in order to distinguish among contexts with different numbers of categories for this reason parresol and edwards 2014 suggest an unnormalized version of 7 8 h p z c h o z c r 1 i 2 p z r c log p z r c thus ranging from log i 2 to 0 both the relative contagion and parresol edward s index have an opposite interpretation than o neill s entropy increasing departures from 0 denote a stronger spatial structure in the data which affects the heterogeneity o neill s entropy and its modified versions are conceived for areal data as they require information on sharing a border a practical example on entropy measures based on contiguity relies on turin s urban data in the case of biodiversity point data such as the tree species dataset this approach implies a pixelization of the observations which is a sensible choice as it arbitrarily assignes a spatial dimension and a block structure to points with great loss of information point data may be analyzed using the proposal of distance rather than contiguity between observations in such a situation more suitable measures can be actually applied such as the ones presented in the next sections 2 4 leibovici s entropy following the idea of o neill et al 1988 leibovici 2009 and leibovici et al 2014 propose a richer measure of entropy by extending h o z c in two directions firstly z can now represent not only couples but also triples and further degrees m of co occurrences the authors develop the case of ordered co occurrences so that the number of categories of z is r m i m secondly space is now allowed to be continuous so that areal as well as point data may be considered and associations may not coincide with contiguity the concept of distance between occurrences generalizes the concept of contiguity once a distance d of interest is fixed then co occurrences are defined for each m and d as the m th degree simultaneous realizations of x at any distance d d i e distances are considered according to a cumulative perspective to this aim an adjacency hypercube a d is built and a subset of all co occurrences z is selected conditional on it i e z a d or for the sake of simplicity z d then leibovici s spatial entropy is again a version of shannon s entropy defined on the pmf p z d 9 h l z d r 1 i m p z r d log 1 p z r d the derivation of o neill s entropy 6 is straightforward as it is the special case when m 2 and d is defined for contiguous co occurrences in applications only co occurrences of degree m 2 i e couples of realizations are usually considered therefore unless otherwise specified from now on we refer to the case of couples m 2 leibovici s approach finally allows to include the coordinates information for point data in the computation of the entropy if we again consider the example about the spatial pattern of the species 16 couples can be observed acalypha acalypha acalypha chamguava acalypha inga acalypha rinorea chamguava acalypha and so on once a distance d has been fixed only couples of species x i x j located at a distance lower than d are isolated and enter the computation of h l z d clearly the choice of d affects this measure of entropy thus in real applications different scenarios for d need to be explored and compared overcoming this feature is among the reasons why a further generalization of this approach has been proposed by altieri et al 2018a and described in the following section 2 5 altieri s set of entropy measures altieri et al 2018a 2019a 2019b developed a more general approach defining a set of spatial entropy measures starting from o neill and leibovici s indices such set should be employed when the interest lies in understanding the role of the spatial configuration in determining the entropy of a biodiversity variable not only at one isolated specific distance but also at a global level or at several distance ranges simultaneously in addition it should be used when the influence of space needs to be quantified as a percentage of the entropy e g in the rainforest tree example how much the global entropy across species is due to the spatial location of the trees from a statistical perspective this approach constitutes a more sophisticated tool that allows more flexibility and interpretability than other available measures of entropy the important starting point is a different way of computing the variable z with respect to previuos definitions the general degree m of co occurrences is defined discarding the order within co occurrences for the sake of simplicity let s consider only the case of m 2 where under this approach pairs not couples of realizations are considered i e the relative spatial location of the two realizations is irrelevant the reason for this choice is two fold firstly ordering occurrences is not sensible in spatial statistics where spatial configurations are not generally assumed to have a direction secondly discarding the order ensures a one to one correspondence between shannon s entropy of x and z note that when order is discarded the number of categories of z is smaller see altieri et al 2019a for further details on discarding the order the gap between the two options grows as i increases and induces a different computational burden for large datasets adding a practical advantage to the choice of discarding the order the second step of this approach to spatial entropy is to introduce a discrete variable w that represents space by classifying the distances at which the two occurrences of a pair take place classes w k are defined with k 1 k covering all possible distances within the observation window each distance class w k implies the choice of a corresponding adjacency matrix a k which identifies pairs where the two realizations of x lie at a distance belonging to the range w k in biodiversity contexts considering different choices of distances at which pairs e g pairs of observable species occur is crucial as well as quantifying the whole contribution of space on the global entropy measure thanks to the introduction of w and following the basis of information theory cover and thomas 2006 the total entropy of z may be decomposed as 10 h z r 1 r p z r log 1 p z r m i z w h z w where in the spatial context defined by the variable w mi z w is the spatial mutual information and is the part of entropy of z due to the spatial configuration w while h z w is the spatial global residual entropy and represents the remaining information brought by z after space has been taken into account the more z depends on w i e the more the realizations of x are spatially associated e g nearby observations of the same species the higher the spatial mutual information conversely when the spatial association among the realizations of x is weak e g observations of a species tend to have neighbours of different species the entropy of z is mainly due to spatial global residual entropy the entropy h z is a stable reference value while its two components mi z w and h z w vary and are able to assess the role of space for datasets with different spatial configurations for the sake of interpretation and diffusion of the results a proportional measure of spatial mutual information is 11 m i prop z w m i z w h z which ranges in 0 1 and quantifies the contribution of space according to the total entropy of z the overall value of mi z w however is often negatively influenced by what happens at large distance ranges where scarce spatial correlation is usually present hence spatial mutual information for the whole dataset may be low even when a clustered pattern occurs e g groups of nearby realizations belonging to same species are observed the variable w helps in overcoming this drawback since k subsets of realizations of z denoted by z w k are available and spatial mutual information can be also decomposed as 12 m i z w k 1 k p w k p i z w k that is a weighted sum of partial terms pi z w k each denoted spatial partial information and defined as 13 p i z w k r 1 r p z r w k log p z r w k p z r each partial term pi z w k quantifies the contribution to the departure from independence of each conditional distribution p z w k i e the contribution of the k th distance range to the global mutual information between z and w as regards biodiversity each partial term measures the degree of association across species over space at each distance range if this association is high then pairs of observations belonging to the same species are more likely that is a low level of biodiversity is detected conversely a high biodiversity situation is revealed when the spatial associations of species is low i e pairs of different species are more often observed the interest usually lies on short distance ranges but all depends on research focus and on the definition dimensions of the observation window analogously to 12 for spatial residual entropy the following additive decomposition holds 14 h z w k 1 k p w k h z w k where the terms h z w k denoted as partial residual entropies measure the partial contributions to the entropy of z once the spatial configuration is controlled 15 h z w k r 1 r p z r w k log 1 p z r w k each term h z w k thus represents a leibovici s entropy index 9 on unordered couples which is now included in a wider and complete framework for spatial entropy measures in biodiversity a high value for h z w k especially at short distance ranges is a hint for declaring large levels of biodiversity 3 a biodiversity study with spatentropy the rainforest tree data this section offers a practical guide to the spatentropy package via the rainforest tree data example in what follows the functions and the possible options are described following the application more detailed information is available in the package manual downloadable at https cran r project org package spatentropy where a number of toy examples are included to further practice of the package for simplicity many results are presented by rounding to two decimal places while r provides more precise values after spatentropy and its dependence package spatstat 2 0 0 are installed from cran https cran r project org the graphical representation of data as in fig 1 may be produced with the commands r data raintrees r plot raintrees main pch 16 cols 1 4 as introduced in section 1 the variable x here represents the tree species with i 4 categories briefly coded as x 1 acaldi x 2 cha2sc x 3 ingape and x 4 rinosy consequently the variable z has 42 16 categories when ordering within species couples is considered or 5 2 10 categories when unordered pairs are built the total number of trees is n 7251 where n 1 2678 trees belong to species x 1 n 2 544 to species x 2 n 3 311 to species x 3 and n 4 3718 to species x 4 the relative amounts for each species are the frequencies that enter entropy computations the input for batty s entropy is different as explained in section 3 1 the four species have different spatial configurations acaldi is evenly distributed over the region cha2sc has a clustered configuration ingape shows a tendency to grow on the right hand side of the area and rinosy is multiclustered therefore space is likely to play a role in the data heterogeneity as a starting point the traditional version of shannon s entropy is easily computed by r shannon raintrees the function produces three output elements the core information is returned by shshann 1 1 04 which gives the value of shannon s entropy for the four tree species a comment on the entropy value may be facilitated by the relative value obtained in the second element of output by the command rel shann 1 0 75 this value states that the overall data entropy is 75 of the maximum possible entropy in this case equal to log 4 such measure is easier to interpret across disciplines and allows comparison to other datasets a low value for the relative heterogeneity hints at a structure in the data possibly of spatial nature that decreases the level of chaoticity but no deeper exploration is allowed by this traditional shannon s entropy the third element of the output is a table with the relative frequencies that enter the computation of h x in place of the unknown probabilities probabilities category frequency 1 acaldi 0 37 2 cha2sc 0 08 3 ingape 0 04 4 rinosy 0 51 the entropy that is obtained as a first output is an estimate h x of h x if an uncertainty assessment is requested the variance of shannon s entropy can be estimated as v h x h x 2 h x 2 paninski 2003 where h x 2 i 1 i n i n log n n i 2 in the package such variance is obtained with r varshannon raintrees 1 0 44 afterwards we illustrate what the different spatial entropy measures bring to the study of the biodiversity of rainforest tree data 3 1 results for batty s entropy as introduced in section 2 1 the computation of batty s entropy imposes two exogenous decisions first the definition of the phenomenon of interest and second the choice of suitable sub areas as a running example let us compute entropy on all rainforest trees i e on the presence absence of trees without distinction among the 4 species when there are no reasons for fixing a specific area partition the package allows to randomly partition the observation window into g portions and we propose the former g 10 of fig 3 by using r batty ent batty unmark raintrees partition 10 the function unmark discards the information about the species the argument partition allows to choose g in the function output we first see areas freq area id abs freq rel freq area size 1 1 1160 0 16 56466 98 2 2351 0 05 26545 91 3 3704 0 10 77000 24 4 4 72 0 01 24134 30 reporting descriptive information about each sub area the number of trees abs freq the corresponding relative frequency rel freq which stands for p g and the sub area size area size i e t g then the outputs batty 1 13 07 rel batty 1 0 99 provide batty s entropy and its relative version respectively these results approach the maximum possible heterogeneity and are different from shannon s entropy values this happens because they take different perspectives batty s entropy says that with the given area partition rainforest trees are almost equally intensely distributed over the sub areas for plotting the data according to the area partition obtaining fig 3 the following code is used r plot unmark raintrees pch 16 cex 0 6 main r plot batty ent areas tess add true border 2 if the interest lies in focusing on a specific tree species e g chamguava batty s entropy can be computed by including the argument category into the command and by removing the function unmark r batty ent batty raintrees category cha2sc partition 10 batty 1 11 84 rel batty 1 0 90 in this case the entropy value is lower than the one referred to all species meaning that this particular species is not as evenly distributed as the overall dataset according to the area partition a more interesting approach is based on partitioning the area according to a covariate rainforest trees dataset is accompanied by the object raintreescov i e a list of two environmental covariates soil elevation and slope these variables may affect the degree of biodiversity of species therefore they can be used to define sub areas based on their values as an example let s consider slope which is a continuous variable and 4 intervals according to its values the categories of soil to derive these intervals are here obtained by using quantiles as breakpoints through the following code r slopecut cut raintreescov grad breaks quantile raintreescov grad probs 0 4 4 labels 1 4 discretize the covariate r maskv tiles list for ii in 1 nlevels slopecut r maskv ii as logical c slopecut v levels slopecut ii r tiles ii owin xrange data window xrange yrange data window yrange mask matrix maskv ii nrow slopecut v the map of the original covariate slope for the whole area is shown in fig 4 left panel and is produced by r par mfrow c 1 2 r plot raintreescov grad main col gray seq 1 0 l 100 fig 4 right panel shows the area partition based on the above discretization of the covariate slope together with species chamguava it is obtained by running the following code r plot slopecut main r plot split ppp raintrees cha2sc add true pch 16 cex 0 6 main once the partition is built the relative frequencies of trees are computed for each discretized level of the variable slope and batty s entropy is derived by using the same command batty and specifying the area partition the following commands refer to species chamguava r slopetess list tiles tiles n nlevels slopecut r batty raintrees category cha2sc partition slopetess batty 1 12 83 rel batty 1 0 98 interpretation is analogous to the former one but something more can be said here firstly the two partition options for batty s entropy on the same species show how sensible the measure is to the chosen area partition any interpretation must be carried out accounting for the choice in addition despite the evident tendency to spatial clustering species chamguava does not seem to have any dependence on the covariate slope its entropy computed over the slope partition is high close to the maximum denoting that trees grow evenly across all covariate levels a computation of batty s entropy under a covariate based partition can give hints on how biodiversity can be due to environmental covariates 3 2 results for batty s lisa entropy the lisa version of batty s entropy can be built in the package spatentropy by using the same commands as before to obtain an area partition of interest moreover this version needs a definition for the neighbourhood that can be done by choosing either the number of neighbours for each sub area or a neighbourhood distance measured over the sub areas centroids in the former case consider as an example the case of 3 neighbours a random partition in g 10 sub areas and the species chamguava then running the following command one obtains r kc ent karlstrom raintrees category cha2sc partition 10 neigh 3 karlstrom 1 1 88 rel karl 1 0 82 the following part of the output with details on the area partition is areas freq area id abs freq rel freq neigh mean area size 1 1 0 0 00 0 01 57581 30 2 2 0 0 00 0 01 54517 04 3 3307 0 56 0 20 101940 22 4 4 3 0 01 0 02 33719 95 that with respect to the function output of batty s entropy has one new column with the average frequencies over the neighbouring areas which stand for p g in the computations compared to batty s entropy given the area partition the consideration of the neighbouring probabilities decreases the relative entropy level other neighbourhood options may be tried for further comparison 3 3 results for leibovici s entropy leibovici s entropy refers to z i e the variable which defines co occurrences of rainforest trees as anticipated in section 2 3 o neill s entropy that was the first entropy measure based on such variable is unsuitable when point data are considered the computation of shannon s entropy of z for unordered pairs of rainforest trees r shannonz raintrees produces the same output of shannon but in this case the probability table reports the 10 frequencies of all unordered pairs of tree categories runs as probabilities pair abs freq rel freq 1 acaldiacaldi 3 584 503 0 14 2 cha2sccha2sc 147 696 0 01 3 in gapeingape 48 205 0 002 shannz 1 1 67 rel shannz 1 0 72 again the entropy value is far from the maximum value as can be seen from the relative version and hints at a data structure that cannot be captured without spatial information on the contrary leibovici s entropy which by default works with ordered couples and needs the specification of a distance of interest d among the trees forming each pair is much more appropriate an example with d 10 m follows the command is r leibovici raintrees ccdist 10 where the argument ccdist allows to specify the value of d in the same unit of measurement as the observation area metres in the case of raintrees results are in the output leib 1 1 12 rel leib 1 0 40 the value for leibovici s entropy for d 10 is quite low and different both from shannon s entropy and from batty s entropies they measure different aspects of entropy for leibovici s entropy a low value detects an important role of space for the heterogeneity of the tree species when the potential spatial relationship is measured up to 10 m this measure is very interesting but offers a picture limited to the chosen distance so that computations should be repeated for a number of different values for ds to get a general understanding of the data behaviour 3 4 results for altieri s set of entropy measures the recent set of entropy measures proposed by altieri et al 2018a defines an exhaustive framework which overcomes the drawbacks of leibovici s entropy this approach considers several ranges which cover all the observation window in order to gain information on the spatial data heterogeneity at all distances as a starting point the distance classes are chosen in this example 4 ranges are considered for the variable w as w 1 0 1 w 2 1 2 w 3 2 10 w 4 10 d max where d max 1118 03 in metres in coherence with the observation area then the spatial entropy measures are obtained by running the following code r outp altieri raintrees distbreak c 1 2 10 verbose true where the argument distbreak fixes the internal breaks of the distance classes and verbose controls whether information about the computation progress is printed in the output true or not false this function is the most sophisticated of the package and even if very efficient may take up to a few minutes for large datasets therefore we recommend to use the command verbose true in order to check the progress of the computation at the end of the process the generated output is very detailed first it provides the quantities used to compute the set of entropy measures distance breaks the distribution of the distance classes the number of pairs in total and per class and tables with the absolute and relative frequencies for all distance classes then the global values for spatial mutual information mi z w spatial residual entropy h z w and their sum i e shannon s entropy h z are returned the illustration of his part is omitted in the present text and we refer to the package manual for further details finally partial entropies are yielded in both absolute and relative terms spi terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 70 0 64 0 56 0 09 0 001 rel spi terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 39 0 37 0 33 0 05 0 0007 res terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 1 10 1 07 1 12 1 77 1 65 rel res terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 61 0 63 0 67 0 95 0 9993 the spi terms are the spatial partial information terms pi z w k and show the contribution of space to the data heterogeneity i e to biodiversity at each distance class the res terms are the partial residual entropies h z w k and measure the biodiversity of trees due to other sources of heterogeneity once space is controlled these quantities represent the most interesting output to interpret in particular focusing on each distance range and on relative versions rel spi terms and rel res terms where the two components sum to 1 for each distance range we can identify the role of space for instance for trees lying at most 1 m apart 39 of the heterogeneity is due to the spatial correlation i e to pairs of trees belonging to the same species conversely the remaining percentage 61 detects the part of entropy attributable to pairs of tree of different species i e the biodiversity level conditional on this distance the residual terms h z w k s increase when the distance ranges rise because the role of space naturally decreases at higher distances when they assume high and leading values also at lower distances this implies that data do not present a strong spatial structure and that we can expect high levels of biodiversity in small sites too therefore partial residual entropies represent the main goal as they measure biodiversity once the effect of space is controlled to conclude the several tools proposed by altieri et al assess a remarkable biodiversity across the rainforest trees of barro colorado island by exploring the phenomenon under a complete and very flexible framework 4 spatentropy in other environmental applications turin s urbanization study entropy measures are not only a standard tool in biodiversity studies but also may be widely employed in environmental applications such as landscape land cover and land use data as a further example of usage of the spatentropy package an environmental study for the binary grid data about urbanization in turin is considered as introduced in section 1 land cover data x are managed and dichotomized into urbanized x 1 1 and non urbanized x 2 0 zones i e pixels of size 250 250 m see fig 2 the aim is to measure the urban dispersion of the city by highlighting the differences in the use of both spatial entropy indices and spatentropy with respect to the previous application in biodiversity field the main functions already described in section 3 are only briefly cited in what follow a command is added regarding the computation of o neill s entropy since the concept of contiguity is well defined for grid data while it could not be used in the previous data example r shannon turin r shannonz turin r batty turin cell size 250 partition turintess r karlstrom turin cell size 250 partition turintess neigh 2 r oneill turin r leibovici turin cell size 250 ccdist 400 r altieri turin cell size 250 distbreak c 1 2 5 cell size the main results are collected in table 1 where in the left hand side the absolute entropy values are reported while the right hand side shows the relative versions which are straightforward to comment firstly the two non spatial shannon entropies are computed h x and h z these both take values close to their maximum log 2 and log 3 respectively identifying the metropolitan area of turin as a highly chaotic area the sub areas for the computation of batty s entropy are the administrative borders of the 15 municipalities forming the metropolitan area object turintess introduced in section 1 3 batty s lisa entropy is obtained by further considering the 2 4 or 12 nearest neighbours based on the pixels centroids the argument cell size to be ignored for point data defines the dimension of the pixels in the area measurement unit 250 250 m the relative versions of all indices can be compared batty s entropy is similar to batty s lisa entropy with the most extensive neighbourhood 12 neighbours while smaller neighbourhood systems decrease the level of entropy these values are below their maximum compared to shannon s entropies likely because the main municipality of turin is more intensely urbanized than the municipalities of its belt o neill s and leibovici s entropies can be commented together as the former is a special case of the latter when d equals the cell width in this case 250 m the option ccdist defines the distance between pixel centroids considered for building pairs three versions of leibovici s entropy are computed for d 400 1000 10 000 m o neill s entropy takes a relatively small value equal to 76 of its maximum meaning that when considering adjacent couples the different categories are not evenly distributed so the spatial configuration is not random by exploring the output of oneill which has a similar structure to the other functions outputs and in particular the table of relative frequencies the majority of couples are shown to be homogeneous of type urban urban 48 of the total or non urban non urban 39 of the total while heterogeneous couples are a minority consequently the index shows that the urban tissue tends to be compact with a different conclusion with respect to the traditional shannon s entropy as d increases the entropy value approaches its maximum as couples of pixels lying farther apart are included and the measure becomes more similar to shannon s entropy as regards altieri s set of entropy measures the spatial partial information terms pi h w k here represent the quantities of interest as they measure the urban dispersion or compactness for turin s area a high value for pi h w k especially at short distance ranges is a hint for a compact urban expansion which is a desirable feature as dispersed cities are proved to be inefficient and may impede sustainable development altieri et al 2019b we explore two options of distance ranges with respect to spatial information terms in order to show the sensitivity of results based on such choice the first one defines w as w 1 0 250 w 2 250 500 w 3 500 1250 w 4 1250 d max for the second option w is w 1 0 400 w 2 400 1000 w 3 1000 10000 w 4 10000 d max over the small distance ranges a relevant role of the spatial structure in determining the data heterogeneity can be appreciated that decreases for larger distances in particular focusing on the first distance class of the first option 0 250 i e pixels contiguity and on the relative version of pi h w k we quantify a 24 7 of entropy due to the spatial correlation if we move to the first class of the second option 0 400 this percentage is 22 4 the higher these values the closer we get to maximum urban compactness when medium distance ranges are considered e g 1000 10000 or 500 1250 the relative spatial partial information terms become small denoting the possibility of a future urban expansion in the metropolitan area as a conclusion this set of entropy measures reveals as altogether very exhaustive in explaining the urban structure of turin s metropolitan area which is characterized by a substantive and non negligible compactness of the urban patches at distances up to 1 1 25 km once more the package spatentropy efficiently supported the analysis in a rapid and suitable way the potential and applicability of this study on urbanization may be highlighted by works that compare entropy measures across urban areas over a region or country so that results can be commented both in absolute and relative terms altieri and cocchi 2021b 5 concluding remarks in this work the r package spatentropy is first introduced as the unique and efficient statistical tool to compute a large set of spatial entropy measures after a review of the measures available in the literature spatentropy is applied to real data from biodiversity and environmental fields in particular a first dataset on rainforest trees of barro colorado island has been considered in order to quantify the biodiversity of trees species over space the spatial entropy indices are then employed for measuring the urban sprawl of turin s metropolitan area the potential of the package we propose is shown on these examples in terms of computational efficiency and simplicity of use these features make spatentropy a suitable toolkit to be exploited also by non statisticians and new users the package spatentropy is able to easily deal with different kinds of variables and of spatial data the rainforest tree example refers to point data over an observation window with respect to a multicategorical variable the 4 species while urbanization data on turin are areal i e it considers a grid of pixels is considered and a binary variable urbanized non urbanized characterizes each pixel the ability of the package in managing space comes from the integration and exploitation of the widely known r package spatstat which also offers the possibility of experimenting spatentropy on additional available dataset point data are more precise than areal ones and this implies different ways of defining the neighbourhood we show how to fix distances for grids which are automatically built by spatentropy using pixel centroids multicategorical and dichotomic variables can be equally analyzed for both point and areal data the different spatial entropy measures can be easily built by the spatentropy package and then carefully compared as shown in the two applications sections we focus on the flexible and complete perspective offered by altieri s set of spatial entropy measures despite the approach of spatentropy implies a larger but still feasible computational effort results are very exhaustive and informative of the phenomenon at both global and local levels in particular we showed how for the rainforest tree data the leading quantities are the partial residual terms which increase when biodiversity of species rises at appropriate distance ranges i e by considering space in particular we observed a percentage equal to 61 of residual entropy at the smallest distance range up to 1 m denoting a remarkable biodiversity level even when considering small portions of the whole observation area conversely for land cover data on turin s urbanization the most informative quantities are the spatial partial terms measuring the degree of compactness of the metropolitan area which is a desirable feature for an efficient and sustainable development the city of turin is shown to have a compact urban structure the choice of the distance ranges depends on the research aim different options might be easily and quickly tested thanks to the package flexibility results can be enriched by a comparison to the findings on similar datasets e g other tree species or different cities exploiting the corresponding versions of all indices provided by the package other fields of applications cover any spatial dataset where syntethic measures of heterogeneity may be of interest they span over natural phenomena such as earthquakes altieri et al 2016 and wildfires leuenberger et al 2018 gelfand and monteiro 2014 polluting agents cameletti et al 2014 meteorological events de caceres et al 2018 and epidemiological data blangiardo et al 2016 greco and trivisano 2009 several dataset on these contexts of application are also available in the spatentropy package as additional toy examples declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25767,entropy measures are standard tools in environmental and ecological sciences to describe the heterogeneity of data this paper reviews a selection of spatial entropy indices some of which are very recent suitable to deal with spatial data on variables presenting a finite number of categories a special focus is given on biodiversity data but methods can be applied to any other environmental phenomena the new r package spatentropy is here introduced to compute spatial entropy measures in practice the extension from traditional entropy measures to their spatial version is a unique feature of the package which is able to work with both areal and point data a practical part is also presented where two types of environmental data are considered regarding trees biodiversity and urban expansion respectively the package spatentropy is run over these dataset and shown to represent a user friendly and helpful tool for new package users keywords spatial entropy shannon s entropy biodiversity tree species urban expansion r package software and data availability name of package software spatentropy package of r software title of package spatial entropy measures date first available 2018 02 28 version 0 1 0 date updated version 2021 04 07 version 2 0 1 developers linda altieri daniela cocchi giulia roli maintainer linda altieri contact linda altieri unibo it required software r 3 5 0 a free software environment for statistical computing and graphics available at https cran r project org required package spatstat 2 0 0 available at https cran r project org package spatstat automatically imported packages spatstat geom spatstat core spatstat linnet package availability the spatentropy package is free and downloadable from the cran repository at https cran r project org package spatentropy package reference manual available for version 2 0 1 at https cran r project org web packages spatentropy spatentropy pdf package description the heterogeneity of spatial data presenting a finite number of categories can be measured via computation of spatial entropy functions are available for the computation of the main entropy and spatial entropy measures in the literature they include the traditional version of shannon s entropy shannon 1948 batty s spatial entropy batty 1974 batty s lisa entropy o neill et al 1988 li and reynolds contagion index li and reynolds 1993 karlstrom and ceccato s entropy karlström and ceccato 2002 leibovici s entropy leibovici 2009 parresol and edwards entropy parresol and edwards 2014 and altieri s entropy altieri et al 2018a full references for all measures can be found under the topic spatentropy the package is able to work with lattice and point data the updated version works with the updated spatstat package 2 0 0 it also provides a more intuitive framework for all functions including improved examples and new data the speed of most functions has also been substantially increased data availability the spatentropy package includes two datasets the first one raintrees is a marked point pattern dataset about four rainforest tree species available at http www ctfs si edu it is a ppp object see package spatstat with 7251 points containing window an object of type owin see package spatstat i e the 1000 500 m observation area x y the numeric vectors with points coordinates marks a character vector matching the tree species to the data points this dataset documents the presence of tree species over barro colorado island panama barro colorado island has been the focus of intensive research on lowland tropical rainforest since 1923 http www ctfs si edu research identified several tree species over a rectangular observation window of size 1000 500 m the tree species constitute the point data categorical mark this dataset presents 4 species with different spatial configurations acalypha diversifolia chamguava schippii inga pezizifera and rinorea sylvatica the second dataset included in the package turin is a lattice dataset with turin urban morphological zones values are either 0 non urban or 1 urban pixels outside the administrative borders are classified as na this raster pixel lattice dataset comes from the eu corine land cover project eea 2011 and is dated 2011 it is the result of classifying the original land cover data into urbanized and nonurbanised zones known as urban morphological zones umz umz data are useful to identify shapes and patterns of urban areas and thus to detect what is known as urban sprawl turin s metropolitan area is extracted from the european dataset and is composed by the municipality of turin and the surrounding municipalities beinasco venaria reale san mauro torinese grugliasco borgaro torinese collegno pecetto torinese pino torinese moncalieri nichelino settimo torinese baldissero torinese rivoli orbassano the dataset is made of 111 113 pixels of size 250 250 m many other data examples can be found in the just mentioned package spatstat as well as in other r packages devoted to biodiversity and environmental studies e g vegan 1 introduction 1 1 entropy as a biodiversity and environmental measure measuring biodiversity with appropriate syntheses is a challenging task see e g hoskins et al 2020 and drechsler 2020 the proposals of indicators range from the simplest indices denoting species richness see e g fisher et al 2012 towards more articulated proposals related to the abundance of species in ecological communities see e g scarnati et al 2009 under this perspective entropy indices certainly represent an important and efficient tool widely used for properly measuring biological diversity leinster and cobbold 2012 shannon s entropy shannon 1948 is the most common and successful formulation as it is able to synthesize several concepts in a single number entropy information heterogeneity surprise contagion the flexibility of such index and its ability to describe any kind of data including categorical variables motivate its diffusion across several applied fields for data description and interpretation frosini 2004 over the last decades the concept of entropy has been further developed and generalized in engineering and statistics cover and thomas 2006 also extending to the case of continuous variables firstly dealt by rényi 1961 and batty 1974 1976 2010 2014 in the wide context of environmental studies such as geography ecology biology and landscape analyses a large use of entropy measures has been made in such cases the researcher often deals with spatial data i e data that are georeferenced as points or areas and as a consequence also entropy measures need to be revised to include this spatial information at this regard a number of works are available in the literature aiming at building a spatial entropy index they can be ascribed to three main approaches the first starts with batty 1974 1976 2010 2014 who defined a spatial entropy measure which evaluates the distribution of an event over an area allowing for the unequal space partition into sub areas later karlström and ceccato 2002 modified the initial proposal in order to satisfy the property of additivity in terms of decomposition of the global index into local components firstly introduced by theil 1972 and then defined by local indices of spatial association lisa criteria anselin 1995 the second approach to spatial entropy includes space based on a suitable transformation of the study variable to account for the distance between realizations co occurrences the first proposal was made by o neill et al 1988 for contiguous couples of realizations then extended by leibovici 2009 and leibovici et al 2014 to further distances and general degrees of co occurrences contagion indices li and reynolds 1993 parresol and edwards 2014 are also based on this view spatial contagion is the opposite of entropy as for the third approach a set of spatial entropy measures has been presented by altieri et al 2018a 2019a b starting from the co occurrence approach but overcoming some undesirable features of the previous measures according to this framework shannon s entropy of the transformed variable is decomposed into the information due to space and the remaining one once space is considered the proposal solves the problem of preserving additivity and disaggregating results allowing for partial and global syntheses 1 2 available software packages for entropy the r software r core team 2017 is certainly one of the most flexible options for performing statistical analysis in particular when spatial data have to be managed r is employed by a wide part of the global community of statisticians and has the great advantage of being open source i e it can be downloaded freely and anyone can contribute to the software by submitting packages of functions for additional features packages undergo severe checks by the r team which ensure the quality of the available material the software main drawback is that its traditional interface is not very user friendly this can be solved by installing an interface such as rstudio https www rstudio com as far as entropy measures are concerned several r packages on cran https cran r project org are available the most common ones are entropart entropy and entropyestimation they all allow traditional entropy computation and decomposition into its two terms known as mutual information and conditional entropy generally referred to in information theory applications cover and thomas 2006 in particular entropart provides functions to calculate the alpha beta and gamma diversity indices of communities typical of ecological studies together with simpson s evenness index the package also offers an example dataset about tree species but its data format is very specific of the package and not easy to understand for r beginners the further package entropy is more focused on various estimators of entropy a topic which does not constitute the core of the present work nor of many biodiversity studies in particular it allows to compute the hausser and strimmer shrinkage estimator the maximum likelihood estimator and its miller madow correction various bayesian estimators including the nsb estimator and the chao shen estimator it also provides functions for estimating the kullback leibler divergence and for the partition of entropy between mutual information and residual entropy its functions start from the observed counts of species without any considerations about the spatial location of data the package does not offer any data example lastly entropyestimation partially overlaps with both the aforementioned packages as it includes functions for the estimation of shannon s entropy variants of renyi s entropy mutual information kullback leibler divergence and generalized simpson s indices no data example is available further r packages though containing the word entropy in the description all deal with different data contexts or different fields of study e g sampling entropy see altieri and cocchi 2021a despite a wide literature on spatial entropy measures as illustrated in section 1 1 to our knowledge there is no r package offering an automatic computation of these indices given the potential and the crucial role of spatial entropy measures in real applications this lack needs to be overcome by providing a practical and intuitive support for new users in managing spatial information in form of point or areal data and obtaining fast and interpretable results in the contexts of biodiversity and environmental studies 1 3 spatentropy a tool for easy computation of spatial entropy measures this work aims at introducing the new r package spatentropy an improved version of the original altieri et al 2018b which collects functions for the computation of all the spatial entropy indices mentioned in section 1 1 they include the traditional version of shannon s entropy and shannon s entropy of a transformation of the study variable x known in the literature as z which considers pairs of observations at a given distance over space moreover if offers computation of batty s spatial entropy and its modified lisa version following the observation pairs approach the package provides functions for computing o neill s entropy for adjacent couples and its transformations li and reynolds relative contagion index and parresol and edwards entropy in addition the generalized version of leibovici s entropy can be computed together with the further generalization to a set of spatial entropy indices that we call altieri s entropies the package is able to work with both grid and point data and makes extensive use of the widely known spatstat package version 2 0 0 functions and data structures baddeley et al 2015 spatentropy allows usage by non statisticians provided they have basic knowledge of r the minimum effort is requested from the user and all functions are sided by data examples additionally real datasets are provided for performing as examples the studies introduced in the present paper many other data examples can be found in the just mentioned package spatstat as well as in other r packages devoted to biodiversity and environmental studies e g vegan the present paper illustrates how to make use of spatentropy focusing on a main example concerning ecological data and witnessing the need to assess biodiversity via an entropy index that includes information about the data spatial structure this example consisting of a marked point pattern dataset about four rainforest tree species is available in the spatentropy package under the name raintrees this dataset documents the presence of tropical tree species over barro colorado island panama barro colorado island has been the focus of intensive research on lowland tropical rainforest since 1923 http www ctfs si edu research identified several tree species over a rectangular observation window of size 1000 500 m the tree species constitute the point data categorical mark i e a label attached to each point of the pattern the raintrees dataset presents 4 species with different spatial configurations acalypha diversifolia acaldi chamguava schippii cha2sc inga pezizifera ingape and rinorea sylvatica rinosy the overall dataset has a total number of 7251 points and is shown in fig 1 one further example which is synthetically presented along the paper highlights both the ability of the measures proposed in this paper to deal with different types of spatial data and the wide potential of using properly calibrated entropy measures the package spatentropy offers the binary areal dataset turin which comes from the eu corine land cover project eea 2011 turin s metropolitan area is composed by the main municipality of turin and its 15 surrounding municipalities that constitute its commuting belt the dataset is a rectangular matrix of 111 113 observations each corresponding to a pixel of size 250 250 m pixels within the area administrative borders are classified as 1 urban or 0 non urban while pixels outside the border are missing values the dataset is shown in fig 2 where a dark pixel is urban and a light gray pixel is non urban in the package the dataset is accompanied by the objects turinw i e the enclosing rectangular window and turintess which contains the names and administrative borders of all municipalities the paper develops as follows in section 2 the principles for introducing space in traditional entropy measures are presented to review the leading spatial entropy indices available in the literature the application of the new r package spatentropy on the main running example of trees biodiversity is carried on in section 3 section 4 uses spatentropy and illustrates the results on the second example i e areal data of urban sprawl finally section 5 concludes the work 2 from traditional to spatial entropy measures the traditional concept of entropy dates back to shannon s formula shannon 1948 which in information theory cover and thomas 2006 quantifies the average amount of information brought by a discrete random variable x taking values x i in a set of i outcomes according to the probability mass function pmf p x in biodiversity applications x represents the variety of life considered i e the levels of biodiversity commonly discussed genetic species and ecosystem diversity as an example in the rainforest tree data introduced above x classifies the trees into the i 4 different species x 1 to x 4 i e acalypha diversifolia chamguava schippii inga pezizifera and rinorea sylvatica shannon s entropy of x is then defined as 1 h x i 1 i p x i log 1 p x i and represents the expected value of the so called information function i p x i log 1 p x i entropy measures the information or in other words the surprise coming from observing realizations intuitively outcomes with a very low probability of occurrence i e hardly observable species increase the entropy value while outcomes very likely to occur i e more observable species give a small contribution to entropy thus entropy or analogously information and surprise will be larger when the observed outcomes are not likely to occur entropy ranges in 0 log i and its maximum value is achieved when x is uniformly distributed in many situations entropy is seen as a descriptive measure and the pmf of x is built using the relative frequencies of the observed categories when the goal is to make inference on the underlying process determining the observed outcomes entropy can also be seen as an estimator h x in its standard use the pmf is estimated by the so called plug in estimator paninski 2003 in such case the descriptive measure and the estimator coincide p x i n i n substitutes the probabilities p x i with the observed relative frequencies over n realizations in particular referring to the rainforest tree example the frequencies are the ratios of the count of each species over the total number of observed trees in the pllied part of the paper all entropy measures are computed by substituting the elements of the unknown probability distribution with the observed relative frequencies a major drawback of shannon s entropy is that it does not account for the spatial location of occurrences so that datasets with identical pmf but very different spatial configurations share the same entropy value in biodiversity this point is crucial as the location of the variety of life considered is available often as point coordinates as well as strongly related to the research questions the main aim is to quantify the spread of biodiversity over a certain space indeed in the rainforest tree data example the interest lies in measuring the diversity across species but also in quantifying the spread of species according to the spatial location of the trees the following sections present the mainstream approaches to build spatial entropy measures most indices imply the formal definition of a neighbourhood cressie 1993 the simplest way of representing a neighbourhood system over n spatial units is via an adjacency matrix a anselin 1995 i e a square matrix whose elements indicate whether pairs of units are neighbours a uu 1 if u n u that is the neighbourhood of area u a uu 0 otherwise and a uu 0 by definition spatial units may be points defined via precise coordinate pairs or alternatively areas identified via common representative coordinate pairs such as the area centroids in biodiversity the concept of spatial neighbourhood generalizes to an idea of similarity so that some species are closer e g in a biological taxonomic or logical sense than others leinster and cobbold 2012 once the adjacency matrix is suitably specified computations proceed the same way for any similarity system 2 1 batty s entropy batty s spatial entropy batty 1974 1976 2010 2014 is useful for evaluating the heterogeneity in the distribution of a phenomenon over an area it is particularly appropriate when the observation area is exogenously partitioned into sub areas such as municipality administrative boundaries for a region the use of a meaningful exogenous area partition is very important since conclusions are heavily affected by the partition itself let a phenomenon of interest f occur over an observation window of size t partitioned into g areas of size t g this defines g dummy variables identifying the occurrence of f over a generic area g with g 1 g note that the phenomenon is denoted by the symbol f a presence absence variable different from x a generic categorical variable examples of f are the presence of a single species in biodiversity studies like in the case of point data on rainforest trees or the observation of urban versus non urban patches in city expansion studies like in areal data on binary turin s dataset given that f occurs over the whole window its occurrence in area g takes place with probability p g where 1 p g g g p g and g p g 1 the phenomenon intensity is obtained as λ g p g t g where t g is the area size and is assumed constant within each area batty s spatial entropy is then defined with respect to these quantities as 2 h b f g 1 g p g log t g p g it expresses the average amount of information brought by the occurrence of f in any area in the observation window accounting for unequal space partition through the multiplicative component t g note that p g p x i as the latter is the probability of occurrence of a category i e a species anywhere over the window while p g expresses the probability of occurrence of the phenomenon under study over a sub area analogously to shannon s entropy which is high when the i categories of x are equally represented over a non spatial data collection batty s entropy is high when f is equally intense over the g areas partitioning the observation window i e when λ g λ for all g batty s entropy h b f reaches a minimum value equal to log t g when p g 1 and p g 0 for all g g with g denoting the area with the smallest size the maximum value of batty s entropy is log t reached when the intensity of f is the same over all areas i e λ g 1 t for all g in the case of point data batty s spatial entropy aggregates information over each sub area into a single number p g thus neglecting the detailed information about the points coordinates this represents an unpleasant pitfall for the rainforest tree data batty s entropy may be computed if the aim lies on measuring the entropy observed across sub areas of interests controlling for their dimensions in such case entropy is computed either on the overall dataset discarding the information about the different kinds of species see e g fig 3 where g 10 or by considering each category of species separately in the latter case the starting variable x covering i 4 species is transformed into a dummy variable indicating the presence absence of one species of interest over the sub areas if the interest lies in the entropy levels of each species over the sub areas 4 dummy variables are needed and 4 batty s entropies are obtained and compared without the possibility to have a unique measure 2 2 batty s lisa entropy karlström and ceccato 2002 made a challenging proposal to exploit the contribution of neighbourhood in batty s entropy index following the local indices of spatial association lisa theory anselin 1995 they consider an adjacency matrix for each of the g sub areas introduced in section 2 1 i e a a g g g g 1 g the elements on the diagonal of the adjacency matrix a are a gg 1 i e each area neighbours itself then the elements a gg of this matrix are included to weight the probability of occurrence of f in a given spatial unit g 3 p g g 1 g a g g p g mimicking batty s entropy batty s lisa entropy index is then defined as 4 h lisa f g 1 g p g log 1 p g i e it fixes t g 1 and defines the information function as i p g log 1 p g the main purpose of batty s lisa index lies again on measuring and comparing entropies at different levels of biodiversity e g separately by species across the sub areas of interests but instead of controlling for their dimensions a sub area neighbourhood is included in the computation where again neighbourhood can be extended to a more general meaning than purely spatial adjacency this approach further allows to obtain local entropies 5 l g p g log 1 p g which represent the specific contribution of each sub area and preserve the additive properties of lisa theory in deriving the global index h lisa f the maximum of h lisa f does not depend on the choice of the neighbourhood and is log g as a tends to the identity matrix h lisa f tends to batty s spatial entropy 2 with equality in the case of t g 1 for all g 2 3 o neill s entropy and contagion indices another way to build a spatial entropy measure relies on defining a new categorical variable z where each realization identifies ordered couples x i x j of occurrences of x over space order preservation within couples regards considering the relative spatial location of the observations conventionally if order is preserved the couple x i x j implies that the observation carrying the j th category occurs at the right or below the observation carrying the i th category under this criterion such couple is different from x j x i for i categories of x the new variable z has r i 2 categories the attention moves from the computation of 1 namely h x to an index of the same form i e shannon s entropy of z h z based on the pmf p z in biodiversity entropy measures based on z are useful when the variable of interest has two or more categories e g species and when the goal is to understand how the presence of a species at one location affects neighbouring outcomes i e the adjacent presence of the same or a different species intuitively when the variable is strongly spatially associated neighbouring outcomes are closely related and the surprise and thus the entropy in observing data decreases defining the type of neighbourhood to consider is crucial so that different proposals based on this choice are available in the literature o neill et al 1988 proposed an early spatial entropy index by defining a neighbourhood based on contiguity i e areal units sharing a border shannon s entropy 1 is computed in this case for the subset of values of the variable z made of contiguous couples such couples are identified by non zero elements in a suitable adjacency matrix the contiguity matrix c the subset of couples of contiguous realizations is denoted by z c with pmf p z c then the associated shannon s entropy is 6 h o z c r 1 i 2 p z r c log 1 p z r c this measure ranges from 0 to log i 2 and quantifies the residual amount of entropy associated to the variable of interest once the influence of the spatial configuration in terms of contiguity has been taken into account when it is close to 0 the heterogeneity of data mostly depends on its spatial structure and the residual entropy is low when it is close to its maximum data do not present a strong positive or negative spatial correlation and are more similar to a randomly scattered scheme a direct derivation of this approach is based on the concept of contagion which is the opposite of entropy indeed the relative contagion index rc li and reynolds 1993 is 7 h r c z c 1 h o norm z c 1 1 log i 2 r 1 i 2 p z r c log 1 p z r c where the second term is the normalized entropy of z c obtained via the multiplication of 6 by 1 log i 2 its complement to 1 measures relative contagion the higher the spatial contagion between categories of z c the lower the spatial entropy normalization has the advantage of allowing comparison across datasets presenting a different number of categories giusto non andare a capo if one wants to account for the number of categories of x when computing the contagion index non normalized measures should be computed in order to distinguish among contexts with different numbers of categories for this reason parresol and edwards 2014 suggest an unnormalized version of 7 8 h p z c h o z c r 1 i 2 p z r c log p z r c thus ranging from log i 2 to 0 both the relative contagion and parresol edward s index have an opposite interpretation than o neill s entropy increasing departures from 0 denote a stronger spatial structure in the data which affects the heterogeneity o neill s entropy and its modified versions are conceived for areal data as they require information on sharing a border a practical example on entropy measures based on contiguity relies on turin s urban data in the case of biodiversity point data such as the tree species dataset this approach implies a pixelization of the observations which is a sensible choice as it arbitrarily assignes a spatial dimension and a block structure to points with great loss of information point data may be analyzed using the proposal of distance rather than contiguity between observations in such a situation more suitable measures can be actually applied such as the ones presented in the next sections 2 4 leibovici s entropy following the idea of o neill et al 1988 leibovici 2009 and leibovici et al 2014 propose a richer measure of entropy by extending h o z c in two directions firstly z can now represent not only couples but also triples and further degrees m of co occurrences the authors develop the case of ordered co occurrences so that the number of categories of z is r m i m secondly space is now allowed to be continuous so that areal as well as point data may be considered and associations may not coincide with contiguity the concept of distance between occurrences generalizes the concept of contiguity once a distance d of interest is fixed then co occurrences are defined for each m and d as the m th degree simultaneous realizations of x at any distance d d i e distances are considered according to a cumulative perspective to this aim an adjacency hypercube a d is built and a subset of all co occurrences z is selected conditional on it i e z a d or for the sake of simplicity z d then leibovici s spatial entropy is again a version of shannon s entropy defined on the pmf p z d 9 h l z d r 1 i m p z r d log 1 p z r d the derivation of o neill s entropy 6 is straightforward as it is the special case when m 2 and d is defined for contiguous co occurrences in applications only co occurrences of degree m 2 i e couples of realizations are usually considered therefore unless otherwise specified from now on we refer to the case of couples m 2 leibovici s approach finally allows to include the coordinates information for point data in the computation of the entropy if we again consider the example about the spatial pattern of the species 16 couples can be observed acalypha acalypha acalypha chamguava acalypha inga acalypha rinorea chamguava acalypha and so on once a distance d has been fixed only couples of species x i x j located at a distance lower than d are isolated and enter the computation of h l z d clearly the choice of d affects this measure of entropy thus in real applications different scenarios for d need to be explored and compared overcoming this feature is among the reasons why a further generalization of this approach has been proposed by altieri et al 2018a and described in the following section 2 5 altieri s set of entropy measures altieri et al 2018a 2019a 2019b developed a more general approach defining a set of spatial entropy measures starting from o neill and leibovici s indices such set should be employed when the interest lies in understanding the role of the spatial configuration in determining the entropy of a biodiversity variable not only at one isolated specific distance but also at a global level or at several distance ranges simultaneously in addition it should be used when the influence of space needs to be quantified as a percentage of the entropy e g in the rainforest tree example how much the global entropy across species is due to the spatial location of the trees from a statistical perspective this approach constitutes a more sophisticated tool that allows more flexibility and interpretability than other available measures of entropy the important starting point is a different way of computing the variable z with respect to previuos definitions the general degree m of co occurrences is defined discarding the order within co occurrences for the sake of simplicity let s consider only the case of m 2 where under this approach pairs not couples of realizations are considered i e the relative spatial location of the two realizations is irrelevant the reason for this choice is two fold firstly ordering occurrences is not sensible in spatial statistics where spatial configurations are not generally assumed to have a direction secondly discarding the order ensures a one to one correspondence between shannon s entropy of x and z note that when order is discarded the number of categories of z is smaller see altieri et al 2019a for further details on discarding the order the gap between the two options grows as i increases and induces a different computational burden for large datasets adding a practical advantage to the choice of discarding the order the second step of this approach to spatial entropy is to introduce a discrete variable w that represents space by classifying the distances at which the two occurrences of a pair take place classes w k are defined with k 1 k covering all possible distances within the observation window each distance class w k implies the choice of a corresponding adjacency matrix a k which identifies pairs where the two realizations of x lie at a distance belonging to the range w k in biodiversity contexts considering different choices of distances at which pairs e g pairs of observable species occur is crucial as well as quantifying the whole contribution of space on the global entropy measure thanks to the introduction of w and following the basis of information theory cover and thomas 2006 the total entropy of z may be decomposed as 10 h z r 1 r p z r log 1 p z r m i z w h z w where in the spatial context defined by the variable w mi z w is the spatial mutual information and is the part of entropy of z due to the spatial configuration w while h z w is the spatial global residual entropy and represents the remaining information brought by z after space has been taken into account the more z depends on w i e the more the realizations of x are spatially associated e g nearby observations of the same species the higher the spatial mutual information conversely when the spatial association among the realizations of x is weak e g observations of a species tend to have neighbours of different species the entropy of z is mainly due to spatial global residual entropy the entropy h z is a stable reference value while its two components mi z w and h z w vary and are able to assess the role of space for datasets with different spatial configurations for the sake of interpretation and diffusion of the results a proportional measure of spatial mutual information is 11 m i prop z w m i z w h z which ranges in 0 1 and quantifies the contribution of space according to the total entropy of z the overall value of mi z w however is often negatively influenced by what happens at large distance ranges where scarce spatial correlation is usually present hence spatial mutual information for the whole dataset may be low even when a clustered pattern occurs e g groups of nearby realizations belonging to same species are observed the variable w helps in overcoming this drawback since k subsets of realizations of z denoted by z w k are available and spatial mutual information can be also decomposed as 12 m i z w k 1 k p w k p i z w k that is a weighted sum of partial terms pi z w k each denoted spatial partial information and defined as 13 p i z w k r 1 r p z r w k log p z r w k p z r each partial term pi z w k quantifies the contribution to the departure from independence of each conditional distribution p z w k i e the contribution of the k th distance range to the global mutual information between z and w as regards biodiversity each partial term measures the degree of association across species over space at each distance range if this association is high then pairs of observations belonging to the same species are more likely that is a low level of biodiversity is detected conversely a high biodiversity situation is revealed when the spatial associations of species is low i e pairs of different species are more often observed the interest usually lies on short distance ranges but all depends on research focus and on the definition dimensions of the observation window analogously to 12 for spatial residual entropy the following additive decomposition holds 14 h z w k 1 k p w k h z w k where the terms h z w k denoted as partial residual entropies measure the partial contributions to the entropy of z once the spatial configuration is controlled 15 h z w k r 1 r p z r w k log 1 p z r w k each term h z w k thus represents a leibovici s entropy index 9 on unordered couples which is now included in a wider and complete framework for spatial entropy measures in biodiversity a high value for h z w k especially at short distance ranges is a hint for declaring large levels of biodiversity 3 a biodiversity study with spatentropy the rainforest tree data this section offers a practical guide to the spatentropy package via the rainforest tree data example in what follows the functions and the possible options are described following the application more detailed information is available in the package manual downloadable at https cran r project org package spatentropy where a number of toy examples are included to further practice of the package for simplicity many results are presented by rounding to two decimal places while r provides more precise values after spatentropy and its dependence package spatstat 2 0 0 are installed from cran https cran r project org the graphical representation of data as in fig 1 may be produced with the commands r data raintrees r plot raintrees main pch 16 cols 1 4 as introduced in section 1 the variable x here represents the tree species with i 4 categories briefly coded as x 1 acaldi x 2 cha2sc x 3 ingape and x 4 rinosy consequently the variable z has 42 16 categories when ordering within species couples is considered or 5 2 10 categories when unordered pairs are built the total number of trees is n 7251 where n 1 2678 trees belong to species x 1 n 2 544 to species x 2 n 3 311 to species x 3 and n 4 3718 to species x 4 the relative amounts for each species are the frequencies that enter entropy computations the input for batty s entropy is different as explained in section 3 1 the four species have different spatial configurations acaldi is evenly distributed over the region cha2sc has a clustered configuration ingape shows a tendency to grow on the right hand side of the area and rinosy is multiclustered therefore space is likely to play a role in the data heterogeneity as a starting point the traditional version of shannon s entropy is easily computed by r shannon raintrees the function produces three output elements the core information is returned by shshann 1 1 04 which gives the value of shannon s entropy for the four tree species a comment on the entropy value may be facilitated by the relative value obtained in the second element of output by the command rel shann 1 0 75 this value states that the overall data entropy is 75 of the maximum possible entropy in this case equal to log 4 such measure is easier to interpret across disciplines and allows comparison to other datasets a low value for the relative heterogeneity hints at a structure in the data possibly of spatial nature that decreases the level of chaoticity but no deeper exploration is allowed by this traditional shannon s entropy the third element of the output is a table with the relative frequencies that enter the computation of h x in place of the unknown probabilities probabilities category frequency 1 acaldi 0 37 2 cha2sc 0 08 3 ingape 0 04 4 rinosy 0 51 the entropy that is obtained as a first output is an estimate h x of h x if an uncertainty assessment is requested the variance of shannon s entropy can be estimated as v h x h x 2 h x 2 paninski 2003 where h x 2 i 1 i n i n log n n i 2 in the package such variance is obtained with r varshannon raintrees 1 0 44 afterwards we illustrate what the different spatial entropy measures bring to the study of the biodiversity of rainforest tree data 3 1 results for batty s entropy as introduced in section 2 1 the computation of batty s entropy imposes two exogenous decisions first the definition of the phenomenon of interest and second the choice of suitable sub areas as a running example let us compute entropy on all rainforest trees i e on the presence absence of trees without distinction among the 4 species when there are no reasons for fixing a specific area partition the package allows to randomly partition the observation window into g portions and we propose the former g 10 of fig 3 by using r batty ent batty unmark raintrees partition 10 the function unmark discards the information about the species the argument partition allows to choose g in the function output we first see areas freq area id abs freq rel freq area size 1 1 1160 0 16 56466 98 2 2351 0 05 26545 91 3 3704 0 10 77000 24 4 4 72 0 01 24134 30 reporting descriptive information about each sub area the number of trees abs freq the corresponding relative frequency rel freq which stands for p g and the sub area size area size i e t g then the outputs batty 1 13 07 rel batty 1 0 99 provide batty s entropy and its relative version respectively these results approach the maximum possible heterogeneity and are different from shannon s entropy values this happens because they take different perspectives batty s entropy says that with the given area partition rainforest trees are almost equally intensely distributed over the sub areas for plotting the data according to the area partition obtaining fig 3 the following code is used r plot unmark raintrees pch 16 cex 0 6 main r plot batty ent areas tess add true border 2 if the interest lies in focusing on a specific tree species e g chamguava batty s entropy can be computed by including the argument category into the command and by removing the function unmark r batty ent batty raintrees category cha2sc partition 10 batty 1 11 84 rel batty 1 0 90 in this case the entropy value is lower than the one referred to all species meaning that this particular species is not as evenly distributed as the overall dataset according to the area partition a more interesting approach is based on partitioning the area according to a covariate rainforest trees dataset is accompanied by the object raintreescov i e a list of two environmental covariates soil elevation and slope these variables may affect the degree of biodiversity of species therefore they can be used to define sub areas based on their values as an example let s consider slope which is a continuous variable and 4 intervals according to its values the categories of soil to derive these intervals are here obtained by using quantiles as breakpoints through the following code r slopecut cut raintreescov grad breaks quantile raintreescov grad probs 0 4 4 labels 1 4 discretize the covariate r maskv tiles list for ii in 1 nlevels slopecut r maskv ii as logical c slopecut v levels slopecut ii r tiles ii owin xrange data window xrange yrange data window yrange mask matrix maskv ii nrow slopecut v the map of the original covariate slope for the whole area is shown in fig 4 left panel and is produced by r par mfrow c 1 2 r plot raintreescov grad main col gray seq 1 0 l 100 fig 4 right panel shows the area partition based on the above discretization of the covariate slope together with species chamguava it is obtained by running the following code r plot slopecut main r plot split ppp raintrees cha2sc add true pch 16 cex 0 6 main once the partition is built the relative frequencies of trees are computed for each discretized level of the variable slope and batty s entropy is derived by using the same command batty and specifying the area partition the following commands refer to species chamguava r slopetess list tiles tiles n nlevels slopecut r batty raintrees category cha2sc partition slopetess batty 1 12 83 rel batty 1 0 98 interpretation is analogous to the former one but something more can be said here firstly the two partition options for batty s entropy on the same species show how sensible the measure is to the chosen area partition any interpretation must be carried out accounting for the choice in addition despite the evident tendency to spatial clustering species chamguava does not seem to have any dependence on the covariate slope its entropy computed over the slope partition is high close to the maximum denoting that trees grow evenly across all covariate levels a computation of batty s entropy under a covariate based partition can give hints on how biodiversity can be due to environmental covariates 3 2 results for batty s lisa entropy the lisa version of batty s entropy can be built in the package spatentropy by using the same commands as before to obtain an area partition of interest moreover this version needs a definition for the neighbourhood that can be done by choosing either the number of neighbours for each sub area or a neighbourhood distance measured over the sub areas centroids in the former case consider as an example the case of 3 neighbours a random partition in g 10 sub areas and the species chamguava then running the following command one obtains r kc ent karlstrom raintrees category cha2sc partition 10 neigh 3 karlstrom 1 1 88 rel karl 1 0 82 the following part of the output with details on the area partition is areas freq area id abs freq rel freq neigh mean area size 1 1 0 0 00 0 01 57581 30 2 2 0 0 00 0 01 54517 04 3 3307 0 56 0 20 101940 22 4 4 3 0 01 0 02 33719 95 that with respect to the function output of batty s entropy has one new column with the average frequencies over the neighbouring areas which stand for p g in the computations compared to batty s entropy given the area partition the consideration of the neighbouring probabilities decreases the relative entropy level other neighbourhood options may be tried for further comparison 3 3 results for leibovici s entropy leibovici s entropy refers to z i e the variable which defines co occurrences of rainforest trees as anticipated in section 2 3 o neill s entropy that was the first entropy measure based on such variable is unsuitable when point data are considered the computation of shannon s entropy of z for unordered pairs of rainforest trees r shannonz raintrees produces the same output of shannon but in this case the probability table reports the 10 frequencies of all unordered pairs of tree categories runs as probabilities pair abs freq rel freq 1 acaldiacaldi 3 584 503 0 14 2 cha2sccha2sc 147 696 0 01 3 in gapeingape 48 205 0 002 shannz 1 1 67 rel shannz 1 0 72 again the entropy value is far from the maximum value as can be seen from the relative version and hints at a data structure that cannot be captured without spatial information on the contrary leibovici s entropy which by default works with ordered couples and needs the specification of a distance of interest d among the trees forming each pair is much more appropriate an example with d 10 m follows the command is r leibovici raintrees ccdist 10 where the argument ccdist allows to specify the value of d in the same unit of measurement as the observation area metres in the case of raintrees results are in the output leib 1 1 12 rel leib 1 0 40 the value for leibovici s entropy for d 10 is quite low and different both from shannon s entropy and from batty s entropies they measure different aspects of entropy for leibovici s entropy a low value detects an important role of space for the heterogeneity of the tree species when the potential spatial relationship is measured up to 10 m this measure is very interesting but offers a picture limited to the chosen distance so that computations should be repeated for a number of different values for ds to get a general understanding of the data behaviour 3 4 results for altieri s set of entropy measures the recent set of entropy measures proposed by altieri et al 2018a defines an exhaustive framework which overcomes the drawbacks of leibovici s entropy this approach considers several ranges which cover all the observation window in order to gain information on the spatial data heterogeneity at all distances as a starting point the distance classes are chosen in this example 4 ranges are considered for the variable w as w 1 0 1 w 2 1 2 w 3 2 10 w 4 10 d max where d max 1118 03 in metres in coherence with the observation area then the spatial entropy measures are obtained by running the following code r outp altieri raintrees distbreak c 1 2 10 verbose true where the argument distbreak fixes the internal breaks of the distance classes and verbose controls whether information about the computation progress is printed in the output true or not false this function is the most sophisticated of the package and even if very efficient may take up to a few minutes for large datasets therefore we recommend to use the command verbose true in order to check the progress of the computation at the end of the process the generated output is very detailed first it provides the quantities used to compute the set of entropy measures distance breaks the distribution of the distance classes the number of pairs in total and per class and tables with the absolute and relative frequencies for all distance classes then the global values for spatial mutual information mi z w spatial residual entropy h z w and their sum i e shannon s entropy h z are returned the illustration of his part is omitted in the present text and we refer to the package manual for further details finally partial entropies are yielded in both absolute and relative terms spi terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 70 0 64 0 56 0 09 0 001 rel spi terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 39 0 37 0 33 0 05 0 0007 res terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 1 10 1 07 1 12 1 77 1 65 rel res terms class 0 1 class 1 2 class 2 10 class 10 100 class 100 1118 03 0 61 0 63 0 67 0 95 0 9993 the spi terms are the spatial partial information terms pi z w k and show the contribution of space to the data heterogeneity i e to biodiversity at each distance class the res terms are the partial residual entropies h z w k and measure the biodiversity of trees due to other sources of heterogeneity once space is controlled these quantities represent the most interesting output to interpret in particular focusing on each distance range and on relative versions rel spi terms and rel res terms where the two components sum to 1 for each distance range we can identify the role of space for instance for trees lying at most 1 m apart 39 of the heterogeneity is due to the spatial correlation i e to pairs of trees belonging to the same species conversely the remaining percentage 61 detects the part of entropy attributable to pairs of tree of different species i e the biodiversity level conditional on this distance the residual terms h z w k s increase when the distance ranges rise because the role of space naturally decreases at higher distances when they assume high and leading values also at lower distances this implies that data do not present a strong spatial structure and that we can expect high levels of biodiversity in small sites too therefore partial residual entropies represent the main goal as they measure biodiversity once the effect of space is controlled to conclude the several tools proposed by altieri et al assess a remarkable biodiversity across the rainforest trees of barro colorado island by exploring the phenomenon under a complete and very flexible framework 4 spatentropy in other environmental applications turin s urbanization study entropy measures are not only a standard tool in biodiversity studies but also may be widely employed in environmental applications such as landscape land cover and land use data as a further example of usage of the spatentropy package an environmental study for the binary grid data about urbanization in turin is considered as introduced in section 1 land cover data x are managed and dichotomized into urbanized x 1 1 and non urbanized x 2 0 zones i e pixels of size 250 250 m see fig 2 the aim is to measure the urban dispersion of the city by highlighting the differences in the use of both spatial entropy indices and spatentropy with respect to the previous application in biodiversity field the main functions already described in section 3 are only briefly cited in what follow a command is added regarding the computation of o neill s entropy since the concept of contiguity is well defined for grid data while it could not be used in the previous data example r shannon turin r shannonz turin r batty turin cell size 250 partition turintess r karlstrom turin cell size 250 partition turintess neigh 2 r oneill turin r leibovici turin cell size 250 ccdist 400 r altieri turin cell size 250 distbreak c 1 2 5 cell size the main results are collected in table 1 where in the left hand side the absolute entropy values are reported while the right hand side shows the relative versions which are straightforward to comment firstly the two non spatial shannon entropies are computed h x and h z these both take values close to their maximum log 2 and log 3 respectively identifying the metropolitan area of turin as a highly chaotic area the sub areas for the computation of batty s entropy are the administrative borders of the 15 municipalities forming the metropolitan area object turintess introduced in section 1 3 batty s lisa entropy is obtained by further considering the 2 4 or 12 nearest neighbours based on the pixels centroids the argument cell size to be ignored for point data defines the dimension of the pixels in the area measurement unit 250 250 m the relative versions of all indices can be compared batty s entropy is similar to batty s lisa entropy with the most extensive neighbourhood 12 neighbours while smaller neighbourhood systems decrease the level of entropy these values are below their maximum compared to shannon s entropies likely because the main municipality of turin is more intensely urbanized than the municipalities of its belt o neill s and leibovici s entropies can be commented together as the former is a special case of the latter when d equals the cell width in this case 250 m the option ccdist defines the distance between pixel centroids considered for building pairs three versions of leibovici s entropy are computed for d 400 1000 10 000 m o neill s entropy takes a relatively small value equal to 76 of its maximum meaning that when considering adjacent couples the different categories are not evenly distributed so the spatial configuration is not random by exploring the output of oneill which has a similar structure to the other functions outputs and in particular the table of relative frequencies the majority of couples are shown to be homogeneous of type urban urban 48 of the total or non urban non urban 39 of the total while heterogeneous couples are a minority consequently the index shows that the urban tissue tends to be compact with a different conclusion with respect to the traditional shannon s entropy as d increases the entropy value approaches its maximum as couples of pixels lying farther apart are included and the measure becomes more similar to shannon s entropy as regards altieri s set of entropy measures the spatial partial information terms pi h w k here represent the quantities of interest as they measure the urban dispersion or compactness for turin s area a high value for pi h w k especially at short distance ranges is a hint for a compact urban expansion which is a desirable feature as dispersed cities are proved to be inefficient and may impede sustainable development altieri et al 2019b we explore two options of distance ranges with respect to spatial information terms in order to show the sensitivity of results based on such choice the first one defines w as w 1 0 250 w 2 250 500 w 3 500 1250 w 4 1250 d max for the second option w is w 1 0 400 w 2 400 1000 w 3 1000 10000 w 4 10000 d max over the small distance ranges a relevant role of the spatial structure in determining the data heterogeneity can be appreciated that decreases for larger distances in particular focusing on the first distance class of the first option 0 250 i e pixels contiguity and on the relative version of pi h w k we quantify a 24 7 of entropy due to the spatial correlation if we move to the first class of the second option 0 400 this percentage is 22 4 the higher these values the closer we get to maximum urban compactness when medium distance ranges are considered e g 1000 10000 or 500 1250 the relative spatial partial information terms become small denoting the possibility of a future urban expansion in the metropolitan area as a conclusion this set of entropy measures reveals as altogether very exhaustive in explaining the urban structure of turin s metropolitan area which is characterized by a substantive and non negligible compactness of the urban patches at distances up to 1 1 25 km once more the package spatentropy efficiently supported the analysis in a rapid and suitable way the potential and applicability of this study on urbanization may be highlighted by works that compare entropy measures across urban areas over a region or country so that results can be commented both in absolute and relative terms altieri and cocchi 2021b 5 concluding remarks in this work the r package spatentropy is first introduced as the unique and efficient statistical tool to compute a large set of spatial entropy measures after a review of the measures available in the literature spatentropy is applied to real data from biodiversity and environmental fields in particular a first dataset on rainforest trees of barro colorado island has been considered in order to quantify the biodiversity of trees species over space the spatial entropy indices are then employed for measuring the urban sprawl of turin s metropolitan area the potential of the package we propose is shown on these examples in terms of computational efficiency and simplicity of use these features make spatentropy a suitable toolkit to be exploited also by non statisticians and new users the package spatentropy is able to easily deal with different kinds of variables and of spatial data the rainforest tree example refers to point data over an observation window with respect to a multicategorical variable the 4 species while urbanization data on turin are areal i e it considers a grid of pixels is considered and a binary variable urbanized non urbanized characterizes each pixel the ability of the package in managing space comes from the integration and exploitation of the widely known r package spatstat which also offers the possibility of experimenting spatentropy on additional available dataset point data are more precise than areal ones and this implies different ways of defining the neighbourhood we show how to fix distances for grids which are automatically built by spatentropy using pixel centroids multicategorical and dichotomic variables can be equally analyzed for both point and areal data the different spatial entropy measures can be easily built by the spatentropy package and then carefully compared as shown in the two applications sections we focus on the flexible and complete perspective offered by altieri s set of spatial entropy measures despite the approach of spatentropy implies a larger but still feasible computational effort results are very exhaustive and informative of the phenomenon at both global and local levels in particular we showed how for the rainforest tree data the leading quantities are the partial residual terms which increase when biodiversity of species rises at appropriate distance ranges i e by considering space in particular we observed a percentage equal to 61 of residual entropy at the smallest distance range up to 1 m denoting a remarkable biodiversity level even when considering small portions of the whole observation area conversely for land cover data on turin s urbanization the most informative quantities are the spatial partial terms measuring the degree of compactness of the metropolitan area which is a desirable feature for an efficient and sustainable development the city of turin is shown to have a compact urban structure the choice of the distance ranges depends on the research aim different options might be easily and quickly tested thanks to the package flexibility results can be enriched by a comparison to the findings on similar datasets e g other tree species or different cities exploiting the corresponding versions of all indices provided by the package other fields of applications cover any spatial dataset where syntethic measures of heterogeneity may be of interest they span over natural phenomena such as earthquakes altieri et al 2016 and wildfires leuenberger et al 2018 gelfand and monteiro 2014 polluting agents cameletti et al 2014 meteorological events de caceres et al 2018 and epidemiological data blangiardo et al 2016 greco and trivisano 2009 several dataset on these contexts of application are also available in the spatentropy package as additional toy examples declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25768,owing to the heterogeneity of geo analysis models many scholars and researchers have designed and promulgated standards in an attempt to address this however models based on different standards still cannot be shared and reused easily among different model frameworks for example models based on the openmi bmi and opengms is standards have heterogeneous development styles and formats so they cannot interoperate this article analyses the challenges faced when sharing and reusing models across different standards and provides a solution for model interoperation among them by mapping fields converting functions and reorganizing components our interoperability engine allows models that use one standard to be operated within a framework that supports a different standard this article discusses the developed interoperability method and provides case studies using e g swmm fds and the permamodel frost number component to successfully demonstrate model interoperation keywords model interoperation model standards interoperability engine openmi bmi opengms is 1 introduction modeling and simulation are important analytical methods in geographical and environmental research and various geoanalysis models have been developed and used for different domains such as hydrological ecological geological public healthy and coastal models serreze 2011 granell et al 2013 laniak et al 2013 chen et al 2015 tóth et al 2016 belete et al 2017a conde cid et al 2019 nourani et al 2019 sun et al 2019 wang et al 2019 shi and lin 2020 baig et al 2020 models are useful tools for the simulation of dynamic phenomena and processes analysing global regional geohypotheses and supporting decision policy making chen et al 2013 2020 2021 lin et al 2013a 2013b lin and chen 2015 lü et al 2019 belete et al 2017b ma et al 2021 with the development of geographical and environmental research the reuse of such models can help users replicate studies or reduce time to further develop a model thus existing models can help users integrate these models for geographical simulations and hence geographical and environmental sciences can accelerate by sharing and reusing models thereby reducing repetitive re work different researchers and domain communities use similar techniques to share models for reuse by using open source codes by sharing model executables or by making models sharable through web applications however the lack of universal model standards reduces model reuse between different domain communities and in some cases even within a domain community to address this problem an increasing number of model standards that rely on sets of different best practice techniques have been designed for model sharing owing to their effectiveness and accessibility component based models and service oriented standards are popular in model standard design goodall et al 2011 whelan et al 2014 both of these have advantages and therefore many platforms or groups implement these techniques at various levels the open modeling interface openmi community surface dynamics modeling system csdms and open geographic modeling and simulation opengms have their own respective standards and software platforms which have broad applications for model sharing within their user community but currently this does not happen across communities at any scale openmi has a set of standard interfaces for model sharing through the use of components that are linked together to form a composition gregersen et al 2007 harpham et al 2019 the basic model interface bmi is an open source library specification for plug and play components in the pymt framework designed by csdms peckham et al 2013 jiang et al 2017 opengms is an open platform for model sharing and reuse chen et al 2020 that has the interface standards opengms interface set opengms is which consists of a model encapsulation interface model description interface and sim task operation interface for model sharing by web services zhang et al 2020b each of these different model standards has been demonstrated to operate successfully by case studies and applications in geographical and environmental modeling openmi has been employed in many case studies in hydrological modeling including hydrological modeling systems and model integration bulatewicz et al 2010 castronova et al 2013 shrestha et al 2013 it is an open source software standard that enables model coupling that can be applied to a wide variety of models knapen et al 2013 it has been implemented in a variety of languages including c c java and matlab for both commercial models and those used for research openmi can also be applied to web services that support web processing to help users share and reuse models zhang et al 2020a csdms offers its community a platform through which open source models can be easily listed and discovered through detailed model descriptions to date a subset of the more than 380 listed open source models and tools in the model repository of csdms have been componentized following bmi standards hutton et al 2020 bmi enables the ability to couple models making it possible to simulate complex geographical or environmental processes drost et al 2020 opengms presents many models in opengms portal libraries that are wrapped by opengms is such as the soil and water assessment tool swat storm water management model swmm weather research and forecasting model wrf and more with the help of opengms is these models can be published as web services for sharing and reuse zhang et al 2019 2020b this enables geographical simulation applications for model users such as modeling online systems model integration and collaboration wang et al 2018 xiao et al 2019 chen et al 2019 yue et al 2020 when models and applications are based on one specific standard e g openmi bmi or opengms is these models and applications can be shared and reused with each other however openmi bmi and opengms is based models and applications have heterogeneous development styles and formats for sharing and are therefore not interoperable out of the box notwithstanding any subtle differences of approach and application a number of structural technical aspects prevent this automatic interoperability first these standards have different description fields and strategies to expose these fields model description information provides users with detailed information about the models themselves these description fields for the different standards are heterogeneous so the same information could be expressed in different fields or be communicated in a different manner for example for certain standards the model name field may be marked as component name such minor differences could mislead the user at the least presenting a barrier for model application and making the model incompatible with other platforms and standards each standard may also follow different description strategies fields in some standards are defined in a corresponding description file whereas other standards provide such information through an application programming interface api second different standards have different invoking methods that cannot interoperate with each other directly even if the same functions are used the parameters can be different for example bmi has a function named update for the next simulation step whereas openmi invokes the function getvalue to obtain the model simulation for the next time step both bmi and openmi are component based standards but opengms is is a service oriented standard for model sharing meanwhile different standards have heterogeneous formats for data input output i o for example bmi uses an in memory data stream for data i o and this data is communicated between model components following standard names that define variables in contrast model services in opengms use file transport for data exchange and the data format can follow universal data exchange udx for data mapping and conversion yue et al 2015 third file organizations and file dependencies are different for each standard different standards follow different development styles such as component based and service oriented styles and use different programming languages such as c python java a file used in one standard can be challenging to be reused for another standard which hinders the model invoking across different standards these three structural technical challenges make the base implementations of these different standards incompatible which restricts the reuse and integration of models between frameworks that use these different standards this research presents a solution for model interoperation among standards developed by csdms openmi and opengms the standards presented in this study are specifically developed for sharing and integration of models such a design is intended to help users reuse a model compatible with one standard that is not supported by another and thereby enrich the model repository of systems or applications based the collection of compatible standards for example with the help of the interopability engine the system based on opengms can use the models based on bmi or openmi thus this study would benefit users that prefer to use a particular model that contains a specific standard to be used in a framework that uses a different model standard due to the heterogeneity of these standards this research describes an interoperability engine to reuse models that are based on different standards the interoperability engine contains three modules 1 the field mapping module 2 the functions conversion module and 3 component reorganization module which can help users rewrap standardized models and reuse them in different standards the rewrapped model would keep the original fields and functions while following new standard proof of concept of model interoperations between models with different standards is provided through case studies using the models swmm permamodel frost number and fds the paper is organized as follows section 2 introduces the potential solutions for the model interoperation section 3 presents the design of the interoperability engine including field mapping function conversion and component reorganization section 4 introduces the implementation of the interoperability engine section 5 introduces the case studies of these model interoperation section 6 discusses the advantages and limitations of this research section 7 presents the conclusions and potential directions for future work 2 potential model interoperation solutions this section describes the engine designs to enable model interoperability between models of different standards three solutions are considered for engine design fig 1 presents the three conceptual solutions for engine development solutions a b and c the different shapes describe models that are based on three different standards standards α β and γ the interoperability engines that make the models that are based on different standards interoperable are indicated by lines with an arrow at each end solution a aims to build a set of engines to connect the models with each other as shown in fig 1 a every model following a standard should have an interoperability engine that makes a connection possible with models following a different standard in this solution if a model with a new standard is included the engines that make all former models interoperable should each be adjusted this entails significant work for each new model added to the system solution b describes a universal standard that includes all fields and functions among other standards and operates as a transfer station that connects all the standards as shown in fig 1 b models of each standard are interoperable by using the universal standard if a model is included from an additional standard then this model must be made compatible with the universal standard solution c also makes use of a universal standard however solution c differs from solution b in that this universal standard only remains part of the core and makes only use of rudimentary fields and functions that are essential in supporting interoperability the approach is extendable to unique fields and functions and supports the interoperability between them as shown in fig 1 c the basic universal standard is less elaborate than that of solution b and has other interoperability engines to ensure connections additionally the engine between two models that have different standards should only be developed when necessary solutions a and b both have advantages and limitations when dealing with a variety of model standards with solution a the engine that ensure interoperability will be elaborate and require many communication operations to handle data transfer between all of the models as model standardization is more detailed so the engine development work and complexity will increase without universal standards it becomes difficult to add new standards with this approach solution b is easier to implement new standards adapt to the universal standard to become interoperable with any standard that is compatible with the universal standard however this assumes the presence of a satisfactory universal standard with sufficient coverage across other standards and with a degree of future proofing to other standards that may come along this is clearly difficult to achieve for all fields and methods and may require a degree of foresight solution c achieves a balance between solutions a and b it maintains universal standards only for the basic fields and methods but still has an engine for interoperation between models that have different standards there is therefore a measure of the strengths and weaknesses of solutions a and b and it also requires an assessment of the aspects necessary for inclusion in the universal standard the basic fields and methods 3 engine design based on solution c fig 1 this study has designed and developed an interoperability engine for models that are based on openmi bmi and opengms is fig 2 first a basic universal standard is developed that can help users interoperate models based on the other standards the basic universal standard contains tables for description fields and functions which can support the interoperability engine design between the models that are based on different standards then the operations between every two standards are bidirectional so that there are two interoperation functions developed between each pair of standards for example the engine should two interoperation functions between openmi and opengms convert openmi based models to opengms and the reversed one the model that is initiating a call to another model is named the model in source standards mss while the model that has been called is named the model in target standards mts as shown in fig 3 the model interoperability engine is designed to convert mss to mts the engine design consists of three modules field mapping function conversion and component reorganization the field mapping module maps the description fields between the models in different standards the description fields of different standards are heterogeneous so fields in one standard may not exist in other standards therefore field mapping has two functions information transmitting and attribute retaining and completing information transmitting aims to transmit the matching information from mss to mts attribute retaining and completing is designed to preserve the fields and retain missing fields in the mts from the mss the function conversion module maps functions between the mss and mts to enable interfaces from the mts to invoke and interact with the mss this module consists of function linking and data and parameter exchanging function linking is used to link related functions together between every two standards to allow the mts to invoke or interoperate with the mss data and parameter exchanging is designed to convert the input output data parameter formats or content between every two standards due to the heterogeneity of the model components component reorganization reorders the file to reformat the model from the mss to the mts via component parsing and generating and file completing component parsing and generating is applied to parse the mss and generate the components of the mts after parsing file completion can supply necessary files in the mts that allow the model to be reused with the target standard 3 1 field mapping different standards have different fields to describe a model some are the same or similar such as model name brief description inputs and outputs but some are different a number of fields with the same meaning can be matched together between the mss and mts such as modelname title and modeldes info some fields cannot be matched as some cannot be found in either the mts or the mss moreover the methods used to obtain these fields are also different for some models the fields can be obtained from a file while for other models the information of fields can be obtained from the api therefore the heterogeneity of a model description will lead to misunderstanding and as such create a barrier for the reuse of models in different standards to address these problems this research designs a common field table for basic field mapping the lookup table is presented in table 1 the lookup table contains the basic fields for the three standards which makes it possible for models to map to different description fields with other standards fields or apis to obtain the corresponding information of the fields as shown in fig 3 the field modelname modeldes request and response in the mss are mapped to name description inputs and outputs in the common fields table then these fields can be mapped to the mts as title info inputs and outputs for the fields that differ between the mss and mts the method is designed to create supplement documents and preserved documents supplement documents are put in place to complete missing fields from the mss to the mts preserving documents can be used to retain useful fields missing in the mts that may be used in future as shown in fig 4 the field keywords and categories in the mss are missing in the target standard and these fields can be retained in the preserved documents in the mts meanwhile the field platform and dimensions cannot be found in the source standard so these can be completed by the supplement document the fields in preserving documents may not be used in the mts but they can be useful when the mts or the applications for mts are upgraded 3 2 function conversion the functions in these standards show strong heterogeneity and data in the i o of related functions also need conversion for interoperability purposes first the functions for invoking differ between standards for example in bmi the function update can be used to perform a time step in openmi the equivalent function is getvalues even if the functions can be converted the logic in the corresponding functions may differ the update function in bmi python does not set any parameters but getvalues in openmi can set time and linkid and obtain the corresponding result furthermore the data that need to be transported in functions can also differ as different standards have diverse requirements for data formats and content for example the opengms model service can be inputted with raw data files or streams or udx model based data models based on bmi should be inputted with data formatted as an array and each data point should be associated with one standard name yue et al 2015 jiang et al 2017 to address these issues a function conversion module has been designed that consists of linking functions and data exchange first function linking as shown in fig 5 has a common functions table table 2 for reference to link functions between standards different from the common fields table the function linking in the common functions table may experience cases in which one function can be linked with multiple other functions during different usage situations the reason for this problem is that the standards are used by different roles model wrappers and model users model wrappers would provide the model resource in specific standard and model users play the role of model customers for the simulations in the standards for components such as bmi and openmi the functions can be the same for wrappers and users however in the standards for services such as opengms the functions are different therefore in the table a function may be linked by two functions in different situations or a combination of two or more functions for example as shown in fig 4 the function init in the common functions table can be linked to init in the mss and initialize and prepare in the mts second data exchange as shown in fig 6 can help users convert the data parameter format or map the data parameter content from mts to mss the heterogeneity of data consists of format and content differences therefore the interoperability engine has the functions of format transformation and content mapping format transformation can help users convert the data format between standards for example the input data in opengms models can be a file such as tiff img csv etc and the input for models based on bmi should be provided as arrays if opengms wants to interoperate with a bmi model it should convert the file into an array content mapping aims to reorganize the data content to fit the data to the target standard for example the i o data in openmi may be configured in the arguments together but do not distinguish input and output which is necessary for other standards therefore the interoperability engine should supply a function to map the data in the mts to input and output data to the mss 3 3 component reorganization a model following a specific standard often has a fixed set of dependency files such as dynamic link library dll shared library so or python module py files to support invoking however the file organization between mss and mts is more heterogeneous for example the model organization between the models wrapped by bmi and opengms is different the former needs the file that supports bmi component and the later needs opengms related files for service generating therefore the files in mss are reorganized such that they follow the target standard when the mts needs to interoperate the mss the component reorganization module in the engine has functions for component parsing and generating and file completing first the engine parses the mss to perform the operation for file completion owing to the differences of a model when applied to different standards the components of models have different development styles to represent a model some are plug and play components others are web services there the engine for the different standards have diverse strategies for parsing after file completion with following the target standard the model is rewrapped as mts by the engine then as shown in fig 7 to ensure that the mss can be invoked all the files from the mss should be retained meanwhile the mss should append some other files to support the model reusability for the target standard these files include a wrapper file that includes the dependency files for target standard the wrapper file that is appended in the mts can help the model interoperate with the mss solve functions 4 implement to validate the design in this research an interoperability engine was constructed based on the aforementioned design principles the engine between the mss and the mts is bidirectional which means that there should be at least two functions between every two standards even if models follow the same standard they could have different development styles with different methods for interoperation two sub engines are developed among openmi bmi and opengms is owing to the different development styles these engines have different methods to fit different kinds of models to the source standard this study discusses several functions for the interoperation among these standards the collection of engines is shown in table 3 there are four functions for the two pairs of standards two functions each the engine is developed in python and the engine only contains the interoperation of openmi opengms and bmi opengms owing to the programming hetereogenity of different standards some parts of these engines would use other languages to meet the requirement of standards such as the component of openmi which uses c the rules for field mapping and function conversation can be referred in supplementary information appendix a and b 4 1 opengms openmi the function opengms openmi is designed to convert the opengms model service to the openmi component as shown in fig 8 at first this engine parses the apis of the opengms service and maps the attributes in model services to the omi file by a model service address all the input and output data in the opengms model service are mapped as arguments in the omf file the missing fields are supplied by the supplement document such as component name and data then the engine generates the template file for function conversion between opengms service and openmi component finally the engine copies related resource files and dependency libraries to the component as models in opengms are web services the engine would not read the model attributes directly it would link the related api to the interface in openmi so the attributes displayed in the converted openmi component are dynamic in this case the openmi components use c so the engine for openmi opengms is is developed in c and python the python is used for api and c is used for the template file 4 2 openmi opengms the function openmi opengms is designed to wrap the openmi component to an opengms service as shown in fig 9 first with the help of opengms sdk the engine parses the apis of the openmi component to fields and functions the fields parsed from the apis and the supplied fields in supplement document are mapped in a mdl document the functions in openmi component are then converted to the opengms wrapping interface to a wrapping file which is generated by a configuration file and an entry template file the configuration file is used to indicate the input data and output data and the entry template file can be converting the corresponding wrapping file of the opengms model service package then with the help of the mdl document the wrapping file and the related dependency files the engine reorganizes the component as opengms model service package which can be deployed as an opengms model service 4 3 opengms bmi the function opengms bmi is designed to convert opengms model services to bmi components as shown in fig 10 the engine parses the apis of opengms model services then the engine records the basic fields of the component including service s ip port and id and converts related functions similar to the function opengms openmi the fields of the converted bmi component are dynamic then the engine generates a bmi component by a template file the template file has established rules for function conversion finally the engine appends the bmi necessary files with dependency files and generates a bmi component in the bmi component the data i o in bmi is mapped as an array stream and each i o has a standard name therefore in the conversion of data each i o would be formatted as data files and have a specific parameter name 4 4 bmi opengms the function bmi opengms is designed to convert bmi components to the opengms model service as shown in fig 11 first the engine parses the bmi component and map the fields to the mdl document any missing fields are added by the supplement document like e g running environment information then the engine converts the functions to the opengms wrapping interface by using the template file and generate a wrapping file the data i o in opengms are transferred by files and the data i o in bmi are transferred by a stream therefore the engine would generate a temporary file for data i o finally the engine reorganizes the file as an opengms model service package by the mdl document file and the wrapping file 5 case studies for this study we used different geo analysis models to validate the effectiveness of developed interoperability engines the models are shown in table 4 the permamodel frost number is a model component developed by nelson and outcalt to calculate frost number in permafrost which is a dimensionless ratio based on freezing and thawing degree days in the year swmm model is a model developed by u s environmental protection agency epa to simulate rainfall runoff for an urban area fire dynamic simulators fds is a model developed by u s national institute of standards and technology nist to simulate indoor fire disasters as shown in figs 12 and 13 we use swmm and fds model services to validate the interoperability engine between opengms and openmi for this study as shown in fig 12 a and b the interoperability engine interoperates swmm openmi component as model service then as shown in fig 12 c we use the network test data inp to invoke the swmm opengms service and obtain the result as shown in fig 13 a and b the interoperability engine interoperates fds model service as an openmi component then as shown fig 13 c we use the test data fds to invoke the fds model service by openmi as shown in figs 14 and 15 we use the permamodel and fds model services to validate the interoperability engine between opengms and openmi for this study as shown in fig 14 a and b the interoperability engine interoperates the permamodel frost number component as a model service then as shown in fig 15 c we use the test data to invoke permamodel forestnumber opengms service and obtain the result as shown in fig 13 a and b the interoperability engine interoperate fds model service as a bmi component then as shown fig 13 c we use the test data fds to invoke the fds model service by bmi 6 discussion 6 1 standards selection openmi bmi and opengms is are supported by different communities or groups but all have a focus toward developing or implementing models although when developing models these communities use different technological styles and usage logics they have broad applications in different domains such as hydrology land atmosphere etc openmi has a component based style and pays more attention to model integration in simulations models based on bmi are supported by the csdms community they also use component based models where components written in different languages can be wrapped opengms is is the interface standard applied in opengms and it aims to share and reuse models in an open web environment so models with opengms is are services on the web with wide application and different developing styles of these standards in the simulation for different domains they can be good paradigms for model interoperation among standards in addition to the heterogeneities of these standards they also share common ground for example each standard has a model description for introduction and a statement for the model s input output so the common tables including common fields table and common functions table can be generated for interoperation among the different standards however the tables are only applied among openmi bmi and opengms is therefore if any new standards are introduced the tables can be reconsidered and reorganized such as via field or function appending 6 2 wrapping or linking owing to different developing styles the interoperability engine follows different approaches for the model interoperation among different standards as shown in fig 16 there are two approaches to design interoperability engines wrapping and linking wrapping as shown in fig 13 a means that the engine wraps the model with completed dependency files as model components in a new standard such as openmi or bmi component to opengms model service in contrast as shown in fig 13 b linking means that the engine utilizes mts to link the mss on the web and in the model in mts it would not have or just partly have model files in the source standard such as opengms model service to an openmi or bmi component compared with linking wrapping does not change the original model and its components and a change in the raw mss would not influence the model in the mts thus wrapping is more suitable for component based models i e bmi and openmi however linking only links the functions between the mss and mts and any changes in the mss would change or even destroy the target model thus a linked connection might be more flexible and lighter to make models interoperable and might be more suitable for service oriented models i e opengms 6 3 liebig s law in the model interoperation comparison with the native model and the mss the mts always has fewer fields and functions for example the opengms model service doesn t include the related grid information of input data of the bmi model component after the model interoperation we found that the mts followed the liebig s law in fields and functions transferred from native models liebig s law also called the liebig law of the minimum states that growth is not determined by the total available resources but by the scarcest resources i e the limiting factor danger et al 2008 this law is applicable to interoperability among standards owing to the limitations of standards and the native models the numbers of description fields and available functions decreases from the native model to the mts as shown in fig 17 once the native model is wrapped in mss owing to the limitations of the wrapping standard field b field d and function 2 are missing when the model is made interoperable with mts in addition to field b field d and function 2 function 3 becomes also unavailable thus all the functions in the final standard are determined by the intersection among native models and standards 6 4 models independency of standards after model interoperation some models may still not be interoperable given the new standards that is models that are using the same standard could still be coded using a different architecture owing to the different habits or development styles of researchers and scholars despite using the same standard these models could still need additional files or components for invoking for example models can be part of a special system or framework and as such require dependency files to run in the dependency of models some are based on standards or system libraries such as system dynamic link libraries or bmi interface files however some dependencies are customized for special application such as personal library files dll py these models are tightly coupled in a framework and are no longer independent components that can be reused in other systems or frameworks therefore although they follow the same standard dependent models cannot be shared and reused in other systems that follow specific standards 7 conclusions and future work this research analyzed the interoperability of models that are developed using different standards by comparing three potential solutions to make models more interoperable this research offers a suitable solution for model interoperation among different standards openmi bmi and opengms is the solution includes a design that consists of three modules for field mapping function conversion and component reorganization to interoperate models across these standards by means of this design this research developed an interoperability engines among openmi bmi and opengms is that can reuse models across these standards this research used models including swmm permamodel fds and fvcom to demonstrate that such a design can be helpful in model interoperation among different standards finally some key points for model interoperation are discussed including model selection and interoperation approaches the presented work also identified certain limitations the engines presented in this paper are tight couplings which makes them more difficult for reuse with the help of a basic universal standard the engine development is simpler but there is still much work needed to develop each engine with more model standards being developed the need for engines is increasing as well therefore reusable and plug and play components for engine development are necessary to grant interoperability between different model standards different standards have different rules for their data exchange within models openmi has interface iexchangeitem for data exchanging when users link different models bmi has standard names for their input and output data exchanging in different models opengms also has udx for data exchanging in model integration this research presented a data exchanging method for format transmitting and content mapping which can be helpful for data exchanging among different standards however owing to heterogeneity of data exchange rules the data in models can be problematic when reused in other models so a set of data preprocessing or post processing methods should be provided in data exchanging the engine can be customized in different modules for the engine design based on the designed universal standards the modules in the engine design can be reused in other engines the modules including field mapping function conversion and component reorganization in the engine between two standards have something in common so it can be a base class supporting the design of all modules more standards could be incorporated into the method to make models interoperable as described in this paper thereby extending the community of interoperability this research only presented three standards for model interoperability but there are more standards for model sharing and reuse that need to be considered the basic universal standard as presented in this research would be extended accordingly the design of these standards benefits from flexibility to enable them to be incorporated into this method software availability software name model interoperable engine developer fengyuan zhang min chen year first official release 2020 hardware requirements pc system requirements windows linux mac program language python3 7 program size 5 mb availability https github com franklinzhanggis model interoperable engine license mit documentation https github com franklinzhanggis model interoperable engine blob master readme md declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the detailed suggestions and comments from the anonymous reviewers we express heartfelt thanks to the other members of the opengms team this work was supported by nsf of china grant numbers 41930648 42071363 42071361 41871285 and u1811464 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105164 
25768,owing to the heterogeneity of geo analysis models many scholars and researchers have designed and promulgated standards in an attempt to address this however models based on different standards still cannot be shared and reused easily among different model frameworks for example models based on the openmi bmi and opengms is standards have heterogeneous development styles and formats so they cannot interoperate this article analyses the challenges faced when sharing and reusing models across different standards and provides a solution for model interoperation among them by mapping fields converting functions and reorganizing components our interoperability engine allows models that use one standard to be operated within a framework that supports a different standard this article discusses the developed interoperability method and provides case studies using e g swmm fds and the permamodel frost number component to successfully demonstrate model interoperation keywords model interoperation model standards interoperability engine openmi bmi opengms is 1 introduction modeling and simulation are important analytical methods in geographical and environmental research and various geoanalysis models have been developed and used for different domains such as hydrological ecological geological public healthy and coastal models serreze 2011 granell et al 2013 laniak et al 2013 chen et al 2015 tóth et al 2016 belete et al 2017a conde cid et al 2019 nourani et al 2019 sun et al 2019 wang et al 2019 shi and lin 2020 baig et al 2020 models are useful tools for the simulation of dynamic phenomena and processes analysing global regional geohypotheses and supporting decision policy making chen et al 2013 2020 2021 lin et al 2013a 2013b lin and chen 2015 lü et al 2019 belete et al 2017b ma et al 2021 with the development of geographical and environmental research the reuse of such models can help users replicate studies or reduce time to further develop a model thus existing models can help users integrate these models for geographical simulations and hence geographical and environmental sciences can accelerate by sharing and reusing models thereby reducing repetitive re work different researchers and domain communities use similar techniques to share models for reuse by using open source codes by sharing model executables or by making models sharable through web applications however the lack of universal model standards reduces model reuse between different domain communities and in some cases even within a domain community to address this problem an increasing number of model standards that rely on sets of different best practice techniques have been designed for model sharing owing to their effectiveness and accessibility component based models and service oriented standards are popular in model standard design goodall et al 2011 whelan et al 2014 both of these have advantages and therefore many platforms or groups implement these techniques at various levels the open modeling interface openmi community surface dynamics modeling system csdms and open geographic modeling and simulation opengms have their own respective standards and software platforms which have broad applications for model sharing within their user community but currently this does not happen across communities at any scale openmi has a set of standard interfaces for model sharing through the use of components that are linked together to form a composition gregersen et al 2007 harpham et al 2019 the basic model interface bmi is an open source library specification for plug and play components in the pymt framework designed by csdms peckham et al 2013 jiang et al 2017 opengms is an open platform for model sharing and reuse chen et al 2020 that has the interface standards opengms interface set opengms is which consists of a model encapsulation interface model description interface and sim task operation interface for model sharing by web services zhang et al 2020b each of these different model standards has been demonstrated to operate successfully by case studies and applications in geographical and environmental modeling openmi has been employed in many case studies in hydrological modeling including hydrological modeling systems and model integration bulatewicz et al 2010 castronova et al 2013 shrestha et al 2013 it is an open source software standard that enables model coupling that can be applied to a wide variety of models knapen et al 2013 it has been implemented in a variety of languages including c c java and matlab for both commercial models and those used for research openmi can also be applied to web services that support web processing to help users share and reuse models zhang et al 2020a csdms offers its community a platform through which open source models can be easily listed and discovered through detailed model descriptions to date a subset of the more than 380 listed open source models and tools in the model repository of csdms have been componentized following bmi standards hutton et al 2020 bmi enables the ability to couple models making it possible to simulate complex geographical or environmental processes drost et al 2020 opengms presents many models in opengms portal libraries that are wrapped by opengms is such as the soil and water assessment tool swat storm water management model swmm weather research and forecasting model wrf and more with the help of opengms is these models can be published as web services for sharing and reuse zhang et al 2019 2020b this enables geographical simulation applications for model users such as modeling online systems model integration and collaboration wang et al 2018 xiao et al 2019 chen et al 2019 yue et al 2020 when models and applications are based on one specific standard e g openmi bmi or opengms is these models and applications can be shared and reused with each other however openmi bmi and opengms is based models and applications have heterogeneous development styles and formats for sharing and are therefore not interoperable out of the box notwithstanding any subtle differences of approach and application a number of structural technical aspects prevent this automatic interoperability first these standards have different description fields and strategies to expose these fields model description information provides users with detailed information about the models themselves these description fields for the different standards are heterogeneous so the same information could be expressed in different fields or be communicated in a different manner for example for certain standards the model name field may be marked as component name such minor differences could mislead the user at the least presenting a barrier for model application and making the model incompatible with other platforms and standards each standard may also follow different description strategies fields in some standards are defined in a corresponding description file whereas other standards provide such information through an application programming interface api second different standards have different invoking methods that cannot interoperate with each other directly even if the same functions are used the parameters can be different for example bmi has a function named update for the next simulation step whereas openmi invokes the function getvalue to obtain the model simulation for the next time step both bmi and openmi are component based standards but opengms is is a service oriented standard for model sharing meanwhile different standards have heterogeneous formats for data input output i o for example bmi uses an in memory data stream for data i o and this data is communicated between model components following standard names that define variables in contrast model services in opengms use file transport for data exchange and the data format can follow universal data exchange udx for data mapping and conversion yue et al 2015 third file organizations and file dependencies are different for each standard different standards follow different development styles such as component based and service oriented styles and use different programming languages such as c python java a file used in one standard can be challenging to be reused for another standard which hinders the model invoking across different standards these three structural technical challenges make the base implementations of these different standards incompatible which restricts the reuse and integration of models between frameworks that use these different standards this research presents a solution for model interoperation among standards developed by csdms openmi and opengms the standards presented in this study are specifically developed for sharing and integration of models such a design is intended to help users reuse a model compatible with one standard that is not supported by another and thereby enrich the model repository of systems or applications based the collection of compatible standards for example with the help of the interopability engine the system based on opengms can use the models based on bmi or openmi thus this study would benefit users that prefer to use a particular model that contains a specific standard to be used in a framework that uses a different model standard due to the heterogeneity of these standards this research describes an interoperability engine to reuse models that are based on different standards the interoperability engine contains three modules 1 the field mapping module 2 the functions conversion module and 3 component reorganization module which can help users rewrap standardized models and reuse them in different standards the rewrapped model would keep the original fields and functions while following new standard proof of concept of model interoperations between models with different standards is provided through case studies using the models swmm permamodel frost number and fds the paper is organized as follows section 2 introduces the potential solutions for the model interoperation section 3 presents the design of the interoperability engine including field mapping function conversion and component reorganization section 4 introduces the implementation of the interoperability engine section 5 introduces the case studies of these model interoperation section 6 discusses the advantages and limitations of this research section 7 presents the conclusions and potential directions for future work 2 potential model interoperation solutions this section describes the engine designs to enable model interoperability between models of different standards three solutions are considered for engine design fig 1 presents the three conceptual solutions for engine development solutions a b and c the different shapes describe models that are based on three different standards standards α β and γ the interoperability engines that make the models that are based on different standards interoperable are indicated by lines with an arrow at each end solution a aims to build a set of engines to connect the models with each other as shown in fig 1 a every model following a standard should have an interoperability engine that makes a connection possible with models following a different standard in this solution if a model with a new standard is included the engines that make all former models interoperable should each be adjusted this entails significant work for each new model added to the system solution b describes a universal standard that includes all fields and functions among other standards and operates as a transfer station that connects all the standards as shown in fig 1 b models of each standard are interoperable by using the universal standard if a model is included from an additional standard then this model must be made compatible with the universal standard solution c also makes use of a universal standard however solution c differs from solution b in that this universal standard only remains part of the core and makes only use of rudimentary fields and functions that are essential in supporting interoperability the approach is extendable to unique fields and functions and supports the interoperability between them as shown in fig 1 c the basic universal standard is less elaborate than that of solution b and has other interoperability engines to ensure connections additionally the engine between two models that have different standards should only be developed when necessary solutions a and b both have advantages and limitations when dealing with a variety of model standards with solution a the engine that ensure interoperability will be elaborate and require many communication operations to handle data transfer between all of the models as model standardization is more detailed so the engine development work and complexity will increase without universal standards it becomes difficult to add new standards with this approach solution b is easier to implement new standards adapt to the universal standard to become interoperable with any standard that is compatible with the universal standard however this assumes the presence of a satisfactory universal standard with sufficient coverage across other standards and with a degree of future proofing to other standards that may come along this is clearly difficult to achieve for all fields and methods and may require a degree of foresight solution c achieves a balance between solutions a and b it maintains universal standards only for the basic fields and methods but still has an engine for interoperation between models that have different standards there is therefore a measure of the strengths and weaknesses of solutions a and b and it also requires an assessment of the aspects necessary for inclusion in the universal standard the basic fields and methods 3 engine design based on solution c fig 1 this study has designed and developed an interoperability engine for models that are based on openmi bmi and opengms is fig 2 first a basic universal standard is developed that can help users interoperate models based on the other standards the basic universal standard contains tables for description fields and functions which can support the interoperability engine design between the models that are based on different standards then the operations between every two standards are bidirectional so that there are two interoperation functions developed between each pair of standards for example the engine should two interoperation functions between openmi and opengms convert openmi based models to opengms and the reversed one the model that is initiating a call to another model is named the model in source standards mss while the model that has been called is named the model in target standards mts as shown in fig 3 the model interoperability engine is designed to convert mss to mts the engine design consists of three modules field mapping function conversion and component reorganization the field mapping module maps the description fields between the models in different standards the description fields of different standards are heterogeneous so fields in one standard may not exist in other standards therefore field mapping has two functions information transmitting and attribute retaining and completing information transmitting aims to transmit the matching information from mss to mts attribute retaining and completing is designed to preserve the fields and retain missing fields in the mts from the mss the function conversion module maps functions between the mss and mts to enable interfaces from the mts to invoke and interact with the mss this module consists of function linking and data and parameter exchanging function linking is used to link related functions together between every two standards to allow the mts to invoke or interoperate with the mss data and parameter exchanging is designed to convert the input output data parameter formats or content between every two standards due to the heterogeneity of the model components component reorganization reorders the file to reformat the model from the mss to the mts via component parsing and generating and file completing component parsing and generating is applied to parse the mss and generate the components of the mts after parsing file completion can supply necessary files in the mts that allow the model to be reused with the target standard 3 1 field mapping different standards have different fields to describe a model some are the same or similar such as model name brief description inputs and outputs but some are different a number of fields with the same meaning can be matched together between the mss and mts such as modelname title and modeldes info some fields cannot be matched as some cannot be found in either the mts or the mss moreover the methods used to obtain these fields are also different for some models the fields can be obtained from a file while for other models the information of fields can be obtained from the api therefore the heterogeneity of a model description will lead to misunderstanding and as such create a barrier for the reuse of models in different standards to address these problems this research designs a common field table for basic field mapping the lookup table is presented in table 1 the lookup table contains the basic fields for the three standards which makes it possible for models to map to different description fields with other standards fields or apis to obtain the corresponding information of the fields as shown in fig 3 the field modelname modeldes request and response in the mss are mapped to name description inputs and outputs in the common fields table then these fields can be mapped to the mts as title info inputs and outputs for the fields that differ between the mss and mts the method is designed to create supplement documents and preserved documents supplement documents are put in place to complete missing fields from the mss to the mts preserving documents can be used to retain useful fields missing in the mts that may be used in future as shown in fig 4 the field keywords and categories in the mss are missing in the target standard and these fields can be retained in the preserved documents in the mts meanwhile the field platform and dimensions cannot be found in the source standard so these can be completed by the supplement document the fields in preserving documents may not be used in the mts but they can be useful when the mts or the applications for mts are upgraded 3 2 function conversion the functions in these standards show strong heterogeneity and data in the i o of related functions also need conversion for interoperability purposes first the functions for invoking differ between standards for example in bmi the function update can be used to perform a time step in openmi the equivalent function is getvalues even if the functions can be converted the logic in the corresponding functions may differ the update function in bmi python does not set any parameters but getvalues in openmi can set time and linkid and obtain the corresponding result furthermore the data that need to be transported in functions can also differ as different standards have diverse requirements for data formats and content for example the opengms model service can be inputted with raw data files or streams or udx model based data models based on bmi should be inputted with data formatted as an array and each data point should be associated with one standard name yue et al 2015 jiang et al 2017 to address these issues a function conversion module has been designed that consists of linking functions and data exchange first function linking as shown in fig 5 has a common functions table table 2 for reference to link functions between standards different from the common fields table the function linking in the common functions table may experience cases in which one function can be linked with multiple other functions during different usage situations the reason for this problem is that the standards are used by different roles model wrappers and model users model wrappers would provide the model resource in specific standard and model users play the role of model customers for the simulations in the standards for components such as bmi and openmi the functions can be the same for wrappers and users however in the standards for services such as opengms the functions are different therefore in the table a function may be linked by two functions in different situations or a combination of two or more functions for example as shown in fig 4 the function init in the common functions table can be linked to init in the mss and initialize and prepare in the mts second data exchange as shown in fig 6 can help users convert the data parameter format or map the data parameter content from mts to mss the heterogeneity of data consists of format and content differences therefore the interoperability engine has the functions of format transformation and content mapping format transformation can help users convert the data format between standards for example the input data in opengms models can be a file such as tiff img csv etc and the input for models based on bmi should be provided as arrays if opengms wants to interoperate with a bmi model it should convert the file into an array content mapping aims to reorganize the data content to fit the data to the target standard for example the i o data in openmi may be configured in the arguments together but do not distinguish input and output which is necessary for other standards therefore the interoperability engine should supply a function to map the data in the mts to input and output data to the mss 3 3 component reorganization a model following a specific standard often has a fixed set of dependency files such as dynamic link library dll shared library so or python module py files to support invoking however the file organization between mss and mts is more heterogeneous for example the model organization between the models wrapped by bmi and opengms is different the former needs the file that supports bmi component and the later needs opengms related files for service generating therefore the files in mss are reorganized such that they follow the target standard when the mts needs to interoperate the mss the component reorganization module in the engine has functions for component parsing and generating and file completing first the engine parses the mss to perform the operation for file completion owing to the differences of a model when applied to different standards the components of models have different development styles to represent a model some are plug and play components others are web services there the engine for the different standards have diverse strategies for parsing after file completion with following the target standard the model is rewrapped as mts by the engine then as shown in fig 7 to ensure that the mss can be invoked all the files from the mss should be retained meanwhile the mss should append some other files to support the model reusability for the target standard these files include a wrapper file that includes the dependency files for target standard the wrapper file that is appended in the mts can help the model interoperate with the mss solve functions 4 implement to validate the design in this research an interoperability engine was constructed based on the aforementioned design principles the engine between the mss and the mts is bidirectional which means that there should be at least two functions between every two standards even if models follow the same standard they could have different development styles with different methods for interoperation two sub engines are developed among openmi bmi and opengms is owing to the different development styles these engines have different methods to fit different kinds of models to the source standard this study discusses several functions for the interoperation among these standards the collection of engines is shown in table 3 there are four functions for the two pairs of standards two functions each the engine is developed in python and the engine only contains the interoperation of openmi opengms and bmi opengms owing to the programming hetereogenity of different standards some parts of these engines would use other languages to meet the requirement of standards such as the component of openmi which uses c the rules for field mapping and function conversation can be referred in supplementary information appendix a and b 4 1 opengms openmi the function opengms openmi is designed to convert the opengms model service to the openmi component as shown in fig 8 at first this engine parses the apis of the opengms service and maps the attributes in model services to the omi file by a model service address all the input and output data in the opengms model service are mapped as arguments in the omf file the missing fields are supplied by the supplement document such as component name and data then the engine generates the template file for function conversion between opengms service and openmi component finally the engine copies related resource files and dependency libraries to the component as models in opengms are web services the engine would not read the model attributes directly it would link the related api to the interface in openmi so the attributes displayed in the converted openmi component are dynamic in this case the openmi components use c so the engine for openmi opengms is is developed in c and python the python is used for api and c is used for the template file 4 2 openmi opengms the function openmi opengms is designed to wrap the openmi component to an opengms service as shown in fig 9 first with the help of opengms sdk the engine parses the apis of the openmi component to fields and functions the fields parsed from the apis and the supplied fields in supplement document are mapped in a mdl document the functions in openmi component are then converted to the opengms wrapping interface to a wrapping file which is generated by a configuration file and an entry template file the configuration file is used to indicate the input data and output data and the entry template file can be converting the corresponding wrapping file of the opengms model service package then with the help of the mdl document the wrapping file and the related dependency files the engine reorganizes the component as opengms model service package which can be deployed as an opengms model service 4 3 opengms bmi the function opengms bmi is designed to convert opengms model services to bmi components as shown in fig 10 the engine parses the apis of opengms model services then the engine records the basic fields of the component including service s ip port and id and converts related functions similar to the function opengms openmi the fields of the converted bmi component are dynamic then the engine generates a bmi component by a template file the template file has established rules for function conversion finally the engine appends the bmi necessary files with dependency files and generates a bmi component in the bmi component the data i o in bmi is mapped as an array stream and each i o has a standard name therefore in the conversion of data each i o would be formatted as data files and have a specific parameter name 4 4 bmi opengms the function bmi opengms is designed to convert bmi components to the opengms model service as shown in fig 11 first the engine parses the bmi component and map the fields to the mdl document any missing fields are added by the supplement document like e g running environment information then the engine converts the functions to the opengms wrapping interface by using the template file and generate a wrapping file the data i o in opengms are transferred by files and the data i o in bmi are transferred by a stream therefore the engine would generate a temporary file for data i o finally the engine reorganizes the file as an opengms model service package by the mdl document file and the wrapping file 5 case studies for this study we used different geo analysis models to validate the effectiveness of developed interoperability engines the models are shown in table 4 the permamodel frost number is a model component developed by nelson and outcalt to calculate frost number in permafrost which is a dimensionless ratio based on freezing and thawing degree days in the year swmm model is a model developed by u s environmental protection agency epa to simulate rainfall runoff for an urban area fire dynamic simulators fds is a model developed by u s national institute of standards and technology nist to simulate indoor fire disasters as shown in figs 12 and 13 we use swmm and fds model services to validate the interoperability engine between opengms and openmi for this study as shown in fig 12 a and b the interoperability engine interoperates swmm openmi component as model service then as shown in fig 12 c we use the network test data inp to invoke the swmm opengms service and obtain the result as shown in fig 13 a and b the interoperability engine interoperates fds model service as an openmi component then as shown fig 13 c we use the test data fds to invoke the fds model service by openmi as shown in figs 14 and 15 we use the permamodel and fds model services to validate the interoperability engine between opengms and openmi for this study as shown in fig 14 a and b the interoperability engine interoperates the permamodel frost number component as a model service then as shown in fig 15 c we use the test data to invoke permamodel forestnumber opengms service and obtain the result as shown in fig 13 a and b the interoperability engine interoperate fds model service as a bmi component then as shown fig 13 c we use the test data fds to invoke the fds model service by bmi 6 discussion 6 1 standards selection openmi bmi and opengms is are supported by different communities or groups but all have a focus toward developing or implementing models although when developing models these communities use different technological styles and usage logics they have broad applications in different domains such as hydrology land atmosphere etc openmi has a component based style and pays more attention to model integration in simulations models based on bmi are supported by the csdms community they also use component based models where components written in different languages can be wrapped opengms is is the interface standard applied in opengms and it aims to share and reuse models in an open web environment so models with opengms is are services on the web with wide application and different developing styles of these standards in the simulation for different domains they can be good paradigms for model interoperation among standards in addition to the heterogeneities of these standards they also share common ground for example each standard has a model description for introduction and a statement for the model s input output so the common tables including common fields table and common functions table can be generated for interoperation among the different standards however the tables are only applied among openmi bmi and opengms is therefore if any new standards are introduced the tables can be reconsidered and reorganized such as via field or function appending 6 2 wrapping or linking owing to different developing styles the interoperability engine follows different approaches for the model interoperation among different standards as shown in fig 16 there are two approaches to design interoperability engines wrapping and linking wrapping as shown in fig 13 a means that the engine wraps the model with completed dependency files as model components in a new standard such as openmi or bmi component to opengms model service in contrast as shown in fig 13 b linking means that the engine utilizes mts to link the mss on the web and in the model in mts it would not have or just partly have model files in the source standard such as opengms model service to an openmi or bmi component compared with linking wrapping does not change the original model and its components and a change in the raw mss would not influence the model in the mts thus wrapping is more suitable for component based models i e bmi and openmi however linking only links the functions between the mss and mts and any changes in the mss would change or even destroy the target model thus a linked connection might be more flexible and lighter to make models interoperable and might be more suitable for service oriented models i e opengms 6 3 liebig s law in the model interoperation comparison with the native model and the mss the mts always has fewer fields and functions for example the opengms model service doesn t include the related grid information of input data of the bmi model component after the model interoperation we found that the mts followed the liebig s law in fields and functions transferred from native models liebig s law also called the liebig law of the minimum states that growth is not determined by the total available resources but by the scarcest resources i e the limiting factor danger et al 2008 this law is applicable to interoperability among standards owing to the limitations of standards and the native models the numbers of description fields and available functions decreases from the native model to the mts as shown in fig 17 once the native model is wrapped in mss owing to the limitations of the wrapping standard field b field d and function 2 are missing when the model is made interoperable with mts in addition to field b field d and function 2 function 3 becomes also unavailable thus all the functions in the final standard are determined by the intersection among native models and standards 6 4 models independency of standards after model interoperation some models may still not be interoperable given the new standards that is models that are using the same standard could still be coded using a different architecture owing to the different habits or development styles of researchers and scholars despite using the same standard these models could still need additional files or components for invoking for example models can be part of a special system or framework and as such require dependency files to run in the dependency of models some are based on standards or system libraries such as system dynamic link libraries or bmi interface files however some dependencies are customized for special application such as personal library files dll py these models are tightly coupled in a framework and are no longer independent components that can be reused in other systems or frameworks therefore although they follow the same standard dependent models cannot be shared and reused in other systems that follow specific standards 7 conclusions and future work this research analyzed the interoperability of models that are developed using different standards by comparing three potential solutions to make models more interoperable this research offers a suitable solution for model interoperation among different standards openmi bmi and opengms is the solution includes a design that consists of three modules for field mapping function conversion and component reorganization to interoperate models across these standards by means of this design this research developed an interoperability engines among openmi bmi and opengms is that can reuse models across these standards this research used models including swmm permamodel fds and fvcom to demonstrate that such a design can be helpful in model interoperation among different standards finally some key points for model interoperation are discussed including model selection and interoperation approaches the presented work also identified certain limitations the engines presented in this paper are tight couplings which makes them more difficult for reuse with the help of a basic universal standard the engine development is simpler but there is still much work needed to develop each engine with more model standards being developed the need for engines is increasing as well therefore reusable and plug and play components for engine development are necessary to grant interoperability between different model standards different standards have different rules for their data exchange within models openmi has interface iexchangeitem for data exchanging when users link different models bmi has standard names for their input and output data exchanging in different models opengms also has udx for data exchanging in model integration this research presented a data exchanging method for format transmitting and content mapping which can be helpful for data exchanging among different standards however owing to heterogeneity of data exchange rules the data in models can be problematic when reused in other models so a set of data preprocessing or post processing methods should be provided in data exchanging the engine can be customized in different modules for the engine design based on the designed universal standards the modules in the engine design can be reused in other engines the modules including field mapping function conversion and component reorganization in the engine between two standards have something in common so it can be a base class supporting the design of all modules more standards could be incorporated into the method to make models interoperable as described in this paper thereby extending the community of interoperability this research only presented three standards for model interoperability but there are more standards for model sharing and reuse that need to be considered the basic universal standard as presented in this research would be extended accordingly the design of these standards benefits from flexibility to enable them to be incorporated into this method software availability software name model interoperable engine developer fengyuan zhang min chen year first official release 2020 hardware requirements pc system requirements windows linux mac program language python3 7 program size 5 mb availability https github com franklinzhanggis model interoperable engine license mit documentation https github com franklinzhanggis model interoperable engine blob master readme md declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the detailed suggestions and comments from the anonymous reviewers we express heartfelt thanks to the other members of the opengms team this work was supported by nsf of china grant numbers 41930648 42071363 42071361 41871285 and u1811464 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105164 
25769,sub daily rainfall observations are vital to help us understand model and adapt to changing climate extremes however gauge records often have quality issues for example due to equipment malfunctions and recording errors this paper presents a new open source quality control algorithm gsdr qc to identify these issues in hourly rainfall data along with an application of the algorithm to the global sub daily rainfall gsdr observational dataset the algorithm is based on 25 quality checks which are combined into a simple rule base to remove suspicious data the quality checks and rule base are adaptable to help incorporate local or regional information comparison with manually quality controlled gauge data shows that the procedure results in an overall improvement to the quality of the gsdr dataset a uk case study further demonstrates the performance of the gsdr qc procedure while showing how region specific data and understanding can be incorporated into the quality control process keywords quality control sub daily hourly precipitation observations 1 introduction sub daily rainfall observations are essential for improving our understanding of variability and changes in short duration rainfall patterns around the world allan et al 2020 barbero et al 2019 fowler et al 2021 lenderink and fowler 2017 li et al 2020 effectively harnessing these data is especially important in the context of changing risks from flooding and other hazards in line with a warmer and moister atmosphere under climate change a new generation of very high resolution convection permitting climate models project significant changes in the intensity duration frequency and spatial characteristics of short duration rainfall extremes with large implications for flood risk ban et al 2015 kendon et al 2017 2014 prein et al 2017 robust sub daily observational data are needed to assess these models and inform appropriate climate change adaptation strategies however records of observed rainfall are often subject to errors which may compromise data use if not corrected or removed these errors come from various types of gauge malfunction losses and tampering as well as recording transmission and processing issues e g durre et al 2010 qi et al 2016 identifying such errors in hourly rainfall data can be extremely difficult individual hourly values at any location can be very high and extreme rainfall from convective systems can be localised which can make it difficult to distinguish genuine extremes from gauge errors yet retaining true extremes during quality control is vital to ensure risks are not underestimated visual inspection and comparison to known rainfall records historic flood drought events and neighbouring gauges are thus often needed some examples of different errors in selected raw hourly rainfall records from lewis et al 2019 are shown in fig 1 the present study focuses on a range of gauge malfunction and recording errors in the raw gsdr such as streaks of repeated values and both sporadic and persistent suspiciously large values fig 1 of course other systematic longstanding and complex factors also affect the representativeness of rainfall data these factors include wind induced undercatch instrument and site limitations and changes and uncertainties in spatial and temporal variations of rainfall intensity e g mcmillan et al 2012 pollock et al 2018 sevruk 1996 sieck et al 2007 while many advances have been made in automated quality control for large datasets of climate observations existing procedures are difficult to apply to sub daily rainfall records like those in the gsdr one important reason for this is that many procedures have focused on daily rather than sub daily data e g durre et al 2010 hamada et al 2011 scherrer et al 2011 sciuto et al 2009 you et al 2007 compared with daily aggregations sub daily e g hourly rainfall data possess different statistical characteristics and patterns of spatiotemporal variability patterns that in fact better capture the essential character of precipitation trenberth and zhang 2018 695 however these differences mean that thresholds and other elements of existing quality control procedures are not directly transferable from daily to sub daily intervals indeed leading daily rainfall quality procedures e g durre et al 2010 are not customised to the particular errors typical in sub daily rainfall data in many regions for example sub daily records can include periods where daily aggregations have been incorrectly incorporated into hourly data blenkinsop et al 2017 these errors may take the form of uniform repetition or disaggregation e g fig 1a alternatively daily aggregations may be incorrectly assigned to individual hours i e 23 zeros or missing flags followed by a potentially non zero value e g fig 1c issues like these may be missed by checks operating only at the daily interval some progress has been made in automated quality control of sub daily rainfall data de vos et al 2019 kim et al 2009 lawrimore et al 2020 qi et al 2016 but wider applicability is often compromised by a number of factors in particular most procedures for both daily and sub daily intervals do not implement all of the recommendations of the world meteorological organisation wmo wmo guidance recommends that rainfall data be subject to constraint consistency spike rapid change flat line streak and domain tests wmo 2019 2018 1986 previous work has focused mainly on constraint and consistency checks by testing for outliers and spatial temporal and physical inconsistencies the small number of existing procedures for sub daily rainfall tend to vary in auxiliary data requirements to support these tests as well as the sophistication of spatial and physical consistency checks e g blenkinsop et al 2017 de vos et al 2019 kim et al 2009 lawrimore et al 2020 lewis et al 2018 qi et al 2016 some important tests for streaks daily accumulations and other issues may also be absent these limitations lead some data providers such as the german meteorological service deutscher wetterdienst dwd to warn that their sub daily rainfall products are of inferior quality to their daily products because they are checked less thoroughly dwd 2017 furthermore from a practical perspective most of the aforementioned work on sub daily quality control has tended to focus on specific regional or national datasets and applications far less emphasis has been placed on developing generally applicable adaptable or extensible methods implemented in open source software this paper thus presents a new systematic and open source quality control algorithm gsdr qc to identify potential errors in hourly rainfall data along with an application of the algorithm to the gsdr observational dataset as described in section 3 the quality control procedure builds on previous work geared towards uk hourly rainfall data blenkinsop et al 2017 lewis et al 2018 the quality control algorithm is built around 25 quality checks which are combined into a simple rule base to remove suspicious data section 3 the quality checks and rule base are designed to be adaptable in order to help incorporate local information if available and customise the checking procedure section 4 assesses the effectiveness of the quality control procedure as applied to the gsdr dataset with a detailed case study of the uk comparing the global procedure with a locally refined version 2 data 2 1 the global sub daily rainfall gsdr dataset the gsdr dataset lewis et al 2019 is the product of intensive efforts to identify and collate observed sub daily rainfall records from around the world in the european research council funded intense project intelligent use of climate models for adaptation to non stationary hydrological extremes blenkinsop et al 2018 the dataset currently consists of 24 394 rain gauges with an hourly or shorter measurement interval fig 2 this is an increase of 707 gauges primarily in brazil and new zealand over the initial version presented in lewis et al 2019 the average record length is 17 8 years but this varies substantially from 1 to 104 years with a few exceptions the earliest records begin in the 1950s all sub hourly records have been aggregated to an hourly interval for this study the majority of the data were collected from national hydrological and meteorological services with some data obtained from environmental agencies and researchers the remaining 32 of the dataset is taken from the isd smith et al 2011 quality control of data incorporated in the isd varies according to the protocols of different data providers some automated checks are undertaken by the isd team to flag but not remove suspicious data although a number of key checks such as spatial consistency are not undertaken dunn et al 2012 performed detailed quality control of a subset of the isd to improve its quality and usability but they focused entirely on non precipitation variables such that the quality of isd precipitation records is still not well established the raw gsdr dataset is highly variable in its spatial coverage record length completeness and quality the data have been recorded by different instruments in very different climates with no overall conventions for dealing with common problems such as missing data blockages recording errors spurious values and infilling there are also major variations in the degree of quality control already applied to the data for example the us hourly precipitation dataset comes with a set of quality flags which endeavour to indicate periods of missing data regional state record exceedances and accumulations due to melting of snow amongst other issues other network operators may use very simple procedures as exemplified by the uk case study in section 4 2 in contrast for many countries it is unclear exactly what has been checked or altered in the original data due to the absence of any time series of quality flags given the large number of diverse records in gsdr quality control methods appropriate to a wide range of issues are thus needed to check the data 2 2 global precipitation climatology centre gpcc database to assist with the spatial consistency checks introduced in sections 3 2 5 and 3 2 6 the quality control procedure draws on reference datasets of high quality daily and monthly precipitation data the reference datasets need to have high accuracy which precludes satellite or reanalysis data that are less accurate than gauge data a high quality radar dataset could potentially be used but this is not globally available instead we use the extensively quality controlled global precipitation climatology centre gpcc daily and monthly precipitation databases through a collaboration between the intense project and the gpcc the authors were able to conduct a comparative study at the gpcc using the daily data base owing to its data policy and to respect the copyright of the data providers the gpcc does not provide the station based data but the gridded precipitation analyses products however alternative sources of daily and monthly reference data could be chosen by users such as the global historical climatology network ghcn archives menne et al 2012 peterson and vose 1997 as well as regional or national datasets the gpcc collects its precipitation data from monthly climate reports and in near real time from synoptic reports through the wmo global telecommunication system gts schneider et al 2014 the gpcc has additionally collated data from national meteorological and hydrological agencies in around 190 countries as well as monthly and daily rainfall records held by the climatic research unit cru new et al 2000 1999 the ghcn and the food and agriculture organisation of the united nations fao 2001 the gpcc thus holds a very large amount of precipitation data with their monthly precipitation database containing data for over 122 000 stations records in the gpcc database are subject to extensive semi automatic quality control with automatic statistical checks and an additional manual qc by a team of experts of the data flagged as questionable in the automatic checks schneider et al 2014 which makes them highly suitable as reference data 2 3 climate indices the quality control procedure uses a selection of the climate indices defined by the expert team on climate change detection and indices etccdi frich et al 2002 zhang et al 2011 the indices used are r99p prcptot rx1day cdd and sdii see definitions in table 1 for the application of quality control to the gsdr dataset gridded versions of the required indices at the global scale were obtained from both the hadex2 donat et al 2013b and the ghcndex donat et al 2013a datasets both of these versions of the climate indices are based on underlying observed daily precipitation records that have undergone substantial quality control the hadex2 and ghcndex climate indices were first calculated for each station and then gridded using an interpolation method accounting for the spatial correlation structure of the station level indices both datasets are used in this qc method to maximise spatial coverage and increase the robustness of the qc thresholds there is greater coverage more quality control and homogenization of stations underlying the hadex2 dataset than for ghcndex but ghcndex is routinely updated and the underlying station data is more accessible than the hadex2 dataset the quality checks applying etccdi indices see section 3 2 use the maximum of the index time series for the grid box in which the relevant gauge is located the maximum was selected because the etccdi indices represent an average of multiple gauges within a grid box given the smoothing of spatial heterogeneity arising from the gridding process using the maximum index value reduces instances of unfair overly cautious data flagging compared with using a time varying index value in cases where a gauge does not fall within a grid box that has an etccdi value a value from a neighbouring grid box is used i e the maximum value in a 3x3 window centred on the gauge location use of a neighbouring grid box is indicated in the quality control output file in the case that index values from both hadex2 and ghcndex are available the larger of the two index values is adopted users of the quality control procedure could also source or calculate their own versions of the indices as exemplified in section 4 2 3 quality control procedure the gsdr qc quality control qc procedure builds on previous work focused on uk hourly rainfall records based on understanding of uk rainfall patterns and instrumentation blenkinsop et al 2017 developed 11 quality checks on hourly rainfall amounts and dry periods these checks included tests for values exceeding uk hourly and daily rainfall records daily and monthly accumulated totals entered into hourly series consecutive large values and frequent tips for tipping bucket rain gauge records lewis et al 2018 extended this procedure by introducing four additional spatial consistency checks against neighbouring gauges to identify erroneous dry spells and high values accounting for uk rainfall seasonality lewis et al also introduced a rule base to exclude values and periods of data likely to be in error according to the outcomes of the quality checks the lewis et al code was written in python r and matlab and was only freely available on request the gsdr qc procedure generalises and extends this work for application to hourly rainfall data across a range of geographical climatic and instrumental contexts and is presented as an open source python package available for installation the procedure consists of 3 stages 1 station metadata checks for accurate location data and duplicated stations 2 flagging of suspicious data using a set of time series checks 3 removal of suspicious periods in time series based on a set of user defined rules as per lewis et al 2018 the procedure is split in this way so that it can be tailored to user requirements indeed this approach enables users to test different rules and develop a customised rule base for their own applications as necessary this flexibility for refinement and sensitivity analysis is valuable given the intrinsic challenges of automatically determining true data errors in large datasets e g dunn et al 2012 durre et al 2008 3 1 station metadata checks manual checks the first quality control checks applied to the gsdr dataset examined the station metadata beginning with whether the reported latitude and longitude coordinates were plausible the reported coordinates were tested for consistency with a polygon shapefile delineating national borders any stations outside the territory of the expected country were then checked manually this check highlighted that some coordinates were indeed recorded incorrectly with the latitude and longitude switched the gsdr dataset was then assessed for duplicate gauges a gis query was performed to identify the nearest neighbour for each gauge and the distance between them any gauges less than 100 m apart were then inspected further gauges apparently duplicated often by gauges in the isd were flagged then checked to see if their data series were identical for the period of overlap both stations were kept if the data series were different for gauges with identical data the gauge with the longest effective record length record length x 1 proportion missing data was retained and any incorrect metadata was updated only a small number of gauges 100 were removed by metadata checks 3 2 time series checks automated checks the 25 time series checks that underpin the procedure are presented in table 2 these checks fall broadly into five groups as explained below the explanations below use the identifiers e g qc1 and names e g breakpoints of the checks given in table 2 3 3 suspect gauges the first group of checks qc1 qc7 seeks to identify gauge records in which substantial portions of the data or even the whole record could be suspicious the percentiles qc1 check returns two lists of years where the 95th and 99th percentiles of the hourly rainfall distribution are zero which corresponds to suspiciously low rainfall in different climate regions however as some regions are very dry we also conduct a k largest qc2 check in this check the hourly rainfall values are put in descending order separately for each year the check then returns the years where the 1st 5th and 10th largest hourly rainfall values are all equal to zero the k largest check thus helps to identify suspiciously low annual rainfall totals in all but the very driest climates very low annual totals may occur where missing values have been entered as zeros for example the data are also checked to see if there are any clear biases towards certain days of the week or hours of the day wilby et al 2017 the days of week check qc3 performs a two sided t test on the distribution of mean rainfall over the days of the week this helps to identify cases such as spuriously higher rainfall on mondays due to data being preferentially reported as a weekend accumulation similarly the hours of day check qc4 uses a two sided t test on the distribution of mean rainfall over the hours of the day to help indicate if daily accumulations of rainfall have been reported at 0900 or another standard time blenkinsop et al 2017 the final checks for suspect periods or records focus on data continuity and homogeneity generally patchy records i e numerous sequences of missing data are identified using the intermittency check qc5 which returns years where more than five periods of missing data are bounded by zeros the lack of continuity in the record highlighted by this check suggests that rainfall events may not have been properly recorded for example due to dry periods being recorded but not rainfall events the records are also checked for breakpoints qc6 using a pettit test which may indicate changes in station location or measuring equipment however a breakpoint could also be detected due to a real change in rainfall induced by climate change for example as such the results of the pettit test are tentative and supplied for the user s reference possible changes to the measuring equipment are also reflected by the minimum value change check qc7 which identifies how many times the minimum recorded value greater than zero changes from one year to the next 3 3 1 suspiciously high values the next group of checks qc8 qc11 focuses on identifying suspiciously high values by using a set of reference thresholds beginning with the annual interval the r99p qc8 and prcptot qc9 quality checks compare the annual gauge statistics against the maximum etccdi r99p and prcptot climate indices section 2 3 r99p indicates annual total precipitation from days where precipitation exceeds the 99th percentile of wet day precipitation while prcptot quantifies annual total precipitation from wet days 1 mm as detailed in table 2 a flag for each year is returned whose value depends on whether the gauge s annual total exceeds the maximum etccdi r99p or prcptot and if so by how much the world record qc10 check compares each individual hourly rainfall value against the world record hourly rainfall total different sources indicate different values for the world record hourly rainfall with values ranging from 305 mm to 401 mm noaa 2017 wmo 1994 to try not to remove data unnecessarily we selected the upper value 401 mm for quality control of the gsdr dataset additional confidence in flagging very high values is provided by the rx1day check qc11 each individual hourly value is compared against the corresponding maximum etccdi rx1day maximum daily precipitation value for the corresponding grid cell note that the check compares hourly values with rx1day rather than daily values again in order to account for the smoothing of spatial variation inherent in the gridded etccdi indices 3 3 2 long periods without rainfall in addition to the percentiles qc1 and k largest qc2 checks potentially suspicious long dry period are assessed using the etccdi cdd consecutive dry days index each run of dry days is compared to the maximum cdd value and flagged as suspicious if this threshold is exceeded this additional cdd check qc12 is intended to provide more confidence in the identification of periods in which missing data may have been recorded as zeros it is complemented by checks against neighbouring stations which are introduced in section 3 2 5 3 3 3 suspect accumulations and repeated values the daily accumulations qc13 and monthly accumulations qc14 checks look for day and month long periods of zeros followed by a very wet hour and then another spell of 23 zeros i e the rest of the day is dry the purpose is to highlight cases where rainfall accumulated over a full day or month is mistakenly associated with a single hour the checks use a threshold of double the etccdi sdii index for the grid cell underlying the gauge location to classify the wet hour as potentially spurious the sdii is an index of the average daily precipitation total on wet days experimentation showed that doubling sdii represented a reasonable general threshold this threshold was determined through trial and error testing multiples of sdii that gave a large enough value that was unlikely to exclude true hourly rainfall values but low enough to exclude suspicious values inspecting gauge time series showed that specifying that the wet hour needs to be followed by a period of 23 zeros strongly reduces the possibility of flagging genuinely large rainfall totals that happen to follow a legitimate dry period two additional elements are incorporated in the daily accumulations qc13 check to help identify long periods of systematic daily accumulations of any magnitude firstly periods of two or more days with any non zero potential daily accumulations are identified secondly periods of zeros in between potential daily accumulations are also flagged this helps to identify continuous periods of potential daily accumulations i e not only periods where a day receives rainfall in addition the qc procedure checks for repeated identical values based on several conditions in the streaks check qc15 first streaks of 2 or more consecutive hours of identical high rainfall values are flagged using double the etccdi sdii index as a threshold again it is considered unlikely that 2 or more hours of identical large values would occur by chance second streaks of 12 or more consecutive hours of identical values greater than the measurement resolution are also flagged third streaks of 24 or more repeated hourly values greater than zero are flagged i e very long streaks at the data resolution the different thresholds in the second and third components of the check were chosen to reduce the chance of wrongly identifying spells of drizzle as potentially erroneous finally the check flags periods of zeros bounded by streaks of 24 repeated values this helps to find periods of uniform disaggregation or repetition of daily totals 3 3 4 neighbouring gauge checks on large values the next set of checks qc16 qc17 use neighbouring gauges to help identify suspicious large rainfall values these checks are performed after aggregation of the hourly series to the daily interval neighbour checks are not currently conducted on an hourly basis due to the heightened intermittency and spatial variability at sub daily time intervals the daily neighbours wet qc16 check uses the quality controlled gpcc daily precipitation database section 2 2 as reference data while the hourly neighbours wet qc17 check uses neighbouring gauges in the gsdr dataset being checked after aggregation to the daily interval both checks use the same method a number of steps are undertaken in these neighbouring gauge checks which are explained in detail in the supplementary material section s1 in brief the method is based on the approach introduced by upton and rahimi 2003 and modified previously for application to uk hourly data blenkinsop et al 2017 lewis et al 2018 the checks involve the following steps 1 select the closest up to 10 neighbouring gauges within 50 km with sufficient overlap 3 years and close correspondence i e in wet dry matching statistics and correlation with the target gauge the check is not undertaken if there are insufficient gauges meeting these criteria 2 calculate a time series of differences between the target gauge and each neighbour after normalisation of each record to the interval 0 1 by rescaling between the series minima and maxima see equation s1 in the supplementary material 3 for each neighbouring gauge produce a time series of flags using the following approach a subset the differences on wet days where the target gauge exceeds the normalised value of the neighbouring gauge b fit an exponential distribution to the subset of differences c identify threshold difference values associated with the 95th 99th and 99 9th percentiles use these thresholds to flag exceedances i e notably large differences 4 combine the flags i e one series per neighbour into a single series of flags where the ultimate flag value is equal to the minimum flag value of all neighbours at each time step a lower flag value equates to more agreement between the neighbours and the gauge i e higher quality data the modifications relative to blenkinsop et al 2017 and lewis et al 2018 are thus primarily in 1 focusing on wet days with larger normalised rainfall at the target gauge and 2 fitting an exponential distribution to the normalised differences rather than assuming an approximately normal distribution these developments were undertaken because a preliminary analysis showed generally variable performance in a sample of non uk stations often through too many false positives i e incorrect flagging of plausible data outside of the climate where the method has been previously applied further explanation and justification is given in section s1 of the supplementary material 3 3 5 other neighbouring gauge checks additional checks against neighbouring gauges are the daily neighbours dry qc18 and hourly neighbours dry qc19 checks these tests examine the agreement between a target gauge and its neighbours on dry periods the neighbour selection follows the same procedure outlined in section 3 2 5 and the supplementary material section s1 the checks are again differentiated by use of the gpcc daily precipitation database qc18 and neighbouring gauges within the gsdr dataset being checked qc19 for a moving 15 day window if the target gauge shows no rainfall in the window the number of wet days 0 mm in the same window for each neighbour are counted and the average taken to receive the highest flag all neighbours must have three or more wet days in a 15 day window that is dry in the record of the target gauge successively lower flags are used if all neighbours corroborate the occurrence of a lower number of wet days table 2 the monthly neighbours check qc20 uses reference data from the quality controlled gpcc monthly precipitation database monthly neighbouring gauges are selected based on proximity only for months with more than 95 completeness the check proceeds by calculating the percentage difference in monthly totals between each neighbouring gpcc monthly gauge and the target gauge after aggregation to the monthly interval the percentage differences are grouped into classes table 2 and the flags associated with each of the neighbours are then combined into a single series of flags again all neighbours need to signal notably high or low monthly totals for the most severe flags to be invoked for example if the target gauge monthly total is more than double the totals of each of the active neighbouring gauges then a high flag is assigned see details in table 2 the final neighbour checks aim to support the identification of generally suspicious gauges by augmenting the tests discussed in section 3 2 1 the timing offset check qc21 computes the affinity index ai and correlation at the daily interval between the target gauge and its nearest neighbour in the gpcc database at lags of 1 0 and 1 day the ai is a matching statistic for wet and dry days a dry day is defined here as having 0 1 mm rain the lag associated with the highest values of the statistics is returned as an indication of whether or not there could be potential timing issues as part of the procedure we also calculate the pre qc affinity index qc22 and pre qc correlation qc23 relative to the nearest daily neighbour in the gpcc database as a means of identifying generally questionable records finally the daily factor qc24 and monthly factor qc25 checks provide the mean difference expressed as a factor between the target gauge and its nearest daily or monthly gpcc neighbour respectively to help identify possible unit errors 3 4 rule base the quality flags from the checks described in section 3 2 are combined into a rule base to remove suspect data points from the gauge records building on the approach described by lewis et al 2018 following the strategy guiding other automated quality control procedures e g dunn et al 2012 durre et al 2008 we iterated the rule base for the gsdr dataset to strike a reasonable balance between excluding erroneous data and retaining as many of the observations as possible judgements were taken based on reviewing samples of gauges from around the world iteration of the rules was also based on assessing the spatial patterns of data exclusion and the corresponding effects on key rainfall summary statistics these analyses led to the rule base configuration presented in table 3 the intention was to ensure that only a low percentage of reasonable values were removed incorrectly false positive rate in order to avoid removal of important extremes this configuration aims to support general usage of the gsdr dataset but given the difficulty of developing a set of checks and rules that work optimally in all situations a user may also test different thresholds and combinations of rules depending on their geographical and thematic focuses the rule base is not currently designed to identify or omit wholly suspicious gauges from the dataset rather it is designed to omit suspicious values within each gauge record the rule base uses almost all of the time series quality checks described in section 3 2 2 3 2 6 i e those checks returning hourly time series of flags as opposed to the checks that return either single flags or one flag per year it is therefore advisable to review the flags computed as indicators of whether a gauge may be generally suspicious i e checks described in section 3 2 1 and 3 2 6 before using the data for a given application to help the user identify suspect gauges the software produces an output summary table of all quality checks detailed in section 3 2 and table 2 and all rule base rules table 3 examples of the value of this summary information are given in section 4 3 5 software design and use the gsdr qc procedure is implemented in python it consists of two primary classes a gauge class and a qc class the gauge class handles station data series and metadata while the qc class contains the methods that implement the quality checks in section 3 2 with a gauge object the quality checks for a gauge are written to text file whose headers indicate overall quality flags from checks returning a single flag per gauge e g breakpoint test qc6 checks returning one flag per year e g prcptot check qc9 or a list of years e g percentiles check qc1 the time series quality flags e g neighbouring gauges checks qc16 20 are provided beneath the headers due to its comparative simplicity the rule base is implemented through a single function based on a pandas dataframe with a conditional statement for each rule an output summary table contains an overview of the operation of each quality check and rule at each gauge examples of the output files are given in section s2 of the supplementary material to exemplify the software usage in outline terms the following steps would be taken to quality control a target hourly gauge record 1 manually check the gauge metadata as described in section 3 1 2 create tables of basic metadata for any hourly daily and monthly gauges to be used in neighbouring gauges checks 3 create k d trees to identify the nearest neighbours for the gauges using create kdtree set of functions in the utils module 4 read in the target gauge data and metadata by calling the read intense function in the gauge module to create a gauge object 5 create an instance of the qc class using the gauge object step 4 and a small number of arguments including the metadata of neighbouring gauges from step 3 6 run the quality checks and write the flags to file using the get flags and write methods of the qc class 7 run the rule base and write output to file using the apply rulebase function in the rulebase module the code has been designed to be adapted by the user through its object oriented design each quality check in the qc class has its own method which allows a user with a small amount of python knowledge to create a new subclass according to their own requirements quality checks can therefore be changed omitted or added by the user for example in the uk case study below the rx1day qc11 test was modified to use a known regional threshold by overriding the rx1day check ts method as a python package the full code is also readily available if the user wants to make substantial changes to suit their needs the software is published as the intense python package with example data etccdi indices and tests further explanation of how to use the code is given in the documentation 3 6 evaluation three approaches were used to evaluate the gsdr qc procedure firstly the operation of the rule base was evaluated with reference to a sample of 300 manually checked gsdr gauges following durre et al 2008 the gauges were selected by randomly sampling 100 gauges from three large geographical domains 1 north and south america 2 europe and 3 asia and australasia only non isd gauges with over five years of record and more than 70 completeness were considered the sample was iterated to ensure a minimum of five gauges from each country with significant data records in each domain for each of the 300 gauges four 3 month periods were randomly chosen for detailed inspection these manual checks were performed by reviewing the hourly time series alongside longer daily and monthly time series mean annual cycles wet day hour statistics wet day hour amount distributions and neighbouring gauge series were also considered each hour was then classified as likely to be reasonable or erroneous using expert judgement the second evaluation approach was a comparison of daily time series between the gsdr gauges and their nearest gauges in the quality controlled gpcc daily database before and after application of the procedure rank correlation coefficients were calculated for gsdr gauges with a gpcc neighbour within 50 km and with at least three years of overlap in the record period the analysis in section 4 1 2 differentiates between those gsdr gauges where gpcc neighbours are used in the quality control procedure and those where they are not to gain further insight into gsdr qc performance in a final evaluation the uk was selected for a detailed case study based on the authors experience with these data and the uk climate 4 results 4 1 gsdr global dataset the analysis presented here focuses on the overall operation of the rule base as well as regional variations in data removals by the rule base this provides an indication of how much data were removed to arrive at a quality controlled version of the gsdr intended for general usage as noted in section 3 3 specific applications may benefit from further analysis of individual quality flags or specific rules in detail which we exemplify below 4 1 1 overall rule base usage application of the rule base to the raw gsdr dataset eliminated a total of 103 890 687 h of data 11 860 years across all of the 24 394 gauges in the dataset for the majority of gauges 87 of all gsdr gauges only small percentages 5 of the record period were removed by the rule base this is consistent with most of the gauges showing reasonable quality records overall but periods of errors due to temporary gauge malfunctions or recording errors that were resolved by the network operators however for some gauges much larger proportions of the time series were removed between 5 and 50 of data was removed for 10 of all gsdr gauges while between 50 and 100 of data was removed for 2 7 of all gsdr gauges especially gauges with very short time series this is reflected in fig 3 which shows the relationship between effective record length and the percentage of data removed by the gsdr qc procedure fig 3 highlights the low percentage of data removed by the rule base across all record lengths for the majority of gauges but that a higher proportion of short records have the largest suspicious data percentages fig 4 shows the number of gauges at which different rules in the rule base are invoked to remove suspicious data from time series the first rule r1 which is based on the k largest check qc2 is triggered at more stations than any other rule this indicates that a number of the raw records in gsdr contain years or partial years with erroneously little rainfall however it should be noted that in many cases this rule identifies long periods of missing data incorrectly entered as zeros it may also remove periods of zeros at the beginning end of data series this latter case may reflect either recording error or simply an often ambiguous partial year without rainfall prior to data series termination the two rules triggered at the next largest number of gauges are r9 and r10 which are invoked by the hourly neighbours dry qc19 and daily neighbours dry qc18 checks respectively both in conjunction with the cdd check qc12 however it should be noted that all three rules r1 r9 and r10 are fairly often triggered at the same time specifically r9 and r10 are both invoked on average on 27 of the same occasions on which r1 is incurred with 32 of the relevant gauges having such co occurrence greater than 50 this co occurrence provides additional confidence in the removal of these data as multiple checks using different data and approaches are often in agreement on suspiciously long dry periods i e there is consensus between the k largest cdd and neighbours dry checks using both gsdr gauges and gpcc gauges the rule r4 removing streaks of repeated values qc15 is also applied to a relatively large number of gauges an appreciable number of gauges thus show at least some periods of suspicious repeated values some of which are known to coincide with instances where daily values have been disaggregated or repeated as a surrogate for true hourly data e g fig 1a the rules r2 and r3 removing possible daily and monthly accumulations qc13 and qc14 are invoked at fewer stations along with the rule r5 checking for world record hourly rainfall exceedances there is notable co occurrence of the r5 world record rule with the rx1day rule r6 which is invoked on average on 75 of the same occasions as r5 4 1 2 rule base evaluation as described in section 3 5 an indication of the overall performance of the gsdr qc rule base can be obtained by comparing its data removal and retention with manual quality control for a sample of gsdr gauges this comparison reveals that the gsdr qc removes or retains data correctly in 99 0 of the 2 425 829 sampled hours relative to manual quality control the accuracy is slightly higher if just wet hours are considered in this case 99 6 of 200 541 hours are classified correctly i e as either reasonable or erroneous by the procedure further insight is gained by considering the proportion of the time that real errors identified manually are classified correctly as errors or incorrectly as reasonable values by gsdr qc similar proportions can be calculated for the correct and incorrect classification of values manually determined to be reasonable the associated confusion matrix is summarised in table 4 for the full sample of all 2 425 829 dry and wet hours the results indicate that 99 6 of the hours considered reasonable in the manual checks were also retained by the gsdr qc rule base therefore only a small proportion 0 4 of reasonable values were removed incorrectly in addition the rule base correctly removes 71 0 of the real errors identified during manual inspection this means that 29 0 of real errors were wrongly retained see discussion of error characteristics in section 5 although it may be noted that real errors only comprise a small proportion 2 4 of the full sample hours table 4 indicates that the performance increases slightly if the sample is restricted to wet hours given the importance of balancing true false positives negatives the performance relative to manual checks is good fig 5 compares rank correlation coefficients for daily time series between the gsdr gauges and their nearest gauges in the quality controlled gpcc daily database before and after application of the procedure see details in section 3 5 fig 5 is also split into two panels to differentiate 1 gauges where flags from gpcc neighbouring gauges checks were explicitly used to remove data in the rule base fig 5a and 2 those gauges where they were not fig 5b the latter gauges in fig 5b provide a more but not entirely independent test of gsdr qc performance with many gauges in fig 5b not having used gpcc data due to insufficient neighbours and weak relationships with the target gauge unfortunately it is difficult to take the comparison further through full cross validation tests as aspects of the procedure would need to be switched off to entirely avoid using the gpcc data in both quality control and evaluation almost all of the points in fig 5 lie on or to the left of the 1 1 line this indicates that where correlation differs before and after the procedure almost all gsdr gauges show improved correlation with their nearest gpcc neighbours this suggests an overall improvement in data quality following the gsdr qc procedure with 40 of gauges with a gpcc neighbour showing larger correlation after the procedure it is also clear that many gauges lie close to the 1 1 line indicating little change in the correlation coefficient for numerous gauges such limited changes may be expected in the many cases where only small proportions of the gsdr gauge time series are removed by the rule base often due to the high quality of the original data series 4 2 regional variation fig 6 suggests that the gauges where large proportions of the data series were removed tend to be found in a few regions for example the procedure removed 50 of the time series for 519 gauges in the us this represents 7 6 of the total number of gsdr gauges 6834 in the us although 86 of these 519 gauges are less than 5 years in length fig 6 also shows that the rule base excludes large parts of the raw time series of some particularly suspect gauges elsewhere for example in the uk scandinavia and malaysia yet similarly to the us the majority of gauges in the uk and scandinavia show relatively reliable time series with the rule base removing less than 5 of the time series in 90 and 96 of gauges in each region respectively indeed the rule base generally retains a large amount of the raw data in most regions with significant observations with the resulting effective record lengths shown in fig 7 for a subset of the dataset fig 8 summarises the percentages of data removed by the rule base according to effective record length this highlights substantial geographical diversity in the gsdr dataset the data from australia europe and japan all exhibit generally low percentage removals across all record lengths brazil and new zealand show a similar pattern albeit with shorter record lengths in the former case and more variation in percentage removals in the latter the isd exhibits fairly low percentage removals overall but the stations counts indicate that the vast majority of records are very short in length in the us longer records tend to be associated with less removal by the rule base but the overall proportion of data removed is higher than in say australia and europe further inspection of the individual rules and quality flags suggests that many of the us data removals are long periods of zeros that are in fact missing periods section s3 in supplementary material aside from these periods the rule base does not generally remove large amounts of the us data series a more detailed view of the types of errors present in data from different countries and regions is given by fig 9 this figure demonstrates how much individual rules contribute to total data removal as noted previously suspiciously long dry periods are one of the most common causes of data removal across regions not just in the us most countries and regions in fig 9 show notable contributions from rules r1 r9 and r10 and often r11 however there is significant geographical variation in the usage of most rules in australia and europe where data removal is generally low but many gauges are present the rules based on neighbouring gauges in wet periods r7 and r8 contribute notably to total data removal streaks of repeated values removed by r4 are especially found in malaysia and new zealand while gauges in brazil show the highest contribution from r5 and r6 which remove hours exceeding the world record and etccdi maximum rx1day indices respectively variations such as these reflect a number of factors including gauge instrumentation differences between regions the gauge maintenance and data collation procedures of different meteorological services and the degree of quality control applied by data providers fig 8 also indicates substantial data removal in malaysia with the rule base removing less than 5 of the time series at only 36 of the gauges using malaysia as an example fig 10 illustrates how the summary output from the procedure can be used to help understand gauge specific and systematic data quality concerns in particular regions of interest in particular fig 10 shows a very close correspondence between the percentage of data records considered to be suspiciously long dry periods and the total percentage of data removed by the rule base inspection of individual gauge time series confirmed that missing data periods had often been entered as zeros into the data record however fig 10 also highlights that the presence of streaks of repeated non zero values is a general issue in the data records see also fig 1a at many gauges the percentage of record affected is less than 10 but long periods of streaks are present at two gauges in particular inspection of time series revealed a mixture of typically 24 h streaks of repeated values as well as numerous occurrences of high or very high identical values repeated for two or more hours 4 3 uk case study although supported by manual inspection of a sample of hourly gauges from around the world the assessment in section 4 1 provides only a global overview therefore a more detailed regional study of the uk dataset is presented to better understand the efficacy of the gsdr qc procedure in addition this section is intended to demonstrate that the gsdr qc process can be easily adapted to incorporate local expertise we also use this case study to examine the implications of not having access to suitable high quality reference data such as the gpcc database for regional applications the rule base rules r8 and r11 using flags that require access to the gpcc records are therefore omitted here note that the presence of the hourly nearest neighbour checks which does not rely on any reference dataset ensures that the spatial consistency of rain gauges is evaluated even when daily reference data is unavailable the uk dataset consists of 1 903 gauges with hourly data covering all of the uk except for northern ireland and spanning the period 1980 2014 the hourly rainfall data was resampled from a mixture of tip time and 15 min resolution data provided by the uk met office ukmo the environment agency ea natural resources wales nrw and the scottish environment protection agency sepa the origins of this dataset are presented in blenkinsop et al 2017 and lewis et al 2018 4 4 local modifications the modifications we made to the gsdr qc to produce a local uk focused version focused on parameter changes firstly the threshold value used in the world record check qc10 was modified from 401 mm to 92 mm which corresponds to the ukmo official record for maximum hourly precipitation in the uk in addition the threshold in the rx1day qc11 test was modified from the variable location based etccdi rx1day value to a fixed value corresponding to the uk 24 h rainfall record of 341 5 mm this check was also modified to operate on a 24 h rolling window sum rather than on every hour to incorporate the availability of a uk specific record finally the threshold in the streaks check qc15 which is ordinarily based on the etccdi sdii index was changed to a fixed value of 20 mm this value was selected based on our experience from studying the uk climate and to ensure consistency with the storm which generated the uk 24 h precipitation record the effect of these threshold checks can be seen in fig 11 which plots a portion of the rainfall time series at the ea shalbourne rain gauge multiple suspiciously large hourly values are classified as unchecked in the original data in fig 11 the world record value used by the global gsdr qc default settings is too high to flag most of the abnormally high hourly records in contrast the lower uk specific record value is much more effective at flagging questionable hours the rx1day check ensures that the global gsdr qc includes local information through its use of reference indices derived from daily data at this gauge the rx1day check in the global version of the gsdr qc is more conservative at flagging suspicious hours than the uk specific record hourly value the uk rx1day threshold appears high but as noted above it was modified to operate on a 24 h rolling window total to ensure that the entire suspect period shown in fig 11 is flagged as suspect the uk case study also provides an opportunity to compare the performance of gsdr qc with the procedures implemented by the ea the ea rain gauges which represent 71 of the gauge dataset are set up in pairs with one operational gauge and a check gauge in close proximity data are flagged as suspect if there is a discrepancy equal to or greater than 8 between both gauges or if some other ad hoc error such as tampering or electrical faults is discovered during an inspection the ea provides the flagged data and it is up to users to determine how to leverage the qc flags a basic qc process could consider all suspect flagged data as erroneous but this would potentially remove large amounts of valid data due to months long intervals between inspections observed in the gauge records we illustrate the effect of such an assumption in fig 12 4 4 1 uk wide results the top row of fig 12 shows the percentage of record hours removed by three different qc procedures the left panel uses the ea quality flags as the only source of qc this approach yields inconsistent results across the uk and the effect of having multiple operators is readily apparent as rain gauges in scotland and northern ireland are operated by the scottish environmental protection agency sepa and the ukmo a uk qc procedure which relied on operator flags would therefore need to apply different rules for each provider resulting in regional disparities in the quality of the resulting dataset however temporal and regional differences in the amount of data flagged as suspect exist within the ea dataset due to different operational procedures in different regions as well as temporal variation in the proportions of flagged data blenkinsop et al 2017 for example most gauges in norfolk and suffolk have 25 of their data flagged while nearby london has 0 flagged ea qc fig 12 the percentage of data removed by both uk local and global versions of the gsdr qc shown in the centre left and centre right panels of fig 12 respectively show similar spatial patterns clusters of gauges with higher percentages of data removed are apparent in the central belt of scotland south eastern england and on the channel coast individual gauges with close to 100 of hours removed are also identified by both gsdr qc versions in devon somerset and north wales differences between the two gsdr qc procedures are highlighted in the right side panels in fig 12 the light purple shades indicate that the uk local procedure removed fewer hours overall overall for the entire dataset the local and global versions removed 2 06 and 2 08 respectively out of total record hours which represents a difference of 67 000 h in terms of percentage rainfall removed the bottom row of fig 12 shows that the ea flags a considerable portion of its rainfall data as suspect removing all of this could result in the removal of valuable data the central panels demonstrate how local and global gsdr qc implementations yield similar results despite the removal of the gpcc reference from the local implementation in fact the differences shown in the right most panel are positive such that overall the local gsdr qc implementation removes a greater amount of rainfall 2 66 than the global implementation 2 47 due to the lower 1 hr and 24 hr thresholds used individual gauge differences range from 13 to 25 however they are tightly clustered around 0 and 50 of all gauges have differences in rainfall removals between 0 05 and 0 25 5 discussion the gsdr qc automated quality control procedure represents a systematic method for assessing the quality of sub daily rainfall data to our knowledge it is the first time such a process has been implemented in open source code for application to sub daily rainfall observations around the world provision of a comprehensive set of flags means that the procedure is transparent moreover it is applied consistently across gauges which helps to deal with the problem that the raw records obtained to compile the gsdr dataset vary substantially in the often ambiguous level of quality control applied by the data originators a uk case study suggests that the global gsdr qc procedure performs well in relation to a locally refined version however it is certainly possible that particular applications may benefit from alterations or refinements to the approach a number of subjective judgements are made in any automated or manual quality control process which may not be optimal in all places at all times e g dunn et al 2012 durre et al 2008 for example in the checks with neighbouring gauges we had to make judgements on the search radius the required minimum number of operating neighbouring gauges the strength of relationships between the target gauge and its neighbours and the thresholds at which differences between neighbours should be considered suspicious the parameters e g thresholds selected in examples such as this were an attempt to balance removal of problematic data with the minimisation of false flagging for a given application these parameters could of course be varied or their sensitivity further analysed for example in a study using seasonal or annual maxima for trend change or extreme value analysis it may be worth testing the sensitivity of results to rules based on the daily neighbours wet qc16 and hourly neighbours wet qc17 checks these flags generally remove at most a few large values but the few very high values are of course relevant in maxima based analyses sensitivity testing could thus be undertaken through varying thresholds and including excluding the relevant rules such analyses might be particularly valuable for studies focusing on a region or season that experiences say localised convective rainfall where neighbours may miss some key rainfall events varying the rule base in this way would allow a user to learn about their data it would help to indicate whether further quality review is needed namely in the case that analyses appear too sensitive to particular rules of course in other cases and contexts such as those dominated by stratiform precipitation the neighbours checks may exhibit very strong performance owing to the higher spatial correlation of precipitation fields in general the quality control procedure introduced here aims to provide a starting point for a range of applications such as climatological characterisations of the overall sub daily rainfall probability distribution diurnal cycles persistence of wet and dry spells frequencies of moderate to high rainfall threshold exceedances and various types of scaling behaviour e g spatial temporal and temperature related scaling inter alia the procedure eliminates many implausible extremes but given the rarity of such events by definition it is strongly recommended that a user reviews the summary outputs of the procedure and considers additional sensitivity testing combined with judicious manual review this includes reviewing the checks on whether a gauge is generally suspicious e g section 3 2 1 which are not incorporated in the current rule base the same recommendation applies to climate change studies looking at trends changes and breakpoints where further analysis of the homogeneity flags should be conducted the quality checks presented here are based on known errors identified by the authors as well as comparison to a higher quality gpcc daily rainfall dataset the intention is to preserve as many realistic extreme values as possible while still eliminating erroneous data evaluation against the manually checked sample suggests that the procedure performs well although some issues may still be missed for example the manual checks suggest that there are cases of 1 irregular or intermittent sequences of missing values complicating interpretation of the sometimes reasonable sometimes erroneous remaining values 2 probable complex snowfall or snow melt signatures in rain gauge records and 3 streaks of repeating patterns rather than single values there are also many other checks that could be included in this software which we hope would be developed by the software user community in the future for example additional checks could be based on the distribution of hourly values within rainfall events checks based on extreme value distributions fitted to each time series or comparisons against expected rainfall values predicted by infilling methods other datasets could ultimately also be incorporated to support the procedure such as river flows radar rainfall and other meteorological variables 6 conclusions this paper presents a new open source quality control algorithm gsdr qc to identify equipment malfunctions and recording errors in hourly rainfall data along with an application of the algorithm to the global gsdr observational dataset the algorithm is based on 25 quality checks which are combined into a simple rule base to remove suspicious data we hope that this procedure can form the beginning of a community code base that can be further refined and extended there is no perfect reference against which to evaluate the procedure but the results suggest that the gsdr qc process improves the correspondence between the gsdr dataset and records in the semi automatic quality controlled gpcc daily precipitation database the procedure also performs well in relation to manual checks on a sample of gauges while multiple quality checks and rules often agree on whether a period of data should be considered suspect thereby providing additional confidence in general the results indicate that most datasets obtained from national and regional data providers to form the gsdr exhibit reasonable quality overall albeit with some occasional errors problematic gauges and spatially varying systematic issues in contrast the isd data typically show short record lengths and often notably incomplete series except in regions where national datasets are typically available e g us the uk case study indicates that the global version of gsdr qc performs well in comparison with a locally refined version of the procedure although the local version removes some additional errors on account of its refined thresholds the case study also shows that the gsdr qc can improve upon and rescue data from conservative and regionally inconsistent flags applied by different data collectors this leads to a more spatially coherent regional dataset with a longer effective record length we hope that this code will be further developed by a community of rainfall data users around the world and that further work will be undertaken to create similar case studies of the regional performance of the qc system in different climates e g tropical continental summer and dry climates drawing upon the regional expertise of the community declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the european research council intense grant erc 2013 cog 617329 and the uk natural environment research council under the grants future storms ne r01079x 1 hjf sb dp and pyramid ne v00378x 1 el hjf hjf is also supported by the wolfson foundation and the royal society as a royal society wolfson research merit award wm140025 holder appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105169 software and data availability software name gsdr qc implemented in the intense python package developer elizabeth lewis elizabeth lewis newcastle ac uk year first available 2020 hardware required pc software required python availability and cost open source program language python program size 1 mb software access https github com nclwater intense qc https pypi org project intense data type text files for subset of shareable gsdr sub daily rainfall dataset data access the supporting data needed to run the code gridded etccdi indices are included in the python package along with sample gauge time series 
25769,sub daily rainfall observations are vital to help us understand model and adapt to changing climate extremes however gauge records often have quality issues for example due to equipment malfunctions and recording errors this paper presents a new open source quality control algorithm gsdr qc to identify these issues in hourly rainfall data along with an application of the algorithm to the global sub daily rainfall gsdr observational dataset the algorithm is based on 25 quality checks which are combined into a simple rule base to remove suspicious data the quality checks and rule base are adaptable to help incorporate local or regional information comparison with manually quality controlled gauge data shows that the procedure results in an overall improvement to the quality of the gsdr dataset a uk case study further demonstrates the performance of the gsdr qc procedure while showing how region specific data and understanding can be incorporated into the quality control process keywords quality control sub daily hourly precipitation observations 1 introduction sub daily rainfall observations are essential for improving our understanding of variability and changes in short duration rainfall patterns around the world allan et al 2020 barbero et al 2019 fowler et al 2021 lenderink and fowler 2017 li et al 2020 effectively harnessing these data is especially important in the context of changing risks from flooding and other hazards in line with a warmer and moister atmosphere under climate change a new generation of very high resolution convection permitting climate models project significant changes in the intensity duration frequency and spatial characteristics of short duration rainfall extremes with large implications for flood risk ban et al 2015 kendon et al 2017 2014 prein et al 2017 robust sub daily observational data are needed to assess these models and inform appropriate climate change adaptation strategies however records of observed rainfall are often subject to errors which may compromise data use if not corrected or removed these errors come from various types of gauge malfunction losses and tampering as well as recording transmission and processing issues e g durre et al 2010 qi et al 2016 identifying such errors in hourly rainfall data can be extremely difficult individual hourly values at any location can be very high and extreme rainfall from convective systems can be localised which can make it difficult to distinguish genuine extremes from gauge errors yet retaining true extremes during quality control is vital to ensure risks are not underestimated visual inspection and comparison to known rainfall records historic flood drought events and neighbouring gauges are thus often needed some examples of different errors in selected raw hourly rainfall records from lewis et al 2019 are shown in fig 1 the present study focuses on a range of gauge malfunction and recording errors in the raw gsdr such as streaks of repeated values and both sporadic and persistent suspiciously large values fig 1 of course other systematic longstanding and complex factors also affect the representativeness of rainfall data these factors include wind induced undercatch instrument and site limitations and changes and uncertainties in spatial and temporal variations of rainfall intensity e g mcmillan et al 2012 pollock et al 2018 sevruk 1996 sieck et al 2007 while many advances have been made in automated quality control for large datasets of climate observations existing procedures are difficult to apply to sub daily rainfall records like those in the gsdr one important reason for this is that many procedures have focused on daily rather than sub daily data e g durre et al 2010 hamada et al 2011 scherrer et al 2011 sciuto et al 2009 you et al 2007 compared with daily aggregations sub daily e g hourly rainfall data possess different statistical characteristics and patterns of spatiotemporal variability patterns that in fact better capture the essential character of precipitation trenberth and zhang 2018 695 however these differences mean that thresholds and other elements of existing quality control procedures are not directly transferable from daily to sub daily intervals indeed leading daily rainfall quality procedures e g durre et al 2010 are not customised to the particular errors typical in sub daily rainfall data in many regions for example sub daily records can include periods where daily aggregations have been incorrectly incorporated into hourly data blenkinsop et al 2017 these errors may take the form of uniform repetition or disaggregation e g fig 1a alternatively daily aggregations may be incorrectly assigned to individual hours i e 23 zeros or missing flags followed by a potentially non zero value e g fig 1c issues like these may be missed by checks operating only at the daily interval some progress has been made in automated quality control of sub daily rainfall data de vos et al 2019 kim et al 2009 lawrimore et al 2020 qi et al 2016 but wider applicability is often compromised by a number of factors in particular most procedures for both daily and sub daily intervals do not implement all of the recommendations of the world meteorological organisation wmo wmo guidance recommends that rainfall data be subject to constraint consistency spike rapid change flat line streak and domain tests wmo 2019 2018 1986 previous work has focused mainly on constraint and consistency checks by testing for outliers and spatial temporal and physical inconsistencies the small number of existing procedures for sub daily rainfall tend to vary in auxiliary data requirements to support these tests as well as the sophistication of spatial and physical consistency checks e g blenkinsop et al 2017 de vos et al 2019 kim et al 2009 lawrimore et al 2020 lewis et al 2018 qi et al 2016 some important tests for streaks daily accumulations and other issues may also be absent these limitations lead some data providers such as the german meteorological service deutscher wetterdienst dwd to warn that their sub daily rainfall products are of inferior quality to their daily products because they are checked less thoroughly dwd 2017 furthermore from a practical perspective most of the aforementioned work on sub daily quality control has tended to focus on specific regional or national datasets and applications far less emphasis has been placed on developing generally applicable adaptable or extensible methods implemented in open source software this paper thus presents a new systematic and open source quality control algorithm gsdr qc to identify potential errors in hourly rainfall data along with an application of the algorithm to the gsdr observational dataset as described in section 3 the quality control procedure builds on previous work geared towards uk hourly rainfall data blenkinsop et al 2017 lewis et al 2018 the quality control algorithm is built around 25 quality checks which are combined into a simple rule base to remove suspicious data section 3 the quality checks and rule base are designed to be adaptable in order to help incorporate local information if available and customise the checking procedure section 4 assesses the effectiveness of the quality control procedure as applied to the gsdr dataset with a detailed case study of the uk comparing the global procedure with a locally refined version 2 data 2 1 the global sub daily rainfall gsdr dataset the gsdr dataset lewis et al 2019 is the product of intensive efforts to identify and collate observed sub daily rainfall records from around the world in the european research council funded intense project intelligent use of climate models for adaptation to non stationary hydrological extremes blenkinsop et al 2018 the dataset currently consists of 24 394 rain gauges with an hourly or shorter measurement interval fig 2 this is an increase of 707 gauges primarily in brazil and new zealand over the initial version presented in lewis et al 2019 the average record length is 17 8 years but this varies substantially from 1 to 104 years with a few exceptions the earliest records begin in the 1950s all sub hourly records have been aggregated to an hourly interval for this study the majority of the data were collected from national hydrological and meteorological services with some data obtained from environmental agencies and researchers the remaining 32 of the dataset is taken from the isd smith et al 2011 quality control of data incorporated in the isd varies according to the protocols of different data providers some automated checks are undertaken by the isd team to flag but not remove suspicious data although a number of key checks such as spatial consistency are not undertaken dunn et al 2012 performed detailed quality control of a subset of the isd to improve its quality and usability but they focused entirely on non precipitation variables such that the quality of isd precipitation records is still not well established the raw gsdr dataset is highly variable in its spatial coverage record length completeness and quality the data have been recorded by different instruments in very different climates with no overall conventions for dealing with common problems such as missing data blockages recording errors spurious values and infilling there are also major variations in the degree of quality control already applied to the data for example the us hourly precipitation dataset comes with a set of quality flags which endeavour to indicate periods of missing data regional state record exceedances and accumulations due to melting of snow amongst other issues other network operators may use very simple procedures as exemplified by the uk case study in section 4 2 in contrast for many countries it is unclear exactly what has been checked or altered in the original data due to the absence of any time series of quality flags given the large number of diverse records in gsdr quality control methods appropriate to a wide range of issues are thus needed to check the data 2 2 global precipitation climatology centre gpcc database to assist with the spatial consistency checks introduced in sections 3 2 5 and 3 2 6 the quality control procedure draws on reference datasets of high quality daily and monthly precipitation data the reference datasets need to have high accuracy which precludes satellite or reanalysis data that are less accurate than gauge data a high quality radar dataset could potentially be used but this is not globally available instead we use the extensively quality controlled global precipitation climatology centre gpcc daily and monthly precipitation databases through a collaboration between the intense project and the gpcc the authors were able to conduct a comparative study at the gpcc using the daily data base owing to its data policy and to respect the copyright of the data providers the gpcc does not provide the station based data but the gridded precipitation analyses products however alternative sources of daily and monthly reference data could be chosen by users such as the global historical climatology network ghcn archives menne et al 2012 peterson and vose 1997 as well as regional or national datasets the gpcc collects its precipitation data from monthly climate reports and in near real time from synoptic reports through the wmo global telecommunication system gts schneider et al 2014 the gpcc has additionally collated data from national meteorological and hydrological agencies in around 190 countries as well as monthly and daily rainfall records held by the climatic research unit cru new et al 2000 1999 the ghcn and the food and agriculture organisation of the united nations fao 2001 the gpcc thus holds a very large amount of precipitation data with their monthly precipitation database containing data for over 122 000 stations records in the gpcc database are subject to extensive semi automatic quality control with automatic statistical checks and an additional manual qc by a team of experts of the data flagged as questionable in the automatic checks schneider et al 2014 which makes them highly suitable as reference data 2 3 climate indices the quality control procedure uses a selection of the climate indices defined by the expert team on climate change detection and indices etccdi frich et al 2002 zhang et al 2011 the indices used are r99p prcptot rx1day cdd and sdii see definitions in table 1 for the application of quality control to the gsdr dataset gridded versions of the required indices at the global scale were obtained from both the hadex2 donat et al 2013b and the ghcndex donat et al 2013a datasets both of these versions of the climate indices are based on underlying observed daily precipitation records that have undergone substantial quality control the hadex2 and ghcndex climate indices were first calculated for each station and then gridded using an interpolation method accounting for the spatial correlation structure of the station level indices both datasets are used in this qc method to maximise spatial coverage and increase the robustness of the qc thresholds there is greater coverage more quality control and homogenization of stations underlying the hadex2 dataset than for ghcndex but ghcndex is routinely updated and the underlying station data is more accessible than the hadex2 dataset the quality checks applying etccdi indices see section 3 2 use the maximum of the index time series for the grid box in which the relevant gauge is located the maximum was selected because the etccdi indices represent an average of multiple gauges within a grid box given the smoothing of spatial heterogeneity arising from the gridding process using the maximum index value reduces instances of unfair overly cautious data flagging compared with using a time varying index value in cases where a gauge does not fall within a grid box that has an etccdi value a value from a neighbouring grid box is used i e the maximum value in a 3x3 window centred on the gauge location use of a neighbouring grid box is indicated in the quality control output file in the case that index values from both hadex2 and ghcndex are available the larger of the two index values is adopted users of the quality control procedure could also source or calculate their own versions of the indices as exemplified in section 4 2 3 quality control procedure the gsdr qc quality control qc procedure builds on previous work focused on uk hourly rainfall records based on understanding of uk rainfall patterns and instrumentation blenkinsop et al 2017 developed 11 quality checks on hourly rainfall amounts and dry periods these checks included tests for values exceeding uk hourly and daily rainfall records daily and monthly accumulated totals entered into hourly series consecutive large values and frequent tips for tipping bucket rain gauge records lewis et al 2018 extended this procedure by introducing four additional spatial consistency checks against neighbouring gauges to identify erroneous dry spells and high values accounting for uk rainfall seasonality lewis et al also introduced a rule base to exclude values and periods of data likely to be in error according to the outcomes of the quality checks the lewis et al code was written in python r and matlab and was only freely available on request the gsdr qc procedure generalises and extends this work for application to hourly rainfall data across a range of geographical climatic and instrumental contexts and is presented as an open source python package available for installation the procedure consists of 3 stages 1 station metadata checks for accurate location data and duplicated stations 2 flagging of suspicious data using a set of time series checks 3 removal of suspicious periods in time series based on a set of user defined rules as per lewis et al 2018 the procedure is split in this way so that it can be tailored to user requirements indeed this approach enables users to test different rules and develop a customised rule base for their own applications as necessary this flexibility for refinement and sensitivity analysis is valuable given the intrinsic challenges of automatically determining true data errors in large datasets e g dunn et al 2012 durre et al 2008 3 1 station metadata checks manual checks the first quality control checks applied to the gsdr dataset examined the station metadata beginning with whether the reported latitude and longitude coordinates were plausible the reported coordinates were tested for consistency with a polygon shapefile delineating national borders any stations outside the territory of the expected country were then checked manually this check highlighted that some coordinates were indeed recorded incorrectly with the latitude and longitude switched the gsdr dataset was then assessed for duplicate gauges a gis query was performed to identify the nearest neighbour for each gauge and the distance between them any gauges less than 100 m apart were then inspected further gauges apparently duplicated often by gauges in the isd were flagged then checked to see if their data series were identical for the period of overlap both stations were kept if the data series were different for gauges with identical data the gauge with the longest effective record length record length x 1 proportion missing data was retained and any incorrect metadata was updated only a small number of gauges 100 were removed by metadata checks 3 2 time series checks automated checks the 25 time series checks that underpin the procedure are presented in table 2 these checks fall broadly into five groups as explained below the explanations below use the identifiers e g qc1 and names e g breakpoints of the checks given in table 2 3 3 suspect gauges the first group of checks qc1 qc7 seeks to identify gauge records in which substantial portions of the data or even the whole record could be suspicious the percentiles qc1 check returns two lists of years where the 95th and 99th percentiles of the hourly rainfall distribution are zero which corresponds to suspiciously low rainfall in different climate regions however as some regions are very dry we also conduct a k largest qc2 check in this check the hourly rainfall values are put in descending order separately for each year the check then returns the years where the 1st 5th and 10th largest hourly rainfall values are all equal to zero the k largest check thus helps to identify suspiciously low annual rainfall totals in all but the very driest climates very low annual totals may occur where missing values have been entered as zeros for example the data are also checked to see if there are any clear biases towards certain days of the week or hours of the day wilby et al 2017 the days of week check qc3 performs a two sided t test on the distribution of mean rainfall over the days of the week this helps to identify cases such as spuriously higher rainfall on mondays due to data being preferentially reported as a weekend accumulation similarly the hours of day check qc4 uses a two sided t test on the distribution of mean rainfall over the hours of the day to help indicate if daily accumulations of rainfall have been reported at 0900 or another standard time blenkinsop et al 2017 the final checks for suspect periods or records focus on data continuity and homogeneity generally patchy records i e numerous sequences of missing data are identified using the intermittency check qc5 which returns years where more than five periods of missing data are bounded by zeros the lack of continuity in the record highlighted by this check suggests that rainfall events may not have been properly recorded for example due to dry periods being recorded but not rainfall events the records are also checked for breakpoints qc6 using a pettit test which may indicate changes in station location or measuring equipment however a breakpoint could also be detected due to a real change in rainfall induced by climate change for example as such the results of the pettit test are tentative and supplied for the user s reference possible changes to the measuring equipment are also reflected by the minimum value change check qc7 which identifies how many times the minimum recorded value greater than zero changes from one year to the next 3 3 1 suspiciously high values the next group of checks qc8 qc11 focuses on identifying suspiciously high values by using a set of reference thresholds beginning with the annual interval the r99p qc8 and prcptot qc9 quality checks compare the annual gauge statistics against the maximum etccdi r99p and prcptot climate indices section 2 3 r99p indicates annual total precipitation from days where precipitation exceeds the 99th percentile of wet day precipitation while prcptot quantifies annual total precipitation from wet days 1 mm as detailed in table 2 a flag for each year is returned whose value depends on whether the gauge s annual total exceeds the maximum etccdi r99p or prcptot and if so by how much the world record qc10 check compares each individual hourly rainfall value against the world record hourly rainfall total different sources indicate different values for the world record hourly rainfall with values ranging from 305 mm to 401 mm noaa 2017 wmo 1994 to try not to remove data unnecessarily we selected the upper value 401 mm for quality control of the gsdr dataset additional confidence in flagging very high values is provided by the rx1day check qc11 each individual hourly value is compared against the corresponding maximum etccdi rx1day maximum daily precipitation value for the corresponding grid cell note that the check compares hourly values with rx1day rather than daily values again in order to account for the smoothing of spatial variation inherent in the gridded etccdi indices 3 3 2 long periods without rainfall in addition to the percentiles qc1 and k largest qc2 checks potentially suspicious long dry period are assessed using the etccdi cdd consecutive dry days index each run of dry days is compared to the maximum cdd value and flagged as suspicious if this threshold is exceeded this additional cdd check qc12 is intended to provide more confidence in the identification of periods in which missing data may have been recorded as zeros it is complemented by checks against neighbouring stations which are introduced in section 3 2 5 3 3 3 suspect accumulations and repeated values the daily accumulations qc13 and monthly accumulations qc14 checks look for day and month long periods of zeros followed by a very wet hour and then another spell of 23 zeros i e the rest of the day is dry the purpose is to highlight cases where rainfall accumulated over a full day or month is mistakenly associated with a single hour the checks use a threshold of double the etccdi sdii index for the grid cell underlying the gauge location to classify the wet hour as potentially spurious the sdii is an index of the average daily precipitation total on wet days experimentation showed that doubling sdii represented a reasonable general threshold this threshold was determined through trial and error testing multiples of sdii that gave a large enough value that was unlikely to exclude true hourly rainfall values but low enough to exclude suspicious values inspecting gauge time series showed that specifying that the wet hour needs to be followed by a period of 23 zeros strongly reduces the possibility of flagging genuinely large rainfall totals that happen to follow a legitimate dry period two additional elements are incorporated in the daily accumulations qc13 check to help identify long periods of systematic daily accumulations of any magnitude firstly periods of two or more days with any non zero potential daily accumulations are identified secondly periods of zeros in between potential daily accumulations are also flagged this helps to identify continuous periods of potential daily accumulations i e not only periods where a day receives rainfall in addition the qc procedure checks for repeated identical values based on several conditions in the streaks check qc15 first streaks of 2 or more consecutive hours of identical high rainfall values are flagged using double the etccdi sdii index as a threshold again it is considered unlikely that 2 or more hours of identical large values would occur by chance second streaks of 12 or more consecutive hours of identical values greater than the measurement resolution are also flagged third streaks of 24 or more repeated hourly values greater than zero are flagged i e very long streaks at the data resolution the different thresholds in the second and third components of the check were chosen to reduce the chance of wrongly identifying spells of drizzle as potentially erroneous finally the check flags periods of zeros bounded by streaks of 24 repeated values this helps to find periods of uniform disaggregation or repetition of daily totals 3 3 4 neighbouring gauge checks on large values the next set of checks qc16 qc17 use neighbouring gauges to help identify suspicious large rainfall values these checks are performed after aggregation of the hourly series to the daily interval neighbour checks are not currently conducted on an hourly basis due to the heightened intermittency and spatial variability at sub daily time intervals the daily neighbours wet qc16 check uses the quality controlled gpcc daily precipitation database section 2 2 as reference data while the hourly neighbours wet qc17 check uses neighbouring gauges in the gsdr dataset being checked after aggregation to the daily interval both checks use the same method a number of steps are undertaken in these neighbouring gauge checks which are explained in detail in the supplementary material section s1 in brief the method is based on the approach introduced by upton and rahimi 2003 and modified previously for application to uk hourly data blenkinsop et al 2017 lewis et al 2018 the checks involve the following steps 1 select the closest up to 10 neighbouring gauges within 50 km with sufficient overlap 3 years and close correspondence i e in wet dry matching statistics and correlation with the target gauge the check is not undertaken if there are insufficient gauges meeting these criteria 2 calculate a time series of differences between the target gauge and each neighbour after normalisation of each record to the interval 0 1 by rescaling between the series minima and maxima see equation s1 in the supplementary material 3 for each neighbouring gauge produce a time series of flags using the following approach a subset the differences on wet days where the target gauge exceeds the normalised value of the neighbouring gauge b fit an exponential distribution to the subset of differences c identify threshold difference values associated with the 95th 99th and 99 9th percentiles use these thresholds to flag exceedances i e notably large differences 4 combine the flags i e one series per neighbour into a single series of flags where the ultimate flag value is equal to the minimum flag value of all neighbours at each time step a lower flag value equates to more agreement between the neighbours and the gauge i e higher quality data the modifications relative to blenkinsop et al 2017 and lewis et al 2018 are thus primarily in 1 focusing on wet days with larger normalised rainfall at the target gauge and 2 fitting an exponential distribution to the normalised differences rather than assuming an approximately normal distribution these developments were undertaken because a preliminary analysis showed generally variable performance in a sample of non uk stations often through too many false positives i e incorrect flagging of plausible data outside of the climate where the method has been previously applied further explanation and justification is given in section s1 of the supplementary material 3 3 5 other neighbouring gauge checks additional checks against neighbouring gauges are the daily neighbours dry qc18 and hourly neighbours dry qc19 checks these tests examine the agreement between a target gauge and its neighbours on dry periods the neighbour selection follows the same procedure outlined in section 3 2 5 and the supplementary material section s1 the checks are again differentiated by use of the gpcc daily precipitation database qc18 and neighbouring gauges within the gsdr dataset being checked qc19 for a moving 15 day window if the target gauge shows no rainfall in the window the number of wet days 0 mm in the same window for each neighbour are counted and the average taken to receive the highest flag all neighbours must have three or more wet days in a 15 day window that is dry in the record of the target gauge successively lower flags are used if all neighbours corroborate the occurrence of a lower number of wet days table 2 the monthly neighbours check qc20 uses reference data from the quality controlled gpcc monthly precipitation database monthly neighbouring gauges are selected based on proximity only for months with more than 95 completeness the check proceeds by calculating the percentage difference in monthly totals between each neighbouring gpcc monthly gauge and the target gauge after aggregation to the monthly interval the percentage differences are grouped into classes table 2 and the flags associated with each of the neighbours are then combined into a single series of flags again all neighbours need to signal notably high or low monthly totals for the most severe flags to be invoked for example if the target gauge monthly total is more than double the totals of each of the active neighbouring gauges then a high flag is assigned see details in table 2 the final neighbour checks aim to support the identification of generally suspicious gauges by augmenting the tests discussed in section 3 2 1 the timing offset check qc21 computes the affinity index ai and correlation at the daily interval between the target gauge and its nearest neighbour in the gpcc database at lags of 1 0 and 1 day the ai is a matching statistic for wet and dry days a dry day is defined here as having 0 1 mm rain the lag associated with the highest values of the statistics is returned as an indication of whether or not there could be potential timing issues as part of the procedure we also calculate the pre qc affinity index qc22 and pre qc correlation qc23 relative to the nearest daily neighbour in the gpcc database as a means of identifying generally questionable records finally the daily factor qc24 and monthly factor qc25 checks provide the mean difference expressed as a factor between the target gauge and its nearest daily or monthly gpcc neighbour respectively to help identify possible unit errors 3 4 rule base the quality flags from the checks described in section 3 2 are combined into a rule base to remove suspect data points from the gauge records building on the approach described by lewis et al 2018 following the strategy guiding other automated quality control procedures e g dunn et al 2012 durre et al 2008 we iterated the rule base for the gsdr dataset to strike a reasonable balance between excluding erroneous data and retaining as many of the observations as possible judgements were taken based on reviewing samples of gauges from around the world iteration of the rules was also based on assessing the spatial patterns of data exclusion and the corresponding effects on key rainfall summary statistics these analyses led to the rule base configuration presented in table 3 the intention was to ensure that only a low percentage of reasonable values were removed incorrectly false positive rate in order to avoid removal of important extremes this configuration aims to support general usage of the gsdr dataset but given the difficulty of developing a set of checks and rules that work optimally in all situations a user may also test different thresholds and combinations of rules depending on their geographical and thematic focuses the rule base is not currently designed to identify or omit wholly suspicious gauges from the dataset rather it is designed to omit suspicious values within each gauge record the rule base uses almost all of the time series quality checks described in section 3 2 2 3 2 6 i e those checks returning hourly time series of flags as opposed to the checks that return either single flags or one flag per year it is therefore advisable to review the flags computed as indicators of whether a gauge may be generally suspicious i e checks described in section 3 2 1 and 3 2 6 before using the data for a given application to help the user identify suspect gauges the software produces an output summary table of all quality checks detailed in section 3 2 and table 2 and all rule base rules table 3 examples of the value of this summary information are given in section 4 3 5 software design and use the gsdr qc procedure is implemented in python it consists of two primary classes a gauge class and a qc class the gauge class handles station data series and metadata while the qc class contains the methods that implement the quality checks in section 3 2 with a gauge object the quality checks for a gauge are written to text file whose headers indicate overall quality flags from checks returning a single flag per gauge e g breakpoint test qc6 checks returning one flag per year e g prcptot check qc9 or a list of years e g percentiles check qc1 the time series quality flags e g neighbouring gauges checks qc16 20 are provided beneath the headers due to its comparative simplicity the rule base is implemented through a single function based on a pandas dataframe with a conditional statement for each rule an output summary table contains an overview of the operation of each quality check and rule at each gauge examples of the output files are given in section s2 of the supplementary material to exemplify the software usage in outline terms the following steps would be taken to quality control a target hourly gauge record 1 manually check the gauge metadata as described in section 3 1 2 create tables of basic metadata for any hourly daily and monthly gauges to be used in neighbouring gauges checks 3 create k d trees to identify the nearest neighbours for the gauges using create kdtree set of functions in the utils module 4 read in the target gauge data and metadata by calling the read intense function in the gauge module to create a gauge object 5 create an instance of the qc class using the gauge object step 4 and a small number of arguments including the metadata of neighbouring gauges from step 3 6 run the quality checks and write the flags to file using the get flags and write methods of the qc class 7 run the rule base and write output to file using the apply rulebase function in the rulebase module the code has been designed to be adapted by the user through its object oriented design each quality check in the qc class has its own method which allows a user with a small amount of python knowledge to create a new subclass according to their own requirements quality checks can therefore be changed omitted or added by the user for example in the uk case study below the rx1day qc11 test was modified to use a known regional threshold by overriding the rx1day check ts method as a python package the full code is also readily available if the user wants to make substantial changes to suit their needs the software is published as the intense python package with example data etccdi indices and tests further explanation of how to use the code is given in the documentation 3 6 evaluation three approaches were used to evaluate the gsdr qc procedure firstly the operation of the rule base was evaluated with reference to a sample of 300 manually checked gsdr gauges following durre et al 2008 the gauges were selected by randomly sampling 100 gauges from three large geographical domains 1 north and south america 2 europe and 3 asia and australasia only non isd gauges with over five years of record and more than 70 completeness were considered the sample was iterated to ensure a minimum of five gauges from each country with significant data records in each domain for each of the 300 gauges four 3 month periods were randomly chosen for detailed inspection these manual checks were performed by reviewing the hourly time series alongside longer daily and monthly time series mean annual cycles wet day hour statistics wet day hour amount distributions and neighbouring gauge series were also considered each hour was then classified as likely to be reasonable or erroneous using expert judgement the second evaluation approach was a comparison of daily time series between the gsdr gauges and their nearest gauges in the quality controlled gpcc daily database before and after application of the procedure rank correlation coefficients were calculated for gsdr gauges with a gpcc neighbour within 50 km and with at least three years of overlap in the record period the analysis in section 4 1 2 differentiates between those gsdr gauges where gpcc neighbours are used in the quality control procedure and those where they are not to gain further insight into gsdr qc performance in a final evaluation the uk was selected for a detailed case study based on the authors experience with these data and the uk climate 4 results 4 1 gsdr global dataset the analysis presented here focuses on the overall operation of the rule base as well as regional variations in data removals by the rule base this provides an indication of how much data were removed to arrive at a quality controlled version of the gsdr intended for general usage as noted in section 3 3 specific applications may benefit from further analysis of individual quality flags or specific rules in detail which we exemplify below 4 1 1 overall rule base usage application of the rule base to the raw gsdr dataset eliminated a total of 103 890 687 h of data 11 860 years across all of the 24 394 gauges in the dataset for the majority of gauges 87 of all gsdr gauges only small percentages 5 of the record period were removed by the rule base this is consistent with most of the gauges showing reasonable quality records overall but periods of errors due to temporary gauge malfunctions or recording errors that were resolved by the network operators however for some gauges much larger proportions of the time series were removed between 5 and 50 of data was removed for 10 of all gsdr gauges while between 50 and 100 of data was removed for 2 7 of all gsdr gauges especially gauges with very short time series this is reflected in fig 3 which shows the relationship between effective record length and the percentage of data removed by the gsdr qc procedure fig 3 highlights the low percentage of data removed by the rule base across all record lengths for the majority of gauges but that a higher proportion of short records have the largest suspicious data percentages fig 4 shows the number of gauges at which different rules in the rule base are invoked to remove suspicious data from time series the first rule r1 which is based on the k largest check qc2 is triggered at more stations than any other rule this indicates that a number of the raw records in gsdr contain years or partial years with erroneously little rainfall however it should be noted that in many cases this rule identifies long periods of missing data incorrectly entered as zeros it may also remove periods of zeros at the beginning end of data series this latter case may reflect either recording error or simply an often ambiguous partial year without rainfall prior to data series termination the two rules triggered at the next largest number of gauges are r9 and r10 which are invoked by the hourly neighbours dry qc19 and daily neighbours dry qc18 checks respectively both in conjunction with the cdd check qc12 however it should be noted that all three rules r1 r9 and r10 are fairly often triggered at the same time specifically r9 and r10 are both invoked on average on 27 of the same occasions on which r1 is incurred with 32 of the relevant gauges having such co occurrence greater than 50 this co occurrence provides additional confidence in the removal of these data as multiple checks using different data and approaches are often in agreement on suspiciously long dry periods i e there is consensus between the k largest cdd and neighbours dry checks using both gsdr gauges and gpcc gauges the rule r4 removing streaks of repeated values qc15 is also applied to a relatively large number of gauges an appreciable number of gauges thus show at least some periods of suspicious repeated values some of which are known to coincide with instances where daily values have been disaggregated or repeated as a surrogate for true hourly data e g fig 1a the rules r2 and r3 removing possible daily and monthly accumulations qc13 and qc14 are invoked at fewer stations along with the rule r5 checking for world record hourly rainfall exceedances there is notable co occurrence of the r5 world record rule with the rx1day rule r6 which is invoked on average on 75 of the same occasions as r5 4 1 2 rule base evaluation as described in section 3 5 an indication of the overall performance of the gsdr qc rule base can be obtained by comparing its data removal and retention with manual quality control for a sample of gsdr gauges this comparison reveals that the gsdr qc removes or retains data correctly in 99 0 of the 2 425 829 sampled hours relative to manual quality control the accuracy is slightly higher if just wet hours are considered in this case 99 6 of 200 541 hours are classified correctly i e as either reasonable or erroneous by the procedure further insight is gained by considering the proportion of the time that real errors identified manually are classified correctly as errors or incorrectly as reasonable values by gsdr qc similar proportions can be calculated for the correct and incorrect classification of values manually determined to be reasonable the associated confusion matrix is summarised in table 4 for the full sample of all 2 425 829 dry and wet hours the results indicate that 99 6 of the hours considered reasonable in the manual checks were also retained by the gsdr qc rule base therefore only a small proportion 0 4 of reasonable values were removed incorrectly in addition the rule base correctly removes 71 0 of the real errors identified during manual inspection this means that 29 0 of real errors were wrongly retained see discussion of error characteristics in section 5 although it may be noted that real errors only comprise a small proportion 2 4 of the full sample hours table 4 indicates that the performance increases slightly if the sample is restricted to wet hours given the importance of balancing true false positives negatives the performance relative to manual checks is good fig 5 compares rank correlation coefficients for daily time series between the gsdr gauges and their nearest gauges in the quality controlled gpcc daily database before and after application of the procedure see details in section 3 5 fig 5 is also split into two panels to differentiate 1 gauges where flags from gpcc neighbouring gauges checks were explicitly used to remove data in the rule base fig 5a and 2 those gauges where they were not fig 5b the latter gauges in fig 5b provide a more but not entirely independent test of gsdr qc performance with many gauges in fig 5b not having used gpcc data due to insufficient neighbours and weak relationships with the target gauge unfortunately it is difficult to take the comparison further through full cross validation tests as aspects of the procedure would need to be switched off to entirely avoid using the gpcc data in both quality control and evaluation almost all of the points in fig 5 lie on or to the left of the 1 1 line this indicates that where correlation differs before and after the procedure almost all gsdr gauges show improved correlation with their nearest gpcc neighbours this suggests an overall improvement in data quality following the gsdr qc procedure with 40 of gauges with a gpcc neighbour showing larger correlation after the procedure it is also clear that many gauges lie close to the 1 1 line indicating little change in the correlation coefficient for numerous gauges such limited changes may be expected in the many cases where only small proportions of the gsdr gauge time series are removed by the rule base often due to the high quality of the original data series 4 2 regional variation fig 6 suggests that the gauges where large proportions of the data series were removed tend to be found in a few regions for example the procedure removed 50 of the time series for 519 gauges in the us this represents 7 6 of the total number of gsdr gauges 6834 in the us although 86 of these 519 gauges are less than 5 years in length fig 6 also shows that the rule base excludes large parts of the raw time series of some particularly suspect gauges elsewhere for example in the uk scandinavia and malaysia yet similarly to the us the majority of gauges in the uk and scandinavia show relatively reliable time series with the rule base removing less than 5 of the time series in 90 and 96 of gauges in each region respectively indeed the rule base generally retains a large amount of the raw data in most regions with significant observations with the resulting effective record lengths shown in fig 7 for a subset of the dataset fig 8 summarises the percentages of data removed by the rule base according to effective record length this highlights substantial geographical diversity in the gsdr dataset the data from australia europe and japan all exhibit generally low percentage removals across all record lengths brazil and new zealand show a similar pattern albeit with shorter record lengths in the former case and more variation in percentage removals in the latter the isd exhibits fairly low percentage removals overall but the stations counts indicate that the vast majority of records are very short in length in the us longer records tend to be associated with less removal by the rule base but the overall proportion of data removed is higher than in say australia and europe further inspection of the individual rules and quality flags suggests that many of the us data removals are long periods of zeros that are in fact missing periods section s3 in supplementary material aside from these periods the rule base does not generally remove large amounts of the us data series a more detailed view of the types of errors present in data from different countries and regions is given by fig 9 this figure demonstrates how much individual rules contribute to total data removal as noted previously suspiciously long dry periods are one of the most common causes of data removal across regions not just in the us most countries and regions in fig 9 show notable contributions from rules r1 r9 and r10 and often r11 however there is significant geographical variation in the usage of most rules in australia and europe where data removal is generally low but many gauges are present the rules based on neighbouring gauges in wet periods r7 and r8 contribute notably to total data removal streaks of repeated values removed by r4 are especially found in malaysia and new zealand while gauges in brazil show the highest contribution from r5 and r6 which remove hours exceeding the world record and etccdi maximum rx1day indices respectively variations such as these reflect a number of factors including gauge instrumentation differences between regions the gauge maintenance and data collation procedures of different meteorological services and the degree of quality control applied by data providers fig 8 also indicates substantial data removal in malaysia with the rule base removing less than 5 of the time series at only 36 of the gauges using malaysia as an example fig 10 illustrates how the summary output from the procedure can be used to help understand gauge specific and systematic data quality concerns in particular regions of interest in particular fig 10 shows a very close correspondence between the percentage of data records considered to be suspiciously long dry periods and the total percentage of data removed by the rule base inspection of individual gauge time series confirmed that missing data periods had often been entered as zeros into the data record however fig 10 also highlights that the presence of streaks of repeated non zero values is a general issue in the data records see also fig 1a at many gauges the percentage of record affected is less than 10 but long periods of streaks are present at two gauges in particular inspection of time series revealed a mixture of typically 24 h streaks of repeated values as well as numerous occurrences of high or very high identical values repeated for two or more hours 4 3 uk case study although supported by manual inspection of a sample of hourly gauges from around the world the assessment in section 4 1 provides only a global overview therefore a more detailed regional study of the uk dataset is presented to better understand the efficacy of the gsdr qc procedure in addition this section is intended to demonstrate that the gsdr qc process can be easily adapted to incorporate local expertise we also use this case study to examine the implications of not having access to suitable high quality reference data such as the gpcc database for regional applications the rule base rules r8 and r11 using flags that require access to the gpcc records are therefore omitted here note that the presence of the hourly nearest neighbour checks which does not rely on any reference dataset ensures that the spatial consistency of rain gauges is evaluated even when daily reference data is unavailable the uk dataset consists of 1 903 gauges with hourly data covering all of the uk except for northern ireland and spanning the period 1980 2014 the hourly rainfall data was resampled from a mixture of tip time and 15 min resolution data provided by the uk met office ukmo the environment agency ea natural resources wales nrw and the scottish environment protection agency sepa the origins of this dataset are presented in blenkinsop et al 2017 and lewis et al 2018 4 4 local modifications the modifications we made to the gsdr qc to produce a local uk focused version focused on parameter changes firstly the threshold value used in the world record check qc10 was modified from 401 mm to 92 mm which corresponds to the ukmo official record for maximum hourly precipitation in the uk in addition the threshold in the rx1day qc11 test was modified from the variable location based etccdi rx1day value to a fixed value corresponding to the uk 24 h rainfall record of 341 5 mm this check was also modified to operate on a 24 h rolling window sum rather than on every hour to incorporate the availability of a uk specific record finally the threshold in the streaks check qc15 which is ordinarily based on the etccdi sdii index was changed to a fixed value of 20 mm this value was selected based on our experience from studying the uk climate and to ensure consistency with the storm which generated the uk 24 h precipitation record the effect of these threshold checks can be seen in fig 11 which plots a portion of the rainfall time series at the ea shalbourne rain gauge multiple suspiciously large hourly values are classified as unchecked in the original data in fig 11 the world record value used by the global gsdr qc default settings is too high to flag most of the abnormally high hourly records in contrast the lower uk specific record value is much more effective at flagging questionable hours the rx1day check ensures that the global gsdr qc includes local information through its use of reference indices derived from daily data at this gauge the rx1day check in the global version of the gsdr qc is more conservative at flagging suspicious hours than the uk specific record hourly value the uk rx1day threshold appears high but as noted above it was modified to operate on a 24 h rolling window total to ensure that the entire suspect period shown in fig 11 is flagged as suspect the uk case study also provides an opportunity to compare the performance of gsdr qc with the procedures implemented by the ea the ea rain gauges which represent 71 of the gauge dataset are set up in pairs with one operational gauge and a check gauge in close proximity data are flagged as suspect if there is a discrepancy equal to or greater than 8 between both gauges or if some other ad hoc error such as tampering or electrical faults is discovered during an inspection the ea provides the flagged data and it is up to users to determine how to leverage the qc flags a basic qc process could consider all suspect flagged data as erroneous but this would potentially remove large amounts of valid data due to months long intervals between inspections observed in the gauge records we illustrate the effect of such an assumption in fig 12 4 4 1 uk wide results the top row of fig 12 shows the percentage of record hours removed by three different qc procedures the left panel uses the ea quality flags as the only source of qc this approach yields inconsistent results across the uk and the effect of having multiple operators is readily apparent as rain gauges in scotland and northern ireland are operated by the scottish environmental protection agency sepa and the ukmo a uk qc procedure which relied on operator flags would therefore need to apply different rules for each provider resulting in regional disparities in the quality of the resulting dataset however temporal and regional differences in the amount of data flagged as suspect exist within the ea dataset due to different operational procedures in different regions as well as temporal variation in the proportions of flagged data blenkinsop et al 2017 for example most gauges in norfolk and suffolk have 25 of their data flagged while nearby london has 0 flagged ea qc fig 12 the percentage of data removed by both uk local and global versions of the gsdr qc shown in the centre left and centre right panels of fig 12 respectively show similar spatial patterns clusters of gauges with higher percentages of data removed are apparent in the central belt of scotland south eastern england and on the channel coast individual gauges with close to 100 of hours removed are also identified by both gsdr qc versions in devon somerset and north wales differences between the two gsdr qc procedures are highlighted in the right side panels in fig 12 the light purple shades indicate that the uk local procedure removed fewer hours overall overall for the entire dataset the local and global versions removed 2 06 and 2 08 respectively out of total record hours which represents a difference of 67 000 h in terms of percentage rainfall removed the bottom row of fig 12 shows that the ea flags a considerable portion of its rainfall data as suspect removing all of this could result in the removal of valuable data the central panels demonstrate how local and global gsdr qc implementations yield similar results despite the removal of the gpcc reference from the local implementation in fact the differences shown in the right most panel are positive such that overall the local gsdr qc implementation removes a greater amount of rainfall 2 66 than the global implementation 2 47 due to the lower 1 hr and 24 hr thresholds used individual gauge differences range from 13 to 25 however they are tightly clustered around 0 and 50 of all gauges have differences in rainfall removals between 0 05 and 0 25 5 discussion the gsdr qc automated quality control procedure represents a systematic method for assessing the quality of sub daily rainfall data to our knowledge it is the first time such a process has been implemented in open source code for application to sub daily rainfall observations around the world provision of a comprehensive set of flags means that the procedure is transparent moreover it is applied consistently across gauges which helps to deal with the problem that the raw records obtained to compile the gsdr dataset vary substantially in the often ambiguous level of quality control applied by the data originators a uk case study suggests that the global gsdr qc procedure performs well in relation to a locally refined version however it is certainly possible that particular applications may benefit from alterations or refinements to the approach a number of subjective judgements are made in any automated or manual quality control process which may not be optimal in all places at all times e g dunn et al 2012 durre et al 2008 for example in the checks with neighbouring gauges we had to make judgements on the search radius the required minimum number of operating neighbouring gauges the strength of relationships between the target gauge and its neighbours and the thresholds at which differences between neighbours should be considered suspicious the parameters e g thresholds selected in examples such as this were an attempt to balance removal of problematic data with the minimisation of false flagging for a given application these parameters could of course be varied or their sensitivity further analysed for example in a study using seasonal or annual maxima for trend change or extreme value analysis it may be worth testing the sensitivity of results to rules based on the daily neighbours wet qc16 and hourly neighbours wet qc17 checks these flags generally remove at most a few large values but the few very high values are of course relevant in maxima based analyses sensitivity testing could thus be undertaken through varying thresholds and including excluding the relevant rules such analyses might be particularly valuable for studies focusing on a region or season that experiences say localised convective rainfall where neighbours may miss some key rainfall events varying the rule base in this way would allow a user to learn about their data it would help to indicate whether further quality review is needed namely in the case that analyses appear too sensitive to particular rules of course in other cases and contexts such as those dominated by stratiform precipitation the neighbours checks may exhibit very strong performance owing to the higher spatial correlation of precipitation fields in general the quality control procedure introduced here aims to provide a starting point for a range of applications such as climatological characterisations of the overall sub daily rainfall probability distribution diurnal cycles persistence of wet and dry spells frequencies of moderate to high rainfall threshold exceedances and various types of scaling behaviour e g spatial temporal and temperature related scaling inter alia the procedure eliminates many implausible extremes but given the rarity of such events by definition it is strongly recommended that a user reviews the summary outputs of the procedure and considers additional sensitivity testing combined with judicious manual review this includes reviewing the checks on whether a gauge is generally suspicious e g section 3 2 1 which are not incorporated in the current rule base the same recommendation applies to climate change studies looking at trends changes and breakpoints where further analysis of the homogeneity flags should be conducted the quality checks presented here are based on known errors identified by the authors as well as comparison to a higher quality gpcc daily rainfall dataset the intention is to preserve as many realistic extreme values as possible while still eliminating erroneous data evaluation against the manually checked sample suggests that the procedure performs well although some issues may still be missed for example the manual checks suggest that there are cases of 1 irregular or intermittent sequences of missing values complicating interpretation of the sometimes reasonable sometimes erroneous remaining values 2 probable complex snowfall or snow melt signatures in rain gauge records and 3 streaks of repeating patterns rather than single values there are also many other checks that could be included in this software which we hope would be developed by the software user community in the future for example additional checks could be based on the distribution of hourly values within rainfall events checks based on extreme value distributions fitted to each time series or comparisons against expected rainfall values predicted by infilling methods other datasets could ultimately also be incorporated to support the procedure such as river flows radar rainfall and other meteorological variables 6 conclusions this paper presents a new open source quality control algorithm gsdr qc to identify equipment malfunctions and recording errors in hourly rainfall data along with an application of the algorithm to the global gsdr observational dataset the algorithm is based on 25 quality checks which are combined into a simple rule base to remove suspicious data we hope that this procedure can form the beginning of a community code base that can be further refined and extended there is no perfect reference against which to evaluate the procedure but the results suggest that the gsdr qc process improves the correspondence between the gsdr dataset and records in the semi automatic quality controlled gpcc daily precipitation database the procedure also performs well in relation to manual checks on a sample of gauges while multiple quality checks and rules often agree on whether a period of data should be considered suspect thereby providing additional confidence in general the results indicate that most datasets obtained from national and regional data providers to form the gsdr exhibit reasonable quality overall albeit with some occasional errors problematic gauges and spatially varying systematic issues in contrast the isd data typically show short record lengths and often notably incomplete series except in regions where national datasets are typically available e g us the uk case study indicates that the global version of gsdr qc performs well in comparison with a locally refined version of the procedure although the local version removes some additional errors on account of its refined thresholds the case study also shows that the gsdr qc can improve upon and rescue data from conservative and regionally inconsistent flags applied by different data collectors this leads to a more spatially coherent regional dataset with a longer effective record length we hope that this code will be further developed by a community of rainfall data users around the world and that further work will be undertaken to create similar case studies of the regional performance of the qc system in different climates e g tropical continental summer and dry climates drawing upon the regional expertise of the community declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the european research council intense grant erc 2013 cog 617329 and the uk natural environment research council under the grants future storms ne r01079x 1 hjf sb dp and pyramid ne v00378x 1 el hjf hjf is also supported by the wolfson foundation and the royal society as a royal society wolfson research merit award wm140025 holder appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105169 software and data availability software name gsdr qc implemented in the intense python package developer elizabeth lewis elizabeth lewis newcastle ac uk year first available 2020 hardware required pc software required python availability and cost open source program language python program size 1 mb software access https github com nclwater intense qc https pypi org project intense data type text files for subset of shareable gsdr sub daily rainfall dataset data access the supporting data needed to run the code gridded etccdi indices are included in the python package along with sample gauge time series 
