index,text
26295,the hyporheic zone hz is an area of interaction between surface and ground waters present in and around river beds bidirectional mixing within the hz termed hyporheic exchange flow hef plays significant roles in nutrient transport organic matter and biogeochemical processing in rivers the functional importance of the hz in river ecology and hydrology suggests that river managers should consider the hz in their planning to help compromised systems recover however current river restoration planning tools do not take into account the hz this paper describes a novel multiscale transferable method that combines existing environmental information at different spatial scales to identify areas with potentially significant hef for use in restoration prioritization and planning it uses a deductive approach that is suited for data poor case studies which is common for most rivers given the very limited data on the spatial occurrence of areas of hyporheic exchange results on nine contrasting european rivers demonstrate its potential to inform river management graphical abstract image 1 keywords hyporheic zone statistics hyporheic exchange flow cluster analysis catchment management river basin management 1 introduction the hyporheic zone hz orghidan 1959 is a region where surface and ground waters mix together within the bed and banks of a river it is characterized by a diverse fauna and by a bidirectional flow of water known as hyporheic exchange flow hef robertson and wood 2010 a large body of scientific literature has shown that both the physical and the biological components of the hz play a major role in river functioning findlay 1995 brunke and gonser 1997 krause et al 2011 hef is important for nutrient transport and cycling triska et al 1993 battin et al 2008 stream water temperature variation dugdale et al 2018 contaminant deposition and breakdown palumbo roe et al 2017 fuller and harvey 2000 organic matter processing sobczak and findlay 2002 zarnetske et al 2011 drummond et al 2014 danczak et al 2016 and the distribution and abundance of ecological communities dole olivier et al 2014 boulton 2007 battin et al 2016 perhaps the best known examples of the importance of hef on driving ecological processes concern the supply of oxygen into the sediment corson rikert et al 2016 gibbins et al 2016 and the modulation of biogeochemical transformation i e denitrification and nitrification processes wood and armitage 1999 mendoza lera and datry 2017 nogaro et al 2010 heppell et al 2014 as result of the strong and growing scientific evidence that hef support ecosystem level processes in river systems restoration practitioners have started to incorporate measures that promote hef to mitigate water quality impacts support biodiversity and increase ecological resilience hester and gooseff 2011 mendoza lera and datry 2017 restoration measures can induce or enhance hef through the generation of hydraulic gradients e g large wood step pools creation of geomorphological heterogeneity i e bedforms sediment sorting meandering realignment and reduction in sediment load e g sediment traps hester and doyle 2008 schirmer et al 2014 gordon et al 2013 tuttle et al 2014 however at present there is little guidance on appropriate siting of restoration measures to locations where hef has the greatest potential to be enhanced furthermore most of the hyporheic restoration work has thus far focused on in channel factors and has not expressly considered the hierarchy of processes at larger spatial scales that may influence hef as hef is defined by the interaction between surface and groundwater both surface and subsurface conditions influence the occurrence of hef at multiple spatial scales boano et al 2014 in fact hyporheic exchange exhibits scale dependency where hef at reach and sub reach scale is influenced significantly by larger scale hydrogeological patterns and processes boano et al 2006 wörman et al 2007 cardenas 2007 2008 stonedahl et al 2010 aubeneau et al 2015 this fractal dimension to hef wörman et al 2007 means that the occurrence rates spatial patterns and temporal variability of hef are determined by the interaction of physical chemical and biological processes in the river valley and catchment boano et al 2014 ward 2016 there are a large number of factors that influence these processes which can be divided into five broad and overlapping categories 1 hydrological 2 hydrogeological 3 topographic 4 anthropogenic and 5 ecological table a1 in supplementary material table 1 table 2 currently no framework exists to represent the complexity of multiple inter related and cross scale processes affecting the importance of hef taking account of typical data availability ward 2016 in river restoration prioritization and planning several analytical probabilistic and deterministic approaches have been developed to quantify and predict hef e g stream tracer injection experiments one dimensional advection dispersion transient storage models river network models hester et al 2017 cardenas 2008 2015 gomez velez and harvey 2014 boano et al 2014 cardenas and wilson 2007 cardenas et al 2004 kasahara and wondzell 2003 storey et al 2003 wroblicky et al 1998 wondzell and swanson 1996 harvey and bencala 1993 these different modelling approaches have helped to disentangle the mechanisms driving hyporheic mixing from a theoretical perspective and to quantify hef at very fine scales e g sub reach where detailed topographical data are available approaches based on channel planform and bedforms like nexss are applicable gomez velez and harvey 2014 however the bathymetric data needed to accurately map channel bedforms for nexss are only available for a limited number of rivers either large navigable lowland rivers like the mississippi gomez velez and harvey 2014 or headwater streams with low turbidity for which bathymetry data can be measured using bathymetric lidar or photogrammetric techniques for a review see grabowski et al 2014 consequently such approaches are not suitable for initial evaluation of hyporheic exchange for all channels in a river network in most catchments alternatives to these methods are hydrological classifications approaches which have been identified as both organizing frameworks and scientific tools for river research and management olden et al 2012 those approaches are common in the literature because they integrate factors and principles controlling hydrological processes and the causes of variations they have several advantages they are geographically independent and use available high quality hydrological geological topographical and ecological datasets that make deductive reasoning a valid approach to define spatial patterns in hydrological characteristics the deductive approach requires an accurate choice of environmental factors and the underlying process interactions in order to ensure that the data are representative of the total existing variation kennard et al 2010 restoration measures could be used at different scales to promote hef but tools are needed for practitioners that target the hz to help them prioritize restoration sites select approaches i e measures and monitor physical and ecological responses palmer et al 2010 hester and gooseff 2011 hester et al 2016 mendoza lera and datry 2017 in this paper we propose a novel and transferable method to identify potential areas of hef in river networks by combining and evaluating environmental data at reach segment and catchment scales the multiscale method merges statistical analyses with a priori knowledge on the processes controlling the hef and their relationships to provide an assessment of hef across broad spatial scales and where the availability of measured or modelled hyporheic data is scarce or absent this deductive approach using high quality hydrologically relevant environmental datasets that relate to the processes that enhance or limit hef avoids the reliance on detailed site specific information of hef which is rarely available for most rivers to inform restoration prioritization and planning 2 material and methods in this research we developed and applied a multiscale statistical method to identify potential suitable areas for hef focused restoration fig 1 the term suitable refers to conditions where factors indicate that hef has the potential to exist the method is used in hierarchy and consists of a supervised system that classifies hef at three spatial scales catchment segment and reach it is based on environmental factors that hydrological theory suggests be related with hyporheic flow tables 1 and 2 and table a1 in supplementary material but which association to diagnose hef in river systems has not been studied the multiscale method represents a deductive approach to hef classification that is geographically independent and depicted by a mosaic of factors across the catchment it uses readily available spatially comprehensive datasets rather than extensive hyporheic data as inputs cause those are often not available at scales of analysis greater than sub reach and reach scale 100 m and finally expert knowledge in this paper we present the application of the method to three scales but the formulae and the rationale explained are applicable to a finer resolution of scales the multiscale statistical approach involves a series of steps applied sequentially to the harmonized data at catchment segment and reach scales fig 1 1 step 1 variable subsetting the definition of several subsets of variables from factors that are identified as linked to hef section 2 2 the outcome of step 1 is a set of testable datasets 2 step 2 variable selection uses exploratory data mining techniques pca and x means cluster analysis to reduce the dimensionality of the input space from step 1 and to identify factors that are the most related to potential hef the outcome of step 2 is several clusters from each of the tested subsets from step 1 section 2 3 3 step 3 hyporheic classifier the semantic characterization of clusters and the assignment of a cl assifier 1 i e suitable and 0 i e unsuitable for every cluster in each tested subsets by an expert section 2 4 4 step 4 classifier merger uses a mathematical combination function to merge the classifier produced for each cluster and each subset by step 3 section 2 5 the output of step 4 is a single dataset of the merged cluster classifiers across subsets 5 step 5 large scale information merger the final step involves the application of a mathematical combination function to join the output of step 4 from one scale with the next larger scale section 2 6 the output of step 5 is a single dataset of the merged cluster classifiers across scales the end result of the classification is a binary classification of suitable and unsuitable areas of hef for clusters of unique variable combinations at each spatial scale fig 1 the algorithm was developed using the r scripting language r core team 2015 and relies on the implementations of x means 1 1 https i marine d4science org group biodiversitylab data miner operatorid org gcube dataanalysis wps statisticalmanager synchserver mappedclasses clusterers xmeans running on the d4science 2 2 https i marine d4science org group biodiversitylab data miner services coro et al 2013 2015 fig 1 2 1 environmental data 2 1 1 selection of environmental data the environmental data used to develop our method consisted of factors identified in the literature as potential influencing hef within detailed studies the association of these factors to diagnose hyporheic conditions in river system has not been studied before data were retrieved from remotely sensed and national datasets and consisted of hydrological hydrogeological topographic anthropogenic and ecological factors table 1 table 2 hydrological factors related to the quantity of water entering and flowing through the catchment and expression of surface and groundwater flows includes river and groundwater discharge dragoni and sukhija 2008 ward et al 2012 voltz et al 2013 hydrogeology encompasses factors that affect the distribution of groundwater in aquifers and subsurface flows geologic properties porosity grain size hydraulic conductivity heterogeneity of rocks type of aquifers and soils brunke and gonser 1997 kasahara and wondzell 2003 jones et al 2008 packman et al 2006 bardini et al 2012 hartwig and borchardt 2015 kasahara et al 2013 topographic factors were included because topography produces discontinuities in the direction of groundwater flows thus determining areas of groundwater discharge and recharge and of stream gradient and channel sinuosity anderson et al 2005 boano et al 2006 wörman et al 2006 2007 caruso et al 2016 similar to topography and hydrogeology anthropogenic factors influence hef at multiple spatial and temporal scales for instance land cover and use e g agricultural practices were included as a factor because directly impacting on evapotranspiration surface runoff soil compaction and erosion at valley scale all of which significantly impact on river hydrology and might represent a sediment source to reduce hef ryan et al 2010 didoné et al 2014 finally ecological factors related to the river valley lateral and vertical hydrological connectivity include riparian in channel vegetation and in channel wood vegetation dynamics can potentially feedback on the temporal variability of hef and likely increase the spatial heterogeneity of this ecological hydrological relationship 2 1 2 spatial discretization and data transformation data pre processing included spatial delineation of catchments segments and reaches for our case of study at first catchment boundaries were delineated using the hydrology toolset of the spatial analyst toolbox of arcgis 10 2 secondly segment units as sections of river that experience similar valley scale influences and energy conditions were delineated based on discontinuities in the gradient along the longitudinal profile of the river network and in sub catchment areas the number of segments in a catchment was related to the increase in catchment area due to tributary confluences the confluence was deemed significant when the sub catchment area drained by the tributary was greater than 20 of the main stem catchment area immediately upstream of the junction gurnell et al 2014 river reaches were delineated based primarily on their channel planform the river channel was divided into sinuosity units based on changes in the axis of the overall planimetric course the units that differed in sinuosity by more than 10 were considered separate reaches continuous temporal and spatial variables i e temperature and elevation were summarized by summary statistics mean standard deviation minimum and maximum fig 1 table a2 and table a3 in the supplementary material for spatial fuzzy variables i e bedrock geology the relative contribution of each bedrock class i e chalk geology was expressed as percentage of occupied surface area with respect to the variable overall area and then scale in the range 0 and 1 fig 1 table a2 and table a3 in the supplementary material spatial categorical variables as permeability classes were numerically ranked according to the number of classes i e very high 4 high 3 low 2 very low 1 fig 1 table a2 and table a3 in the supplementary material 2 2 step 1 variables subsetting the full set of data containing the environmental variables for all case study is manually subset into groups of variables this is a necessary preliminary step to statistical discriminant analysis otherwise not directly applicable given the large set of information reporting dependent variables noise or missing data furthermore there are usually more variables than rivers that cause difficulties in identify similarity between variables of each group of rivers and minimize the similarity between groups using statistical discriminant analysis these subsets can contain overlapping variables e g sharing one variable and can be semantically driven e g subset of aquifer type or temperature ranges fig 1 the subsets will be analysed independently at the end the independent analysis of multiple variable subsets will provide information about discarded variables that are not correlated to hef in either step 2 or step 4 2 3 step 2 variables selection in step 2 the variable subsets are analysed independently using principal component analysis pca to explore patterns in data variability among rivers and then complemented by cluster analysis to identify combinations of variables possibly indicating hyporheic responses in a given river area first a pca is performed to reduce the dimensionality of the input space jolliffe 2002 by selecting only the principal components associated with the largest eigenvalues new vectors are obtained in the transformed space that have smaller dimensions these vectors are associated to the largest variance directions of the principal components and hence selected for the cluster analysis variables selection fig 2 discarded variables can still be included and analysed in other variable subsets or scale if the presence of those variables is known to be important for hef at this stage the reduced dimensional space is optimized with respect to the information variance contained in the data thus facilitating the application of cluster analysis to the pca output ding and he 2004 our method uses the distance based x means algorithm pelleg et al 2000 a variant of the most common k means macqueen 1967 the x means algorithm was chosen after testing the dbscan density based clustering algorithm ester et al 1996 which did not produce meaningful grouping of the case studies i e in most of the cases vectors were all classified as outliers contrarily to k means xmeans requires indicating a minimum and a maximum number of clusters kmin and kmax the algorithm applies kmeans to the data for all the possible k values in the indicated range kmeans finds the best assignment of the vectors to the k clusters and produces a score for this assignment based on the average squared distance of the points to their clusters centroids distortion measure xmeans reports the output of the kmeans execution that produced the best score the associated k is the best number of clusters xmeans is also more efficient with respect to kmeans because it uses kd trees bentley 1975 and blacklisting as support to the processing the x means algorithm pelleg et al 2000 is applied to the pca transformed vectors generating optimal grouping clusters of vectors according to their distances clustering the dimensionally reduced pca transformed vectors helps to find the best grouping in this space since the vectors belonging to the same cluster are close in the pca transformed space ding and he 2004 each cluster produced by xmeans is characterized by a centroid which is a representative vector of the cluster in our method the centroid is interpreted as a summary of the characteristics of the cluster in the pca transformed space re projecting the clusters centroids to the original space allows obtaining the coordinates of the centroids expressed in terms of the original variables re projection is mathematically possible although the pca transformed space has reduced dimensionality with respect to the original space however during this step some information is lost hence our method analyses the distribution of the variables onto the re projected centroids specifically we calculate the distances between the variable value and the coordinates of the re projected centroids for each variable the number of times a centroid coordinate is closest to a real data value is also recorded a tolerance threshold of 25 is applied before the final clustering on the features having the most uniform distributions over the centroids this step allows the selection of variables that are equally distributed over the centroids and accounts for the loss of information during the re projection the following example illustrates the criteria used to retain or discard the variables suppose 2 data clusters are identified for 8 rivers defined by vectors of elevation channel gradient and temperature if 4 elevation values are determined to be closest to cluster a and the other 4 to cluster b the elevation variable would be retained because the 25 tolerance threshold is exceeded i e 2 rivers assigned to a cluster if 2 channel gradient values were assigned to cluster a and 6 to cluster b the channel gradient variable would be discarded because the threshold 2 is not exceeded and if 5 temperature values were assigned to cluster a and 3 to cluster b temperature would be retained in the analysis in conclusion by construction of the pca algorithm if the variables are independent and carry high variance then the pca transformed space would correspond to the original space thus the centroids would take all of the variables into account resulting in equal distributions of the vectors coordinates on the centroids coordinates ding and he 2004 a variable that is not assigned to a cluster does not indicate a missing value for that cluster but it has been discarded during the clustering analysis 2 4 step 3 hyporheic classifiers the unique combinations of variables that are generated by the cluster analysis step 2 and their centroids are used to assess suitable and unsuitable areas for hef restoration for a river area using human expertise the expert provides a semantic description to each cluster in each subset using the centroid of the cluster and then assigns a hyporheic classifier 1 suitable or 0 unsuitable which indicates if the environmental conditions depicted by the clusters lead i e 1 or not i e 0 to hef the use of expert knowledge is required because empirical data on hef is not available for all of these unique combinations the expert bases this assignment on the variable types the distribution of the variables in each cluster and on the knowledge of the hydrological hydrogeologic topographic anthropogenic and ecological factors that yield hef following the relationships summarized in table a1 in the supplementary material at the end of the step 3 the initial set of variables has been factored into clusters semantically described and labelled examples tables a6 a7 a8 in the supplementary material the next section explains how these clusters are combined which corrects errors in the cluster label assignment and cluster analysis 2 5 step 4 classifier merger classifiers for each cluster and subset are merged together using a mathematical combination function the criterion used for the mathematical combination function is to indicate that areas of hef are suitable only if over half of the hyporheic classifiers indicate that it is suitable the mathematical combination function allows us to account for errors in the hyporheic classifiers due to mis labelling of the clusters the combination function is the normalized sum of all the sub classification for each case study c s r i 1 n c s i r n c r 1 c s r 50 0 o t h e r w i s e where r is the complete set of variables associated to a river area s i is the i th of n variable subset c s i r is the i th binary hyporheic classification over the s i variable subset c s r is the normalized sum of all the sub classifications for the river area r and c r is the final classification function if c s r is higher than 50 the river area r is classified as suitable otherwise the classifier assesses unsuitable this threshold was set after heuristic evaluation of a small 20 subset of our data 2 6 step 5 large scale merging to increase the accuracy of predictions as the spatial scale becomes finer the last step of the method is to combine the binary classifiers from different scales using a downscaling approach the rationale behind the combination function is the following if the system predicts that hef areas are suitable in a river at a large spatial scale then it is more likely to present suitable areas at smaller spatial scales nestled within the larger area for example a positive binary 1 classification at catchment scale suggests that suitable environmental conditions exist for hef in the catchment area at this scale of analysis the accuracy of the classification is generally higher because it is not required to precisely identify the specific location of hyporheic exchange hence a smaller scale classifier can use the information from a larger scale classifier because it represents the presence of factors that drive hef our method embeds this approach using a bonus function 20 weighting in the equation that combines the output of a classifier with the output of the next largest scale classifier the classification is recalculated for finer scales as follows c l a r g e r c s r 20 c l a r g e s c a l e r c r 1 c l a r g e r 50 0 o t h e r w i s e where c s r is the normalized sum of all the sub classifications for river area r and c l a r g e s c a l e r is the dichotomic score of the first larger scale also in this case the threshold 50 has been set after heuristic analysis on a small 20 subset of our data 3 results this section reports the results of the application of the multiscale statistical method to the nine test catchments the cluster results were compared to expert opinion section 3 1 and discussed at each spatial scale section 3 2 3 1 validation and reliability of the classification results the x means algorithm identified three optimal clusters in all the three spatial scales considered in the study to evaluate whether the developed multiscale statistical approach could identify suitable and unsuitable areas for hyporheic exchange to occur the reliability of the identified clusters was evaluated by examining the representativeness of the variables among the clusters against human expertise by the authors in the assessment the lead author manually assigned one of the interpretations of the xmeans clusters i e 1 or 0 to each river catchment i e 8 catchments and 118 variables for the uk case of study 86 variables for the polish case study segment 51 segments and 48 variables for the uk case of study 10 segments and 35 variables for the polish case study and reach 135 reaches and 59 variables for the uk case of study 11 reaches and 74 variables for the polish case study at this stage the expert evaluation differs from the expert information within the model step 4 because it is performed on the original environmental data section 2 1 and not on the clusters a confusion matrix was used to assess the agreement between the expert assignment binary 1 and 0 and x means clusters as the percentage of matching assignments absolute percentage of agreement furthermore the cohen s kappa cohen 1960 was calculated to estimate the agreement between the expert and the model compared to purely random assignments the x means results agreed generally with expert opinion indicating reliable semantic interpretations of the categories identified in the clusters variations at catchment scale the absolute percentage of agreement is 88 and 75 at segment 75 and 78 and at reach 74 and 82 for the uk and polish case studies respectively table 4 table 5 as the binary classifiers for each scale in step 5 take account of the information from the next largest scale i e catchment classifiers influencing segment classifiers to represent the scale dependence in hef the model performance is expected to increase within decreasing scale in the uk case of study the catchment scale effectively added information to the segment scale step 5 because the agreement increases of 1 percentage point table 6 however in the biebrza application no performance increase was detected table a4 in supplementary material 3 2 prediction of hef at different spatial scales hef suitable and unsuitable areas were predicted at all three spatial scales for the examined rivers fig 3 table 3 at catchment scale unsuitable conditions for hef are predicted for the rivers dove exe tone and wye fig 4 table 7 these rivers are predominantly characterized by confined or semiconfined aquifers poorly sorted superficial deposits from coarse sand to silt and clay 50 cover over the catchment in contrast for the rivers frome piddle tern and rother the semi automatic classification method predicts suitable areas for hef to occur the clusters for these rivers depict predominantly complex aquifers with flows though fractures and discontinuities terrigenous deposits with sorted sand and gravel 30 45 silt and clay deposits less than 20 of cover on the catchment at segment scale hef is found to be characterized by suitable areas for all the identified segments in the rivers piddle tern wye and the biebrza river fig 4 tables 3 and 7 conversely hef is predicted to be low for all the segments in the rivers dove rother and tone the rivers exe and frome are predicted to have a mixture of suitable and unsuitable hef areas in different segments where suitable hef condition is predicted the clusters are mainly characterized by sandstone geology a low fraction fine sediments between 10 and 30 cover over the segments large fraction of sorted gravel and sand deposits between 20 and 50 cover over the segments channel sinuosity of 1 2 and low channel gradient 0 002 in segments with unsuitable conditions for hef the clusters describe mudstone and sandstone geology low channel gradients high percentage of clay and fines 55 cover and high percentage of arable and grassland 70 cover within 150 m of the river channel for the biebrza river the segments which are predicted to have suitable hef conditions are characterized by sinuosity 1 3 high percentage of gravel and sand deposits 40 high percentage of productive aquifer and low percentage of pasture lands 10 within 150 m of the main river channel finally at reach scale the multiscale statistical method predicted suitable hef areas for 3 rivers of the 9 evaluated the frome piddle and biebrza fig 4 tables 3 and 7 generally the clusters indicating suitable conditions for hef exhibit a low percentage of in channel vegetation 2 10 of the reach gravel substrates 10 very low percentage of silt and clay deposits 1 presence of pools and riffles 5 10 and a low percentage of poached or overgrazed river banks 5 cluster indicating unsuitable hef areas are mainly described by poached river banks presence of in channel emergent vegetation and reeds low percentage of gravel substrates low number of pools and riffles and low mean flow velocity in the biebrza river clusters indicating suitability relate to superficial geology dominated by peat 80 cover on the entire reach and mud 10 while those indicating unsuitability are dominated by mud 60 and peat 10 deposits low percentage of sand and gravels and high percentage of unsorted till deposit 50 and pasture lands 4 discussion and conclusion the multiscale statistical method was developed and applied to nine rivers across europe to identify suitable and unsuitable reaches segments and catchments for hef focused restoration the results of the classification showed good to moderate agreement cohen s kappa with expert opinion indicating reliable categories and semantic interpretations of the clusters reasonable agreement is also observed with in situ empirical data from previous studies given the unavoidable differences in scale between these detailed local research studies and our broad scale approach in this section we discuss the results of the classification against field observations of actual hef the major predictors of suitable and unsuitable areas section 4 1 and finally the domain of application of the method section 4 2 4 1 linking processes to factors at each spatial scale catchment segment and reach cluster results show groups of predictors that influence the determination of suitable and unsuitable areas for hef restoration hydrological factors i e groundwater level discharge influence hef by changing surface water flow regimes and distributions of hydraulic head table a1 supplementary material hydrogeological factors affect water flowing through the river bed by sediment grain size sediment heterogeneity and depth therefore promoting spatially diverse hyporheic exchange packman and salehin 2003 table a1 supplementary material topographic factors such as catchment gradient individual bedforms and bedforms sequences and valley confinement drive hydrodynamic and hydrostatic forces that affect the variability of hef from cm to km scale table a1 supplementary material anthropogenic factors such as in channel structures i e weirs dams land management and land use impact hef by modifying river stage fluctuations changing sediment delivery and channel complexity and by altering vertical hydraulic gradients table a1 supplementary material also vegetation has long been known to exert a strong control on land surface hydrology by moderating streamflow and groundwater recharge table a1 supplementary material as an ecological factor vegetation feedbacks on the temporal variability of hef and likely increase the spatial heterogeneity of this ecological hydrological relationship this section presents the different factors affecting suitable and unsuitable hef restoration areas and compares the hef predictions at reach scale to in situ empirical data from previous studies high percentages of poached banks emergent in channel vegetation improved grassland and low geomorphological complexity and low number of pool and riffle sequences were associated with unsuitable reaches in the frome 1 reach and in the piddle catchments 6 reaches table 7 dunscombe 2011 observed weak vertical hydraulic gradients vhgs at the head and tail of riffles in both the rivers frome and piddle indicating little to no hef at this scale this is a finer scale than the prediction of our model which overall classifies that reach as unsuitable fig 4e these neighboring catchments are found in the south of england and are underlain by chalk bedrock chalk has a high secondary porosity and groundwater flows easily through fractures and fissures in the bedrock to these gravel bed rivers waters and banks 1997 the combination of a permeable chalk geology and coarse sediment would be expected to strongly support hef morrice et al 1997 hiscock 2007 however there are several reasons for unsuitable conditions in these rivers i the pronounced groundwater flows create strongly gaining and losing conditions in reaches which drive contraction gaining or expansion losing of hz and shortening of hef paths wondzell and gooseff 2013 fox et al 2014 malzone et al 2016a 2016b ii the rivers have few instream geomorphic features that would generate advective pore water flow into through and out of the river bed elliott and brooks 1997 tonina and buffington 2009 and iii high fine sediment loads have led to clogging of the coarse gravel bed boulton and hancock 2006 pretty et al 2006 several studies have shown that chalk rivers in england have elevated fine sediment loads derived principally from cultivated agricultural land walling and amos 1999 collins and walling 2007 grabowski and gurnell 2016 and grazing pressure trimble and mendel 1995 bilotta and brazier 2008 bilotta et al 2010 also in channel vegetation appears be an important factor at this scale of analysis while vegetation patches have been shown to narrow the active channel increasing water velocities and mobilizing the gravel bed cotton et al 2006 the localised reduced velocities within vegetation patches promote deposition of sediment and organic matter decreasing bed permeability and reducing or eliminating hef salehin et al 2004 ensign and doyle 2005 corenblit et al 2007 for the wye river the results of the statistical method agreed with dunscombe 2011 observations weak vhgs while for the rivers tone dove the predictions did not align with field data our method predicts unsuitable areas for hef at the reach scale along the tone and the dove while dunscombe 2011 observed strong patterns of up and downwelling flows at the head and tail of riffles on both rivers for the river tern all reaches were identified as unsuitable areas by our method however empirical hef data at a pool riffle pool sequence showed temporal flow patterns occurring around this geomorphic feature at the sub reach scale krause et al 2011 hannah et al 2009 suitable areas for hef were predicted consistently across all spatial scales for the rivers dove and the tone but not for the tern wye rother piddle frome exe and biebrza at catchment scale the clusters for the dove and tone are characterized by well distributed variables sandstone is mixed with mudstone and siltstone bedrock geology and clay and silt superficial deposits represent more than the 50 of the catchment similarly the hydrogeology is dominated by unconfined but low producing aquifers while the sandstone bedrock would normally support surface subsurface exchange hiscock 2007 the low conductivity superficial deposits characterizing the clusters more than 50 of the catchment area would likely limit or restrict vertical hyporheic flow indeed the role of local sediment deposits in preventing or limiting groundwater surface water interactions has been recognised for unconfined alluvial channels gurnell et al 2014 at segment scale clusters characterized by low slopes high percentage of in channel fine sediments and extensive arable lands around the river channel are depicted in the clusters possibly suggesting an impact of sediment delivery from the surrounding lands and simplification of landscape complexity gooseff et al 2007 boano et al 2014 at reach scale suitable conditions for hef were predicted in some reaches of the biebrza frome and piddle fig 4 for the biebrza river the reaches identified as suitable fig 4a in our classification corresponded in spatial extent to one reach of our analysis which were previously observed to have upwelling and sections of recharge anibas et al 2012 these reaches were characterized mainly by a geology of peat and peat mixed with mud our clusters identified peat as an important variable controlling hef at the reach scale this reflects the underlying process controls as the physical structure and stratigraphy of peat has pronounced influence on the dynamics of water retention storage and solute transport rezanezhad et al 2016 anibas et al 2012 described two main types of peat soils that showed different behaviors in driving hef flows at the sediment water interface soil i has a loose structure covered in reed vegetation and characterized by high flow fluxes while soil ii is more compact and has lower flow fluxes in our data for the biebrza peat characteristics are heterogeneous across reaches varying from loose similar to soil type i to more compact and mud dominated similar to soil type ii therefore the overall assessment and spatial distribution of hef predictions at reach scale in the biebrza catchment are supported by the findings of anibas et al 2012 a possible reason of the difference in outputs between the predicted hef conditions by the multiscale approach and in situ observations is the different spatial and temporal resolutions in situ measurements commonly focuses on an individual bedform or feature or sequences of them i e meter to 10 s m scale are influenced by temporal variations that are not considered in the proposed approach moreover the resolution of geomorphological data used in these case studies is coarser than the detailed sub reach scale observations of hef river habitat survey rhs data was used as point estimates of in channel conditions for uk rivers while rhs data is ideal for this type of analyses in many ways e g national coverage reach survey scale it is a visual appraisal of river habitats and geomorphic features and does not involve topographical or hydrogeological measurements raven et al 1996 rhs assesses river habitat within a 500 m long reach using 10 spot checks and a sweep up survey to count key features or river channel while it does record many features relevant to hyporheic flow e g vegetation type artificial structures channel substrate and emergent bedforms it does not quantify or map these features at a sub reach scale which is the scale used in many empirical studies of hyporheic flow spatial resolution explains differences by scale where suitable areas for hef to occur are predicted only at spatial scales larger than the reach scale i e river tern and river rother finally results in table 6 depict a scale dependence effect between catchment and segment scales the small increment 1 percentage point in the confusion matrix suggests that upper hierarchical levels inform on general conditions at low resolution and exert constraints on the lower level which informs at higher resolution and provides mechanistic explanation for higher levels 4 2 application to river restoration planning this study proposes a multiscale statistical method to identify where hef potentially occurs at catchment segment and reach scale i e an area that is suitable for hef based restoration the approach and results presented in this study use readily available environmental datasets enabling the method to be transferred to other catchments restoration practitioners are increasingly considering the hz in their management plans because of the crucial role it plays in river biogeochemical processing and the transferring of solutes and oxygen between surface waters groundwater and the hz findlay 1995 nogaro et al 2010 mendoza lera and datry 2017 thus there is a strong need to provide river managers and restoration practitioners with a tool that can be applied to any catchment and which is flexible enough to work with the data sources available in different regions and countries as highlighted by other framework approaches i e reform gurnell et al 2014 structuring the analysis around multiple scales improves spatial and temporal understanding of the variability of environmental factors in river systems and how reaches have been impacted by catchment scale changes therefore our approach supports broader restoration planning that includes catchment scale solutions merill and tonjes 2014 wortley et al 2013 hester and gooseff 2011 to assist river restoration practitioners we propose that this multi scale statistical process be run as a preliminary assessment step in restoration planning to identify and possibly prioritize restoration actions i e reach locations across a catchment restoration managers can benefit from the classification analysis by evaluating how well hydrological hydrogeological topographical anthropogenic and ecological factors describe hyporheic drivers fig 5 first by interrogating the clusters generated by step 2 managers can be informed about i environmental and hyporheic drivers on the targeted areas ii identify areas with the same hydrological hydrogeological topographical anthropogenic and ecological context and iii their spatially uniqueness second by examining the final confusion matrices step 4 which embed a summary of knowledge across the domains of hydrology geology and hyporheic theories and their related environmental data and provide insights into the spatial variability of hef in a catchment finally by using the results of the multi scale assessment step 5 river managers can define a posteriori what processes management actions are important for each reaches and then feedback to management actions considering the above information river managers can choose between passive and active approaches for example some of the factors depicted in the clusters will be intrinsic i e bedrock geology and cannot be changed by management measures while others will be dynamic i e land use vegetation channel geomorphology and therefore might become a target for catchment or river management if suitable hef conditions are predicted a passive approach will likely be preferred and include measures that do not directly address hyporheic conditions but that take advantage of hef to preserve and maintain for example habitat diversity or soil erosion reduction the river restoration centre 2013 the passive approach would include in situ evaluation to verify that the method predictions are representative of local conditions conversely if unsuitable hef conditions are predicted an active approach can be adopted and local or restoration measures applied accordingly to the factors involved for example the case study on the river rother showed suitable conditions for hef to occur at catchment scale i e complex aquifer gravel to sand deposits while unsuitable conditions were predicted in segments and reaches i e low channel gradient and sinuosity clay and lenses an active restoration approach would be appropriate to implement local restoration measures for enhancing local hyporheic flows and ecological functioning in this river fig 5 in our opinion the identified factors for hef have intuitive general validity but we expect that in other applications the method would be tailored to site specific characteristics and applied to other factors at reach and sub reach scales the classification is generally limited by the resolution and quality of the available data this is a general issue when using environmental surrogates of hydrological processes especially due to the coarse resolution of the data olden et al 2012 we qualitatively compared the prediction of the method on available empirical hyporheic evidence that was i spatially and temporally limited to local scales ii collected using multiple methods and iii focused on specific geomorphic features such as bedforms that likely trigger local advective hef even when catchment conditions limit larger scale flows in the future we expect this evidence based problem to be overcome by technology and more complete and uniform metadata associated with hyporheic studies finally existing scientific literature suggests that knowing how and what to prioritize in restoration actions for aquatic ecosystems are fundamental to effective restoration planning wohl et al 2005 there is an increasing emphasis on addressing hyporheic zones into restoration to allow more comprehensive hydro ecological understanding of aquatic ecosystems our model can support restoration as a first order assessment to target hz and thus provide the greatest benefits to restoration plans author contributions chiara magliozzi conceived designed and analysed the dataset producing and assessing the final clusters gianpaolo coro supervised the whole design and contributed to the development of the methodology from the statistical perspective aaron packman and stefan krause provided hydrological perspectives robert grabowski edited and assisted the revision of the mansucript and provided hydromorphological and ecological perspectives all authors read and approved the manuscript conflicts of interest the authors declare no conflicts of interest acknowledgements this work was supported by the marie sklodowska curie action horizon2020 within the project hypotrain grant agreement number 641939 g coro was also supported by the bluebridge project grant agreement number 675680 a i packman was also supported by the u s national science foundation grant agreement number ear 1344280 we thank the networked multimedia information systems laboratory ne mis research laboratory of isti cnr italy for providing full support for the development of this research the biebrza national park for providing the needed data for the development of the river biebrza case of study data sources preservation of wetland habitats in the upper biebrza valley life11 nat pl 422 co funded by life the financial instrument of the european commission the national fund for environmental protection and water management and biebrza national park restoration of hydrological system in middle basin of the biebrza valley phase i co funded by life project thanks to dr christian anibas for his availability in sharing information on the river biebrza the environment agency and dr marc naura who provided the river habitat survey data for the u k catchments we thank the british geological survey the centre for ecology hydrology the uk met office the european soil data centre the european environment agency and the polish geological institute as data providers we also thank prof ian holman and the three anonymous reviewers for their helpful comments on the manuscript appendix a supplementary data the following is the supplementary data to this article supplementarymaterial supplementarymaterial appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 006 
26295,the hyporheic zone hz is an area of interaction between surface and ground waters present in and around river beds bidirectional mixing within the hz termed hyporheic exchange flow hef plays significant roles in nutrient transport organic matter and biogeochemical processing in rivers the functional importance of the hz in river ecology and hydrology suggests that river managers should consider the hz in their planning to help compromised systems recover however current river restoration planning tools do not take into account the hz this paper describes a novel multiscale transferable method that combines existing environmental information at different spatial scales to identify areas with potentially significant hef for use in restoration prioritization and planning it uses a deductive approach that is suited for data poor case studies which is common for most rivers given the very limited data on the spatial occurrence of areas of hyporheic exchange results on nine contrasting european rivers demonstrate its potential to inform river management graphical abstract image 1 keywords hyporheic zone statistics hyporheic exchange flow cluster analysis catchment management river basin management 1 introduction the hyporheic zone hz orghidan 1959 is a region where surface and ground waters mix together within the bed and banks of a river it is characterized by a diverse fauna and by a bidirectional flow of water known as hyporheic exchange flow hef robertson and wood 2010 a large body of scientific literature has shown that both the physical and the biological components of the hz play a major role in river functioning findlay 1995 brunke and gonser 1997 krause et al 2011 hef is important for nutrient transport and cycling triska et al 1993 battin et al 2008 stream water temperature variation dugdale et al 2018 contaminant deposition and breakdown palumbo roe et al 2017 fuller and harvey 2000 organic matter processing sobczak and findlay 2002 zarnetske et al 2011 drummond et al 2014 danczak et al 2016 and the distribution and abundance of ecological communities dole olivier et al 2014 boulton 2007 battin et al 2016 perhaps the best known examples of the importance of hef on driving ecological processes concern the supply of oxygen into the sediment corson rikert et al 2016 gibbins et al 2016 and the modulation of biogeochemical transformation i e denitrification and nitrification processes wood and armitage 1999 mendoza lera and datry 2017 nogaro et al 2010 heppell et al 2014 as result of the strong and growing scientific evidence that hef support ecosystem level processes in river systems restoration practitioners have started to incorporate measures that promote hef to mitigate water quality impacts support biodiversity and increase ecological resilience hester and gooseff 2011 mendoza lera and datry 2017 restoration measures can induce or enhance hef through the generation of hydraulic gradients e g large wood step pools creation of geomorphological heterogeneity i e bedforms sediment sorting meandering realignment and reduction in sediment load e g sediment traps hester and doyle 2008 schirmer et al 2014 gordon et al 2013 tuttle et al 2014 however at present there is little guidance on appropriate siting of restoration measures to locations where hef has the greatest potential to be enhanced furthermore most of the hyporheic restoration work has thus far focused on in channel factors and has not expressly considered the hierarchy of processes at larger spatial scales that may influence hef as hef is defined by the interaction between surface and groundwater both surface and subsurface conditions influence the occurrence of hef at multiple spatial scales boano et al 2014 in fact hyporheic exchange exhibits scale dependency where hef at reach and sub reach scale is influenced significantly by larger scale hydrogeological patterns and processes boano et al 2006 wörman et al 2007 cardenas 2007 2008 stonedahl et al 2010 aubeneau et al 2015 this fractal dimension to hef wörman et al 2007 means that the occurrence rates spatial patterns and temporal variability of hef are determined by the interaction of physical chemical and biological processes in the river valley and catchment boano et al 2014 ward 2016 there are a large number of factors that influence these processes which can be divided into five broad and overlapping categories 1 hydrological 2 hydrogeological 3 topographic 4 anthropogenic and 5 ecological table a1 in supplementary material table 1 table 2 currently no framework exists to represent the complexity of multiple inter related and cross scale processes affecting the importance of hef taking account of typical data availability ward 2016 in river restoration prioritization and planning several analytical probabilistic and deterministic approaches have been developed to quantify and predict hef e g stream tracer injection experiments one dimensional advection dispersion transient storage models river network models hester et al 2017 cardenas 2008 2015 gomez velez and harvey 2014 boano et al 2014 cardenas and wilson 2007 cardenas et al 2004 kasahara and wondzell 2003 storey et al 2003 wroblicky et al 1998 wondzell and swanson 1996 harvey and bencala 1993 these different modelling approaches have helped to disentangle the mechanisms driving hyporheic mixing from a theoretical perspective and to quantify hef at very fine scales e g sub reach where detailed topographical data are available approaches based on channel planform and bedforms like nexss are applicable gomez velez and harvey 2014 however the bathymetric data needed to accurately map channel bedforms for nexss are only available for a limited number of rivers either large navigable lowland rivers like the mississippi gomez velez and harvey 2014 or headwater streams with low turbidity for which bathymetry data can be measured using bathymetric lidar or photogrammetric techniques for a review see grabowski et al 2014 consequently such approaches are not suitable for initial evaluation of hyporheic exchange for all channels in a river network in most catchments alternatives to these methods are hydrological classifications approaches which have been identified as both organizing frameworks and scientific tools for river research and management olden et al 2012 those approaches are common in the literature because they integrate factors and principles controlling hydrological processes and the causes of variations they have several advantages they are geographically independent and use available high quality hydrological geological topographical and ecological datasets that make deductive reasoning a valid approach to define spatial patterns in hydrological characteristics the deductive approach requires an accurate choice of environmental factors and the underlying process interactions in order to ensure that the data are representative of the total existing variation kennard et al 2010 restoration measures could be used at different scales to promote hef but tools are needed for practitioners that target the hz to help them prioritize restoration sites select approaches i e measures and monitor physical and ecological responses palmer et al 2010 hester and gooseff 2011 hester et al 2016 mendoza lera and datry 2017 in this paper we propose a novel and transferable method to identify potential areas of hef in river networks by combining and evaluating environmental data at reach segment and catchment scales the multiscale method merges statistical analyses with a priori knowledge on the processes controlling the hef and their relationships to provide an assessment of hef across broad spatial scales and where the availability of measured or modelled hyporheic data is scarce or absent this deductive approach using high quality hydrologically relevant environmental datasets that relate to the processes that enhance or limit hef avoids the reliance on detailed site specific information of hef which is rarely available for most rivers to inform restoration prioritization and planning 2 material and methods in this research we developed and applied a multiscale statistical method to identify potential suitable areas for hef focused restoration fig 1 the term suitable refers to conditions where factors indicate that hef has the potential to exist the method is used in hierarchy and consists of a supervised system that classifies hef at three spatial scales catchment segment and reach it is based on environmental factors that hydrological theory suggests be related with hyporheic flow tables 1 and 2 and table a1 in supplementary material but which association to diagnose hef in river systems has not been studied the multiscale method represents a deductive approach to hef classification that is geographically independent and depicted by a mosaic of factors across the catchment it uses readily available spatially comprehensive datasets rather than extensive hyporheic data as inputs cause those are often not available at scales of analysis greater than sub reach and reach scale 100 m and finally expert knowledge in this paper we present the application of the method to three scales but the formulae and the rationale explained are applicable to a finer resolution of scales the multiscale statistical approach involves a series of steps applied sequentially to the harmonized data at catchment segment and reach scales fig 1 1 step 1 variable subsetting the definition of several subsets of variables from factors that are identified as linked to hef section 2 2 the outcome of step 1 is a set of testable datasets 2 step 2 variable selection uses exploratory data mining techniques pca and x means cluster analysis to reduce the dimensionality of the input space from step 1 and to identify factors that are the most related to potential hef the outcome of step 2 is several clusters from each of the tested subsets from step 1 section 2 3 3 step 3 hyporheic classifier the semantic characterization of clusters and the assignment of a cl assifier 1 i e suitable and 0 i e unsuitable for every cluster in each tested subsets by an expert section 2 4 4 step 4 classifier merger uses a mathematical combination function to merge the classifier produced for each cluster and each subset by step 3 section 2 5 the output of step 4 is a single dataset of the merged cluster classifiers across subsets 5 step 5 large scale information merger the final step involves the application of a mathematical combination function to join the output of step 4 from one scale with the next larger scale section 2 6 the output of step 5 is a single dataset of the merged cluster classifiers across scales the end result of the classification is a binary classification of suitable and unsuitable areas of hef for clusters of unique variable combinations at each spatial scale fig 1 the algorithm was developed using the r scripting language r core team 2015 and relies on the implementations of x means 1 1 https i marine d4science org group biodiversitylab data miner operatorid org gcube dataanalysis wps statisticalmanager synchserver mappedclasses clusterers xmeans running on the d4science 2 2 https i marine d4science org group biodiversitylab data miner services coro et al 2013 2015 fig 1 2 1 environmental data 2 1 1 selection of environmental data the environmental data used to develop our method consisted of factors identified in the literature as potential influencing hef within detailed studies the association of these factors to diagnose hyporheic conditions in river system has not been studied before data were retrieved from remotely sensed and national datasets and consisted of hydrological hydrogeological topographic anthropogenic and ecological factors table 1 table 2 hydrological factors related to the quantity of water entering and flowing through the catchment and expression of surface and groundwater flows includes river and groundwater discharge dragoni and sukhija 2008 ward et al 2012 voltz et al 2013 hydrogeology encompasses factors that affect the distribution of groundwater in aquifers and subsurface flows geologic properties porosity grain size hydraulic conductivity heterogeneity of rocks type of aquifers and soils brunke and gonser 1997 kasahara and wondzell 2003 jones et al 2008 packman et al 2006 bardini et al 2012 hartwig and borchardt 2015 kasahara et al 2013 topographic factors were included because topography produces discontinuities in the direction of groundwater flows thus determining areas of groundwater discharge and recharge and of stream gradient and channel sinuosity anderson et al 2005 boano et al 2006 wörman et al 2006 2007 caruso et al 2016 similar to topography and hydrogeology anthropogenic factors influence hef at multiple spatial and temporal scales for instance land cover and use e g agricultural practices were included as a factor because directly impacting on evapotranspiration surface runoff soil compaction and erosion at valley scale all of which significantly impact on river hydrology and might represent a sediment source to reduce hef ryan et al 2010 didoné et al 2014 finally ecological factors related to the river valley lateral and vertical hydrological connectivity include riparian in channel vegetation and in channel wood vegetation dynamics can potentially feedback on the temporal variability of hef and likely increase the spatial heterogeneity of this ecological hydrological relationship 2 1 2 spatial discretization and data transformation data pre processing included spatial delineation of catchments segments and reaches for our case of study at first catchment boundaries were delineated using the hydrology toolset of the spatial analyst toolbox of arcgis 10 2 secondly segment units as sections of river that experience similar valley scale influences and energy conditions were delineated based on discontinuities in the gradient along the longitudinal profile of the river network and in sub catchment areas the number of segments in a catchment was related to the increase in catchment area due to tributary confluences the confluence was deemed significant when the sub catchment area drained by the tributary was greater than 20 of the main stem catchment area immediately upstream of the junction gurnell et al 2014 river reaches were delineated based primarily on their channel planform the river channel was divided into sinuosity units based on changes in the axis of the overall planimetric course the units that differed in sinuosity by more than 10 were considered separate reaches continuous temporal and spatial variables i e temperature and elevation were summarized by summary statistics mean standard deviation minimum and maximum fig 1 table a2 and table a3 in the supplementary material for spatial fuzzy variables i e bedrock geology the relative contribution of each bedrock class i e chalk geology was expressed as percentage of occupied surface area with respect to the variable overall area and then scale in the range 0 and 1 fig 1 table a2 and table a3 in the supplementary material spatial categorical variables as permeability classes were numerically ranked according to the number of classes i e very high 4 high 3 low 2 very low 1 fig 1 table a2 and table a3 in the supplementary material 2 2 step 1 variables subsetting the full set of data containing the environmental variables for all case study is manually subset into groups of variables this is a necessary preliminary step to statistical discriminant analysis otherwise not directly applicable given the large set of information reporting dependent variables noise or missing data furthermore there are usually more variables than rivers that cause difficulties in identify similarity between variables of each group of rivers and minimize the similarity between groups using statistical discriminant analysis these subsets can contain overlapping variables e g sharing one variable and can be semantically driven e g subset of aquifer type or temperature ranges fig 1 the subsets will be analysed independently at the end the independent analysis of multiple variable subsets will provide information about discarded variables that are not correlated to hef in either step 2 or step 4 2 3 step 2 variables selection in step 2 the variable subsets are analysed independently using principal component analysis pca to explore patterns in data variability among rivers and then complemented by cluster analysis to identify combinations of variables possibly indicating hyporheic responses in a given river area first a pca is performed to reduce the dimensionality of the input space jolliffe 2002 by selecting only the principal components associated with the largest eigenvalues new vectors are obtained in the transformed space that have smaller dimensions these vectors are associated to the largest variance directions of the principal components and hence selected for the cluster analysis variables selection fig 2 discarded variables can still be included and analysed in other variable subsets or scale if the presence of those variables is known to be important for hef at this stage the reduced dimensional space is optimized with respect to the information variance contained in the data thus facilitating the application of cluster analysis to the pca output ding and he 2004 our method uses the distance based x means algorithm pelleg et al 2000 a variant of the most common k means macqueen 1967 the x means algorithm was chosen after testing the dbscan density based clustering algorithm ester et al 1996 which did not produce meaningful grouping of the case studies i e in most of the cases vectors were all classified as outliers contrarily to k means xmeans requires indicating a minimum and a maximum number of clusters kmin and kmax the algorithm applies kmeans to the data for all the possible k values in the indicated range kmeans finds the best assignment of the vectors to the k clusters and produces a score for this assignment based on the average squared distance of the points to their clusters centroids distortion measure xmeans reports the output of the kmeans execution that produced the best score the associated k is the best number of clusters xmeans is also more efficient with respect to kmeans because it uses kd trees bentley 1975 and blacklisting as support to the processing the x means algorithm pelleg et al 2000 is applied to the pca transformed vectors generating optimal grouping clusters of vectors according to their distances clustering the dimensionally reduced pca transformed vectors helps to find the best grouping in this space since the vectors belonging to the same cluster are close in the pca transformed space ding and he 2004 each cluster produced by xmeans is characterized by a centroid which is a representative vector of the cluster in our method the centroid is interpreted as a summary of the characteristics of the cluster in the pca transformed space re projecting the clusters centroids to the original space allows obtaining the coordinates of the centroids expressed in terms of the original variables re projection is mathematically possible although the pca transformed space has reduced dimensionality with respect to the original space however during this step some information is lost hence our method analyses the distribution of the variables onto the re projected centroids specifically we calculate the distances between the variable value and the coordinates of the re projected centroids for each variable the number of times a centroid coordinate is closest to a real data value is also recorded a tolerance threshold of 25 is applied before the final clustering on the features having the most uniform distributions over the centroids this step allows the selection of variables that are equally distributed over the centroids and accounts for the loss of information during the re projection the following example illustrates the criteria used to retain or discard the variables suppose 2 data clusters are identified for 8 rivers defined by vectors of elevation channel gradient and temperature if 4 elevation values are determined to be closest to cluster a and the other 4 to cluster b the elevation variable would be retained because the 25 tolerance threshold is exceeded i e 2 rivers assigned to a cluster if 2 channel gradient values were assigned to cluster a and 6 to cluster b the channel gradient variable would be discarded because the threshold 2 is not exceeded and if 5 temperature values were assigned to cluster a and 3 to cluster b temperature would be retained in the analysis in conclusion by construction of the pca algorithm if the variables are independent and carry high variance then the pca transformed space would correspond to the original space thus the centroids would take all of the variables into account resulting in equal distributions of the vectors coordinates on the centroids coordinates ding and he 2004 a variable that is not assigned to a cluster does not indicate a missing value for that cluster but it has been discarded during the clustering analysis 2 4 step 3 hyporheic classifiers the unique combinations of variables that are generated by the cluster analysis step 2 and their centroids are used to assess suitable and unsuitable areas for hef restoration for a river area using human expertise the expert provides a semantic description to each cluster in each subset using the centroid of the cluster and then assigns a hyporheic classifier 1 suitable or 0 unsuitable which indicates if the environmental conditions depicted by the clusters lead i e 1 or not i e 0 to hef the use of expert knowledge is required because empirical data on hef is not available for all of these unique combinations the expert bases this assignment on the variable types the distribution of the variables in each cluster and on the knowledge of the hydrological hydrogeologic topographic anthropogenic and ecological factors that yield hef following the relationships summarized in table a1 in the supplementary material at the end of the step 3 the initial set of variables has been factored into clusters semantically described and labelled examples tables a6 a7 a8 in the supplementary material the next section explains how these clusters are combined which corrects errors in the cluster label assignment and cluster analysis 2 5 step 4 classifier merger classifiers for each cluster and subset are merged together using a mathematical combination function the criterion used for the mathematical combination function is to indicate that areas of hef are suitable only if over half of the hyporheic classifiers indicate that it is suitable the mathematical combination function allows us to account for errors in the hyporheic classifiers due to mis labelling of the clusters the combination function is the normalized sum of all the sub classification for each case study c s r i 1 n c s i r n c r 1 c s r 50 0 o t h e r w i s e where r is the complete set of variables associated to a river area s i is the i th of n variable subset c s i r is the i th binary hyporheic classification over the s i variable subset c s r is the normalized sum of all the sub classifications for the river area r and c r is the final classification function if c s r is higher than 50 the river area r is classified as suitable otherwise the classifier assesses unsuitable this threshold was set after heuristic evaluation of a small 20 subset of our data 2 6 step 5 large scale merging to increase the accuracy of predictions as the spatial scale becomes finer the last step of the method is to combine the binary classifiers from different scales using a downscaling approach the rationale behind the combination function is the following if the system predicts that hef areas are suitable in a river at a large spatial scale then it is more likely to present suitable areas at smaller spatial scales nestled within the larger area for example a positive binary 1 classification at catchment scale suggests that suitable environmental conditions exist for hef in the catchment area at this scale of analysis the accuracy of the classification is generally higher because it is not required to precisely identify the specific location of hyporheic exchange hence a smaller scale classifier can use the information from a larger scale classifier because it represents the presence of factors that drive hef our method embeds this approach using a bonus function 20 weighting in the equation that combines the output of a classifier with the output of the next largest scale classifier the classification is recalculated for finer scales as follows c l a r g e r c s r 20 c l a r g e s c a l e r c r 1 c l a r g e r 50 0 o t h e r w i s e where c s r is the normalized sum of all the sub classifications for river area r and c l a r g e s c a l e r is the dichotomic score of the first larger scale also in this case the threshold 50 has been set after heuristic analysis on a small 20 subset of our data 3 results this section reports the results of the application of the multiscale statistical method to the nine test catchments the cluster results were compared to expert opinion section 3 1 and discussed at each spatial scale section 3 2 3 1 validation and reliability of the classification results the x means algorithm identified three optimal clusters in all the three spatial scales considered in the study to evaluate whether the developed multiscale statistical approach could identify suitable and unsuitable areas for hyporheic exchange to occur the reliability of the identified clusters was evaluated by examining the representativeness of the variables among the clusters against human expertise by the authors in the assessment the lead author manually assigned one of the interpretations of the xmeans clusters i e 1 or 0 to each river catchment i e 8 catchments and 118 variables for the uk case of study 86 variables for the polish case study segment 51 segments and 48 variables for the uk case of study 10 segments and 35 variables for the polish case study and reach 135 reaches and 59 variables for the uk case of study 11 reaches and 74 variables for the polish case study at this stage the expert evaluation differs from the expert information within the model step 4 because it is performed on the original environmental data section 2 1 and not on the clusters a confusion matrix was used to assess the agreement between the expert assignment binary 1 and 0 and x means clusters as the percentage of matching assignments absolute percentage of agreement furthermore the cohen s kappa cohen 1960 was calculated to estimate the agreement between the expert and the model compared to purely random assignments the x means results agreed generally with expert opinion indicating reliable semantic interpretations of the categories identified in the clusters variations at catchment scale the absolute percentage of agreement is 88 and 75 at segment 75 and 78 and at reach 74 and 82 for the uk and polish case studies respectively table 4 table 5 as the binary classifiers for each scale in step 5 take account of the information from the next largest scale i e catchment classifiers influencing segment classifiers to represent the scale dependence in hef the model performance is expected to increase within decreasing scale in the uk case of study the catchment scale effectively added information to the segment scale step 5 because the agreement increases of 1 percentage point table 6 however in the biebrza application no performance increase was detected table a4 in supplementary material 3 2 prediction of hef at different spatial scales hef suitable and unsuitable areas were predicted at all three spatial scales for the examined rivers fig 3 table 3 at catchment scale unsuitable conditions for hef are predicted for the rivers dove exe tone and wye fig 4 table 7 these rivers are predominantly characterized by confined or semiconfined aquifers poorly sorted superficial deposits from coarse sand to silt and clay 50 cover over the catchment in contrast for the rivers frome piddle tern and rother the semi automatic classification method predicts suitable areas for hef to occur the clusters for these rivers depict predominantly complex aquifers with flows though fractures and discontinuities terrigenous deposits with sorted sand and gravel 30 45 silt and clay deposits less than 20 of cover on the catchment at segment scale hef is found to be characterized by suitable areas for all the identified segments in the rivers piddle tern wye and the biebrza river fig 4 tables 3 and 7 conversely hef is predicted to be low for all the segments in the rivers dove rother and tone the rivers exe and frome are predicted to have a mixture of suitable and unsuitable hef areas in different segments where suitable hef condition is predicted the clusters are mainly characterized by sandstone geology a low fraction fine sediments between 10 and 30 cover over the segments large fraction of sorted gravel and sand deposits between 20 and 50 cover over the segments channel sinuosity of 1 2 and low channel gradient 0 002 in segments with unsuitable conditions for hef the clusters describe mudstone and sandstone geology low channel gradients high percentage of clay and fines 55 cover and high percentage of arable and grassland 70 cover within 150 m of the river channel for the biebrza river the segments which are predicted to have suitable hef conditions are characterized by sinuosity 1 3 high percentage of gravel and sand deposits 40 high percentage of productive aquifer and low percentage of pasture lands 10 within 150 m of the main river channel finally at reach scale the multiscale statistical method predicted suitable hef areas for 3 rivers of the 9 evaluated the frome piddle and biebrza fig 4 tables 3 and 7 generally the clusters indicating suitable conditions for hef exhibit a low percentage of in channel vegetation 2 10 of the reach gravel substrates 10 very low percentage of silt and clay deposits 1 presence of pools and riffles 5 10 and a low percentage of poached or overgrazed river banks 5 cluster indicating unsuitable hef areas are mainly described by poached river banks presence of in channel emergent vegetation and reeds low percentage of gravel substrates low number of pools and riffles and low mean flow velocity in the biebrza river clusters indicating suitability relate to superficial geology dominated by peat 80 cover on the entire reach and mud 10 while those indicating unsuitability are dominated by mud 60 and peat 10 deposits low percentage of sand and gravels and high percentage of unsorted till deposit 50 and pasture lands 4 discussion and conclusion the multiscale statistical method was developed and applied to nine rivers across europe to identify suitable and unsuitable reaches segments and catchments for hef focused restoration the results of the classification showed good to moderate agreement cohen s kappa with expert opinion indicating reliable categories and semantic interpretations of the clusters reasonable agreement is also observed with in situ empirical data from previous studies given the unavoidable differences in scale between these detailed local research studies and our broad scale approach in this section we discuss the results of the classification against field observations of actual hef the major predictors of suitable and unsuitable areas section 4 1 and finally the domain of application of the method section 4 2 4 1 linking processes to factors at each spatial scale catchment segment and reach cluster results show groups of predictors that influence the determination of suitable and unsuitable areas for hef restoration hydrological factors i e groundwater level discharge influence hef by changing surface water flow regimes and distributions of hydraulic head table a1 supplementary material hydrogeological factors affect water flowing through the river bed by sediment grain size sediment heterogeneity and depth therefore promoting spatially diverse hyporheic exchange packman and salehin 2003 table a1 supplementary material topographic factors such as catchment gradient individual bedforms and bedforms sequences and valley confinement drive hydrodynamic and hydrostatic forces that affect the variability of hef from cm to km scale table a1 supplementary material anthropogenic factors such as in channel structures i e weirs dams land management and land use impact hef by modifying river stage fluctuations changing sediment delivery and channel complexity and by altering vertical hydraulic gradients table a1 supplementary material also vegetation has long been known to exert a strong control on land surface hydrology by moderating streamflow and groundwater recharge table a1 supplementary material as an ecological factor vegetation feedbacks on the temporal variability of hef and likely increase the spatial heterogeneity of this ecological hydrological relationship this section presents the different factors affecting suitable and unsuitable hef restoration areas and compares the hef predictions at reach scale to in situ empirical data from previous studies high percentages of poached banks emergent in channel vegetation improved grassland and low geomorphological complexity and low number of pool and riffle sequences were associated with unsuitable reaches in the frome 1 reach and in the piddle catchments 6 reaches table 7 dunscombe 2011 observed weak vertical hydraulic gradients vhgs at the head and tail of riffles in both the rivers frome and piddle indicating little to no hef at this scale this is a finer scale than the prediction of our model which overall classifies that reach as unsuitable fig 4e these neighboring catchments are found in the south of england and are underlain by chalk bedrock chalk has a high secondary porosity and groundwater flows easily through fractures and fissures in the bedrock to these gravel bed rivers waters and banks 1997 the combination of a permeable chalk geology and coarse sediment would be expected to strongly support hef morrice et al 1997 hiscock 2007 however there are several reasons for unsuitable conditions in these rivers i the pronounced groundwater flows create strongly gaining and losing conditions in reaches which drive contraction gaining or expansion losing of hz and shortening of hef paths wondzell and gooseff 2013 fox et al 2014 malzone et al 2016a 2016b ii the rivers have few instream geomorphic features that would generate advective pore water flow into through and out of the river bed elliott and brooks 1997 tonina and buffington 2009 and iii high fine sediment loads have led to clogging of the coarse gravel bed boulton and hancock 2006 pretty et al 2006 several studies have shown that chalk rivers in england have elevated fine sediment loads derived principally from cultivated agricultural land walling and amos 1999 collins and walling 2007 grabowski and gurnell 2016 and grazing pressure trimble and mendel 1995 bilotta and brazier 2008 bilotta et al 2010 also in channel vegetation appears be an important factor at this scale of analysis while vegetation patches have been shown to narrow the active channel increasing water velocities and mobilizing the gravel bed cotton et al 2006 the localised reduced velocities within vegetation patches promote deposition of sediment and organic matter decreasing bed permeability and reducing or eliminating hef salehin et al 2004 ensign and doyle 2005 corenblit et al 2007 for the wye river the results of the statistical method agreed with dunscombe 2011 observations weak vhgs while for the rivers tone dove the predictions did not align with field data our method predicts unsuitable areas for hef at the reach scale along the tone and the dove while dunscombe 2011 observed strong patterns of up and downwelling flows at the head and tail of riffles on both rivers for the river tern all reaches were identified as unsuitable areas by our method however empirical hef data at a pool riffle pool sequence showed temporal flow patterns occurring around this geomorphic feature at the sub reach scale krause et al 2011 hannah et al 2009 suitable areas for hef were predicted consistently across all spatial scales for the rivers dove and the tone but not for the tern wye rother piddle frome exe and biebrza at catchment scale the clusters for the dove and tone are characterized by well distributed variables sandstone is mixed with mudstone and siltstone bedrock geology and clay and silt superficial deposits represent more than the 50 of the catchment similarly the hydrogeology is dominated by unconfined but low producing aquifers while the sandstone bedrock would normally support surface subsurface exchange hiscock 2007 the low conductivity superficial deposits characterizing the clusters more than 50 of the catchment area would likely limit or restrict vertical hyporheic flow indeed the role of local sediment deposits in preventing or limiting groundwater surface water interactions has been recognised for unconfined alluvial channels gurnell et al 2014 at segment scale clusters characterized by low slopes high percentage of in channel fine sediments and extensive arable lands around the river channel are depicted in the clusters possibly suggesting an impact of sediment delivery from the surrounding lands and simplification of landscape complexity gooseff et al 2007 boano et al 2014 at reach scale suitable conditions for hef were predicted in some reaches of the biebrza frome and piddle fig 4 for the biebrza river the reaches identified as suitable fig 4a in our classification corresponded in spatial extent to one reach of our analysis which were previously observed to have upwelling and sections of recharge anibas et al 2012 these reaches were characterized mainly by a geology of peat and peat mixed with mud our clusters identified peat as an important variable controlling hef at the reach scale this reflects the underlying process controls as the physical structure and stratigraphy of peat has pronounced influence on the dynamics of water retention storage and solute transport rezanezhad et al 2016 anibas et al 2012 described two main types of peat soils that showed different behaviors in driving hef flows at the sediment water interface soil i has a loose structure covered in reed vegetation and characterized by high flow fluxes while soil ii is more compact and has lower flow fluxes in our data for the biebrza peat characteristics are heterogeneous across reaches varying from loose similar to soil type i to more compact and mud dominated similar to soil type ii therefore the overall assessment and spatial distribution of hef predictions at reach scale in the biebrza catchment are supported by the findings of anibas et al 2012 a possible reason of the difference in outputs between the predicted hef conditions by the multiscale approach and in situ observations is the different spatial and temporal resolutions in situ measurements commonly focuses on an individual bedform or feature or sequences of them i e meter to 10 s m scale are influenced by temporal variations that are not considered in the proposed approach moreover the resolution of geomorphological data used in these case studies is coarser than the detailed sub reach scale observations of hef river habitat survey rhs data was used as point estimates of in channel conditions for uk rivers while rhs data is ideal for this type of analyses in many ways e g national coverage reach survey scale it is a visual appraisal of river habitats and geomorphic features and does not involve topographical or hydrogeological measurements raven et al 1996 rhs assesses river habitat within a 500 m long reach using 10 spot checks and a sweep up survey to count key features or river channel while it does record many features relevant to hyporheic flow e g vegetation type artificial structures channel substrate and emergent bedforms it does not quantify or map these features at a sub reach scale which is the scale used in many empirical studies of hyporheic flow spatial resolution explains differences by scale where suitable areas for hef to occur are predicted only at spatial scales larger than the reach scale i e river tern and river rother finally results in table 6 depict a scale dependence effect between catchment and segment scales the small increment 1 percentage point in the confusion matrix suggests that upper hierarchical levels inform on general conditions at low resolution and exert constraints on the lower level which informs at higher resolution and provides mechanistic explanation for higher levels 4 2 application to river restoration planning this study proposes a multiscale statistical method to identify where hef potentially occurs at catchment segment and reach scale i e an area that is suitable for hef based restoration the approach and results presented in this study use readily available environmental datasets enabling the method to be transferred to other catchments restoration practitioners are increasingly considering the hz in their management plans because of the crucial role it plays in river biogeochemical processing and the transferring of solutes and oxygen between surface waters groundwater and the hz findlay 1995 nogaro et al 2010 mendoza lera and datry 2017 thus there is a strong need to provide river managers and restoration practitioners with a tool that can be applied to any catchment and which is flexible enough to work with the data sources available in different regions and countries as highlighted by other framework approaches i e reform gurnell et al 2014 structuring the analysis around multiple scales improves spatial and temporal understanding of the variability of environmental factors in river systems and how reaches have been impacted by catchment scale changes therefore our approach supports broader restoration planning that includes catchment scale solutions merill and tonjes 2014 wortley et al 2013 hester and gooseff 2011 to assist river restoration practitioners we propose that this multi scale statistical process be run as a preliminary assessment step in restoration planning to identify and possibly prioritize restoration actions i e reach locations across a catchment restoration managers can benefit from the classification analysis by evaluating how well hydrological hydrogeological topographical anthropogenic and ecological factors describe hyporheic drivers fig 5 first by interrogating the clusters generated by step 2 managers can be informed about i environmental and hyporheic drivers on the targeted areas ii identify areas with the same hydrological hydrogeological topographical anthropogenic and ecological context and iii their spatially uniqueness second by examining the final confusion matrices step 4 which embed a summary of knowledge across the domains of hydrology geology and hyporheic theories and their related environmental data and provide insights into the spatial variability of hef in a catchment finally by using the results of the multi scale assessment step 5 river managers can define a posteriori what processes management actions are important for each reaches and then feedback to management actions considering the above information river managers can choose between passive and active approaches for example some of the factors depicted in the clusters will be intrinsic i e bedrock geology and cannot be changed by management measures while others will be dynamic i e land use vegetation channel geomorphology and therefore might become a target for catchment or river management if suitable hef conditions are predicted a passive approach will likely be preferred and include measures that do not directly address hyporheic conditions but that take advantage of hef to preserve and maintain for example habitat diversity or soil erosion reduction the river restoration centre 2013 the passive approach would include in situ evaluation to verify that the method predictions are representative of local conditions conversely if unsuitable hef conditions are predicted an active approach can be adopted and local or restoration measures applied accordingly to the factors involved for example the case study on the river rother showed suitable conditions for hef to occur at catchment scale i e complex aquifer gravel to sand deposits while unsuitable conditions were predicted in segments and reaches i e low channel gradient and sinuosity clay and lenses an active restoration approach would be appropriate to implement local restoration measures for enhancing local hyporheic flows and ecological functioning in this river fig 5 in our opinion the identified factors for hef have intuitive general validity but we expect that in other applications the method would be tailored to site specific characteristics and applied to other factors at reach and sub reach scales the classification is generally limited by the resolution and quality of the available data this is a general issue when using environmental surrogates of hydrological processes especially due to the coarse resolution of the data olden et al 2012 we qualitatively compared the prediction of the method on available empirical hyporheic evidence that was i spatially and temporally limited to local scales ii collected using multiple methods and iii focused on specific geomorphic features such as bedforms that likely trigger local advective hef even when catchment conditions limit larger scale flows in the future we expect this evidence based problem to be overcome by technology and more complete and uniform metadata associated with hyporheic studies finally existing scientific literature suggests that knowing how and what to prioritize in restoration actions for aquatic ecosystems are fundamental to effective restoration planning wohl et al 2005 there is an increasing emphasis on addressing hyporheic zones into restoration to allow more comprehensive hydro ecological understanding of aquatic ecosystems our model can support restoration as a first order assessment to target hz and thus provide the greatest benefits to restoration plans author contributions chiara magliozzi conceived designed and analysed the dataset producing and assessing the final clusters gianpaolo coro supervised the whole design and contributed to the development of the methodology from the statistical perspective aaron packman and stefan krause provided hydrological perspectives robert grabowski edited and assisted the revision of the mansucript and provided hydromorphological and ecological perspectives all authors read and approved the manuscript conflicts of interest the authors declare no conflicts of interest acknowledgements this work was supported by the marie sklodowska curie action horizon2020 within the project hypotrain grant agreement number 641939 g coro was also supported by the bluebridge project grant agreement number 675680 a i packman was also supported by the u s national science foundation grant agreement number ear 1344280 we thank the networked multimedia information systems laboratory ne mis research laboratory of isti cnr italy for providing full support for the development of this research the biebrza national park for providing the needed data for the development of the river biebrza case of study data sources preservation of wetland habitats in the upper biebrza valley life11 nat pl 422 co funded by life the financial instrument of the european commission the national fund for environmental protection and water management and biebrza national park restoration of hydrological system in middle basin of the biebrza valley phase i co funded by life project thanks to dr christian anibas for his availability in sharing information on the river biebrza the environment agency and dr marc naura who provided the river habitat survey data for the u k catchments we thank the british geological survey the centre for ecology hydrology the uk met office the european soil data centre the european environment agency and the polish geological institute as data providers we also thank prof ian holman and the three anonymous reviewers for their helpful comments on the manuscript appendix a supplementary data the following is the supplementary data to this article supplementarymaterial supplementarymaterial appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 006 
26296,as the amount of environmental data expands exponentially worldwide researchers are challenged to efficiently analyze data maintained in multiple data centers because distributed data access server side analysis multinode collaboration and extensible analytic functions are still research gaps in this field this paper introduces a collaborative analysis framework for gridded environmental data i e cafe multiple cafe nodes can collaborate to perform complex data analysis analytic functions are performed near where data are stored a web based user interface allows researchers to search for data of interest submit analytic tasks check the status of tasks visualize the analysis results and download the resulting data files cafe facilitates overall research efficiency by dramatically lowering the amount of data that must be transmitted from data centers to researchers for analysis the results of this study may lead to the further development of collaborative computing paradigm for environmental data analysis keywords collaborative analysis web based system distributed data environmental data software availability product title cafe collaborative analysis framework for environmental data developers ke yan chang xiao hao xu sha li yuqi bai contact address department of earth system science meng minwei sci tech building s917 tsinghua university beijing china 100084 contact email yuqibai tsinghua edu cn available since 2015 hardware requirement a central storage server as central node computation server s as worker node s web server s for web based user interface these three parts are all required for a complete cafe network and can be integrated in the same machine or separate machines software requirement linux environment mysql tomcat 7 netcdf operators nco climate data operators cdo ncar command language ncl java development kit jdk version 1 7 for cafe node package apache 2 and php 5 for cafe portal package programming language java cafe node php cafe portal availability https github com thu earthinformationsciencelab cafe node https github com thu earthinformationsciencelab cafe portal cost free license gnu general public license 1 introduction environmental science has experienced considerable progress in recent years resulting in part from significant advances in high performance computing data storage and networking technologies the paradigm for environmental research has now moved into a new phase focused on data intensive analysis and integration tolle et al 2011 the amount of environmental data has grown rapidly because of the increasing means of observation such as remote sensors and in situ probes and through the use of advanced numerical models many of these data are stored in gridded file formats many specialty fields in environmental research are transitioning from being data and simulation poor to being data and simulation rich nrc 2001 for instance data produced by a community studying climate change reached the petabyte level in 2008 williams et al 2008 and data will grow to hundreds of petabytes by 2020 overpeck et al 2011 scientists now utilize climate models to study climate change in long time series johns et al 2003 delworth et al 2012 they use multispectral remote sensing images to measure decade long air quality van donkelaar et al 2006 lee et al 2011 and they leverage synthetic aperture radar images to monitor crops blaes et al 2005 and floods matgen et al 2011 all of these studies involve processing of large quantities of gridded environmental data because environmental simulation or observation datasets are often independently produced and maintained by a number of providers scientists usually have to download large volumes of data from multiple data centers it is time consuming for researchers to search for and download data and then to manage and preprocess the data on equipment local to them this common paradigm of everything locally owned and operated bai and di 2012 faces the following challenges 1 datasets usually are managed and stored at multiple sites to discover and analyze the data transparently researchers require distributed data access with a common interface 2 client side analysis is limited by computational and storage capabilities server side analysis is needed to assist users in reducing the volume of data transmission 3 analysis may involve the extraction of data from multiple datasets maintained in different data centers it is essential that the analysis process be executed near data storage facilities to reduce communication time and bandwidth costs and to make all the computing nodes work collaboratively 4 researchers who require access to distributed data and related analytic processes and who need to share their methods may be accustomed to using a variety of tools therefore a system is needed that can be expanded easily to support analytic scripts written in different programming languages such as ncl r and python one promising solution is to enable a new paradigm of everything shared over the web bai and di 2012 because the internet provides powerful communication mechanism for data processing visualization simulation prediction and sharing vitolo et al 2015 online processing and analysis have been playing an important role in enabling data intensive science as proved by nasa s giovanni berrick et al 2009 and google earth engine gorelick 2013 system since researchers often need to analyze environmental data that usually reside on multiple data centers a multinode collaborative solution is further needed to allow complex analysis to be automatically fulfilled near data storage facilities in response to this need we designed a collaborative analysis framework for gridded environmental data cafe a java based distributed data management and analysis framework that allows environmental researchers to work efficiently with distributed datasets freeing them from downloading archiving and preprocessing data locally cafe enables a collaborative environment in which multiple cafe nodes can be integrated to perform complex large scale data analysis it can provide data discovery and task submission functions and deliver analysis results as ready to use graphs and as data files that can be downloaded for further processing to end users via a web based interface another characteristic of cafe is that it supports the deployment of new analytic functions data files in cafe are maintained in a distributed manner they follow the same conventions of file directory file naming and parameter naming the descriptive information about these data files is automatically gathered on a central node multiple nodes can collaborate with each other to perform complex data analysis tasks in a way that data is processed on the node where it resides for this reason task decomposition subtask assignment and result combination mechanisms are further introduced in our system to illustrate the feasibility of multi node collaboration in cafe model output data produced during the phase 5 of coupled model intercomparison project cmip5 and satellite based observational data are used as exemplars the purpose of this research is to leverage the well defined data naming syntax and variable naming specification used in the cmip5 community to enable a web based collaborative data analysis environment in which multiple data nodes can jointly present a logically complete view of distributed data and can automatically perform distributed data analysis without having to download the original datasets the remainder of this paper is organized as follows section 2 provides the literature reviews of related work to reveal the gaps between the available functions provided by a variety of environmental data analysis software packages and the ones highly expected by researchers section 3 describes the system architecture and each component of cafe section 4 presents the implementation details of the prototype system the strengths and current limitations of cafe are discussed in section 5 which leads to the conclusions in section 6 2 related work the typical workflow for analyzing environmental data includes data acquisition preprocessing computing and visualization a complete data analysis framework needs to execute this whole workflow automatically these data must be organized and published by data centers in a parsable way several software packages are available for researchers to preprocess compute and visualize these data each of them has its strengths and limitations thematic real time environmental distributed data services thredds domenico et al 2006 provides remote access to the data files via open source project for a network data access protocol opendap cornillon et al 2003 however improving remote data access cannot solve the problems of transferring large volumes of data some simple tools are available to process gridded environmental data such as netcdf operators nco zender 2008 and climate data operators cdo https code zmaw de projects cdo these tools support batch processing of a large number of data files but they do not provide a uniform user interface many powerful toolkits are available to process and visualize environmental data such as ncar command language ncl http www ncl ucar edu the grid analysis and display system grads berman et al 2001 and ferret http www ferret noaa gov ferret these tools can access remote data via opendap but transferring large volume of data files is still needed when using them to perform intercomparison of distributed datasets in addition many general purpose programming languages such as r https www r project org python https www python org interactive data language idl http www ittvis com idl and matlab http www math works com can support environmental data analysis and visualization the major limitation with these systems is that they cannot support straightforward online processing of environmental data so that researchers still need to write code to perform similar analyses to make environmental science research more efficient several systems have emerged to support access to remote data and to provide web based services each offers an approach to performing the entire workflow for data analysis these systems can be classified based on the major way for researchers to interact with script graphical user interface gui and web based interface the script workflow analysis for multi processing swamp wang et al 2007 is a script based analysis system for gridded data built on nco and opendap it melds computation with data hosting which shifts computation to data sources and eliminates data movement inefficiencies it does not however support multinode collaboration and its analytic functions are not extensible in addition swamp does not provide a gui which further limits its functionality and usability a number of gui based environmental data analysis systems support remote data access and analysis for example the integrated data viewer idv murray et al 2003 is a java based software framework for analyzing and visualizing geoscience data the ultra scale visualization climate data analysis tools uv cdat santos et al 2013 williams 2014 is a provenance based system that supports interactive analytic functions and data intercomparison for climate model output these two desktop applications also support remote data access via opendap hydrodesktop is a web service based tool used for discovering downloading managing visualizing and analyzing hydrologic data ames et al 2012 users can retrieve data from the remote catalog hydrodesktop also provides an extensible interface for creating custom plug ins although these systems provide strong analysis functions poor cross platform ability and the inevitable need for large data transfers have limited their convenience recent years have seen rapid development of web based visualization and analysis systems the national climate change viewer nccv alder and hostetler 2015 is a web based system that visualizes future climate projections from global climate models across the united states this single node system has centralized computation giovanni berrick et al 2009 is a web based system that can easily be used to explore and analyze multisource environmental data although it has demonstrated how its analytic functions can be extended and described berrick et al 2009 giovanni does not provides solution to enable researchers to register new analytic functions to be used in the system nasa s earth exchange nex nemani 2015 provides strong computational resources for end users and user level collaboration as a centralized storage system it is like a virtual machine that offers shared tools and data for users instead of ready to use analysis results climate4impact joussaume 2013 is a web based system with a user friendly interface that can access and analyze remote data based on the data infrastructure established by the earth system grid federation esgf williams et al 2009 2011 cinquini et al 2014 this system supports submission of tasks data intercomparisons and batch analysis through web processing service wps it utilizes centralized computation to handle remote data files therefore it still relies on transmitting large amounts of data the live access server las hankin et al 2002 cinquini et al 2014 is an open source web based system that has been included in esgf as a tool for online analysis and visualization las allows the processing of data at the place where they are stored based on ferret and opendap it has realized multinode data regridding differencing overlaying and intercomparison nonetheless las does not allow researchers to provide ready to use analytics functions into the system although different nodes can work together in las a limitation that only a maximum of four datasets can be analyzed at one time exists on the basis of our assessment of these systems table 1 provides an overview of the current state of support for the four desired environmental data analysis functions outlined in section 1 this table clearly shows that none of these tools or systems fully supports all four desired workflow functions in particular these systems provide little support for multinode collaboration although las could make different nodes work together information exchange does not occur between nodes which severely limits multinode collaboration unlike these tools cafe can merge results from different nodes and provide service level multinode collaboration to end users making all data and analysis transparent 3 system design of cafe to enable efficient data queries and large scale collaborative analysis of gridded environmental data we designed a unique framework for cafe that has four key parts data index module task managing module data analysis module and web based user interface ui a cafe network should have at least one central server and more than one worker nodes a server configured with the cafe node package in the worker mode including data index module task managing module and data analysis module is a cafe node cafe nodes are required to be configured in each data center near the data storage facilities and expected to be installed and maintained by each data center data centers are expected to provide powerful computation and visualization hardware for cafe s server side analysis all cafe nodes can collaborate with each other to automatically fulfill server side analysis of datasets that could be distributed in multiple data centers in a way that data analysis is performed at the place where data resides in addition a central server is needed for the management of descriptive information about each node all the data files managed on all the nodes and all the available analytic functions deployed on these nodes the logical system design of cafe is depicted in fig 1 the data index module is designed to update and manage the data index and descriptive information for all the data locally maintained on the node it also provides data query capability to the web based ui the task managing module is designed for task submission task dispatching and task status results query services on each cafe node the data analysis module is responsible for executing the user selected analytic function the web based ui provides scientific data access functions for end users in particular it can search data define analytic tasks check the status of tasks and present analysis results the data analysis workflow in cafe consists of the following steps 1 the user selects datasets of interest and specific analytic functions 2 the user submits the analysis task through the web based ui 3 the task managing module parses the task and decomposes it into several subtasks based on which nodes are located for the selected data 4 all subtasks are dispatched to the nodes where the datasets are stored 5 all subtasks are fulfilled on the related node side by invoking the corresponding analytic functions on those nodes 6 the task managing module can then present the status of each subtask to the web based ui 7 all results are collected as uniform resource locator url or javascript object notation json format text data results are visualized through the web based ui 8 the user can check the analysis results shown as images or download the original processed data files for further processing the sequence diagram in fig 2 shows the workflow for the complete cafe process 3 1 data index module descriptive information about data stored on a node are maintained by the data index module located on the same node to enhance collaborative analysis capabilities across the whole network cafe maintains global data information e g data file name data attributes and storage path for details see section b in the supplementary information si on the central server to enable fast data query and task decomposition the data index module supports the web based ui to present the available data and functions to end users in particular it contains the data scan index update query page initialization and dataset query services as shown in fig 3 the data scan service can scan data directories iteratively updating the local data information on the node and concurrently updating the global data information whenever the data information is updated on one node the index update service on that node will be executed to update the data information maintained on the central server accordingly the query page initialization service is used to provide available search criteria for users to check through the web based ui it relies on the global data information database to know the selectable attributes of all datasets the dataset query service is responsible for searching the global data information database and getting the information about matched datasets back to the web based ui through application programming interfaces apis details about apis are described in the si 3 2 task managing module the task managing module is the key to fulfill collaborative analysis in cafe as shown in fig 4 this module contains five parts the available functions retriever task scheduler subtask receiver task status checking service and task result retrieval service the available functions retriever can query the global analytic functions database and return the information of available analytic functions to the web based ui according to the attributes of the selected datasets a selectable analytic function can appear in the function list only if it exists on all the corresponding nodes and meets the filtering criteria defined in the function description file the task scheduler can submit the task and decompose it into subtasks when a task is submitted to a node the task information will be maintained in the database on that node this module queries the global data information database on the central server to determine the locations of datasets that are to be analyzed and then decomposes the task into several subtasks the principle is that one subtask corresponds to one dataset according to the dataset s location please note that the situation that one dataset is separately stored in multiple data centers is not considered because data centers typically host complete datasets the relationships between tasks and subtasks also are stored in this node all the cafe nodes are peer to peer they all can assign or receive subtasks subtasks are dispatched to the remote nodes using a web api exposed by the subtask receiver the subtask receiver on a node can receive the subtasks assigned by other nodes the module will store the information of received subtasks into the local subtask status table on that node all the subtasks assigned to a node form a queue they will be performed locally by the data analysis module and the results of each subtask will be stored on that node the resulting information could be urls if the result files are images or gridded files or json texts if the result files are texts and it will be stored in the database on that node when a user requests the status of previously submitted tasks the task status checking service will collect information about all of the subtasks by sending queries to each node and it will send the information back to the web based ui when a user requests analysis results the task result retrieval service will gather each subtask result from the corresponding nodes and will send the results back to the web based ui 3 3 data analysis module the data analysis module consists of an analysis launcher a command executor and several analytic scripts as depicted in fig 5 a complete analytic function includes an analytic script and a description file written by extensible markup language xml examples of analytic scripts include ncl python r and linux bash shell scripts java language is used to wrap these analytic scripts as executable functions in cafe based on their xml descriptions because runtime class in java is used to invoke these scripts by command line these scripts need to expose an input output i o interface to be accessed in the command line taking ncl script as an example the analytic function contributor i e a researcher can define the parameters in the script that can be input externally from the command line the script is required to be tested in the command line environment by the contributor so that it can be invoked successfully in java the xml description file which is expected to be provided associated with the script defines details of properties parameters and required preprocessing of the analytic function the workflow of the data analysis module consists of the following steps 1 the analysis launcher constantly monitors the queue of subtasks to be processed locally it picks up subtasks on a first come first served basis 2 the analysis launcher queries the local analytic functions table to obtain the information of the user selected analytic function 3 the analysis launcher parses the information of user selected analytic function and instantiates it as a java object 4 the command executor invokes the commands to preprocess and analyze the user selected dataset based on the parsed information at the same time the module updates the subtask status maintained in the local subtask status table 5 the command executor maintains the information of analysis results in the local subtask results table when the subtask is finished the database design is described in detail in the si the contributor can set a new analytic function to be invoked in one of two ways either to be used in a specified node only or to be deployed on all the cafe nodes in the latter case the function will be distributed to each node after a contributor submits a new analytic function it will be evaluated and examined by an evaluation group set up by the administrative personnel of the cafe network the corresponding nodes will receive an update request to configure and register the function once it is approved for release once a new analytic function is registered on a cafe node the information of this analytic function will be maintained both in this node and the central server 3 4 web based user interface the web based ui provides a straightforward way for researchers to query and analyze supported datasets through a web browser as shown in fig 6 the ui includes six parts initialization search task submission task list checking task detail checking and results retrieval initialization loads selectable attributes by api search provides the dataset search service the user submits his or her task through the task submission portion task list checking makes it easy for the user to check the task lists she or he has submitted task detail checking provides task details and status to the user the user can retrieve the results through the results retrieval part a relational database is used to store user and task information the task information table records the corresponding relationship between users and tasks a user who wants to use analytic functions in cafe must register an account through the user registration page and then log in once the query page is loaded the web based ui will send the request to the appropriate connected node after which the application will generate the selectable attributes for dataset filtering the user can filter the dataset by specifying institute model frequency and other parameters of the datasets after the user selects desired datasets from the list the web based ui sends requests to the connected node and obtains information about the available analytic functions the user can choose one of the available analytic functions then the web based ui utilizes the information of the function and generates a form for parameter input and validation the user can set the corresponding parameters and submit a task the web based ui sends requests to the node regularly to retrieve real time status of tasks by apis asynchronous refreshing technique is used to show the real time status of each task once the status of all the subtasks turns to be finished the web based ui retrieves all the urls or json format texts of the results through the api the images will be displayed directly on the result page through their urls while the json format texts are designed to be merged and visualized using highcharts highsoft 2015 the user can compare the results of different subtasks from the task detail page and download different kinds of results the webpage will display the graphs or charts of the results and it will provide links for downloading multiformat result files 4 prototype system to prove the feasibility of the design of cafe a use case that requires intercomparison of climate model output and satellite observation data was selected to develop a prototype system 4 1 data during cmip5 more than 20 climate modeling groups around the world contributed more than 60 climate models more than 3 5 petabytes of data are now stored across multiple nodes around the world cinquini et al 2014 for example the coupled climate model fgoals g2 was developed jointly by the state key laboratory of numerical modeling for atmospheric sciences and geophysical fluid dynamics in the chinese academy of sciences and department of earth system science in tsinghua one of the data centers for this model which hosts about 19 terabytes tb of data is located in tsinghua university li et al 2013 all of the cmip5 model data are stored in netcdf format these data follow the climate and forecast cf metadata conventions eaton et al 2011 data files are also organized to follow the rules defined in the cmip5 model output requirements taylor and doutriaux 2010 to test the effectiveness and performance of cafe four test cafe nodes that together host more than 1 tb of data were set up in china these data included model output produced by about 60 cmip5 models available at https esgf node llnl gov projects cmip5 as well as observation data from the advanced very high resolution radiometer avhrr available at https www ncdc noaa gov oisst data access and nimbus 7 available at http nsidc org data nsidc 0051 different nodes stored a few of the same datasets 4 2 implementation the data index module task managing module and data analysis module were integrated into the cafe node package it was developed as a java web application using a combination of the spring framework johnson et al 2005 spring model view controller mvc framework gupta et al 2010 and mybatis wen and jianhua 2012 framework the web based ui which was integrated into the cafe portal package was developed in php using a yii mvc framework winesett 2012 apache2 http httpd apache org and tomcat7 http tomcat apache org were utilized as the hosting environments and mysql was selected http www mysql com as the database engine nco cdo and ncl were incorporated in this prototype because they are good examples of existing simple data processing tools for gridded environmental data and powerful toolkits widely used in the climate research community nco is good for data subsetting and manipulating cdo is convenient for data regridding and averaging and ncl is a powerful command based tool used to analyze and visualize gridded data the web based ui could provide controls dynamically for users to input parameters based on the analytic function description for demonstration purposes following analytic functions were developed and deployed 1 empirical orthogonal function eof analysis eof is a decomposition method that can compute empirical orthogonal functions for user selected variables using the time series of the amplitudes associated with each eigenvalue monthly and seasonal eof analysis results were provided 2 long term mean ltm analysisltm was used to compute the average value of each cell according to user selected variable across a user specified long time period 3 trend analysistrend analysis was used to derive the slope coefficient of a linear regression model based on a user selected variable the result was applied to each cell across the user specified long time period 4 seasonal contoura seasonal contour was used to compute the average value of each cell according to a user selected variable across the user specified long time period for four seasons 5 time seriesthe time series function could compute the time series of the yearly or seasonal value and the corresponding linear regression function according to a user selected variable these five functions could render the analysis results as images or maps and plots based on the ncl script some example results based on ncl are shown in fig 7 4 3 extensibility of analytic functions cafe aims to enable researchers to contribute their own analytic scripts for public access in a simple way the cafe prototype enabled feasible extensibility for analytic functions the contributor needs only to provide his or her script and associated xml description file a new analytic function will be evaluated manually by the evaluation group currently the professionals in the development team and be examined in our test cafe nodes voluntary experts will also be recruited for evaluating the analytic functions the analytic function will be released for registration after evaluation of its usability and functionality it then can be added simply to the cafe system after configuration and registration by the corresponding cafe nodes a new script needs an i o interface for the command line so that java can invoke the script and pass the parameters to the script these parameters also must be indicated in its xml description file a standardized xml file is a prerequisite for a new analytic function fig 8 shows an example format of the analytic function description in the cafe prototype the contributor first defines the name of the analytic function the detailed descriptions of the function are given as well the input can be a single file or multiple files the contributor can set whether or not the function can be distributed to all cafe nodes the properties section is used to define what kind of datasets can be used all the input parameters are set in the controls the inputfileparameters and outputfileparameters sections the contributor can define the parameters that must be input from the web based ui in the controls section parameters of input files and result files are indicated separately in the inputfileparameters and outputfileparameters sections some commonly used preprocessing approaches based on nco or cdo such as file combination regridding yearly or seasonal averaging and long term averaging were encapsulated as java methods in the prototype the contributor can set the preprocessing type in the preprocessing section and then the specified preprocessing type will be triggered the detailed specifications of the xml description file are provided in the si after a new function passes the examination corresponding nodes will receive an update request and can configure and register the new function through a webpage on the basis of the analytic script and xml description java code was used to parse the information of the analytic function and to execute the function dynamically all the parameters defined in the xml texts were transformed to java objects java interfaces were implemented to invoke nco or cdo commands and ncl scripts using the parameters originally defined by the contributor 4 4 configuration as shown in fig 9 the cafe prototype system was deployed on four servers each with a linux 64 bit environment the data index module task managing module and data analysis module were deployed on each node along with the local database system the central server component of cafe was deployed on server a and the web based ui was deployed on server b a webpage wizard and an initialization script were provided for configuration the instructions can be found on our github page https github com thu earthinformationsciencelab 4 5 research use case as a demonstration of the use of cafe following is a typical scenario in which we compared the data produced by various climate models and the observational data usually different model output data would be maintained in different data centers in cmip5 in this scenario a climate researcher tom wants to know the variation of the sea surface temperature in the north pacific area over the past two decades in particular he would like to compare sea surface temperature variable name tos data produced by the gfdl cm3 and ipsl cm5b lr models against the observational data produced by avhrr for the period 1985 2005 these datasets are hosted in three different cafe nodes after logging in via cafe s web based ui tom can search the datasets of interest firstly by specifying query criteria and adding the desired datasets to the list fig 10 he then specifies the time range as 1985 2005 and chooses the analytic function eof analysis specified region because this function can present eof analysis results in a user specified spatial area he defines the area of interest as the north pacific by setting the spatial range and he submits the analysis task fig 11 on the my task page fig 12 tom can view information about all the submitted tasks after clicking the detail link he can see the visualized images charts graphs of the analytic results right on the page fig 13 he can click the button below each analysis result image to obtain the original analysis data files for any further processing in his own computing environment 4 6 performance evaluation to test the performance of the cafe prototype system we used the eof function to analyze a century long 1900 1999 variation of sea ice concentration in the northern hemisphere sea ice concentration sic data produced by the inmcm4 gfdl cm3 miroc esm and hadgem2 es models were used in the first test these four datasets were hosted by four different cafe test nodes our first test was designed to analyze one two three and four model dataset s respectively each test consisted of two parts using a local server aforementioned server b as a client to access the cafe portal to retrieve analytic results and using the same server to accomplish the same analysis in a local computing environment in the local environment we had to download datasets from data centers pre process the datasets and finally compute the results the same workflow and same analytic scripts were used as used by cafe when multiple datasets needed to be analyzed we opened multiple command line windows and caused multiple processes to execute simultaneously we assumed that the local machine had adequate hardware and software environments to do the analysis and all the computing scripts had been written out the information from the nodes and tested datasets are listed in table 2 and table 3 table 4 provides the test results for cafe scenario the total time is for the time consumed from submitting the task until obtaining the results time consumed on the corresponding server means the recorded time consumed for executing subtasks on each participating cafe node time consumed from the client side is the recorded time at the client side from the time the task was submitted to the time the results were obtained the difference between these two reflects the network transportation needed between the client side and cafe node to fulfil the test case for the local analysis scenario the total time is the sum of the time needed to download the datasets plus the time consumed for executing the task in this way we were able to compare the total time consumed in the two scenarios table 4 clearly shows that for cafe growth in the number of datasets did not cause the time consumed to increase substantially the cafe scenario in which the local machine was used as a client to access the cafe portal was greatly faster than the scenario in which the local machine was used to execute the analysis though the configuration of the servers and internet speed might have impacted the results of this comparison the results show clearly that cafe was able to bring a substantial increase in efficiency in terms of the time used to complete the research in actual use convenience for researchers will also be improved since they only have to submit the tasks to cafe without going through additional steps another test was conducted using the four cafe nodes in which 4 8 15 24 30 and 41 datasets were selected to test how long it would take for cafe to execute the aforementioned task the results were 70 s 93 s 102 s 254 s 272 s and 315 s respectively the results suggest that cafe is able to handle multiple datasets in a relatively short time period in the four node situation cafe took only about 5 min to accomplish a task with 41 datasets if these datasets have all been hosted by different nodes the time consumed may have been decreased to about 1 min for this test thus it can be seen that cafe can provide considerable advantage for researchers when handling analysis and intercomparison for multiple datasets 5 discussion cafe supports multinode collaboration for gridded environmental data analysis with flexile extensibility compared to similar web applications cafe can increase researchers efficiency when performing batch analyses visualizations and intercomparison of data from multiple nodes in this way researchers can focus on research instead of on the time consuming tasks of preparing and processing the data cafe software package is expected to be installed in multiple data centers online documents and a web based wizard are provided for ease of installation configuration and maintenance for a cafe node java tomcat and mysql are necessary packages all running in a linux environment to fulfill data analysis as shown by the cafe prototype system tools such as nco cdo netcdf and ncl are expected the web based ui can be configured either on one cafe node or on a standalone server packages of php and apache 2 are needed to host this user interface in our current cafe prototype system metadata and naming system of data files follow cf conventions and cmip5 model output requirements to support more kinds of scientific data some other common data interfaces or protocols such as opendap and web map service wps will be considered in the future a wms implementation such as ncwms blower et al 2013 will be used for visualization of multidimensional gridded environmental data more low level tools like python and r will be accessed to meet the preferences of a wide range of users analytic functions can be extended easily and registered in cafe the contributor needs only provide the analytic script and its xml description file provided that these scripts have an i o interface for the command line and can be invoked by java currently the contributor must manually submit the analytic script and its xml description file to the evaluation group because not all the researchers are familiar with xml we plan to provide a tool for them to publish new functions this will help them to define the parameters in xml and automatically generate the xml document once a new analytic function has passed the evaluation and been authenticated it can be configured and registered either in one specific node or the whole cafe network according to the contributor s request currently cafe does not have a version control mechanism for analytic functions if a new version of a function must be published it will be a totally new function in the whole environment only shared analytic functions of user selected datasets can be used for subsequent analysis support of a version control mechanism will be considered in a future version the cafe prototype system does not maintain the intermediate analysis result in its current design because intermediate results may be of value to end users a future release of cafe will support the reservation of all the intermediate results for a limited period of time for access by end users the same strategy will be applied to server side cache management to handle analysis results users will be able to manage and delete their tasks by themselves and the results will be kept in their workspace for a specified number of days in cafe a simple relational database is used to manage user information encryption techniques for apis have not been used in our prototype which might bring security problems in future versions a stronger security module will be added in addition an authorization system may be introduced for data searching and analysis concurrent access strategy has also been considered in cafe cafe uses a queuing strategy to serve concurrent users that is subtasks dispatched to one node will generate a queue in the cafe node package a configuration file can be used to set parallel parameters and control the number of subtasks to be executed at one time simultaneously although this strategy satisfies the concurrent access issue to a certain extent it may sacrifice analysis efficiency because some subtasks may need to wait for resources cafe aims to avoid large data transfer as much as possible between data nodes or between data node and end users our prototype is capable of processing individual datasets on the node where data are stored and returning the combined information of analysis results urls or json format texts back to the web based ui for further display including maps plots download links etc it does not support direct processing of multiple datasets that reside on different data nodes we plan to support this in the near future if we see a great demand for this functionality in cafe prototype one possible solution is enabling remote access to the data among multiple nodes in addition the situation that one dataset is split on multiple data centers has not been considered because data centers typically host complete datasets taking cmip5 as an example a dataset consists of a series of physical data files share the same attributes e g institute model experiment frequency modeling realm variable name etc with continuous time coverage these files are always stored in the same directory in a decentralized peer to peer file sharing system such as bittorrent a centralized coordination mechanism is still needed for the discovery of a list of nodes which are participating in the system qiu and srikant 2004 following the same design rule cafe introduces a central server to maintain a list of locations addresses of affiliating nodes another role that this central server plays is discovering all the matched data files maintained in the whole cafe network the advantage of this design is that data discovery could be fulfilled promptly while the overhead is that metadata about the data files maintained in the cafe nodes needs to be automatically ingested into central server the connection between the cafe central server and the cafe node s is based on http therefore cafe central server can be physically hosted in a server that is different from the ones where cafe node package is deployed or just reside on one cafe node since there is no data computation load on the central server agreement on hosting the central server besides hosting one cafe node is not hard to be reached in the real world for one institution who is willing to participate in cafe network although cafe is designed to support multi node collaborative analysis each node can still work in single mode if this is the case the local database will be used for dataset and analytic function queries if a node disconnects from the cafe network for any reason or if the central server breaks down this single node and a web based ui that connects to it can jointly provide online data discovery and analysis services for the data resides on this node because cafe has a strong need for server side computation it requires a large server infrastructure for each node the cafe node package must be installed by different data centers data centers need to provide high performance computing and storage resources and personnel at data centers must be trained to use these packages at the same time to accommodate more concurrent users a reasonable resource allocation strategy must be investigated it should be considered that tasks run in a distributed parallel environment several subtasks can run simultaneously in parallel on each node if data centers can offer more computing resources cafe will be able to enhance its performance for concurrent users we understand that this paradigm puts a potential financial burden on data centers however this is a more economically viable solution to face the challenge of larger volumes of data analysis e g cmip6 than requiring all researchers to invest in hardware and software locally because data centers can provide data storage capabilities it is reasonable to expect them to equip themselves with data analysis functionalities this investment could open the door for researchers who need to perform analyses and compare datasets without strong local data storage and computing capabilities handling other types of data is practical for cafe it can support remote sensing data and other kinds of observational data and also can provide collaborative data analysis these data may be produced by multiple sensors or monitors and stored in different data centers provided these data files and their metadata are organized in a certain way e g following cf conventions and cmip5 data reference syntax and their attributes can be parsed successfully these data can be indexed and loaded into the system for different data organization structures the data index module of cafe would require adjustments to adapt to the new data attributes and make the data parsable four main steps may be needed to use cafe in other fields first the attributes that can be indexed and queried need to be redefined according to the metadata of the data files and the data scan service needs to be rewritten to extract the data information second both local and global data information tables need to be redesigned to accommodate to the attributes of the data so that the data information can be successfully stored and queried third analytic scripts must be provided and can be invoked and encapsulated by java and the xml descriptions of analytic scripts need to accommodate to the data attributes fourth a specific web based ui is necessary for data discovery and visualization of results other common features include task management task decomposition and combination and status query and results retrieval of subtasks 6 conclusion cafe is a software package dedicated to collaborative analysis of large volumes of distributed environmental data this paper presented architecture components implementation and deployment details the main contributions of this work are as follows 1 cafe can support web based multinode collaborative analysis of gridded environmental data 2 cafe can significantly reduce the amount of data transferred from data centers to users 3 cafe can be extended to support new data analysis functions and new data formats 4 cafe is promising in facilitating overall research efficiency when dealing with large volumes of gridded environmental data that are distributed on multiple nodes climate model intercomparison e g cmip is a typical scenario of using cafe for more than 20 institutions currently host about 1 5 petabytes of climate model data researchers and practitioners worldwide must be able to diagnose and then compare these data from all possible aspects future development plans for cafe mainly include user workspace management authentication and encryption data provenance management intermediate result management and retrieval acknowledgements this work was carried out with financial support from microsoft research asia collaborative research funding fy14 res sponsor 094 and from the scientific research foundation of tsinghua university 20131089277 appendix a supplementary data the following are the supplementary data to this article data profile data profile supplementary information supplementary information appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 007 
26296,as the amount of environmental data expands exponentially worldwide researchers are challenged to efficiently analyze data maintained in multiple data centers because distributed data access server side analysis multinode collaboration and extensible analytic functions are still research gaps in this field this paper introduces a collaborative analysis framework for gridded environmental data i e cafe multiple cafe nodes can collaborate to perform complex data analysis analytic functions are performed near where data are stored a web based user interface allows researchers to search for data of interest submit analytic tasks check the status of tasks visualize the analysis results and download the resulting data files cafe facilitates overall research efficiency by dramatically lowering the amount of data that must be transmitted from data centers to researchers for analysis the results of this study may lead to the further development of collaborative computing paradigm for environmental data analysis keywords collaborative analysis web based system distributed data environmental data software availability product title cafe collaborative analysis framework for environmental data developers ke yan chang xiao hao xu sha li yuqi bai contact address department of earth system science meng minwei sci tech building s917 tsinghua university beijing china 100084 contact email yuqibai tsinghua edu cn available since 2015 hardware requirement a central storage server as central node computation server s as worker node s web server s for web based user interface these three parts are all required for a complete cafe network and can be integrated in the same machine or separate machines software requirement linux environment mysql tomcat 7 netcdf operators nco climate data operators cdo ncar command language ncl java development kit jdk version 1 7 for cafe node package apache 2 and php 5 for cafe portal package programming language java cafe node php cafe portal availability https github com thu earthinformationsciencelab cafe node https github com thu earthinformationsciencelab cafe portal cost free license gnu general public license 1 introduction environmental science has experienced considerable progress in recent years resulting in part from significant advances in high performance computing data storage and networking technologies the paradigm for environmental research has now moved into a new phase focused on data intensive analysis and integration tolle et al 2011 the amount of environmental data has grown rapidly because of the increasing means of observation such as remote sensors and in situ probes and through the use of advanced numerical models many of these data are stored in gridded file formats many specialty fields in environmental research are transitioning from being data and simulation poor to being data and simulation rich nrc 2001 for instance data produced by a community studying climate change reached the petabyte level in 2008 williams et al 2008 and data will grow to hundreds of petabytes by 2020 overpeck et al 2011 scientists now utilize climate models to study climate change in long time series johns et al 2003 delworth et al 2012 they use multispectral remote sensing images to measure decade long air quality van donkelaar et al 2006 lee et al 2011 and they leverage synthetic aperture radar images to monitor crops blaes et al 2005 and floods matgen et al 2011 all of these studies involve processing of large quantities of gridded environmental data because environmental simulation or observation datasets are often independently produced and maintained by a number of providers scientists usually have to download large volumes of data from multiple data centers it is time consuming for researchers to search for and download data and then to manage and preprocess the data on equipment local to them this common paradigm of everything locally owned and operated bai and di 2012 faces the following challenges 1 datasets usually are managed and stored at multiple sites to discover and analyze the data transparently researchers require distributed data access with a common interface 2 client side analysis is limited by computational and storage capabilities server side analysis is needed to assist users in reducing the volume of data transmission 3 analysis may involve the extraction of data from multiple datasets maintained in different data centers it is essential that the analysis process be executed near data storage facilities to reduce communication time and bandwidth costs and to make all the computing nodes work collaboratively 4 researchers who require access to distributed data and related analytic processes and who need to share their methods may be accustomed to using a variety of tools therefore a system is needed that can be expanded easily to support analytic scripts written in different programming languages such as ncl r and python one promising solution is to enable a new paradigm of everything shared over the web bai and di 2012 because the internet provides powerful communication mechanism for data processing visualization simulation prediction and sharing vitolo et al 2015 online processing and analysis have been playing an important role in enabling data intensive science as proved by nasa s giovanni berrick et al 2009 and google earth engine gorelick 2013 system since researchers often need to analyze environmental data that usually reside on multiple data centers a multinode collaborative solution is further needed to allow complex analysis to be automatically fulfilled near data storage facilities in response to this need we designed a collaborative analysis framework for gridded environmental data cafe a java based distributed data management and analysis framework that allows environmental researchers to work efficiently with distributed datasets freeing them from downloading archiving and preprocessing data locally cafe enables a collaborative environment in which multiple cafe nodes can be integrated to perform complex large scale data analysis it can provide data discovery and task submission functions and deliver analysis results as ready to use graphs and as data files that can be downloaded for further processing to end users via a web based interface another characteristic of cafe is that it supports the deployment of new analytic functions data files in cafe are maintained in a distributed manner they follow the same conventions of file directory file naming and parameter naming the descriptive information about these data files is automatically gathered on a central node multiple nodes can collaborate with each other to perform complex data analysis tasks in a way that data is processed on the node where it resides for this reason task decomposition subtask assignment and result combination mechanisms are further introduced in our system to illustrate the feasibility of multi node collaboration in cafe model output data produced during the phase 5 of coupled model intercomparison project cmip5 and satellite based observational data are used as exemplars the purpose of this research is to leverage the well defined data naming syntax and variable naming specification used in the cmip5 community to enable a web based collaborative data analysis environment in which multiple data nodes can jointly present a logically complete view of distributed data and can automatically perform distributed data analysis without having to download the original datasets the remainder of this paper is organized as follows section 2 provides the literature reviews of related work to reveal the gaps between the available functions provided by a variety of environmental data analysis software packages and the ones highly expected by researchers section 3 describes the system architecture and each component of cafe section 4 presents the implementation details of the prototype system the strengths and current limitations of cafe are discussed in section 5 which leads to the conclusions in section 6 2 related work the typical workflow for analyzing environmental data includes data acquisition preprocessing computing and visualization a complete data analysis framework needs to execute this whole workflow automatically these data must be organized and published by data centers in a parsable way several software packages are available for researchers to preprocess compute and visualize these data each of them has its strengths and limitations thematic real time environmental distributed data services thredds domenico et al 2006 provides remote access to the data files via open source project for a network data access protocol opendap cornillon et al 2003 however improving remote data access cannot solve the problems of transferring large volumes of data some simple tools are available to process gridded environmental data such as netcdf operators nco zender 2008 and climate data operators cdo https code zmaw de projects cdo these tools support batch processing of a large number of data files but they do not provide a uniform user interface many powerful toolkits are available to process and visualize environmental data such as ncar command language ncl http www ncl ucar edu the grid analysis and display system grads berman et al 2001 and ferret http www ferret noaa gov ferret these tools can access remote data via opendap but transferring large volume of data files is still needed when using them to perform intercomparison of distributed datasets in addition many general purpose programming languages such as r https www r project org python https www python org interactive data language idl http www ittvis com idl and matlab http www math works com can support environmental data analysis and visualization the major limitation with these systems is that they cannot support straightforward online processing of environmental data so that researchers still need to write code to perform similar analyses to make environmental science research more efficient several systems have emerged to support access to remote data and to provide web based services each offers an approach to performing the entire workflow for data analysis these systems can be classified based on the major way for researchers to interact with script graphical user interface gui and web based interface the script workflow analysis for multi processing swamp wang et al 2007 is a script based analysis system for gridded data built on nco and opendap it melds computation with data hosting which shifts computation to data sources and eliminates data movement inefficiencies it does not however support multinode collaboration and its analytic functions are not extensible in addition swamp does not provide a gui which further limits its functionality and usability a number of gui based environmental data analysis systems support remote data access and analysis for example the integrated data viewer idv murray et al 2003 is a java based software framework for analyzing and visualizing geoscience data the ultra scale visualization climate data analysis tools uv cdat santos et al 2013 williams 2014 is a provenance based system that supports interactive analytic functions and data intercomparison for climate model output these two desktop applications also support remote data access via opendap hydrodesktop is a web service based tool used for discovering downloading managing visualizing and analyzing hydrologic data ames et al 2012 users can retrieve data from the remote catalog hydrodesktop also provides an extensible interface for creating custom plug ins although these systems provide strong analysis functions poor cross platform ability and the inevitable need for large data transfers have limited their convenience recent years have seen rapid development of web based visualization and analysis systems the national climate change viewer nccv alder and hostetler 2015 is a web based system that visualizes future climate projections from global climate models across the united states this single node system has centralized computation giovanni berrick et al 2009 is a web based system that can easily be used to explore and analyze multisource environmental data although it has demonstrated how its analytic functions can be extended and described berrick et al 2009 giovanni does not provides solution to enable researchers to register new analytic functions to be used in the system nasa s earth exchange nex nemani 2015 provides strong computational resources for end users and user level collaboration as a centralized storage system it is like a virtual machine that offers shared tools and data for users instead of ready to use analysis results climate4impact joussaume 2013 is a web based system with a user friendly interface that can access and analyze remote data based on the data infrastructure established by the earth system grid federation esgf williams et al 2009 2011 cinquini et al 2014 this system supports submission of tasks data intercomparisons and batch analysis through web processing service wps it utilizes centralized computation to handle remote data files therefore it still relies on transmitting large amounts of data the live access server las hankin et al 2002 cinquini et al 2014 is an open source web based system that has been included in esgf as a tool for online analysis and visualization las allows the processing of data at the place where they are stored based on ferret and opendap it has realized multinode data regridding differencing overlaying and intercomparison nonetheless las does not allow researchers to provide ready to use analytics functions into the system although different nodes can work together in las a limitation that only a maximum of four datasets can be analyzed at one time exists on the basis of our assessment of these systems table 1 provides an overview of the current state of support for the four desired environmental data analysis functions outlined in section 1 this table clearly shows that none of these tools or systems fully supports all four desired workflow functions in particular these systems provide little support for multinode collaboration although las could make different nodes work together information exchange does not occur between nodes which severely limits multinode collaboration unlike these tools cafe can merge results from different nodes and provide service level multinode collaboration to end users making all data and analysis transparent 3 system design of cafe to enable efficient data queries and large scale collaborative analysis of gridded environmental data we designed a unique framework for cafe that has four key parts data index module task managing module data analysis module and web based user interface ui a cafe network should have at least one central server and more than one worker nodes a server configured with the cafe node package in the worker mode including data index module task managing module and data analysis module is a cafe node cafe nodes are required to be configured in each data center near the data storage facilities and expected to be installed and maintained by each data center data centers are expected to provide powerful computation and visualization hardware for cafe s server side analysis all cafe nodes can collaborate with each other to automatically fulfill server side analysis of datasets that could be distributed in multiple data centers in a way that data analysis is performed at the place where data resides in addition a central server is needed for the management of descriptive information about each node all the data files managed on all the nodes and all the available analytic functions deployed on these nodes the logical system design of cafe is depicted in fig 1 the data index module is designed to update and manage the data index and descriptive information for all the data locally maintained on the node it also provides data query capability to the web based ui the task managing module is designed for task submission task dispatching and task status results query services on each cafe node the data analysis module is responsible for executing the user selected analytic function the web based ui provides scientific data access functions for end users in particular it can search data define analytic tasks check the status of tasks and present analysis results the data analysis workflow in cafe consists of the following steps 1 the user selects datasets of interest and specific analytic functions 2 the user submits the analysis task through the web based ui 3 the task managing module parses the task and decomposes it into several subtasks based on which nodes are located for the selected data 4 all subtasks are dispatched to the nodes where the datasets are stored 5 all subtasks are fulfilled on the related node side by invoking the corresponding analytic functions on those nodes 6 the task managing module can then present the status of each subtask to the web based ui 7 all results are collected as uniform resource locator url or javascript object notation json format text data results are visualized through the web based ui 8 the user can check the analysis results shown as images or download the original processed data files for further processing the sequence diagram in fig 2 shows the workflow for the complete cafe process 3 1 data index module descriptive information about data stored on a node are maintained by the data index module located on the same node to enhance collaborative analysis capabilities across the whole network cafe maintains global data information e g data file name data attributes and storage path for details see section b in the supplementary information si on the central server to enable fast data query and task decomposition the data index module supports the web based ui to present the available data and functions to end users in particular it contains the data scan index update query page initialization and dataset query services as shown in fig 3 the data scan service can scan data directories iteratively updating the local data information on the node and concurrently updating the global data information whenever the data information is updated on one node the index update service on that node will be executed to update the data information maintained on the central server accordingly the query page initialization service is used to provide available search criteria for users to check through the web based ui it relies on the global data information database to know the selectable attributes of all datasets the dataset query service is responsible for searching the global data information database and getting the information about matched datasets back to the web based ui through application programming interfaces apis details about apis are described in the si 3 2 task managing module the task managing module is the key to fulfill collaborative analysis in cafe as shown in fig 4 this module contains five parts the available functions retriever task scheduler subtask receiver task status checking service and task result retrieval service the available functions retriever can query the global analytic functions database and return the information of available analytic functions to the web based ui according to the attributes of the selected datasets a selectable analytic function can appear in the function list only if it exists on all the corresponding nodes and meets the filtering criteria defined in the function description file the task scheduler can submit the task and decompose it into subtasks when a task is submitted to a node the task information will be maintained in the database on that node this module queries the global data information database on the central server to determine the locations of datasets that are to be analyzed and then decomposes the task into several subtasks the principle is that one subtask corresponds to one dataset according to the dataset s location please note that the situation that one dataset is separately stored in multiple data centers is not considered because data centers typically host complete datasets the relationships between tasks and subtasks also are stored in this node all the cafe nodes are peer to peer they all can assign or receive subtasks subtasks are dispatched to the remote nodes using a web api exposed by the subtask receiver the subtask receiver on a node can receive the subtasks assigned by other nodes the module will store the information of received subtasks into the local subtask status table on that node all the subtasks assigned to a node form a queue they will be performed locally by the data analysis module and the results of each subtask will be stored on that node the resulting information could be urls if the result files are images or gridded files or json texts if the result files are texts and it will be stored in the database on that node when a user requests the status of previously submitted tasks the task status checking service will collect information about all of the subtasks by sending queries to each node and it will send the information back to the web based ui when a user requests analysis results the task result retrieval service will gather each subtask result from the corresponding nodes and will send the results back to the web based ui 3 3 data analysis module the data analysis module consists of an analysis launcher a command executor and several analytic scripts as depicted in fig 5 a complete analytic function includes an analytic script and a description file written by extensible markup language xml examples of analytic scripts include ncl python r and linux bash shell scripts java language is used to wrap these analytic scripts as executable functions in cafe based on their xml descriptions because runtime class in java is used to invoke these scripts by command line these scripts need to expose an input output i o interface to be accessed in the command line taking ncl script as an example the analytic function contributor i e a researcher can define the parameters in the script that can be input externally from the command line the script is required to be tested in the command line environment by the contributor so that it can be invoked successfully in java the xml description file which is expected to be provided associated with the script defines details of properties parameters and required preprocessing of the analytic function the workflow of the data analysis module consists of the following steps 1 the analysis launcher constantly monitors the queue of subtasks to be processed locally it picks up subtasks on a first come first served basis 2 the analysis launcher queries the local analytic functions table to obtain the information of the user selected analytic function 3 the analysis launcher parses the information of user selected analytic function and instantiates it as a java object 4 the command executor invokes the commands to preprocess and analyze the user selected dataset based on the parsed information at the same time the module updates the subtask status maintained in the local subtask status table 5 the command executor maintains the information of analysis results in the local subtask results table when the subtask is finished the database design is described in detail in the si the contributor can set a new analytic function to be invoked in one of two ways either to be used in a specified node only or to be deployed on all the cafe nodes in the latter case the function will be distributed to each node after a contributor submits a new analytic function it will be evaluated and examined by an evaluation group set up by the administrative personnel of the cafe network the corresponding nodes will receive an update request to configure and register the function once it is approved for release once a new analytic function is registered on a cafe node the information of this analytic function will be maintained both in this node and the central server 3 4 web based user interface the web based ui provides a straightforward way for researchers to query and analyze supported datasets through a web browser as shown in fig 6 the ui includes six parts initialization search task submission task list checking task detail checking and results retrieval initialization loads selectable attributes by api search provides the dataset search service the user submits his or her task through the task submission portion task list checking makes it easy for the user to check the task lists she or he has submitted task detail checking provides task details and status to the user the user can retrieve the results through the results retrieval part a relational database is used to store user and task information the task information table records the corresponding relationship between users and tasks a user who wants to use analytic functions in cafe must register an account through the user registration page and then log in once the query page is loaded the web based ui will send the request to the appropriate connected node after which the application will generate the selectable attributes for dataset filtering the user can filter the dataset by specifying institute model frequency and other parameters of the datasets after the user selects desired datasets from the list the web based ui sends requests to the connected node and obtains information about the available analytic functions the user can choose one of the available analytic functions then the web based ui utilizes the information of the function and generates a form for parameter input and validation the user can set the corresponding parameters and submit a task the web based ui sends requests to the node regularly to retrieve real time status of tasks by apis asynchronous refreshing technique is used to show the real time status of each task once the status of all the subtasks turns to be finished the web based ui retrieves all the urls or json format texts of the results through the api the images will be displayed directly on the result page through their urls while the json format texts are designed to be merged and visualized using highcharts highsoft 2015 the user can compare the results of different subtasks from the task detail page and download different kinds of results the webpage will display the graphs or charts of the results and it will provide links for downloading multiformat result files 4 prototype system to prove the feasibility of the design of cafe a use case that requires intercomparison of climate model output and satellite observation data was selected to develop a prototype system 4 1 data during cmip5 more than 20 climate modeling groups around the world contributed more than 60 climate models more than 3 5 petabytes of data are now stored across multiple nodes around the world cinquini et al 2014 for example the coupled climate model fgoals g2 was developed jointly by the state key laboratory of numerical modeling for atmospheric sciences and geophysical fluid dynamics in the chinese academy of sciences and department of earth system science in tsinghua one of the data centers for this model which hosts about 19 terabytes tb of data is located in tsinghua university li et al 2013 all of the cmip5 model data are stored in netcdf format these data follow the climate and forecast cf metadata conventions eaton et al 2011 data files are also organized to follow the rules defined in the cmip5 model output requirements taylor and doutriaux 2010 to test the effectiveness and performance of cafe four test cafe nodes that together host more than 1 tb of data were set up in china these data included model output produced by about 60 cmip5 models available at https esgf node llnl gov projects cmip5 as well as observation data from the advanced very high resolution radiometer avhrr available at https www ncdc noaa gov oisst data access and nimbus 7 available at http nsidc org data nsidc 0051 different nodes stored a few of the same datasets 4 2 implementation the data index module task managing module and data analysis module were integrated into the cafe node package it was developed as a java web application using a combination of the spring framework johnson et al 2005 spring model view controller mvc framework gupta et al 2010 and mybatis wen and jianhua 2012 framework the web based ui which was integrated into the cafe portal package was developed in php using a yii mvc framework winesett 2012 apache2 http httpd apache org and tomcat7 http tomcat apache org were utilized as the hosting environments and mysql was selected http www mysql com as the database engine nco cdo and ncl were incorporated in this prototype because they are good examples of existing simple data processing tools for gridded environmental data and powerful toolkits widely used in the climate research community nco is good for data subsetting and manipulating cdo is convenient for data regridding and averaging and ncl is a powerful command based tool used to analyze and visualize gridded data the web based ui could provide controls dynamically for users to input parameters based on the analytic function description for demonstration purposes following analytic functions were developed and deployed 1 empirical orthogonal function eof analysis eof is a decomposition method that can compute empirical orthogonal functions for user selected variables using the time series of the amplitudes associated with each eigenvalue monthly and seasonal eof analysis results were provided 2 long term mean ltm analysisltm was used to compute the average value of each cell according to user selected variable across a user specified long time period 3 trend analysistrend analysis was used to derive the slope coefficient of a linear regression model based on a user selected variable the result was applied to each cell across the user specified long time period 4 seasonal contoura seasonal contour was used to compute the average value of each cell according to a user selected variable across the user specified long time period for four seasons 5 time seriesthe time series function could compute the time series of the yearly or seasonal value and the corresponding linear regression function according to a user selected variable these five functions could render the analysis results as images or maps and plots based on the ncl script some example results based on ncl are shown in fig 7 4 3 extensibility of analytic functions cafe aims to enable researchers to contribute their own analytic scripts for public access in a simple way the cafe prototype enabled feasible extensibility for analytic functions the contributor needs only to provide his or her script and associated xml description file a new analytic function will be evaluated manually by the evaluation group currently the professionals in the development team and be examined in our test cafe nodes voluntary experts will also be recruited for evaluating the analytic functions the analytic function will be released for registration after evaluation of its usability and functionality it then can be added simply to the cafe system after configuration and registration by the corresponding cafe nodes a new script needs an i o interface for the command line so that java can invoke the script and pass the parameters to the script these parameters also must be indicated in its xml description file a standardized xml file is a prerequisite for a new analytic function fig 8 shows an example format of the analytic function description in the cafe prototype the contributor first defines the name of the analytic function the detailed descriptions of the function are given as well the input can be a single file or multiple files the contributor can set whether or not the function can be distributed to all cafe nodes the properties section is used to define what kind of datasets can be used all the input parameters are set in the controls the inputfileparameters and outputfileparameters sections the contributor can define the parameters that must be input from the web based ui in the controls section parameters of input files and result files are indicated separately in the inputfileparameters and outputfileparameters sections some commonly used preprocessing approaches based on nco or cdo such as file combination regridding yearly or seasonal averaging and long term averaging were encapsulated as java methods in the prototype the contributor can set the preprocessing type in the preprocessing section and then the specified preprocessing type will be triggered the detailed specifications of the xml description file are provided in the si after a new function passes the examination corresponding nodes will receive an update request and can configure and register the new function through a webpage on the basis of the analytic script and xml description java code was used to parse the information of the analytic function and to execute the function dynamically all the parameters defined in the xml texts were transformed to java objects java interfaces were implemented to invoke nco or cdo commands and ncl scripts using the parameters originally defined by the contributor 4 4 configuration as shown in fig 9 the cafe prototype system was deployed on four servers each with a linux 64 bit environment the data index module task managing module and data analysis module were deployed on each node along with the local database system the central server component of cafe was deployed on server a and the web based ui was deployed on server b a webpage wizard and an initialization script were provided for configuration the instructions can be found on our github page https github com thu earthinformationsciencelab 4 5 research use case as a demonstration of the use of cafe following is a typical scenario in which we compared the data produced by various climate models and the observational data usually different model output data would be maintained in different data centers in cmip5 in this scenario a climate researcher tom wants to know the variation of the sea surface temperature in the north pacific area over the past two decades in particular he would like to compare sea surface temperature variable name tos data produced by the gfdl cm3 and ipsl cm5b lr models against the observational data produced by avhrr for the period 1985 2005 these datasets are hosted in three different cafe nodes after logging in via cafe s web based ui tom can search the datasets of interest firstly by specifying query criteria and adding the desired datasets to the list fig 10 he then specifies the time range as 1985 2005 and chooses the analytic function eof analysis specified region because this function can present eof analysis results in a user specified spatial area he defines the area of interest as the north pacific by setting the spatial range and he submits the analysis task fig 11 on the my task page fig 12 tom can view information about all the submitted tasks after clicking the detail link he can see the visualized images charts graphs of the analytic results right on the page fig 13 he can click the button below each analysis result image to obtain the original analysis data files for any further processing in his own computing environment 4 6 performance evaluation to test the performance of the cafe prototype system we used the eof function to analyze a century long 1900 1999 variation of sea ice concentration in the northern hemisphere sea ice concentration sic data produced by the inmcm4 gfdl cm3 miroc esm and hadgem2 es models were used in the first test these four datasets were hosted by four different cafe test nodes our first test was designed to analyze one two three and four model dataset s respectively each test consisted of two parts using a local server aforementioned server b as a client to access the cafe portal to retrieve analytic results and using the same server to accomplish the same analysis in a local computing environment in the local environment we had to download datasets from data centers pre process the datasets and finally compute the results the same workflow and same analytic scripts were used as used by cafe when multiple datasets needed to be analyzed we opened multiple command line windows and caused multiple processes to execute simultaneously we assumed that the local machine had adequate hardware and software environments to do the analysis and all the computing scripts had been written out the information from the nodes and tested datasets are listed in table 2 and table 3 table 4 provides the test results for cafe scenario the total time is for the time consumed from submitting the task until obtaining the results time consumed on the corresponding server means the recorded time consumed for executing subtasks on each participating cafe node time consumed from the client side is the recorded time at the client side from the time the task was submitted to the time the results were obtained the difference between these two reflects the network transportation needed between the client side and cafe node to fulfil the test case for the local analysis scenario the total time is the sum of the time needed to download the datasets plus the time consumed for executing the task in this way we were able to compare the total time consumed in the two scenarios table 4 clearly shows that for cafe growth in the number of datasets did not cause the time consumed to increase substantially the cafe scenario in which the local machine was used as a client to access the cafe portal was greatly faster than the scenario in which the local machine was used to execute the analysis though the configuration of the servers and internet speed might have impacted the results of this comparison the results show clearly that cafe was able to bring a substantial increase in efficiency in terms of the time used to complete the research in actual use convenience for researchers will also be improved since they only have to submit the tasks to cafe without going through additional steps another test was conducted using the four cafe nodes in which 4 8 15 24 30 and 41 datasets were selected to test how long it would take for cafe to execute the aforementioned task the results were 70 s 93 s 102 s 254 s 272 s and 315 s respectively the results suggest that cafe is able to handle multiple datasets in a relatively short time period in the four node situation cafe took only about 5 min to accomplish a task with 41 datasets if these datasets have all been hosted by different nodes the time consumed may have been decreased to about 1 min for this test thus it can be seen that cafe can provide considerable advantage for researchers when handling analysis and intercomparison for multiple datasets 5 discussion cafe supports multinode collaboration for gridded environmental data analysis with flexile extensibility compared to similar web applications cafe can increase researchers efficiency when performing batch analyses visualizations and intercomparison of data from multiple nodes in this way researchers can focus on research instead of on the time consuming tasks of preparing and processing the data cafe software package is expected to be installed in multiple data centers online documents and a web based wizard are provided for ease of installation configuration and maintenance for a cafe node java tomcat and mysql are necessary packages all running in a linux environment to fulfill data analysis as shown by the cafe prototype system tools such as nco cdo netcdf and ncl are expected the web based ui can be configured either on one cafe node or on a standalone server packages of php and apache 2 are needed to host this user interface in our current cafe prototype system metadata and naming system of data files follow cf conventions and cmip5 model output requirements to support more kinds of scientific data some other common data interfaces or protocols such as opendap and web map service wps will be considered in the future a wms implementation such as ncwms blower et al 2013 will be used for visualization of multidimensional gridded environmental data more low level tools like python and r will be accessed to meet the preferences of a wide range of users analytic functions can be extended easily and registered in cafe the contributor needs only provide the analytic script and its xml description file provided that these scripts have an i o interface for the command line and can be invoked by java currently the contributor must manually submit the analytic script and its xml description file to the evaluation group because not all the researchers are familiar with xml we plan to provide a tool for them to publish new functions this will help them to define the parameters in xml and automatically generate the xml document once a new analytic function has passed the evaluation and been authenticated it can be configured and registered either in one specific node or the whole cafe network according to the contributor s request currently cafe does not have a version control mechanism for analytic functions if a new version of a function must be published it will be a totally new function in the whole environment only shared analytic functions of user selected datasets can be used for subsequent analysis support of a version control mechanism will be considered in a future version the cafe prototype system does not maintain the intermediate analysis result in its current design because intermediate results may be of value to end users a future release of cafe will support the reservation of all the intermediate results for a limited period of time for access by end users the same strategy will be applied to server side cache management to handle analysis results users will be able to manage and delete their tasks by themselves and the results will be kept in their workspace for a specified number of days in cafe a simple relational database is used to manage user information encryption techniques for apis have not been used in our prototype which might bring security problems in future versions a stronger security module will be added in addition an authorization system may be introduced for data searching and analysis concurrent access strategy has also been considered in cafe cafe uses a queuing strategy to serve concurrent users that is subtasks dispatched to one node will generate a queue in the cafe node package a configuration file can be used to set parallel parameters and control the number of subtasks to be executed at one time simultaneously although this strategy satisfies the concurrent access issue to a certain extent it may sacrifice analysis efficiency because some subtasks may need to wait for resources cafe aims to avoid large data transfer as much as possible between data nodes or between data node and end users our prototype is capable of processing individual datasets on the node where data are stored and returning the combined information of analysis results urls or json format texts back to the web based ui for further display including maps plots download links etc it does not support direct processing of multiple datasets that reside on different data nodes we plan to support this in the near future if we see a great demand for this functionality in cafe prototype one possible solution is enabling remote access to the data among multiple nodes in addition the situation that one dataset is split on multiple data centers has not been considered because data centers typically host complete datasets taking cmip5 as an example a dataset consists of a series of physical data files share the same attributes e g institute model experiment frequency modeling realm variable name etc with continuous time coverage these files are always stored in the same directory in a decentralized peer to peer file sharing system such as bittorrent a centralized coordination mechanism is still needed for the discovery of a list of nodes which are participating in the system qiu and srikant 2004 following the same design rule cafe introduces a central server to maintain a list of locations addresses of affiliating nodes another role that this central server plays is discovering all the matched data files maintained in the whole cafe network the advantage of this design is that data discovery could be fulfilled promptly while the overhead is that metadata about the data files maintained in the cafe nodes needs to be automatically ingested into central server the connection between the cafe central server and the cafe node s is based on http therefore cafe central server can be physically hosted in a server that is different from the ones where cafe node package is deployed or just reside on one cafe node since there is no data computation load on the central server agreement on hosting the central server besides hosting one cafe node is not hard to be reached in the real world for one institution who is willing to participate in cafe network although cafe is designed to support multi node collaborative analysis each node can still work in single mode if this is the case the local database will be used for dataset and analytic function queries if a node disconnects from the cafe network for any reason or if the central server breaks down this single node and a web based ui that connects to it can jointly provide online data discovery and analysis services for the data resides on this node because cafe has a strong need for server side computation it requires a large server infrastructure for each node the cafe node package must be installed by different data centers data centers need to provide high performance computing and storage resources and personnel at data centers must be trained to use these packages at the same time to accommodate more concurrent users a reasonable resource allocation strategy must be investigated it should be considered that tasks run in a distributed parallel environment several subtasks can run simultaneously in parallel on each node if data centers can offer more computing resources cafe will be able to enhance its performance for concurrent users we understand that this paradigm puts a potential financial burden on data centers however this is a more economically viable solution to face the challenge of larger volumes of data analysis e g cmip6 than requiring all researchers to invest in hardware and software locally because data centers can provide data storage capabilities it is reasonable to expect them to equip themselves with data analysis functionalities this investment could open the door for researchers who need to perform analyses and compare datasets without strong local data storage and computing capabilities handling other types of data is practical for cafe it can support remote sensing data and other kinds of observational data and also can provide collaborative data analysis these data may be produced by multiple sensors or monitors and stored in different data centers provided these data files and their metadata are organized in a certain way e g following cf conventions and cmip5 data reference syntax and their attributes can be parsed successfully these data can be indexed and loaded into the system for different data organization structures the data index module of cafe would require adjustments to adapt to the new data attributes and make the data parsable four main steps may be needed to use cafe in other fields first the attributes that can be indexed and queried need to be redefined according to the metadata of the data files and the data scan service needs to be rewritten to extract the data information second both local and global data information tables need to be redesigned to accommodate to the attributes of the data so that the data information can be successfully stored and queried third analytic scripts must be provided and can be invoked and encapsulated by java and the xml descriptions of analytic scripts need to accommodate to the data attributes fourth a specific web based ui is necessary for data discovery and visualization of results other common features include task management task decomposition and combination and status query and results retrieval of subtasks 6 conclusion cafe is a software package dedicated to collaborative analysis of large volumes of distributed environmental data this paper presented architecture components implementation and deployment details the main contributions of this work are as follows 1 cafe can support web based multinode collaborative analysis of gridded environmental data 2 cafe can significantly reduce the amount of data transferred from data centers to users 3 cafe can be extended to support new data analysis functions and new data formats 4 cafe is promising in facilitating overall research efficiency when dealing with large volumes of gridded environmental data that are distributed on multiple nodes climate model intercomparison e g cmip is a typical scenario of using cafe for more than 20 institutions currently host about 1 5 petabytes of climate model data researchers and practitioners worldwide must be able to diagnose and then compare these data from all possible aspects future development plans for cafe mainly include user workspace management authentication and encryption data provenance management intermediate result management and retrieval acknowledgements this work was carried out with financial support from microsoft research asia collaborative research funding fy14 res sponsor 094 and from the scientific research foundation of tsinghua university 20131089277 appendix a supplementary data the following are the supplementary data to this article data profile data profile supplementary information supplementary information appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 007 
26297,although formal simulation optimization approaches have been shown to be able to identify near optimal outcomes for a range of stormwater management problems stakeholder acceptance of these solutions can be problematic especially if there is a lack of familiarity with the optimization processes and simulation model used to arrive at these solutions to address this problem a portfolio optimization problem formulation is introduced that allows stormwater best management practices bmps to be evaluated by stakeholders before the portfolio selection process this enables the search space to be constrained before the bmp optimization process ensuring that model results are transparent and only represent solutions that are trusted by experienced practitioners this has the effect of reducing reliance on simulation optimization involving complex stormwater simulation models and increasing buy in to the optimization results the portfolio optimization formulation is applied to a catchment management problem in australia using a typical many objective optimization approach including visualization techniques keywords stormwater management water sensitive urban design portfolio optimization many objective optimization visual analytics multi criteria decision analysis 1 introduction recently many objective evolutionary algorithms have been developed and applied to identify pareto optimal solutions to water resources planning problems kollat and reed 2007 matrosov et al 2015 these approaches have used visual analytics techniques to aid exploration and analysis of the typically large numbers e g 1000s of pareto optimal solutions identified and to select several suitable schemes to present to decision makers the many objective optimization visual analytics approaches have enabled trade offs between four 4 or more planning objectives to be considered which better reflects the number of planning objectives considered by practitioners in real world water resources planning problems i e better than typical single or bi objective optimization problem formulations as discussed in recent many objective optimization studies kasprzyk et al 2012 2015 matrosov et al 2015 woodruff 2016 optimizing planning solutions for a sub problem of what is in fact a many objective problem can lead to cognitive myopia which is a negative decision making bias that arises due to drawing incorrect inferences and conclusions from limited problem information in this light consideration of a limited number of formal objectives in optimization studies can encourage the identification of solutions with sub optimal performance with respect to criteria that are not included as formal objectives but are important to contemporary water resources managers woodruff et al 2013 it is therefore preferable to optimize with respect to all relevant formal objectives where possible in the stormwater management optimization field recent stormwater best management practice bmp optimization approaches have typically included an integrated stormwater simulation model bach et al 2014 linked with an evolutionary algorithm maier et al 2014 for the optimal sizing and placement of bmps di matteo et al 2017 within a watershed to achieve environmental benefits from treating stormwater runoff however for regional scale stormwater management problems formal objectives have been limited to two including ecosystem health benefits including water quality improvement and cost chen et al 2015 chichakly et al 2013 lee et al 2012 zou et al 2015 this is despite the fact that in many cases stormwater management optimization can be better represented as a many objective optimization problem considering a larger number of objectives this is because stormwater managers must consider a range of performance criteria due to a number of socio political drivers including water supply security public health protection social amenity urban flow regime improvement environmental protection and flood mitigation askarizadeh et al 2015 marlow et al 2013 in response to these drivers bmps have been developed to provide multiple functions in addition to water quality improvement for example stormwater harvesting clark et al 2015 di matteo et al 2017 mitchell et al 2007 and urban vegetation and amenity improvement sharma et al 2016 such bmps may include structural and non structural measures for detention harvesting infiltration evaporation and transport of non point source urban stormwater runoff lerer et al 2015 conventional formulations of bmp selection problems are likely to contribute to a lack of acceptance of solutions obtained from optimization studies this is because the practical relevance of the optimization solutions depends largely on how decision makers feel about the credibility of the evaluation techniques and data used in the decision making process aumann 2011 for example stormwater management strategies developed by algorithms may not be trusted and adopted by decision makers who are unfamiliar with the optimization process and how the strategies are generated maier et al 2014 in addition stormwater simulation optimization approaches maringanti et al 2009 srivastava et al 2002 may not complement current practice for management of large regional catchments which typically involves ad hoc selection and implementation of bmps as funding becomes available in order to develop trusted stormwater management strategies that are likely to be adopted in practice decision maker engagement should be encouraged in all aspects of optimization studies applied to water resources problems maier et al 2014 voinov and bousquet 2010 wu et al 2016 therefore the problem formulation and system models used should incorporate existing modelling practice and practitioners should aim to use optimization as a complementary tool to existing approaches where possible such an approach is likely to encourage the uptake of formal many objective optimization approaches by decision makers as it seeks to provide advice on the best course of action under the institutional and political constraints that exist in the real world this would improve upon current practices in order to address the shortcomings of existing optimization problem formulations discussed above the objectives of this paper are i to present a novel optimization problem formulation for selecting combinations of stormwater bmps that a can cater to a large number of performance criteria b can handle a large number of decision options and potential strategies c can consider detailed interactions between interdependent parts of the systems d can enable the identification of solutions that represent the best possible trade offs between performance criteria e enables trade off information to be communicated in an easy to understand fashion and f enables the development of solutions that are trusted by decision makers ii to demonstrate the utility of the optimization problem formulation by applying it as part of a generic optimization framework to a case study focused on the selection of stormwater bmps for a major city in australia the generic optimization framework includes the novel portfolio optimization problem formulation see objective i a many objective optimization technique to identify solutions to the problem and a visual analytics package to explore analyze and select portfolios of bmps and iii to use the case study to a investigate the possible many objective trade offs between lifecycle cost water quality improvement stormwater harvesting capacity and urban vegetation and amenity improvement b investigate the importance of a many objective approach compared with a bi objective water quality cost optimization as has been undertaken in most previous studies and c demonstrate trends in the impact of particular bmp projects on pareto optimal portfolio performance and how these may influence decision making 2 proposed stormwater bmp selection optimization formulation 2 1 outline of proposed bmp selection approach a conceptual outline of the proposed stormwater bmp selection optimization formulation is shown as items 1 and 2 within a generic many objective optimization approach kollat and reed 2007 fig 1 the portfolio optimization formulation presented allows a framework to satisfy desirable criteria for stormwater bmp selection methods which is not the case with existing approaches as outlined in the introduction as follows i the ability to develop solutions that are trusted by and have buy in from decision makers is accounted for by formulating the problem as a portfolio optimization problem as part of which only stormwater bmps that are suggested by decision makers are considered as potential options and decision maker driven evaluation of bmps is used ii the ability to cater to a large number of performance criteria and options as well as the ability to identify solutions that represent the best trade offs between the performance criteria is facilitated because the portfolio optimization formulation allows for the use of look up tables for the evaluation of solution performance as such the optimization process allows detailed interactions between interdependent parts of a system to be considered without the need for a complex stormwater model that is linked with an optimization algorithm the formulation handles the evaluation of performance of 1 independently functioning bmps by calling on performance data stored in a look up table generated a priori i e before the optimization process by experienced practitioners using appropriate techniques to evaluate the performance of each individual bmp 2 interdependently functioning bmps by calling on performance data stored in a look up table that are results of simulations of smaller and localized bmp systems conducted a priori in the first step of the overall optimization framework fig 1 a list of potential stormwater management bmps p is identified these bmps are then evaluated individually by practitioners and the interdependencies between them determined all possible combinations of these individual projects make up the full portfolio solution space which is expected to be too large to adequately evaluate by trial and error or enumeration therefore in order to enable consideration of many performance criteria f and a wide exploration of the potential portfolios p a formal optimization approach is adopted the best combinations of bmps are represented as pareto optimal solutions p to a many objective portfolio optimization problem formulation cruz et al 2014 in order to analyze the large number of pareto optimal solutions produced by the optimization process and to present the optimal trade offs to decision makers in a manner that is easy to understand interactive visual analytics are used to explore trade offs and impacts of bmps on portfolio performance the proposed formulation is in alignment with approaches using many objective portfolio optimization problem formulations with decision maker driven evaluation of objective function values as pointed out by maier et al 2014 such a portfolio optimization formulation is likely to make many objective optimization accessible to decision makers whose current level of decision making sophistication includes multi criteria decision analysis this is because the options under consideration as well as the final selection of the portfolio to be implemented are based on the domain knowledge of individual practitioners in contrast there is likely to be less decision maker buy in and trust when simulation optimization approaches are used to determine optimal solutions as interactions between complex systems of bmps and therefore the rationale behind the performance values of portfolios are not transparent to decision makers who may not use complex simulation models to support decision making having said this it must be noted that in practice there is a trade off between obtaining mathematically optimal solutions which may be better approximated using a simulation optimization approach versus obtaining solutions that encourage decision maker buy in and will therefore more likely influence the final stormwater management strategy adopted which may be better achieved by having decision maker driven evaluation of portfolios the proposed approach may not identify the mathematically optimal solutions since the sizes of bmps are not considered as decision variables and the evaluation of interactions between bmps is performed a priori and or informed by decision makers however as the approach balances competing desires to produce mathematically optimal solutions and solutions that are trusted by decision makers it makes the benefits of optimization more accessible to practitioners and should encourage better stormwater management strategies to be adopted in practice the detailed steps for implementing the conceptual approach presented in fig 1 including the proposed problem formulation are given in fig 2 which are explained in the following sections 2 2 problem formulation the first part of the optimization framework consists of steps required to formulate a portfolio optimization problem that represents the stormwater management problem to achieve multiple catchment benefits numerous stormwater best management practices bmps are typically considered to intercept and deal with runoff at locations distributed throughout a catchment examples of bmps may include biofiltration systems biofilters which typically consist of a basin overlaying a filter medium constructed wetlands which are shallow extensively vegetated basins that use enhanced sedimentation fine filtration and pollutant uptake processes to remove runoff pollutants and swales which are vegetated channels appropriate types and locations of bmps largely depend on site characteristics including soil type and properties topography infiltration rate contributing connected impervious area and the space available to access for maintenance site characteristics are typically assessed through on site and geospatial studies inamdar 2014 after site assessment a short list of feasible bmps is agreed upon amongst decision makers taking into account the potential to achieve desired performance criteria and other socio political factors chichakly et al 2013 sharma et al 2016 the performance of each bmp is then evaluated independently against multiple criteria using accepted models based on the contributing sub watershed for each bmp and in consultation with experienced local experts inamdar 2014 in the absence of an adequate regional scale integrated model to evaluate the downstream impact of bmps interactions between bmps that influence individual bmp performance are evaluated based on expert judgment and modelling of bmps and multiple contributing sub watersheds to determine decision making rules or performance models for interdependent projects for examples of formulating interactions in portfolio optimization see section 2 description and formalization of the problem in cruz et al 2014 the individual projects their performance interdependencies and practical limitations on portfolio size are then formulated as the decision variables objectives and constraints of a mathematical optimization problem 2 2 1 a priori evaluation of independent and interdependent bmps as part of the proposed formulation the performance of independent and interdependent bmps can be evaluated a priori to eliminate the need for a complex stormwater model to be called during the optimization process in the case where a small number of bmps are interdependent an a priori analysis of interactions between those particular bmps can be used to estimate their objective function values when adopted together in different combination when one or more interdependent bmps appear together in a potential solution alternate objective function values can be used i e by the algorithm by referring to a look up table of simulation results to reflect the interdependency compared to where they appear individually to illustrate how dependency between bmps can be handled in the optimization formulation consider the regional stormwater bmp system in fig 3 where bmps treat stormwater runoff from upstream catchments as described below the individual bmps to the left can be considered to operate independently from all others this is because 1 inflows from these bmps cannot consist of treated outflows from another bmp and 2 outflows from these bmps cannot be inflows into another bmp as there are no bmps located at the downstream end of the 1st order stream this means where bmps are assumed not to be implemented e g as an outcome of the optimization process the performance of an independent bmp if it is present in the system will be independent of any other system configuration as it will have the same inflow characteristics therefore the performance of each independent bmp can be simulated using a model with one node representing the bmp a priori of the optimization process rather than needing a complex model consisting of all bmps to be called during the optimization process as is typical of previous studies the bmps to the right are interdependent since removing a and or b will affect inflow characteristics and therefore the performance of c the performance of the local sub system can be simulated using a stormwater model with three nodes and two drainage links which is still simpler than a model of the whole system solutions to the sub system of bmps can be enumerated a priori the enumeration may include several sizes for each bmp and several combinations of bmps the objective function values of pareto optimal configurations of the sub system can be kept in a look up table to be called upon by the optimization process to efficiently calculate the whole portfolio objective function values the appropriate sizes for the configuration selected by the optimization algorithm can be made available for further analysis this removes the requirement for a large and complex stormwater model to be called to simulate every solution again as is typical of previous studies importantly this approach is effective where there is a large number of bmps in the regional system and the sub systems consist of small numbers of bmps e g 2 to 5 such that the computational time to enumerate the sub system using a simple model is short enough to be appropriate for the decision making task all sub systems within a regional system can be enumerated separately using simple models to provide performance data for the look up table 2 2 2 limiting the solution search space in the proposed formulation to further reduce the solution search space and complexity of analysis of solutions additional considerations including the size type and location of bmps can be handled without them being decision variables as follows the size of bmps is often largely dependent on the water quality improvement required by regulators where there are regulatory water quality improvement targets to minimize costs the size of bmps is set such that these targets are just met for example where total nitrogen tn is a limiting pollutant and the target reduction of tn from increased runoff from a development is 45 removal the size of a bmp is increased until it just achieves 45 reduction for an individual bmp this is a simple optimization that can be undertaken a priori for small sub systems the target may apply as a constraint for the system rather than individual bmps when considering the pareto optimal configurations to include in the look up table the type of bmp e g biofilter wetland swale is often highly dependent on the site characteristics design objectives and preferences of planners therefore often a small number of bmp types is available for each particular location the proposed approach can handle more than one bmp type at a location where two or more bmp projects exist in the same location this would require a simple mutual exclusivity constraint on the decision variables for these projects e g a constraint could be if project a at location 1 is selected for the portfolio then project b at location 1 cannot be selected in the proposed approach the project decision variables act as a surrogate for the location of bmps for example project c may exist at location 2 in a catchment so by selecting project c in a portfolio this means location 2 is also selected impacts on bmp performance specific to a location can be reflected in the costs and benefits associated with the project at the location and in terms of the objective functions representing interactions with other projects 2 2 3 encouraging stakeholder buy in the proposed formulation assists with building trust in the solutions generated by the optimization process by only considering solutions proposed by end users this aspect of the optimization approach aligns with existing multi criteria analysis mca approaches currently used in practice without introducing additional complexity through typical optimization approaches in practice typically individual projects are identified by consultants and stakeholders through a consultation process then a guided scoring of individual projects is carried out typically the stakeholders provide weighted scores reflecting their preferences for different criteria and a final short list of solutions is selected based on these preferences one of the benefits of this approach is that the scoring is transparent to stakeholders and therefore stakeholders are likely to buy into the results of the mca however often stakeholder preferences change once a diverse set of solutions and trade offs between the set of solutions objectives are visualized traditional optimization problems can provide diverse solution sets and trade offs but the solutions are typically assessed using a simulation model the results of which may not be trusted by all stakeholders especially where stakeholders have limited familiarity with modelling techniques and where systems are complex and their performance is not easily traceable to design decision changes in addition the proposed solutions are likely to be unfamiliar to stakeholders making them more difficult to be trusted however with the proposed formulation the scores attributed to individual bmps by stakeholders can be used as objective function values for a portfolio optimization analysis therefore the scores attributed by stakeholders are reflected in the diverse trade offs across a range of preferences identified in the optimization process that can be explored and analyzed and reveal optimal combinations of projects not possible with mca techniques with the direct link between the scores attributed by stakeholders and the optimization results there is arguably a better chance that optimization results will be used to support decision making especially where decision makers have not been trained extensively in decision support approaches or bmp systems modelling with the proposed formulation a variety of expertise can be accommodated whilst maintaining a diverse set of solutions to select from it is acknowledged there are further issues related to trust at other stages of the decision making process that have not been considered here and require further work however being able to use the results generated through existing analysis techniques improves the likelihood that optimization will be adopted for bmp selection problems in practice especially since the individual components of solutions identified by the optimization process were proposed by and are therefore familiar to stakeholders 2 2 4 portfolio optimization problem to identify portfolios that represent the best trade off between many objectives the project portfolio selection problem is defined as the optimization of vector f p composed of n objective functions 1 f p f 1 f 2 f n where p is a portfolio of projects and f is a vector of the associated costs and benefits of a portfolio for a more in depth description of a generic portfolio optimization formulation the reader is referred to cruz et al 2014 the generic decision variables objectives and constraints particular to the stormwater management portfolio selection problem are as follows 2 2 4 1 decision variables it is assumed that each bmp project has a pre determined size type and location as such each decision variable is a binary variable d i that represents the decision whether or not to adopt project p i there are np possible projects and thus np decision variables given by d d 1 d 2 d np where d i 0 1 for all i n 0 1 i np a portfolio p is defined as the set of projects p i for all i where d i 1 2 2 4 2 objectives although objectives depend on decision maker interests four formal objectives addressing one or more economic social or environmental stormwater management goals are included in the proposed generic formulation economic cost water quality improvement stormwater harvesting capacity and combined urban vegetation and amenity improvement the objectives for lifecycle cost water quality improvement and stormwater harvesting are adapted from general objectives presented in di matteo et al 2017 a generic optimization approach for distributed stormwater harvesting systems the green score objective formulation is provided as an example of adapting multi criteria analysis results here scores for multiple sub criteria including tree cover amenity within a single overarching criterion here urban amenity for use as a formal optimization objective economic cost is a primary concern for decision makers responsible for maximizing return on investment including capital maintenance and operating costs water quality improvement is a key environmental objective considered by stormwater management authorities chichakly et al 2013 yang and best 2015 maximizing stormwater harvesting swh volume is a primary motivation for implementing projects with swh capacity in order to reliably meet irrigation demand and can also contribute to runoff volume reduction and groundwater recharge known to produce ecosystem health benefits askarizadeh et al 2015 an amenity improvement score is proposed as the social criterion as bmps are typically located in public open spaces and are maintained using public resources and urban vegetation and amenity improvement is often an important criterion for evaluating bmps in the proposed formulation the economic cost of a portfolio of projects is represented as a life cycle cost lcc eq 2 di matteo et al 2017 which is a discounted sum of expected future costs for stormwater management assets including bmps and transfer infrastructure required to harvest stormwater taylor and wong 2002 the life cycle cost objective function for each candidate portfolio of bmps is given by 2 minimize f c o s t l c c b m p l c c s w h where 3 l c c b m p i 1 n t a c b m p i p w f e s t a b b m p i s a b m p i e c f b m p i m b m p i p w f m a i n t b m p i s a b m p i m b m p i 4 l c c s w h c c a p t a n k c c a p p i p e c c a p c o n t r o l c c a p p u m p p w f m a i n t c m t a n k c m p i p e c m c o n t r o l c m p u m p where the sum of the cost of bmps to capture and treat stormwater runoff lcc bmp eq 3 and to transfer harvested water to a balancing storage for further treatment and distribution lcc swh eq 4 is applied with bmp i representing the ith bmp in the candidate portfolio n integer is the number of projects in the portfolio tac is the total acquisition cost as a function of sa the surface area of bmp i and n integer is the number of projects in a portfolio pfw estab fraction for the establishment period and pwf maint for the remaining design life of system components are the present worth factor for a series of annual costs computed using a discount rate ecf fraction is the establishment cost factor i e multiplier for the annual maintenance cost m during the establishment period typically 1 2 years for each bmp for bmps with a stormwater harvesting function c captank c cappipe c capcontrol and c cappump are the capital costs for required storage tank control systems pipes and pump stations and c mtank c mpipe c mcontrol and c mpump are the annual maintenance costs for the tank pipes control systems and pumps and operating costs respectively the water quality improvement indicator adopted in the proposed framework is the total average annual pollutant load reduction of one target pollutant eq 5 only one target pollutant is adopted to limit the number of objectives and therefore limit the difficulty in identifying optimal solutions however if the trade offs between multiple water quality indicators need to be known then these can be added as objectives this indicator is widely adopted to assess the performance of wsud approaches including swh systems browne et al 2012 the target pollutant s will depend on decision maker interests the water quality improvement objective function is 5 maximize f q u a l i t y i 1 n s o u r c e i r e s i d i where f q u a l i t y mass year 1 is the mean annual pollutant mass retained by bmps in each candidate portfolio n is the number of bmps in a portfolio resid i mass year 1 is the mean annual mass of pollutant leaving the ith bmp s contributing catchment area and source mass year 1 is the mean annual mass of pollutant that reaches the ith bmp s catchment outlet in a post development catchment baseline scenario without intervention resid and source should be determined using a stormwater quality assessment model accepted by the stormwater management authority bach et al 2014 coombes et al 2002 average annual supply capacity eq 6 is adopted as an indicator of stormwater harvesting performance mitchell et al 2008 this metric is proposed because it can be determined from generic storage yield reliability curves for a catchment at the project screening phase of stormwater management browne et al 2012 hanson and vogel 2014 or other techniques inamdar 2014 in addition the average annual capacity approximates runoff volume reduction due to harvesting which has ecosystem health benefits askarizadeh et al 2015 the supply stormwater harvesting objective function is 6 maximize f s u p p l y i 1 n s u p p l y i where supply i volume is the average annual stormwater harvesting supply capacity for the ith bmp in a portfolio the urban vegetation and amenity improvement indicator depends on decision maker interests which may include maximizing vegetation and tree coverage and quality of recreation spaces each project should be appraised and evaluated scored by vegetation experts the cumulative urban vegetation improvement objective function is 7 maximize f g r e e n i 1 n g r e e n i where green i integer is a score determined by expert assessment attributed to the ith project in a portfolio 2 2 4 3 constraints strategic and logical constraints on the selection of projects and performance of portfolios could be considered and are case specific cruz et al 2014 for example where multiple sub region catchment institutions fund an integrated catchment strategy constraints on the selection of projects could 1 ensure equitable distribution of projects amongst constituent stormwater management sub regions 2 limit the maximum number of projects in a portfolio nmax and projects within each sub region 3 prevent the presence of mutually exclusive projects as some bmps may be redundant in the same portfolio and 4 limit the budget allocated to projects within each sub region additional considerations for portfolio based constraints are discussed in cruz et al 2014 2 3 optimization process the second part of the optimization framework fig 2 describes the algorithmic processes used to solve the optimization problem only portfolios that are non dominated i e none of the objective functions can be improved in value without degrading one or more of the other objective function values can be considered as portfolios that represent the best trade off between objectives to identify the non dominated or pareto optimal solutions to the mathematical optimization formulation use of a many objective metaheuristic algorithm is suggested metaheuristic algorithms have several advantages over traditional optimization approaches such as linear programming they can deal with multiple objectives simultaneously maier et al 2014 and have been successful in recent planning and design optimization studies considering urban water beh et al 2014 2017 blinco et al 2017 marchi et al 2016 newman et al 2014 paton et al 2014 wu et al 2017 and distributed bmp systems chichakly et al 2013 di matteo et al 2017 as part of the generic optimization process a number of solutions is generated with the aid of a many objective metaheuristic algorithm each solution represents a set of binary decisions on whether or not to adopt each available project in a portfolio in the construction of a solution projects are added to a portfolio until a constraint on the maximum number of projects is reached or all projects have been considered i e a portfolio can consist of fewer than the maximum number of projects then portfolios are evaluated against logical and strategic conditions for example mutual exclusivity of projects if a portfolio violates these conditions the objective function values are set to a penalty value the penalty value will depend on the optimization problem considered next the performance of valid portfolios is evaluated by calculating objective functions including interactions see section 2 2 1 after evaluation final penalties are applied to objective function values of solutions that fail to meet defined constraints the metaheuristic algorithm uses objective function values to assess the fitness of solutions and to iteratively modify solutions over a number of iterations solutions converge towards the set of pareto optimal portfolios which are non dominated in the set of all feasible portfolios the metaheuristic iterative approach continues until specific termination criteria are met for example a maximum number of iterations the non dominated solutions identified by the optimization process are pareto optimal or near optimal as one can never prove that the true pareto front has been found when using evolutionary algorithms stormwater management portfolios 2 4 visual analysis of pareto optimal portfolios an interactive visual analytics package hadka et al 2015 kollat and reed 2007 is suggested to assist decision makers to explore analyze and ultimately select appropriate portfolios that represent a desired compromise between performance criteria and practical stormwater management strategies maier et al 2014 firstly the pareto optimal portfolio performance and decision data as well as alternative data that may be useful for decision making e g average contributing catchment size bmp type number of projects are uploaded into the visual analytics package next high dimensional coordinate plots or parallel coordinate plots inselberg 2009 are used to visualize the performance of the large number of pareto optimal portfolios in many objective space then in order to reduce the number of portfolios considered for further analysis dynamic filtering to eliminate undesirable solutions can be carried out by analysts based on the decision maker s budget constraints and minimum preferences for each benefit and eliminate apparently undesirable combinations of bmps not anticipated a priori piscopo et al 2015 within the reduced set decision makers and analysts can use brushing to highlight sub sets of interesting solutions multiple linked plots of the same data set can assist with identifying and rationalizing trade offs such as conflicts and areas of diminishing returns between objectives and emergent behavior caused by the inclusion of particular bmps within portfolios interactive visualization of optimization objectives and decision spaces simultaneously enables decision makers with the assistance of analysts to rapidly identify subsets of portfolios that contain preferred projects and compare their performance to that of other portfolios in this way browsing through solutions to investigate and learn about the impact of individual project preferences on total catchment benefits can allow decision makers to overcome institutional decision making biases kollat and reed 2007 matrosov et al 2015 ultimately several desirable portfolios are selected for further consideration 3 case study in this study we demonstrate the many objective bmp selection approach on a regional stormwater management strategy for a major coastal city in australia a catchment management authority commissioned engineering consultants to identify sites for stormwater bmps within an integrated catchment with an outlet flowing into a prominent marine body the integrated catchment covers an area of approximately 700 km2 with average annual rainfall of 400 700 mm and is comprised of highly urbanized and peri urban regions managed by three local government authorities a primary objective for the catchment management authority was to reduce the nutrient load from urban stormwater runoff flowing into the marine body in addition since the potential sites for bmps were within public open spaces managed by local government authorities stormwater harvesting for irrigation of open spaces and increasing vegetation and public amenity value were considered important additional benefits the consultants identified 70 np 70 potential biofiltration wetland and swale projects at locations distributed in open spaces throughout the three local government authority regions through stakeholder consultation thirteen of these have a capacity for stormwater harvesting in addition the consultants suggested that a portfolio of 20 projects or fewer nmax 20 was practical the bmps were considered to be mutually independent for the purposes of demonstrating the optimization approach as the contributing catchment areas to each bmp did not coincide i e downstream impact of bmps would not affect the performance of other bmps within the large regional catchment and as such interdependencies between bmps were not considered in the case study application the number of possible portfolios was k 1 20 70 70 k k 2 59898 10 17 which is too large to fully enumerate using a typical desktop computer the application of the proposed optimization approach was part of a real world study involving a multi criteria analysis conducted to identify a portfolio of bmp projects for a regional catchment this allowed the authors to demonstrate how the proposed approach can consider existing bmp selection practices which is a study objective as the case study application was only intended to demonstrate the optimization approach the results of the study were reviewed by consultants but were not used to inform decision making engagement between decision makers engineering consultants and the optimization analysts who are the authors of this study was carried out as follows firstly the engineering consultants ran one workshop where the broad stormwater management objectives were established which was attended by a working group of 16 decision makers from local government authorities and the catchment management authority the consultants then identified sites assessed them for quantitative metrics e g required size of bmps to meet water quality constraints cost and stormwater harvesting capacity and carried out a preliminary scoring each of the qualitative metrics e g vegetation improvement and amenity value using objective thresholds consultants then sent these preliminary scores to local government authorities who were asked to provide a response these were generally reviewed by landscape bushland horticultural and parks and open space staff the staff involved and level of response varied between the local government authorities consultants then had a workshop with each of the individual local government authorities to review the sites establish a common understanding of the whole catchment management opportunity and confirm the proposed individual project scoring then important objectives were refined into formal optimization objectives by the consultants and optimization analysts the analysts used the multi criteria evaluation data to inform the optimization problem formulation including decision variables projects and to develop objective functions objective function values and constraints the names of the decision makers and catchment regions involved are not disclosed in this study the data used for this study are listed in the references tables supplements and repository at di matteo et al 2016 based on information provided by regulators a single climate scenario was considered however it should be noted that alternative climate scenarios could have been considered for example the pareto optimal solutions could be each assessed against a dry climate scenario and solutions dominated under that scenario removed from further consideration as in chichakly et al 2013 it should also be noted the formulation does not preclude a robustness evaluation maier et al 2016 mcphail et al 2018 riddell et al 2018 however this would require the analyst to evaluate the independent bmps and sub systems of bmps under different climate scenarios and to use multiple look up tables to store the a priori evaluation results the portfolio optimization problem formulation developed for the case study the optimization process used to solve the problem and the visual analytic approach used to analyze explore and select from optimal bmp portfolios are presented in the following sections 3 1 problem formulation 3 1 1 decision variables the 70 potential bmps table 1 were formulated as 70 decision variables with two corresponding decision options to adopt or not adopt a bmp in a portfolio following a preliminary desktop analysis bmps were determined by decision makers to have contributing catchments ranging in size from 3 ha to 421 2 ha with an assumed 50 pervious and 50 impervious area the functional areas of bmps were pre determined by consultants and sized to meet functional requirements for total nitrogen total phosphorous and total suspended solids runoff pollutant reduction targets dr dale browne personal communication 2016 3 1 2 objectives 3 1 2 1 cost the objective function for lifecycle cost of each portfolio lcc was calculated using eqs 2 4 the parameters for lcc bmp eq 3 were estimated from cost schedules developed by melbourne water australia 2013 table 2 a typical lifecycle period of 25 years a discount rate of 6 5 per year an establishment cost factor of 3 and an establishment period of 2 years were adopted the parameters for lcc swh eq 4 were estimated as follows a cost model for the total net present value npv of stormwater harvesting components was determined using linear regression r2 0 814 between levelized lifecycle cost ml and estimated annual volume supplied ml yr using detailed costing data for six stormwater harvesting projects derived by inamdar 2014 thus the lifecycle cost of stormwater harvesting components from eqn 4 was calculated using the following equation 8 l c c s w h i 1 n 104 49 s u p p l y i 6622 6 m l supply i m l 0 o t h e r w i s e if s u p p l y i 0 where supply i is the average annual supply capacity of the ith bmp in a candidate portfolio of n bmps 3 1 2 2 water quality improvement total nitrogen tn was the specific pollutant constituent adopted for the water quality objective tn load reduction was particularly important since in the urban catchment it was found by the consultants that maximizing tn reduction through treatment of stormwater also tended to reduce phosphorous total suspended solids and other pollutants to within target levels dr dale browne personal communication 2016 the introduction of excess anthropogenically generated nutrients into coastal systems can cause eutrophication which has negative impacts these impacts often include excessive and sometimes toxic production of algal biomass loss of important nearshore habitat changes in marine biodiversity and species distribution increased sedimentation of organic particles and depletion of dissolved oxygen the mean annual pollutant mass of tn retained by each candidate portfolio f q u a l i t y eqn 5 was calculated based on the sum of average annual tn mass retained by individual bmps in a portfolio the water quality improvement of individual bmps i e not an integrated system of a portfolio of bmps eqn 5 was assessed using the integrated stormwater model music version 6 1 model for urban stormwater improvement conceptualizion ewater 2009 as suggested by the relevant catchment management authority regulations music is an integrated stormwater model that evaluates rainfall runoff and pollutant generation and transport as well as the hydraulic and pollutant removal performance of bmps bach et al 2014 music algorithms simulate runoff based on models developed by chiew and mcmahon 1999 and urban pollutant load relationships based on analysis by duncan 1999 3 1 2 3 stormwater harvesting to determine stormwater harvesting capacity of projects experts on stormwater harvesting from each local government authority were asked to evaluate the stormwater harvesting potential of bmps within their jurisdiction they estimated the expected irrigation demand required by open spaces near each bmp and the average annual potential capacity to supply the demand the estimates were based on procedures specific to each local government authority and reflect the stormwater harvesting objective performance values accepted by decision makers 3 1 2 4 urban vegetation and amenity improvement the green score of individual projects which is a weighted score of several indicators that was developed by the authors and agreed to be used as an optimization objective by the consultants uses scores assigned by experts from each local government authority interviewed in a workshop session by the consultants the experts were asked to answer the following questions about the bmp projects within their jurisdiction answer yes no or maybe to the following questions 1 will native vegetation increase at the site 2 will tree cover increase at the site and 3 will the quality of recreation spaces in the area increase the total catchment green score objective function was 9 g r e e n i j 1 3 s c o r e j 10 s c o r e j 3 i f a n s w e r i s y e s 2 i f a n s w e r i s m a y b e 1 i f a n s w e r i s n o where green i is the sum of scores for each project and score j is the number of points assigned to the answer to the jth question since there were three questions each project could achieve a maximum of 9 green points and each portfolio a theoretical maximum of 20 9 180 green points 3 1 2 5 evaluation of individual bmps before the optimization process was run the costs and performance values of each bmp were determined table 1 firstly the stormwater harvesting capacity of individual projects was determined from local government authority expert interviews secondly the individual project lifecycle costs were determined using cost parameters from eqns 2 4 and 8 for each project thirdly the water quality performance of each bmp was determined with the aid of music to do this a stormwater model for a 1 ha catchment area for each local government authority was developed the model consisted of a 0 5 ha pervious catchment node a 0 5 ha impervious catchment node and an outlet node to estimate the average annual tn load per unit area of catchment with an average 50 impervious surface area browne et al 2012 one year of continuous climate data and pervious surface parameters provided by the catchment management authority were used for the catchment nodes to estimate source kg for each bmp the tn load from a 1 ha unit catchment area for the respective local government authority was multiplied by the contributing catchment area to each bmp in hectares each bmp was assumed to remove 45 of the tn load from its contributing catchment i e resid i 1 0 45 source i which was suggested as an acceptable performance based on advice from the consultants dr dale browne personal communication 2016 finally eqns 7 9 and 10 were applied to determine the individual project green scores 3 1 3 constraints a single constraint was applied to limit portfolios to 20 or fewer projects since more than 20 projects was determined to be impractical to design and construct by the catchment management authority as mentioned previously the projects were assumed to be independent in that the inclusion of one project did not influence the expected benefit cost or feasibility of another this assumption was considered acceptable since the catchments contributing to each bmp were mutually exclusive and customers for stormwater harvesting projects could receive supply from only one project 3 2 pareto ant colony optimization p aco algorithm to solve the optimization problem a variant of the original pareto ant colony optimization algorithm p aco doerner et al 2004 metaheuristic search algorithm was used p aco was selected because it was originally developed to solve portfolio optimization problems doerner et al 2004 2006 it has been used successfully and adopted as a benchmark algorithm in recent three objective portfolio optimization applications cruz et al 2014 and it has been applied to complex multi objective water resources problems nguyen et al 2016 szemis et al 2013 2014 the variant adopted here pacoa was demonstrated to outperform other multi objective ant colony optimization algorithms in a recent water resources allocation study szemis et al 2013 the algorithm mimics the cooperative foraging behavior of an ant species that leaves a chemical pheromone on a ground surface in real life since ants traverse short paths to food more frequently more pheromone is laid on short efficient paths thus paths with higher pheromone levels are more likely to be selected by an ant in the algorithm artificial ants select between paths which in this instance represent decisions whether or not to adopt a bmp in a portfolio an input template and executable for the algorithm are available as data set 3 in di matteo et al 2016 a summary of the steps in the paco algorithm is shown in fig 4 in the initialization phase the paco search control parameters are set the iterative process commences when b ants are generated each ant starting with an empty portfolio x 0 and the objective weights i e the ant s individual preferences are determined randomly for each ant in the construction phase of the algorithm first the order of bmps is randomly shuffled to ensure bmps are provided an equal chance of being considered first by each ant see golding et al 2017 then the ant decides whether to add each bmp to a portfolio x by applying a pseudo random proportional rule using pheromone information τ i the pheromone information is stored in one 2xn matrix for each jth objective representing the binary options for the n possible bmps if the ant adds the maximum number of bmps n max before all bmps have been considered then none of the remaining bmps are selected after a portfolio has been constructed its performance is evaluated using the objective functions eqns 2 and 5 7 in this case as individual projects were determined to be independent the portfolio objective functions were a summation of the constituent individual project objective function values in table 1 after each iteration of the b portfolios generated by the b ants the non dominated portfolios are stored offline in an array then as part of a global update of every element of the j pheromone matrices the first and second best performing solutions ranked for each jth objective are used to apply the following equation 11 τ t j 1 ρ τ t j ρ δ τ t j δ τ t j 15 t i n b o t h b e s t a n d 2 n d b e s t p o r t f o l i o 10 t i n b e s t p o r t f o l i o 5 t i n s e c o n d b e s t p o r t f o l i o 0 o t h e r w i s e where for each bmp the current pheromone value for each tth binary option and jth objective is reduced by pheromone evaporation ρ and increased by a pheromone value δ τ t j pheromone is evaporated from decisions that are not in the best solutions for each objective which makes it less likely these decisions will be selected again in future iterations in this way the ant s decision making landscape is modified to guide ants into regions of the search space that contain non dominated portfolios since the single constraint was handled in the construction phase no penalty function is required for this case study as all constructed portfolios are feasible the process of developing assessing and updating the pheromone trails to guide the pacoa to near optimal trade offs continues until a specified maximum number of iterations w is reached before the pacoa was applied a sensitivity analysis was conducted to identify suitable values of parameters that control the searching behavior of the algorithm to maximize the likelihood that the best possible approximation of the pareto front was generated the ranges of parameter values tested and the final parameters selected are given in table 3 in this study the paco was run for 1200 iterations of 500 ants which equates to 600 000 objective function evaluations this number of evaluations was selected because the progress of the pareto front ceased to be meaningful assessed by visually inspecting the pareto optimal solution set at 5000 evaluation intervals after this number of evaluations in a trial run of 2 000 000 evaluations the optimization results were replicated 50 times using different random starting seeds for the pseudo random number generator used in the algorithm to minimize the impact of probabilistic effects of some of the operators that influence the search each run took approximately 26 min on a 3 10 ghz computer with 8 gb of ram although multiple instances were run on one machine simultaneously the pareto optimal solutions shown in this paper are the result of a non dominated sort of the solutions from the 50 replicate runs 3 3 interactive visual analytics to explore pareto optimal solutions to visualize and analyze the objective and decision space trade offs of the pareto optimal set of portfolios an interactive visual analytics package was selected the combined objective space and decision space visualizations were carried out using the approach of kollat and reed 2007 using the discoverydv software package discoverydv version 0 72 available at https www decisionvis com discoverydv the package features an interactive data plot that allows brushing linked views of solutions marking and tracing of solutions of interest as well as rapid browsing through solution objective decision and non objective performance data the package has been used successfully in several recent many objective optimization studies piscopo et al 2015 woodruff et al 2013 the pareto optimal solution objective and decision data were uploaded into the interactive visual analytics package this allowed the analyst to 1 visualize and analyze trade offs between the four objectives 2 isolate portfolios from several regions of the trade off front using interactive brushing and visualization in multiple linked plots and 3 visualize the decision and objective space to analyze the impact and prevalence of particular projects on the performance of pareto optimal solutions the pareto optimal solution data file uploaded into the package is available as data set 3 and a ddv file for the discoverydv program containing the visualizations is included as data set 4 in di matteo et al 2016 4 results and discussion this section presents the results of the many objective optimization process for the stormwater management portfolio selection case study outlined in section 3 the results of the pacoa runs from 50 random starting positions show the algorithm identified 3654 pareto optimal or near pareto optimal portfolios as solutions to the optimization problem 4 1 identifying many objective trade offs between pareto optimal stormwater management portfolios fig 5 shows the trade offs between four objectives of the pareto optimal portfolios in a 4 dimensional coordinate plot a sharp trade off exists between tn reduction and cost and between reuse capacity and cost indicating small increments in cost can return large increases in both of these objectives in contrast green score tends to increase with cost which is expected as higher cost portfolios have more bmps distributed in the catchment to enable larger total catchment urban greening and amenity improvement the above inferences are supported and supplemented by the alternate representation of the trade off surface in parallel coordinates inselberg 1997 in fig 6 small slopes on some line segments between the adjacent axes of lifecycle cost and stormwater reuse indicate high reuse portfolios exist for low costs however these low cost high reuse capacity solutions appear to have lower tn reduction and green score compared to other solutions as mentioned above green score appears to be correlated with lifecycle cost however some solutions exist that have a high green score and relatively low cost in the low cost region from fig 5 clusters of solutions form in tn reduction reuse capacity space this indicates that individual projects dominate the contribution to total portfolio reuse capacity or total nitrogen reduction in this region analysis of the bmps comprising solutions in these clusters shows that these portfolios contain a small number of flagship projects with exceptionally large reuse capacity e g project 61 40 ml year project 67 12 8 ml year project 18 12 0 ml year or tn reduction e g project 48 1152 kg year project 18 1141 kg year portfolios containing only a few of these flagship projects are able to achieve relatively high total reuse capacity or tn capacity at relatively low cost but also a low green score this causes the noticeable discontinuity in the objective space in the low cost region characterized by clusters of solutions emanating from a small number of portfolios in the low cost region in fig 5 and overlapping dark blue low cost line segments joining parallel axes in fig 6 moving in the preferred objective direction adding a flagship project to create a new portfolio on the front can cause a large increase in tn reduction or reuse capacity therefore decision makers desiring low cost trade off solutions could consider portfolios of a small number of flagship projects but this would considerably compromise the urban greening and amenity performance of the stormwater management strategy 4 2 importance of a many objective problem formulation for bmp selection the cost and total nitrogen reduction trade off projections in fig 7 show trade offs between water quality and cost objectives which correspond to the most frequently used formulation in stormwater management optimization studies to date on the front a slight knee region appears such that when moving along the front away from the knee region there is a diminishing return in these objectives this suggests that solutions in this region may represent a desirable trade off between total nitrogen reduction and cost the trade off pattern is consistent with those in other bmp selection studies chichakly et al 2013 lee et al 2012 maringanti et al 2009 however only considering trade offs between water quality and cost objectives neglects the influence of other objectives that may be important to stormwater management decision makers moglia et al 2012 this could bias decision makers towards selection of solutions that would lie at extremities in objective space should other formal objectives be considered kollat et al 2011 the importance of the many objective representation of the bmp selection problem adopted in this study is demonstrated by tracing a solution from the two objective knee region in fig 7 through higher dimensional objective space represented in fig 8 for this purpose portfolio 1 table 4 is selected and marked for further analysis because it lies at an inflection point observed by visual inspection in the knee region of the two objective trade off front fig 7 using the visual analytics package an additional harvesting capacity axis and a green score colour axis are added to create a 4 dimensional plot of the objective space fig 8 to compare portfolio 1 with other solutions similar in cost the analytics package s data brushing tool is used to highlight solutions with lifecycle costs in the range 1 90 m 2 70 m in fig 8 these solutions of interest appear opaque and the remaining solutions that have been brushed out appear transparent portfolio 2 table 4 is selected for comparison because although it has a 22 greater lifecycle cost and similar tn reduction compared to portfolio 1 it has a vastly higher reuse capacity and green score therefore although portfolio 1 appeared in the region of best trade off knee region in the lower dimensional tn reduction cost representation of the objective space fig 7 it performed poorly in the reuse capacity and green score objectives portfolio 2 lies near but not on the non dominated water quality cost front in fig 7 thus it would not have been considered by decision makers in a bi objective pareto optimization approach which has been typical in bmp selection optimization studies to date when considering the project options selected in the two portfolios table 4 it is apparent portfolio 2 is similar to portfolio 1 except for one small project project 38 instead of project 33 and importantly two additional projects located in local government authority 1 projects 60 and 61 consequently decision makers may consider that portfolio 2 provides a better compromise between objectives compared with portfolio 1 due to the reuse capacity and green score benefit that the two additional projects provide importantly for stormwater managers the results of this study show that when they use water quality and cost as the only optimization objectives they may not identify solutions that represent good trade offs between water quality cost and other important objectives including stormwater harvesting capacity and green score the above results are consistent with findings in several other studies including 1 findings by kollat et al 2011 kasprzyk et al 2015 and woodruff et al 2013 that generally in optimization studies lower dimensional problem formulations may bias selection of solutions that would otherwise exist at low performing extremes if additional performance criteria were considered as formal optimization objectives 2 a finding by chichakly et al 2013 that for bmp selection optimization desirable solutions lie near but away from the two objective non dominated pareto front for water quality improvement and cost objectives and 3 trade offs for a stormwater harvesting system design determined by di matteo et al 2017 which showed slight increases in system costs could provide large increases in both water quality improvement and harvesting capacity 4 3 identifying impacts of project options on pareto optimal portfolio performance fig 9 shows combined objective performance and decision characteristics of the pareto optimal portfolios which helps the analyst to overcome biases arising from artificial distinctions between objective performance and other characteristics of the problem matrosov et al 2015 for example the visual interactive plot allows the analyst to inspect which area of the trade off front each project features in pareto approximate portfolios in this way an analyst can infer the impact of particular projects on portfolio performance in fig 9 a the opaque spheres represent portfolios containing project 61 lifecycle cost 381 297 tn reduction 157 92 kg year reuse capacity 40 ml year green score 6 which was the project with the highest reuse capacity importantly all portfolios with 40 ml year or greater reuse capacity include project 61 and these portfolios occur in nearly the full range of cost tn reduction and green score of pareto solutions therefore this indicates decision makers should probably consider project 61 in their final portfolio in fig 9 b the opaque spheres represent portfolios containing project 48 lifecycle cost 915 472 tn reduction 1152 kg year reuse capacity 0 ml year green score 7 which was the project with the highest tn reduction importantly in the lower cost region pareto optimal portfolios with a number of smaller solutions dominated inferior portfolios containing project 48 this was because although the green score of project 48 was high 7 out of 9 the cumulative green score and or reuse capacity of low cost portfolios with more projects dominated portfolios containing a small number of larger projects including project 48 this indicates multiple additional benefits can be achieved for a similar cost by using a portfolio of projects rather than one flagship project in addition decision makers can view and assess additional non objective characteristics that may influence decision making for example the percentage of the catchment treated spatial distribution of projects throughout the catchment or socio political preferences for particular projects 5 summary and conclusion a general multi objective optimization formulation was developed for the selection of a portfolio of bmps for stormwater management this study advances the bmp selection optimization field of research chen et al 2015 chichakly et al 2013 di matteo et al 2017 lee et al 2012 maringanti et al 2009 zare et al 2012 as it addresses the need for a many objective optimization formulation for the selection of stormwater bmps that a can cater to a large number of performance criteria b can handle a large number of decision options and potential strategies c can identify solutions that represent the best possible trade offs between performance criteria d enables trade off information to be communicated in an easy to understand fashion and e enables the development of solutions that are trusted by decision makers the approach was applied to a case study catchment plan for a catchment authority in a major coastal city in australia the benefits of the proposed formulation specifically with regard to stakeholder acceptance of solutions were evident for the case study by using the proposed formulation the only options considered were solutions that stakeholders had proposed and were familiar with consequently the final set of solutions obtained after the optimization process consisted of combinations of trusted solutions rather than solutions that were obtained via processes and models end users were not familiar with stakeholders would have been able to identify portfolios that contain particular projects they may have a preference for or bias towards selecting and compare these with other optimal portfolios in this way when analyzing optimal solutions stakeholders could identify other projects that when packaged with preferred projects maximize the total portfolio benefits or experience a shift in preferences for particular projects by visualizing the prevalence of other projects within optimal solutions the results demonstrate the benefits of exploring full portfolio solution trade offs in a many dimensional pareto optimal front which can reduce the prevalence of decision making biases due to the optimization formulation a comparison between the trade off spaces of the lower dimensional water quality cost problem formulation and the many objective formulation demonstrated that low objective formulations can result in pareto optimal portfolios with low performance in non objective performance criteria in this study when stormwater harvesting and vegetation and amenity improvement scores were included as objective functions solutions that were in a region of best trade off in water quality cost space performed poorly in these additional objectives the many objective optimization results show that sharp trade offs exist between tn reduction and cost and between reuse capacity and cost indicating small increments in cost can return large increases in both of these objectives portfolios in the low cost regions typically featured a small number of projects including cost efficient flagship projects that provide high tn reduction or reuse capacity however in order to maximize the vegetation improvement and amenity benefits portfolios with a larger number of lower cost bmps distributed throughout the catchment were preferred notably the optimization formulation in the case study does not consider that interaction between having a higher harvest capacity might allow for more irrigation of green spaces using the visual analytics approach to explore combined optimization and decision spaces the impact of individual projects that may be preferred by decision makers was rapidly visualized this approach could assist in overcoming institutionally influenced biases to include particular projects or bmp technologies to demonstrate alternative similar cost options to decision makers future studies applying the framework could account for differences in preferences between multiple decision makers that may be responsible for funding over different periods of the project lifecycle for example in some funding schemes catchment management authorities fund the capital expenses whereas local government authorities fund the maintenance and ongoing expenses the many objective problem formulation could be adapted to include specific objectives important to local government authorities which might include the operating expenses for each individual local government authority in addition to total catchment benefits thus identifying the tradeoffs between capex and opex in addition the pareto optimal solutions could be explored taking into account individual formal objective and informal non objective preferences of multiple decision makers in this way decision makers can visualize their preferences on a trade off curve and compare and through an iterative approach visualize and negotiate acceptable outcomes and solutions this may be preferable to other approaches where weightings are set a priori which do not account for decision maker preferences in the decision space nor allow a visual comparison of the regions of interest preferred by several decision makers for the green index and stormwater harvesting objectives the objective formulations do not consider how diminishing returns due to interdependencies between projects in these objectives may be represented finally the constraint for the number of projects should consider the difficulty of constructing individual bmp types e g 20 swales might be easier to construct than 20 wetlands data and software availability name of software pareto ant colony optimization algorithm and problem data set developers michael di matteo holger r maier graeme c dandy year first available 2017 hardware required pc mac program language fortran program size 10 0 mb contact address school of civil environmental and mining engineering university of adelaide adelaide south australia telephone 61 8 8303 4313 12 fax 61 8 8313 4359 13 e mail michael dimatteo adelaide edu au url https figshare com articles a many objective optimization and visual analytics approach to project selection for integrated catchment management 4233119 data set software required the ddv files require discoveydv visualization software discoverydv is available for academic use under a beta license as of april 2017 at https www decisionvis com discoverydv some data files are as csv or xlsx which can be opened using microsoft excel other spreadsheet packages matlab scripts and data files which were used to convert the raw optimization run data into a useful form are available and require matlab software package availability software and data are available via figshare public and online repository and submitted to the journal editor for publication cost free for non commercial use acknowledgements research funding was provided by the australian postgraduate award the university of adelaide and the goyder institute for water research the authors thank dr dale browne and e2designlab australia for assistance with the case study data and the anonymous reviewers for their comments which helped improve the quality of the paper the data used are listed in the references tables supplements and repository at https doi org 10 6084 m9 figshare 4233119 appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 008 
26297,although formal simulation optimization approaches have been shown to be able to identify near optimal outcomes for a range of stormwater management problems stakeholder acceptance of these solutions can be problematic especially if there is a lack of familiarity with the optimization processes and simulation model used to arrive at these solutions to address this problem a portfolio optimization problem formulation is introduced that allows stormwater best management practices bmps to be evaluated by stakeholders before the portfolio selection process this enables the search space to be constrained before the bmp optimization process ensuring that model results are transparent and only represent solutions that are trusted by experienced practitioners this has the effect of reducing reliance on simulation optimization involving complex stormwater simulation models and increasing buy in to the optimization results the portfolio optimization formulation is applied to a catchment management problem in australia using a typical many objective optimization approach including visualization techniques keywords stormwater management water sensitive urban design portfolio optimization many objective optimization visual analytics multi criteria decision analysis 1 introduction recently many objective evolutionary algorithms have been developed and applied to identify pareto optimal solutions to water resources planning problems kollat and reed 2007 matrosov et al 2015 these approaches have used visual analytics techniques to aid exploration and analysis of the typically large numbers e g 1000s of pareto optimal solutions identified and to select several suitable schemes to present to decision makers the many objective optimization visual analytics approaches have enabled trade offs between four 4 or more planning objectives to be considered which better reflects the number of planning objectives considered by practitioners in real world water resources planning problems i e better than typical single or bi objective optimization problem formulations as discussed in recent many objective optimization studies kasprzyk et al 2012 2015 matrosov et al 2015 woodruff 2016 optimizing planning solutions for a sub problem of what is in fact a many objective problem can lead to cognitive myopia which is a negative decision making bias that arises due to drawing incorrect inferences and conclusions from limited problem information in this light consideration of a limited number of formal objectives in optimization studies can encourage the identification of solutions with sub optimal performance with respect to criteria that are not included as formal objectives but are important to contemporary water resources managers woodruff et al 2013 it is therefore preferable to optimize with respect to all relevant formal objectives where possible in the stormwater management optimization field recent stormwater best management practice bmp optimization approaches have typically included an integrated stormwater simulation model bach et al 2014 linked with an evolutionary algorithm maier et al 2014 for the optimal sizing and placement of bmps di matteo et al 2017 within a watershed to achieve environmental benefits from treating stormwater runoff however for regional scale stormwater management problems formal objectives have been limited to two including ecosystem health benefits including water quality improvement and cost chen et al 2015 chichakly et al 2013 lee et al 2012 zou et al 2015 this is despite the fact that in many cases stormwater management optimization can be better represented as a many objective optimization problem considering a larger number of objectives this is because stormwater managers must consider a range of performance criteria due to a number of socio political drivers including water supply security public health protection social amenity urban flow regime improvement environmental protection and flood mitigation askarizadeh et al 2015 marlow et al 2013 in response to these drivers bmps have been developed to provide multiple functions in addition to water quality improvement for example stormwater harvesting clark et al 2015 di matteo et al 2017 mitchell et al 2007 and urban vegetation and amenity improvement sharma et al 2016 such bmps may include structural and non structural measures for detention harvesting infiltration evaporation and transport of non point source urban stormwater runoff lerer et al 2015 conventional formulations of bmp selection problems are likely to contribute to a lack of acceptance of solutions obtained from optimization studies this is because the practical relevance of the optimization solutions depends largely on how decision makers feel about the credibility of the evaluation techniques and data used in the decision making process aumann 2011 for example stormwater management strategies developed by algorithms may not be trusted and adopted by decision makers who are unfamiliar with the optimization process and how the strategies are generated maier et al 2014 in addition stormwater simulation optimization approaches maringanti et al 2009 srivastava et al 2002 may not complement current practice for management of large regional catchments which typically involves ad hoc selection and implementation of bmps as funding becomes available in order to develop trusted stormwater management strategies that are likely to be adopted in practice decision maker engagement should be encouraged in all aspects of optimization studies applied to water resources problems maier et al 2014 voinov and bousquet 2010 wu et al 2016 therefore the problem formulation and system models used should incorporate existing modelling practice and practitioners should aim to use optimization as a complementary tool to existing approaches where possible such an approach is likely to encourage the uptake of formal many objective optimization approaches by decision makers as it seeks to provide advice on the best course of action under the institutional and political constraints that exist in the real world this would improve upon current practices in order to address the shortcomings of existing optimization problem formulations discussed above the objectives of this paper are i to present a novel optimization problem formulation for selecting combinations of stormwater bmps that a can cater to a large number of performance criteria b can handle a large number of decision options and potential strategies c can consider detailed interactions between interdependent parts of the systems d can enable the identification of solutions that represent the best possible trade offs between performance criteria e enables trade off information to be communicated in an easy to understand fashion and f enables the development of solutions that are trusted by decision makers ii to demonstrate the utility of the optimization problem formulation by applying it as part of a generic optimization framework to a case study focused on the selection of stormwater bmps for a major city in australia the generic optimization framework includes the novel portfolio optimization problem formulation see objective i a many objective optimization technique to identify solutions to the problem and a visual analytics package to explore analyze and select portfolios of bmps and iii to use the case study to a investigate the possible many objective trade offs between lifecycle cost water quality improvement stormwater harvesting capacity and urban vegetation and amenity improvement b investigate the importance of a many objective approach compared with a bi objective water quality cost optimization as has been undertaken in most previous studies and c demonstrate trends in the impact of particular bmp projects on pareto optimal portfolio performance and how these may influence decision making 2 proposed stormwater bmp selection optimization formulation 2 1 outline of proposed bmp selection approach a conceptual outline of the proposed stormwater bmp selection optimization formulation is shown as items 1 and 2 within a generic many objective optimization approach kollat and reed 2007 fig 1 the portfolio optimization formulation presented allows a framework to satisfy desirable criteria for stormwater bmp selection methods which is not the case with existing approaches as outlined in the introduction as follows i the ability to develop solutions that are trusted by and have buy in from decision makers is accounted for by formulating the problem as a portfolio optimization problem as part of which only stormwater bmps that are suggested by decision makers are considered as potential options and decision maker driven evaluation of bmps is used ii the ability to cater to a large number of performance criteria and options as well as the ability to identify solutions that represent the best trade offs between the performance criteria is facilitated because the portfolio optimization formulation allows for the use of look up tables for the evaluation of solution performance as such the optimization process allows detailed interactions between interdependent parts of a system to be considered without the need for a complex stormwater model that is linked with an optimization algorithm the formulation handles the evaluation of performance of 1 independently functioning bmps by calling on performance data stored in a look up table generated a priori i e before the optimization process by experienced practitioners using appropriate techniques to evaluate the performance of each individual bmp 2 interdependently functioning bmps by calling on performance data stored in a look up table that are results of simulations of smaller and localized bmp systems conducted a priori in the first step of the overall optimization framework fig 1 a list of potential stormwater management bmps p is identified these bmps are then evaluated individually by practitioners and the interdependencies between them determined all possible combinations of these individual projects make up the full portfolio solution space which is expected to be too large to adequately evaluate by trial and error or enumeration therefore in order to enable consideration of many performance criteria f and a wide exploration of the potential portfolios p a formal optimization approach is adopted the best combinations of bmps are represented as pareto optimal solutions p to a many objective portfolio optimization problem formulation cruz et al 2014 in order to analyze the large number of pareto optimal solutions produced by the optimization process and to present the optimal trade offs to decision makers in a manner that is easy to understand interactive visual analytics are used to explore trade offs and impacts of bmps on portfolio performance the proposed formulation is in alignment with approaches using many objective portfolio optimization problem formulations with decision maker driven evaluation of objective function values as pointed out by maier et al 2014 such a portfolio optimization formulation is likely to make many objective optimization accessible to decision makers whose current level of decision making sophistication includes multi criteria decision analysis this is because the options under consideration as well as the final selection of the portfolio to be implemented are based on the domain knowledge of individual practitioners in contrast there is likely to be less decision maker buy in and trust when simulation optimization approaches are used to determine optimal solutions as interactions between complex systems of bmps and therefore the rationale behind the performance values of portfolios are not transparent to decision makers who may not use complex simulation models to support decision making having said this it must be noted that in practice there is a trade off between obtaining mathematically optimal solutions which may be better approximated using a simulation optimization approach versus obtaining solutions that encourage decision maker buy in and will therefore more likely influence the final stormwater management strategy adopted which may be better achieved by having decision maker driven evaluation of portfolios the proposed approach may not identify the mathematically optimal solutions since the sizes of bmps are not considered as decision variables and the evaluation of interactions between bmps is performed a priori and or informed by decision makers however as the approach balances competing desires to produce mathematically optimal solutions and solutions that are trusted by decision makers it makes the benefits of optimization more accessible to practitioners and should encourage better stormwater management strategies to be adopted in practice the detailed steps for implementing the conceptual approach presented in fig 1 including the proposed problem formulation are given in fig 2 which are explained in the following sections 2 2 problem formulation the first part of the optimization framework consists of steps required to formulate a portfolio optimization problem that represents the stormwater management problem to achieve multiple catchment benefits numerous stormwater best management practices bmps are typically considered to intercept and deal with runoff at locations distributed throughout a catchment examples of bmps may include biofiltration systems biofilters which typically consist of a basin overlaying a filter medium constructed wetlands which are shallow extensively vegetated basins that use enhanced sedimentation fine filtration and pollutant uptake processes to remove runoff pollutants and swales which are vegetated channels appropriate types and locations of bmps largely depend on site characteristics including soil type and properties topography infiltration rate contributing connected impervious area and the space available to access for maintenance site characteristics are typically assessed through on site and geospatial studies inamdar 2014 after site assessment a short list of feasible bmps is agreed upon amongst decision makers taking into account the potential to achieve desired performance criteria and other socio political factors chichakly et al 2013 sharma et al 2016 the performance of each bmp is then evaluated independently against multiple criteria using accepted models based on the contributing sub watershed for each bmp and in consultation with experienced local experts inamdar 2014 in the absence of an adequate regional scale integrated model to evaluate the downstream impact of bmps interactions between bmps that influence individual bmp performance are evaluated based on expert judgment and modelling of bmps and multiple contributing sub watersheds to determine decision making rules or performance models for interdependent projects for examples of formulating interactions in portfolio optimization see section 2 description and formalization of the problem in cruz et al 2014 the individual projects their performance interdependencies and practical limitations on portfolio size are then formulated as the decision variables objectives and constraints of a mathematical optimization problem 2 2 1 a priori evaluation of independent and interdependent bmps as part of the proposed formulation the performance of independent and interdependent bmps can be evaluated a priori to eliminate the need for a complex stormwater model to be called during the optimization process in the case where a small number of bmps are interdependent an a priori analysis of interactions between those particular bmps can be used to estimate their objective function values when adopted together in different combination when one or more interdependent bmps appear together in a potential solution alternate objective function values can be used i e by the algorithm by referring to a look up table of simulation results to reflect the interdependency compared to where they appear individually to illustrate how dependency between bmps can be handled in the optimization formulation consider the regional stormwater bmp system in fig 3 where bmps treat stormwater runoff from upstream catchments as described below the individual bmps to the left can be considered to operate independently from all others this is because 1 inflows from these bmps cannot consist of treated outflows from another bmp and 2 outflows from these bmps cannot be inflows into another bmp as there are no bmps located at the downstream end of the 1st order stream this means where bmps are assumed not to be implemented e g as an outcome of the optimization process the performance of an independent bmp if it is present in the system will be independent of any other system configuration as it will have the same inflow characteristics therefore the performance of each independent bmp can be simulated using a model with one node representing the bmp a priori of the optimization process rather than needing a complex model consisting of all bmps to be called during the optimization process as is typical of previous studies the bmps to the right are interdependent since removing a and or b will affect inflow characteristics and therefore the performance of c the performance of the local sub system can be simulated using a stormwater model with three nodes and two drainage links which is still simpler than a model of the whole system solutions to the sub system of bmps can be enumerated a priori the enumeration may include several sizes for each bmp and several combinations of bmps the objective function values of pareto optimal configurations of the sub system can be kept in a look up table to be called upon by the optimization process to efficiently calculate the whole portfolio objective function values the appropriate sizes for the configuration selected by the optimization algorithm can be made available for further analysis this removes the requirement for a large and complex stormwater model to be called to simulate every solution again as is typical of previous studies importantly this approach is effective where there is a large number of bmps in the regional system and the sub systems consist of small numbers of bmps e g 2 to 5 such that the computational time to enumerate the sub system using a simple model is short enough to be appropriate for the decision making task all sub systems within a regional system can be enumerated separately using simple models to provide performance data for the look up table 2 2 2 limiting the solution search space in the proposed formulation to further reduce the solution search space and complexity of analysis of solutions additional considerations including the size type and location of bmps can be handled without them being decision variables as follows the size of bmps is often largely dependent on the water quality improvement required by regulators where there are regulatory water quality improvement targets to minimize costs the size of bmps is set such that these targets are just met for example where total nitrogen tn is a limiting pollutant and the target reduction of tn from increased runoff from a development is 45 removal the size of a bmp is increased until it just achieves 45 reduction for an individual bmp this is a simple optimization that can be undertaken a priori for small sub systems the target may apply as a constraint for the system rather than individual bmps when considering the pareto optimal configurations to include in the look up table the type of bmp e g biofilter wetland swale is often highly dependent on the site characteristics design objectives and preferences of planners therefore often a small number of bmp types is available for each particular location the proposed approach can handle more than one bmp type at a location where two or more bmp projects exist in the same location this would require a simple mutual exclusivity constraint on the decision variables for these projects e g a constraint could be if project a at location 1 is selected for the portfolio then project b at location 1 cannot be selected in the proposed approach the project decision variables act as a surrogate for the location of bmps for example project c may exist at location 2 in a catchment so by selecting project c in a portfolio this means location 2 is also selected impacts on bmp performance specific to a location can be reflected in the costs and benefits associated with the project at the location and in terms of the objective functions representing interactions with other projects 2 2 3 encouraging stakeholder buy in the proposed formulation assists with building trust in the solutions generated by the optimization process by only considering solutions proposed by end users this aspect of the optimization approach aligns with existing multi criteria analysis mca approaches currently used in practice without introducing additional complexity through typical optimization approaches in practice typically individual projects are identified by consultants and stakeholders through a consultation process then a guided scoring of individual projects is carried out typically the stakeholders provide weighted scores reflecting their preferences for different criteria and a final short list of solutions is selected based on these preferences one of the benefits of this approach is that the scoring is transparent to stakeholders and therefore stakeholders are likely to buy into the results of the mca however often stakeholder preferences change once a diverse set of solutions and trade offs between the set of solutions objectives are visualized traditional optimization problems can provide diverse solution sets and trade offs but the solutions are typically assessed using a simulation model the results of which may not be trusted by all stakeholders especially where stakeholders have limited familiarity with modelling techniques and where systems are complex and their performance is not easily traceable to design decision changes in addition the proposed solutions are likely to be unfamiliar to stakeholders making them more difficult to be trusted however with the proposed formulation the scores attributed to individual bmps by stakeholders can be used as objective function values for a portfolio optimization analysis therefore the scores attributed by stakeholders are reflected in the diverse trade offs across a range of preferences identified in the optimization process that can be explored and analyzed and reveal optimal combinations of projects not possible with mca techniques with the direct link between the scores attributed by stakeholders and the optimization results there is arguably a better chance that optimization results will be used to support decision making especially where decision makers have not been trained extensively in decision support approaches or bmp systems modelling with the proposed formulation a variety of expertise can be accommodated whilst maintaining a diverse set of solutions to select from it is acknowledged there are further issues related to trust at other stages of the decision making process that have not been considered here and require further work however being able to use the results generated through existing analysis techniques improves the likelihood that optimization will be adopted for bmp selection problems in practice especially since the individual components of solutions identified by the optimization process were proposed by and are therefore familiar to stakeholders 2 2 4 portfolio optimization problem to identify portfolios that represent the best trade off between many objectives the project portfolio selection problem is defined as the optimization of vector f p composed of n objective functions 1 f p f 1 f 2 f n where p is a portfolio of projects and f is a vector of the associated costs and benefits of a portfolio for a more in depth description of a generic portfolio optimization formulation the reader is referred to cruz et al 2014 the generic decision variables objectives and constraints particular to the stormwater management portfolio selection problem are as follows 2 2 4 1 decision variables it is assumed that each bmp project has a pre determined size type and location as such each decision variable is a binary variable d i that represents the decision whether or not to adopt project p i there are np possible projects and thus np decision variables given by d d 1 d 2 d np where d i 0 1 for all i n 0 1 i np a portfolio p is defined as the set of projects p i for all i where d i 1 2 2 4 2 objectives although objectives depend on decision maker interests four formal objectives addressing one or more economic social or environmental stormwater management goals are included in the proposed generic formulation economic cost water quality improvement stormwater harvesting capacity and combined urban vegetation and amenity improvement the objectives for lifecycle cost water quality improvement and stormwater harvesting are adapted from general objectives presented in di matteo et al 2017 a generic optimization approach for distributed stormwater harvesting systems the green score objective formulation is provided as an example of adapting multi criteria analysis results here scores for multiple sub criteria including tree cover amenity within a single overarching criterion here urban amenity for use as a formal optimization objective economic cost is a primary concern for decision makers responsible for maximizing return on investment including capital maintenance and operating costs water quality improvement is a key environmental objective considered by stormwater management authorities chichakly et al 2013 yang and best 2015 maximizing stormwater harvesting swh volume is a primary motivation for implementing projects with swh capacity in order to reliably meet irrigation demand and can also contribute to runoff volume reduction and groundwater recharge known to produce ecosystem health benefits askarizadeh et al 2015 an amenity improvement score is proposed as the social criterion as bmps are typically located in public open spaces and are maintained using public resources and urban vegetation and amenity improvement is often an important criterion for evaluating bmps in the proposed formulation the economic cost of a portfolio of projects is represented as a life cycle cost lcc eq 2 di matteo et al 2017 which is a discounted sum of expected future costs for stormwater management assets including bmps and transfer infrastructure required to harvest stormwater taylor and wong 2002 the life cycle cost objective function for each candidate portfolio of bmps is given by 2 minimize f c o s t l c c b m p l c c s w h where 3 l c c b m p i 1 n t a c b m p i p w f e s t a b b m p i s a b m p i e c f b m p i m b m p i p w f m a i n t b m p i s a b m p i m b m p i 4 l c c s w h c c a p t a n k c c a p p i p e c c a p c o n t r o l c c a p p u m p p w f m a i n t c m t a n k c m p i p e c m c o n t r o l c m p u m p where the sum of the cost of bmps to capture and treat stormwater runoff lcc bmp eq 3 and to transfer harvested water to a balancing storage for further treatment and distribution lcc swh eq 4 is applied with bmp i representing the ith bmp in the candidate portfolio n integer is the number of projects in the portfolio tac is the total acquisition cost as a function of sa the surface area of bmp i and n integer is the number of projects in a portfolio pfw estab fraction for the establishment period and pwf maint for the remaining design life of system components are the present worth factor for a series of annual costs computed using a discount rate ecf fraction is the establishment cost factor i e multiplier for the annual maintenance cost m during the establishment period typically 1 2 years for each bmp for bmps with a stormwater harvesting function c captank c cappipe c capcontrol and c cappump are the capital costs for required storage tank control systems pipes and pump stations and c mtank c mpipe c mcontrol and c mpump are the annual maintenance costs for the tank pipes control systems and pumps and operating costs respectively the water quality improvement indicator adopted in the proposed framework is the total average annual pollutant load reduction of one target pollutant eq 5 only one target pollutant is adopted to limit the number of objectives and therefore limit the difficulty in identifying optimal solutions however if the trade offs between multiple water quality indicators need to be known then these can be added as objectives this indicator is widely adopted to assess the performance of wsud approaches including swh systems browne et al 2012 the target pollutant s will depend on decision maker interests the water quality improvement objective function is 5 maximize f q u a l i t y i 1 n s o u r c e i r e s i d i where f q u a l i t y mass year 1 is the mean annual pollutant mass retained by bmps in each candidate portfolio n is the number of bmps in a portfolio resid i mass year 1 is the mean annual mass of pollutant leaving the ith bmp s contributing catchment area and source mass year 1 is the mean annual mass of pollutant that reaches the ith bmp s catchment outlet in a post development catchment baseline scenario without intervention resid and source should be determined using a stormwater quality assessment model accepted by the stormwater management authority bach et al 2014 coombes et al 2002 average annual supply capacity eq 6 is adopted as an indicator of stormwater harvesting performance mitchell et al 2008 this metric is proposed because it can be determined from generic storage yield reliability curves for a catchment at the project screening phase of stormwater management browne et al 2012 hanson and vogel 2014 or other techniques inamdar 2014 in addition the average annual capacity approximates runoff volume reduction due to harvesting which has ecosystem health benefits askarizadeh et al 2015 the supply stormwater harvesting objective function is 6 maximize f s u p p l y i 1 n s u p p l y i where supply i volume is the average annual stormwater harvesting supply capacity for the ith bmp in a portfolio the urban vegetation and amenity improvement indicator depends on decision maker interests which may include maximizing vegetation and tree coverage and quality of recreation spaces each project should be appraised and evaluated scored by vegetation experts the cumulative urban vegetation improvement objective function is 7 maximize f g r e e n i 1 n g r e e n i where green i integer is a score determined by expert assessment attributed to the ith project in a portfolio 2 2 4 3 constraints strategic and logical constraints on the selection of projects and performance of portfolios could be considered and are case specific cruz et al 2014 for example where multiple sub region catchment institutions fund an integrated catchment strategy constraints on the selection of projects could 1 ensure equitable distribution of projects amongst constituent stormwater management sub regions 2 limit the maximum number of projects in a portfolio nmax and projects within each sub region 3 prevent the presence of mutually exclusive projects as some bmps may be redundant in the same portfolio and 4 limit the budget allocated to projects within each sub region additional considerations for portfolio based constraints are discussed in cruz et al 2014 2 3 optimization process the second part of the optimization framework fig 2 describes the algorithmic processes used to solve the optimization problem only portfolios that are non dominated i e none of the objective functions can be improved in value without degrading one or more of the other objective function values can be considered as portfolios that represent the best trade off between objectives to identify the non dominated or pareto optimal solutions to the mathematical optimization formulation use of a many objective metaheuristic algorithm is suggested metaheuristic algorithms have several advantages over traditional optimization approaches such as linear programming they can deal with multiple objectives simultaneously maier et al 2014 and have been successful in recent planning and design optimization studies considering urban water beh et al 2014 2017 blinco et al 2017 marchi et al 2016 newman et al 2014 paton et al 2014 wu et al 2017 and distributed bmp systems chichakly et al 2013 di matteo et al 2017 as part of the generic optimization process a number of solutions is generated with the aid of a many objective metaheuristic algorithm each solution represents a set of binary decisions on whether or not to adopt each available project in a portfolio in the construction of a solution projects are added to a portfolio until a constraint on the maximum number of projects is reached or all projects have been considered i e a portfolio can consist of fewer than the maximum number of projects then portfolios are evaluated against logical and strategic conditions for example mutual exclusivity of projects if a portfolio violates these conditions the objective function values are set to a penalty value the penalty value will depend on the optimization problem considered next the performance of valid portfolios is evaluated by calculating objective functions including interactions see section 2 2 1 after evaluation final penalties are applied to objective function values of solutions that fail to meet defined constraints the metaheuristic algorithm uses objective function values to assess the fitness of solutions and to iteratively modify solutions over a number of iterations solutions converge towards the set of pareto optimal portfolios which are non dominated in the set of all feasible portfolios the metaheuristic iterative approach continues until specific termination criteria are met for example a maximum number of iterations the non dominated solutions identified by the optimization process are pareto optimal or near optimal as one can never prove that the true pareto front has been found when using evolutionary algorithms stormwater management portfolios 2 4 visual analysis of pareto optimal portfolios an interactive visual analytics package hadka et al 2015 kollat and reed 2007 is suggested to assist decision makers to explore analyze and ultimately select appropriate portfolios that represent a desired compromise between performance criteria and practical stormwater management strategies maier et al 2014 firstly the pareto optimal portfolio performance and decision data as well as alternative data that may be useful for decision making e g average contributing catchment size bmp type number of projects are uploaded into the visual analytics package next high dimensional coordinate plots or parallel coordinate plots inselberg 2009 are used to visualize the performance of the large number of pareto optimal portfolios in many objective space then in order to reduce the number of portfolios considered for further analysis dynamic filtering to eliminate undesirable solutions can be carried out by analysts based on the decision maker s budget constraints and minimum preferences for each benefit and eliminate apparently undesirable combinations of bmps not anticipated a priori piscopo et al 2015 within the reduced set decision makers and analysts can use brushing to highlight sub sets of interesting solutions multiple linked plots of the same data set can assist with identifying and rationalizing trade offs such as conflicts and areas of diminishing returns between objectives and emergent behavior caused by the inclusion of particular bmps within portfolios interactive visualization of optimization objectives and decision spaces simultaneously enables decision makers with the assistance of analysts to rapidly identify subsets of portfolios that contain preferred projects and compare their performance to that of other portfolios in this way browsing through solutions to investigate and learn about the impact of individual project preferences on total catchment benefits can allow decision makers to overcome institutional decision making biases kollat and reed 2007 matrosov et al 2015 ultimately several desirable portfolios are selected for further consideration 3 case study in this study we demonstrate the many objective bmp selection approach on a regional stormwater management strategy for a major coastal city in australia a catchment management authority commissioned engineering consultants to identify sites for stormwater bmps within an integrated catchment with an outlet flowing into a prominent marine body the integrated catchment covers an area of approximately 700 km2 with average annual rainfall of 400 700 mm and is comprised of highly urbanized and peri urban regions managed by three local government authorities a primary objective for the catchment management authority was to reduce the nutrient load from urban stormwater runoff flowing into the marine body in addition since the potential sites for bmps were within public open spaces managed by local government authorities stormwater harvesting for irrigation of open spaces and increasing vegetation and public amenity value were considered important additional benefits the consultants identified 70 np 70 potential biofiltration wetland and swale projects at locations distributed in open spaces throughout the three local government authority regions through stakeholder consultation thirteen of these have a capacity for stormwater harvesting in addition the consultants suggested that a portfolio of 20 projects or fewer nmax 20 was practical the bmps were considered to be mutually independent for the purposes of demonstrating the optimization approach as the contributing catchment areas to each bmp did not coincide i e downstream impact of bmps would not affect the performance of other bmps within the large regional catchment and as such interdependencies between bmps were not considered in the case study application the number of possible portfolios was k 1 20 70 70 k k 2 59898 10 17 which is too large to fully enumerate using a typical desktop computer the application of the proposed optimization approach was part of a real world study involving a multi criteria analysis conducted to identify a portfolio of bmp projects for a regional catchment this allowed the authors to demonstrate how the proposed approach can consider existing bmp selection practices which is a study objective as the case study application was only intended to demonstrate the optimization approach the results of the study were reviewed by consultants but were not used to inform decision making engagement between decision makers engineering consultants and the optimization analysts who are the authors of this study was carried out as follows firstly the engineering consultants ran one workshop where the broad stormwater management objectives were established which was attended by a working group of 16 decision makers from local government authorities and the catchment management authority the consultants then identified sites assessed them for quantitative metrics e g required size of bmps to meet water quality constraints cost and stormwater harvesting capacity and carried out a preliminary scoring each of the qualitative metrics e g vegetation improvement and amenity value using objective thresholds consultants then sent these preliminary scores to local government authorities who were asked to provide a response these were generally reviewed by landscape bushland horticultural and parks and open space staff the staff involved and level of response varied between the local government authorities consultants then had a workshop with each of the individual local government authorities to review the sites establish a common understanding of the whole catchment management opportunity and confirm the proposed individual project scoring then important objectives were refined into formal optimization objectives by the consultants and optimization analysts the analysts used the multi criteria evaluation data to inform the optimization problem formulation including decision variables projects and to develop objective functions objective function values and constraints the names of the decision makers and catchment regions involved are not disclosed in this study the data used for this study are listed in the references tables supplements and repository at di matteo et al 2016 based on information provided by regulators a single climate scenario was considered however it should be noted that alternative climate scenarios could have been considered for example the pareto optimal solutions could be each assessed against a dry climate scenario and solutions dominated under that scenario removed from further consideration as in chichakly et al 2013 it should also be noted the formulation does not preclude a robustness evaluation maier et al 2016 mcphail et al 2018 riddell et al 2018 however this would require the analyst to evaluate the independent bmps and sub systems of bmps under different climate scenarios and to use multiple look up tables to store the a priori evaluation results the portfolio optimization problem formulation developed for the case study the optimization process used to solve the problem and the visual analytic approach used to analyze explore and select from optimal bmp portfolios are presented in the following sections 3 1 problem formulation 3 1 1 decision variables the 70 potential bmps table 1 were formulated as 70 decision variables with two corresponding decision options to adopt or not adopt a bmp in a portfolio following a preliminary desktop analysis bmps were determined by decision makers to have contributing catchments ranging in size from 3 ha to 421 2 ha with an assumed 50 pervious and 50 impervious area the functional areas of bmps were pre determined by consultants and sized to meet functional requirements for total nitrogen total phosphorous and total suspended solids runoff pollutant reduction targets dr dale browne personal communication 2016 3 1 2 objectives 3 1 2 1 cost the objective function for lifecycle cost of each portfolio lcc was calculated using eqs 2 4 the parameters for lcc bmp eq 3 were estimated from cost schedules developed by melbourne water australia 2013 table 2 a typical lifecycle period of 25 years a discount rate of 6 5 per year an establishment cost factor of 3 and an establishment period of 2 years were adopted the parameters for lcc swh eq 4 were estimated as follows a cost model for the total net present value npv of stormwater harvesting components was determined using linear regression r2 0 814 between levelized lifecycle cost ml and estimated annual volume supplied ml yr using detailed costing data for six stormwater harvesting projects derived by inamdar 2014 thus the lifecycle cost of stormwater harvesting components from eqn 4 was calculated using the following equation 8 l c c s w h i 1 n 104 49 s u p p l y i 6622 6 m l supply i m l 0 o t h e r w i s e if s u p p l y i 0 where supply i is the average annual supply capacity of the ith bmp in a candidate portfolio of n bmps 3 1 2 2 water quality improvement total nitrogen tn was the specific pollutant constituent adopted for the water quality objective tn load reduction was particularly important since in the urban catchment it was found by the consultants that maximizing tn reduction through treatment of stormwater also tended to reduce phosphorous total suspended solids and other pollutants to within target levels dr dale browne personal communication 2016 the introduction of excess anthropogenically generated nutrients into coastal systems can cause eutrophication which has negative impacts these impacts often include excessive and sometimes toxic production of algal biomass loss of important nearshore habitat changes in marine biodiversity and species distribution increased sedimentation of organic particles and depletion of dissolved oxygen the mean annual pollutant mass of tn retained by each candidate portfolio f q u a l i t y eqn 5 was calculated based on the sum of average annual tn mass retained by individual bmps in a portfolio the water quality improvement of individual bmps i e not an integrated system of a portfolio of bmps eqn 5 was assessed using the integrated stormwater model music version 6 1 model for urban stormwater improvement conceptualizion ewater 2009 as suggested by the relevant catchment management authority regulations music is an integrated stormwater model that evaluates rainfall runoff and pollutant generation and transport as well as the hydraulic and pollutant removal performance of bmps bach et al 2014 music algorithms simulate runoff based on models developed by chiew and mcmahon 1999 and urban pollutant load relationships based on analysis by duncan 1999 3 1 2 3 stormwater harvesting to determine stormwater harvesting capacity of projects experts on stormwater harvesting from each local government authority were asked to evaluate the stormwater harvesting potential of bmps within their jurisdiction they estimated the expected irrigation demand required by open spaces near each bmp and the average annual potential capacity to supply the demand the estimates were based on procedures specific to each local government authority and reflect the stormwater harvesting objective performance values accepted by decision makers 3 1 2 4 urban vegetation and amenity improvement the green score of individual projects which is a weighted score of several indicators that was developed by the authors and agreed to be used as an optimization objective by the consultants uses scores assigned by experts from each local government authority interviewed in a workshop session by the consultants the experts were asked to answer the following questions about the bmp projects within their jurisdiction answer yes no or maybe to the following questions 1 will native vegetation increase at the site 2 will tree cover increase at the site and 3 will the quality of recreation spaces in the area increase the total catchment green score objective function was 9 g r e e n i j 1 3 s c o r e j 10 s c o r e j 3 i f a n s w e r i s y e s 2 i f a n s w e r i s m a y b e 1 i f a n s w e r i s n o where green i is the sum of scores for each project and score j is the number of points assigned to the answer to the jth question since there were three questions each project could achieve a maximum of 9 green points and each portfolio a theoretical maximum of 20 9 180 green points 3 1 2 5 evaluation of individual bmps before the optimization process was run the costs and performance values of each bmp were determined table 1 firstly the stormwater harvesting capacity of individual projects was determined from local government authority expert interviews secondly the individual project lifecycle costs were determined using cost parameters from eqns 2 4 and 8 for each project thirdly the water quality performance of each bmp was determined with the aid of music to do this a stormwater model for a 1 ha catchment area for each local government authority was developed the model consisted of a 0 5 ha pervious catchment node a 0 5 ha impervious catchment node and an outlet node to estimate the average annual tn load per unit area of catchment with an average 50 impervious surface area browne et al 2012 one year of continuous climate data and pervious surface parameters provided by the catchment management authority were used for the catchment nodes to estimate source kg for each bmp the tn load from a 1 ha unit catchment area for the respective local government authority was multiplied by the contributing catchment area to each bmp in hectares each bmp was assumed to remove 45 of the tn load from its contributing catchment i e resid i 1 0 45 source i which was suggested as an acceptable performance based on advice from the consultants dr dale browne personal communication 2016 finally eqns 7 9 and 10 were applied to determine the individual project green scores 3 1 3 constraints a single constraint was applied to limit portfolios to 20 or fewer projects since more than 20 projects was determined to be impractical to design and construct by the catchment management authority as mentioned previously the projects were assumed to be independent in that the inclusion of one project did not influence the expected benefit cost or feasibility of another this assumption was considered acceptable since the catchments contributing to each bmp were mutually exclusive and customers for stormwater harvesting projects could receive supply from only one project 3 2 pareto ant colony optimization p aco algorithm to solve the optimization problem a variant of the original pareto ant colony optimization algorithm p aco doerner et al 2004 metaheuristic search algorithm was used p aco was selected because it was originally developed to solve portfolio optimization problems doerner et al 2004 2006 it has been used successfully and adopted as a benchmark algorithm in recent three objective portfolio optimization applications cruz et al 2014 and it has been applied to complex multi objective water resources problems nguyen et al 2016 szemis et al 2013 2014 the variant adopted here pacoa was demonstrated to outperform other multi objective ant colony optimization algorithms in a recent water resources allocation study szemis et al 2013 the algorithm mimics the cooperative foraging behavior of an ant species that leaves a chemical pheromone on a ground surface in real life since ants traverse short paths to food more frequently more pheromone is laid on short efficient paths thus paths with higher pheromone levels are more likely to be selected by an ant in the algorithm artificial ants select between paths which in this instance represent decisions whether or not to adopt a bmp in a portfolio an input template and executable for the algorithm are available as data set 3 in di matteo et al 2016 a summary of the steps in the paco algorithm is shown in fig 4 in the initialization phase the paco search control parameters are set the iterative process commences when b ants are generated each ant starting with an empty portfolio x 0 and the objective weights i e the ant s individual preferences are determined randomly for each ant in the construction phase of the algorithm first the order of bmps is randomly shuffled to ensure bmps are provided an equal chance of being considered first by each ant see golding et al 2017 then the ant decides whether to add each bmp to a portfolio x by applying a pseudo random proportional rule using pheromone information τ i the pheromone information is stored in one 2xn matrix for each jth objective representing the binary options for the n possible bmps if the ant adds the maximum number of bmps n max before all bmps have been considered then none of the remaining bmps are selected after a portfolio has been constructed its performance is evaluated using the objective functions eqns 2 and 5 7 in this case as individual projects were determined to be independent the portfolio objective functions were a summation of the constituent individual project objective function values in table 1 after each iteration of the b portfolios generated by the b ants the non dominated portfolios are stored offline in an array then as part of a global update of every element of the j pheromone matrices the first and second best performing solutions ranked for each jth objective are used to apply the following equation 11 τ t j 1 ρ τ t j ρ δ τ t j δ τ t j 15 t i n b o t h b e s t a n d 2 n d b e s t p o r t f o l i o 10 t i n b e s t p o r t f o l i o 5 t i n s e c o n d b e s t p o r t f o l i o 0 o t h e r w i s e where for each bmp the current pheromone value for each tth binary option and jth objective is reduced by pheromone evaporation ρ and increased by a pheromone value δ τ t j pheromone is evaporated from decisions that are not in the best solutions for each objective which makes it less likely these decisions will be selected again in future iterations in this way the ant s decision making landscape is modified to guide ants into regions of the search space that contain non dominated portfolios since the single constraint was handled in the construction phase no penalty function is required for this case study as all constructed portfolios are feasible the process of developing assessing and updating the pheromone trails to guide the pacoa to near optimal trade offs continues until a specified maximum number of iterations w is reached before the pacoa was applied a sensitivity analysis was conducted to identify suitable values of parameters that control the searching behavior of the algorithm to maximize the likelihood that the best possible approximation of the pareto front was generated the ranges of parameter values tested and the final parameters selected are given in table 3 in this study the paco was run for 1200 iterations of 500 ants which equates to 600 000 objective function evaluations this number of evaluations was selected because the progress of the pareto front ceased to be meaningful assessed by visually inspecting the pareto optimal solution set at 5000 evaluation intervals after this number of evaluations in a trial run of 2 000 000 evaluations the optimization results were replicated 50 times using different random starting seeds for the pseudo random number generator used in the algorithm to minimize the impact of probabilistic effects of some of the operators that influence the search each run took approximately 26 min on a 3 10 ghz computer with 8 gb of ram although multiple instances were run on one machine simultaneously the pareto optimal solutions shown in this paper are the result of a non dominated sort of the solutions from the 50 replicate runs 3 3 interactive visual analytics to explore pareto optimal solutions to visualize and analyze the objective and decision space trade offs of the pareto optimal set of portfolios an interactive visual analytics package was selected the combined objective space and decision space visualizations were carried out using the approach of kollat and reed 2007 using the discoverydv software package discoverydv version 0 72 available at https www decisionvis com discoverydv the package features an interactive data plot that allows brushing linked views of solutions marking and tracing of solutions of interest as well as rapid browsing through solution objective decision and non objective performance data the package has been used successfully in several recent many objective optimization studies piscopo et al 2015 woodruff et al 2013 the pareto optimal solution objective and decision data were uploaded into the interactive visual analytics package this allowed the analyst to 1 visualize and analyze trade offs between the four objectives 2 isolate portfolios from several regions of the trade off front using interactive brushing and visualization in multiple linked plots and 3 visualize the decision and objective space to analyze the impact and prevalence of particular projects on the performance of pareto optimal solutions the pareto optimal solution data file uploaded into the package is available as data set 3 and a ddv file for the discoverydv program containing the visualizations is included as data set 4 in di matteo et al 2016 4 results and discussion this section presents the results of the many objective optimization process for the stormwater management portfolio selection case study outlined in section 3 the results of the pacoa runs from 50 random starting positions show the algorithm identified 3654 pareto optimal or near pareto optimal portfolios as solutions to the optimization problem 4 1 identifying many objective trade offs between pareto optimal stormwater management portfolios fig 5 shows the trade offs between four objectives of the pareto optimal portfolios in a 4 dimensional coordinate plot a sharp trade off exists between tn reduction and cost and between reuse capacity and cost indicating small increments in cost can return large increases in both of these objectives in contrast green score tends to increase with cost which is expected as higher cost portfolios have more bmps distributed in the catchment to enable larger total catchment urban greening and amenity improvement the above inferences are supported and supplemented by the alternate representation of the trade off surface in parallel coordinates inselberg 1997 in fig 6 small slopes on some line segments between the adjacent axes of lifecycle cost and stormwater reuse indicate high reuse portfolios exist for low costs however these low cost high reuse capacity solutions appear to have lower tn reduction and green score compared to other solutions as mentioned above green score appears to be correlated with lifecycle cost however some solutions exist that have a high green score and relatively low cost in the low cost region from fig 5 clusters of solutions form in tn reduction reuse capacity space this indicates that individual projects dominate the contribution to total portfolio reuse capacity or total nitrogen reduction in this region analysis of the bmps comprising solutions in these clusters shows that these portfolios contain a small number of flagship projects with exceptionally large reuse capacity e g project 61 40 ml year project 67 12 8 ml year project 18 12 0 ml year or tn reduction e g project 48 1152 kg year project 18 1141 kg year portfolios containing only a few of these flagship projects are able to achieve relatively high total reuse capacity or tn capacity at relatively low cost but also a low green score this causes the noticeable discontinuity in the objective space in the low cost region characterized by clusters of solutions emanating from a small number of portfolios in the low cost region in fig 5 and overlapping dark blue low cost line segments joining parallel axes in fig 6 moving in the preferred objective direction adding a flagship project to create a new portfolio on the front can cause a large increase in tn reduction or reuse capacity therefore decision makers desiring low cost trade off solutions could consider portfolios of a small number of flagship projects but this would considerably compromise the urban greening and amenity performance of the stormwater management strategy 4 2 importance of a many objective problem formulation for bmp selection the cost and total nitrogen reduction trade off projections in fig 7 show trade offs between water quality and cost objectives which correspond to the most frequently used formulation in stormwater management optimization studies to date on the front a slight knee region appears such that when moving along the front away from the knee region there is a diminishing return in these objectives this suggests that solutions in this region may represent a desirable trade off between total nitrogen reduction and cost the trade off pattern is consistent with those in other bmp selection studies chichakly et al 2013 lee et al 2012 maringanti et al 2009 however only considering trade offs between water quality and cost objectives neglects the influence of other objectives that may be important to stormwater management decision makers moglia et al 2012 this could bias decision makers towards selection of solutions that would lie at extremities in objective space should other formal objectives be considered kollat et al 2011 the importance of the many objective representation of the bmp selection problem adopted in this study is demonstrated by tracing a solution from the two objective knee region in fig 7 through higher dimensional objective space represented in fig 8 for this purpose portfolio 1 table 4 is selected and marked for further analysis because it lies at an inflection point observed by visual inspection in the knee region of the two objective trade off front fig 7 using the visual analytics package an additional harvesting capacity axis and a green score colour axis are added to create a 4 dimensional plot of the objective space fig 8 to compare portfolio 1 with other solutions similar in cost the analytics package s data brushing tool is used to highlight solutions with lifecycle costs in the range 1 90 m 2 70 m in fig 8 these solutions of interest appear opaque and the remaining solutions that have been brushed out appear transparent portfolio 2 table 4 is selected for comparison because although it has a 22 greater lifecycle cost and similar tn reduction compared to portfolio 1 it has a vastly higher reuse capacity and green score therefore although portfolio 1 appeared in the region of best trade off knee region in the lower dimensional tn reduction cost representation of the objective space fig 7 it performed poorly in the reuse capacity and green score objectives portfolio 2 lies near but not on the non dominated water quality cost front in fig 7 thus it would not have been considered by decision makers in a bi objective pareto optimization approach which has been typical in bmp selection optimization studies to date when considering the project options selected in the two portfolios table 4 it is apparent portfolio 2 is similar to portfolio 1 except for one small project project 38 instead of project 33 and importantly two additional projects located in local government authority 1 projects 60 and 61 consequently decision makers may consider that portfolio 2 provides a better compromise between objectives compared with portfolio 1 due to the reuse capacity and green score benefit that the two additional projects provide importantly for stormwater managers the results of this study show that when they use water quality and cost as the only optimization objectives they may not identify solutions that represent good trade offs between water quality cost and other important objectives including stormwater harvesting capacity and green score the above results are consistent with findings in several other studies including 1 findings by kollat et al 2011 kasprzyk et al 2015 and woodruff et al 2013 that generally in optimization studies lower dimensional problem formulations may bias selection of solutions that would otherwise exist at low performing extremes if additional performance criteria were considered as formal optimization objectives 2 a finding by chichakly et al 2013 that for bmp selection optimization desirable solutions lie near but away from the two objective non dominated pareto front for water quality improvement and cost objectives and 3 trade offs for a stormwater harvesting system design determined by di matteo et al 2017 which showed slight increases in system costs could provide large increases in both water quality improvement and harvesting capacity 4 3 identifying impacts of project options on pareto optimal portfolio performance fig 9 shows combined objective performance and decision characteristics of the pareto optimal portfolios which helps the analyst to overcome biases arising from artificial distinctions between objective performance and other characteristics of the problem matrosov et al 2015 for example the visual interactive plot allows the analyst to inspect which area of the trade off front each project features in pareto approximate portfolios in this way an analyst can infer the impact of particular projects on portfolio performance in fig 9 a the opaque spheres represent portfolios containing project 61 lifecycle cost 381 297 tn reduction 157 92 kg year reuse capacity 40 ml year green score 6 which was the project with the highest reuse capacity importantly all portfolios with 40 ml year or greater reuse capacity include project 61 and these portfolios occur in nearly the full range of cost tn reduction and green score of pareto solutions therefore this indicates decision makers should probably consider project 61 in their final portfolio in fig 9 b the opaque spheres represent portfolios containing project 48 lifecycle cost 915 472 tn reduction 1152 kg year reuse capacity 0 ml year green score 7 which was the project with the highest tn reduction importantly in the lower cost region pareto optimal portfolios with a number of smaller solutions dominated inferior portfolios containing project 48 this was because although the green score of project 48 was high 7 out of 9 the cumulative green score and or reuse capacity of low cost portfolios with more projects dominated portfolios containing a small number of larger projects including project 48 this indicates multiple additional benefits can be achieved for a similar cost by using a portfolio of projects rather than one flagship project in addition decision makers can view and assess additional non objective characteristics that may influence decision making for example the percentage of the catchment treated spatial distribution of projects throughout the catchment or socio political preferences for particular projects 5 summary and conclusion a general multi objective optimization formulation was developed for the selection of a portfolio of bmps for stormwater management this study advances the bmp selection optimization field of research chen et al 2015 chichakly et al 2013 di matteo et al 2017 lee et al 2012 maringanti et al 2009 zare et al 2012 as it addresses the need for a many objective optimization formulation for the selection of stormwater bmps that a can cater to a large number of performance criteria b can handle a large number of decision options and potential strategies c can identify solutions that represent the best possible trade offs between performance criteria d enables trade off information to be communicated in an easy to understand fashion and e enables the development of solutions that are trusted by decision makers the approach was applied to a case study catchment plan for a catchment authority in a major coastal city in australia the benefits of the proposed formulation specifically with regard to stakeholder acceptance of solutions were evident for the case study by using the proposed formulation the only options considered were solutions that stakeholders had proposed and were familiar with consequently the final set of solutions obtained after the optimization process consisted of combinations of trusted solutions rather than solutions that were obtained via processes and models end users were not familiar with stakeholders would have been able to identify portfolios that contain particular projects they may have a preference for or bias towards selecting and compare these with other optimal portfolios in this way when analyzing optimal solutions stakeholders could identify other projects that when packaged with preferred projects maximize the total portfolio benefits or experience a shift in preferences for particular projects by visualizing the prevalence of other projects within optimal solutions the results demonstrate the benefits of exploring full portfolio solution trade offs in a many dimensional pareto optimal front which can reduce the prevalence of decision making biases due to the optimization formulation a comparison between the trade off spaces of the lower dimensional water quality cost problem formulation and the many objective formulation demonstrated that low objective formulations can result in pareto optimal portfolios with low performance in non objective performance criteria in this study when stormwater harvesting and vegetation and amenity improvement scores were included as objective functions solutions that were in a region of best trade off in water quality cost space performed poorly in these additional objectives the many objective optimization results show that sharp trade offs exist between tn reduction and cost and between reuse capacity and cost indicating small increments in cost can return large increases in both of these objectives portfolios in the low cost regions typically featured a small number of projects including cost efficient flagship projects that provide high tn reduction or reuse capacity however in order to maximize the vegetation improvement and amenity benefits portfolios with a larger number of lower cost bmps distributed throughout the catchment were preferred notably the optimization formulation in the case study does not consider that interaction between having a higher harvest capacity might allow for more irrigation of green spaces using the visual analytics approach to explore combined optimization and decision spaces the impact of individual projects that may be preferred by decision makers was rapidly visualized this approach could assist in overcoming institutionally influenced biases to include particular projects or bmp technologies to demonstrate alternative similar cost options to decision makers future studies applying the framework could account for differences in preferences between multiple decision makers that may be responsible for funding over different periods of the project lifecycle for example in some funding schemes catchment management authorities fund the capital expenses whereas local government authorities fund the maintenance and ongoing expenses the many objective problem formulation could be adapted to include specific objectives important to local government authorities which might include the operating expenses for each individual local government authority in addition to total catchment benefits thus identifying the tradeoffs between capex and opex in addition the pareto optimal solutions could be explored taking into account individual formal objective and informal non objective preferences of multiple decision makers in this way decision makers can visualize their preferences on a trade off curve and compare and through an iterative approach visualize and negotiate acceptable outcomes and solutions this may be preferable to other approaches where weightings are set a priori which do not account for decision maker preferences in the decision space nor allow a visual comparison of the regions of interest preferred by several decision makers for the green index and stormwater harvesting objectives the objective formulations do not consider how diminishing returns due to interdependencies between projects in these objectives may be represented finally the constraint for the number of projects should consider the difficulty of constructing individual bmp types e g 20 swales might be easier to construct than 20 wetlands data and software availability name of software pareto ant colony optimization algorithm and problem data set developers michael di matteo holger r maier graeme c dandy year first available 2017 hardware required pc mac program language fortran program size 10 0 mb contact address school of civil environmental and mining engineering university of adelaide adelaide south australia telephone 61 8 8303 4313 12 fax 61 8 8313 4359 13 e mail michael dimatteo adelaide edu au url https figshare com articles a many objective optimization and visual analytics approach to project selection for integrated catchment management 4233119 data set software required the ddv files require discoveydv visualization software discoverydv is available for academic use under a beta license as of april 2017 at https www decisionvis com discoverydv some data files are as csv or xlsx which can be opened using microsoft excel other spreadsheet packages matlab scripts and data files which were used to convert the raw optimization run data into a useful form are available and require matlab software package availability software and data are available via figshare public and online repository and submitted to the journal editor for publication cost free for non commercial use acknowledgements research funding was provided by the australian postgraduate award the university of adelaide and the goyder institute for water research the authors thank dr dale browne and e2designlab australia for assistance with the case study data and the anonymous reviewers for their comments which helped improve the quality of the paper the data used are listed in the references tables supplements and repository at https doi org 10 6084 m9 figshare 4233119 appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 008 
26298,the diffuse nature of nonpoint source nps pollution as well as that of the effectiveness of best management practices bmp to control nps pollution necessitates bmp evaluation at the field scale in this study a web interface was developed for application of the agricultural policy environmental extender apex model at the field scale the interface contains background databases for field location soil agricultural management and climate across the contiguous united states users can specify properties to run the model for an individual field and compare the results under various land management and conservation practice choices a case study was conducted to demonstrate the capability of the web interface for simulating various land management scenarios this tool can help provide on site information for nps pollution management related policies and serve as a communication tool among scientists engineers and stakeholders keywords apex model web interface western lake erie basin wleb best management practices bmp nonpoint source nps pollution cover crop software and data availability name of software apex web interface developers qingyu feng software required internet brower tested on firefox chrome edge safari and internet explorer programming language php html javascript python and c url of the web page http horizon nserl purdue edu apexonline source code available https github com qingyufeng apexwebinterface first available april 2017 1 introduction lake erie on the us canada border is frequently experiencing harmful algal blooms habs bridgeman et al 2013 these habs not only result in direct negative impacts on the lake s ecologies such as hypoxia and loss of habitats for aquatic life scavia et al 2014 but also indirect social impacts including undrinkable domestic water supplies and economic losses to tourism king et al 2017 habs in lake erie are mainly a result of excess phosphorus p losses from agricultural lands during the last two decades with the maumee river basin as the major contributor stow et al 2015 in response to these series of water quality problems the us and canadian governments set a new annual total phosphorus loading reduction target of 40 below the 2008 loads from the maumee river basin kalcic et al 2016 muenich et al 2016 one of the prospective pathways to reach this goal proposed by the scientific community is to implement agricultural best management practices bmps at a large scale scavia et al 2017 implementations of bmps at a large scale require accurate and detailed evaluation of potential bmp options selection of proper implementation locations and other considerations affecting bmp effectiveness this usually involves application of computer simulation models to facilitate the determination of bmp effects and locations several modeling efforts have been conducted to determine proper pathways of reducing p losses from agricultural lands through bmps francesconi et al 2015 muenich et al 2016 scavia et al 2017 yen et al 2016 the majority of these studies were conducted at the watershed scale except that the bmp evaluations in francesconi et al 2015 were conducted at the field scale watershed scale evaluations of bmps provide valuable insights on overall effectiveness which can include important information for creating watershed management plans however modeling at the watershed scale usually provides very limited information on hydrologic sediment and nutrient processes at the field scale in which most bmps are actually installed zhang and zhang 2011 zhu et al 2017 evaluation of bmps at the field scale where they are actually implemented will provide better guidance for actual installation since most bmps function at this scale instead of at the watershed scale zhang and zhang 2011 on site evaluation requires utilization of site specific soil topographic land use and climate information several ways exist to evaluate the potential for nutrient losses from a field these include using direct measurements of nutrient losses and or soil test p levels in fields harmel et al 2018 p indices osmond et al 2006 field scale models such as agricultural policy environmental extender apex gassman et al 2010 or watershed scale models such as soil and water assessment tool swat arnold et al 1998 direct measurement is the most accurate way to evaluate the nutrient loss from a field however it is expensive and time consuming besides it is not feasible to conduct direct measurements on all fields of interest and the results of measurements on one field are only applicable to the specific field or at most to fields with similar conditions alternatively p indices have been developed to assess the relative risk of p loss from fields based on source and transport factors and can be an inexpensive and quick approach however p indices should be used with caution sharpley et al 2012 since they may not accurately predict off site p losses harmel et al 2018 watershed scale models after careful calibration and validation are able to provide evaluations of nutrient losses with high accuracy and assess effectiveness of numerous bmps shao et al 2017 and results for the field scale can also be extracted from simulations conducted with watershed scale models however watershed scale models are generally too complex to be applied by farmers and conservation planners in recent years increased understanding of hydrologic and nutrient cycling processes have enhanced the development of field scale water quality models for better prediction of nutrient loss and evaluation of the effectiveness of various bmps harmel et al 2018 existing field scale models such as apex or the environmental policy integrated climate epic model izaurralde et al 2006 are also complex and not easy to apply by farmers and conservation planners thus now is a good time to develop a web interface for model implementation babbar sebens et al 2015 webb et al 2015 kurtz et al 2017 to provide for evaluations of nutrient loss reduction effects by various bmps at the field scale the specific objectives of this study were to i develop a web interface for the apex model so that modeling outputs can be demonstrated informatively and efficiently and ii compare the simulated effects of various bmps on nutrient loss with the web interface to enhance the quality of the corresponding decision making process 2 methods 2 1 the apex model the apex model was developed to fill the gap of simulating landscape processes at the farm or small watershed scale the scale at which the environmental policy impact climate epic model izaurralde et al 2006 and the swat model arnold et al 1998 were not quite applicable gassman et al 2005 essentially apex is a multi field version of the epic model which means it simulates each field called subarea in apex with the epic model and then routes the outputs to simulate multiple fields or a small watershed apex calculates important landscape processes hydrological processes soil erosion crop growth nutrient transport etc on either a daily or sub daily time step which is then summarized into monthly or annual results as requested by model users the model is capable of simulating the impacts of different land management and conservation practices on those processes apex requires data for topography soil climate and land management a detailed description of how the apex model functions is available in the theoretical documentation for the model williams et al 2012 2 2 web interface development the proposed interface is designed to simulate a single field one subarea in the apex model version 1501 in apex one subarea is defined as one land area where the soil land management topography and climate are homogeneous the design of the web interface includes three components front end webpages background database and modules for processing apex input and output files and searching nearest climate stations the web interface was developed using multiple computer programming languages including html for the front end webpage php for connection between the webpage and background database javascript for user operation of the webpage python for generation of inputs files for the apex model sql for extracting information from the background database and c for searching climate stations the schematic of the web interface among the three components is shown in fig 1 the web interface includes 5 webpages home project management results and help figs 1 and 2 users start using the web interface by initiating a project and providing a project name in the home webpage one temporary folder will be created to store all files generated for each project with one unique session id and access date for example 20171203 projectname mte2spentngiag584gj6fvinq6 to differentiate between projects created by different users from different places and times in this web interface a project is a collection of user input variables which include those with bold text in fig 1 the location variables state county and zip code are selected from three background spatial databases and used to determine the list of soil options and the climate station to be used on the project webpage fig 2 users are also able to configure soil test n and p values topographic characteristics area slope and slope length and agricultural management of the field the soil test and topographic features of the field are imported as numeric inputs users can also generate their own custom cropping and management practices in the management webpage to generate a user specified management users begin by selecting an existing template in the management editing table users can then add delete or edit each line and specify the details for the agricultural management options fig 3 for example users can specify the date application method type and amount for fertilizer management there are 25 operation types that can be simulated by the apex model including agricultural management operations such as planting harvesting fertilizing and applying pesticide operation names are options for each operation type for example manure operation name fertilizer can be applied using a manure spreader operation types as shown in fig 3 when one user finishes configuring a field and clicks on the run apex model button in the project webpage fig 3 the interface calls the climate station search program to determine the station name extracts climate variable values from the station and soil properties values from the soil database then several python scripts are called to write the values for variables related to location climate soil management and topography into input files for the apex model next the apex model executable is called to run the model after apex simulation is completed the web interface will extract the annual awp and monthly msa output files in the temporary project folder and process and display the results for each run in the results page which is described in the case study section below 2 3 background database development as shown in fig 1 there are 7 background databases for this interface these databases include data for location state county and zip code soil ssurgo climate monthly and daily and management the development of each database is described below 2 3 1 location database the location database includes tables for states counties and zip codes the state table is a list of the 48 states of the contiguous united states alaska and hawaii are not included the county table includes one column for the state and one column for the county the zip code table includes three columns state county and zip code when the user updates the selected state in the project page the county list is updated to display counties for the selected state the zip code list is also updated to display zip codes for the first county in alphabetic order 2 3 2 soil database the soil database is prepared by extracting required soil variables for the apex model from the soil survey geographic ssurgo database soil survey staff nrcs usda 2017 three tables in the ssurgo database are used chorizon component and muaggatt in the ssurgo database each soil entry is assigned a unique identification number called a mukey map unit key each soil entry may have several components with one major component and each component may have up to 10 layers the muaggatt map unit aggregate attribute table stores the data at the map unit level the component table stores those at the component level and the chorizon table stores those for each layer only the major component data are included since most data for the non major components are missing in the ssurgo database these three tables are combined into one table in summary there are approximately 300 000 soil entries in ssurgo those soil entries marked as water or where the depth is less than 10 cm are excluded in the soil database for the apex web interface based on their spatial location the extracted soils data were grouped by the zip codes of each county in each state for the contiguous united states the soil database includes two tables one table includes five columns state county zip code mukey and soil names and is included for display purposes when the location is changed by the user in the project webpage the displayed list of soil names is updated for the corresponding zip code the other table includes columns for 21 variables at the map unit or component level and for 18 variables at each layer of one soil entry supporting information si table s1 this table is designed for the purpose of extracting soil properties by mukey the first table size is smaller than the second helping to increase the search speed using the sql language and thus the speed of the web interface 2 3 3 climate database the climate database includes two tables for each of the 48 states in the contiguous united states one table is for the monthly statistics which are used to write the wp1 and wnd files for apex the other table is for daily observed data which are used to prepare the dly file for apex data for the monthly statistics table are extracted from the output files generated by the climate generator cligen software zhang and garbrecht 2003 columns in the table for monthly statistics are provided in table s2 data for the table of daily observed values for precipitation and minimum maximum temperature are extracted from those used to calculate monthly statistics by cligen the columns in these tables for daily data include data entry id variable id station name rainfall type year cligen version and data variable id refers to daily precipitation and maximum and minimum temperatures the source data have been downloaded from the national climatic data center ncdc https www ncdc noaa gov cdo web for 1974 to 2013 totally there are over 2700 climate stations across the u s in the database 2 3 4 management database the management database includes tables for operation types operation names fertilizers pesticides and crops as shown in fig 3 when the operation type for example fertilizer is selected the display for the operation name for example manure spreader list for the corresponding type is updated the table for the operation type and name are converted from the apex tillcom dat input files tables for fertilizers pesticides and crops are converted from fertcom dat pestcom dat and cropcom dat from apex respectively the management database also contains one table with templates of management operations which is provided as a list of blueprints for creating customized management operations these templates were extracted from the database embedded in the arcapex software https epicapex tamu edu install arcapex for apex v 1501 in addition some additional management operations were included by adding conservation practices to those already available in arcapex 2 4 web interface implementations the web interface will be executed at an apache http server version 2 4 29 which is installed on an ubuntu 16 04 operating system server edition the background database was created and stored in the postgresql software the source code of the web interface is available for download on github com see the software availability section the actual data of the database are available upon request via email to the corresponding author they were not uploaded due to the large size of the files future users who are interested in testing the interface in their local machine can run the interface with preferred php environment with the downloaded source code and the associated database 3 case study 3 1 study area the interface was applied to simulate the effects of various conservation practices on one field located in the st joseph river watershed in northeastern indiana this field has been under no till cropping corn soybeans for the past 28 years tile drainage is installed on this field at a depth of about 1 meter flow and water quality variables from the field have been monitored by the usda ars national soil erosion research laboratory nserl since 2004 as part of the conservation effects assessment project ceap francesconi et al 2014 during the monitoring period the farmer management practices were recorded apex was calibrated and validated in francesconi et al 2014 for this field and detailed monitoring results are available in that paper in this study this field was used to test the functions of the web interface the configuration of this field at the project webpage is shown in fig 2 a project file json for this case study was saved and provided as appendix 1 this json file can be uploaded to the home page of the web interface for test runs 3 2 simulations of conservation practices in this case study simulation of four conservation practices were conducted with the web interface these conservation practices included no till nutrient management cover crop and conversion to grassland the detailed management operations for each conservation practice are generated using the management page of the web interface the screenshots of them are provided in the supplemental material to this article as figure s1 4 3 3 demonstration of simulation results the screenshots of the results page for the simulations of the four conservation practices are provided in figs 4 6 fig 4 presents the average annual values for hydrologic and water quality variables this page is self increasing and will list results for all runs made with the web interface within the current session this table is designed to present a general comparison among runs by the user in fig 5 four figures are provided using google graph objects in each figure users can specify the run number variables and time scale annual or monthly all the four figures are updated simultaneously which means the user can specify figure display for either single or multiple ones at the same time for this case study annual results of total phosphorus losses are displayed for runs 1 and 2 and average monthly results are displayed for runs 3 and 4 the variables that can be displayed in the figure include all those in the average annual results table finally the run history is presented in the table shown in fig 6 the two tables in run history include all variables that might be modified by users for each run these two tables are designed to record user modifications and facilitate the comparison and analysis of results among runs as for the simulated results management operations are the only factors affecting results from these four runs from no till figure s1 in run 1 to nutrient management figure s2 half manure and half chemical phosphorus fertilizer in run 2 soil erosion se total nitrogen tn and tp losses are reduced when a cover crop is planted in the winter as in run 3 figure s3 se tn and tp losses are further reduced these three water quality variables are reduced the most when crop land is converted to grassland in run 4 figure s4 4 results and discussion 4 1 web interface for the apex model in this study an efficient web interface was developed to apply the apex model at the field scale across the entire contiguous united states this interface accesses background databases for soil management and climate and users can easily make model runs by specifying field configurations in terms of location slope and area users can also select and edit agricultural land management operations with these options nutrient losses can be simulated under various scenarios particularly agricultural crop production and with commonly adopted bmps such as nutrient management tile drainage and cover crops the advantages of this interface include 1 users are able to use the interface and make apex model runs as long as they have access to the internet they do not need to install offline software to use the model 2 preparation of input data for using existing interfaces for the apex model such as arcapex and winapex is time consuming and has a relatively long learning curve for example arcgis software and proficiency are required for arcapex and users have to collect data for digital elevation model dem soil land use and climate data and convert them into arcapex required formats the new web interface contains a database with soil and climate data for the contiguous us as well as templates of management practices relevant data is automatically converted to input files for the apex model by the interface 3 the results page provides an overview of common nonpoint source pollution output variables and also has the convenience of comparing results from multiple runs users do not have to extract results from the text outputs from the apex model which can require tedious learning and understanding of the apex output structures 4 the project downloading function enables user to save their project for future use a small size text file for the project contains information including customized management practices for all runs they make if needed users can contact the developers to obtain the entire set of apex input and output files which might be more useful for advanced users who are interested in further customizing the model viewing other model outputs or using the model for other scenarios which are beyond the capability of this interface these features make the use of the apex model easier and more efficient by alleviating barriers in applying the model most decision makers may not have the expertise to use the standalone apex model the new web interface also enhances interactivity an important aspect of environmental model application moeseneder et al 2015 with its designed capability and features the intended users of this interface are farmers conservation planners apex modelers and others who are interested in water quality modeling and evaluation of bmp effectiveness during the development stage staff members of the united states department of agriculture natural resources conservation service usda nrcs in indianapolis were involved in the initial design and final product testing as potential users the release of this web interface can serve as a convenient communication tool between nrcs staff and local farmers for identifying effective bmps for specific farms and making conservation plans the evaluation of bmp effectiveness at field scales was considered more relevant to farmers than using effectiveness evaluated at watershed scales as has been done in most existing studies the web interface was also tested by peer researchers at purdue university who provided valuable feedback on user inputs management practice editing capabilities climate data options and results display layout that have already been incorporated into the software 4 2 future development directions the web interface still has room for improvement in several areas first the web interface currently only simulates one field one apex model subarea in the future the small watershed capability of apex could be used if online geographic information system gis techniques are incorporated second only one set of parameters for internal model processes such as hydrologic processes crop growth sediment loss and nutrient loss is used across all simulations in other words the parameters are not calibrated and validated against measured data during web interface development we also consulted the apex modeling group at the blackland research center temple texas us to determine proper values of some parameters the apex model has also been used in the conservation effects assessment project ceap u s department of agriculture 2017 u s department of agriculture 2016 the accuracy of the apex model in predicting hydrologic processes and nutrient losses has been validated in various previous studies gassman et al 2010 lastly the web interface is only capable of simulating management based bmps other bmps such as buffer strips contours and sediment ponds should be added as options to the interface in future research efforts 5 summary and conclusions currently major evaluations of nps pollutant losses have been conducted at the watershed scale which provide valuable information on possible overall impacts of bmps on delivery of nutrients to the watershed outlet however the range of nps pollutant losses and the effectiveness of bmps vary greatly and large scale management plans may not be suitable for an individual field conducting evaluations at the field scale is important since it is where most agricultural bmps are installed and function proper evaluation of nps pollutant losses selection of bmps and proper applications of bmps at this scale are the prerequisite for successful management plans to reduce nps pollutant losses and their negative consequences this study developed a web interface for apex a field scale model that is physically based and has been widely used by the ceap project in the us this new web interface can conveniently provide field to field evaluations for policymakers and stakeholders a case study was conducted in order to demonstrate the capability of this interface in facilitating evaluation of various management based decisions this interface can be a useful tool for various potential model users for field evaluations and other model application purposes acknowledgement the authors would like to thank jim frankenberger of the usda agricultural research service national soil erosion research lab for providing the framework of the interface the climate station searching program and guidance in computer programming we would also like to thank dr anurag srivastava of the agricultural and biological engineering department purdue university shannon zezula and tony bailey at usda nrcs in indianapolis dr jimmy williams and dr jaehek jeong at the blackland research center temple texas for providing technical support feedback and tests of the interface this research was also supported by the shandong provincial natural science foundation china no zr2016eem18 appendix a supplementary data the following is the supplementary data to this article supportinginformation supportinginformation appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 011 
26298,the diffuse nature of nonpoint source nps pollution as well as that of the effectiveness of best management practices bmp to control nps pollution necessitates bmp evaluation at the field scale in this study a web interface was developed for application of the agricultural policy environmental extender apex model at the field scale the interface contains background databases for field location soil agricultural management and climate across the contiguous united states users can specify properties to run the model for an individual field and compare the results under various land management and conservation practice choices a case study was conducted to demonstrate the capability of the web interface for simulating various land management scenarios this tool can help provide on site information for nps pollution management related policies and serve as a communication tool among scientists engineers and stakeholders keywords apex model web interface western lake erie basin wleb best management practices bmp nonpoint source nps pollution cover crop software and data availability name of software apex web interface developers qingyu feng software required internet brower tested on firefox chrome edge safari and internet explorer programming language php html javascript python and c url of the web page http horizon nserl purdue edu apexonline source code available https github com qingyufeng apexwebinterface first available april 2017 1 introduction lake erie on the us canada border is frequently experiencing harmful algal blooms habs bridgeman et al 2013 these habs not only result in direct negative impacts on the lake s ecologies such as hypoxia and loss of habitats for aquatic life scavia et al 2014 but also indirect social impacts including undrinkable domestic water supplies and economic losses to tourism king et al 2017 habs in lake erie are mainly a result of excess phosphorus p losses from agricultural lands during the last two decades with the maumee river basin as the major contributor stow et al 2015 in response to these series of water quality problems the us and canadian governments set a new annual total phosphorus loading reduction target of 40 below the 2008 loads from the maumee river basin kalcic et al 2016 muenich et al 2016 one of the prospective pathways to reach this goal proposed by the scientific community is to implement agricultural best management practices bmps at a large scale scavia et al 2017 implementations of bmps at a large scale require accurate and detailed evaluation of potential bmp options selection of proper implementation locations and other considerations affecting bmp effectiveness this usually involves application of computer simulation models to facilitate the determination of bmp effects and locations several modeling efforts have been conducted to determine proper pathways of reducing p losses from agricultural lands through bmps francesconi et al 2015 muenich et al 2016 scavia et al 2017 yen et al 2016 the majority of these studies were conducted at the watershed scale except that the bmp evaluations in francesconi et al 2015 were conducted at the field scale watershed scale evaluations of bmps provide valuable insights on overall effectiveness which can include important information for creating watershed management plans however modeling at the watershed scale usually provides very limited information on hydrologic sediment and nutrient processes at the field scale in which most bmps are actually installed zhang and zhang 2011 zhu et al 2017 evaluation of bmps at the field scale where they are actually implemented will provide better guidance for actual installation since most bmps function at this scale instead of at the watershed scale zhang and zhang 2011 on site evaluation requires utilization of site specific soil topographic land use and climate information several ways exist to evaluate the potential for nutrient losses from a field these include using direct measurements of nutrient losses and or soil test p levels in fields harmel et al 2018 p indices osmond et al 2006 field scale models such as agricultural policy environmental extender apex gassman et al 2010 or watershed scale models such as soil and water assessment tool swat arnold et al 1998 direct measurement is the most accurate way to evaluate the nutrient loss from a field however it is expensive and time consuming besides it is not feasible to conduct direct measurements on all fields of interest and the results of measurements on one field are only applicable to the specific field or at most to fields with similar conditions alternatively p indices have been developed to assess the relative risk of p loss from fields based on source and transport factors and can be an inexpensive and quick approach however p indices should be used with caution sharpley et al 2012 since they may not accurately predict off site p losses harmel et al 2018 watershed scale models after careful calibration and validation are able to provide evaluations of nutrient losses with high accuracy and assess effectiveness of numerous bmps shao et al 2017 and results for the field scale can also be extracted from simulations conducted with watershed scale models however watershed scale models are generally too complex to be applied by farmers and conservation planners in recent years increased understanding of hydrologic and nutrient cycling processes have enhanced the development of field scale water quality models for better prediction of nutrient loss and evaluation of the effectiveness of various bmps harmel et al 2018 existing field scale models such as apex or the environmental policy integrated climate epic model izaurralde et al 2006 are also complex and not easy to apply by farmers and conservation planners thus now is a good time to develop a web interface for model implementation babbar sebens et al 2015 webb et al 2015 kurtz et al 2017 to provide for evaluations of nutrient loss reduction effects by various bmps at the field scale the specific objectives of this study were to i develop a web interface for the apex model so that modeling outputs can be demonstrated informatively and efficiently and ii compare the simulated effects of various bmps on nutrient loss with the web interface to enhance the quality of the corresponding decision making process 2 methods 2 1 the apex model the apex model was developed to fill the gap of simulating landscape processes at the farm or small watershed scale the scale at which the environmental policy impact climate epic model izaurralde et al 2006 and the swat model arnold et al 1998 were not quite applicable gassman et al 2005 essentially apex is a multi field version of the epic model which means it simulates each field called subarea in apex with the epic model and then routes the outputs to simulate multiple fields or a small watershed apex calculates important landscape processes hydrological processes soil erosion crop growth nutrient transport etc on either a daily or sub daily time step which is then summarized into monthly or annual results as requested by model users the model is capable of simulating the impacts of different land management and conservation practices on those processes apex requires data for topography soil climate and land management a detailed description of how the apex model functions is available in the theoretical documentation for the model williams et al 2012 2 2 web interface development the proposed interface is designed to simulate a single field one subarea in the apex model version 1501 in apex one subarea is defined as one land area where the soil land management topography and climate are homogeneous the design of the web interface includes three components front end webpages background database and modules for processing apex input and output files and searching nearest climate stations the web interface was developed using multiple computer programming languages including html for the front end webpage php for connection between the webpage and background database javascript for user operation of the webpage python for generation of inputs files for the apex model sql for extracting information from the background database and c for searching climate stations the schematic of the web interface among the three components is shown in fig 1 the web interface includes 5 webpages home project management results and help figs 1 and 2 users start using the web interface by initiating a project and providing a project name in the home webpage one temporary folder will be created to store all files generated for each project with one unique session id and access date for example 20171203 projectname mte2spentngiag584gj6fvinq6 to differentiate between projects created by different users from different places and times in this web interface a project is a collection of user input variables which include those with bold text in fig 1 the location variables state county and zip code are selected from three background spatial databases and used to determine the list of soil options and the climate station to be used on the project webpage fig 2 users are also able to configure soil test n and p values topographic characteristics area slope and slope length and agricultural management of the field the soil test and topographic features of the field are imported as numeric inputs users can also generate their own custom cropping and management practices in the management webpage to generate a user specified management users begin by selecting an existing template in the management editing table users can then add delete or edit each line and specify the details for the agricultural management options fig 3 for example users can specify the date application method type and amount for fertilizer management there are 25 operation types that can be simulated by the apex model including agricultural management operations such as planting harvesting fertilizing and applying pesticide operation names are options for each operation type for example manure operation name fertilizer can be applied using a manure spreader operation types as shown in fig 3 when one user finishes configuring a field and clicks on the run apex model button in the project webpage fig 3 the interface calls the climate station search program to determine the station name extracts climate variable values from the station and soil properties values from the soil database then several python scripts are called to write the values for variables related to location climate soil management and topography into input files for the apex model next the apex model executable is called to run the model after apex simulation is completed the web interface will extract the annual awp and monthly msa output files in the temporary project folder and process and display the results for each run in the results page which is described in the case study section below 2 3 background database development as shown in fig 1 there are 7 background databases for this interface these databases include data for location state county and zip code soil ssurgo climate monthly and daily and management the development of each database is described below 2 3 1 location database the location database includes tables for states counties and zip codes the state table is a list of the 48 states of the contiguous united states alaska and hawaii are not included the county table includes one column for the state and one column for the county the zip code table includes three columns state county and zip code when the user updates the selected state in the project page the county list is updated to display counties for the selected state the zip code list is also updated to display zip codes for the first county in alphabetic order 2 3 2 soil database the soil database is prepared by extracting required soil variables for the apex model from the soil survey geographic ssurgo database soil survey staff nrcs usda 2017 three tables in the ssurgo database are used chorizon component and muaggatt in the ssurgo database each soil entry is assigned a unique identification number called a mukey map unit key each soil entry may have several components with one major component and each component may have up to 10 layers the muaggatt map unit aggregate attribute table stores the data at the map unit level the component table stores those at the component level and the chorizon table stores those for each layer only the major component data are included since most data for the non major components are missing in the ssurgo database these three tables are combined into one table in summary there are approximately 300 000 soil entries in ssurgo those soil entries marked as water or where the depth is less than 10 cm are excluded in the soil database for the apex web interface based on their spatial location the extracted soils data were grouped by the zip codes of each county in each state for the contiguous united states the soil database includes two tables one table includes five columns state county zip code mukey and soil names and is included for display purposes when the location is changed by the user in the project webpage the displayed list of soil names is updated for the corresponding zip code the other table includes columns for 21 variables at the map unit or component level and for 18 variables at each layer of one soil entry supporting information si table s1 this table is designed for the purpose of extracting soil properties by mukey the first table size is smaller than the second helping to increase the search speed using the sql language and thus the speed of the web interface 2 3 3 climate database the climate database includes two tables for each of the 48 states in the contiguous united states one table is for the monthly statistics which are used to write the wp1 and wnd files for apex the other table is for daily observed data which are used to prepare the dly file for apex data for the monthly statistics table are extracted from the output files generated by the climate generator cligen software zhang and garbrecht 2003 columns in the table for monthly statistics are provided in table s2 data for the table of daily observed values for precipitation and minimum maximum temperature are extracted from those used to calculate monthly statistics by cligen the columns in these tables for daily data include data entry id variable id station name rainfall type year cligen version and data variable id refers to daily precipitation and maximum and minimum temperatures the source data have been downloaded from the national climatic data center ncdc https www ncdc noaa gov cdo web for 1974 to 2013 totally there are over 2700 climate stations across the u s in the database 2 3 4 management database the management database includes tables for operation types operation names fertilizers pesticides and crops as shown in fig 3 when the operation type for example fertilizer is selected the display for the operation name for example manure spreader list for the corresponding type is updated the table for the operation type and name are converted from the apex tillcom dat input files tables for fertilizers pesticides and crops are converted from fertcom dat pestcom dat and cropcom dat from apex respectively the management database also contains one table with templates of management operations which is provided as a list of blueprints for creating customized management operations these templates were extracted from the database embedded in the arcapex software https epicapex tamu edu install arcapex for apex v 1501 in addition some additional management operations were included by adding conservation practices to those already available in arcapex 2 4 web interface implementations the web interface will be executed at an apache http server version 2 4 29 which is installed on an ubuntu 16 04 operating system server edition the background database was created and stored in the postgresql software the source code of the web interface is available for download on github com see the software availability section the actual data of the database are available upon request via email to the corresponding author they were not uploaded due to the large size of the files future users who are interested in testing the interface in their local machine can run the interface with preferred php environment with the downloaded source code and the associated database 3 case study 3 1 study area the interface was applied to simulate the effects of various conservation practices on one field located in the st joseph river watershed in northeastern indiana this field has been under no till cropping corn soybeans for the past 28 years tile drainage is installed on this field at a depth of about 1 meter flow and water quality variables from the field have been monitored by the usda ars national soil erosion research laboratory nserl since 2004 as part of the conservation effects assessment project ceap francesconi et al 2014 during the monitoring period the farmer management practices were recorded apex was calibrated and validated in francesconi et al 2014 for this field and detailed monitoring results are available in that paper in this study this field was used to test the functions of the web interface the configuration of this field at the project webpage is shown in fig 2 a project file json for this case study was saved and provided as appendix 1 this json file can be uploaded to the home page of the web interface for test runs 3 2 simulations of conservation practices in this case study simulation of four conservation practices were conducted with the web interface these conservation practices included no till nutrient management cover crop and conversion to grassland the detailed management operations for each conservation practice are generated using the management page of the web interface the screenshots of them are provided in the supplemental material to this article as figure s1 4 3 3 demonstration of simulation results the screenshots of the results page for the simulations of the four conservation practices are provided in figs 4 6 fig 4 presents the average annual values for hydrologic and water quality variables this page is self increasing and will list results for all runs made with the web interface within the current session this table is designed to present a general comparison among runs by the user in fig 5 four figures are provided using google graph objects in each figure users can specify the run number variables and time scale annual or monthly all the four figures are updated simultaneously which means the user can specify figure display for either single or multiple ones at the same time for this case study annual results of total phosphorus losses are displayed for runs 1 and 2 and average monthly results are displayed for runs 3 and 4 the variables that can be displayed in the figure include all those in the average annual results table finally the run history is presented in the table shown in fig 6 the two tables in run history include all variables that might be modified by users for each run these two tables are designed to record user modifications and facilitate the comparison and analysis of results among runs as for the simulated results management operations are the only factors affecting results from these four runs from no till figure s1 in run 1 to nutrient management figure s2 half manure and half chemical phosphorus fertilizer in run 2 soil erosion se total nitrogen tn and tp losses are reduced when a cover crop is planted in the winter as in run 3 figure s3 se tn and tp losses are further reduced these three water quality variables are reduced the most when crop land is converted to grassland in run 4 figure s4 4 results and discussion 4 1 web interface for the apex model in this study an efficient web interface was developed to apply the apex model at the field scale across the entire contiguous united states this interface accesses background databases for soil management and climate and users can easily make model runs by specifying field configurations in terms of location slope and area users can also select and edit agricultural land management operations with these options nutrient losses can be simulated under various scenarios particularly agricultural crop production and with commonly adopted bmps such as nutrient management tile drainage and cover crops the advantages of this interface include 1 users are able to use the interface and make apex model runs as long as they have access to the internet they do not need to install offline software to use the model 2 preparation of input data for using existing interfaces for the apex model such as arcapex and winapex is time consuming and has a relatively long learning curve for example arcgis software and proficiency are required for arcapex and users have to collect data for digital elevation model dem soil land use and climate data and convert them into arcapex required formats the new web interface contains a database with soil and climate data for the contiguous us as well as templates of management practices relevant data is automatically converted to input files for the apex model by the interface 3 the results page provides an overview of common nonpoint source pollution output variables and also has the convenience of comparing results from multiple runs users do not have to extract results from the text outputs from the apex model which can require tedious learning and understanding of the apex output structures 4 the project downloading function enables user to save their project for future use a small size text file for the project contains information including customized management practices for all runs they make if needed users can contact the developers to obtain the entire set of apex input and output files which might be more useful for advanced users who are interested in further customizing the model viewing other model outputs or using the model for other scenarios which are beyond the capability of this interface these features make the use of the apex model easier and more efficient by alleviating barriers in applying the model most decision makers may not have the expertise to use the standalone apex model the new web interface also enhances interactivity an important aspect of environmental model application moeseneder et al 2015 with its designed capability and features the intended users of this interface are farmers conservation planners apex modelers and others who are interested in water quality modeling and evaluation of bmp effectiveness during the development stage staff members of the united states department of agriculture natural resources conservation service usda nrcs in indianapolis were involved in the initial design and final product testing as potential users the release of this web interface can serve as a convenient communication tool between nrcs staff and local farmers for identifying effective bmps for specific farms and making conservation plans the evaluation of bmp effectiveness at field scales was considered more relevant to farmers than using effectiveness evaluated at watershed scales as has been done in most existing studies the web interface was also tested by peer researchers at purdue university who provided valuable feedback on user inputs management practice editing capabilities climate data options and results display layout that have already been incorporated into the software 4 2 future development directions the web interface still has room for improvement in several areas first the web interface currently only simulates one field one apex model subarea in the future the small watershed capability of apex could be used if online geographic information system gis techniques are incorporated second only one set of parameters for internal model processes such as hydrologic processes crop growth sediment loss and nutrient loss is used across all simulations in other words the parameters are not calibrated and validated against measured data during web interface development we also consulted the apex modeling group at the blackland research center temple texas us to determine proper values of some parameters the apex model has also been used in the conservation effects assessment project ceap u s department of agriculture 2017 u s department of agriculture 2016 the accuracy of the apex model in predicting hydrologic processes and nutrient losses has been validated in various previous studies gassman et al 2010 lastly the web interface is only capable of simulating management based bmps other bmps such as buffer strips contours and sediment ponds should be added as options to the interface in future research efforts 5 summary and conclusions currently major evaluations of nps pollutant losses have been conducted at the watershed scale which provide valuable information on possible overall impacts of bmps on delivery of nutrients to the watershed outlet however the range of nps pollutant losses and the effectiveness of bmps vary greatly and large scale management plans may not be suitable for an individual field conducting evaluations at the field scale is important since it is where most agricultural bmps are installed and function proper evaluation of nps pollutant losses selection of bmps and proper applications of bmps at this scale are the prerequisite for successful management plans to reduce nps pollutant losses and their negative consequences this study developed a web interface for apex a field scale model that is physically based and has been widely used by the ceap project in the us this new web interface can conveniently provide field to field evaluations for policymakers and stakeholders a case study was conducted in order to demonstrate the capability of this interface in facilitating evaluation of various management based decisions this interface can be a useful tool for various potential model users for field evaluations and other model application purposes acknowledgement the authors would like to thank jim frankenberger of the usda agricultural research service national soil erosion research lab for providing the framework of the interface the climate station searching program and guidance in computer programming we would also like to thank dr anurag srivastava of the agricultural and biological engineering department purdue university shannon zezula and tony bailey at usda nrcs in indianapolis dr jimmy williams and dr jaehek jeong at the blackland research center temple texas for providing technical support feedback and tests of the interface this research was also supported by the shandong provincial natural science foundation china no zr2016eem18 appendix a supplementary data the following is the supplementary data to this article supportinginformation supportinginformation appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 011 
26299,hyporheic flow and nutrient turnover in hyporheic systems are strongly influenced by in stream bedforms an accurate representation of topographical variations of the stream streambed interface is therefore essential in analytical models in order to represent the couplings between hydrological and biogeochemical processes correctly the classical toth approach replaces the streambed surface topography by a flat surface which is identical to a truncation of the original physical flow domain into a rectangle this simplification can lead to biased estimates of hyporheic flow and nutrient cycling within hyporheic systems we present an alternative analytical modeling approach for solving hyporheic problems without domain truncation that explicitly accounts for topographical variations of the streambed the presented approach is based on the application of perturbation theory applications of the method to hyporheic systems ranging from the centimeter scale of rippled bedforms to riffle structures of 10 m and larger scale indicate a high accuracy of the approach keywords analytical modeling perturbation method first order approximation hyporheic flow biogeochemical transformations 1 introduction exchange of water and solutes across the streambed interface and biogeochemical cycling of nutrients in hyporheic areas are important factors controlling the water quality of rivers and streams different approaches are used in hydrological models to represent exchange of solutes and water across the hyporheic interface boano et al 2014 in models used on large scale river networks and catchments the effect of riparian and hyporheic areas on nutrient removal are often represented by spatially lumped sink terms e g rassam et al 2007 by applying transient storage concepts e g pelletier et al 2006 or are neglected entirely marsili libelli and giusti 2008 bouadi et al 2017 in physically based models flow and degradation processes are simulated based on physical laws by an explicit consideration of spatially distributed system properties analytical models traditionally have been very important for conceptual understanding of hydrological systems analytical models may not be as efficient as their numerical counterparts in treatment of site specific characteristics like aquifer heterogeneity or the shapes and types of boundary conditions however they assist in mathematical description of the physical movement of water in hydrological systems e g wörman et al 2006 craig et al 2008 marklund and wörmann wond and craig 2010 the classic work of toth 1962 who investigated gravity driven regional flow is one of the prominent examples salient hydrodynamic features of the concept introduced by toth are mainly based on three traits of the regional systems 1 they have a near rectangular or near parallelepiped domain with a curvilinear top boundary where the magnitude of undulations is much smaller than the vertical scale of the flow domain 2 the lateral boundaries and the aquifer base are impermeable as appropriate for analyses of groundwater basins and 3 head values at the phreatic surface locations define the upper boundary which is a characteristic assumption for unconfined aquifers in hyporheic research the original ideas introduced by toth to analytically solve the governing equations for hyporheic flow for a rectangular flow domain by neglecting topographical variations of the streambed are frequently used in analytical models the first analytical representation of hyporheic flow elliot and brooks 1997a often referred to as advective pumping model apm considered periodical head and velocity distributions under the horizontal streambed surface that approximates periodical undulating ripples acceptance of the horizontal interface in the apm model was identical to toth s approach in the apm model periodical pressure changes were taken as major drivers of the water fluxes across the hyporheic interface applications of the apm approach involved studies of bedform affected hyporheic water and material exchange e g bottacin busolin and marion 2010 packman and brooks 2001 marion et al 2002 boano et al 2007 2008 wörmann et al 2007 chemical transformation of contaminants grant et al 2014 and hyporheic cycling of nutrients marzadri et al 2012 2014 azizian et al 2015 stohnedahl et al 2012 the original apm approach later was modified to consider the dynamical changes in stream velocity boano et al 2007 and was extended to 3d by wörman et al 2006 and stohnedahl et al 2010 toth s approach and the apm model both have in common that they shift all head values from the original curvilinear boundary condition to a horizontal boundary permitting a closed form analytical solution in the truncated model domain by separation of variables technique although various authors mentioned that the flat bed assumption for their specific cases only had a minor effect on hyporheic flow characteristics for dune like morphologies elliot and brooks 1997a b gravel bars marzadri et al 2010 or riffle pool structures tonina and buffington 2007 toth type solutions do not present processes that occur in the actual physical domain or inside small scale structures like ripples zlotnik et al 2015 this artifact stems from inconsistent application of the perturbation technique of boundary deformation which is well known in fluid dynamics e g van dyke 1975 similarly if used as the basis for estimating hyporheic flow or subsurface biogeochemical reactions the flat bed assumption may also introduce errors in the calculations due to the missing topography that are difficult to quantify or interpret zlotnik et al 2015 an analytical approach that circumvents the problems associated with domain truncation by explicitly accounting for topographical variations of the streambed therefore would represent a valuable addition to existing analytical techniques in hyporheic research to address the problems of introduced artifacts that are associated with domain truncation and in order to accurately represent effects of topography zlotnik et al 2015 proposed consistent use of the perturbation theory for obtaining analytical approximate solutions that are valid for the entire un truncated flow domain and verified their accuracy and functionality with a numerical finite element solution comsol using an example of flow through a unit basin a similar approach of using approximate solutions can be universally applied to any 2d or 3d steady state or transient toth type solutions including solutions derived from the apm model the main purpose of this study is to highlight the opportunities that are offered by applying the perturbation method in order to explicitly represent the streambed topography and its influence on flow and nutrient cycling in analytical hyporheic flow models for this purpose we developed a matlab modeling environment that is capable of deriving flexible 1st order approximate solutions in order to represent arbitrary curvilinear flow domains the code is primarily intended and optimized to work at scales of in stream bedforms such as dunes ripples or riffle pool structures but also can be used for at the scales of hill slopes or catchments with special emphasis on in stream structures and their crucial role for nitrate cycling the matlab code also includes a reactive transport routine based on a pumping and streamline segregation model this type of models use a hyporheic flow field derived from analytical or numerical solutions and represent kinetically controlled bio geochemical processes along isolated streamlines the pumping and streamline segregation approach has been widely applied in the field of hyporheic research in order to simulate important nutrient turnover processes in the hyporheic zone e g boano et al 2010b marzadri et al 2011 azizian et al 2015 the matlab code is parallelized using the pmatlab libraries bliss and kepner 2007 to improve the overall model performance and to reduce simulation runtimes because of the analytical nature of the presented model it does not require a computational grid thereby offering high flexibility and easy use our model referred to by the acronym p pass perturbation pass is applied to various hyporheic systems including 1 hyporheic flow through small scale ripple structures typical of a flume experiment and 2 artificially constructed riffle pool structures for these systems with scales that differ by almost two orders of magnitude the p pass model is used to analyze the coupling between flow and biogeochemical cycling of nitrate in the hyporheic zone results are compared with predictions obtained by the apm toth model and numerical simulations by using perturbation theory our p pass model combines the best of both worlds the simplicity of analytical solutions and a more faithful representation of surface topography 2 methodology prior to introducing the p pass model for hyporheic water flow simulations we briefly summarize the major aspects of the perturbation method in section 2 1 in section 2 2 a short introduction into the biogeochemical component of the model is presented information about the model implementation and parallelization in matlab along with a benchmark analysis is relegated to the supplement online 2 1 representing subsurface flow 2 1 1 perturbation method and approximate solutions the interested reader is referred to the publications of van dyke 1975 and bender and orszag 1999 providing a detailed and comprehensive introduction into perturbation here we briefly introduce the important steps of the perturbation method that are needed for our applications asymptotic and perturbation analysis produces approximate analytical solutions to differential and difference equations that cannot be solved exactly or in a closed form analytical expression the general idea of applying the perturbation method is to introduce a small dimensionless parameter the so called perturbation parameter ε into the governing equations the general procedure of solving ordinary or partial differential equations by applying the perturbation method can be subdivided into three major steps bender and orszag 1999 step 1 convert the original problem into a perturbation problem by delineating a small perturbation parameter ε step 2 assume an expression for the answer in the form of an infinite truncated perturbation series by this parameter and derive equations for the coefficients of these series step 3 recover the answer of the original problem by using the perturbation series for the given value of ε commonly the leading term in the perturbation series 0th order term is obtainable in a closed form because generally it is this term that ensures the success of the perturbation method in deriving accurate analytical approximate solutions bender and orszag 1999 as it was done by toth 1962 in the following section we will describe how the perturbation parameter ε is introduced into a general formulation of hyporheic flow problems and how to derive the corresponding perturbation series further we will explain how to estimate the coefficients of that series and finally how to derive an approximate solution for hyporheic flow 2 1 2 the boundary value problem 2d steady state flow in a uniform anisotropic aquifer with saturated hydraulic conductivities k x lt 1 and k z lt 1 and a spatial hydraulic head distribution h x z l can be described as follows 1 k x 2 h x 2 k z 2 h z 2 0 a flow domain is bounded by a spatially varying top boundary z x l and impermeable lateral and base boundaries fig 1 a 2 h 0 z x 0 0 z z 0 3 h l z x 0 0 z z l 4 h x 0 z 0 0 x l 5 h x z x h x 0 x l the head distribution on the upper boundary h x is a function either known from observations over the streambed or more often from computational fluid dynamics cfd simulations lateral and bottom no flow boundary conditions as used in the original work of toth when applied to hyporheic systems probably represent some oversimplification hyporheic systems under real conditions can be influenced by ambient groundwater flow and underflow conditions following a local topographical gradient e g boano et al 2008 fox et al 2014 marzadri et al 2015 nevertheless for deriving the governing hyporheic flow equations and various applications these simplifications are still valid toth 1962 solved a similar problem for gravity driven flow where h x z x by assuming that z x slightly deviates from the horizontal plane z z 0 or z x z 0 that resulted in a modified equation 5 as follows 6 h x z 0 z x 0 x l this approach reduces the domain to a rectangle in the vertical plane the so called toth domain for which equation 1 can be solved analytically for various functions z x along the boundary z z 0 fig 1b zlotnik et al 2015 investigated this approach using consistent application of the boundary deformation perturbation method e g van dyke 1975 and demonstrated the resulting advantages and deficiencies the problem can be solved more accurately utilizing the fact that the undulations of the streambed boundary in hyporheic processes occur in a small range between the highest elevation z max and lowest elevation z min in other words the range of elevations is much smaller than the characteristic thickness of the hyporheic zone z0 z max z min z 0 in general the upper boundary can be represented as follows 7 z x z 0 1 ε f x in equation 7 ε represents a small dimensionless perturbation parameter that together with f x l describes the spatial deviation in elevation between some horizontal elevation z0 and the actual topography z x fig 1 obeying no flow conditions at the basin boundaries d f 0 d x d f l d x 0 2 1 3 scaling of the problem and approximate solution the flow domain transformation from a curvilinear into a unit rectangular flow domain of the dimensions 0 x 1 0 z z 0 is achieved by scaling 8 x x l z z z 0 z x z 0 a z 0 l a k x k z where x and z are dimensionless coordinates l l is the length of the flow domain fig 1 and z 0 is the scaled value of z 0 accounting for anisotropy via the dimensionless parameter a after substitution of these variables into equation 1 and applying the chain rule to the partial derivatives in the new coordinates the resulting pde in scaled coordinates yields see zlotnik et al 2015 for a detailed derivation 9 2 h x 2 ε b 2 h x z 1 ε c 2 h z 2 ε d h z 0 b 2 l d f l x d x e l x 1 c ε l 2 z 2 d f l x d x 2 2 f l x ε f l x 2 1 e 2 l x d z l 2 2 ε d f l x d x 2 d 2 f l x d x 2 1 ε f l x 1 e 2 l x contrary to equation 1 this partial differential equation cannot be solved exactly by analytical techniques however as equation 9 contains small perturbation ε defined by equation 7 it can be solved approximately by applying perturbation method the solution to equation 9 can be given in form of a power series of this parameter 10 h x z n 0 n h n x z ε n here n is the number of retained terms in the perturbation series the coefficients h n x z can be determined by substituting this equation for h x z into equation 9 and the boundary conditions and collecting terms with the same powers of the parameter ε the accuracy of the approximate solution depends on the magnitude of ε and the number of successive terms n zlotnik et al 2015 showed that for a typical flow problem described by equation 1 and subject to the applied boundary conditions equation 2 to equation 5 a 1st order approximation n 1 for ε 1 is accurate enough for practical purposes while terms of order n 2 are neglected 11 h x z h 1 x z ε h 1 x z the 0th order term h 0 x z can be derived from solving equation 9 by setting ε 0 which reduces the problem to a standard laplace equation where the solution can be written in the form of an infinite series 12 h 0 x z c 0 0 2 m 1 c m 0 cos π m x cosh π m z cosh π m z 0 with the coefficients c m 0 for the arbitrary head function h x as follows 13 c m 0 1 l 0 l h x cos π m x l d x after finding the function f x z defined as follows 14 f x z 2 l z f l x h 0 x z z x 2 f l x 2 h 0 x z z 2 f l x z l 2 h 0 x z z the 1st order term h 1 x z can be found using separation of variables as follo 15 h 1 x z m 0 n 0 c n m 1 cos π m x cos π n 1 2 z z 0 with coefficients c n m 1 defined as 16 c 1 n m 4 1 δ m 0 λ m n l 0 l 1 z x 0 z x f x l z z 0 z x cos π m x l cos π n 1 2 z z x d z d x c n m 1 4 1 δ m 0 λ m n l 0 l 1 z x 0 z x f x l z z 0 l cos π m x l cos π n 1 2 z z x d z d x where 17 λ m n π 2 m 2 n 1 2 2 z 0 2 δ n m 1 n m 0 n m in equation 17 δ n m represents the kronecker delta function once h 0 x z and h 1 x z are known an approximate 1st order solution to the original flow problem equation 1 is obtained in transformed coordinates the solution in the original system of coordinates can be obtained by replacing the scaled variables x and z with the expressions shown in equation 8 h x z h x z which together with the problem specific perturbation parameter ε leads to the final approximate solution h x z based on h x z linear velocity components v x x z v z x z lt 1 which are necessary for particle tracking and the reactive transport simulations can be derived 18 v x x z k x θ d h d x 19 v z x z k z θ d h d z here θ represents the streambed porosity 2 2 reactive transport model for representation of nitrogen cycling simulated biogeochemical reactions include aerobic respiration ar nitrification ni denitrification dn and ammonification am in the hyporheic zone the coupling of flow and reaction is implemented using the pass model azizian et al 2015 in which oxygen ammonium and nitrate concentrations undergo advective transport and reaction in the hyporheic zone assuming negligible molecular diffusion and mechanical dispersion coefficients according to the so called segregated streamline hypothesis rawlings and ekerdt 2013 in pass biogeochemical processes are represented along advective flow lines a conceptually identical approach to what has been used in the past to describe hyporheic nutrient turnover e g boano et al 2010b marzadri et al 2011 the magnitude of errors associated with neglecting mixing by molecular diffusion and mechanical dispersion in the pass model are in detail discussed in grant et al 2014 the biogeochemical model does not account for the possible formation of redox micro zones as a consequence of material heterogeneity or spatially variable patterns of organic carbon and respiration rates material heterogeneity in the hyporheic area can lead to the formation of redox micro zones such as localized pockets of denitrification in oxygenated areas of the streambed enhancing overall nitrate turnover rates in natural sediments briggs et al 2015 sawyer 2015 additionally it is known that organic carbon and respiration rates can vary in space and time and are not uniformly distributed as assumed as part of the pass model e g jorgensen 1977 cook et al 2006 reeder et al 2018 the biogeochemical reaction kinetics assumed for ar ni dn and am together with the choice of parameter values evaluated here are as follows under the assumptions mentioned above concentrations of oxygen co2 ml 3 ammonium cnh4 ml 3 and nitrate cno3 ml 3 along streamlines evolve with water parcel age τ t in advective flow according to the following set of mass balance equations 20 d d τ c o 2 c n o 3 c n h 4 r a r 2 r n i r n i r d n r a m r n i monod rate expressions based on analyses of field and laboratory data fossing et al 2004 were used for aerobic respiration rar ml 3t 1 ammonification ram ml 3t 1 nitrification rni ml 3t 1 and denitrification rdn ml 3t 1 21 r a r r min c o 2 c o 2 k o 2 s a t 22 r a m 1 γ c n r min 23 r n i k n i c o 2 c n h 4 24 r d n θ o 2 i n h κ r min c n o 3 c n o 3 k n o 3 s a t θ o 2 i n h k o 2 i n h c o 2 k o 2 i n h with r min ml 3t 1 as the respiration rate of sediment organic matter γ c n represents a constant specific for ammonification k n i ml 3t 1 a biomolecular nitrification rate constant the half saturation constants k o 2 s a t k n o 3 s a t k o 2 i n h ml 3 for aerobic respiration denitrification and oxygen inhibition and an empirical factor κ 0 05 prior to solving this system of nonlinear ordinary differential equations equation 20 it is necessary to obtain streamlines and evaluate water parcel ages τ numerically based on the linear velocity distributions equations 18 and 19 this is done by using the matlab routine ode45 based on a 4th order runge kutta scheme shampine and reichelt 1997 3 applications besides a purely generic benchmark scenario that was set up to test the computational efficiencies of the model presented in the supplement we selected two different applications of hyporheic flow problems a flume experiment at the scale of a few centimeters described previously by kessler et al 2012 and a constructed riffle structure several meters long and described by kasahara and hill 2006 both applications and the benchmark scenario exhibit the elevation variation of a stream streambed interface z x that can be described in analytical form and illustrate the selection of the small parameters z 0 ε and f x as shown in table 1 3 1 flume experiment in a recirculating flume sediment was added and artificially sculpted into ripples kessler et al 2012 the streambed topography is represented by a sinusoidal function with a wavelength of λ z 0 1 m and amplitude of 0 005 m the head distribution at the surface water interface h x is represented by another sinusoidal function with identical wavelength and shifted phase the head minimum is located approximately 2 mm downstream of the ripple crest as expected for systems with moderate reynolds numbers re 3000 in flume experiments cardenas and wilson 2007 the amplitude of h x corresponds to a total pressure difference of 2 pa that was reported by kessler et al 2012 for the flume example z x is represented by an analytical function table 1 that has to be re structured into an appropriate form that is equal to equation 7 for which ε f x and z 0 can be derived this can be achieved by factoring out the constant z 0 0 3 m from z x leading to ε 1 60 and f x cos 2 π x λ z table 1 the 1st order approximate solution for the spatial head distribution h x z is derived assuming a homogenous and isotropic streambed material with k x k z 33 86 m d and porosity of θ 0 35 kessler et al 2012 nitrate removal in the hyporehic zone was simulated using in stream concentrations for oxygen co2 stream 2 2 10 1 mol m³ nitrate c no3 stream 5 1 10 2 mol m³ and ammonium c nh4 stream 5 0 10 3 mol m³ these values are used as initial conditions to solve equation 20 numerically in stream concentrations are identical to the values originally extracted from kessler et al 2012 and later used by azizian et al 2015 in order to analytically simulate hyporheic flow in the flume as potential flow and nitrate transformation for the flume experiment were simulated previously by kessler et al 2013 and azizian et al 2015 the results derived from the 1st order approximate solution directly can be compared kessler et al 2013 used numerical comsol simulations based on experimental observations to virtually represent the conditions in the flume while azizian et al 2015 applied the advective pumping model apm proposed and experimentally validated by elliot and brooks 1997a 1997b the apm model represents an analytical solution to the laplace equation in a truncated rectangular flow domain and is practically identical to the toth solution but without re scaling the derived solution back into the actual physical flow domain 3 2 constructed riffle a riffle structure reported by kasahara and hill 2006 and referred to as kh was constructed in a lowland stream in ontario canada to apply the p pass to the riffle structure we assumed 1 the streambed is homogenous although kh noted a high degree of sediment heterogeneity 2 streambed topography z x and the upper boundary head distributions h x can be represented by superposition of sinusoidal functions function coefficients were fit to the streambed and head distribution data presented by kh see table 1 3 the model uses a steady state head distribution h x that accounts for hydrostatic pressure variations only dynamic pressure variations for the riffle structure are ignored 4 nitrate transport simulations were carried out assuming that all biogeochemical parameters including in stream concentrations for oxygen nitrate and ammonium are the same as those described earlier for the flume simulations and 5 except for the upper streambed stream interface all other boundary conditions are no flow boundaries which is a simplification originally made by kasahara and hill 2006 as part of their modflow simulations in the latter it is demonstrated how to derive the perturbation parameter ε for non trivial cases like the riffle system the bed form topography for the constructed riffle z x is approximated by a combination of several sine and cosine terms with weights a i b i i 0 1 2 l and a uniform frequency w l 1 25 z x a 0 a 1 cos w x b 1 sin w x a 2 cos w x b 2 sin w x derivations of f x z 0 and ε from equation 25 start by factoring out the leading term a 0 to be re arranged into a form that corresponds to equation 7 26 z x a 0 1 1 a 0 a 1 cos w x b 1 sin w x a 2 cos w x b 2 sin w x the final step is to identify and factor out the term of largest magnitude with absolute value δ max a 1 b 1 a 2 b 2 resulting in equation 27 27 z x a 0 1 δ a 0 a 1 δ cos w x b 1 δ sin w x a 2 δ cos w x b 2 δ sin w x by comparing equation 27 with equation 7 one obtains the small parameter ε δ a 0 and a function f x a 1 δ cos w x b 1 δ sin w x a 2 δ cos w x b 2 δ sin w x the value of z 0 follows directly from equation 27 by comparing it with equation 7 which leads to z 0 a 0 in case of the riffle example table 1 4 results 4 1 flume experiment spatial head distributions derived from the 1st order approximate solution together with simulated concentration and turnover plots are shown in fig 2 for the rippled bed forms according to the head distribution and the hyporheic exchange flux fig 2a infiltration of stream water preferentially occurs below the troughs while exfiltration occurs below the crest structures the reason for this spatial in and exfiltration pattern is the shifted minimum and maximum of the superimposed head function h x relative to the streambed topography z x fig 2c the upwelling areas below the crest structures in literature are referred to as chimneys e g kessler et al 2012 in that they draw deeper anoxic water up to the surface fig 2d f and h those chimneys directly connect the deeper anoxic areas of the hyporheic zone with the stream water which can be seen in the concentration profiles in fig 2d f and h aerobic respiration and nitrification rates are highest within the upper 4 5 cm and reduce to zero as oxygen is depleted denitrification rates are highest in a crescent shaped interfacial region roughly located at the boundary of oxic and anoxic regions fig 2g the spatial distributions of redox sensitive species predicted by our 1st order approximate solution are consistent with the comsol numerical results presented in kessler et al 2013 and the analytical pass results presented by azizian et al 2015 both simulations also indicate that nitrate concentration is stable only in the upper 5 7 cm below trough structures and both models also show the formation of the anoxic chimneys below the ripple peaks simulated and measured oxygen profiles presented in kessler et al 2013 are also similar to our model simulations in both cases near the sediment water interface interstitial fluid oxygen concentrations are highest in the downwelling zones and lowest in the upwelling areas fig 2 differences among these different models are attributed to the use of 1 a more realistic ripple topography in the comsol simulations 2 an idealized ripple topography i e a sinusoidal function in the 1st order approximate solution and 3 a truncated rectangular model domain in the pass model in addition the comsol model accounts for hydrodynamic dispersion while both analytical approaches only account for advective solute transport both the pass model and the 1st order approximate solution predict symmetrical concentration fields while those simulated with comsol are asymmetric an asymmetric flow field as predicted by the comsol simulation represents a more realistic situation as expected for a turbulent streamflow over a rippled streambed azizian et al 2015 asymmetric flow fields can be simulated with the 1st order approximation by using the non idealized bedform topography and head distributions from the comsol simulations instead of the idealized sinusoidal functions this was not done because the main purpose was to compare the results directly to the pass model outputs that use the sinusoidal representation of topography and pressure variations a quantitative comparison of hyporheic exchange and the average steady state mass turnover rates of nitrate for the pass model the comsol simulations and the 1st order approximate solution table 2 yields that the 1st order approximate solution predicts an average mass turnover rate that lies much closer to the once predicted by the numerical comsol simulations compared to the pass model the more simplified representation of the streambed topography as a flat surface as done by the pass model seems to result in an underestimation of average nitrate turnover rates hyporheic water exchange as predicted by the pass simulation and the 1st order approximation differs by 6 both analytical models predict that subsurface residence times follow a log normal distribution simulated distributions are shown in the supplement however the mean residence time and the associated standard variation are higher for the 1st order approximation compared to the pass simulation 4 2 constructed riffle the spatial distribution of nitrate for the riffle example shown in fig 3 e exhibits similar patterns to those described for the flume experiment namely anoxic chimneys are evident in both upwelling areas and the highest denitrification rates are confined to a crescent shaped zone located just below the oxic anoxic interface fig 3f similar to the upwelling chimneys below the ripples spatial head distributions and streamlines in fig 3c and d indicate the existence of a shallow upwelling zone that is responsible for the generation of an upwelling chimney in the case of lateral and basal no flow boundaries hyporheic flow cells extend down to a depth of z 0 and hyporehic exchange takes place for the entire model domain simulated hyporheic water exchange and nitrate turnover table 3 only slightly deviate for the toth and the 1st order approximation solution as kasahara and hill 2016 used modflow simulations to represent hyporheic flow only a comparative value for nitrate turnover is not available in addition hyporehic exchange of water for the modlfow simulation table 3 are not directly comparable to the ones obtained from the toth 1st order approximation as the numerical model was set up using a heterogeneous material distribution in the subsurface areas of lower hydraulic conductivities in the modlfow simulations presented by kasahara and hill 2016 reduce advective velocities in the subsurface and increase hyporheic residence times the latter reduces hyporheic exchange rates between the stream and the subsurface which are higher for the toth and 1st order approximation both analytical approaches predict gamma distributed subsurface residence times simulated distributions are shown in the supplement with a higher mean value and standard deviation for the toth approximation 4 3 accuracy of the 0th and 1st order approximations the purpose of this section is 1 to evaluate the accuracy of the derived 1st order approximation solutions for the application and the benchmark examples as well as 2 to investigate to what degree the accuracy of the flow solutions increase if the fully 1st order approximation solution equation 11 is used to describe hyporheic flow instead of only using the much simpler derived 0th order approximation equation 12 by only accounting for the 0th order correction term the resulting solution is identical to the toth solution if re scaled back into the physical domain zlotnik et al 2015 to evaluate accuracy we considered two different metrics namely the ability of the model to close the volume balance on water circulating through the hyporheic zone and the mass of nitrate that is either produced or consumed within the hyporheic domain the first metric can be expressed as a dimensionless relative error e r q i n q o u t q i n water fluxes that enter q i n l2t 1 or leave q o u t l2t 1 the model domain can be estimated by sequentially integrating the darcy fluxes q q x q z t lt 1 see si for details the second metric net mass turnover for nitrate m no3 ml 1t 1 is the difference between the entering mass fluxes no 3in ml 1t 1 and exiting mass fluxes no 3out ml 1t 1 of the model domain these latter two parameters can be estimated from q in and q out and from the p pass model simulations for nitrate turnover see si for details in general relative errors for the 0th order representations are as expected consistently higher compared to the 1st order approximations table 4 in case of the benchmark scenario the water balance errors associated with the 0th order approximation are already diminishingly small table 4 and by including the 1st order correction term in the perturbation series the accuracy only increases marginally here a simple 0th order approximation would be adequate to achieve a high level of accuracy in closing the water balance in the case of the constructed riffle by additionally accounting for the 1st order correction term the water balance error decreases from 3 8 to below 1 table 4 for the flume experiment the water balance error obtained with the 0th order approximation is very large 63 indicating a high imbalance between in and outgoing water fluxes caused by an inaccurate representation of hyporheic flow by additionally including the 1st order correction term the error drops dramatically to below 1 a similar improvement in accuracy was reported by zlotnik et al 2015 for hydrological systems at the catchment scale for the flume experiment the biogeochemical simulations based on the 0th order approximation predict that the ripple structures are a net source for nitrate with a net production rate of 38 μmol m 1 d 1 however the more accurate 1st order solution indicates that the rippled bedforms are a net sink of nitrate with a net consumption rate of 138 μmol m 1 d 1 which is consistent with experimental observations and comsol modeling studies performed by kessler et al 2012 and pass modeling simulations performed by azizian et al 2015 although kessler et al 2012 explicitly stated that they were unable to experimentally calculate the bulk flux from the sediment or measure n2 production we think that the 1st order approximation solution represent the sediment s functional role in biogeochemical cycling in this case predicting that the sediments are a net sink of nitrate more correctly compared to the 0th order approximation with its large error in closing the water balance 4 4 sensitivity analysis and limitations this analysis aims to understand how the accuracy of the approximate solutions is influenced by typical characteristics of the flow domain such as the maximum elevation difference of the bedform z max z min and the depth of the horizontal plane z 0 with this analysis we intend to identify specific types of flow problems that may lead to inaccurate solutions according to equation 28 and as outlined in equations 25 27 the perturbation factor for the riffle structure depends on the term with the largest magnitude in the sine cosine series b 1 and the height of the horizontal plane z 0 28 ε b 1 z 0 for the sensitivity analysis the original flow domain of the riffle structure is modified in two different ways both leading to an increase of the perturbation factor ε 1 an increase in ε can be achieved by decreasing the depth of the horizontal plane z 0 while the shape of the bedform topography remains the same z max z min const 2 increasing b 1 in equation 28 will also lead to an increase of ε while this time z 0 remains constant the latter modification leads to a change in the shape of the bedform where the elevation difference z max z min is increasing representing a steeper topographical gradient results of this analysis are shown in fig 4 were the water balance errors are plotted against the respective perturbation factors the dashed lines in fig 4 represent simulations where the depth of the horizontal plane z 0 was modified to achieve a change in ε the solid lines are associated with simulations where z 0 remains constant while the shape of the bedform was modified by increasing b 1 as expected the 0th order solutions always are associated with higher water balance errors compared to the respective 1st order solutions for higher ε values approximate solutions with identical bedforms but different z 0 are more accurate 10 compared to the simulations with steeper bedforms here results clearly indicate that the accuracy of the approximate solutions is more sensitive to a change in the elevation difference compared to the depth of the horizontal plane elevation differences for the 1st order approximation associated with the highest mass balance error 47 lies around 1 7 m which is probably a quite unrealistic scenario for most hyporheic systems for the riffle example with the original value for z 0 a maximum elevation differences that exceed 0 5 m leads to water balance errors above 10 here a 1st order approximation is not accurate enough and higher order solutions have to be considered by accounting for additional terms in the perturbation series equation 10 leading to improved accuracies 5 discussion for hyporheic systems virtually all aspects of hyporheic exchange of water solutes and energy are controlled by site specific topographical variations of the streambed the vertical extent of hyporheic flow cells e g boano et al 2010a marzadri et al 2010 trauth et al 2013 locations of stagnation points bottacin busolin and marion 2010 hyporheic exchange fluxes and subsurface residence times e g cardenas et al 2004 boano et al 2014 marzadri et al 2015 and other characteristics are directly or indirectly controlled by the morphological shape of the streambed channeling the water movement streambed topography also affects spatial nutrient availability in hyporehic areas precht et al 2004 cardenas 2009 kessler et al 2013 the spatial zonation of redox conditions marzadri et al 2012 and the emission of nitrous oxide from hyporheic areas marzadri et al 2014 an accurate mathematical representation of flow conditions with an explicit consideration of streambed topography therefore can be essential for a qualitative and quantitative understanding of hyporheic systems marion et al 2002 in hyporheic applications the classic toth technique represents the streambed topography of hyporheic systems as a flat surface although the toth approximations are useful for estimating the integral characteristics of solute transport e g azizian et al 2015 caruso et al 2016 wörman et al 2006 the solutions are strictly only valid in a truncated flow domain toth domain but important local aspects of topographically driven flow are missing zlotnik et al 2015 for hyporheic systems where important biogeochemical processes such as aerobic respiration or denitrification occur close to the streambed water interface flow domain truncation can lead to error prone quantifications of hyporheic water exchange subsurface residence times and nutrient turnover this was shown by comparing nitrate turnover rates predicted by models that use the toth approach pass model and by models that explicitly account for the streambed topography comsol and p pass by ignoring topographical features of the streambed the toth solutions tend to underestimate exchange of water between the stream and the hyporheic zone which seems to be especially relevant for small scale structures like the simulated ripples an explicit representation of the streambed topography in analytical models can be achieved by a consistent use of the perturbation method to describe hyporheic flow in general the 0th and 1st order approximations lead to solutions that show similar behavior in terms of simulated patterns of subsurface flow and biogeochemical turnover as expected the 1st order correction term reduces water volume and mass balance errors although the degree of improvement depends on the specific nature of the problem in question and magnitude of the perturbation parameter the question that arises is whether it is worth the extra effort of deriving a 1st order approximation by using the perturbation method compared to the easily derived 0th order approximation solution that is practically identical to a re scaled toth solution here the simulated flume provides salient example where the accuracy of the solution can be increased by the factor of 70 by additionally accounting for the 1st order correction term if the 0th order approximation is used to describe physical water flow for the flume experiment the high inaccuracies in the flow solution can lead to a very different assessment regarding the hyporheic zone s role in n cycling specifically we found that the re scaled toth solution which is equivalent to a 0th order approximation predicts that the sediments beneath ripples are a net source for nitrate when in fact numerical models and the 1st order approximation suggest that the opposite is true although discrepancies for total nitrate turnover are small for the riffle system the differences between the solutions predicted by the 0th and 1st order approximation solutions can become quite significant if those results are used to upscale the denitrification capacity at scales of entire streams or river basins further it can be expected that such inaccuracies in the flow solution are also present after applying a 0th approximation on larger scales such as catchments zlotnik et al 2015 or if approximation solutions are being used for 3d flow systems wörman et al 2006 caruso et al 2016 the accuracy of the perturbation solutions primarily depends 1 on the magnitude of the small dimensionless parameter ε that describes the deviation in elevation between a horizontal plane elevation z0 and the actual topography and 2 the number of the successive terms in the perturbation series probably in the majority of realistic hyporheic flow problems elevation differences for hyporheic structures are small and the resulting perturbation parameter lies below 0 1 for such cases the 1st order approximation achieves accuracies similar to those of conventional numerical models however for hyporheic structures that are associated with high topographical gradients like cascades or steep pool structures commonly observed for mountainous streams with reach slopes 0 03 montgomery and buffington 1997 a 1st order approximation may not be accurate enough as was shown by the sensitivity analysis for the latter systems the accuracy can be increased by accounting for additional terms in the perturbation series however this will also increase the level of complexity of the resulting solution further it must be mentioned that a 2d representation of the real system of course always represents a simplification this simplification might not be uniformly appropriate for situations with complex 3d structures of the streambed which cannot simply be represented in a 2d model so far the presented matlab model is limited to 2d steady state flow scenarios with a homogenous isotropic or anisotropic streambed the approach has potential for generalization to 3d simulations of problems like those presented by marklund and wörman 2011 and possibly to heterogeneous conditions like those discussed by craig 2008 and wond and craig 2010 streambed heterogeneity can have an important influence on hyporheic exchange characteristics pryshlak et al 2015 hyporheic residence times tonina et al 2016 and can be responsible for the formation of biogeochemical hot spots gomez velez et al 2014 as material heterogeneity can affect physical movement of water and biogeochemical degradation equally an accurate representation in the p pass model is needed the latter will be addressed in future research finally transient cases can be considered as well zlotnik et al 2015 the model also can be extended to the hyporheic systems that are affected by lateral or vertical groundwater exchange cardenas et al 2004 where ambient fluxes can significantly alter the hyporheic flow cells boano et al 2008 or even can totally suppress hyporheic flow trauth et al 2013 based on the results of this study it is well justified to use the perturbation method as a powerful tool in for the mathematical description and understanding of the hyporheic systems approximate solutions offer an advanced alternative to the classical toth solutions or the apm model as they explicitly present the streambed topography and its impact on local hyporheic flow and transport conditions a major advantage of the presented technique compared to numerical models is its flexibility and usability an inexperienced user only has to define spatial head variations and streambed topography in functional form in addition to the aquifer parameters thereby avoiding a grid development which is laborious task without specialized software numerical as well as analytical models always need the upper forcing term as input to represent hyporheic flow processes here the important task remains namely derivation of the upper head distribution above the streambed interface which controls the hyporheic exchange either through cfd modeling or observations this is an essential and difficult step in majority of field studies only a few measurement points on bedform topography are available e g data used to set up the modflow simulation presented in kasahara and hill 2016 however it is known that small topographical features of the streambed that are easily smoothed out by parametrization or discretization can have a significant impact on hyporheic flow processes the latter affects both analytical as well as numerical models nevertheless if a highly spatially resolved bedfrom topography is available it also can be used as part of the p pass model once known measured streambed elevations and upper forcing terms easily can be implemented into the model by superposition of sinusoidal functions with as many terms as needed to account for all the necessary details and for which the problem specific perturbation parameter ε can be derived generally implementation of this type of data requires less effort with analytical approaches sensitivity analysis where multiple bedform shapes are being represented also can be performed much more efficiently with analytical approaches in a numerical model such as comsol or modflow replacing the topography is always associated with setting up a new computational grid which is a time consuming procedure a typical example hyporheic flow and nutrient turnover for the riffle takes only about 15 20 min for the parallel matlab code running on 5 cpus 6 conclusions 1 representing topography and the head distribution of the sediment water interface correctly is important for a quantitative assessment of hyporheic exchange and nutrient cycling in hyporheic systems the commonly used toth approach of flow domain truncation may lack necessary accuracy and can introduce computational artifacts 2 the perturbation method offers a way to substantially improve local representation of flow transport and hyporheic exchange analyses within and beneath the ripples by explicitly accounting for a spatially varying topography and the head distribution at the boundary 3 the presented matlab based model is capable of representing flexible 1st order approximate solutions for user defined 2d hyporheic systems the mass balance errors are comparable to numerical solutions but do not require laborious numerical grids and offer more flexibility for users with little modeling background acknowledgments the original matlab source code of the p pass model can be obtained by contacting the corresponding author or alternatively downloaded from http www hydro uni bayreuth de hydro de software software software dl php we thank the editorial committee of environmental modelling and software and the anonymous reviewers who provided many useful suggestions that improved the original manuscript we gratefully acknowledge financial support from the german research foundation dfg fr 2858 2 1 appendix a supplementary data the following are the supplementary data to this article data profile data profile manuscript docx 37 46 manuscript docx 37 46 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 015 
26299,hyporheic flow and nutrient turnover in hyporheic systems are strongly influenced by in stream bedforms an accurate representation of topographical variations of the stream streambed interface is therefore essential in analytical models in order to represent the couplings between hydrological and biogeochemical processes correctly the classical toth approach replaces the streambed surface topography by a flat surface which is identical to a truncation of the original physical flow domain into a rectangle this simplification can lead to biased estimates of hyporheic flow and nutrient cycling within hyporheic systems we present an alternative analytical modeling approach for solving hyporheic problems without domain truncation that explicitly accounts for topographical variations of the streambed the presented approach is based on the application of perturbation theory applications of the method to hyporheic systems ranging from the centimeter scale of rippled bedforms to riffle structures of 10 m and larger scale indicate a high accuracy of the approach keywords analytical modeling perturbation method first order approximation hyporheic flow biogeochemical transformations 1 introduction exchange of water and solutes across the streambed interface and biogeochemical cycling of nutrients in hyporheic areas are important factors controlling the water quality of rivers and streams different approaches are used in hydrological models to represent exchange of solutes and water across the hyporheic interface boano et al 2014 in models used on large scale river networks and catchments the effect of riparian and hyporheic areas on nutrient removal are often represented by spatially lumped sink terms e g rassam et al 2007 by applying transient storage concepts e g pelletier et al 2006 or are neglected entirely marsili libelli and giusti 2008 bouadi et al 2017 in physically based models flow and degradation processes are simulated based on physical laws by an explicit consideration of spatially distributed system properties analytical models traditionally have been very important for conceptual understanding of hydrological systems analytical models may not be as efficient as their numerical counterparts in treatment of site specific characteristics like aquifer heterogeneity or the shapes and types of boundary conditions however they assist in mathematical description of the physical movement of water in hydrological systems e g wörman et al 2006 craig et al 2008 marklund and wörmann wond and craig 2010 the classic work of toth 1962 who investigated gravity driven regional flow is one of the prominent examples salient hydrodynamic features of the concept introduced by toth are mainly based on three traits of the regional systems 1 they have a near rectangular or near parallelepiped domain with a curvilinear top boundary where the magnitude of undulations is much smaller than the vertical scale of the flow domain 2 the lateral boundaries and the aquifer base are impermeable as appropriate for analyses of groundwater basins and 3 head values at the phreatic surface locations define the upper boundary which is a characteristic assumption for unconfined aquifers in hyporheic research the original ideas introduced by toth to analytically solve the governing equations for hyporheic flow for a rectangular flow domain by neglecting topographical variations of the streambed are frequently used in analytical models the first analytical representation of hyporheic flow elliot and brooks 1997a often referred to as advective pumping model apm considered periodical head and velocity distributions under the horizontal streambed surface that approximates periodical undulating ripples acceptance of the horizontal interface in the apm model was identical to toth s approach in the apm model periodical pressure changes were taken as major drivers of the water fluxes across the hyporheic interface applications of the apm approach involved studies of bedform affected hyporheic water and material exchange e g bottacin busolin and marion 2010 packman and brooks 2001 marion et al 2002 boano et al 2007 2008 wörmann et al 2007 chemical transformation of contaminants grant et al 2014 and hyporheic cycling of nutrients marzadri et al 2012 2014 azizian et al 2015 stohnedahl et al 2012 the original apm approach later was modified to consider the dynamical changes in stream velocity boano et al 2007 and was extended to 3d by wörman et al 2006 and stohnedahl et al 2010 toth s approach and the apm model both have in common that they shift all head values from the original curvilinear boundary condition to a horizontal boundary permitting a closed form analytical solution in the truncated model domain by separation of variables technique although various authors mentioned that the flat bed assumption for their specific cases only had a minor effect on hyporheic flow characteristics for dune like morphologies elliot and brooks 1997a b gravel bars marzadri et al 2010 or riffle pool structures tonina and buffington 2007 toth type solutions do not present processes that occur in the actual physical domain or inside small scale structures like ripples zlotnik et al 2015 this artifact stems from inconsistent application of the perturbation technique of boundary deformation which is well known in fluid dynamics e g van dyke 1975 similarly if used as the basis for estimating hyporheic flow or subsurface biogeochemical reactions the flat bed assumption may also introduce errors in the calculations due to the missing topography that are difficult to quantify or interpret zlotnik et al 2015 an analytical approach that circumvents the problems associated with domain truncation by explicitly accounting for topographical variations of the streambed therefore would represent a valuable addition to existing analytical techniques in hyporheic research to address the problems of introduced artifacts that are associated with domain truncation and in order to accurately represent effects of topography zlotnik et al 2015 proposed consistent use of the perturbation theory for obtaining analytical approximate solutions that are valid for the entire un truncated flow domain and verified their accuracy and functionality with a numerical finite element solution comsol using an example of flow through a unit basin a similar approach of using approximate solutions can be universally applied to any 2d or 3d steady state or transient toth type solutions including solutions derived from the apm model the main purpose of this study is to highlight the opportunities that are offered by applying the perturbation method in order to explicitly represent the streambed topography and its influence on flow and nutrient cycling in analytical hyporheic flow models for this purpose we developed a matlab modeling environment that is capable of deriving flexible 1st order approximate solutions in order to represent arbitrary curvilinear flow domains the code is primarily intended and optimized to work at scales of in stream bedforms such as dunes ripples or riffle pool structures but also can be used for at the scales of hill slopes or catchments with special emphasis on in stream structures and their crucial role for nitrate cycling the matlab code also includes a reactive transport routine based on a pumping and streamline segregation model this type of models use a hyporheic flow field derived from analytical or numerical solutions and represent kinetically controlled bio geochemical processes along isolated streamlines the pumping and streamline segregation approach has been widely applied in the field of hyporheic research in order to simulate important nutrient turnover processes in the hyporheic zone e g boano et al 2010b marzadri et al 2011 azizian et al 2015 the matlab code is parallelized using the pmatlab libraries bliss and kepner 2007 to improve the overall model performance and to reduce simulation runtimes because of the analytical nature of the presented model it does not require a computational grid thereby offering high flexibility and easy use our model referred to by the acronym p pass perturbation pass is applied to various hyporheic systems including 1 hyporheic flow through small scale ripple structures typical of a flume experiment and 2 artificially constructed riffle pool structures for these systems with scales that differ by almost two orders of magnitude the p pass model is used to analyze the coupling between flow and biogeochemical cycling of nitrate in the hyporheic zone results are compared with predictions obtained by the apm toth model and numerical simulations by using perturbation theory our p pass model combines the best of both worlds the simplicity of analytical solutions and a more faithful representation of surface topography 2 methodology prior to introducing the p pass model for hyporheic water flow simulations we briefly summarize the major aspects of the perturbation method in section 2 1 in section 2 2 a short introduction into the biogeochemical component of the model is presented information about the model implementation and parallelization in matlab along with a benchmark analysis is relegated to the supplement online 2 1 representing subsurface flow 2 1 1 perturbation method and approximate solutions the interested reader is referred to the publications of van dyke 1975 and bender and orszag 1999 providing a detailed and comprehensive introduction into perturbation here we briefly introduce the important steps of the perturbation method that are needed for our applications asymptotic and perturbation analysis produces approximate analytical solutions to differential and difference equations that cannot be solved exactly or in a closed form analytical expression the general idea of applying the perturbation method is to introduce a small dimensionless parameter the so called perturbation parameter ε into the governing equations the general procedure of solving ordinary or partial differential equations by applying the perturbation method can be subdivided into three major steps bender and orszag 1999 step 1 convert the original problem into a perturbation problem by delineating a small perturbation parameter ε step 2 assume an expression for the answer in the form of an infinite truncated perturbation series by this parameter and derive equations for the coefficients of these series step 3 recover the answer of the original problem by using the perturbation series for the given value of ε commonly the leading term in the perturbation series 0th order term is obtainable in a closed form because generally it is this term that ensures the success of the perturbation method in deriving accurate analytical approximate solutions bender and orszag 1999 as it was done by toth 1962 in the following section we will describe how the perturbation parameter ε is introduced into a general formulation of hyporheic flow problems and how to derive the corresponding perturbation series further we will explain how to estimate the coefficients of that series and finally how to derive an approximate solution for hyporheic flow 2 1 2 the boundary value problem 2d steady state flow in a uniform anisotropic aquifer with saturated hydraulic conductivities k x lt 1 and k z lt 1 and a spatial hydraulic head distribution h x z l can be described as follows 1 k x 2 h x 2 k z 2 h z 2 0 a flow domain is bounded by a spatially varying top boundary z x l and impermeable lateral and base boundaries fig 1 a 2 h 0 z x 0 0 z z 0 3 h l z x 0 0 z z l 4 h x 0 z 0 0 x l 5 h x z x h x 0 x l the head distribution on the upper boundary h x is a function either known from observations over the streambed or more often from computational fluid dynamics cfd simulations lateral and bottom no flow boundary conditions as used in the original work of toth when applied to hyporheic systems probably represent some oversimplification hyporheic systems under real conditions can be influenced by ambient groundwater flow and underflow conditions following a local topographical gradient e g boano et al 2008 fox et al 2014 marzadri et al 2015 nevertheless for deriving the governing hyporheic flow equations and various applications these simplifications are still valid toth 1962 solved a similar problem for gravity driven flow where h x z x by assuming that z x slightly deviates from the horizontal plane z z 0 or z x z 0 that resulted in a modified equation 5 as follows 6 h x z 0 z x 0 x l this approach reduces the domain to a rectangle in the vertical plane the so called toth domain for which equation 1 can be solved analytically for various functions z x along the boundary z z 0 fig 1b zlotnik et al 2015 investigated this approach using consistent application of the boundary deformation perturbation method e g van dyke 1975 and demonstrated the resulting advantages and deficiencies the problem can be solved more accurately utilizing the fact that the undulations of the streambed boundary in hyporheic processes occur in a small range between the highest elevation z max and lowest elevation z min in other words the range of elevations is much smaller than the characteristic thickness of the hyporheic zone z0 z max z min z 0 in general the upper boundary can be represented as follows 7 z x z 0 1 ε f x in equation 7 ε represents a small dimensionless perturbation parameter that together with f x l describes the spatial deviation in elevation between some horizontal elevation z0 and the actual topography z x fig 1 obeying no flow conditions at the basin boundaries d f 0 d x d f l d x 0 2 1 3 scaling of the problem and approximate solution the flow domain transformation from a curvilinear into a unit rectangular flow domain of the dimensions 0 x 1 0 z z 0 is achieved by scaling 8 x x l z z z 0 z x z 0 a z 0 l a k x k z where x and z are dimensionless coordinates l l is the length of the flow domain fig 1 and z 0 is the scaled value of z 0 accounting for anisotropy via the dimensionless parameter a after substitution of these variables into equation 1 and applying the chain rule to the partial derivatives in the new coordinates the resulting pde in scaled coordinates yields see zlotnik et al 2015 for a detailed derivation 9 2 h x 2 ε b 2 h x z 1 ε c 2 h z 2 ε d h z 0 b 2 l d f l x d x e l x 1 c ε l 2 z 2 d f l x d x 2 2 f l x ε f l x 2 1 e 2 l x d z l 2 2 ε d f l x d x 2 d 2 f l x d x 2 1 ε f l x 1 e 2 l x contrary to equation 1 this partial differential equation cannot be solved exactly by analytical techniques however as equation 9 contains small perturbation ε defined by equation 7 it can be solved approximately by applying perturbation method the solution to equation 9 can be given in form of a power series of this parameter 10 h x z n 0 n h n x z ε n here n is the number of retained terms in the perturbation series the coefficients h n x z can be determined by substituting this equation for h x z into equation 9 and the boundary conditions and collecting terms with the same powers of the parameter ε the accuracy of the approximate solution depends on the magnitude of ε and the number of successive terms n zlotnik et al 2015 showed that for a typical flow problem described by equation 1 and subject to the applied boundary conditions equation 2 to equation 5 a 1st order approximation n 1 for ε 1 is accurate enough for practical purposes while terms of order n 2 are neglected 11 h x z h 1 x z ε h 1 x z the 0th order term h 0 x z can be derived from solving equation 9 by setting ε 0 which reduces the problem to a standard laplace equation where the solution can be written in the form of an infinite series 12 h 0 x z c 0 0 2 m 1 c m 0 cos π m x cosh π m z cosh π m z 0 with the coefficients c m 0 for the arbitrary head function h x as follows 13 c m 0 1 l 0 l h x cos π m x l d x after finding the function f x z defined as follows 14 f x z 2 l z f l x h 0 x z z x 2 f l x 2 h 0 x z z 2 f l x z l 2 h 0 x z z the 1st order term h 1 x z can be found using separation of variables as follo 15 h 1 x z m 0 n 0 c n m 1 cos π m x cos π n 1 2 z z 0 with coefficients c n m 1 defined as 16 c 1 n m 4 1 δ m 0 λ m n l 0 l 1 z x 0 z x f x l z z 0 z x cos π m x l cos π n 1 2 z z x d z d x c n m 1 4 1 δ m 0 λ m n l 0 l 1 z x 0 z x f x l z z 0 l cos π m x l cos π n 1 2 z z x d z d x where 17 λ m n π 2 m 2 n 1 2 2 z 0 2 δ n m 1 n m 0 n m in equation 17 δ n m represents the kronecker delta function once h 0 x z and h 1 x z are known an approximate 1st order solution to the original flow problem equation 1 is obtained in transformed coordinates the solution in the original system of coordinates can be obtained by replacing the scaled variables x and z with the expressions shown in equation 8 h x z h x z which together with the problem specific perturbation parameter ε leads to the final approximate solution h x z based on h x z linear velocity components v x x z v z x z lt 1 which are necessary for particle tracking and the reactive transport simulations can be derived 18 v x x z k x θ d h d x 19 v z x z k z θ d h d z here θ represents the streambed porosity 2 2 reactive transport model for representation of nitrogen cycling simulated biogeochemical reactions include aerobic respiration ar nitrification ni denitrification dn and ammonification am in the hyporheic zone the coupling of flow and reaction is implemented using the pass model azizian et al 2015 in which oxygen ammonium and nitrate concentrations undergo advective transport and reaction in the hyporheic zone assuming negligible molecular diffusion and mechanical dispersion coefficients according to the so called segregated streamline hypothesis rawlings and ekerdt 2013 in pass biogeochemical processes are represented along advective flow lines a conceptually identical approach to what has been used in the past to describe hyporheic nutrient turnover e g boano et al 2010b marzadri et al 2011 the magnitude of errors associated with neglecting mixing by molecular diffusion and mechanical dispersion in the pass model are in detail discussed in grant et al 2014 the biogeochemical model does not account for the possible formation of redox micro zones as a consequence of material heterogeneity or spatially variable patterns of organic carbon and respiration rates material heterogeneity in the hyporheic area can lead to the formation of redox micro zones such as localized pockets of denitrification in oxygenated areas of the streambed enhancing overall nitrate turnover rates in natural sediments briggs et al 2015 sawyer 2015 additionally it is known that organic carbon and respiration rates can vary in space and time and are not uniformly distributed as assumed as part of the pass model e g jorgensen 1977 cook et al 2006 reeder et al 2018 the biogeochemical reaction kinetics assumed for ar ni dn and am together with the choice of parameter values evaluated here are as follows under the assumptions mentioned above concentrations of oxygen co2 ml 3 ammonium cnh4 ml 3 and nitrate cno3 ml 3 along streamlines evolve with water parcel age τ t in advective flow according to the following set of mass balance equations 20 d d τ c o 2 c n o 3 c n h 4 r a r 2 r n i r n i r d n r a m r n i monod rate expressions based on analyses of field and laboratory data fossing et al 2004 were used for aerobic respiration rar ml 3t 1 ammonification ram ml 3t 1 nitrification rni ml 3t 1 and denitrification rdn ml 3t 1 21 r a r r min c o 2 c o 2 k o 2 s a t 22 r a m 1 γ c n r min 23 r n i k n i c o 2 c n h 4 24 r d n θ o 2 i n h κ r min c n o 3 c n o 3 k n o 3 s a t θ o 2 i n h k o 2 i n h c o 2 k o 2 i n h with r min ml 3t 1 as the respiration rate of sediment organic matter γ c n represents a constant specific for ammonification k n i ml 3t 1 a biomolecular nitrification rate constant the half saturation constants k o 2 s a t k n o 3 s a t k o 2 i n h ml 3 for aerobic respiration denitrification and oxygen inhibition and an empirical factor κ 0 05 prior to solving this system of nonlinear ordinary differential equations equation 20 it is necessary to obtain streamlines and evaluate water parcel ages τ numerically based on the linear velocity distributions equations 18 and 19 this is done by using the matlab routine ode45 based on a 4th order runge kutta scheme shampine and reichelt 1997 3 applications besides a purely generic benchmark scenario that was set up to test the computational efficiencies of the model presented in the supplement we selected two different applications of hyporheic flow problems a flume experiment at the scale of a few centimeters described previously by kessler et al 2012 and a constructed riffle structure several meters long and described by kasahara and hill 2006 both applications and the benchmark scenario exhibit the elevation variation of a stream streambed interface z x that can be described in analytical form and illustrate the selection of the small parameters z 0 ε and f x as shown in table 1 3 1 flume experiment in a recirculating flume sediment was added and artificially sculpted into ripples kessler et al 2012 the streambed topography is represented by a sinusoidal function with a wavelength of λ z 0 1 m and amplitude of 0 005 m the head distribution at the surface water interface h x is represented by another sinusoidal function with identical wavelength and shifted phase the head minimum is located approximately 2 mm downstream of the ripple crest as expected for systems with moderate reynolds numbers re 3000 in flume experiments cardenas and wilson 2007 the amplitude of h x corresponds to a total pressure difference of 2 pa that was reported by kessler et al 2012 for the flume example z x is represented by an analytical function table 1 that has to be re structured into an appropriate form that is equal to equation 7 for which ε f x and z 0 can be derived this can be achieved by factoring out the constant z 0 0 3 m from z x leading to ε 1 60 and f x cos 2 π x λ z table 1 the 1st order approximate solution for the spatial head distribution h x z is derived assuming a homogenous and isotropic streambed material with k x k z 33 86 m d and porosity of θ 0 35 kessler et al 2012 nitrate removal in the hyporehic zone was simulated using in stream concentrations for oxygen co2 stream 2 2 10 1 mol m³ nitrate c no3 stream 5 1 10 2 mol m³ and ammonium c nh4 stream 5 0 10 3 mol m³ these values are used as initial conditions to solve equation 20 numerically in stream concentrations are identical to the values originally extracted from kessler et al 2012 and later used by azizian et al 2015 in order to analytically simulate hyporheic flow in the flume as potential flow and nitrate transformation for the flume experiment were simulated previously by kessler et al 2013 and azizian et al 2015 the results derived from the 1st order approximate solution directly can be compared kessler et al 2013 used numerical comsol simulations based on experimental observations to virtually represent the conditions in the flume while azizian et al 2015 applied the advective pumping model apm proposed and experimentally validated by elliot and brooks 1997a 1997b the apm model represents an analytical solution to the laplace equation in a truncated rectangular flow domain and is practically identical to the toth solution but without re scaling the derived solution back into the actual physical flow domain 3 2 constructed riffle a riffle structure reported by kasahara and hill 2006 and referred to as kh was constructed in a lowland stream in ontario canada to apply the p pass to the riffle structure we assumed 1 the streambed is homogenous although kh noted a high degree of sediment heterogeneity 2 streambed topography z x and the upper boundary head distributions h x can be represented by superposition of sinusoidal functions function coefficients were fit to the streambed and head distribution data presented by kh see table 1 3 the model uses a steady state head distribution h x that accounts for hydrostatic pressure variations only dynamic pressure variations for the riffle structure are ignored 4 nitrate transport simulations were carried out assuming that all biogeochemical parameters including in stream concentrations for oxygen nitrate and ammonium are the same as those described earlier for the flume simulations and 5 except for the upper streambed stream interface all other boundary conditions are no flow boundaries which is a simplification originally made by kasahara and hill 2006 as part of their modflow simulations in the latter it is demonstrated how to derive the perturbation parameter ε for non trivial cases like the riffle system the bed form topography for the constructed riffle z x is approximated by a combination of several sine and cosine terms with weights a i b i i 0 1 2 l and a uniform frequency w l 1 25 z x a 0 a 1 cos w x b 1 sin w x a 2 cos w x b 2 sin w x derivations of f x z 0 and ε from equation 25 start by factoring out the leading term a 0 to be re arranged into a form that corresponds to equation 7 26 z x a 0 1 1 a 0 a 1 cos w x b 1 sin w x a 2 cos w x b 2 sin w x the final step is to identify and factor out the term of largest magnitude with absolute value δ max a 1 b 1 a 2 b 2 resulting in equation 27 27 z x a 0 1 δ a 0 a 1 δ cos w x b 1 δ sin w x a 2 δ cos w x b 2 δ sin w x by comparing equation 27 with equation 7 one obtains the small parameter ε δ a 0 and a function f x a 1 δ cos w x b 1 δ sin w x a 2 δ cos w x b 2 δ sin w x the value of z 0 follows directly from equation 27 by comparing it with equation 7 which leads to z 0 a 0 in case of the riffle example table 1 4 results 4 1 flume experiment spatial head distributions derived from the 1st order approximate solution together with simulated concentration and turnover plots are shown in fig 2 for the rippled bed forms according to the head distribution and the hyporheic exchange flux fig 2a infiltration of stream water preferentially occurs below the troughs while exfiltration occurs below the crest structures the reason for this spatial in and exfiltration pattern is the shifted minimum and maximum of the superimposed head function h x relative to the streambed topography z x fig 2c the upwelling areas below the crest structures in literature are referred to as chimneys e g kessler et al 2012 in that they draw deeper anoxic water up to the surface fig 2d f and h those chimneys directly connect the deeper anoxic areas of the hyporheic zone with the stream water which can be seen in the concentration profiles in fig 2d f and h aerobic respiration and nitrification rates are highest within the upper 4 5 cm and reduce to zero as oxygen is depleted denitrification rates are highest in a crescent shaped interfacial region roughly located at the boundary of oxic and anoxic regions fig 2g the spatial distributions of redox sensitive species predicted by our 1st order approximate solution are consistent with the comsol numerical results presented in kessler et al 2013 and the analytical pass results presented by azizian et al 2015 both simulations also indicate that nitrate concentration is stable only in the upper 5 7 cm below trough structures and both models also show the formation of the anoxic chimneys below the ripple peaks simulated and measured oxygen profiles presented in kessler et al 2013 are also similar to our model simulations in both cases near the sediment water interface interstitial fluid oxygen concentrations are highest in the downwelling zones and lowest in the upwelling areas fig 2 differences among these different models are attributed to the use of 1 a more realistic ripple topography in the comsol simulations 2 an idealized ripple topography i e a sinusoidal function in the 1st order approximate solution and 3 a truncated rectangular model domain in the pass model in addition the comsol model accounts for hydrodynamic dispersion while both analytical approaches only account for advective solute transport both the pass model and the 1st order approximate solution predict symmetrical concentration fields while those simulated with comsol are asymmetric an asymmetric flow field as predicted by the comsol simulation represents a more realistic situation as expected for a turbulent streamflow over a rippled streambed azizian et al 2015 asymmetric flow fields can be simulated with the 1st order approximation by using the non idealized bedform topography and head distributions from the comsol simulations instead of the idealized sinusoidal functions this was not done because the main purpose was to compare the results directly to the pass model outputs that use the sinusoidal representation of topography and pressure variations a quantitative comparison of hyporheic exchange and the average steady state mass turnover rates of nitrate for the pass model the comsol simulations and the 1st order approximate solution table 2 yields that the 1st order approximate solution predicts an average mass turnover rate that lies much closer to the once predicted by the numerical comsol simulations compared to the pass model the more simplified representation of the streambed topography as a flat surface as done by the pass model seems to result in an underestimation of average nitrate turnover rates hyporheic water exchange as predicted by the pass simulation and the 1st order approximation differs by 6 both analytical models predict that subsurface residence times follow a log normal distribution simulated distributions are shown in the supplement however the mean residence time and the associated standard variation are higher for the 1st order approximation compared to the pass simulation 4 2 constructed riffle the spatial distribution of nitrate for the riffle example shown in fig 3 e exhibits similar patterns to those described for the flume experiment namely anoxic chimneys are evident in both upwelling areas and the highest denitrification rates are confined to a crescent shaped zone located just below the oxic anoxic interface fig 3f similar to the upwelling chimneys below the ripples spatial head distributions and streamlines in fig 3c and d indicate the existence of a shallow upwelling zone that is responsible for the generation of an upwelling chimney in the case of lateral and basal no flow boundaries hyporheic flow cells extend down to a depth of z 0 and hyporehic exchange takes place for the entire model domain simulated hyporheic water exchange and nitrate turnover table 3 only slightly deviate for the toth and the 1st order approximation solution as kasahara and hill 2016 used modflow simulations to represent hyporheic flow only a comparative value for nitrate turnover is not available in addition hyporehic exchange of water for the modlfow simulation table 3 are not directly comparable to the ones obtained from the toth 1st order approximation as the numerical model was set up using a heterogeneous material distribution in the subsurface areas of lower hydraulic conductivities in the modlfow simulations presented by kasahara and hill 2016 reduce advective velocities in the subsurface and increase hyporheic residence times the latter reduces hyporheic exchange rates between the stream and the subsurface which are higher for the toth and 1st order approximation both analytical approaches predict gamma distributed subsurface residence times simulated distributions are shown in the supplement with a higher mean value and standard deviation for the toth approximation 4 3 accuracy of the 0th and 1st order approximations the purpose of this section is 1 to evaluate the accuracy of the derived 1st order approximation solutions for the application and the benchmark examples as well as 2 to investigate to what degree the accuracy of the flow solutions increase if the fully 1st order approximation solution equation 11 is used to describe hyporheic flow instead of only using the much simpler derived 0th order approximation equation 12 by only accounting for the 0th order correction term the resulting solution is identical to the toth solution if re scaled back into the physical domain zlotnik et al 2015 to evaluate accuracy we considered two different metrics namely the ability of the model to close the volume balance on water circulating through the hyporheic zone and the mass of nitrate that is either produced or consumed within the hyporheic domain the first metric can be expressed as a dimensionless relative error e r q i n q o u t q i n water fluxes that enter q i n l2t 1 or leave q o u t l2t 1 the model domain can be estimated by sequentially integrating the darcy fluxes q q x q z t lt 1 see si for details the second metric net mass turnover for nitrate m no3 ml 1t 1 is the difference between the entering mass fluxes no 3in ml 1t 1 and exiting mass fluxes no 3out ml 1t 1 of the model domain these latter two parameters can be estimated from q in and q out and from the p pass model simulations for nitrate turnover see si for details in general relative errors for the 0th order representations are as expected consistently higher compared to the 1st order approximations table 4 in case of the benchmark scenario the water balance errors associated with the 0th order approximation are already diminishingly small table 4 and by including the 1st order correction term in the perturbation series the accuracy only increases marginally here a simple 0th order approximation would be adequate to achieve a high level of accuracy in closing the water balance in the case of the constructed riffle by additionally accounting for the 1st order correction term the water balance error decreases from 3 8 to below 1 table 4 for the flume experiment the water balance error obtained with the 0th order approximation is very large 63 indicating a high imbalance between in and outgoing water fluxes caused by an inaccurate representation of hyporheic flow by additionally including the 1st order correction term the error drops dramatically to below 1 a similar improvement in accuracy was reported by zlotnik et al 2015 for hydrological systems at the catchment scale for the flume experiment the biogeochemical simulations based on the 0th order approximation predict that the ripple structures are a net source for nitrate with a net production rate of 38 μmol m 1 d 1 however the more accurate 1st order solution indicates that the rippled bedforms are a net sink of nitrate with a net consumption rate of 138 μmol m 1 d 1 which is consistent with experimental observations and comsol modeling studies performed by kessler et al 2012 and pass modeling simulations performed by azizian et al 2015 although kessler et al 2012 explicitly stated that they were unable to experimentally calculate the bulk flux from the sediment or measure n2 production we think that the 1st order approximation solution represent the sediment s functional role in biogeochemical cycling in this case predicting that the sediments are a net sink of nitrate more correctly compared to the 0th order approximation with its large error in closing the water balance 4 4 sensitivity analysis and limitations this analysis aims to understand how the accuracy of the approximate solutions is influenced by typical characteristics of the flow domain such as the maximum elevation difference of the bedform z max z min and the depth of the horizontal plane z 0 with this analysis we intend to identify specific types of flow problems that may lead to inaccurate solutions according to equation 28 and as outlined in equations 25 27 the perturbation factor for the riffle structure depends on the term with the largest magnitude in the sine cosine series b 1 and the height of the horizontal plane z 0 28 ε b 1 z 0 for the sensitivity analysis the original flow domain of the riffle structure is modified in two different ways both leading to an increase of the perturbation factor ε 1 an increase in ε can be achieved by decreasing the depth of the horizontal plane z 0 while the shape of the bedform topography remains the same z max z min const 2 increasing b 1 in equation 28 will also lead to an increase of ε while this time z 0 remains constant the latter modification leads to a change in the shape of the bedform where the elevation difference z max z min is increasing representing a steeper topographical gradient results of this analysis are shown in fig 4 were the water balance errors are plotted against the respective perturbation factors the dashed lines in fig 4 represent simulations where the depth of the horizontal plane z 0 was modified to achieve a change in ε the solid lines are associated with simulations where z 0 remains constant while the shape of the bedform was modified by increasing b 1 as expected the 0th order solutions always are associated with higher water balance errors compared to the respective 1st order solutions for higher ε values approximate solutions with identical bedforms but different z 0 are more accurate 10 compared to the simulations with steeper bedforms here results clearly indicate that the accuracy of the approximate solutions is more sensitive to a change in the elevation difference compared to the depth of the horizontal plane elevation differences for the 1st order approximation associated with the highest mass balance error 47 lies around 1 7 m which is probably a quite unrealistic scenario for most hyporheic systems for the riffle example with the original value for z 0 a maximum elevation differences that exceed 0 5 m leads to water balance errors above 10 here a 1st order approximation is not accurate enough and higher order solutions have to be considered by accounting for additional terms in the perturbation series equation 10 leading to improved accuracies 5 discussion for hyporheic systems virtually all aspects of hyporheic exchange of water solutes and energy are controlled by site specific topographical variations of the streambed the vertical extent of hyporheic flow cells e g boano et al 2010a marzadri et al 2010 trauth et al 2013 locations of stagnation points bottacin busolin and marion 2010 hyporheic exchange fluxes and subsurface residence times e g cardenas et al 2004 boano et al 2014 marzadri et al 2015 and other characteristics are directly or indirectly controlled by the morphological shape of the streambed channeling the water movement streambed topography also affects spatial nutrient availability in hyporehic areas precht et al 2004 cardenas 2009 kessler et al 2013 the spatial zonation of redox conditions marzadri et al 2012 and the emission of nitrous oxide from hyporheic areas marzadri et al 2014 an accurate mathematical representation of flow conditions with an explicit consideration of streambed topography therefore can be essential for a qualitative and quantitative understanding of hyporheic systems marion et al 2002 in hyporheic applications the classic toth technique represents the streambed topography of hyporheic systems as a flat surface although the toth approximations are useful for estimating the integral characteristics of solute transport e g azizian et al 2015 caruso et al 2016 wörman et al 2006 the solutions are strictly only valid in a truncated flow domain toth domain but important local aspects of topographically driven flow are missing zlotnik et al 2015 for hyporheic systems where important biogeochemical processes such as aerobic respiration or denitrification occur close to the streambed water interface flow domain truncation can lead to error prone quantifications of hyporheic water exchange subsurface residence times and nutrient turnover this was shown by comparing nitrate turnover rates predicted by models that use the toth approach pass model and by models that explicitly account for the streambed topography comsol and p pass by ignoring topographical features of the streambed the toth solutions tend to underestimate exchange of water between the stream and the hyporheic zone which seems to be especially relevant for small scale structures like the simulated ripples an explicit representation of the streambed topography in analytical models can be achieved by a consistent use of the perturbation method to describe hyporheic flow in general the 0th and 1st order approximations lead to solutions that show similar behavior in terms of simulated patterns of subsurface flow and biogeochemical turnover as expected the 1st order correction term reduces water volume and mass balance errors although the degree of improvement depends on the specific nature of the problem in question and magnitude of the perturbation parameter the question that arises is whether it is worth the extra effort of deriving a 1st order approximation by using the perturbation method compared to the easily derived 0th order approximation solution that is practically identical to a re scaled toth solution here the simulated flume provides salient example where the accuracy of the solution can be increased by the factor of 70 by additionally accounting for the 1st order correction term if the 0th order approximation is used to describe physical water flow for the flume experiment the high inaccuracies in the flow solution can lead to a very different assessment regarding the hyporheic zone s role in n cycling specifically we found that the re scaled toth solution which is equivalent to a 0th order approximation predicts that the sediments beneath ripples are a net source for nitrate when in fact numerical models and the 1st order approximation suggest that the opposite is true although discrepancies for total nitrate turnover are small for the riffle system the differences between the solutions predicted by the 0th and 1st order approximation solutions can become quite significant if those results are used to upscale the denitrification capacity at scales of entire streams or river basins further it can be expected that such inaccuracies in the flow solution are also present after applying a 0th approximation on larger scales such as catchments zlotnik et al 2015 or if approximation solutions are being used for 3d flow systems wörman et al 2006 caruso et al 2016 the accuracy of the perturbation solutions primarily depends 1 on the magnitude of the small dimensionless parameter ε that describes the deviation in elevation between a horizontal plane elevation z0 and the actual topography and 2 the number of the successive terms in the perturbation series probably in the majority of realistic hyporheic flow problems elevation differences for hyporheic structures are small and the resulting perturbation parameter lies below 0 1 for such cases the 1st order approximation achieves accuracies similar to those of conventional numerical models however for hyporheic structures that are associated with high topographical gradients like cascades or steep pool structures commonly observed for mountainous streams with reach slopes 0 03 montgomery and buffington 1997 a 1st order approximation may not be accurate enough as was shown by the sensitivity analysis for the latter systems the accuracy can be increased by accounting for additional terms in the perturbation series however this will also increase the level of complexity of the resulting solution further it must be mentioned that a 2d representation of the real system of course always represents a simplification this simplification might not be uniformly appropriate for situations with complex 3d structures of the streambed which cannot simply be represented in a 2d model so far the presented matlab model is limited to 2d steady state flow scenarios with a homogenous isotropic or anisotropic streambed the approach has potential for generalization to 3d simulations of problems like those presented by marklund and wörman 2011 and possibly to heterogeneous conditions like those discussed by craig 2008 and wond and craig 2010 streambed heterogeneity can have an important influence on hyporheic exchange characteristics pryshlak et al 2015 hyporheic residence times tonina et al 2016 and can be responsible for the formation of biogeochemical hot spots gomez velez et al 2014 as material heterogeneity can affect physical movement of water and biogeochemical degradation equally an accurate representation in the p pass model is needed the latter will be addressed in future research finally transient cases can be considered as well zlotnik et al 2015 the model also can be extended to the hyporheic systems that are affected by lateral or vertical groundwater exchange cardenas et al 2004 where ambient fluxes can significantly alter the hyporheic flow cells boano et al 2008 or even can totally suppress hyporheic flow trauth et al 2013 based on the results of this study it is well justified to use the perturbation method as a powerful tool in for the mathematical description and understanding of the hyporheic systems approximate solutions offer an advanced alternative to the classical toth solutions or the apm model as they explicitly present the streambed topography and its impact on local hyporheic flow and transport conditions a major advantage of the presented technique compared to numerical models is its flexibility and usability an inexperienced user only has to define spatial head variations and streambed topography in functional form in addition to the aquifer parameters thereby avoiding a grid development which is laborious task without specialized software numerical as well as analytical models always need the upper forcing term as input to represent hyporheic flow processes here the important task remains namely derivation of the upper head distribution above the streambed interface which controls the hyporheic exchange either through cfd modeling or observations this is an essential and difficult step in majority of field studies only a few measurement points on bedform topography are available e g data used to set up the modflow simulation presented in kasahara and hill 2016 however it is known that small topographical features of the streambed that are easily smoothed out by parametrization or discretization can have a significant impact on hyporheic flow processes the latter affects both analytical as well as numerical models nevertheless if a highly spatially resolved bedfrom topography is available it also can be used as part of the p pass model once known measured streambed elevations and upper forcing terms easily can be implemented into the model by superposition of sinusoidal functions with as many terms as needed to account for all the necessary details and for which the problem specific perturbation parameter ε can be derived generally implementation of this type of data requires less effort with analytical approaches sensitivity analysis where multiple bedform shapes are being represented also can be performed much more efficiently with analytical approaches in a numerical model such as comsol or modflow replacing the topography is always associated with setting up a new computational grid which is a time consuming procedure a typical example hyporheic flow and nutrient turnover for the riffle takes only about 15 20 min for the parallel matlab code running on 5 cpus 6 conclusions 1 representing topography and the head distribution of the sediment water interface correctly is important for a quantitative assessment of hyporheic exchange and nutrient cycling in hyporheic systems the commonly used toth approach of flow domain truncation may lack necessary accuracy and can introduce computational artifacts 2 the perturbation method offers a way to substantially improve local representation of flow transport and hyporheic exchange analyses within and beneath the ripples by explicitly accounting for a spatially varying topography and the head distribution at the boundary 3 the presented matlab based model is capable of representing flexible 1st order approximate solutions for user defined 2d hyporheic systems the mass balance errors are comparable to numerical solutions but do not require laborious numerical grids and offer more flexibility for users with little modeling background acknowledgments the original matlab source code of the p pass model can be obtained by contacting the corresponding author or alternatively downloaded from http www hydro uni bayreuth de hydro de software software software dl php we thank the editorial committee of environmental modelling and software and the anonymous reviewers who provided many useful suggestions that improved the original manuscript we gratefully acknowledge financial support from the german research foundation dfg fr 2858 2 1 appendix a supplementary data the following are the supplementary data to this article data profile data profile manuscript docx 37 46 manuscript docx 37 46 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 015 
