index,text
26205,hydrologic models such as the storm water management model swmm and the hydrologic simulation program fortran hspf are widely used to evaluate the impacts of urban development on watersheds and receiving waters we compare the ability of these two models at simulating streamflow peak flow and baseflow from an urban watershed the most sensitive hydrologic parameters for hspf were related to groundwater for swmm it was imperviousness both models simulated streamflow adequately however hspf simulated baseflow better than swmm while swmm simulated peak flow better than hspf global sensitivity analysis showed that variability of streamflow for swmm was higher than that of hspf while variability of baseflow for hspf was greater than that of swmm further analysis of extreme storm events indicated that the runoff coefficient for swmm was slightly greater than hspf for recurrence intervals of 1 2 5 and 10 yr the opposite was the case for recurrence intervals greater than 10 yrs graphical abstract image 107955 keywords hspf swmm streamflow baseflow peak flow sensitivity analysis 1 introduction urbanization alters watershed hydrology by increasing imperviousness and channelizing or piping natural drainageways hester and bauman 2013 li et al 2013 liu et al 2015 these changes reduce infiltration increase runoff volume accelerate the time to runoff peak lag time and reduce baseflow to streams chen et al 2017 lacher et al 2019 locatelli et al 2017 rosburg et al 2017 increasing runoff volume results in higher streambank and channel erosion whitney et al 2015 yousefi et al 2017 increases in peak runoff and decreasing lag time increases flooding roodsari and chandler 2017 zope et al 2016 damaging public or private property urbanization also leads to higher sediment and nutrient loads delivered to downstream water bodies causing eutrophication and degrading water quality threatening aquatic ecosystems daghighi 2017 liu et al 2018 luo et al 2018 stoner and arrington 2017 a variety of stormwater control measures scms also known as best management practices bmps have been developed for mitigating urban impacts historically management of urban runoff meant mitigating peaks using storage this practice has given way to a more holistic focus on the restoration of the natural hydroperiod known as low impact development lid or green stormwater infrastructure gsi scms that assist in these goals tend to focus on infiltration golden and hoghooghi 2017 liu et al 2018 lucas and sample 2015 watershed models are used to 1 simulate hydrology and water quality in runoff streams and water bodies 2 evaluate the impacts of urban development and 3 investigate effectiveness of watershed restoration strategies borah et al 2019 niazi et al 2017 while numerous watershed models exist limited information is available to guide in their selection two commonly used watershed models include the u s environmental protection agency s usepa storm water management model swmm usepa 2018 and the hydrologic simulation program fortran hspf usepa 2014 swmm is a dynamic physically based hydrologic and hydraulic model which is used to simulate runoff quantity and quality during discrete events and continuous periods huber and dickinson 1988 james et al 2010 rossman 2010 swmm is often used in urban areas because it is capable of simulating conveyance systems hspf is a comprehensive process based watershed model that simulates watershed hydrology and water quality bicknell et al 2001 linsley et al 1975 both swmm and hspf were developed by the usepa hspf has been applied across large regional watersheds such as the chesapeake bay watershed a 166 000 km2 watershed usepa 2010 the hspf based chesapeake bay watershed model discretizes subwatersheds based upon huc 12 hydrologic unit code watershed delineations and geopolitical considerations such as city county and state boundaries shenk et al 2012 due to the complexity inherent in urban storm drainage networks and their flashy runoff swmm models tend to be used at smaller scales to capture this response niazi et al 2017 recent 10 years published research based upon use of swmm or hspf that used at least two statistical methods for evaluating model performance were compiled in table 1 based on the references provided in table 1 swmm has been applied to watershed ranging in size from 2 ha to 40 000 km2 however it has primarily been used within smaller urban watersheds 2 km2 swmm has specific functionality for simulation of scms and lid incorporating a variety of physical processes such as storage routing and infiltration on the other side hspf has also been applied across a wide range of larger watersheds 3 70 000 km2 although hspf has been applied to urban watersheds it has several limitations hspf does not directly simulate conveyance systems nor does it directly simulate scms hspf models scms by shifting some of the watershed area s land use from urban to undeveloped and changing the f tables as these govern stream dimensions in the hspf model dudula and randhir 2016 mohamoud et al 2010 u s epa 2014 the lack of explicit scm representation is a key weakness of hspf mohamoud et al 2010 hspf is typically based upon readily available spatial data and must be calibrated to monitoring data in contrast swmm depends upon physically based parameters that are collected or derived from spatial data gathered at smaller scales a comparative assessment of hspf and swmm in simulating hydrology of watersheds has been conducted only in a few studies both were conducted in forested not urban watersheds lee et al 2010 compared swmm output with average streamflow from a large watershed during seven events the authors indicated that both models performed adequately however hspf simulated hourly streamflow better than swmm tsai et al 2017 applied swmm and hspf to a highly pervious forested watershed the authors indicated that hspf matched observed streamflow better than swmm this may have been due to the highly permeable soil of the watershed which likely created a strong baseflow response a key application of hspf is the simulation of hydrology and water quality of the chesapeake bay watershed usepa 2010 this is directly the result of hspf s simplicity which allows hspf to execute simulations of this large watershed faster this computational advantage is evident in execution of large watershed models for long times swmm s advantages are its ability to simulate flashy urban watersheds and assess scm performance as both models are widely used in urban areas understanding the similarities and differences between them is critical yet a comprehensive comparison has not been done the objective of this paper was to address this research need by comparing the capabilities of hspf and swmm as applied to a case study urban watershed hspf and swmm were each assessed in terms of the 1 most sensitive hydrologic parameters in the watershed 2 simulation of daily and monthly streamflows in comparison with observed data 3 simulation of peak flows baseflows and their respective durations and 4 predicted runoff coefficients during storm events with set return periods these results were then used to compare the subcomponents of the long term watershed hydrograph achieving a better understanding of the similarities and differences of swmm and hspf will help relate information from each model to the other which will assist in meeting water quality goals at the regional scale 2 materials and methods 2 1 site description stroubles creek located within montgomery county virginia lies within the valley and ridge physiographic province of virginia stroubles creek is a tributary to the new river which is tributary to the kanawha river and part of the mississippi river basin an urbanized 14 8 km2 headwater portion of the stroubles creek watershed was selected for this study fig 1 this subwatershed includes much of downtown blacksburg and the campus of virginia polytechnic institute and state university virginia tech this watershed was selected because 1 its headwaters are predominately 73 8 urbanized and 2 long term monitoring data are available the virginia tech stream research education and management stream lab stream lab 2009 continuously measures groundwater levels streamflows and records precipitation and other climatological data within the stroubles creek watershed land cover is 73 8 urban with a total imperviousness of 32 21 agricultural 4 forested and 1 2 water body multi resolution land use consortium 2011 fig 1 the dominant hydrologic soil group hsg of the headwaters is category c as classified by the natural resource conservation service nrcs 2007 1999a while downstream consists mainly of silt loam and loam soils which are category b mostaghimi et al 2003 the average elevation of the watershed is 670 m above sea level mean annual precipitation is 1030 mm hofmeister et al 2015 liao et al 2015 2 2 data collection storm sewer street parcel boundary and surface elevation geographic information system gis data were provided by the town of blacksburg town of blacksburg 2015 and virginia tech separate datasets were merged soil information was obtained from the soil survey geographic database ssurgo of the natural resources conservation service with scales ranging from 1 12 000 to 1 64 000 nrcs 1999b the monitoring station measures stream stage every 15 min using a pressure transducer cs451 campbell scientific inc logan ut usa with a water level resolution of 0 0035 fs full scale full span the difference between the lowest and highest measured point and a cr1000 datalogger campbell scientific inc u s stage was converted to discharge using a rating curve computed through the historical monitoring of stage flow precipitation was recorded at 15 min intervals at the stream lab metrological station using a tipping bucket rain gages tr 525usw texas electronics inc dallas tx 1 the stream lab weather station measured air temperature every 30 min at the approximately 300 m downstream of the stroubles creek monitoring station stream lab and the meteorological station are located at the watershed outlet the depth to surficial groundwater was measured every 10 min by two piezometers installed in the floodplain adjacent to stream lab using two cs451 water level loggers campbell scientific u s groundwater table elevation was quantified using geological maps of geology and mineral resources division of commonwealth of virginia appendix a and data from the stream lab pre installed floodplain piezometers 2 3 model initialization land cover data was initially used to initialize the models in a process described by ketabchy 2018 the principal input parameters used in development of the hspf and swmm models were land use soil properties stream characteristics and time series of precipitation and temperature a total of 43 subwatersheds were delineated within the stroubles creek watershed the watershed was delineated through arcgis 10 5 ketabchy et al 2018 correcting the delineation for urban features i e topography slope elevation land use etc where necessary the differences and similarities of each process feature and main input output variables for hspf and swmm are summarized in table 2 swmm uses a simplified darcy s law to simulate groundwater flows and interaction of surface water and groundwater of an aquifer through a number of parameters bottom elevation of aquifer groundwater surface water interaction parameters a1 a2 b1 and b2 which are listed in table 3 depth of unsaturated upper zone and lower saturated zone aquifer porosity and saturated hydraulic conductivity rossman 2010 these parameters control flow from the aquifer into the stream and vice versa and compute groundwater flow as a function of groundwater and surface water levels green ampt ga infiltration was applied for the infiltration module of swmm primarily because the watershed was semi urbanized and the physical basis of ga parameters such as suction head hydraulic conductivity and initial moisture deficit values are available through the soil survey geographic database ssurgo the dynamic wave dw algorithm was selected for hydraulic routing within swmm because this method can simulate non uniform and unsteady state flow conditions accurately the longest flow paths of each subcatchment were used to calculate its hydraulic width hw a required swmm parameter excess rainfall that exceeds depression storage is routed from each subcatchment through a nonlinear reservoir algorithm macro et al 2019 palla and gnecco 2015 xing et al 2016 each subcatchment is split into pervious and impervious portions and runoff is directed to a user defined outlet node or is routed across pervious areas the manning s roughness coefficient for pervious and impervious area is used to compute normal flow across a plane the plane being the subcatchment these eventually flow into either conveyance piping ditches and or streams through which flow is calculated by use of the manning s equation or through culvert formulas which depend upon upstream and downstream conditions hspf includes three principle modules perlnd pervious land implnd impervious land segments and rchres routing through reaches processes in receiving streams can be simulated using the rchres reach and reservoir module of hspf implnd module generates surface runoff whereas the perlnd module analyzes all three major processes including surface runoff interflow and groundwater all processes related to soil infiltration soil moisture groundwater baseflow separation etc are analyzed in these modules enabling hspf to predict the hydrology and water quality of watersheds berndt et al 2016 bicknell et al 2001 mohamoud and prieto 2012 xu et al 2007 the pwater and iwater sections in hspf control the water budget allocations between surface flow interflow baseflow storage interception detention and evaporation et pwat parm3 is one section of pwater which has two parameters of deepfr and agwetp for simulating groundwater recharge philips equation a physically based method that uses an hourly time step chezy manning s equation and kinematic wave kw were applied within hspf for simulating infiltration streamflow and hydraulic routing respectively bicknell et al 2001 within hspf the parameters lzsn and uzsn table 3 that control lower and upper zone storage are used to simulate water outflow from streams bicknell et al 2001 the inflt parameter is an index associated with the philips infiltration method for quantifying soil infiltration capacity there are three parameters that control groundwater and baseflow in hspf these are kvary agwrc and deeper which are functions of baseflow recession variation and the interactions between groundwater and surface water basetp is a parameter that represents the et of riparian vegetation when riparian vegetation is present its value starts with 0 03 singh et al 2005 intfw and irc are interflow parameters which are a function of soil topography and land use bicknell et al 2001 the major components of the water balance within the stroubles creek watershed include precipitation total runoff sum of overland flow interflow and baseflow total actual et sum of interception et aquifer upper zone et aquifer lower zone et baseflow et and active groundwater et and deep groundwater recharge each of the aforementioned water balance components have corresponding parameters in swmm and hspf table 3 2 4 baseflow separation direct runoff during storm events is the sum of overland flow and interflow while baseflow consists of groundwater discharge from the saturated zone of an underlying aquifer directly to streams lott and stewart 2013 miller et al 2016 rumsey et al 2015 baseflow affects aquatic habitats during dry periods and low intensity storm events during periods of high groundwater levels mccargo and peterson 2010 there are several methods to determine and separate baseflow from streamflow which are grouped into three general categories graphical analytical and mass balance methods lott and stewart 2016 baseflow separation partitions a stream hydrograph into baseflow and runoff the most widely used methods of baseflow separation are analytical lott and stewart 2016 eckhardt 2008 developed a two parameters equation through numerical analysis for baseflow separation which is calculated by eq 1 1 b k 1 b f i max a b k 1 1 a b f i y k 1 a b f i m a x where a is the groundwater recession constant y is the total streamflow b is the baseflow k is the time step and bfimax is maximum baseflow index there are three values for maximum baseflow index bfimax parameter including 0 80 for perennial streams with porous aquifers 0 50 for ephemeral streams with porous aquifers and 0 25 for perennial streams with hard rock aquifers in this study a hydrograph analysis tool kyoung et al 2005 was used for baseflow separation which uses the eckhardt method eckhardt 2008 the aforementioned method is able to separate baseflow more accurately than other numerical methods since it utilizes two parameter filters eckhardt 2008 neff et al 2005 since the current stream study is perennial with porous aquifers underneath a bfimax of 0 80 was used 2 5 analysis of storm events the behavior of each model during storms events with a set return period was assessed each calibrated model was used to simulate streamflow for the 1 2 5 10 25 50 and 100 year 24 hr precipitation frequency pf estimates at the outlet of the stroubles creek watershed the pf estimates were produced by national oceanic and atmospheric administration noaa atlas 14 with 90 confidence intervals noaa 2016 using the partial duration time series type natural resources conservation service nrcs type ii storm distribution was used to develop time series of 24 hr precipitation events nrcs 2015 groundwater discharge was assumed to be negligible during large storm events the runoff volume simulated at the outlet of the watershed by both models during the 24 hr precipitation was normalized to runoff depth through dividing by the connected impervious area of the watershed further runoff coefficients were calculated as the runoff depth divided by precipitation depth as essentially all streamflow was runoff during the event 2 6 sensitivity analysis sensitivity analysis sa is process of the adjusting inputs of a model and calculating the rate of change in model results sa techniques are grouped into local and global methods javaheri et al 2018 local sa methods evaluate the sensitivity of parameters around one local point the value of one particular input parameter was changed while other parameters were held constant during the simulation hence the sensitivity of streamflow as the main output of the models to input parameters can be represented by the sensitivity coefficient eq 2 james et al 1982 2 s c p y y 1 y 2 p m a x p m i n where sc is sensitivity coefficient p is the input parameter and y is the predicted output pmax and pmin are the maximum and minimum ranges of the initial default value and y1 and y2 are the corresponding output values the most sensitive model parameters in watershed hydrology have higher values of sc in addition global sensitivity analysis gsa techniques evaluate the sensitivity of parameters around the whole parameter space dobler and pappenberger 2013 javaheri et al 2018 the sensitivity analysis identified several key parameters that had a substantial impact on simulation results the sensitive parameters have the potential to significantly influence swmm and hspf simulation results in applying gsa to the case study the calibrated value of each model s input parameter was used as the baseline value each key model parameter value was varied one at a time with simulations run for plus and minus 10 of the published range in the parameter value table 2 this produced a total spread of 20 in the parameter value which was assumed to provide a reasonable estimate of inputs the two simulations produced for the modification of each input parameter provided upper and lower bounds of the simulation results these limits can be interpreted as error limits of simulation results this approach is often applied to address the performance evaluation of best management practices bmps and hydrologic models janke et al 2013 park et al 2011 2 7 calibration and validation hspf and swmm models represent hydrologic and hydraulic features of a watershed using fixed and process related parameters castanedo et al 2006 fixed parameters represent the hydraulic features of drainage networks while physical properties represent drainage basins properties such as length slope width depth and roughness of a watershed and areas covered by various soil types and land covers a flow chart describing the process of developing the hspf and swmm models in this study is shown in fig 2 process related parameters cannot normally be measured directly or cannot be calculated through gis information these include soil moisture storage groundwater discharge into stream et etc bicknell et al 2001 castanedo et al 2006 these parameters were adjusted manually during the calibration process between january 1 2013 and december 30 2013 for each model using hourly streamflow obtaining from stream lab there were 22 storm events during calibration period validation which consists of running the models with the calibrated parameters without adjustment was conducted for the period between january 1 2009 and december 31 2012 with 61 storm events the purpose of model validation is to assess if the calibrated models can simulate streamflow behavior for events outside of the calibration period the goodness of fit criteria for both calibration and validation periods were investigated using a group of statistical methods including coefficient of determination r2 gebremariam et al 2014 nasr et al 2007 seong et al 2015 nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias gupta et al 1999 according to duda et al 2012 and moriasi et al 2015 multiple statistics should be used rather than a single criterion a model performance rating system which compared the simulated versus observed datasets qualitatively was developed to assess model performance table 4 bennett et al 2013 ketabchy et al 2019 moriasi et al 2015 nayeb yazdi et al 2019a if the statistical parameters showed good or satisfactory agreement table 4 the model calibration was considered complete otherwise the model calibration parameters were adjusted further the calibration process stops when r2 and nse are greater than 0 6 and 0 5 respectively and pbias is lower than 0 25 fig 3 3 results 3 1 sensitivity analysis the results of the local sensitivity analysis for selected input parameters are presented in table 5 the most sensitive parameters in the hspf model were groundwater parameters deepfr agwrc followed by infilt lzsn parameters which are functions of soil and land use the most sensitive parameters of the swmm model was imperviousness sc 0 38 following by impervious depression storage sc 0 11 and subwatershed hydraulic width sc 0 03 these results are similar to previous studies ali and bruen 2016 seong et al 2015 tsai et al 2017 xing et al 2016 compared to hspf model the groundwater parameters within swmm including the groundwater flow coefficient groundwater flow exponent surface water flow exponent and surface water flow coefficient did not substantially affect swmm results 3 2 global sensitivity analysis results the baseline values of model outputs i e average streamflow average baseflow and associated variation in modeled outputs are shown in table 6 gsa was conducted on the most sensitive parameters in hspf and swmm with the upper and lower bounds serving as the extreme endpoints of simulation outputs during gsa variability of average streamflow for swmm was higher than that of hspf while variability of average baseflow for hspf was significantly greater than that of swmm the most sensitive parameters of the hspf model were attributed to groundwater discharge thus altering those parameters had direct a significant effect on baseflow this likely explains why hspf simulated baseflow had a larger variability during simulation than similar outputs from the corresponding swmm model the sensitive parameters of swmm were primarily attributed to imperviousness and infiltration which have a direct effect on runoff and streamflow table 6 3 3 comparison of models without calibration as a baseline for our study the hspf and swmm models were initially run for the entire period of record without calibration to assess the relative abilities of each model to match the observed data it should not be construed that the authors are recommending use of the models without calibration our supposition is that swmm would perform better than hspf without calibration for the aforementioned reasons parameter values for both models were left as estimated from external data sources or model defaults nse r2 and pbias for swmm was 0 52 0 58 and 22 and for hspf was 0 38 0 47 and 0 42 respectively the results indicated that without calibration swmm simulated streamflow far better than hspf earning an acceptable vs poor according to the metrics by moriasi et al 2015 this is due to the finer spatial scale of the inputs to swmm which are based more on the externally sourced data such as gis and the physics of the hydrological processes which control the catchment response while hspf is a process based model that relies on many parameters which can only be determined through calibration thus hspf is not useful without calibration whereas swmm without calibration while diminished somewhat may still provide useful information thus hspf is better for watersheds with monitoring data but only limited physical information the opposite is the case for swmm 3 4 calibrated input parameters the calibrated value ranges of input parameters for hspf and swmm models are presented in table 7 the hspf calibrated input parameters for soil and land use lzsn infilt were categorized for forest agricultural and urban land covers 3 5 comparison of models for average streamflow simulation goodness of fit results for calibration and validation periods are provided in table 8 the statistical analysis results showed good agreement between the simulated and observed streamflow the observed and simulated hydrographs of swmm and hspf for calibration and validation periods are shown in fig 4 during the calibration and validation periods swmm showed slightly better agreement between simulated and observed streamflow than hspf based on the statistical values of nse r2 and pbias the positive values of pbias for models during validation period indicates the propensity of the models to underestimate streamflow since visual comparison of the models results using fig 4b was hard to see two months i e december 2009 and may 2011 were separated for better visualization in a narrower data range fig 4c and d goodness of fit was also assessed by plotting the observed vs simulated values of streamflow in calibration and validation periods as shown in fig 5 swmm calibration replicates many of the storm event peaks reasonably well the slope of the regression line for the hspf calibration was less than 1 0 fig 5a while that of for swmm calibration period was close to 1 0 fig 5b some of the errors are likely due to the inability of the swmm and hspf models to capture streamflow peaks for some of the events fig 5a and b the slope of regression line for validation periods of swmm and hspf was approximately 0 7 indicating highly relative magnitude of the residuals to standardized residuals residuals equal to 0 0 swmm generally overestimated high magnitude flood events fig 5d while there was no certain pattern in simulating high magnitude flood events through hspf fig 5c the residual time series of daily streamflow versus time and precipitation is provided in fig 6 the hspf streamflow simulation average error during wet periods days with at least 0 25 cm precipitation and dry periods were 0 002 and 0 067 m3 s respectively while those of for swmm streamflow simulation were 0 067 and 0 070 m3 s respectively the aforementioned analysis indicates relatively better performance of both models in wet period than dry periods in terms of averaged error hspf appeared to be a better predictor of streamflow in wet periods rather than swmm during high magnitude storm events days with at least 2 cm precipitation swmm generally over estimated the streamflow while there was no specific pattern for hspf simulation error flow duration curves of simulated streamflow by hspf and swmm and observed streamflow are shown in fig 7 models simulated streamflow close to observed streamflow during high flows between 0 and 10 flow exceedance q10 hspf simulated streamflow between 10 and 90 of flow exceedance were slightly beneath observed streamflow while swmm over predicted streamflow during low flow overall based on a visual look the hspf simulation matched better in terms of flow exceedance pattern with observed streamflow compared to the swmm simulation fig 7 the top 10 of streamflow in magnitude according to fig 7 were selected as peak flows to evaluate the capability of hspf and swmm in peak flow simulation there was 81 days of high streamflow for observed dataset the corresponding pbias values of swmm and hspf models for peak flow were 0 098 and 0 120 respectively indicating that swmm was better at reproducing observed peak flows the average errors simulated observed of peak flows 7 1 for swmm and 8 1 for hspf confirmed the pbias statistical analysis results the pbias values average percent errors of models and fig 6 represent the overestimation and underestimation of peak flows by swmm and hspf respectively 3 6 comparison of models for monthly streamflow simulation the average monthly streamflow representing streamflow seasonally variation indicated that hspf and swmm models achieve better agreement with observed streamflow during winter months jan and feb rather than summer months may jun jul and aug fig 8 the swmm averaged percent differences of all months resulted in 15 while that of for hspf was 22 indicating swmm is a better predictor of seasonally streamflow variation the percentage difference between the swmm and hspf monthly simulated streamflow and the observed monthly streamflow ranged from 6 to 39 and from 3 to 48 respectively which can be classified as not good results for models when pbias is higher than 25 al abed and al sharif 2008 swmm performed better than hspf in summer months while hspf simulation matched relatively better with observed averaged monthly streamflow in winter than swmm generally both models under estimated the averaged monthly streamflow between january 2009 and december 2013 fig 8 the simulation of average monthly streamflow can be beneficial for assessing impact of projected climate and land use changes 3 7 comparison of models for baseflow simulation the baseflow was plotted as 1 total baseflow and 2 baseflow for dry periods dps or the periods in which precipitation and direct runoff are zero and groundwater discharge is the only source of streamflow fig 9 the observed dps baseflows between 2009 and 2011 was 317 days while that for swmm and hspf simulations were 693 and 199 days respectively fig 9b it indicates better performance of hspf in coverage of the number of dry days period the pbias values of swmm model for total baseflow and dps baseflow were 0 4 and 0 61 respectively while those of for hspf model were 0 31 and 0 53 respectively indicating better performance of hspf model in capturing observed total baseflow and dps baseflow as swmm and hspf models were not calibrated through observed baseflow the aforementioned pbias calculations and the respective discussion were only based on baseflow calculation using the eckhardt 2008 method and the calibrated average streamflow hspf captured the observed baseflow pattern better than swmm model fig 9a and b in contrast swmm followed a relatively constant baseflow pattern throughout the dps fig 9b our results are similar to previous study indicating that swmm has a limitation concerning baseflow simulation during dry periods particularly during winter months liu et al 2013 3 8 comparison of model response to standard storm events hspf and swmm models were compared during set return period events by running each using standard nrcs 24 h storms the blacksburg virginia 1 yr recurrence precipitation is 55 mm 2 2 in noaa 2016 during the monitoring period an event 07 july 2013 was identified and separated and are shown in fig 10a during this event nse r2 and pbias between observed and simulated data for swmm were 0 51 0 58 and 33 and for hspf were 0 45 0 52 and 20 respectively since the models were calibrated continuously these results for that event can be can be considered to be acceptable moriasi et al 2015 the simulated hydrograph for 1 yr recurrence interval are presented in fig 10 b results indicated that for extreme storm events swmm simulated peak flows greater than hspf while hspf simulated higher baseflow than swmm swmm tended to produce more runoff than hspf for simulated storms with recurrence interval equal or less than 10 yr fig 11 although the peak flows of swmm and hspf 24 hr storm distribution for the 100 yr recurrence interval were somewhat similar a steeper receding limb was evident in the swmm results compared to hspf this accounted for the difference in runoff volume 4 discussion statistical analysis indicated that both hspf and swmm models simulated streamflow adequately however the positive values of pbias for hspf and swmm indicated that both models had a propensity to underestimate streamflow in addition the performance of both models for simulating streamflow during wet periods days with at least 0 25 cm precipitation was relatively better than dry periods it may have been stemmed from the capability of the respective philips and ga models which were used for estimating infiltration rate in hspf and swmm respectively this is because during storm events the philips and ga models estimate infiltration rate relatively better than during dry periods chahinian et al 2005 wilson 2017 during high magnitude storm events days with at least 2 cm precipitation swmm generally over estimated the streamflow while there was no specific pattern for hspf simulation error hspf appeared to be a relatively better predictor of streamflow in wet periods rather than swmm this may stem from the relative performance of the ga and philips models as the philips infiltration model represented wet periods closer to reality than ga did wilson 2017 in terms of simulating streamflow seasonally swmm performed better than hspf in summer months while hspf simulated streamflow better than swmm in winter generally the philip model estimated higher infiltration rates compared to ga turner 2006 wilson 2017 this difference could explain the previously mentioned better performance of hspf in capturing total baseflow and dps baseflow in comparison to swmm furthermore hsfp and swmm use kw and dw methods for runoff stream routing respectively previous studies indicated that the dw method is more appropriate for obtaining the reference discharge and can capture high flows better than kw moramarco et al 2008 soentoro 1991 this may explain why peak flows were better represented by swmm than hspf overall the performance difference between hspf and swmm in simulating streamflow may be due to the methods were employed for simulating infiltration rate and water routing these methods resulted in swmm simulating streamflow better than hspf within the case study urban watershed in addition in the absence of available monitoring data within a watershed swmm likely provides better results 5 conclusion models developed using hspf and swmm were used to simulate streamflow for a case study urban watershed the stroubles creek watershed in blacksburg virginia sensitivity analysis was applied only on process related parameters based on sensitivity analysis the most sensitive hydrologic parameters within hspf were groundwater parameters i e deepfr and agwrc while for swmm it was the percentage of imperviousness gsa indicated that variation for simulating baseflow averaged for hspf was greater than swmm while for simulating streamflow the variability of swmm outputs was greater than hspf swmm performed better than hspf sans calibration due to the inclusion of more detailed watershed topology and scms analysis of the residual time series of daily streamflow simulated observed indicated that both models performed better during wet rather than dry periods the comparison results of models for dry periods indicated that hspf could simulate the total baseflow and dps baseflow better than swmm while the opposite was the case for peak flows analysis of extreme storm events was also conducted the runoff coefficient for swmm was generally greater than hspf for recurrence intervals of 1 2 5 and 10 yr and the opposite was true for recurrence intervals greater than 10 years the results of this study can assist urban watershed planners in translating their results from small scale urban watershed models where scms are implemented to larger regional scale models where compliance is assessed it can also guide in the selection of the most appropriate model for their urban watershed acknowledgements funding for this work was provided in part by the virginia agricultural experiment station and the hatch program of the national institute of food and agriculture u s department of agriculture project s1063 the authors appreciate the data provided by town of blacksburg and stream lab with w cully hession and laura lehmann as director and manager respectively the authors would like to acknowledge karen kline and adil godrej virginia tech and robert burgholzer virginia department of environmental quality for their helpful comments appendix a the geologic map of the stroubles creek watershed the geologic map of the stroubles creek watershed georeferenced and digitized as a hard copy from the geology and mineral resources division of commonwealth of virginia 1985 is displayed in fig a1 below are the descriptions concerning characteristics of each geologic type of the watershed td talus deposits the area beneath the ponds of the watershed unconsolidated unsorted boulder fields composed of 0 3 1 8 m thick angular boulders of quartzite siliceous sandstone and quartzes conglomerate this detritus has been derived from nearby state of mississippian silurian and devonian age thickness 0 9 2 m ce elbrook formation the uppermost part of the formation is characterized by interbedded sandy commonly crossbed fine grain dolomite containing thin 1 10 cm lenses of fine to medium grained sandstone and 0 3 1 2 m thick ribbon banded limestone dolomite cr rome formation the area mostly at the eastern portion of the watershed consists of interbedded mottled maroon and green phylittic mudstone fine grained sandstone and siltstone and dark gray fine grained dolomite ccr copper ridge formation the area upstream of the watershed inter bedded medium gray fine to medium grained locally grained massive dolomite supper siliceous oolite and quartzose sandstone total thickness is about 366 m fig a 1the geologic map of the stroubles creek watershed fig a 
26205,hydrologic models such as the storm water management model swmm and the hydrologic simulation program fortran hspf are widely used to evaluate the impacts of urban development on watersheds and receiving waters we compare the ability of these two models at simulating streamflow peak flow and baseflow from an urban watershed the most sensitive hydrologic parameters for hspf were related to groundwater for swmm it was imperviousness both models simulated streamflow adequately however hspf simulated baseflow better than swmm while swmm simulated peak flow better than hspf global sensitivity analysis showed that variability of streamflow for swmm was higher than that of hspf while variability of baseflow for hspf was greater than that of swmm further analysis of extreme storm events indicated that the runoff coefficient for swmm was slightly greater than hspf for recurrence intervals of 1 2 5 and 10 yr the opposite was the case for recurrence intervals greater than 10 yrs graphical abstract image 107955 keywords hspf swmm streamflow baseflow peak flow sensitivity analysis 1 introduction urbanization alters watershed hydrology by increasing imperviousness and channelizing or piping natural drainageways hester and bauman 2013 li et al 2013 liu et al 2015 these changes reduce infiltration increase runoff volume accelerate the time to runoff peak lag time and reduce baseflow to streams chen et al 2017 lacher et al 2019 locatelli et al 2017 rosburg et al 2017 increasing runoff volume results in higher streambank and channel erosion whitney et al 2015 yousefi et al 2017 increases in peak runoff and decreasing lag time increases flooding roodsari and chandler 2017 zope et al 2016 damaging public or private property urbanization also leads to higher sediment and nutrient loads delivered to downstream water bodies causing eutrophication and degrading water quality threatening aquatic ecosystems daghighi 2017 liu et al 2018 luo et al 2018 stoner and arrington 2017 a variety of stormwater control measures scms also known as best management practices bmps have been developed for mitigating urban impacts historically management of urban runoff meant mitigating peaks using storage this practice has given way to a more holistic focus on the restoration of the natural hydroperiod known as low impact development lid or green stormwater infrastructure gsi scms that assist in these goals tend to focus on infiltration golden and hoghooghi 2017 liu et al 2018 lucas and sample 2015 watershed models are used to 1 simulate hydrology and water quality in runoff streams and water bodies 2 evaluate the impacts of urban development and 3 investigate effectiveness of watershed restoration strategies borah et al 2019 niazi et al 2017 while numerous watershed models exist limited information is available to guide in their selection two commonly used watershed models include the u s environmental protection agency s usepa storm water management model swmm usepa 2018 and the hydrologic simulation program fortran hspf usepa 2014 swmm is a dynamic physically based hydrologic and hydraulic model which is used to simulate runoff quantity and quality during discrete events and continuous periods huber and dickinson 1988 james et al 2010 rossman 2010 swmm is often used in urban areas because it is capable of simulating conveyance systems hspf is a comprehensive process based watershed model that simulates watershed hydrology and water quality bicknell et al 2001 linsley et al 1975 both swmm and hspf were developed by the usepa hspf has been applied across large regional watersheds such as the chesapeake bay watershed a 166 000 km2 watershed usepa 2010 the hspf based chesapeake bay watershed model discretizes subwatersheds based upon huc 12 hydrologic unit code watershed delineations and geopolitical considerations such as city county and state boundaries shenk et al 2012 due to the complexity inherent in urban storm drainage networks and their flashy runoff swmm models tend to be used at smaller scales to capture this response niazi et al 2017 recent 10 years published research based upon use of swmm or hspf that used at least two statistical methods for evaluating model performance were compiled in table 1 based on the references provided in table 1 swmm has been applied to watershed ranging in size from 2 ha to 40 000 km2 however it has primarily been used within smaller urban watersheds 2 km2 swmm has specific functionality for simulation of scms and lid incorporating a variety of physical processes such as storage routing and infiltration on the other side hspf has also been applied across a wide range of larger watersheds 3 70 000 km2 although hspf has been applied to urban watersheds it has several limitations hspf does not directly simulate conveyance systems nor does it directly simulate scms hspf models scms by shifting some of the watershed area s land use from urban to undeveloped and changing the f tables as these govern stream dimensions in the hspf model dudula and randhir 2016 mohamoud et al 2010 u s epa 2014 the lack of explicit scm representation is a key weakness of hspf mohamoud et al 2010 hspf is typically based upon readily available spatial data and must be calibrated to monitoring data in contrast swmm depends upon physically based parameters that are collected or derived from spatial data gathered at smaller scales a comparative assessment of hspf and swmm in simulating hydrology of watersheds has been conducted only in a few studies both were conducted in forested not urban watersheds lee et al 2010 compared swmm output with average streamflow from a large watershed during seven events the authors indicated that both models performed adequately however hspf simulated hourly streamflow better than swmm tsai et al 2017 applied swmm and hspf to a highly pervious forested watershed the authors indicated that hspf matched observed streamflow better than swmm this may have been due to the highly permeable soil of the watershed which likely created a strong baseflow response a key application of hspf is the simulation of hydrology and water quality of the chesapeake bay watershed usepa 2010 this is directly the result of hspf s simplicity which allows hspf to execute simulations of this large watershed faster this computational advantage is evident in execution of large watershed models for long times swmm s advantages are its ability to simulate flashy urban watersheds and assess scm performance as both models are widely used in urban areas understanding the similarities and differences between them is critical yet a comprehensive comparison has not been done the objective of this paper was to address this research need by comparing the capabilities of hspf and swmm as applied to a case study urban watershed hspf and swmm were each assessed in terms of the 1 most sensitive hydrologic parameters in the watershed 2 simulation of daily and monthly streamflows in comparison with observed data 3 simulation of peak flows baseflows and their respective durations and 4 predicted runoff coefficients during storm events with set return periods these results were then used to compare the subcomponents of the long term watershed hydrograph achieving a better understanding of the similarities and differences of swmm and hspf will help relate information from each model to the other which will assist in meeting water quality goals at the regional scale 2 materials and methods 2 1 site description stroubles creek located within montgomery county virginia lies within the valley and ridge physiographic province of virginia stroubles creek is a tributary to the new river which is tributary to the kanawha river and part of the mississippi river basin an urbanized 14 8 km2 headwater portion of the stroubles creek watershed was selected for this study fig 1 this subwatershed includes much of downtown blacksburg and the campus of virginia polytechnic institute and state university virginia tech this watershed was selected because 1 its headwaters are predominately 73 8 urbanized and 2 long term monitoring data are available the virginia tech stream research education and management stream lab stream lab 2009 continuously measures groundwater levels streamflows and records precipitation and other climatological data within the stroubles creek watershed land cover is 73 8 urban with a total imperviousness of 32 21 agricultural 4 forested and 1 2 water body multi resolution land use consortium 2011 fig 1 the dominant hydrologic soil group hsg of the headwaters is category c as classified by the natural resource conservation service nrcs 2007 1999a while downstream consists mainly of silt loam and loam soils which are category b mostaghimi et al 2003 the average elevation of the watershed is 670 m above sea level mean annual precipitation is 1030 mm hofmeister et al 2015 liao et al 2015 2 2 data collection storm sewer street parcel boundary and surface elevation geographic information system gis data were provided by the town of blacksburg town of blacksburg 2015 and virginia tech separate datasets were merged soil information was obtained from the soil survey geographic database ssurgo of the natural resources conservation service with scales ranging from 1 12 000 to 1 64 000 nrcs 1999b the monitoring station measures stream stage every 15 min using a pressure transducer cs451 campbell scientific inc logan ut usa with a water level resolution of 0 0035 fs full scale full span the difference between the lowest and highest measured point and a cr1000 datalogger campbell scientific inc u s stage was converted to discharge using a rating curve computed through the historical monitoring of stage flow precipitation was recorded at 15 min intervals at the stream lab metrological station using a tipping bucket rain gages tr 525usw texas electronics inc dallas tx 1 the stream lab weather station measured air temperature every 30 min at the approximately 300 m downstream of the stroubles creek monitoring station stream lab and the meteorological station are located at the watershed outlet the depth to surficial groundwater was measured every 10 min by two piezometers installed in the floodplain adjacent to stream lab using two cs451 water level loggers campbell scientific u s groundwater table elevation was quantified using geological maps of geology and mineral resources division of commonwealth of virginia appendix a and data from the stream lab pre installed floodplain piezometers 2 3 model initialization land cover data was initially used to initialize the models in a process described by ketabchy 2018 the principal input parameters used in development of the hspf and swmm models were land use soil properties stream characteristics and time series of precipitation and temperature a total of 43 subwatersheds were delineated within the stroubles creek watershed the watershed was delineated through arcgis 10 5 ketabchy et al 2018 correcting the delineation for urban features i e topography slope elevation land use etc where necessary the differences and similarities of each process feature and main input output variables for hspf and swmm are summarized in table 2 swmm uses a simplified darcy s law to simulate groundwater flows and interaction of surface water and groundwater of an aquifer through a number of parameters bottom elevation of aquifer groundwater surface water interaction parameters a1 a2 b1 and b2 which are listed in table 3 depth of unsaturated upper zone and lower saturated zone aquifer porosity and saturated hydraulic conductivity rossman 2010 these parameters control flow from the aquifer into the stream and vice versa and compute groundwater flow as a function of groundwater and surface water levels green ampt ga infiltration was applied for the infiltration module of swmm primarily because the watershed was semi urbanized and the physical basis of ga parameters such as suction head hydraulic conductivity and initial moisture deficit values are available through the soil survey geographic database ssurgo the dynamic wave dw algorithm was selected for hydraulic routing within swmm because this method can simulate non uniform and unsteady state flow conditions accurately the longest flow paths of each subcatchment were used to calculate its hydraulic width hw a required swmm parameter excess rainfall that exceeds depression storage is routed from each subcatchment through a nonlinear reservoir algorithm macro et al 2019 palla and gnecco 2015 xing et al 2016 each subcatchment is split into pervious and impervious portions and runoff is directed to a user defined outlet node or is routed across pervious areas the manning s roughness coefficient for pervious and impervious area is used to compute normal flow across a plane the plane being the subcatchment these eventually flow into either conveyance piping ditches and or streams through which flow is calculated by use of the manning s equation or through culvert formulas which depend upon upstream and downstream conditions hspf includes three principle modules perlnd pervious land implnd impervious land segments and rchres routing through reaches processes in receiving streams can be simulated using the rchres reach and reservoir module of hspf implnd module generates surface runoff whereas the perlnd module analyzes all three major processes including surface runoff interflow and groundwater all processes related to soil infiltration soil moisture groundwater baseflow separation etc are analyzed in these modules enabling hspf to predict the hydrology and water quality of watersheds berndt et al 2016 bicknell et al 2001 mohamoud and prieto 2012 xu et al 2007 the pwater and iwater sections in hspf control the water budget allocations between surface flow interflow baseflow storage interception detention and evaporation et pwat parm3 is one section of pwater which has two parameters of deepfr and agwetp for simulating groundwater recharge philips equation a physically based method that uses an hourly time step chezy manning s equation and kinematic wave kw were applied within hspf for simulating infiltration streamflow and hydraulic routing respectively bicknell et al 2001 within hspf the parameters lzsn and uzsn table 3 that control lower and upper zone storage are used to simulate water outflow from streams bicknell et al 2001 the inflt parameter is an index associated with the philips infiltration method for quantifying soil infiltration capacity there are three parameters that control groundwater and baseflow in hspf these are kvary agwrc and deeper which are functions of baseflow recession variation and the interactions between groundwater and surface water basetp is a parameter that represents the et of riparian vegetation when riparian vegetation is present its value starts with 0 03 singh et al 2005 intfw and irc are interflow parameters which are a function of soil topography and land use bicknell et al 2001 the major components of the water balance within the stroubles creek watershed include precipitation total runoff sum of overland flow interflow and baseflow total actual et sum of interception et aquifer upper zone et aquifer lower zone et baseflow et and active groundwater et and deep groundwater recharge each of the aforementioned water balance components have corresponding parameters in swmm and hspf table 3 2 4 baseflow separation direct runoff during storm events is the sum of overland flow and interflow while baseflow consists of groundwater discharge from the saturated zone of an underlying aquifer directly to streams lott and stewart 2013 miller et al 2016 rumsey et al 2015 baseflow affects aquatic habitats during dry periods and low intensity storm events during periods of high groundwater levels mccargo and peterson 2010 there are several methods to determine and separate baseflow from streamflow which are grouped into three general categories graphical analytical and mass balance methods lott and stewart 2016 baseflow separation partitions a stream hydrograph into baseflow and runoff the most widely used methods of baseflow separation are analytical lott and stewart 2016 eckhardt 2008 developed a two parameters equation through numerical analysis for baseflow separation which is calculated by eq 1 1 b k 1 b f i max a b k 1 1 a b f i y k 1 a b f i m a x where a is the groundwater recession constant y is the total streamflow b is the baseflow k is the time step and bfimax is maximum baseflow index there are three values for maximum baseflow index bfimax parameter including 0 80 for perennial streams with porous aquifers 0 50 for ephemeral streams with porous aquifers and 0 25 for perennial streams with hard rock aquifers in this study a hydrograph analysis tool kyoung et al 2005 was used for baseflow separation which uses the eckhardt method eckhardt 2008 the aforementioned method is able to separate baseflow more accurately than other numerical methods since it utilizes two parameter filters eckhardt 2008 neff et al 2005 since the current stream study is perennial with porous aquifers underneath a bfimax of 0 80 was used 2 5 analysis of storm events the behavior of each model during storms events with a set return period was assessed each calibrated model was used to simulate streamflow for the 1 2 5 10 25 50 and 100 year 24 hr precipitation frequency pf estimates at the outlet of the stroubles creek watershed the pf estimates were produced by national oceanic and atmospheric administration noaa atlas 14 with 90 confidence intervals noaa 2016 using the partial duration time series type natural resources conservation service nrcs type ii storm distribution was used to develop time series of 24 hr precipitation events nrcs 2015 groundwater discharge was assumed to be negligible during large storm events the runoff volume simulated at the outlet of the watershed by both models during the 24 hr precipitation was normalized to runoff depth through dividing by the connected impervious area of the watershed further runoff coefficients were calculated as the runoff depth divided by precipitation depth as essentially all streamflow was runoff during the event 2 6 sensitivity analysis sensitivity analysis sa is process of the adjusting inputs of a model and calculating the rate of change in model results sa techniques are grouped into local and global methods javaheri et al 2018 local sa methods evaluate the sensitivity of parameters around one local point the value of one particular input parameter was changed while other parameters were held constant during the simulation hence the sensitivity of streamflow as the main output of the models to input parameters can be represented by the sensitivity coefficient eq 2 james et al 1982 2 s c p y y 1 y 2 p m a x p m i n where sc is sensitivity coefficient p is the input parameter and y is the predicted output pmax and pmin are the maximum and minimum ranges of the initial default value and y1 and y2 are the corresponding output values the most sensitive model parameters in watershed hydrology have higher values of sc in addition global sensitivity analysis gsa techniques evaluate the sensitivity of parameters around the whole parameter space dobler and pappenberger 2013 javaheri et al 2018 the sensitivity analysis identified several key parameters that had a substantial impact on simulation results the sensitive parameters have the potential to significantly influence swmm and hspf simulation results in applying gsa to the case study the calibrated value of each model s input parameter was used as the baseline value each key model parameter value was varied one at a time with simulations run for plus and minus 10 of the published range in the parameter value table 2 this produced a total spread of 20 in the parameter value which was assumed to provide a reasonable estimate of inputs the two simulations produced for the modification of each input parameter provided upper and lower bounds of the simulation results these limits can be interpreted as error limits of simulation results this approach is often applied to address the performance evaluation of best management practices bmps and hydrologic models janke et al 2013 park et al 2011 2 7 calibration and validation hspf and swmm models represent hydrologic and hydraulic features of a watershed using fixed and process related parameters castanedo et al 2006 fixed parameters represent the hydraulic features of drainage networks while physical properties represent drainage basins properties such as length slope width depth and roughness of a watershed and areas covered by various soil types and land covers a flow chart describing the process of developing the hspf and swmm models in this study is shown in fig 2 process related parameters cannot normally be measured directly or cannot be calculated through gis information these include soil moisture storage groundwater discharge into stream et etc bicknell et al 2001 castanedo et al 2006 these parameters were adjusted manually during the calibration process between january 1 2013 and december 30 2013 for each model using hourly streamflow obtaining from stream lab there were 22 storm events during calibration period validation which consists of running the models with the calibrated parameters without adjustment was conducted for the period between january 1 2009 and december 31 2012 with 61 storm events the purpose of model validation is to assess if the calibrated models can simulate streamflow behavior for events outside of the calibration period the goodness of fit criteria for both calibration and validation periods were investigated using a group of statistical methods including coefficient of determination r2 gebremariam et al 2014 nasr et al 2007 seong et al 2015 nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias gupta et al 1999 according to duda et al 2012 and moriasi et al 2015 multiple statistics should be used rather than a single criterion a model performance rating system which compared the simulated versus observed datasets qualitatively was developed to assess model performance table 4 bennett et al 2013 ketabchy et al 2019 moriasi et al 2015 nayeb yazdi et al 2019a if the statistical parameters showed good or satisfactory agreement table 4 the model calibration was considered complete otherwise the model calibration parameters were adjusted further the calibration process stops when r2 and nse are greater than 0 6 and 0 5 respectively and pbias is lower than 0 25 fig 3 3 results 3 1 sensitivity analysis the results of the local sensitivity analysis for selected input parameters are presented in table 5 the most sensitive parameters in the hspf model were groundwater parameters deepfr agwrc followed by infilt lzsn parameters which are functions of soil and land use the most sensitive parameters of the swmm model was imperviousness sc 0 38 following by impervious depression storage sc 0 11 and subwatershed hydraulic width sc 0 03 these results are similar to previous studies ali and bruen 2016 seong et al 2015 tsai et al 2017 xing et al 2016 compared to hspf model the groundwater parameters within swmm including the groundwater flow coefficient groundwater flow exponent surface water flow exponent and surface water flow coefficient did not substantially affect swmm results 3 2 global sensitivity analysis results the baseline values of model outputs i e average streamflow average baseflow and associated variation in modeled outputs are shown in table 6 gsa was conducted on the most sensitive parameters in hspf and swmm with the upper and lower bounds serving as the extreme endpoints of simulation outputs during gsa variability of average streamflow for swmm was higher than that of hspf while variability of average baseflow for hspf was significantly greater than that of swmm the most sensitive parameters of the hspf model were attributed to groundwater discharge thus altering those parameters had direct a significant effect on baseflow this likely explains why hspf simulated baseflow had a larger variability during simulation than similar outputs from the corresponding swmm model the sensitive parameters of swmm were primarily attributed to imperviousness and infiltration which have a direct effect on runoff and streamflow table 6 3 3 comparison of models without calibration as a baseline for our study the hspf and swmm models were initially run for the entire period of record without calibration to assess the relative abilities of each model to match the observed data it should not be construed that the authors are recommending use of the models without calibration our supposition is that swmm would perform better than hspf without calibration for the aforementioned reasons parameter values for both models were left as estimated from external data sources or model defaults nse r2 and pbias for swmm was 0 52 0 58 and 22 and for hspf was 0 38 0 47 and 0 42 respectively the results indicated that without calibration swmm simulated streamflow far better than hspf earning an acceptable vs poor according to the metrics by moriasi et al 2015 this is due to the finer spatial scale of the inputs to swmm which are based more on the externally sourced data such as gis and the physics of the hydrological processes which control the catchment response while hspf is a process based model that relies on many parameters which can only be determined through calibration thus hspf is not useful without calibration whereas swmm without calibration while diminished somewhat may still provide useful information thus hspf is better for watersheds with monitoring data but only limited physical information the opposite is the case for swmm 3 4 calibrated input parameters the calibrated value ranges of input parameters for hspf and swmm models are presented in table 7 the hspf calibrated input parameters for soil and land use lzsn infilt were categorized for forest agricultural and urban land covers 3 5 comparison of models for average streamflow simulation goodness of fit results for calibration and validation periods are provided in table 8 the statistical analysis results showed good agreement between the simulated and observed streamflow the observed and simulated hydrographs of swmm and hspf for calibration and validation periods are shown in fig 4 during the calibration and validation periods swmm showed slightly better agreement between simulated and observed streamflow than hspf based on the statistical values of nse r2 and pbias the positive values of pbias for models during validation period indicates the propensity of the models to underestimate streamflow since visual comparison of the models results using fig 4b was hard to see two months i e december 2009 and may 2011 were separated for better visualization in a narrower data range fig 4c and d goodness of fit was also assessed by plotting the observed vs simulated values of streamflow in calibration and validation periods as shown in fig 5 swmm calibration replicates many of the storm event peaks reasonably well the slope of the regression line for the hspf calibration was less than 1 0 fig 5a while that of for swmm calibration period was close to 1 0 fig 5b some of the errors are likely due to the inability of the swmm and hspf models to capture streamflow peaks for some of the events fig 5a and b the slope of regression line for validation periods of swmm and hspf was approximately 0 7 indicating highly relative magnitude of the residuals to standardized residuals residuals equal to 0 0 swmm generally overestimated high magnitude flood events fig 5d while there was no certain pattern in simulating high magnitude flood events through hspf fig 5c the residual time series of daily streamflow versus time and precipitation is provided in fig 6 the hspf streamflow simulation average error during wet periods days with at least 0 25 cm precipitation and dry periods were 0 002 and 0 067 m3 s respectively while those of for swmm streamflow simulation were 0 067 and 0 070 m3 s respectively the aforementioned analysis indicates relatively better performance of both models in wet period than dry periods in terms of averaged error hspf appeared to be a better predictor of streamflow in wet periods rather than swmm during high magnitude storm events days with at least 2 cm precipitation swmm generally over estimated the streamflow while there was no specific pattern for hspf simulation error flow duration curves of simulated streamflow by hspf and swmm and observed streamflow are shown in fig 7 models simulated streamflow close to observed streamflow during high flows between 0 and 10 flow exceedance q10 hspf simulated streamflow between 10 and 90 of flow exceedance were slightly beneath observed streamflow while swmm over predicted streamflow during low flow overall based on a visual look the hspf simulation matched better in terms of flow exceedance pattern with observed streamflow compared to the swmm simulation fig 7 the top 10 of streamflow in magnitude according to fig 7 were selected as peak flows to evaluate the capability of hspf and swmm in peak flow simulation there was 81 days of high streamflow for observed dataset the corresponding pbias values of swmm and hspf models for peak flow were 0 098 and 0 120 respectively indicating that swmm was better at reproducing observed peak flows the average errors simulated observed of peak flows 7 1 for swmm and 8 1 for hspf confirmed the pbias statistical analysis results the pbias values average percent errors of models and fig 6 represent the overestimation and underestimation of peak flows by swmm and hspf respectively 3 6 comparison of models for monthly streamflow simulation the average monthly streamflow representing streamflow seasonally variation indicated that hspf and swmm models achieve better agreement with observed streamflow during winter months jan and feb rather than summer months may jun jul and aug fig 8 the swmm averaged percent differences of all months resulted in 15 while that of for hspf was 22 indicating swmm is a better predictor of seasonally streamflow variation the percentage difference between the swmm and hspf monthly simulated streamflow and the observed monthly streamflow ranged from 6 to 39 and from 3 to 48 respectively which can be classified as not good results for models when pbias is higher than 25 al abed and al sharif 2008 swmm performed better than hspf in summer months while hspf simulation matched relatively better with observed averaged monthly streamflow in winter than swmm generally both models under estimated the averaged monthly streamflow between january 2009 and december 2013 fig 8 the simulation of average monthly streamflow can be beneficial for assessing impact of projected climate and land use changes 3 7 comparison of models for baseflow simulation the baseflow was plotted as 1 total baseflow and 2 baseflow for dry periods dps or the periods in which precipitation and direct runoff are zero and groundwater discharge is the only source of streamflow fig 9 the observed dps baseflows between 2009 and 2011 was 317 days while that for swmm and hspf simulations were 693 and 199 days respectively fig 9b it indicates better performance of hspf in coverage of the number of dry days period the pbias values of swmm model for total baseflow and dps baseflow were 0 4 and 0 61 respectively while those of for hspf model were 0 31 and 0 53 respectively indicating better performance of hspf model in capturing observed total baseflow and dps baseflow as swmm and hspf models were not calibrated through observed baseflow the aforementioned pbias calculations and the respective discussion were only based on baseflow calculation using the eckhardt 2008 method and the calibrated average streamflow hspf captured the observed baseflow pattern better than swmm model fig 9a and b in contrast swmm followed a relatively constant baseflow pattern throughout the dps fig 9b our results are similar to previous study indicating that swmm has a limitation concerning baseflow simulation during dry periods particularly during winter months liu et al 2013 3 8 comparison of model response to standard storm events hspf and swmm models were compared during set return period events by running each using standard nrcs 24 h storms the blacksburg virginia 1 yr recurrence precipitation is 55 mm 2 2 in noaa 2016 during the monitoring period an event 07 july 2013 was identified and separated and are shown in fig 10a during this event nse r2 and pbias between observed and simulated data for swmm were 0 51 0 58 and 33 and for hspf were 0 45 0 52 and 20 respectively since the models were calibrated continuously these results for that event can be can be considered to be acceptable moriasi et al 2015 the simulated hydrograph for 1 yr recurrence interval are presented in fig 10 b results indicated that for extreme storm events swmm simulated peak flows greater than hspf while hspf simulated higher baseflow than swmm swmm tended to produce more runoff than hspf for simulated storms with recurrence interval equal or less than 10 yr fig 11 although the peak flows of swmm and hspf 24 hr storm distribution for the 100 yr recurrence interval were somewhat similar a steeper receding limb was evident in the swmm results compared to hspf this accounted for the difference in runoff volume 4 discussion statistical analysis indicated that both hspf and swmm models simulated streamflow adequately however the positive values of pbias for hspf and swmm indicated that both models had a propensity to underestimate streamflow in addition the performance of both models for simulating streamflow during wet periods days with at least 0 25 cm precipitation was relatively better than dry periods it may have been stemmed from the capability of the respective philips and ga models which were used for estimating infiltration rate in hspf and swmm respectively this is because during storm events the philips and ga models estimate infiltration rate relatively better than during dry periods chahinian et al 2005 wilson 2017 during high magnitude storm events days with at least 2 cm precipitation swmm generally over estimated the streamflow while there was no specific pattern for hspf simulation error hspf appeared to be a relatively better predictor of streamflow in wet periods rather than swmm this may stem from the relative performance of the ga and philips models as the philips infiltration model represented wet periods closer to reality than ga did wilson 2017 in terms of simulating streamflow seasonally swmm performed better than hspf in summer months while hspf simulated streamflow better than swmm in winter generally the philip model estimated higher infiltration rates compared to ga turner 2006 wilson 2017 this difference could explain the previously mentioned better performance of hspf in capturing total baseflow and dps baseflow in comparison to swmm furthermore hsfp and swmm use kw and dw methods for runoff stream routing respectively previous studies indicated that the dw method is more appropriate for obtaining the reference discharge and can capture high flows better than kw moramarco et al 2008 soentoro 1991 this may explain why peak flows were better represented by swmm than hspf overall the performance difference between hspf and swmm in simulating streamflow may be due to the methods were employed for simulating infiltration rate and water routing these methods resulted in swmm simulating streamflow better than hspf within the case study urban watershed in addition in the absence of available monitoring data within a watershed swmm likely provides better results 5 conclusion models developed using hspf and swmm were used to simulate streamflow for a case study urban watershed the stroubles creek watershed in blacksburg virginia sensitivity analysis was applied only on process related parameters based on sensitivity analysis the most sensitive hydrologic parameters within hspf were groundwater parameters i e deepfr and agwrc while for swmm it was the percentage of imperviousness gsa indicated that variation for simulating baseflow averaged for hspf was greater than swmm while for simulating streamflow the variability of swmm outputs was greater than hspf swmm performed better than hspf sans calibration due to the inclusion of more detailed watershed topology and scms analysis of the residual time series of daily streamflow simulated observed indicated that both models performed better during wet rather than dry periods the comparison results of models for dry periods indicated that hspf could simulate the total baseflow and dps baseflow better than swmm while the opposite was the case for peak flows analysis of extreme storm events was also conducted the runoff coefficient for swmm was generally greater than hspf for recurrence intervals of 1 2 5 and 10 yr and the opposite was true for recurrence intervals greater than 10 years the results of this study can assist urban watershed planners in translating their results from small scale urban watershed models where scms are implemented to larger regional scale models where compliance is assessed it can also guide in the selection of the most appropriate model for their urban watershed acknowledgements funding for this work was provided in part by the virginia agricultural experiment station and the hatch program of the national institute of food and agriculture u s department of agriculture project s1063 the authors appreciate the data provided by town of blacksburg and stream lab with w cully hession and laura lehmann as director and manager respectively the authors would like to acknowledge karen kline and adil godrej virginia tech and robert burgholzer virginia department of environmental quality for their helpful comments appendix a the geologic map of the stroubles creek watershed the geologic map of the stroubles creek watershed georeferenced and digitized as a hard copy from the geology and mineral resources division of commonwealth of virginia 1985 is displayed in fig a1 below are the descriptions concerning characteristics of each geologic type of the watershed td talus deposits the area beneath the ponds of the watershed unconsolidated unsorted boulder fields composed of 0 3 1 8 m thick angular boulders of quartzite siliceous sandstone and quartzes conglomerate this detritus has been derived from nearby state of mississippian silurian and devonian age thickness 0 9 2 m ce elbrook formation the uppermost part of the formation is characterized by interbedded sandy commonly crossbed fine grain dolomite containing thin 1 10 cm lenses of fine to medium grained sandstone and 0 3 1 2 m thick ribbon banded limestone dolomite cr rome formation the area mostly at the eastern portion of the watershed consists of interbedded mottled maroon and green phylittic mudstone fine grained sandstone and siltstone and dark gray fine grained dolomite ccr copper ridge formation the area upstream of the watershed inter bedded medium gray fine to medium grained locally grained massive dolomite supper siliceous oolite and quartzose sandstone total thickness is about 366 m fig a 1the geologic map of the stroubles creek watershed fig a 
26206,combining simulation models and multi objective optimization can help solving complex land use allocation problems by considering multiple often competing demands on landscapes such as agriculture drinking water provision or biodiversity conservation the search for optimal land use allocations has to result in feasible solutions satisfying real world constraints we here introduce a generic and readily applicable tool to integrate user specific spatial models e g assessing different ecosystem services for a constrained multi objective optimization of land use allocation comola the tool can handle basic land use conversion constraints by either a newly and specifically developed method to repair infeasible solutions or by penalizing constraint violation comola was systematically tested for different levels of complexity using a virtual landscape and simple ecosystem service and biodiversity models our study shows that using repair mechanisms seems to be more effective in exploring the feasible solution space while penalizing constraint violation likely results in infeasible solutions keywords land use allocation ecosystem services biodiversity multi objective optimization constraint handling software availability name of software comola programming language python packages used inspyred developers carola pätzold christian schweitzer sven lautenbach michael strauch contact address helmholtz centre for environmental research ufz department of computational landscape ecology permoserstrasse 15 d 04318 leipzig germany e mail michael strauch ufz de availability https github com michstrauch comola 1 introduction achieving the goals of global environmental sustainability requires the conservation of biodiversity as well as ensuring the provision of ecosystem services such as drinking water food or timber while these objectives are inextricably linked they are often studied and managed separately from each other liu et al 2015 sustainable resource use and management strategies that enhance landscape multifunctionality require not only understanding the manifold interactions among multiple demands but also finding solutions to minimize their trade offs while progress has been made in the model based quantification of land use effects on ecosystem services and biodiversity over the past few years the question of w here to put things polasky et al 2008 is still an important challenge land allocation problems can be solved by model based approaches combined with either scenario analysis e g fontana et al 2013 backcasting approaches e g brunner et al 2016 or multi objective optimization techniques optimization methods are capable to explore a large number of land use land management configurations memmah et al 2015 seppelt et al 2013 combined with simulation models e g hydrological biodiversity or socio economic models they are applicable for a wide variety of problems existing studies on land use optimization were reviewed for example by kaim et al 2018 kanter et al 2016 and memmah et al 2015 optimization applications differ in terms of scale from single fields farms to large river basins the objectives considered e g agricultural productivity water quality biodiversity recreational value and the timing of preference articulation of stakeholders and decision makers before during or after the optimization process such multi objective land use allocation problems can be solved either by integrating all objectives into one single function following a weighted sum approach marler and arora 2010 or by pareto based methods where all objectives are treated individually using the concept of pareto optimality deb 2014 the latter approach provides a set of pareto optimal solutions i e solutions for which no objective can be further improved without compromising at least one of the other objectives as those solutions are usually only approximations of true pareto optimality we refer to them as non dominated solutions pareto based optimization methods have been used in several land use optimization studies e g in bennett et al 2004 cao et al 2011 chikumbo et al 2015 duh and brown 2007 groot et al 2012 huang et al 2013 lautenbach et al 2013 roberts et al 2011 and schwaab et al 2017 2018a the advantage of non dominated solutions is that they directly illustrate the best possible trade offs among conflicting objectives from such a set of best alternatives decision makers can discuss and select appropriate solutions according to their preferences cord et al 2017 to obtain non dominated solutions population based meta heuristics i e methods based on evolutionary computation or swarm intelligence are particularly suitable because they generate and evaluate a set of potential solutions in parallel and approach the pareto front within a single run of the algorithm boussaïd et al 2013 coello et al 2007 identifying non dominated solutions also allows evaluating certain projections on future land use change regarding their overall optimality seppelt et al 2013 verstegen et al 2017 such non dominated solutions however have little value for decision making if they ignore real world constraints for example depending on rules and regulations in the study region grassland may be transformed into forest but not into cropland or urban areas in the same case study context no specific transition rules may exist for other land use classes e g cropland alternatively the goal might be to efficiently allocate a specified demand for urban development in a district or municipality schwaab et al 2017 this problem is different from the problem of constraining based on objective values such as setting threshold values for one or several objectives in contrast to constraining the objective values e g a minimum level of species richness or a maximum value for nitrate concentration in the groundwater constraints are put on the control variables here for land use allocation problems these constraints on the control variables could involve the minimum and maximum allowed total area of certain land use classes or transition rules controlling which classes are allowed to be replaced by others and which not since evolutionary algorithms are by nature unconstrained it is necessary to find proper ways of incorporating real world constraints coello coello 2002 there are three constraint handling methods commonly applied in land use optimization studies 1 penalty functions that degrade the fitness value of an infeasible solution e g chikumbo et al 2015 shaygan et al 2014 stewart et al 2004 2 feasibility operators which create feasible only child solutions e g garcía et al 2017 karakostas and economou 2014 and 3 repair mechanisms for infeasible individuals e g cao et al 2012 datta et al 2007 schwaab et al 2018b the latter two approaches require modifications of the genetic algorithm which are often specific for the optimization problem at hand schwaab et al 2018b their adaptation to different objective functions different land use types including their transition or total area rules may require extensive effort and programming skills if the source code is available at all to foster the use of population based optimization in landscape management we here present a generic tool for constrained multi objective optimization of land use allocation comola including constraint handling for the control variables with large flexibility and broad applicability comola employs the non dominated sorting genetic algorithm ii nsga ii developed by deb et al 2002 among multi objective evolutionary algorithms nsga ii is the most often used procedure for solving spatial allocation problems malczewski and rinner 2015 we enhanced the algorithm by implementing two types of constraint handling methods 1 a novel repair algorithm specifically developed for land use transition and composition constraints and 2 constrained tournament selection a commonly used penalty function approach deb 2000 osyczka and krenich 2000 the objective of this research paper is to introduce the functionality of comola and compare the performance of comola s repair algorithm with the established method of constraint tournament selection to test all important facets of the tool we used a virtual land use map which was designed to optimize multiple conflicting objectives maximizing crop and water yield as well as two biodiversity indicators while considering basic land conversion constraints total area and transition rules our optimization experiments include different levels of complexity with respect to the number of objectives the strength of the pre defined constraints and the number of spatial units to be optimized 2 description of comola comola is a free python tool to optimize raster maps for multiple objectives it is based on the open source inspyred python library garrett 2012 which is extended to include functions for reading encoding and writing raster maps as well as algorithms to consider constraints during the optimization procedure it is platform independent and allows for the integration of any model which reads gridded maps as input data for instance for estimating ecosystem service indicators comola can be used immediately by inputting a raster map representing the baseline situation e g for land use ready to run models written in r r core team 2016 or python and optional information on constraints as constraints for the control variables the tool is able to consider both 1 transition rules defining the possible land use transformations and 2 minimum and maximum area proportions for each land use class within the study area all relevant settings such as paths to input data and models as well as optimization specific parameters e g population size crossover and mutation rates and settings related to constraint handling and raster map analysis are managed in a single control file as the framework allows users to integrate any kind of simulation models e g process based statistical or both at the same time it can be a useful tool for a broad range of spatial allocation studies for more details and examples how to adapt and apply comola the reader is referred to the user guide on github https github com michstrauch comola the optimization procedure is summarized in fig 1 a and can be briefly described as follows assuming a land use allocation problem 1 preprocessing encoding the original raster map to decrease the number of spatial units and hence the computational effort the algorithm first transforms the input raster map representing the current status of land use into a patch or cluster map fig 2 where neighboring raster cells of the same type are aggregated cf holzkämper and seppelt 2007 liu et al 2016 the patch id map is then encoded as a string of integers each value is called a gene representing the land use of a patch that forms the genome of the initial individual i e the first individual of the initial population however users may also specify their own patch id map to control the aggregation of cells into patches the patch id map hence allows complete flexibility to delineate spatial units if desired the user can also conduct a cell level optimization the patch id map can be thought of as a polygon map that defines the units that the optimizer modifies with respect to land use the approach combines the possibility to use irregularly shaped patches as modeling units while allowing the use of raster processing functions which are more efficient for neighborhood analysis problems 2 constraint controlled genome generation cg to fill up the initial population in the second step all further individuals of the initial population are generated the original land use map is always included in the initial population the user defined size of the population depends on the nature of the problem but should usually not be higher than a few hundred individuals malczewski and rinner 2015 without considering constraints the genome generation would be completely at random and based on a uniform sampling among all available land use classes if constraints are defined the initialization uses a constraint controlled genome generation cg algorithm to ensure all individuals of the first population are feasible and fulfill all given constraints it is known that the search process of an optimizer becomes more efficient if its initial guess is feasible or near feasible datta et al 2012 cg operates on a gene by gene i e if not defined otherwise a patch wise basis to generate a genome for each gene the random choice of a land use class is constrained according to the pre defined transition rules fig 3 and a tabu memory which is a list of already identified infeasible gene sequences and visited feasible solutions each time a land use class is assigned to a gene the algorithm examines the genome sequence generated up to that point for any violation of the total area constraint if pre defined for this land use class if there is no violation the genome generation proceeds with the next gene i e the next patch id if the total area constraint is violated the gene sequence is added to the tabu memory and the algorithm returns to a suitable previous gene the search is then repeated for a feasible land use class according to the transition rules and the updated tabu memory in addition the user can choose to include boundary solutions in the initial population which are solutions at the boundary of the feasible solution space in cases where total area constraints are defined two boundary solutions are automatically generated within comola for each land use class one solution with the minimum permissible and another with the maximum permissible coverage of that respective land use class boundary solutions may have a good fitness for at least one objective and good initial estimates also referred to as seeds may generate better solutions with faster convergence friedrich and wagner 2015 3 run model s quantify objectives fitness values for each individual i e each land use map of the population are estimated through user defined models since the search ability of pareto dominance based evolutionary algorithms such as nsga ii deteriorates with increasing number of objectives ishibuchi et al 2008 we recommend using a maximum of four objectives however due to the modular architecture of comola it is possible to implement other optimization algorithms such as nsga iii deb and jain 2014 to handle more than four objectives for quantifying the single objectives e g agricultural productivity water quality biodiversity etc any type of model can be integrated as long as the model is able to handle a raster map as land use input and produces only one aggregated value per objective that is representative for the whole map 4 non dominated sorting tournament selection crossover mutation based on the fitness values of each member of the current population nsga ii applies a non dominated sorting scheme archives the best solutions and uses binary tournament selection mating crossover and mutation operators to create an offspring population as described in deb et al 2002 5 constraint controlled repair mutation cm even if the parent population exclusively consisted of feasible individuals the offspring population likely contains infeasible solutions due to crossover and mutation yoon and kim 2014 infeasible individuals of the offspring population are transformed into feasible individuals using a constraint controlled repair mutation cm algorithm which operates patch wise exactly as constraint controlled genome generation step 2 but with the premise that only a minimum number of genes should be modified in order to make an infeasible genome feasible hence the repaired feasible individuals remain as similar as possible to the original infeasible solutions suggested by nsga ii cm is thus a further operator besides crossover and mutation that repairs infeasible offspring the implemented tabu memory ensures both an efficient repair of infeasible individuals and uniqueness among all feasible individuals generated avoiding redundant model evaluations and fostering diversity in the solution space steps 3 to 5 are repeated until a termination criterion is reached e g the user defined maximum number of generations or any other pre defined terminator of the inspyred package the combination of cg and cm cg cm is a novel and straight forward constraint handling method specifically designed for comola to handle transition and area proportion constraints in land use optimization studies moreover comola can employ constraint tournament selection cts as an alternative method fig 1b cts has been suggested by osyczka and krenich 2000 and deb 2000 and represents an enhanced version of the binary tournament selection where two solutions are picked from the parent population and the better solution is chosen for mating without using repair mutation operators each solution can be either feasible or infeasible feasible solutions are preferred over infeasible solutions if both solutions are feasible cts prefers the one with better fitness in case both solutions competing in a tournament are infeasible cts prefers the solution with smaller constraint violation deb et al 2002 such a modification of the binary tournament selection is also proposed for the recent nsga iii algorithm to solve constrained many objective problems jain and deb 2014 in comola constraint violation δ o is composed of two terms one refers to the land use area violation δ a and the other to the violation of transition rules δ t 1 δ o δ a δ t with 2 δ a k 1 k l k c c 0 0 c 0 and 3 δ t j 1 j φ j φ j p j t r a n s i t i o n r u l e i s v i o l a t e d f o r p a t c h j 0 e l s e k is the total number of land use classes with k 1 k while c is the number of land use classes violating the area constraint l denotes the area deviation of land use k from its permissible min max range in of the total study area i e l 0 if the area constraint for land use k is violated and l 0 if not j is the total number of land use patches with j 1 j and p j denotes the area of patch j where transition rules are violated in of the total study area 3 optimization experiment 3 1 input land use data and objective functions for the performance test a virtual landscape of 10 10 raster cells was designed fig 4 a for which a discrete encoding strategy was applied defining eight land use classes 1 5 cropland ranging from lowest 1 to highest production intensity 5 6 pasture 7 forest 8 urban area we assumed a cell size within the range of 100 to 10 000 m2 as is typical for landscape scale assessments we considered up to four contrasting objectives including agricultural production water yield and biodiversity while taking into account land use intensity landscape composition and landscape configuration as suggested by seppelt et al 2016 all four objectives are to be maximized for the ease of evaluation and interpretation each objective was quantified by simple yet plausible conceptual models as summarized in table 1 3 2 experimental design we compared our novel repair algorithm cg cm with cts a standard approach to handle constraints and a combination of both approaches where cts is based on a feasible start population cg cts the comparison considered different levels of complexity as briefly explained in the following and illustrated in fig 5 3 2 1 individual size number of spatial units scalability is a key issue for optimization algorithms especially for solving spatial allocation problems for which the number of decision variables is determined by the size and heterogeneity of the study area the spatial resolution and the precise nature of the optimization problem e g the number of optional land use classes there is empirical evidence that efficacy and efficiency of multi objective metaheuristics decrease for large numbers of decision variables antonio and coello coello 2016 durillo et al 2010 constraint handling based on repair mechanisms such as gc gm comes at additional computational cost which might increase excessively with individual size i e in our case with the number of spatial units considered our experiment therefore included performance tests with a varying number of spatial units ranging from 41 to 400 the lowest complexity n 41 was represented by clustering neighboring cells of the same land use type land use map fig 4a into patches the number of spatial units was increased to 100 in a cell level optimization using the same map we further considered individuals of size 400 by increasing the spatial resolution by factor two in a cell level optimization in each case the initial land use distribution fig 4a was retained 3 2 2 number of objectives the performance of pareto based algorithms also generally decreases with increasing number of objectives due to the inefficiency of the pareto relation in high dimensional spaces n 3 knowles and corne 2007 li et al 2015 lópez jaimes and coello coello 2015 we tested the applicability of comola for maximizing two crop yield and habitat heterogeneity three crop yield habitat heterogeneity and forest species richness and four objectives crop yield habitat heterogeneity forest species richness and water yield 3 2 3 strength of constraints one can assume that the effect of constraint handling methods on optimization performance depends on the strength of the pre defined constraints we therefore tested two different constraint scenarios the first scenario cons i took only a set of transition rules into consideration where 1 existing cropland could be converted either into a different type intensity of cropland or into pasture or forest 2 existing pasture could be converted into forest but not the other way around meaning that 3 existing forest had to remain at the same location while 4 urban areas were not allowed to change at all the second scenario cons ii used the same transition rules but also took into account the permissible total area for different land use classes ranging from 10 to 30 for forest from 10 to 20 for pasture and from 10 to 25 for cropland 1 and 5 respectively in addition we applied comola without constraints using the original nsga ii algorithm without any modification the unconstrained optimization runs served as a reference for the maximum solution space regardless of their realism 3 3 general settings and performance evaluation for each optimization run we used the same population size 300 number of generations 300 crossover rate 0 9 mutation rate 1 individual size and included boundary solutions seeds within the initial population in addition to the number of feasible non dominated solutions we used the hypervolume metric as proposed by zitzler and thiele 1999 to evaluate each of the different solution sets this widely accepted multi objective performance metric measures both convergence and diversity on a single scale without requiring the knowledge of the true pareto front for comparison jiang et al 2014 hypervolume represents the volume in the objective space that is dominated by the set of feasible solutions given a certain reference point such as the origin of coordinates higher values of the hypervolume hence indicate that the solutions are closer to the true pareto front and at the same time that they are more evenly scattered in the objective space jiang et al 2014 we normalized the fitness values for each objective by dividing by their unconstrained theoretical maximum value section 3 1 which resulted in values for each objective between 0 and 1 this ensured no objective was overrepresented in the calculation of hypervolumes moreover using the unconstrained theoretical maximum as a fixed normalization factor allowed comparability among different constraint scenarios and constraint handling methods the r package mco mersmann 2014 was applied where the origin of coordinates was defined as reference point we used wall time or real time includes the cpu and the kernel time as a measure of computation time since we are dealing with stochastic algorithms we have executed ten independent runs for each problem instance which led to a total of 350 optimization runs each optimization run was performed on a linux cluster using 20 cores in parallel routines to run the models in parallel are included in the tool 4 results discussion 4 1 plausibility and visualization of non dominated solutions as expected the front of non dominated solutions for the unconstrained optimization problem completely dominated the identified front for a constrained problem see two objective optimization example in fig 6 if land use composition and land use transitions are unconstrained the optimizer is able to explore a larger search space however not a single solution of the unconstrained optimization run satisfied our hypothetical constraints on land use transition and total area for the individual land use classes pointing out the obvious need for constraint handling in our example application using repair mutation in contrast ensured feasibility of any single individual generated during the optimization the shape of the fronts implies an almost linear negative relationship trade off between the two objectives which is plausible since increasing habitat heterogeneity impairs crop production as defined in our models the shape of the true pareto fronts might be more convex due to the impact of soil fertility on crop yield however from analyzing the example maps also shown in fig 6 it is fair to assume that the optimization runs identified at least near optimal solutions this assumption seems plausible especially because highest intensity cropland has been pre dominantly allocated in the right part of the maps where soil fertility is highest cf fig 4b leaving the mosaic of extensive land use classes responsible for highest habitat heterogeneity in the left part of the maps results for the constrained optimization runs considering three and four objectives were plausible in the same sense see figs 7 and 8 forest species richness as a third objective showed a clear trade off with crop yield but not with habitat heterogeneity because of the pre defined constraints allowing forest not to exceed a total area of 30 there was no single maximum solution for forest species richness but a whole set of solutions ranging over large parts of the objective space including the maximum of habitat heterogeneity this is why the marked maximum solutions for forest species richness coincided with the maximum solutions for habitat heterogeneity in figs 7 and 8 water yield as fourth objective fig 8 favored low intensity cropland and thus showed a clear trade off with forest species richness similar to the crop yield forest species richness relationship however the relationship between water yield and both crop yield and habitat heterogeneity at the pareto frontier was not that clear moderate crop yield and habitat heterogeneity occurred at maximum water yield caused by the highest share of low intensity cropland for this solution what becomes obvious from fig 8 is that visualizing a four dimensional solution space in just one graph is extremely challenging in particular for a high number of solutions it is known that the number of non dominated solutions increases enormously with the number of objectives deb 2001 causing incomparability of solutions and inefficiency of pareto based sorting schemes e g li et al 2015 this is why nsga ii is usually not used for more than three or four objectives the number of solutions could have been reduced by applying an ε dominance analysis laumanns et al 2002 in such a case colored 3d scatterplots or even 2d scatterplots with color and point size as third and fourth dimension respectively seem to be adequate cf lautenbach et al 2013 our suggestion of projecting a colored 3d pareto cloud onto three orthogonal planes of a cube while leaving out the actual 3d plot as shown in fig 8 might be a possibility to visualize a high number of 4d solutions however finding proper techniques for visualizing high dimensional solutions is a research field on its own cf chiu et al 2009 he and yen 2016 4 2 optimization performance the performance of comola was determined by 1 the selection of the constraint handling method 2 the definition of the optimization problem in terms of constraints and 3 the size of an individual i e the number of spatial units involved in general we found that repair mutation cg cm clearly outperformed both methods based on constraint tournament selection cg cts and cts this was indicated by both a larger number of feasible individuals in the final set of non dominated solutions and larger hypervolumes dominated by the feasible solutions cf fig 9 and additionally fig a2 in the supplementary for considering two and four objectives our results thus imply that for discrete optimization problems repair mechanisms might be the preferable option to handle hard constraints as also found by zydallis and lamont 2001 for non spatial problems the differences in optimization performance among constraint handling methods increased with the strength of constraints and the size of an individual while a considerable amount of feasible solutions was found with all methods for the less restrictive constraint scenario cons i transition constraints only the number of feasible solutions was substantially smaller for the more restrictive scenario considering both transition and total area constraints cons ii for cons ii in combination with large individual sizes 100 and 400 not even a single feasible individual was included in the final set of non dominated solutions when using constraint tournament selection which is why hypervolumes were assigned a value of zero in this case fig 9 in contrast our proposed repair algorithm ensured feasibility and uniqueness of all individuals n 90 300 generated during an optimization run the improved performance of repair mutation however came at a cost as coello et al 2007 stated repairing infeasible solutions in order to make them feasible may be computationally expensive we found that the runtime for cg cm increased substantially with the strength of constraints fig 9 the smaller the feasible solution space the more effort is required to generate adequate individuals compared to the other methods that we tested cg cm required three times more computation time in the more restrictive constraint scenario with the smallest size of individuals while the difference was not as pronounced for the less strict constraint scenario further the difference in runtime increased enormously with larger size of individuals in our case up to factor 20 when handling individuals of size 400 compared to 41 in contrast runtime with cts based constraint handling increased only marginally fig 9 however cts and cg cts might be of little use for larger individuals and strong constraints as they may result in only infeasible solutions as discussed above the achieved level of hypervolumes and thus the optimization performance decreased substantially with individual size c f fig 10 this is also indicated by the different slopes of the hypervolume curves in case of repair mutation hypervolume curves converging laterally towards a certain threshold as it is the case for the smallest individual size might suggest but not guarantee a good approximation of the true pareto front curves progressing with a distinct slope until the end of the optimization process after 300 generations as shown for larger individual sizes clearly indicate further potential of progress for constrained tournament selection the hypervolumes of the entire solution space i e all solutions with no guarantee of feasibility were considerably larger than the hypervolumes achieved with repair mutation however in the cts based optimization with larger individual sizes 100 and 400 infeasible individuals completely displaced the feasible ones after only a few generations this is why here the hypervolume progress of the feasible solution space as reconstructed a posteriori from the optimization archive comes to an abrupt end see middle and right panel of fig 10 although constrained tournament selection generally favors feasible individuals over infeasible ones the displacement of feasible individuals might be caused by crossover and mutation which presumably tended to violate the given constraints the types of constraints considered here especially the minimum and maximum allowable total area of a specific land use class hampered the generation of feasible offspring even if feasible parents were selected for mating offspring individuals in spatial optimization are always a mosaic of subsets of parent maps and even though the parent maps are feasible according to the total area constraints that does not have to be true for their subsets and even less so for the product of recombination constraint handling methods focusing only on the selection process for mating such as cts may therefore not be adequate for spatial optimization problems that include hard total area constraints the decrease in the number of feasible solutions with increasing number of generations for cg cts and cts was stronger for optimization problems with larger individuals because the likelihood of a constraint violating land use change increases with the number of spatial units the difference in terms of feasible individuals per population for cg cts and cts was marginal between individual size 100 and 400 in both cases the number of feasible individuals in a population dropped to zero after around 50 and 20 generations for cg cts and cts respectively fig 10 and for a closer look on the first 75 generations fig a4 in the supplementary in case of cg cts all 300 individuals of the initial population generation 0 were forced to feasibility due to the constraint controlled genome generation cg algorithm in cts the optimization started with only 14 feasible individuals representing the boundary seeds which were included in each experiment the higher number of feasible individuals in the initial population explains why hypervolumes were larger for cg cts compared to cts the benefit of accounting for feasibility in the initial population was also previously reported by other studies e g datta et al 2012 haubelt et al 2005 for the smallest individual size feasible individuals only became nearly extinct for cg cts and cts from around generation 75 the number of feasible individuals plateaued at a low level irrespective from their initial number 300 in cg cts vs 14 in cts this explains why differences in hypervolume evolution between cg cm and the constraint tournament selection approaches were less pronounced for the smallest individual size the loss of performance with increasing size of individuals was observed for all settings independent from whether constraints were considered or which constraint handling method was used cf figs 9 and 10 hypervolumes for cg cm decreased on average by 10 and 38 when increasing the individual size from 41 to 100 and 400 respectively the average performance loss was similar for the unconstrained optimization runs with 10 and 46 both increases in runtime and decreases in performance with increasing individual size were caused by the exponential growth of the number of possible combinations n that equaled k j in optimization runs without constraints where k is the number of possible land use options and j is the number of spatial units patches or grid cells this amounted to n 8 41 8 100 8 400 possible options for each of the different individual sizes tested respectively if constraints are defined n is smaller but cannot be predicted because the number of possible land use options varies across the spatial units and can be even variable for a single spatial unit when considering total area constraints despite the performance loss for larger individual sizes comola with its implemented repair algorithm was able to detect a multitude of feasible solutions that significantly outperformed the status quo land use simultaneously for all objectives and such a set of solutions might still have utility in real world case studies cf fig a5 in the supplementary this was also shown by verhagen et al 2018 who applied comola for 277 pasture farms and 141 fruit orchards which equals a total individual size of 418 in the kromme rijn area 219 km2 in the netherlands to optimally allocate different agri environment measures under consideration of both land use transition and total area constraints the authors aimed to simultaneously maximize the yearly fruit yield profit the potential habitat of the great crested newt triturus cristatus and the aesthetic value of the landscape while minimizing the loss in pasture production the study also showed the potential of comola as a model integration and optimization framework to provide insights into the functional trade offs associated with different management options and their respective potential to increase landscape multifunctionality by using comola verhagen et al 2018 further showed how landscape optimization approaches can be integrated into spatial planning and inform policy design and implementation 5 conclusions this paper introduces comola as a tool for constrained multi objective optimization of land use allocation and analyzed its functionality and performance using virtual data and simplified models under consideration of different levels of complexity we found that comola is able to identify near optimal solutions for up to four objectives and the smallest individual size tested n 41 if solutions close to the true pareto front are sought the individual size in an unconstrained optimization should therefore not exceed a value of 120 40 for two eight possible land use options as this amounts to a potential decision space of 2120 similar to our smallest individual size experiment since constraints reduce the potential decision space these limits might be higher for constrained optimization runs this analysis found that further increasing the individual size can lead to a substantial loss of optimization performance by using our approach of a constraint controlled genome generation and repair mutation comola was still able to identify multiple solutions that were significantly better than the status quo land use distribution in all objectives simultaneously thus if it is sufficient to improve the status quo of a real world case study without approaching true pareto optimality individuals with a size of several hundred might still be viable for decently sized computational resources for an application at larger scales it might become necessary to aggregate spatial units apart from that time consuming simulation models can increase computational time dramatically if they have to be called repeatedly during the optimization the use of meta models also called surrogate model is therefore encouraged where feasible maier et al 2014 repair mutation was found to be advantageous over constraint tournament selection at least for spatial optimization problems with transition and total area constraints repairing infeasible individuals is however computationally intensive and can thus require longer runtimes depending on the strength of the pre defined constraints each of our constraint scenarios included hard constraints which we wanted to be satisfied by all means in case a land use optimization problem includes only soft constraints such as for example total area rules which should be matched only approximately penalizing constraint violation during tournament selection could be a better option preferably with a feasible initial population comola is able to handle basic constraints on land use transition and the total area for each land use class future developments should focus on the implementation of more sophisticated constraints such as spatial relationships between land use types or compactness of land use patches e g eikelboom et al 2015 to increase the efficiency of comola it might be promising to implement and test further constraint handling methods such as crossover operators explicitly accounting for feasibility as proposed by garcía et al 2017 and karakostas and economou 2014 or the repair mutation methods suggested by schwaab et al 2018b it might also be worth utilizing more advanced many objective evolutionary algorithms deb and jain 2014 li et al 2015 rui et al 2013 to overcome problems related to inefficient pareto based ranking and thus to include more than four objectives in future applications comola provides a generic framework for combining knowledge and models from various disciplines so that the best possible trade offs between different landscape related objectives can be explored stakeholders can participate in such a framework by defining optimization objectives and constraints as well as by critically evaluating the set of pareto optimal solutions to derive recommendations for desired land use and conservation strategies it therefore has a high relevance and application potential for studies focused on spatial planning integrated land and water management and any other field of research that aims to optimize spatial patterns for multiple objectives acknowledgements this research was funded through the 2013 2014 biodiversa facce jpi joint call with the national funder bmbf german federal ministry of education and research project tale towards multifunctional agricultural landscapes in europe assessing and governing synergies between food production biodiversity and ecosystem services grant 01 lc 1404 a author contributions m s a c r s and m v designed the research c p m s s l c s and a k contributed with programming and software documentation m s performed the research all authors contributed in writing the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 003 
26206,combining simulation models and multi objective optimization can help solving complex land use allocation problems by considering multiple often competing demands on landscapes such as agriculture drinking water provision or biodiversity conservation the search for optimal land use allocations has to result in feasible solutions satisfying real world constraints we here introduce a generic and readily applicable tool to integrate user specific spatial models e g assessing different ecosystem services for a constrained multi objective optimization of land use allocation comola the tool can handle basic land use conversion constraints by either a newly and specifically developed method to repair infeasible solutions or by penalizing constraint violation comola was systematically tested for different levels of complexity using a virtual landscape and simple ecosystem service and biodiversity models our study shows that using repair mechanisms seems to be more effective in exploring the feasible solution space while penalizing constraint violation likely results in infeasible solutions keywords land use allocation ecosystem services biodiversity multi objective optimization constraint handling software availability name of software comola programming language python packages used inspyred developers carola pätzold christian schweitzer sven lautenbach michael strauch contact address helmholtz centre for environmental research ufz department of computational landscape ecology permoserstrasse 15 d 04318 leipzig germany e mail michael strauch ufz de availability https github com michstrauch comola 1 introduction achieving the goals of global environmental sustainability requires the conservation of biodiversity as well as ensuring the provision of ecosystem services such as drinking water food or timber while these objectives are inextricably linked they are often studied and managed separately from each other liu et al 2015 sustainable resource use and management strategies that enhance landscape multifunctionality require not only understanding the manifold interactions among multiple demands but also finding solutions to minimize their trade offs while progress has been made in the model based quantification of land use effects on ecosystem services and biodiversity over the past few years the question of w here to put things polasky et al 2008 is still an important challenge land allocation problems can be solved by model based approaches combined with either scenario analysis e g fontana et al 2013 backcasting approaches e g brunner et al 2016 or multi objective optimization techniques optimization methods are capable to explore a large number of land use land management configurations memmah et al 2015 seppelt et al 2013 combined with simulation models e g hydrological biodiversity or socio economic models they are applicable for a wide variety of problems existing studies on land use optimization were reviewed for example by kaim et al 2018 kanter et al 2016 and memmah et al 2015 optimization applications differ in terms of scale from single fields farms to large river basins the objectives considered e g agricultural productivity water quality biodiversity recreational value and the timing of preference articulation of stakeholders and decision makers before during or after the optimization process such multi objective land use allocation problems can be solved either by integrating all objectives into one single function following a weighted sum approach marler and arora 2010 or by pareto based methods where all objectives are treated individually using the concept of pareto optimality deb 2014 the latter approach provides a set of pareto optimal solutions i e solutions for which no objective can be further improved without compromising at least one of the other objectives as those solutions are usually only approximations of true pareto optimality we refer to them as non dominated solutions pareto based optimization methods have been used in several land use optimization studies e g in bennett et al 2004 cao et al 2011 chikumbo et al 2015 duh and brown 2007 groot et al 2012 huang et al 2013 lautenbach et al 2013 roberts et al 2011 and schwaab et al 2017 2018a the advantage of non dominated solutions is that they directly illustrate the best possible trade offs among conflicting objectives from such a set of best alternatives decision makers can discuss and select appropriate solutions according to their preferences cord et al 2017 to obtain non dominated solutions population based meta heuristics i e methods based on evolutionary computation or swarm intelligence are particularly suitable because they generate and evaluate a set of potential solutions in parallel and approach the pareto front within a single run of the algorithm boussaïd et al 2013 coello et al 2007 identifying non dominated solutions also allows evaluating certain projections on future land use change regarding their overall optimality seppelt et al 2013 verstegen et al 2017 such non dominated solutions however have little value for decision making if they ignore real world constraints for example depending on rules and regulations in the study region grassland may be transformed into forest but not into cropland or urban areas in the same case study context no specific transition rules may exist for other land use classes e g cropland alternatively the goal might be to efficiently allocate a specified demand for urban development in a district or municipality schwaab et al 2017 this problem is different from the problem of constraining based on objective values such as setting threshold values for one or several objectives in contrast to constraining the objective values e g a minimum level of species richness or a maximum value for nitrate concentration in the groundwater constraints are put on the control variables here for land use allocation problems these constraints on the control variables could involve the minimum and maximum allowed total area of certain land use classes or transition rules controlling which classes are allowed to be replaced by others and which not since evolutionary algorithms are by nature unconstrained it is necessary to find proper ways of incorporating real world constraints coello coello 2002 there are three constraint handling methods commonly applied in land use optimization studies 1 penalty functions that degrade the fitness value of an infeasible solution e g chikumbo et al 2015 shaygan et al 2014 stewart et al 2004 2 feasibility operators which create feasible only child solutions e g garcía et al 2017 karakostas and economou 2014 and 3 repair mechanisms for infeasible individuals e g cao et al 2012 datta et al 2007 schwaab et al 2018b the latter two approaches require modifications of the genetic algorithm which are often specific for the optimization problem at hand schwaab et al 2018b their adaptation to different objective functions different land use types including their transition or total area rules may require extensive effort and programming skills if the source code is available at all to foster the use of population based optimization in landscape management we here present a generic tool for constrained multi objective optimization of land use allocation comola including constraint handling for the control variables with large flexibility and broad applicability comola employs the non dominated sorting genetic algorithm ii nsga ii developed by deb et al 2002 among multi objective evolutionary algorithms nsga ii is the most often used procedure for solving spatial allocation problems malczewski and rinner 2015 we enhanced the algorithm by implementing two types of constraint handling methods 1 a novel repair algorithm specifically developed for land use transition and composition constraints and 2 constrained tournament selection a commonly used penalty function approach deb 2000 osyczka and krenich 2000 the objective of this research paper is to introduce the functionality of comola and compare the performance of comola s repair algorithm with the established method of constraint tournament selection to test all important facets of the tool we used a virtual land use map which was designed to optimize multiple conflicting objectives maximizing crop and water yield as well as two biodiversity indicators while considering basic land conversion constraints total area and transition rules our optimization experiments include different levels of complexity with respect to the number of objectives the strength of the pre defined constraints and the number of spatial units to be optimized 2 description of comola comola is a free python tool to optimize raster maps for multiple objectives it is based on the open source inspyred python library garrett 2012 which is extended to include functions for reading encoding and writing raster maps as well as algorithms to consider constraints during the optimization procedure it is platform independent and allows for the integration of any model which reads gridded maps as input data for instance for estimating ecosystem service indicators comola can be used immediately by inputting a raster map representing the baseline situation e g for land use ready to run models written in r r core team 2016 or python and optional information on constraints as constraints for the control variables the tool is able to consider both 1 transition rules defining the possible land use transformations and 2 minimum and maximum area proportions for each land use class within the study area all relevant settings such as paths to input data and models as well as optimization specific parameters e g population size crossover and mutation rates and settings related to constraint handling and raster map analysis are managed in a single control file as the framework allows users to integrate any kind of simulation models e g process based statistical or both at the same time it can be a useful tool for a broad range of spatial allocation studies for more details and examples how to adapt and apply comola the reader is referred to the user guide on github https github com michstrauch comola the optimization procedure is summarized in fig 1 a and can be briefly described as follows assuming a land use allocation problem 1 preprocessing encoding the original raster map to decrease the number of spatial units and hence the computational effort the algorithm first transforms the input raster map representing the current status of land use into a patch or cluster map fig 2 where neighboring raster cells of the same type are aggregated cf holzkämper and seppelt 2007 liu et al 2016 the patch id map is then encoded as a string of integers each value is called a gene representing the land use of a patch that forms the genome of the initial individual i e the first individual of the initial population however users may also specify their own patch id map to control the aggregation of cells into patches the patch id map hence allows complete flexibility to delineate spatial units if desired the user can also conduct a cell level optimization the patch id map can be thought of as a polygon map that defines the units that the optimizer modifies with respect to land use the approach combines the possibility to use irregularly shaped patches as modeling units while allowing the use of raster processing functions which are more efficient for neighborhood analysis problems 2 constraint controlled genome generation cg to fill up the initial population in the second step all further individuals of the initial population are generated the original land use map is always included in the initial population the user defined size of the population depends on the nature of the problem but should usually not be higher than a few hundred individuals malczewski and rinner 2015 without considering constraints the genome generation would be completely at random and based on a uniform sampling among all available land use classes if constraints are defined the initialization uses a constraint controlled genome generation cg algorithm to ensure all individuals of the first population are feasible and fulfill all given constraints it is known that the search process of an optimizer becomes more efficient if its initial guess is feasible or near feasible datta et al 2012 cg operates on a gene by gene i e if not defined otherwise a patch wise basis to generate a genome for each gene the random choice of a land use class is constrained according to the pre defined transition rules fig 3 and a tabu memory which is a list of already identified infeasible gene sequences and visited feasible solutions each time a land use class is assigned to a gene the algorithm examines the genome sequence generated up to that point for any violation of the total area constraint if pre defined for this land use class if there is no violation the genome generation proceeds with the next gene i e the next patch id if the total area constraint is violated the gene sequence is added to the tabu memory and the algorithm returns to a suitable previous gene the search is then repeated for a feasible land use class according to the transition rules and the updated tabu memory in addition the user can choose to include boundary solutions in the initial population which are solutions at the boundary of the feasible solution space in cases where total area constraints are defined two boundary solutions are automatically generated within comola for each land use class one solution with the minimum permissible and another with the maximum permissible coverage of that respective land use class boundary solutions may have a good fitness for at least one objective and good initial estimates also referred to as seeds may generate better solutions with faster convergence friedrich and wagner 2015 3 run model s quantify objectives fitness values for each individual i e each land use map of the population are estimated through user defined models since the search ability of pareto dominance based evolutionary algorithms such as nsga ii deteriorates with increasing number of objectives ishibuchi et al 2008 we recommend using a maximum of four objectives however due to the modular architecture of comola it is possible to implement other optimization algorithms such as nsga iii deb and jain 2014 to handle more than four objectives for quantifying the single objectives e g agricultural productivity water quality biodiversity etc any type of model can be integrated as long as the model is able to handle a raster map as land use input and produces only one aggregated value per objective that is representative for the whole map 4 non dominated sorting tournament selection crossover mutation based on the fitness values of each member of the current population nsga ii applies a non dominated sorting scheme archives the best solutions and uses binary tournament selection mating crossover and mutation operators to create an offspring population as described in deb et al 2002 5 constraint controlled repair mutation cm even if the parent population exclusively consisted of feasible individuals the offspring population likely contains infeasible solutions due to crossover and mutation yoon and kim 2014 infeasible individuals of the offspring population are transformed into feasible individuals using a constraint controlled repair mutation cm algorithm which operates patch wise exactly as constraint controlled genome generation step 2 but with the premise that only a minimum number of genes should be modified in order to make an infeasible genome feasible hence the repaired feasible individuals remain as similar as possible to the original infeasible solutions suggested by nsga ii cm is thus a further operator besides crossover and mutation that repairs infeasible offspring the implemented tabu memory ensures both an efficient repair of infeasible individuals and uniqueness among all feasible individuals generated avoiding redundant model evaluations and fostering diversity in the solution space steps 3 to 5 are repeated until a termination criterion is reached e g the user defined maximum number of generations or any other pre defined terminator of the inspyred package the combination of cg and cm cg cm is a novel and straight forward constraint handling method specifically designed for comola to handle transition and area proportion constraints in land use optimization studies moreover comola can employ constraint tournament selection cts as an alternative method fig 1b cts has been suggested by osyczka and krenich 2000 and deb 2000 and represents an enhanced version of the binary tournament selection where two solutions are picked from the parent population and the better solution is chosen for mating without using repair mutation operators each solution can be either feasible or infeasible feasible solutions are preferred over infeasible solutions if both solutions are feasible cts prefers the one with better fitness in case both solutions competing in a tournament are infeasible cts prefers the solution with smaller constraint violation deb et al 2002 such a modification of the binary tournament selection is also proposed for the recent nsga iii algorithm to solve constrained many objective problems jain and deb 2014 in comola constraint violation δ o is composed of two terms one refers to the land use area violation δ a and the other to the violation of transition rules δ t 1 δ o δ a δ t with 2 δ a k 1 k l k c c 0 0 c 0 and 3 δ t j 1 j φ j φ j p j t r a n s i t i o n r u l e i s v i o l a t e d f o r p a t c h j 0 e l s e k is the total number of land use classes with k 1 k while c is the number of land use classes violating the area constraint l denotes the area deviation of land use k from its permissible min max range in of the total study area i e l 0 if the area constraint for land use k is violated and l 0 if not j is the total number of land use patches with j 1 j and p j denotes the area of patch j where transition rules are violated in of the total study area 3 optimization experiment 3 1 input land use data and objective functions for the performance test a virtual landscape of 10 10 raster cells was designed fig 4 a for which a discrete encoding strategy was applied defining eight land use classes 1 5 cropland ranging from lowest 1 to highest production intensity 5 6 pasture 7 forest 8 urban area we assumed a cell size within the range of 100 to 10 000 m2 as is typical for landscape scale assessments we considered up to four contrasting objectives including agricultural production water yield and biodiversity while taking into account land use intensity landscape composition and landscape configuration as suggested by seppelt et al 2016 all four objectives are to be maximized for the ease of evaluation and interpretation each objective was quantified by simple yet plausible conceptual models as summarized in table 1 3 2 experimental design we compared our novel repair algorithm cg cm with cts a standard approach to handle constraints and a combination of both approaches where cts is based on a feasible start population cg cts the comparison considered different levels of complexity as briefly explained in the following and illustrated in fig 5 3 2 1 individual size number of spatial units scalability is a key issue for optimization algorithms especially for solving spatial allocation problems for which the number of decision variables is determined by the size and heterogeneity of the study area the spatial resolution and the precise nature of the optimization problem e g the number of optional land use classes there is empirical evidence that efficacy and efficiency of multi objective metaheuristics decrease for large numbers of decision variables antonio and coello coello 2016 durillo et al 2010 constraint handling based on repair mechanisms such as gc gm comes at additional computational cost which might increase excessively with individual size i e in our case with the number of spatial units considered our experiment therefore included performance tests with a varying number of spatial units ranging from 41 to 400 the lowest complexity n 41 was represented by clustering neighboring cells of the same land use type land use map fig 4a into patches the number of spatial units was increased to 100 in a cell level optimization using the same map we further considered individuals of size 400 by increasing the spatial resolution by factor two in a cell level optimization in each case the initial land use distribution fig 4a was retained 3 2 2 number of objectives the performance of pareto based algorithms also generally decreases with increasing number of objectives due to the inefficiency of the pareto relation in high dimensional spaces n 3 knowles and corne 2007 li et al 2015 lópez jaimes and coello coello 2015 we tested the applicability of comola for maximizing two crop yield and habitat heterogeneity three crop yield habitat heterogeneity and forest species richness and four objectives crop yield habitat heterogeneity forest species richness and water yield 3 2 3 strength of constraints one can assume that the effect of constraint handling methods on optimization performance depends on the strength of the pre defined constraints we therefore tested two different constraint scenarios the first scenario cons i took only a set of transition rules into consideration where 1 existing cropland could be converted either into a different type intensity of cropland or into pasture or forest 2 existing pasture could be converted into forest but not the other way around meaning that 3 existing forest had to remain at the same location while 4 urban areas were not allowed to change at all the second scenario cons ii used the same transition rules but also took into account the permissible total area for different land use classes ranging from 10 to 30 for forest from 10 to 20 for pasture and from 10 to 25 for cropland 1 and 5 respectively in addition we applied comola without constraints using the original nsga ii algorithm without any modification the unconstrained optimization runs served as a reference for the maximum solution space regardless of their realism 3 3 general settings and performance evaluation for each optimization run we used the same population size 300 number of generations 300 crossover rate 0 9 mutation rate 1 individual size and included boundary solutions seeds within the initial population in addition to the number of feasible non dominated solutions we used the hypervolume metric as proposed by zitzler and thiele 1999 to evaluate each of the different solution sets this widely accepted multi objective performance metric measures both convergence and diversity on a single scale without requiring the knowledge of the true pareto front for comparison jiang et al 2014 hypervolume represents the volume in the objective space that is dominated by the set of feasible solutions given a certain reference point such as the origin of coordinates higher values of the hypervolume hence indicate that the solutions are closer to the true pareto front and at the same time that they are more evenly scattered in the objective space jiang et al 2014 we normalized the fitness values for each objective by dividing by their unconstrained theoretical maximum value section 3 1 which resulted in values for each objective between 0 and 1 this ensured no objective was overrepresented in the calculation of hypervolumes moreover using the unconstrained theoretical maximum as a fixed normalization factor allowed comparability among different constraint scenarios and constraint handling methods the r package mco mersmann 2014 was applied where the origin of coordinates was defined as reference point we used wall time or real time includes the cpu and the kernel time as a measure of computation time since we are dealing with stochastic algorithms we have executed ten independent runs for each problem instance which led to a total of 350 optimization runs each optimization run was performed on a linux cluster using 20 cores in parallel routines to run the models in parallel are included in the tool 4 results discussion 4 1 plausibility and visualization of non dominated solutions as expected the front of non dominated solutions for the unconstrained optimization problem completely dominated the identified front for a constrained problem see two objective optimization example in fig 6 if land use composition and land use transitions are unconstrained the optimizer is able to explore a larger search space however not a single solution of the unconstrained optimization run satisfied our hypothetical constraints on land use transition and total area for the individual land use classes pointing out the obvious need for constraint handling in our example application using repair mutation in contrast ensured feasibility of any single individual generated during the optimization the shape of the fronts implies an almost linear negative relationship trade off between the two objectives which is plausible since increasing habitat heterogeneity impairs crop production as defined in our models the shape of the true pareto fronts might be more convex due to the impact of soil fertility on crop yield however from analyzing the example maps also shown in fig 6 it is fair to assume that the optimization runs identified at least near optimal solutions this assumption seems plausible especially because highest intensity cropland has been pre dominantly allocated in the right part of the maps where soil fertility is highest cf fig 4b leaving the mosaic of extensive land use classes responsible for highest habitat heterogeneity in the left part of the maps results for the constrained optimization runs considering three and four objectives were plausible in the same sense see figs 7 and 8 forest species richness as a third objective showed a clear trade off with crop yield but not with habitat heterogeneity because of the pre defined constraints allowing forest not to exceed a total area of 30 there was no single maximum solution for forest species richness but a whole set of solutions ranging over large parts of the objective space including the maximum of habitat heterogeneity this is why the marked maximum solutions for forest species richness coincided with the maximum solutions for habitat heterogeneity in figs 7 and 8 water yield as fourth objective fig 8 favored low intensity cropland and thus showed a clear trade off with forest species richness similar to the crop yield forest species richness relationship however the relationship between water yield and both crop yield and habitat heterogeneity at the pareto frontier was not that clear moderate crop yield and habitat heterogeneity occurred at maximum water yield caused by the highest share of low intensity cropland for this solution what becomes obvious from fig 8 is that visualizing a four dimensional solution space in just one graph is extremely challenging in particular for a high number of solutions it is known that the number of non dominated solutions increases enormously with the number of objectives deb 2001 causing incomparability of solutions and inefficiency of pareto based sorting schemes e g li et al 2015 this is why nsga ii is usually not used for more than three or four objectives the number of solutions could have been reduced by applying an ε dominance analysis laumanns et al 2002 in such a case colored 3d scatterplots or even 2d scatterplots with color and point size as third and fourth dimension respectively seem to be adequate cf lautenbach et al 2013 our suggestion of projecting a colored 3d pareto cloud onto three orthogonal planes of a cube while leaving out the actual 3d plot as shown in fig 8 might be a possibility to visualize a high number of 4d solutions however finding proper techniques for visualizing high dimensional solutions is a research field on its own cf chiu et al 2009 he and yen 2016 4 2 optimization performance the performance of comola was determined by 1 the selection of the constraint handling method 2 the definition of the optimization problem in terms of constraints and 3 the size of an individual i e the number of spatial units involved in general we found that repair mutation cg cm clearly outperformed both methods based on constraint tournament selection cg cts and cts this was indicated by both a larger number of feasible individuals in the final set of non dominated solutions and larger hypervolumes dominated by the feasible solutions cf fig 9 and additionally fig a2 in the supplementary for considering two and four objectives our results thus imply that for discrete optimization problems repair mechanisms might be the preferable option to handle hard constraints as also found by zydallis and lamont 2001 for non spatial problems the differences in optimization performance among constraint handling methods increased with the strength of constraints and the size of an individual while a considerable amount of feasible solutions was found with all methods for the less restrictive constraint scenario cons i transition constraints only the number of feasible solutions was substantially smaller for the more restrictive scenario considering both transition and total area constraints cons ii for cons ii in combination with large individual sizes 100 and 400 not even a single feasible individual was included in the final set of non dominated solutions when using constraint tournament selection which is why hypervolumes were assigned a value of zero in this case fig 9 in contrast our proposed repair algorithm ensured feasibility and uniqueness of all individuals n 90 300 generated during an optimization run the improved performance of repair mutation however came at a cost as coello et al 2007 stated repairing infeasible solutions in order to make them feasible may be computationally expensive we found that the runtime for cg cm increased substantially with the strength of constraints fig 9 the smaller the feasible solution space the more effort is required to generate adequate individuals compared to the other methods that we tested cg cm required three times more computation time in the more restrictive constraint scenario with the smallest size of individuals while the difference was not as pronounced for the less strict constraint scenario further the difference in runtime increased enormously with larger size of individuals in our case up to factor 20 when handling individuals of size 400 compared to 41 in contrast runtime with cts based constraint handling increased only marginally fig 9 however cts and cg cts might be of little use for larger individuals and strong constraints as they may result in only infeasible solutions as discussed above the achieved level of hypervolumes and thus the optimization performance decreased substantially with individual size c f fig 10 this is also indicated by the different slopes of the hypervolume curves in case of repair mutation hypervolume curves converging laterally towards a certain threshold as it is the case for the smallest individual size might suggest but not guarantee a good approximation of the true pareto front curves progressing with a distinct slope until the end of the optimization process after 300 generations as shown for larger individual sizes clearly indicate further potential of progress for constrained tournament selection the hypervolumes of the entire solution space i e all solutions with no guarantee of feasibility were considerably larger than the hypervolumes achieved with repair mutation however in the cts based optimization with larger individual sizes 100 and 400 infeasible individuals completely displaced the feasible ones after only a few generations this is why here the hypervolume progress of the feasible solution space as reconstructed a posteriori from the optimization archive comes to an abrupt end see middle and right panel of fig 10 although constrained tournament selection generally favors feasible individuals over infeasible ones the displacement of feasible individuals might be caused by crossover and mutation which presumably tended to violate the given constraints the types of constraints considered here especially the minimum and maximum allowable total area of a specific land use class hampered the generation of feasible offspring even if feasible parents were selected for mating offspring individuals in spatial optimization are always a mosaic of subsets of parent maps and even though the parent maps are feasible according to the total area constraints that does not have to be true for their subsets and even less so for the product of recombination constraint handling methods focusing only on the selection process for mating such as cts may therefore not be adequate for spatial optimization problems that include hard total area constraints the decrease in the number of feasible solutions with increasing number of generations for cg cts and cts was stronger for optimization problems with larger individuals because the likelihood of a constraint violating land use change increases with the number of spatial units the difference in terms of feasible individuals per population for cg cts and cts was marginal between individual size 100 and 400 in both cases the number of feasible individuals in a population dropped to zero after around 50 and 20 generations for cg cts and cts respectively fig 10 and for a closer look on the first 75 generations fig a4 in the supplementary in case of cg cts all 300 individuals of the initial population generation 0 were forced to feasibility due to the constraint controlled genome generation cg algorithm in cts the optimization started with only 14 feasible individuals representing the boundary seeds which were included in each experiment the higher number of feasible individuals in the initial population explains why hypervolumes were larger for cg cts compared to cts the benefit of accounting for feasibility in the initial population was also previously reported by other studies e g datta et al 2012 haubelt et al 2005 for the smallest individual size feasible individuals only became nearly extinct for cg cts and cts from around generation 75 the number of feasible individuals plateaued at a low level irrespective from their initial number 300 in cg cts vs 14 in cts this explains why differences in hypervolume evolution between cg cm and the constraint tournament selection approaches were less pronounced for the smallest individual size the loss of performance with increasing size of individuals was observed for all settings independent from whether constraints were considered or which constraint handling method was used cf figs 9 and 10 hypervolumes for cg cm decreased on average by 10 and 38 when increasing the individual size from 41 to 100 and 400 respectively the average performance loss was similar for the unconstrained optimization runs with 10 and 46 both increases in runtime and decreases in performance with increasing individual size were caused by the exponential growth of the number of possible combinations n that equaled k j in optimization runs without constraints where k is the number of possible land use options and j is the number of spatial units patches or grid cells this amounted to n 8 41 8 100 8 400 possible options for each of the different individual sizes tested respectively if constraints are defined n is smaller but cannot be predicted because the number of possible land use options varies across the spatial units and can be even variable for a single spatial unit when considering total area constraints despite the performance loss for larger individual sizes comola with its implemented repair algorithm was able to detect a multitude of feasible solutions that significantly outperformed the status quo land use simultaneously for all objectives and such a set of solutions might still have utility in real world case studies cf fig a5 in the supplementary this was also shown by verhagen et al 2018 who applied comola for 277 pasture farms and 141 fruit orchards which equals a total individual size of 418 in the kromme rijn area 219 km2 in the netherlands to optimally allocate different agri environment measures under consideration of both land use transition and total area constraints the authors aimed to simultaneously maximize the yearly fruit yield profit the potential habitat of the great crested newt triturus cristatus and the aesthetic value of the landscape while minimizing the loss in pasture production the study also showed the potential of comola as a model integration and optimization framework to provide insights into the functional trade offs associated with different management options and their respective potential to increase landscape multifunctionality by using comola verhagen et al 2018 further showed how landscape optimization approaches can be integrated into spatial planning and inform policy design and implementation 5 conclusions this paper introduces comola as a tool for constrained multi objective optimization of land use allocation and analyzed its functionality and performance using virtual data and simplified models under consideration of different levels of complexity we found that comola is able to identify near optimal solutions for up to four objectives and the smallest individual size tested n 41 if solutions close to the true pareto front are sought the individual size in an unconstrained optimization should therefore not exceed a value of 120 40 for two eight possible land use options as this amounts to a potential decision space of 2120 similar to our smallest individual size experiment since constraints reduce the potential decision space these limits might be higher for constrained optimization runs this analysis found that further increasing the individual size can lead to a substantial loss of optimization performance by using our approach of a constraint controlled genome generation and repair mutation comola was still able to identify multiple solutions that were significantly better than the status quo land use distribution in all objectives simultaneously thus if it is sufficient to improve the status quo of a real world case study without approaching true pareto optimality individuals with a size of several hundred might still be viable for decently sized computational resources for an application at larger scales it might become necessary to aggregate spatial units apart from that time consuming simulation models can increase computational time dramatically if they have to be called repeatedly during the optimization the use of meta models also called surrogate model is therefore encouraged where feasible maier et al 2014 repair mutation was found to be advantageous over constraint tournament selection at least for spatial optimization problems with transition and total area constraints repairing infeasible individuals is however computationally intensive and can thus require longer runtimes depending on the strength of the pre defined constraints each of our constraint scenarios included hard constraints which we wanted to be satisfied by all means in case a land use optimization problem includes only soft constraints such as for example total area rules which should be matched only approximately penalizing constraint violation during tournament selection could be a better option preferably with a feasible initial population comola is able to handle basic constraints on land use transition and the total area for each land use class future developments should focus on the implementation of more sophisticated constraints such as spatial relationships between land use types or compactness of land use patches e g eikelboom et al 2015 to increase the efficiency of comola it might be promising to implement and test further constraint handling methods such as crossover operators explicitly accounting for feasibility as proposed by garcía et al 2017 and karakostas and economou 2014 or the repair mutation methods suggested by schwaab et al 2018b it might also be worth utilizing more advanced many objective evolutionary algorithms deb and jain 2014 li et al 2015 rui et al 2013 to overcome problems related to inefficient pareto based ranking and thus to include more than four objectives in future applications comola provides a generic framework for combining knowledge and models from various disciplines so that the best possible trade offs between different landscape related objectives can be explored stakeholders can participate in such a framework by defining optimization objectives and constraints as well as by critically evaluating the set of pareto optimal solutions to derive recommendations for desired land use and conservation strategies it therefore has a high relevance and application potential for studies focused on spatial planning integrated land and water management and any other field of research that aims to optimize spatial patterns for multiple objectives acknowledgements this research was funded through the 2013 2014 biodiversa facce jpi joint call with the national funder bmbf german federal ministry of education and research project tale towards multifunctional agricultural landscapes in europe assessing and governing synergies between food production biodiversity and ecosystem services grant 01 lc 1404 a author contributions m s a c r s and m v designed the research c p m s s l c s and a k contributed with programming and software documentation m s performed the research all authors contributed in writing the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 003 
26207,increased storm water runoff and flooding and poor ecosystem health have brought increasing attention to catchment wide implementation of green infrastructure e g bioswales rain gardens permeable pavements tree box filters urban wetlands and forests stream buffers and green roofs to replace or supplement conventional storm water management practices and create more sustainable urban water systems current green infrastructure gi practice aims at mitigating the negative effects of urbanization by restoring pre development hydrology and ultimately addressing water quality issues at an urban catchment scale however the benefits of gi extend well beyond local storm water management as urban green spaces are also major contributors to human health considerable research in the psychological sciences have shown significant human health benefits from appropriately designed green spaces yet impacts on human wellbeing have not yet been formally considered in gi design frameworks this work develops a novel computational green infrastructure gi design framework that integrates storm water management requirements with criteria for human wellbeing a supervised machine learning model is created to identify specific patterns in urban green spaces that promote human wellbeing the model is linked to rhessys hydrological model to evaluate gi designs in terms of both water resource and human health benefits an application of the framework to tree based gi design in dead run watershed baltimore md shows that image mining methods are able to capture key elements of human preferences that could improve gi design the results also show that hydrologic benefits associated with tree based features are substantial indicating that increased urban tree coverage and a more integrated gi design approach can significantly increase both human and hydrologic benefits keywords green stormwater infrastructure hydrologic modeling computer vision human preference machine learning 1 introduction the rapid growth of urbanization interferes with natural water and nutrient cycling by increasing impervious surfaces which in turn increases flashiness of urban drainage systems and reduces water quality causing human health and ecosystem problems downstream nrc 2008 wendel et al 2011 this problem has increased interest in using green infrastructure in urban areas e g bioswales rain gardens permeable pavements tree box filters cisterns urban wetlands and green roofs leroy poff 1997 suggested that implementation of these practices at the watershed scale could restore the riverine ecosystem along with addressing water quality issues roy et al 2008 considered watershed wide implementation of these approaches as a prerequisite for sustainable urban water systems currently green infrastructure design guidelines provide site specific patch design criteria with only qualitative discussion of catchment scale impacts of gi installations e g caltrans 2010 city of portland 2008 harper and baker 2008 ncdwq 2007 catchment scale lumped parameter stormwater models e g marc 2008 tsihrintzis and hamid 1998 and tools such as hspf swmm and hec hms do not represent site specific hydrology or gi processes in these catchment scale models both traditional grey and green infrastructure have typically been modeled as edge of field or in line filters and sinks for storm water runoff received from source catchment areas attenuation of storm water volumes and pollutants are often included as fixed reduction percentages or first order decay reactions based on limited input and output water quality measurements e g lee and riverson 2012 in the sustain modeling framework and wong et al 2002 in the music framework research over the past decade as part of the baltimore ecosystem study suggests that significant carbon sequestration and nitrogen retention can occur in a range of urban ecosystem features including lawns gardens and stormwater detention structures but that these processes are sensitive to specific characteristics of the integrated drainage system including contributing areas flow regimes soils and structure design e g raciti et al 2011 bettez and groffman 2012 living components of green infrastructure will grow and adjust to prevailing water climate and nutrient conditions and there may be a long transient development of ecosystem cycling and retention capacity following development design of sustainable green infrastructure as either edge of field or at source treatment should incorporate transient development as the ecosystem develops in response to local climate soil and drainage position e g location within a flow field it is critical that gi modeling extend to encompass the full catchment as a continuum beyond the discrete gi sites including runoff source areas in addition to edge of field or in line treatment systems most of the current research on green infrastructure designs considers only hydrologic benefits macro et al 2018 massoudieh et al 2017 glenis et al 2018 however the benefits of green infrastructure extend well beyond local storm water control as urban green spaces e g lakes parks and community gardens are also major contributors to human health forty years of research has established the powerful and consistent effects of the presence of particular natural elements in increasing human preferences for urban landscapes kaplan and kaplan 1989 these high preference elements in turn are associated with faster recovery from stressful experiences reduced physiological symptoms of stress thompson et al 2012 chang and chen 2005 and increased life expectancy after controlling for a host of features associated with mortality mitchell and popham 2008 takano et al 2002 furthermore human preferences are also important to address major barriers to gi implementation that were identified in our work with advisors from five u s cities as part of a national gi working group at the national socio environmental synthesis center sesync community attitudes and perceptions about gi installations can lead to resistance and even active vandalism e g throwing trash into a bioswale or mowing weedy plants of gi maintenance of distributed gi is also a major challenge for cities if gi can be designed to better meet human perceptions then community acceptance and adoption of gi maintenance would likely improve researchers in both the field of psychology and computer science have been working together for more than two decades to solve the problem of capturing human perception by means of human computer interactions significant efforts have been made to build machine learning models that replicate and predict human perception to improve social acceptability of designs strategies policies and marketing approaches among others machine perception cannot perform effectively without a wealth of experimental data about human perception in this work we explore how physiological preferences for gi can be captured with machine learning despite the extensive research showing the benefits of green spaces on human health gi design has not yet considered criteria for human wellbeing in any formal design frameworks in this research we propose a new gi design framework that considers both human and hydrologic benefits at patch and catchment scales to aid in rapid initial evaluation of potential gi designs we have developed a human preference model that predicts increasing human benefits using supervised machine learning algorithms and computer vision techniques the training data for this work has been collected using social surveys conducted at galesburg il and the model is validated using data from amazon mechanical turk the resulting algorithm is then coupled with a hydrologic model called rhessys to estimate human benefits for a gi design case study in baltimore md to our knowledge this is the first study that quantifies the significance of urban green infrastructure design for both human wellbeing and hydrologic benefits in the subsequent sections we introduce the methodologies for quantifying human preference with an image based machine learning approach and assessing hydrologic benefits followed by a case study and results that explore the implications of this approach in baltimore md the final section provides conclusions and suggestions for future research on this topic 2 methodology this section presents the fundamental concepts and technologies used to create a gi design framework that integrates human preferences for green spaces which are correlated with improved human health sullivan et al 2004 and hydrologic benefits an overview of the green infrastructure design framework is given in fig 1 in order to predict gi human benefits data acquisition section 2 1 and an image based machine learning approach section 2 2 are used to visually identify landscape features in design images that humans prefer and are therefore linked with improved human health and provide a quantitative ranking of the design to link these human preference criteria with storm water requirements rhessys tague and band 2004 is used to predict hydrologic benefits of the design s landscape features as described in section 2 3 table 2 2 1 data acquisition previous research has shown that preferences of stakeholders can be predicted using digital images that visually represent potential gi designs sullivan 2004 for example fig 1 shows gi design images from galesburg illinois and their human preference rankings both images in fig 2 are from the same setting with the addition of trees to both sides of the street in b resulting in higher preference ratings similar images of streetscapes with varying tree cover and density from galeburg il were shown to 30 participants in a survey conducted by co author sullivan at the university of illinois yuan 2010 the participants were asked how much they like dislike each of 360 images which were presented in random order below is a sample question from the survey fig 3 is illustrative of the inputs to the human preference model which are the raw rgb images from sullivan s study with labels spanning five categories 2 2 human preference modeling approach an image based supervised machine learning technique is trained to predict human preferences or relative human health benefits of tree based gi as shown in fig 4 the approach automates the prediction of human preferences from gi design images by identifying extracting specific landscape features that correlate with high human preferences kaplan and kaplan 1989 using computer vision techniques e g fig 2 a supervised machine learning model is then trained to predict a human preference rating for the image based on the extracted features details on this approach are provided in the following sub sections 2 2 1 feature extraction to identify which image features to include in the model table 1 maps available image segmentation algorithms to a human preference matrix developed by kaplan and kaplan 1989 for urban green spaces which gives gi characteristics that are most linked to human wellbeing the column on the left gives landscape features that attract people and engage them longer by promoting understanding while the column on the right are features that encourage exploration of the landscape these features and algorithms are defined in more detail below 2 2 2 color histogram color histogram quantifies colors distribution of pixel intensities in an image anami et al 2010 since the color green plays an important role in gi design we implemented rgb 256 bin color histogram chapelle et al 1999 in this approach color is represented by a three dimensional vector for each color channel i e rgb and it corresponding to a pixel point in color space cha chapelle et al 1999 showed that for image classification the use of color space i e rgb red green blue vs hsv hue saturation value has very minimal effect on classification accuracy therefore we used rgb color space in this work fig 5 shows the color histogram for the sample survey image shown in fig 3 2 2 3 histogram of oriented gradient the histogram of oriented gradient hog is used to extract the distribution of edges of objects in an image dalal and triggs 2005 hog finds distinctive shapes of objects present in a design image e g the shapes of trees shown in fig 3 which are needed to determine the legibility feature in table 1 that affects human understanding of the scene we implemented the hog feature based on the approach of dalal and triggs 2005 which gives a 31 dimensional descriptor hog features are computed over rectangular regions represented by a grid of 3x3 blocks with an overlap of 8 pixels each block contains 16x16 pixels and hogs are computed for each block with 9 unsigned orientations orientations are the gradient vectors at each pixel for 16x16 pixel cells that are categorized into a 9 bin histogram the histogram ranges from 0 to 180 because the orientations are unsigned so there are 20 per bin 2 2 4 gist descriptors the previous two image features identify specific objects from the images oliva and torralba 2001 proposed a technique to estimate the structure or shape of a scene using spatial properties of the scene called gist descriptors e g degree of naturalness openness roughness or expansion fig 6 shows the gist descriptor for a sample input green space image the results show the local energy spectrum which measures the quantity of radiation passing through or emitted from the surface of an object for all objects in the image the image is divided into a 4x4 cell matrix for spatially grouping objects each sub image of the matrix corresponds to the amount of radiation with multiple wavelengths emitted or passed through objects placed in the corresponding spatial location dark regions in the gist descriptor matrix e g the upper right and lower left sub images in fig 10 thus identify openness in the image objects such as the tree in the middle of the input image on the left in fig 7 emit higher radiation and thus have more energy spectrum captured by the gist descriptors in the corresponding sub image this spatial distribution of local energy spectrum provides a more holistic representation of a scene once the gi design image features are extracted using the methods described above a supervised machine learning approach is used as a regression model to predict human preferences for each design since we average ratings from all survey participants the preference scores range continuously between 1 and 5 i e real numbers we concatenate the above three features into a combined color hist hog gist feature set and use these features for training the model for the supervised machine learning the adaptive boosting algorithm adaboost is used which is an ensemble learning algorithm that combines a weak classifier e g decision trees or k nearest neighbors knn to output a strong regressor freund and schapire 1999 adaboost gives high accuracy and prevents over fitting of parameters to the training dataset by using an iterative learning model which improves accuracy by learning from mistakes made in an earlier training step we use decision trees as the weak classifier in this work 2 2 5 model validation evaluation we split the training data which consisted of 360 images into training 70 and validation 30 sets tuning the model for optimal performance using the training set the accuracy of the model is computed using the coefficient of r2 fig 8 shows examples from the validation set the predicted ratings for each of these images were identical to the actual human ratings the results indicate that the machine learning model predicts human preferences reasonably well particularly since human ratings can have considerable uncertainty james et al 2009 the training set is predominantly tree based green infrastructure and generally human preferences increase with more trees in images as shown in fig 8 however merely adding more trees bushes plants etc are not sufficient to obtain high human preferences the image features extracted from the training set color histogram eoh spatial histogram gist descriptor and their corresponding landscape features from kaplan kaplan s preference matrix 1998 also play an important role for example recall that color histogram measures the green coloring and coherence in the design fig 7 c and d show that the rating changes from 1 to 3 just by adding trees which provides more green richness and also by arranging the trees in an orderly manner along the sidewalks e g fig 8 d is more coherent than fig 8 c this added coherence can be seen in the color histogram results for these images in fig 9 complexity legibility and mystery parameters of the preference matrix also change human preferences these parameters are captured by the gist and hog descriptors as shown in fig 10 2 2 6 evaluation using amazon mechanical turk finally to further evaluate the models with more diverse human preference scores buhrmester et al 2011 we also conducted an online survey with additional participants using amazon mechanical turk the nature of the questionnaire was the same as shown in fig 2 we posted 30 different images of gi and asked 20 turkers to rate the images after we obtained the turkers ratings we ran our human preference model to see how well it performs on the same 30 images using pearson correlation coefficient ρ we obtained ρ 0 83 for the turkers ratings and model predictions which shows that the model can identify key features of gi spaces affecting human preference from visual scenes despite the inherent variability in human preference ratings 2 3 evaluating the hydrologic benefits of gi design in order to evaluate the hydrologic benefits of candidate gi designs rhessys distributed parameter hydrologic model tague and band 2004 is used rhessys is designed to simulate integrated water carbon and nutrient cycling and transport over spatially variable terrain at small to medium scales i e from 1st to 3rd order watersheds its spatially distributed framework enables the modeling of spatiotemporal interactions between different eco hydrological processes from patch to watershed scales hwang et al 2012 fig 11 gives an overview of the steps involved in preparing the required input for gi simulation using rhessys and integrating it with the human preference model for social ranking google street view images are first extracted from the google maps application programming interface api http maps google com for the neighborhood where gi design is planned the coordinates latitude longitude are identified using google maps for the front yard of every house in the neighborhood where gi installation is under consideration these coordinates are used to generate patch identifications ids that are used as inputs for gi parameters in rhessys patch ids are unique numbers associated with particular ecosystem patches which are the smallest resolution spatial units that define areas of similar soil moisture and land cover characteristics in landscapes modified by humans patches can also be defined to contain stream channels road segments storm sewers etc human sources of water and nutrients are also defined at this level tague and band 2004 therefore adding trees to the existing patches requires reflecting the presence of trees in the existing lawn by generating patch ids from their respective coordinate values patch id numbers are generated using grass gis http grass osgeo org which queries the existing raster map layers to output labels associated with input coordinate values using the r what command these labels are patch id numbers that represent the current vegetation type present in the raster map the next step in executing rhessys is to prepare additional non spatial files called worldfiles which represent landscapes patterns such as land use tree canopy etc within rhessys worldfiles are generated with a grass interface program that references input raster maps and a text document defining initial state variables e g saturation deficit within the patch level which is a measure of the degree of saturation within the worldfile each spatial layer is associated with the following identifiers an assigned id based on the map used and state variable values for stores and fluxes at each spatial level that are initialized at the start of the simulation the grass2world g2w program is used to generate the worldfiles automatically from spatial data layers within grass gis next in order to incorporate new trees into the existing worldfile based on the newly generated patch id numbers an awk script is used awk script is an interpreted programming language typically used for data extraction in unix operating systems aho et al 1978 finally the modified worldfile is used to execute the rhessys model and forecast the effects of the gi design on annual mean stream flow and base flow ritcher et al 1996 at the outlet of the urban catchment where the neighborhood is located these statistics have previously been identified as important indicators for measuring the alteration in hydrology of an urban catchment due to changes in land use and land cover by simulating the impacts of gi designs on mean flows using rhessys and comparing flows with current and pre development values we can observe the efficiency of the gi designs in moving towards restoration of pre development urban watershed hydrology this provides an initial environmental rating for the gi designs which can be supplemented with nutrient impacts in the future 3 case study implication for gi design in dead run watershed baltimore md the gi design methodology described in section 2 is applied in the dead run watershed in baltimore md which is part of the baltimore ecosystem study bes as the available human preference training set contains predominantly tree based green infrastructure the case study application focuses on the addition of trees to a neighborhood within dead run watershed shown in fig 12 and observing how the human and hydrologic benefits change with different designs six scenarios are examined 1 existing scenario 2 adding a single tree in the neighborhood 3 adding multiple trees in open vs clustered arrangements 4 adding small vs large trees 5 adding trees on one vs both sides of the street and 6 adding single vs mixed species of trees google street view images of the entire neighborhood are extracted and used as a base case for human preferences the coordinate values latitude longitude of the front yard of every house in the region are chosen as potential locations to add trees google street view overlay api javascript is used to overlay trees in the google image for estimating human preferences using the machine learning model trained and validated in galesburg section 3 the results are given in the following sections once the human preference ratings are obtained rhessys is used to evaluate the hydrologic benefits of the added green infrastructure to the larger dead run watershed using the methodology described in section 2 2 the hydrologic data used in the simulations were collected by the baltimore ecosystem study as outlined by band et al 2012 the hydrologic impacts of the green infrastructure scenarios are compared with the existing scenario using 2007 hydrologic and model data from band et al 2012 and a pre development scenario the earliest hydrologic data available for the pre development scenario is 1960 which was before the design neighborhood was built the hydrologic results are presented in section 3 2 along with a discussion of the interactions between human preferences and hydrologic benefits 3 1 results of human preference modeling the human preference results for the six scenarios in baltimore are shown below for a sample street view image settings that include trees plants bushes etc have been shown to reduce symptoms of both mental fatigue and stress coley et al 1997 sullivan 2004 the existing scenario fig 13 a lacks such a setting and therefore the human preference model predicts a low rating simply by adding a single tree to the existing scenario fig 13 b the model predicts a higher preference rating based on shifts in the trained image features for hog and color histogram the spatial arrangement of trees in gi settings is also an important design factor that affects the level of complexity in the scene kaplan and kaplan 1989 which is captured in the model by the gist descriptor feature highly complex settings are generally less preferable and thus the clustered arrangement of trees in fig 13 d gets a much lower human preference rating than the open arrangement of trees in fig 13 c fig 14 shows how shifting energy spectrums in the gist descriptors captures this change the legibility and mystery factors of human preference require three dimensional inference allowing people to imagine themselves in the scene kaplan and kaplan 1989 the mystery parameter involves exploration and therefore the view of angle plays a very important role in human perceptions the arrangement of trees on both sides of the street in fig 13 d and f create a more three dimensional view where people can imagine more exploration and thus prefer such settings the spatial arrangement of trees on both sides of a street is well captured by the gist feature and color histogram and hog identify tree shape and color the features have captured the visual change in gi setting which encourages human beings to explore the setting further it is important to note that larger trees are more preferable than smaller trees and therefore have higher ratings compare fig 13 e vs f as well as 13 c vs g the reason for this result is the higher degree of richness and complexity in the setting which are captured by gist descriptors the degree of richness and complexity for a large versus small tree is shown in fig 15 the setting with a large tree has fewer dark regions in the gist matrix than the setting with a small tree 3 2 results of hydrologic modeling using rhessys fig 16 which shows the hydrologic impact of the gi design scenarios compares the modeled change in annual mean stream flow and base groundwater flow for the following scenarios 1 existing condition for the year 2007 2 pre development conditions in 1960 3 adding a single tree to every yard in the neighborhood and 4 adding two trees to every yard in the neighborhood because hydrologic impacts are measured at the watershed outlet the physical arrangement of the trees which are important for the human preference results in section 3 1 do not change the estimated hydrologic impacts as shown in fig 16 the tree scenarios significantly reduce the impacts of development on the urban watershed shifting both the mean stream and base flows much closer to the pre development scenario the extent of the shift is particularly impressive given that the neighborhood of the gi design is only 1 2 percent of the land area in the overall dead run watershed the dead run watershed has 36 of its land as impervious gwynn falls water quality management plan 2004 and thus adding more trees in front of every house in the neighborhood has significant beneficial impacts table 3 shows a quantitative comparison of the two different gi scenarios as well as a third scenario adding three trees to every yard from table 3 it is clear that green infrastructure can play an important role in restoring the urban watershed to predevelopment hydrology adding two trees to every front yard in the neighborhood gives better results than one tree but the incremental improvement declines as additional trees are added in fact adding three trees to every yard does not cause any additional changes to flow at the outlet of watershed moreover the results in section 1 indicate that the second and third trees in each yard do not cause significant differences in human preferences compare fig 13 b and d although their arrangements across the neighborhood are important the hydrologic benefits of adding more green components to urban watersheds helps in restoration of pre development flows us epa 2009 but our results show that the human preference model can identify specific patterns for installing these green components that are more beneficial for human wellbeing 4 conclusions this work develops a novel computational green infrastructure gi design framework that couples storm water management requirements with criteria for human wellbeing current approaches to designing green storm water features tend to emphasize rapid removal of storm water runoff to reduce impacts of downstream flows and pollutant loads the approach presented in this paper is a first step towards incorporating the benefits of human health associated with these urban green spaces into the design process in order to map landscape features that are correlated with human wellbeing to features that can be used to train a supervised machine learning model a suite of computer vision algorithms and techniques have been used the result is the first gi design model capable of predicting human preferences with validation accuracy of 81 which is quite high given the variability in human perceptions acknowledgement this material is based upon work supported by the national science foundation under grant no 1216817 and 1261582 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation 
26207,increased storm water runoff and flooding and poor ecosystem health have brought increasing attention to catchment wide implementation of green infrastructure e g bioswales rain gardens permeable pavements tree box filters urban wetlands and forests stream buffers and green roofs to replace or supplement conventional storm water management practices and create more sustainable urban water systems current green infrastructure gi practice aims at mitigating the negative effects of urbanization by restoring pre development hydrology and ultimately addressing water quality issues at an urban catchment scale however the benefits of gi extend well beyond local storm water management as urban green spaces are also major contributors to human health considerable research in the psychological sciences have shown significant human health benefits from appropriately designed green spaces yet impacts on human wellbeing have not yet been formally considered in gi design frameworks this work develops a novel computational green infrastructure gi design framework that integrates storm water management requirements with criteria for human wellbeing a supervised machine learning model is created to identify specific patterns in urban green spaces that promote human wellbeing the model is linked to rhessys hydrological model to evaluate gi designs in terms of both water resource and human health benefits an application of the framework to tree based gi design in dead run watershed baltimore md shows that image mining methods are able to capture key elements of human preferences that could improve gi design the results also show that hydrologic benefits associated with tree based features are substantial indicating that increased urban tree coverage and a more integrated gi design approach can significantly increase both human and hydrologic benefits keywords green stormwater infrastructure hydrologic modeling computer vision human preference machine learning 1 introduction the rapid growth of urbanization interferes with natural water and nutrient cycling by increasing impervious surfaces which in turn increases flashiness of urban drainage systems and reduces water quality causing human health and ecosystem problems downstream nrc 2008 wendel et al 2011 this problem has increased interest in using green infrastructure in urban areas e g bioswales rain gardens permeable pavements tree box filters cisterns urban wetlands and green roofs leroy poff 1997 suggested that implementation of these practices at the watershed scale could restore the riverine ecosystem along with addressing water quality issues roy et al 2008 considered watershed wide implementation of these approaches as a prerequisite for sustainable urban water systems currently green infrastructure design guidelines provide site specific patch design criteria with only qualitative discussion of catchment scale impacts of gi installations e g caltrans 2010 city of portland 2008 harper and baker 2008 ncdwq 2007 catchment scale lumped parameter stormwater models e g marc 2008 tsihrintzis and hamid 1998 and tools such as hspf swmm and hec hms do not represent site specific hydrology or gi processes in these catchment scale models both traditional grey and green infrastructure have typically been modeled as edge of field or in line filters and sinks for storm water runoff received from source catchment areas attenuation of storm water volumes and pollutants are often included as fixed reduction percentages or first order decay reactions based on limited input and output water quality measurements e g lee and riverson 2012 in the sustain modeling framework and wong et al 2002 in the music framework research over the past decade as part of the baltimore ecosystem study suggests that significant carbon sequestration and nitrogen retention can occur in a range of urban ecosystem features including lawns gardens and stormwater detention structures but that these processes are sensitive to specific characteristics of the integrated drainage system including contributing areas flow regimes soils and structure design e g raciti et al 2011 bettez and groffman 2012 living components of green infrastructure will grow and adjust to prevailing water climate and nutrient conditions and there may be a long transient development of ecosystem cycling and retention capacity following development design of sustainable green infrastructure as either edge of field or at source treatment should incorporate transient development as the ecosystem develops in response to local climate soil and drainage position e g location within a flow field it is critical that gi modeling extend to encompass the full catchment as a continuum beyond the discrete gi sites including runoff source areas in addition to edge of field or in line treatment systems most of the current research on green infrastructure designs considers only hydrologic benefits macro et al 2018 massoudieh et al 2017 glenis et al 2018 however the benefits of green infrastructure extend well beyond local storm water control as urban green spaces e g lakes parks and community gardens are also major contributors to human health forty years of research has established the powerful and consistent effects of the presence of particular natural elements in increasing human preferences for urban landscapes kaplan and kaplan 1989 these high preference elements in turn are associated with faster recovery from stressful experiences reduced physiological symptoms of stress thompson et al 2012 chang and chen 2005 and increased life expectancy after controlling for a host of features associated with mortality mitchell and popham 2008 takano et al 2002 furthermore human preferences are also important to address major barriers to gi implementation that were identified in our work with advisors from five u s cities as part of a national gi working group at the national socio environmental synthesis center sesync community attitudes and perceptions about gi installations can lead to resistance and even active vandalism e g throwing trash into a bioswale or mowing weedy plants of gi maintenance of distributed gi is also a major challenge for cities if gi can be designed to better meet human perceptions then community acceptance and adoption of gi maintenance would likely improve researchers in both the field of psychology and computer science have been working together for more than two decades to solve the problem of capturing human perception by means of human computer interactions significant efforts have been made to build machine learning models that replicate and predict human perception to improve social acceptability of designs strategies policies and marketing approaches among others machine perception cannot perform effectively without a wealth of experimental data about human perception in this work we explore how physiological preferences for gi can be captured with machine learning despite the extensive research showing the benefits of green spaces on human health gi design has not yet considered criteria for human wellbeing in any formal design frameworks in this research we propose a new gi design framework that considers both human and hydrologic benefits at patch and catchment scales to aid in rapid initial evaluation of potential gi designs we have developed a human preference model that predicts increasing human benefits using supervised machine learning algorithms and computer vision techniques the training data for this work has been collected using social surveys conducted at galesburg il and the model is validated using data from amazon mechanical turk the resulting algorithm is then coupled with a hydrologic model called rhessys to estimate human benefits for a gi design case study in baltimore md to our knowledge this is the first study that quantifies the significance of urban green infrastructure design for both human wellbeing and hydrologic benefits in the subsequent sections we introduce the methodologies for quantifying human preference with an image based machine learning approach and assessing hydrologic benefits followed by a case study and results that explore the implications of this approach in baltimore md the final section provides conclusions and suggestions for future research on this topic 2 methodology this section presents the fundamental concepts and technologies used to create a gi design framework that integrates human preferences for green spaces which are correlated with improved human health sullivan et al 2004 and hydrologic benefits an overview of the green infrastructure design framework is given in fig 1 in order to predict gi human benefits data acquisition section 2 1 and an image based machine learning approach section 2 2 are used to visually identify landscape features in design images that humans prefer and are therefore linked with improved human health and provide a quantitative ranking of the design to link these human preference criteria with storm water requirements rhessys tague and band 2004 is used to predict hydrologic benefits of the design s landscape features as described in section 2 3 table 2 2 1 data acquisition previous research has shown that preferences of stakeholders can be predicted using digital images that visually represent potential gi designs sullivan 2004 for example fig 1 shows gi design images from galesburg illinois and their human preference rankings both images in fig 2 are from the same setting with the addition of trees to both sides of the street in b resulting in higher preference ratings similar images of streetscapes with varying tree cover and density from galeburg il were shown to 30 participants in a survey conducted by co author sullivan at the university of illinois yuan 2010 the participants were asked how much they like dislike each of 360 images which were presented in random order below is a sample question from the survey fig 3 is illustrative of the inputs to the human preference model which are the raw rgb images from sullivan s study with labels spanning five categories 2 2 human preference modeling approach an image based supervised machine learning technique is trained to predict human preferences or relative human health benefits of tree based gi as shown in fig 4 the approach automates the prediction of human preferences from gi design images by identifying extracting specific landscape features that correlate with high human preferences kaplan and kaplan 1989 using computer vision techniques e g fig 2 a supervised machine learning model is then trained to predict a human preference rating for the image based on the extracted features details on this approach are provided in the following sub sections 2 2 1 feature extraction to identify which image features to include in the model table 1 maps available image segmentation algorithms to a human preference matrix developed by kaplan and kaplan 1989 for urban green spaces which gives gi characteristics that are most linked to human wellbeing the column on the left gives landscape features that attract people and engage them longer by promoting understanding while the column on the right are features that encourage exploration of the landscape these features and algorithms are defined in more detail below 2 2 2 color histogram color histogram quantifies colors distribution of pixel intensities in an image anami et al 2010 since the color green plays an important role in gi design we implemented rgb 256 bin color histogram chapelle et al 1999 in this approach color is represented by a three dimensional vector for each color channel i e rgb and it corresponding to a pixel point in color space cha chapelle et al 1999 showed that for image classification the use of color space i e rgb red green blue vs hsv hue saturation value has very minimal effect on classification accuracy therefore we used rgb color space in this work fig 5 shows the color histogram for the sample survey image shown in fig 3 2 2 3 histogram of oriented gradient the histogram of oriented gradient hog is used to extract the distribution of edges of objects in an image dalal and triggs 2005 hog finds distinctive shapes of objects present in a design image e g the shapes of trees shown in fig 3 which are needed to determine the legibility feature in table 1 that affects human understanding of the scene we implemented the hog feature based on the approach of dalal and triggs 2005 which gives a 31 dimensional descriptor hog features are computed over rectangular regions represented by a grid of 3x3 blocks with an overlap of 8 pixels each block contains 16x16 pixels and hogs are computed for each block with 9 unsigned orientations orientations are the gradient vectors at each pixel for 16x16 pixel cells that are categorized into a 9 bin histogram the histogram ranges from 0 to 180 because the orientations are unsigned so there are 20 per bin 2 2 4 gist descriptors the previous two image features identify specific objects from the images oliva and torralba 2001 proposed a technique to estimate the structure or shape of a scene using spatial properties of the scene called gist descriptors e g degree of naturalness openness roughness or expansion fig 6 shows the gist descriptor for a sample input green space image the results show the local energy spectrum which measures the quantity of radiation passing through or emitted from the surface of an object for all objects in the image the image is divided into a 4x4 cell matrix for spatially grouping objects each sub image of the matrix corresponds to the amount of radiation with multiple wavelengths emitted or passed through objects placed in the corresponding spatial location dark regions in the gist descriptor matrix e g the upper right and lower left sub images in fig 10 thus identify openness in the image objects such as the tree in the middle of the input image on the left in fig 7 emit higher radiation and thus have more energy spectrum captured by the gist descriptors in the corresponding sub image this spatial distribution of local energy spectrum provides a more holistic representation of a scene once the gi design image features are extracted using the methods described above a supervised machine learning approach is used as a regression model to predict human preferences for each design since we average ratings from all survey participants the preference scores range continuously between 1 and 5 i e real numbers we concatenate the above three features into a combined color hist hog gist feature set and use these features for training the model for the supervised machine learning the adaptive boosting algorithm adaboost is used which is an ensemble learning algorithm that combines a weak classifier e g decision trees or k nearest neighbors knn to output a strong regressor freund and schapire 1999 adaboost gives high accuracy and prevents over fitting of parameters to the training dataset by using an iterative learning model which improves accuracy by learning from mistakes made in an earlier training step we use decision trees as the weak classifier in this work 2 2 5 model validation evaluation we split the training data which consisted of 360 images into training 70 and validation 30 sets tuning the model for optimal performance using the training set the accuracy of the model is computed using the coefficient of r2 fig 8 shows examples from the validation set the predicted ratings for each of these images were identical to the actual human ratings the results indicate that the machine learning model predicts human preferences reasonably well particularly since human ratings can have considerable uncertainty james et al 2009 the training set is predominantly tree based green infrastructure and generally human preferences increase with more trees in images as shown in fig 8 however merely adding more trees bushes plants etc are not sufficient to obtain high human preferences the image features extracted from the training set color histogram eoh spatial histogram gist descriptor and their corresponding landscape features from kaplan kaplan s preference matrix 1998 also play an important role for example recall that color histogram measures the green coloring and coherence in the design fig 7 c and d show that the rating changes from 1 to 3 just by adding trees which provides more green richness and also by arranging the trees in an orderly manner along the sidewalks e g fig 8 d is more coherent than fig 8 c this added coherence can be seen in the color histogram results for these images in fig 9 complexity legibility and mystery parameters of the preference matrix also change human preferences these parameters are captured by the gist and hog descriptors as shown in fig 10 2 2 6 evaluation using amazon mechanical turk finally to further evaluate the models with more diverse human preference scores buhrmester et al 2011 we also conducted an online survey with additional participants using amazon mechanical turk the nature of the questionnaire was the same as shown in fig 2 we posted 30 different images of gi and asked 20 turkers to rate the images after we obtained the turkers ratings we ran our human preference model to see how well it performs on the same 30 images using pearson correlation coefficient ρ we obtained ρ 0 83 for the turkers ratings and model predictions which shows that the model can identify key features of gi spaces affecting human preference from visual scenes despite the inherent variability in human preference ratings 2 3 evaluating the hydrologic benefits of gi design in order to evaluate the hydrologic benefits of candidate gi designs rhessys distributed parameter hydrologic model tague and band 2004 is used rhessys is designed to simulate integrated water carbon and nutrient cycling and transport over spatially variable terrain at small to medium scales i e from 1st to 3rd order watersheds its spatially distributed framework enables the modeling of spatiotemporal interactions between different eco hydrological processes from patch to watershed scales hwang et al 2012 fig 11 gives an overview of the steps involved in preparing the required input for gi simulation using rhessys and integrating it with the human preference model for social ranking google street view images are first extracted from the google maps application programming interface api http maps google com for the neighborhood where gi design is planned the coordinates latitude longitude are identified using google maps for the front yard of every house in the neighborhood where gi installation is under consideration these coordinates are used to generate patch identifications ids that are used as inputs for gi parameters in rhessys patch ids are unique numbers associated with particular ecosystem patches which are the smallest resolution spatial units that define areas of similar soil moisture and land cover characteristics in landscapes modified by humans patches can also be defined to contain stream channels road segments storm sewers etc human sources of water and nutrients are also defined at this level tague and band 2004 therefore adding trees to the existing patches requires reflecting the presence of trees in the existing lawn by generating patch ids from their respective coordinate values patch id numbers are generated using grass gis http grass osgeo org which queries the existing raster map layers to output labels associated with input coordinate values using the r what command these labels are patch id numbers that represent the current vegetation type present in the raster map the next step in executing rhessys is to prepare additional non spatial files called worldfiles which represent landscapes patterns such as land use tree canopy etc within rhessys worldfiles are generated with a grass interface program that references input raster maps and a text document defining initial state variables e g saturation deficit within the patch level which is a measure of the degree of saturation within the worldfile each spatial layer is associated with the following identifiers an assigned id based on the map used and state variable values for stores and fluxes at each spatial level that are initialized at the start of the simulation the grass2world g2w program is used to generate the worldfiles automatically from spatial data layers within grass gis next in order to incorporate new trees into the existing worldfile based on the newly generated patch id numbers an awk script is used awk script is an interpreted programming language typically used for data extraction in unix operating systems aho et al 1978 finally the modified worldfile is used to execute the rhessys model and forecast the effects of the gi design on annual mean stream flow and base flow ritcher et al 1996 at the outlet of the urban catchment where the neighborhood is located these statistics have previously been identified as important indicators for measuring the alteration in hydrology of an urban catchment due to changes in land use and land cover by simulating the impacts of gi designs on mean flows using rhessys and comparing flows with current and pre development values we can observe the efficiency of the gi designs in moving towards restoration of pre development urban watershed hydrology this provides an initial environmental rating for the gi designs which can be supplemented with nutrient impacts in the future 3 case study implication for gi design in dead run watershed baltimore md the gi design methodology described in section 2 is applied in the dead run watershed in baltimore md which is part of the baltimore ecosystem study bes as the available human preference training set contains predominantly tree based green infrastructure the case study application focuses on the addition of trees to a neighborhood within dead run watershed shown in fig 12 and observing how the human and hydrologic benefits change with different designs six scenarios are examined 1 existing scenario 2 adding a single tree in the neighborhood 3 adding multiple trees in open vs clustered arrangements 4 adding small vs large trees 5 adding trees on one vs both sides of the street and 6 adding single vs mixed species of trees google street view images of the entire neighborhood are extracted and used as a base case for human preferences the coordinate values latitude longitude of the front yard of every house in the region are chosen as potential locations to add trees google street view overlay api javascript is used to overlay trees in the google image for estimating human preferences using the machine learning model trained and validated in galesburg section 3 the results are given in the following sections once the human preference ratings are obtained rhessys is used to evaluate the hydrologic benefits of the added green infrastructure to the larger dead run watershed using the methodology described in section 2 2 the hydrologic data used in the simulations were collected by the baltimore ecosystem study as outlined by band et al 2012 the hydrologic impacts of the green infrastructure scenarios are compared with the existing scenario using 2007 hydrologic and model data from band et al 2012 and a pre development scenario the earliest hydrologic data available for the pre development scenario is 1960 which was before the design neighborhood was built the hydrologic results are presented in section 3 2 along with a discussion of the interactions between human preferences and hydrologic benefits 3 1 results of human preference modeling the human preference results for the six scenarios in baltimore are shown below for a sample street view image settings that include trees plants bushes etc have been shown to reduce symptoms of both mental fatigue and stress coley et al 1997 sullivan 2004 the existing scenario fig 13 a lacks such a setting and therefore the human preference model predicts a low rating simply by adding a single tree to the existing scenario fig 13 b the model predicts a higher preference rating based on shifts in the trained image features for hog and color histogram the spatial arrangement of trees in gi settings is also an important design factor that affects the level of complexity in the scene kaplan and kaplan 1989 which is captured in the model by the gist descriptor feature highly complex settings are generally less preferable and thus the clustered arrangement of trees in fig 13 d gets a much lower human preference rating than the open arrangement of trees in fig 13 c fig 14 shows how shifting energy spectrums in the gist descriptors captures this change the legibility and mystery factors of human preference require three dimensional inference allowing people to imagine themselves in the scene kaplan and kaplan 1989 the mystery parameter involves exploration and therefore the view of angle plays a very important role in human perceptions the arrangement of trees on both sides of the street in fig 13 d and f create a more three dimensional view where people can imagine more exploration and thus prefer such settings the spatial arrangement of trees on both sides of a street is well captured by the gist feature and color histogram and hog identify tree shape and color the features have captured the visual change in gi setting which encourages human beings to explore the setting further it is important to note that larger trees are more preferable than smaller trees and therefore have higher ratings compare fig 13 e vs f as well as 13 c vs g the reason for this result is the higher degree of richness and complexity in the setting which are captured by gist descriptors the degree of richness and complexity for a large versus small tree is shown in fig 15 the setting with a large tree has fewer dark regions in the gist matrix than the setting with a small tree 3 2 results of hydrologic modeling using rhessys fig 16 which shows the hydrologic impact of the gi design scenarios compares the modeled change in annual mean stream flow and base groundwater flow for the following scenarios 1 existing condition for the year 2007 2 pre development conditions in 1960 3 adding a single tree to every yard in the neighborhood and 4 adding two trees to every yard in the neighborhood because hydrologic impacts are measured at the watershed outlet the physical arrangement of the trees which are important for the human preference results in section 3 1 do not change the estimated hydrologic impacts as shown in fig 16 the tree scenarios significantly reduce the impacts of development on the urban watershed shifting both the mean stream and base flows much closer to the pre development scenario the extent of the shift is particularly impressive given that the neighborhood of the gi design is only 1 2 percent of the land area in the overall dead run watershed the dead run watershed has 36 of its land as impervious gwynn falls water quality management plan 2004 and thus adding more trees in front of every house in the neighborhood has significant beneficial impacts table 3 shows a quantitative comparison of the two different gi scenarios as well as a third scenario adding three trees to every yard from table 3 it is clear that green infrastructure can play an important role in restoring the urban watershed to predevelopment hydrology adding two trees to every front yard in the neighborhood gives better results than one tree but the incremental improvement declines as additional trees are added in fact adding three trees to every yard does not cause any additional changes to flow at the outlet of watershed moreover the results in section 1 indicate that the second and third trees in each yard do not cause significant differences in human preferences compare fig 13 b and d although their arrangements across the neighborhood are important the hydrologic benefits of adding more green components to urban watersheds helps in restoration of pre development flows us epa 2009 but our results show that the human preference model can identify specific patterns for installing these green components that are more beneficial for human wellbeing 4 conclusions this work develops a novel computational green infrastructure gi design framework that couples storm water management requirements with criteria for human wellbeing current approaches to designing green storm water features tend to emphasize rapid removal of storm water runoff to reduce impacts of downstream flows and pollutant loads the approach presented in this paper is a first step towards incorporating the benefits of human health associated with these urban green spaces into the design process in order to map landscape features that are correlated with human wellbeing to features that can be used to train a supervised machine learning model a suite of computer vision algorithms and techniques have been used the result is the first gi design model capable of predicting human preferences with validation accuracy of 81 which is quite high given the variability in human perceptions acknowledgement this material is based upon work supported by the national science foundation under grant no 1216817 and 1261582 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation 
26208,in this article we propose a novel clustering based ensemble model ceesnn for air pollution prediction based on evolving spiking neural networks esnn where each esnn network is trained on a separate set of time series in our approach we generate training sets by clustering an initial set of time series with respect to pollution values each obtained cluster of time series is used to build a single esnn network in the experiments we forecasted ozone and pm10 pollution for greater london area for 1 3 and 6 h ahead based on data from three monitoring sites located there the prediction quality of the proposed ceesnn model as well as the singleton neucube model an mlp network and the arima model was assessed by means of several quality measures the experimental results show that the proposed ensemble model is able to give significantly better forecasting results than the other three models keywords spiking neural network neucube ozone pollution pm10 pollution pollution level prediction clustering spatio temporal data 1 introduction air pollution is a problem that affects many countries and regions of the world the main cause of pollution lies in the outdated industrial installations coal power plants coal furnaces and inoperative combustion engines powering ubiquitous cars fighting air pollution is an expensive and a long term process that requires the engagement of both governments private institutions and ordinary citizens we believe nevertheless that in the short term it is possible to develop effective methods for pollution prediction which would enable sending out warnings of the upcomming high pollution periods to the people in the regions affected by the pollution so that they can individually undertake the necessary steps to avoid the negative consequences for their health in this article we focus on proposing such a promising solution in this field the most commonly measured air pollutants are ozone o3 particulate matter pm10 and particulate matter pm2 5 where 10 and 2 5 denote the maximum diameters in micrometers of particles of pm10 and pm2 5 respectively carbon dioxide co2 nitrogen dioxide no2 nitrogen oxides no x and nitric oxide no significant concentration of air pollution causes serious health problems and is regarded as an important factor in premature deaths martuzzi et al 2006 report that around 9 of deaths among people aged 30 and above living in 13 large italian cities are caused by long term exposure to particulate matter pm10 and ozone pollution short term exposure resulted in around 1 5 of deaths according to naddafi et al 2012 several thousand of deaths in tehran were caused by excessive air pollution the investigation of air pollution and its impact on health of the population of the eleven biggest polish cites for years 2006 2011 shows a significant correlation between pollution concentration and increased mortality rate badyda et al 2017 in order to predict air pollution data with high effectiveness one might consider using ensemble models which are regarded as predicting classifying with much higher quality than singleton prediction models rokach 2010 mendes moreira et al 2012 krawczyk et al 2014 krawczyk 2015 cichosz 2014 ren et al 2016 on the other hand an evolving spiking neural network was designed specifically for spatio temporal data kasabov 2014 which suggests they this type of a network is highly suitable for air prediction based on time series in fact neucube kasabov 2014 implementation of an evolving spiking neural network was successfully applied to solve several types of problems such as the problem of analysing eeg data capecci et al 2015 2016 classification and regression in ecology hartono et al 2014 tu et al 2014 2017 kasabov et al 2016b electrogastrography egg breen et al 2016 gender and age classification alvi et al 2018 having in mind these properties of prediction ensemble models and evolving spiking neural networks we offer a clustering based ensemble model of evolving spiking neural networks ceesnn to distinctive features of our model belong our proposed method of determining sets of training samples of time series by using a clustering algorithm using each generated cluster of training samples to build a corresponding evolving spiking neural network carrying out prediction by an evolving spiking neural network whose corresponding cluster representing pollution time series of training samples is most similar to historical pollution time series of a new sample our ensemble model differs from other solutions available in the literature see for example krogh and vedelsby 1995 zhou et al 2002 perrone and cooper 2005 siwek and osowski 2012 also in that it stores not only evolving spiking neural networks but also clusters with appropriate data from training samples in order to evaluate our proposed ceesnn prediction model we implemented it in particular each evolving spiking neural network of ceesnn is implemented as an instance of neucube which is a neuromorphic tool for processing spatio temporal data kasabov 2014 in experiments we used a dataset of pollutants for the greater london area collected by the uk network for air pollution monitoring defra 2018 we selected two types of pollutants ozone and pm10 both types of pollutants may cause significant health problems such as shortness of breath chest pain fatigue asthma and heart disease katsouyanni et al 1996 macnee and donaldson 2003 martuzzi et al 2006 badyda and grellier 2014 in order to asses the quality of the proposed model we used several types of error measures mean absolute error mae root mean square error rmse index of agreement ia and band error for comparative purposes the experimental evaluation was performed also for plain neucube model in which only a single spiking neural network is used for air pollution forecasting multilayer perceptron neural network mlp and autoregressive integrated moving average arima to summarize the main objectives of this article are to offer a new clustering based ensemble model ceesnn of evolving spiking neural networks to offer the implementation of ceesnn in which evolving spiking neural networks are realized as instances of the neucube software to verify experimentally the prediction effectiveness of the proposed ensemble ceesnn model against the plain neucube model mlp network and arima model with respect to a number of prediction quality measures mae rmse ia person s correlation coefficient and the band error on the greater london area data the paper is structured as follows related work is presented in section 2 in section 3 we give a description of neucube in section 4 we describe selected datasets and data preprocessing methods the proposed clustering based ensemble model for air pollution forecasting is provided in section 5 the results of the experiments are presented in section 6 finally in section 7 we conclude our work and provide possible future research directions in the air pollution prediction domain 2 related work several approaches have been applied to predict air pollution level among the most popular methods are the ones employing artificial neural networks a multilayer perceptron mlp neural network for prediction of hourly no x and no2 concentrations for london was proposed in gardner and dorling 1999 a comparison of effectiveness of predicting air pollution in christchurch new zealand by means of mlp regression trees built with the cart algorithm and linear regression was presented in gardner 1999 the best quality of prediction was obtained by using the mlp networks similar results were presented in kukkonen et al 2003 it was shown there that neural networks perform slightly better than other linear and deterministic modelling systems on the helsinki metropolitan area air pollution dataset the neural network model defined in kukkonen et al 2003 incorporates traffic air pollution concentration and weather data in niska et al 2004 parameters of neural network utilized for air pollution prediction were optimized with a genetic algorithm ga other applications of multilayer neural networks for prediction of pm2 5 and pm10 pollution in santiago chile were introduced in perez et al 2000 perez and reyes 2002 forecasting of pollution level for pm10 and pm2 5 pollutants in thessaloniki and helsinki with principal component analysis and ann was given in voukantsis et al 2011 in kurt and oktay 2010 an artificial neural network ann was proposed to forecast several types of pollutants in besiktas district the proposed model incorporated information about the geographical location of monitoring sites and the moving window technique to improve the quality of prediction based on the history of the pollution level measured for 14 days a prediction with an ann was made for one two and three days ahead in siwek et al 2009 siwek and osowski 2012 a two stage approach to pollution forecasting was proposed first pollution data were decomposed using a wavelet transformation then an ensemble of several types of linear and non linear prediction models were integrated to minimize prediction error this approach was tested on air pollution data for warsaw the usefulness of the wavelet decomposition in air pollution prediction was experimentally proved in osowski and garanty 2006 2007 where it was used in combination with support vector machine svm for forecasting in northern poland li and shue 2004 proposed a two stage approach based on wavelet decomposition and self organizing maps som for air pollution prediction which was applied to predict air pollution in taiwan the correlation between pm10 pollution and several meteorological factors by applying wavelet transformation was studied in shaharuddin et al 2008 zaharim et al 2009 they showed that there was a significant relationship between pm10 and low frequency components of weather factors e g temperature rainfall or wind speed more recently trajectory based and wavelet transformation models combined with the mlp network for predicting daily pm2 5 pollution in several china provinces for 1 or 2 days in advance was introduced in feng et al 2015 prediction of hourly the cm2 5 pollution in china by combining satellite images weather variables with mlp network was presented in mao et al 2017 daily concentration of pm2 5 pollution in china was predicted using principal component analysis pca and least squares support vector machine lssvm with the cuckoo algorithm for optimization of prediction quality in sun and sun 2017 the back propagation neural network with wavelet transformation for prediction of pm10 so2 and no2 concentration in china is presented in bai et al 2016 it was reported in feng et al 2015 mao et al 2017 that a single prediction model used for air pollution forecasting has a tendency to underestimate high pollution levels and to overestimate low pollution levels such a phenomenon affects the quality of the overall prediction in our solution we propose an ensemble of evolving spiking neural networks where each network is trained on its own specific and distinct set of training time series 3 evolving spiking neural networks implementation in neucube in this section we recall the elementary notions of spiking neural networks snn evolving spiking neural networks esnn and implementation of esnn in neucube 3 1 spiking neural networks spiking neural network snn is a type of an artificial neural network in which the communication between neurons and information processing in neural nodes is based on the exchange of pulses spikes ponulak and kasinski 2011 the learning processes neuronal communication and behaviour of snns are highly inspired by the learning processes and functioning of human brain kasabov 2012 2018 the main process of learning in a network is based on the modification of the importance of connections between neurons tavanaei et al 2019 several types of neuronal models for snn have been proposed in the literature among the most popular are the integrate and fire if and leaky integrate and fire lif models thorpe and gautrais 1998 gerstner 2001 other proposed models are the spike responsive model srm jolivet et al 2003 the izhikevich neuron model izhikevich 2003 and probabilistic spiking neuron model kasabov 2010 the review of spiking neuronal models is given in gerstner and kistler 2002 in each of the mentioned models the state of a neuron is expressed by its membrane potential the spikes incoming to a neuron can either increase or decrease the membrane potential the emission of a spike from a neuron occurs when its membrane potential exceeds a predefined threshold value a general architecture of an snn consists of a set of neural nodes and their connections called synapses a synapse refers to a direct connection between pre synaptic and post synaptic neurons the spikes over a synapse are always exchanged from pre synaptic to post synaptic neurons the synapses interconnecting neurons can be either excitatory or inhibitory the spike exchange over an excitatory synapse increases the membrane potential of a post synaptic neuron while the spike exchange over an inhibitory synapse decreases the membrane potential of such neuron the strength of the synapse connection is controlled by the weight of the connection which is modified during the spike exchange process tavanaei et al 2019 the change of the weight of a synapse dependens on the predefined learning parameters and exact times of spike emissions from pre synaptic and post synaptic neurons gerstner 2001 snns can be organized into several learning types unsupervised supervised reinforcement and network architectures feedforward or recurrent ponulak and kasinski 2011 in unsupervised learning there is no target value or decision class and in general output steering signal used during the learning process the origin of unsupervised learning of snn comes from the definition of hebbs rules of storing information by neural synapses hebb et al 1949 contrarily a supervised learning process uses target values or class labels for modification of weights of neural synapses the reinforcement learning is inspired by the general learning principles of animals and is a promising type of learning which can be successfully implemented in snn ponulak and kasinski 2011 in a feedforward snn data is processed directly from the input to the output of the network whereas in a recurrent snn a feedback loop between neurons in the network is applied the architecture of snn may contain both input internal hidden and output layers of neurons or only input and output layers 3 2 evolving spiking neural networks esnn is a special class of snn in which a specific scheme of supervised learning of the network is implemented the notion of evolving refers to a changing repository of output neurons whose synapses weights are shaped during the supervised learning of the network the idea behind the evolving repository of neurons is to reveal hidden temporal patterns of input data schliebs and kasabov 2013 a general supervised learning principle of esnn can be described as follows a repository of output neurons is created and populated until a predefined number of neurons is reached additionally the neurons in the repository are separated into decision classes to which the respective samples belong each neuron in the output repository is directly connected to each other neuron in the internal layer if such a layer exists or to neurons in the input layer after the size of the output repository is reached the replacement of old neurons with new neurons is carried out based on the similarity of candidate neurons to the neurons already existing in the repository if the similarity between a candidate neuron and any neuron in the corresponding decision class from the repository is below a predefined threshold then the weights vector of the latter neuron is modified with the weights vector of the candidate neuron and then the candidate neuron is discarded otherwise if the similarity is greater than the given threshold then the oldest neuron from the corresponding decision class of neurons is removed and the candidate neuron is inserted into the repository the similarity between the newly created neuron and neurons already existing in the repository may be expressed as the euclidean distance between the vector weights of corresponding neurons kasabov et al 2013 lobo et al 2018 3 3 neucube and its implementation of esnn neucube is a software tool which implements an esnn network the architecture of neucube is presented in fig 1 it consists of 4 modules spike encoding module neural network module optimization module and network analysis module samples of sets of multi attribute time series are provided to neucube as input data during the phases of learning and error assessment a value of a target attribute is also provided for each sample depending on a selected mode neucube produces classification results class labels or prediction results real values as an output the spike encoding process is performed as follows each time series of each sample is transformed into a series of spikes using one of the four encoding algorithms available in neucube after data encoding the network is learned in two stages unsupervised and supervised the unsupervised learning of neural network is performed according to the spike time dependent plasticity rules stdp song et al 2000 the role of unsupervised learning stage is to project patterns hidden in data onto the neural network structure the supervised learning utilizes the dynamic evolving spike neural network desnn algorithm proposed in kasabov et al 2013 the neucube architecture is also equipped with more sophisticated components like optimization module for selecting best values of learning parameters and k fold cross validation module the current version of the optimization module provides two algorithms grid search over provided space of learning parameters which is used in our experiments and genetic algorithm heuristic neucube s implementation of an esnn network uses the leaky integrate and fire lif model of neuron thorpe and gautrais 1998 gerstner 2001 gerstner and kistler 2002 according to the lif model the state of an i th neuron is characterized by its post synaptic potential p s p i described by eq 1 1 p s p i 0 if fired j w j i m o d o r d e r j otherwise where j w j i refers to the sum over connections weights from all pre synaptic neurons j of the neuron i m o d is the modulation factor within a range 0 1 and o r d e r j is the rank of an incoming spike from a pre synaptic neuron j when a neuron i emits a spike its post synaptic potential is set to 0 and its refractory period begins during the refractory time the neuron is insensitive to spikes incoming from its pre synaptic neurons the spike emission is triggered when p s p i value achieves a predefined threshold the first spike emitted over all pre synaptic neurons j has rank 0 the rank of subsequent spikes is increased by 1 for each incoming spike lobo et al 2018 in fig 2 we show the lif neuron model implemented in neucube in the first phase the potential of a neuron increases with spikes incoming from pre synaptic neurons until it reaches a predefined threshold and emits a spike after that the neuron enters a refractory period during which it is insensitive to incoming spikes 3 3 1 spike encoding in neucube the spike encoding process in neucube is two stage 1 first a threshold for spike encoding is calculated from a time series s t where t 1 2 n and n is the number of the elements of the time series by means of the adaptive threshold based atb method tu et al 2017 the atb method calculates the threshold according to eq 2 2 atb μ sr σ where μ and σ are mean and standard deviation respectively calculated over a series of values δ s t s t s t 1 where t 2 3 n and sr is a user given spike rate parameter whose value can be specified when a set of samples is loaded to neucube 2 second the atb value is used to generate spikes signal using one of the four encoding algorithms available in neucube threshold based representation tr bens spiker algorithm bsa moving window mv or step forward sf the tr algorithm simply generates a positive spike for time points t 2 3 n when δ s t a t b and a negative one when δ s t a t b the sf algorithm kasabov et al 2016a incorporates past changes over the signal let us assume that b t represents a baseline change at moment t and b 1 s 1 for t 2 3 n if the signal value s t b t 1 a t b then a positive spike is generated and b t is updated to b t 1 a t b otherwise if s t b t 1 a t b then a negative spike is emitted and b t is updated to b t 1 a t b the mw algorithm kasabov et al 2016a is similar to the sf algorithm however the baseline change b t is calculated over a time window t rather than over the whole past signal calculating change over a time window allows to eliminate of the impact of noise occurring in the data the bsa algorithm schrauwen and campenhout 2003 generates only positive spikes and is intended for high frequency signals the spike rate is the average number of generated spikes for each time series divided by the number of time points in each series 3 3 2 learning principles in neucube as already mentioned neucube utilizes the lif neuron model the cube of neurons connections between them and weights of connections are initialized according to the small world principle the neighboring neurons are strongly interconnected the weight of a connection is the ratio of a random value from the interval 0 1 0 1 to the euclidean distance between vectors of neurons coordinates the number and values of neuron coordinates depend on the used neucube template of a neural network in fig 4 we present a three dimensional template of a neural network with 1000 neurons the modification of weights during the unsupervised learning process is performed according to the spike time dependent plasticity stdp learning rule song et al 2000 hebb 2005 espinosa ramos et al 2017 laña et al 2018 and may be described as follows let us assume a weight of a synapse is denoted by w j i with i being a post synaptic and j a pre synaptic neuron the weight w j i t at the moment t is modified according to eq 3 where η is the learning parameter and δ t is the time interval since the last firing of neuron i 3 w j i t w j i t 1 η δ t if t i t j w j i t 1 if t i t j w j i t 1 η δ t otherwise if neuron j fires before neuron i does then the weight w j i is increased otherwise if neuron i fires before neuron j does the weight w j i is decreased the unsupervised learning of network in neucube may be performed several times and the exact number of iterations of this type of learning is controlled by a user specified parameter in fig 3 we illustrated modification of weights of synapses in the case of spike exchange between neurons in part a of fig 3 a spike is emitted from the pre synaptic neuron 4 to the post synaptic neuron 1 at the moment t 1 this results in the accumulation of p s p 1 value let us assume that at the moment t 2 t 1 neuron 3 emits a spike to neuron 1 which results in the accumulation of p s p 1 spike emission by neuron 1 and start of its refractory period in such a case the weight of the synapse directed from neuron 4 to neuron 1 will be increased by the factor η t 1 t 2 and the weight of synapse directed from neuron 3 to neuron 1 will remain unchanged now let us assume that at the moment t 3 t 2 neuron 1 being still in its refractory period receives a spike from its pre synaptic neuron 2 as shown in part b of fig 3 this will result in the weight w 2 1 decrease by η t 2 t 3 as the emission of a spike from the pre synaptic neuron 2 occurred after the emission of a spike from the post synaptic neuron 1 the supervised learning process is based on the above principles of the stdp learning and lif neuron model and is performed according to the desnn algorithm kasabov et al 2016a for each training sample one output neuron is created and connected to every other neuron in the already trained network as follows 1 the initial weights of these connections are set to 0 and are modified according to eq 4 4 w j i m o d o r d e r j i where i is the output post synaptic neuron and j is any other neuron pre synaptic learned at the unsupervised learning stage eq 4 applies only to the first spike incoming to neuron i from neuron j 2 at the subsequent stages weights are modified in accordance with eq 5 5 w j i t w j i t 1 d r i f t if s j t 1 w j i t 1 d r i f t if s j t 0 where d r i f t is a user given steering parameter and s j t denotes an incoming spike from a pre synaptic neuron j as follows from eq 5 the connection between active spikes is strengthened over time and the connection between inactive spikes is weakened in fig 4 we show the main neucube panel as well as visualisation of the forecasting results example results are presented in fig 5 in fig 6 the optimization module of neucube is presented 4 using greater london area pollution data for forecasting this section presents the characteristic of the greater london area pollution data and the way they were used in the experiments fig 7 shows monitoring sites for greater london area with three sites selected for experiments these sites are stations being a part of the automatic urban and rural network aurn which is managed by the department for environment food and rural affairs defra of uk defra 2018 the network is responsible for monitoring hourly pollution of such pollutants as ozone particulate matter pm10 pm2 5 nitric oxide nitrogen dioxide etc in addition to the pollution data observations for each site contain weather related variables temperature wind speed and wind direction there are 225 monitoring sites across the uk data availability begins on 22 02 1973 the aims of the network are in particular measuring and informing about air quality identifying long term trends in air pollution concentrations and verifying effectiveness of air control polices defra 2018 in table 1 we present detailed locations of monitoring sites for greater london area and types of pollutants measured at each of them similarly to the approach presented in siwek and osowski 2012 feng et al 2015 we decided to represent wind related data as two components wind x referring to the southern direction wind component and wind y referring to the eastern direction wind component according to eq 6 6 wind x w c o s ϕ wind y w s i n ϕ where w stands for wind speed and ϕ stands for wind direction in our experiments we used data from three stations london bloomsbury london marylebone road london north kensington the stations monitor the following pollutants no no2 no x o3 pm10 pm2 5 pm2 5 and pm10 contain two components volatile particulate matter vpm and non volatile particulate matter nvpm in our experiments o3 and pm10 pollutants were used as target values violin plots illustrating distribution of pollutants within the period 2016 2017 are shown in fig 9 the summary of statistics of variables including unit range of values average and standard deviation is shown in table 2 the correlation matrices for london bloomsbury london marylebone road and london north kensington monitoring sites are presented in tables 3 5 respectively similarly to the previous studies reported in the literature siwek and osowski 2012 feng et al 2015 mao et al 2017 as the input attributes we include when predicting o3 pollution level historical values of o3 temperature wind x and wind y components when predicting pm10 pollution level historical values of temperature wind x and wind y components as well as pm10 nvpm10 and vpm10 in our experiments prediction of a pollution level in a given location is based on the historical pollution data obtained from the monitoring site in that location for each selected monitoring site from the dataset of 17544 h pollution observations for years 2016 2017 we uniformly randomly select 1000 samples each 12 h long selecting of a sample consists in selecting its starting time t start and then constructing a 12 h time series for each respective pollution and weather variable based on its values at moments t start t start 1 t start 11 thus in the case of ozone prediction a sample will contain four 12 h time series built from o3 wind x wind y and temperature values respectively denoted by a in fig 8 whereas in the case of pm10 prediction a sample will contain five 12 h time series built from pm10 vpm10 nvpm10 wind x wind y and temperature values denoted by b in fig 8 the starting hour of each sample is randomly selected from the 17526 calculated as 17544 12 6 hours of data records random selection of samples is carried out separately for 1 3 and 6 h ahead predictions as a target pollution value used in the learning phase and the assessment of the prediction quality either value from 1st 3rd or 6th hour following the selected 12 h period is used the creation of samples is illustrated in fig 8 the missing pollution values for each selected pollutant and monitoring site are imputed with the corresponding average pollution values calculated over the whole dataset the percentages of missing values for the selected monitoring sites and pollutant types are given in table 2 the weather variables are modeled and hence the problem of missing values does not occur in that case the used model for weather data is the weather research and forecasting model wrf developed by the national center for atmospheric research ncar wrf 2018 5 ceesnn the proposed clustering based esnn ensemble model in this section we propose a novel prediction model ceesnn as an ensemble of evolving spiking neural networks each of which is trained on a cluster of samples similar with respect to pollutant level time series the ceesnn model is presented in subsection 5 1 and its neucube implementation is delivered in subsection 5 2 5 1 the ceesnn model the architecture of the proposed ceesnn model is presented in fig 10 as the input for the proposed model a set of samples is provided to train ceesnn each sample contains air pollution and weather related time series variables as well as a target value denoting the real pollution level from the predicted hour the procedure of training ceesnn is realized in two steps first selected samples are clustered according to pollutant level time series the aim of this process is to partition samples into k distinct clusters such that samples with similar pollutant level time series belong to the same group while samples with dissimilar pollutant level time series belong to different groups each cluster in fig 10 is denoted by c l i where i 1 2 k then for each cluster of samples a unique esnn instance is created and trained according to the learning principles presented in subsection 3 2 this process can be optimized the optimization of esnn applied for the experimental purposes is described in subsection 6 1 the developed ensemble model may be deployed in a real air pollution prediction system in the following way first an ensemble is trained using historical samples containing pollution and weather related time series when a new sample appears in the system a prediction of its pollution level is calculated with ceesnn in two stages based on a pollution level time series t s of the new sample a cluster c l m representing training pollution level time series most similar to t s is identified predicted pollution level is returned by esnn m instance associated with cluster c l m please note that after the creation of the ceesnn model it is sufficient to store only pollution level time series of samples in clusters rather than whole samples for prediction purposes the following operations clustering of samples based on their pollutant level time series identification of a cluster most similar to the pollution level time series of a new sample which are carried out during learning and prediction processes can be realized in several ways for example one may apply either a clustering algorithm such as pam sarda espinosa 2018 which generates a given number of clusters or clustering algorithms such as dbscan ester et al 1996 nbc zhou et al 2005 and their efficient variants offered in kryszkiewicz and skonieczny 2005 kryszkiewicz and lasek 2010b and kryszkiewicz and lasek 2010a which identify the number of clusters automatically the notion of similarity between a cluster of time series and a single time series can be expressed in terms of e g minimum average or maximum distances or as a distance between a medoid of a cluster and the single time series han et al 2011 the proposed ensemble model can evolve in an online learning scenario after processing a sufficient number of new samples all samples are re clustered and each esnn in the ensemble is retrained 5 2 neucube implementation of the ceesnn model we realized the ideas presented in the subsection 5 1 using the implementation of esnn available in neucube for simplicity we will discuss the steps of our approach basing on the assumptions applied in the experiments that 1000 samples are available each 12 h long and the samples are partitioned into 4 clusters the architecture of neucube implementaion of ceesnn following these assumptions is presented in fig 11 learning in the neucube implementation of ceesnn is presented beneath clustering of samples is performed solely according to 12 h long time series of a pollutant ozone or pm10 respectively each pollutant time series is regarded as a vector of 12 consecutive values of the pollutant the partitioning around medoids pam implemented in the r dtwclust package sarda espinosa 2018 was used to create clusters based on such vectors distance between two vectors is computed using the euclidean metric as previously mentioned the assumed number of clusters to be generated was set to 4 the goal of our paper was to propose an ensemble model implement it and evaluate its usefulness evolving spiking neural networks were implemented as instances of the neucube software however this software bases on a graphical user interface which poses some scalability problems as a consequence the number of clusters had to be reasonably limited 4 clusters appeared to be a good compromise between the dataset partitioning and the system complexity and in fact the prediction quality of our proposed ceesnn model using 4 clusters was in most cases higher than the prediction quality of other popular approaches regarded as efficient 1 1 the current efforts of knowledge engineering and discovery research institute at auckland university of technology responsible for neucube s development and management are concentrated on transforming neucube to a software library which may be embedded into an external source code and used in cloud platforms because of that a matter of future studies is to verify the impact of the selected number of clusters on the prediction quality of the proposed ensemble based on each cluster of samples a unique neucube instance is created and trained using 2 fold cross validation in the following way the first fold of samples is used to train a neucube instance which after that predicts values of the pollution level for the samples from the second fold then the analogous process is performed namely the samples from the second fold are used to train a neucube instance which then predicts values of the pollution level for the samples from the first fold as a result each neucube instance returns a table of values with two rows and the number of columns equal to the number of samples in the respective cluster each column in such a table contains two values the predicted pollution level and the real pollution level denoted in fig 11 as a target value for a sample from the cluster at the end the resulting tables obtained for each neucube instance in the proposed ensemble are concatenated to form one resultant table containing two rows one for predicted value and one for real value of pollution level for each of the selected 1000 samples the contents of the resultant table is used for determination of quality prediction measures such as those described in subsection 6 3 6 experimental results in this section we present results of our experiments first the experimental setup is described then we present examples of the obtained clustering results next we present scatter plots of predicted and observed values obtained with the proposed ceesnn ensemble model in order to show the real impact of our ensemble model we give a detailed comparison of prediction quality obtained with the proposed ceesnn ensemble model the plain neucube a single instance of neucube multilayer perceptron neural network mlp and autoregressive integrated moving average arima the quality of prediction is measured using the following measures mean absolute error mae root mean square error rmse index of agreement ia square of pearson s correlation coefficient r2 and band error 6 1 experimental setup for experimental purposes we implemented an ensemble of evolving spiking neural networks where each network is an instance of neucube both in the neucube implementation used in the proposed ensemble and in plain neucube we used the following configuration of neucube the threshold based representation tr algorithm with spike rate sr 0 5 is used for spike encoding the notion of spike encoding is explained in subsection 3 3 1 the used values of neucube parameters as shown in table 6 were found as those for which 2 fold cross validation gave minimal rmse values the considered candidate values of parameters were generated by applying grid search as an mlp network does not accept samples consisting of time series each sample was transformed into a record namely each time series of a sample was replaced by the average value of the time series elements we used the mlp network available in the r package monmlp cannon 2017 which is an implementation of the multilayer perceptron network proposed in zhang and zhang 1999 in table 7 we present the most important parameters of the mlp network used for prediction hidden 1 no and hidden 2 no refer to the number of neurons in the first and in the second hidden layer respectively no ensemble is the number of neural networks in an ensemble used in the experiments no of max iterations and the optimization method refer to the optimization algorithm executed during the network learning process for the prediction with the arima method we used the r package forecast hyndman 2018 for each sample the univariate arima model based on the pollution value was created with parameters suggested by the auto arima function available in the package these parameters are presented in table 8 parameters of the non seasonal part of the arima model are as follows p autoregressive order d the degree of differencing and q the moving average order analogously parameters of the seasonal part of arima are as follows p the autoregressive order d the degree of differencing and q the moving average order 6 2 results of clustering pollution level time series examples of ozone and pm10 time series clustering for the london bloomsbury monitoring site are shown in fig 12 and fig 13 respectively it can be noticed that each generated cluster of samples contains similar pollution levels time series and that its averaged pollution levels time series differs from averaged pollution levels time series of each other generated cluster it can be observed that the third cluster shown in fig 12 contains time series with the lowest values of ozone with the majority of the observations not exceeding the value of the pollutant of 40 μ g m 3 on the other hand cluster 2 contains time series with the highest values of ozone within 12 h period in the performed experiments the number of samples assigned to each cluster varies between 50 and 400 usually the cluster with the highest average pollution level contains the smallest number of samples similarly in fig 13 cluster 3 contains time series with the lowest pm10 pollution value with most observations below 20 μ g m 3 and cluster 1 contains observatios with the highest pollution values table 9 contains all parameters of the tsclust function from package dtwclust used for the clustering process sarda espinosa 2018 the preprocessing technique refers to z score normalization and is not used in the clustering 6 3 quality prediction measures in this subsection we first provide definitions of quality prediction measures which we then use to validate our ceesnn model against three other prediction models plain neucube arima mlp 6 3 1 definitions of quality prediction measures similarly to the previous studies in kurt and oktay 2010 feng et al 2015 mao et al 2017 we assess the prediction quality by means of the following measures mean absolute error mae root mean square error rmse index of agreement ia square of pearson s correlation coefficient r2 and band error both mae given in eq 7 and rmse given in eq 8 are absolute values smaller values of which indicate better prediction quality index of agreement given by eq 9 is a relative measure expressed in percentages for this type of error greater values indicate better prediction of the model in particular if ia is equal to 100 then for each training sample its predicted pollution value is the same as the real one pearson s correlation coefficient expresses the linear dependency between predicted and observed values in equations 7 10 n denotes the number of observations n 1000 in our experiments o i and p i are observed and predicted values respectively and o is the average of the observed values 7 m a e i 1 n o i p i n 8 r m s e i 1 n o i p i 2 n 9 i a 100 i 1 n o i p i 2 i 1 n p i o o i o 2 10 r 2 n i 1 n o i p i i 1 n o i i 1 n p i 2 i 1 n p i 2 i 1 n p i 2 n i 1 n o i 2 i 1 n o i 2 calculation of the band error is illustrated in fig 14 the range of values of a pollutant is divided into a given number of intervals in our experiments 5 intervals were used if the predicted and observed values fall into the same interval the prediction is reported as correct and 0 is returned as the band error otherwise the band error is calculated as one plus the number of intervals between the interval containing the predicted value and the interval containing the observed value 6 3 2 scatter plots of predicted vs observed pollution values in fig 15 we present scatter plots for the london bloomsbury station illustrating the observed and predicted values for ozone and pm10 pollution obtained using our ceesnn ensemble model similar scatter plots for london marylebone road monitoring site are shown in fig 16 and for london north kensington site in fig 17 for all scatter plots the overall dispersion of points around diagonal increases with the increasing prediction time for the london bloomsbury and london north kensington monitoring sites the scatter between observed values and values predicted with the ceesnn ensemble model is smaller for pm10 pollution than for ozone pollution so the air pollution quality prediction of ceesnn for these sites is better for pm10 than for ozone in the case of the london marylebone road monitoring site the results for both pollutants are comparable in figs 15 17 black continuous lines express linear dependency of the predicted values on the observed values the exact values of parameters of linear functions are given as well in these figures 6 3 3 quality prediction comparison values of four error indicators mae rmse ia and r2 were calculated for the proposed ceesnn model as well as for the plain neucube model the mlp network and the arima model and are shown in figs 18 and 19 for both ozone and pm10 pollution the ceesnn ensemble model outperforms the plain neucube model with respect to all these four quality measures additionally it can be noticed that the prediction results obtained with mlp network are always worse than the results obtained from other three used models based on the obtained results we conclude that while the arima model outperforms all other models for 1 h ahead prediction our ceesnn ensemble model gives better results for 3 and 6 h ahead predictions as follows from fig 20 for both ozone and pm10 pollution prediction the ceesnn model always provides larger number of correct predictions than the plain neucube model the total number of correct predictions is greater for ceesnn for 1 3 and 6 h ahead and for each monitoring site we will now compare the band error for the two best models ceesnn and arima in table 10 we show detailed results of ozone prediction for london bloomsbury london marylebone road and london north kensington monitoring sites it can be noticed that band 2 error and band 3 error occur very rarely band 4 error never occurs for any of the used models the overall correct number of predictions decreases with the increasing prediction time in particular for the london bloomsbury monitoring site the percentages of correct predictions for the ceesnn model for 1 3 and 6 h ahead forecasting are 77 4 76 7 and 71 0 respectively whereas for the arima model the respective percentages are 87 8 74 0 and 62 6 similarly for the london marylebone road monitoring site the percentages of correct predictions for 1 3 and 6 h are 80 3 76 9 and 77 5 respectively for the ceesnn model and the respective percentages for the arima model are 88 3 77 5 71 2 for the london north kensington site the percentages of correct predictions for 1 3 and 6 h for ceesnn model are 80 1 75 9 72 2 respectively while for the arima model the respective percentages are 89 6 73 4 and 64 3 the band errors for pm10 pollution are presented in table 11 as follows from this table for the london bloomsbury monitoring site the percentages of correct predictions for 1 3 and 6 h are very good for both models and are 96 2 96 3 94 9 respectively for the ceesnn model and 98 9 96 1 94 6 for the arima model for the london marylebone road monitoring site the percentages of correct predictions for 1 3 and 6 h are 83 8 81 7 79 6 respectively for the ceesnn model and 87 6 81 0 and 70 1 for the arima model for the london north kensington monitoring site the percentages of correct predictions for 1 3 and 6 h are 93 9 93 0 93 0 respectively for the ceesnn model and 96 7 94 2 and 88 6 for the arima model in fig 20 we show the bar plots illustrating correct predictions number versus total band error for both selected pollutants and all three monitoring sites 7 conclusions and future work in the article we proposed a new model clustering based ensemble of evolving spiking neural networks ceesnn for air pollution prediction in the proposed model each esnn of the ensemble is trained on a separate set of time series in our approach we proposed to generate training sets by clustering an initial set of pollution level time series each obtained cluster of time series is used to build a single esnn network as a result the ensemble prediction model consists of as many esnn networks as the number of created time series clusters in our research esnn networks were implemented as instances of neucube which is a neuromorphic tool for processing spatio temporal data the distinctive novelty in our approach is application of clustering of pollution level time series in order to get appropriate sets of training time series samples to be used for training esnn networks in the experimental part of the paper we forecasted ozone and pm10 pollution for greater london area based on data from three monitoring sites located there the pollution forceasting was carried out for 1 3 and 6 h ahead the quality of the prediction was measured by means of mean absolute error root mean square error index of agreement person s correlation coefficient and band error for comparative purposes the experimental evaluation was performed for both the proposed ceesnn ensemble prediction model as well as plain neucube model in which only a single essn network is used and two other models used in recent studies for air pollution prediction a multilayer perceptron and autoregressive integrated moving average model the ceesnn ensemble model turned out to be the predominant winner in the case of forecasting for 3 and 6 h ahead in addition the quality prediction of the ceesnn turned out significantly better than the plain neucube model for all experimental scenarios in the future studies we intend to concentrate on adapting a moving window technique to the proposed model for long time prediction of pollution levels acknowledgements this work was supported by eu project pacific atlantic network for technical higher education and research panther grant number 2013 5659 004 001 ema2 and by the institute of computer science of warsaw university of technology grant no ii 2018 ds 1 the authors are grateful to dr maryam gholami doborjeh and dr elisa capecci from knowledge engineering and discovery research institute auckland university of technology for providing us with the matlab implementation of neucube 
26208,in this article we propose a novel clustering based ensemble model ceesnn for air pollution prediction based on evolving spiking neural networks esnn where each esnn network is trained on a separate set of time series in our approach we generate training sets by clustering an initial set of time series with respect to pollution values each obtained cluster of time series is used to build a single esnn network in the experiments we forecasted ozone and pm10 pollution for greater london area for 1 3 and 6 h ahead based on data from three monitoring sites located there the prediction quality of the proposed ceesnn model as well as the singleton neucube model an mlp network and the arima model was assessed by means of several quality measures the experimental results show that the proposed ensemble model is able to give significantly better forecasting results than the other three models keywords spiking neural network neucube ozone pollution pm10 pollution pollution level prediction clustering spatio temporal data 1 introduction air pollution is a problem that affects many countries and regions of the world the main cause of pollution lies in the outdated industrial installations coal power plants coal furnaces and inoperative combustion engines powering ubiquitous cars fighting air pollution is an expensive and a long term process that requires the engagement of both governments private institutions and ordinary citizens we believe nevertheless that in the short term it is possible to develop effective methods for pollution prediction which would enable sending out warnings of the upcomming high pollution periods to the people in the regions affected by the pollution so that they can individually undertake the necessary steps to avoid the negative consequences for their health in this article we focus on proposing such a promising solution in this field the most commonly measured air pollutants are ozone o3 particulate matter pm10 and particulate matter pm2 5 where 10 and 2 5 denote the maximum diameters in micrometers of particles of pm10 and pm2 5 respectively carbon dioxide co2 nitrogen dioxide no2 nitrogen oxides no x and nitric oxide no significant concentration of air pollution causes serious health problems and is regarded as an important factor in premature deaths martuzzi et al 2006 report that around 9 of deaths among people aged 30 and above living in 13 large italian cities are caused by long term exposure to particulate matter pm10 and ozone pollution short term exposure resulted in around 1 5 of deaths according to naddafi et al 2012 several thousand of deaths in tehran were caused by excessive air pollution the investigation of air pollution and its impact on health of the population of the eleven biggest polish cites for years 2006 2011 shows a significant correlation between pollution concentration and increased mortality rate badyda et al 2017 in order to predict air pollution data with high effectiveness one might consider using ensemble models which are regarded as predicting classifying with much higher quality than singleton prediction models rokach 2010 mendes moreira et al 2012 krawczyk et al 2014 krawczyk 2015 cichosz 2014 ren et al 2016 on the other hand an evolving spiking neural network was designed specifically for spatio temporal data kasabov 2014 which suggests they this type of a network is highly suitable for air prediction based on time series in fact neucube kasabov 2014 implementation of an evolving spiking neural network was successfully applied to solve several types of problems such as the problem of analysing eeg data capecci et al 2015 2016 classification and regression in ecology hartono et al 2014 tu et al 2014 2017 kasabov et al 2016b electrogastrography egg breen et al 2016 gender and age classification alvi et al 2018 having in mind these properties of prediction ensemble models and evolving spiking neural networks we offer a clustering based ensemble model of evolving spiking neural networks ceesnn to distinctive features of our model belong our proposed method of determining sets of training samples of time series by using a clustering algorithm using each generated cluster of training samples to build a corresponding evolving spiking neural network carrying out prediction by an evolving spiking neural network whose corresponding cluster representing pollution time series of training samples is most similar to historical pollution time series of a new sample our ensemble model differs from other solutions available in the literature see for example krogh and vedelsby 1995 zhou et al 2002 perrone and cooper 2005 siwek and osowski 2012 also in that it stores not only evolving spiking neural networks but also clusters with appropriate data from training samples in order to evaluate our proposed ceesnn prediction model we implemented it in particular each evolving spiking neural network of ceesnn is implemented as an instance of neucube which is a neuromorphic tool for processing spatio temporal data kasabov 2014 in experiments we used a dataset of pollutants for the greater london area collected by the uk network for air pollution monitoring defra 2018 we selected two types of pollutants ozone and pm10 both types of pollutants may cause significant health problems such as shortness of breath chest pain fatigue asthma and heart disease katsouyanni et al 1996 macnee and donaldson 2003 martuzzi et al 2006 badyda and grellier 2014 in order to asses the quality of the proposed model we used several types of error measures mean absolute error mae root mean square error rmse index of agreement ia and band error for comparative purposes the experimental evaluation was performed also for plain neucube model in which only a single spiking neural network is used for air pollution forecasting multilayer perceptron neural network mlp and autoregressive integrated moving average arima to summarize the main objectives of this article are to offer a new clustering based ensemble model ceesnn of evolving spiking neural networks to offer the implementation of ceesnn in which evolving spiking neural networks are realized as instances of the neucube software to verify experimentally the prediction effectiveness of the proposed ensemble ceesnn model against the plain neucube model mlp network and arima model with respect to a number of prediction quality measures mae rmse ia person s correlation coefficient and the band error on the greater london area data the paper is structured as follows related work is presented in section 2 in section 3 we give a description of neucube in section 4 we describe selected datasets and data preprocessing methods the proposed clustering based ensemble model for air pollution forecasting is provided in section 5 the results of the experiments are presented in section 6 finally in section 7 we conclude our work and provide possible future research directions in the air pollution prediction domain 2 related work several approaches have been applied to predict air pollution level among the most popular methods are the ones employing artificial neural networks a multilayer perceptron mlp neural network for prediction of hourly no x and no2 concentrations for london was proposed in gardner and dorling 1999 a comparison of effectiveness of predicting air pollution in christchurch new zealand by means of mlp regression trees built with the cart algorithm and linear regression was presented in gardner 1999 the best quality of prediction was obtained by using the mlp networks similar results were presented in kukkonen et al 2003 it was shown there that neural networks perform slightly better than other linear and deterministic modelling systems on the helsinki metropolitan area air pollution dataset the neural network model defined in kukkonen et al 2003 incorporates traffic air pollution concentration and weather data in niska et al 2004 parameters of neural network utilized for air pollution prediction were optimized with a genetic algorithm ga other applications of multilayer neural networks for prediction of pm2 5 and pm10 pollution in santiago chile were introduced in perez et al 2000 perez and reyes 2002 forecasting of pollution level for pm10 and pm2 5 pollutants in thessaloniki and helsinki with principal component analysis and ann was given in voukantsis et al 2011 in kurt and oktay 2010 an artificial neural network ann was proposed to forecast several types of pollutants in besiktas district the proposed model incorporated information about the geographical location of monitoring sites and the moving window technique to improve the quality of prediction based on the history of the pollution level measured for 14 days a prediction with an ann was made for one two and three days ahead in siwek et al 2009 siwek and osowski 2012 a two stage approach to pollution forecasting was proposed first pollution data were decomposed using a wavelet transformation then an ensemble of several types of linear and non linear prediction models were integrated to minimize prediction error this approach was tested on air pollution data for warsaw the usefulness of the wavelet decomposition in air pollution prediction was experimentally proved in osowski and garanty 2006 2007 where it was used in combination with support vector machine svm for forecasting in northern poland li and shue 2004 proposed a two stage approach based on wavelet decomposition and self organizing maps som for air pollution prediction which was applied to predict air pollution in taiwan the correlation between pm10 pollution and several meteorological factors by applying wavelet transformation was studied in shaharuddin et al 2008 zaharim et al 2009 they showed that there was a significant relationship between pm10 and low frequency components of weather factors e g temperature rainfall or wind speed more recently trajectory based and wavelet transformation models combined with the mlp network for predicting daily pm2 5 pollution in several china provinces for 1 or 2 days in advance was introduced in feng et al 2015 prediction of hourly the cm2 5 pollution in china by combining satellite images weather variables with mlp network was presented in mao et al 2017 daily concentration of pm2 5 pollution in china was predicted using principal component analysis pca and least squares support vector machine lssvm with the cuckoo algorithm for optimization of prediction quality in sun and sun 2017 the back propagation neural network with wavelet transformation for prediction of pm10 so2 and no2 concentration in china is presented in bai et al 2016 it was reported in feng et al 2015 mao et al 2017 that a single prediction model used for air pollution forecasting has a tendency to underestimate high pollution levels and to overestimate low pollution levels such a phenomenon affects the quality of the overall prediction in our solution we propose an ensemble of evolving spiking neural networks where each network is trained on its own specific and distinct set of training time series 3 evolving spiking neural networks implementation in neucube in this section we recall the elementary notions of spiking neural networks snn evolving spiking neural networks esnn and implementation of esnn in neucube 3 1 spiking neural networks spiking neural network snn is a type of an artificial neural network in which the communication between neurons and information processing in neural nodes is based on the exchange of pulses spikes ponulak and kasinski 2011 the learning processes neuronal communication and behaviour of snns are highly inspired by the learning processes and functioning of human brain kasabov 2012 2018 the main process of learning in a network is based on the modification of the importance of connections between neurons tavanaei et al 2019 several types of neuronal models for snn have been proposed in the literature among the most popular are the integrate and fire if and leaky integrate and fire lif models thorpe and gautrais 1998 gerstner 2001 other proposed models are the spike responsive model srm jolivet et al 2003 the izhikevich neuron model izhikevich 2003 and probabilistic spiking neuron model kasabov 2010 the review of spiking neuronal models is given in gerstner and kistler 2002 in each of the mentioned models the state of a neuron is expressed by its membrane potential the spikes incoming to a neuron can either increase or decrease the membrane potential the emission of a spike from a neuron occurs when its membrane potential exceeds a predefined threshold value a general architecture of an snn consists of a set of neural nodes and their connections called synapses a synapse refers to a direct connection between pre synaptic and post synaptic neurons the spikes over a synapse are always exchanged from pre synaptic to post synaptic neurons the synapses interconnecting neurons can be either excitatory or inhibitory the spike exchange over an excitatory synapse increases the membrane potential of a post synaptic neuron while the spike exchange over an inhibitory synapse decreases the membrane potential of such neuron the strength of the synapse connection is controlled by the weight of the connection which is modified during the spike exchange process tavanaei et al 2019 the change of the weight of a synapse dependens on the predefined learning parameters and exact times of spike emissions from pre synaptic and post synaptic neurons gerstner 2001 snns can be organized into several learning types unsupervised supervised reinforcement and network architectures feedforward or recurrent ponulak and kasinski 2011 in unsupervised learning there is no target value or decision class and in general output steering signal used during the learning process the origin of unsupervised learning of snn comes from the definition of hebbs rules of storing information by neural synapses hebb et al 1949 contrarily a supervised learning process uses target values or class labels for modification of weights of neural synapses the reinforcement learning is inspired by the general learning principles of animals and is a promising type of learning which can be successfully implemented in snn ponulak and kasinski 2011 in a feedforward snn data is processed directly from the input to the output of the network whereas in a recurrent snn a feedback loop between neurons in the network is applied the architecture of snn may contain both input internal hidden and output layers of neurons or only input and output layers 3 2 evolving spiking neural networks esnn is a special class of snn in which a specific scheme of supervised learning of the network is implemented the notion of evolving refers to a changing repository of output neurons whose synapses weights are shaped during the supervised learning of the network the idea behind the evolving repository of neurons is to reveal hidden temporal patterns of input data schliebs and kasabov 2013 a general supervised learning principle of esnn can be described as follows a repository of output neurons is created and populated until a predefined number of neurons is reached additionally the neurons in the repository are separated into decision classes to which the respective samples belong each neuron in the output repository is directly connected to each other neuron in the internal layer if such a layer exists or to neurons in the input layer after the size of the output repository is reached the replacement of old neurons with new neurons is carried out based on the similarity of candidate neurons to the neurons already existing in the repository if the similarity between a candidate neuron and any neuron in the corresponding decision class from the repository is below a predefined threshold then the weights vector of the latter neuron is modified with the weights vector of the candidate neuron and then the candidate neuron is discarded otherwise if the similarity is greater than the given threshold then the oldest neuron from the corresponding decision class of neurons is removed and the candidate neuron is inserted into the repository the similarity between the newly created neuron and neurons already existing in the repository may be expressed as the euclidean distance between the vector weights of corresponding neurons kasabov et al 2013 lobo et al 2018 3 3 neucube and its implementation of esnn neucube is a software tool which implements an esnn network the architecture of neucube is presented in fig 1 it consists of 4 modules spike encoding module neural network module optimization module and network analysis module samples of sets of multi attribute time series are provided to neucube as input data during the phases of learning and error assessment a value of a target attribute is also provided for each sample depending on a selected mode neucube produces classification results class labels or prediction results real values as an output the spike encoding process is performed as follows each time series of each sample is transformed into a series of spikes using one of the four encoding algorithms available in neucube after data encoding the network is learned in two stages unsupervised and supervised the unsupervised learning of neural network is performed according to the spike time dependent plasticity rules stdp song et al 2000 the role of unsupervised learning stage is to project patterns hidden in data onto the neural network structure the supervised learning utilizes the dynamic evolving spike neural network desnn algorithm proposed in kasabov et al 2013 the neucube architecture is also equipped with more sophisticated components like optimization module for selecting best values of learning parameters and k fold cross validation module the current version of the optimization module provides two algorithms grid search over provided space of learning parameters which is used in our experiments and genetic algorithm heuristic neucube s implementation of an esnn network uses the leaky integrate and fire lif model of neuron thorpe and gautrais 1998 gerstner 2001 gerstner and kistler 2002 according to the lif model the state of an i th neuron is characterized by its post synaptic potential p s p i described by eq 1 1 p s p i 0 if fired j w j i m o d o r d e r j otherwise where j w j i refers to the sum over connections weights from all pre synaptic neurons j of the neuron i m o d is the modulation factor within a range 0 1 and o r d e r j is the rank of an incoming spike from a pre synaptic neuron j when a neuron i emits a spike its post synaptic potential is set to 0 and its refractory period begins during the refractory time the neuron is insensitive to spikes incoming from its pre synaptic neurons the spike emission is triggered when p s p i value achieves a predefined threshold the first spike emitted over all pre synaptic neurons j has rank 0 the rank of subsequent spikes is increased by 1 for each incoming spike lobo et al 2018 in fig 2 we show the lif neuron model implemented in neucube in the first phase the potential of a neuron increases with spikes incoming from pre synaptic neurons until it reaches a predefined threshold and emits a spike after that the neuron enters a refractory period during which it is insensitive to incoming spikes 3 3 1 spike encoding in neucube the spike encoding process in neucube is two stage 1 first a threshold for spike encoding is calculated from a time series s t where t 1 2 n and n is the number of the elements of the time series by means of the adaptive threshold based atb method tu et al 2017 the atb method calculates the threshold according to eq 2 2 atb μ sr σ where μ and σ are mean and standard deviation respectively calculated over a series of values δ s t s t s t 1 where t 2 3 n and sr is a user given spike rate parameter whose value can be specified when a set of samples is loaded to neucube 2 second the atb value is used to generate spikes signal using one of the four encoding algorithms available in neucube threshold based representation tr bens spiker algorithm bsa moving window mv or step forward sf the tr algorithm simply generates a positive spike for time points t 2 3 n when δ s t a t b and a negative one when δ s t a t b the sf algorithm kasabov et al 2016a incorporates past changes over the signal let us assume that b t represents a baseline change at moment t and b 1 s 1 for t 2 3 n if the signal value s t b t 1 a t b then a positive spike is generated and b t is updated to b t 1 a t b otherwise if s t b t 1 a t b then a negative spike is emitted and b t is updated to b t 1 a t b the mw algorithm kasabov et al 2016a is similar to the sf algorithm however the baseline change b t is calculated over a time window t rather than over the whole past signal calculating change over a time window allows to eliminate of the impact of noise occurring in the data the bsa algorithm schrauwen and campenhout 2003 generates only positive spikes and is intended for high frequency signals the spike rate is the average number of generated spikes for each time series divided by the number of time points in each series 3 3 2 learning principles in neucube as already mentioned neucube utilizes the lif neuron model the cube of neurons connections between them and weights of connections are initialized according to the small world principle the neighboring neurons are strongly interconnected the weight of a connection is the ratio of a random value from the interval 0 1 0 1 to the euclidean distance between vectors of neurons coordinates the number and values of neuron coordinates depend on the used neucube template of a neural network in fig 4 we present a three dimensional template of a neural network with 1000 neurons the modification of weights during the unsupervised learning process is performed according to the spike time dependent plasticity stdp learning rule song et al 2000 hebb 2005 espinosa ramos et al 2017 laña et al 2018 and may be described as follows let us assume a weight of a synapse is denoted by w j i with i being a post synaptic and j a pre synaptic neuron the weight w j i t at the moment t is modified according to eq 3 where η is the learning parameter and δ t is the time interval since the last firing of neuron i 3 w j i t w j i t 1 η δ t if t i t j w j i t 1 if t i t j w j i t 1 η δ t otherwise if neuron j fires before neuron i does then the weight w j i is increased otherwise if neuron i fires before neuron j does the weight w j i is decreased the unsupervised learning of network in neucube may be performed several times and the exact number of iterations of this type of learning is controlled by a user specified parameter in fig 3 we illustrated modification of weights of synapses in the case of spike exchange between neurons in part a of fig 3 a spike is emitted from the pre synaptic neuron 4 to the post synaptic neuron 1 at the moment t 1 this results in the accumulation of p s p 1 value let us assume that at the moment t 2 t 1 neuron 3 emits a spike to neuron 1 which results in the accumulation of p s p 1 spike emission by neuron 1 and start of its refractory period in such a case the weight of the synapse directed from neuron 4 to neuron 1 will be increased by the factor η t 1 t 2 and the weight of synapse directed from neuron 3 to neuron 1 will remain unchanged now let us assume that at the moment t 3 t 2 neuron 1 being still in its refractory period receives a spike from its pre synaptic neuron 2 as shown in part b of fig 3 this will result in the weight w 2 1 decrease by η t 2 t 3 as the emission of a spike from the pre synaptic neuron 2 occurred after the emission of a spike from the post synaptic neuron 1 the supervised learning process is based on the above principles of the stdp learning and lif neuron model and is performed according to the desnn algorithm kasabov et al 2016a for each training sample one output neuron is created and connected to every other neuron in the already trained network as follows 1 the initial weights of these connections are set to 0 and are modified according to eq 4 4 w j i m o d o r d e r j i where i is the output post synaptic neuron and j is any other neuron pre synaptic learned at the unsupervised learning stage eq 4 applies only to the first spike incoming to neuron i from neuron j 2 at the subsequent stages weights are modified in accordance with eq 5 5 w j i t w j i t 1 d r i f t if s j t 1 w j i t 1 d r i f t if s j t 0 where d r i f t is a user given steering parameter and s j t denotes an incoming spike from a pre synaptic neuron j as follows from eq 5 the connection between active spikes is strengthened over time and the connection between inactive spikes is weakened in fig 4 we show the main neucube panel as well as visualisation of the forecasting results example results are presented in fig 5 in fig 6 the optimization module of neucube is presented 4 using greater london area pollution data for forecasting this section presents the characteristic of the greater london area pollution data and the way they were used in the experiments fig 7 shows monitoring sites for greater london area with three sites selected for experiments these sites are stations being a part of the automatic urban and rural network aurn which is managed by the department for environment food and rural affairs defra of uk defra 2018 the network is responsible for monitoring hourly pollution of such pollutants as ozone particulate matter pm10 pm2 5 nitric oxide nitrogen dioxide etc in addition to the pollution data observations for each site contain weather related variables temperature wind speed and wind direction there are 225 monitoring sites across the uk data availability begins on 22 02 1973 the aims of the network are in particular measuring and informing about air quality identifying long term trends in air pollution concentrations and verifying effectiveness of air control polices defra 2018 in table 1 we present detailed locations of monitoring sites for greater london area and types of pollutants measured at each of them similarly to the approach presented in siwek and osowski 2012 feng et al 2015 we decided to represent wind related data as two components wind x referring to the southern direction wind component and wind y referring to the eastern direction wind component according to eq 6 6 wind x w c o s ϕ wind y w s i n ϕ where w stands for wind speed and ϕ stands for wind direction in our experiments we used data from three stations london bloomsbury london marylebone road london north kensington the stations monitor the following pollutants no no2 no x o3 pm10 pm2 5 pm2 5 and pm10 contain two components volatile particulate matter vpm and non volatile particulate matter nvpm in our experiments o3 and pm10 pollutants were used as target values violin plots illustrating distribution of pollutants within the period 2016 2017 are shown in fig 9 the summary of statistics of variables including unit range of values average and standard deviation is shown in table 2 the correlation matrices for london bloomsbury london marylebone road and london north kensington monitoring sites are presented in tables 3 5 respectively similarly to the previous studies reported in the literature siwek and osowski 2012 feng et al 2015 mao et al 2017 as the input attributes we include when predicting o3 pollution level historical values of o3 temperature wind x and wind y components when predicting pm10 pollution level historical values of temperature wind x and wind y components as well as pm10 nvpm10 and vpm10 in our experiments prediction of a pollution level in a given location is based on the historical pollution data obtained from the monitoring site in that location for each selected monitoring site from the dataset of 17544 h pollution observations for years 2016 2017 we uniformly randomly select 1000 samples each 12 h long selecting of a sample consists in selecting its starting time t start and then constructing a 12 h time series for each respective pollution and weather variable based on its values at moments t start t start 1 t start 11 thus in the case of ozone prediction a sample will contain four 12 h time series built from o3 wind x wind y and temperature values respectively denoted by a in fig 8 whereas in the case of pm10 prediction a sample will contain five 12 h time series built from pm10 vpm10 nvpm10 wind x wind y and temperature values denoted by b in fig 8 the starting hour of each sample is randomly selected from the 17526 calculated as 17544 12 6 hours of data records random selection of samples is carried out separately for 1 3 and 6 h ahead predictions as a target pollution value used in the learning phase and the assessment of the prediction quality either value from 1st 3rd or 6th hour following the selected 12 h period is used the creation of samples is illustrated in fig 8 the missing pollution values for each selected pollutant and monitoring site are imputed with the corresponding average pollution values calculated over the whole dataset the percentages of missing values for the selected monitoring sites and pollutant types are given in table 2 the weather variables are modeled and hence the problem of missing values does not occur in that case the used model for weather data is the weather research and forecasting model wrf developed by the national center for atmospheric research ncar wrf 2018 5 ceesnn the proposed clustering based esnn ensemble model in this section we propose a novel prediction model ceesnn as an ensemble of evolving spiking neural networks each of which is trained on a cluster of samples similar with respect to pollutant level time series the ceesnn model is presented in subsection 5 1 and its neucube implementation is delivered in subsection 5 2 5 1 the ceesnn model the architecture of the proposed ceesnn model is presented in fig 10 as the input for the proposed model a set of samples is provided to train ceesnn each sample contains air pollution and weather related time series variables as well as a target value denoting the real pollution level from the predicted hour the procedure of training ceesnn is realized in two steps first selected samples are clustered according to pollutant level time series the aim of this process is to partition samples into k distinct clusters such that samples with similar pollutant level time series belong to the same group while samples with dissimilar pollutant level time series belong to different groups each cluster in fig 10 is denoted by c l i where i 1 2 k then for each cluster of samples a unique esnn instance is created and trained according to the learning principles presented in subsection 3 2 this process can be optimized the optimization of esnn applied for the experimental purposes is described in subsection 6 1 the developed ensemble model may be deployed in a real air pollution prediction system in the following way first an ensemble is trained using historical samples containing pollution and weather related time series when a new sample appears in the system a prediction of its pollution level is calculated with ceesnn in two stages based on a pollution level time series t s of the new sample a cluster c l m representing training pollution level time series most similar to t s is identified predicted pollution level is returned by esnn m instance associated with cluster c l m please note that after the creation of the ceesnn model it is sufficient to store only pollution level time series of samples in clusters rather than whole samples for prediction purposes the following operations clustering of samples based on their pollutant level time series identification of a cluster most similar to the pollution level time series of a new sample which are carried out during learning and prediction processes can be realized in several ways for example one may apply either a clustering algorithm such as pam sarda espinosa 2018 which generates a given number of clusters or clustering algorithms such as dbscan ester et al 1996 nbc zhou et al 2005 and their efficient variants offered in kryszkiewicz and skonieczny 2005 kryszkiewicz and lasek 2010b and kryszkiewicz and lasek 2010a which identify the number of clusters automatically the notion of similarity between a cluster of time series and a single time series can be expressed in terms of e g minimum average or maximum distances or as a distance between a medoid of a cluster and the single time series han et al 2011 the proposed ensemble model can evolve in an online learning scenario after processing a sufficient number of new samples all samples are re clustered and each esnn in the ensemble is retrained 5 2 neucube implementation of the ceesnn model we realized the ideas presented in the subsection 5 1 using the implementation of esnn available in neucube for simplicity we will discuss the steps of our approach basing on the assumptions applied in the experiments that 1000 samples are available each 12 h long and the samples are partitioned into 4 clusters the architecture of neucube implementaion of ceesnn following these assumptions is presented in fig 11 learning in the neucube implementation of ceesnn is presented beneath clustering of samples is performed solely according to 12 h long time series of a pollutant ozone or pm10 respectively each pollutant time series is regarded as a vector of 12 consecutive values of the pollutant the partitioning around medoids pam implemented in the r dtwclust package sarda espinosa 2018 was used to create clusters based on such vectors distance between two vectors is computed using the euclidean metric as previously mentioned the assumed number of clusters to be generated was set to 4 the goal of our paper was to propose an ensemble model implement it and evaluate its usefulness evolving spiking neural networks were implemented as instances of the neucube software however this software bases on a graphical user interface which poses some scalability problems as a consequence the number of clusters had to be reasonably limited 4 clusters appeared to be a good compromise between the dataset partitioning and the system complexity and in fact the prediction quality of our proposed ceesnn model using 4 clusters was in most cases higher than the prediction quality of other popular approaches regarded as efficient 1 1 the current efforts of knowledge engineering and discovery research institute at auckland university of technology responsible for neucube s development and management are concentrated on transforming neucube to a software library which may be embedded into an external source code and used in cloud platforms because of that a matter of future studies is to verify the impact of the selected number of clusters on the prediction quality of the proposed ensemble based on each cluster of samples a unique neucube instance is created and trained using 2 fold cross validation in the following way the first fold of samples is used to train a neucube instance which after that predicts values of the pollution level for the samples from the second fold then the analogous process is performed namely the samples from the second fold are used to train a neucube instance which then predicts values of the pollution level for the samples from the first fold as a result each neucube instance returns a table of values with two rows and the number of columns equal to the number of samples in the respective cluster each column in such a table contains two values the predicted pollution level and the real pollution level denoted in fig 11 as a target value for a sample from the cluster at the end the resulting tables obtained for each neucube instance in the proposed ensemble are concatenated to form one resultant table containing two rows one for predicted value and one for real value of pollution level for each of the selected 1000 samples the contents of the resultant table is used for determination of quality prediction measures such as those described in subsection 6 3 6 experimental results in this section we present results of our experiments first the experimental setup is described then we present examples of the obtained clustering results next we present scatter plots of predicted and observed values obtained with the proposed ceesnn ensemble model in order to show the real impact of our ensemble model we give a detailed comparison of prediction quality obtained with the proposed ceesnn ensemble model the plain neucube a single instance of neucube multilayer perceptron neural network mlp and autoregressive integrated moving average arima the quality of prediction is measured using the following measures mean absolute error mae root mean square error rmse index of agreement ia square of pearson s correlation coefficient r2 and band error 6 1 experimental setup for experimental purposes we implemented an ensemble of evolving spiking neural networks where each network is an instance of neucube both in the neucube implementation used in the proposed ensemble and in plain neucube we used the following configuration of neucube the threshold based representation tr algorithm with spike rate sr 0 5 is used for spike encoding the notion of spike encoding is explained in subsection 3 3 1 the used values of neucube parameters as shown in table 6 were found as those for which 2 fold cross validation gave minimal rmse values the considered candidate values of parameters were generated by applying grid search as an mlp network does not accept samples consisting of time series each sample was transformed into a record namely each time series of a sample was replaced by the average value of the time series elements we used the mlp network available in the r package monmlp cannon 2017 which is an implementation of the multilayer perceptron network proposed in zhang and zhang 1999 in table 7 we present the most important parameters of the mlp network used for prediction hidden 1 no and hidden 2 no refer to the number of neurons in the first and in the second hidden layer respectively no ensemble is the number of neural networks in an ensemble used in the experiments no of max iterations and the optimization method refer to the optimization algorithm executed during the network learning process for the prediction with the arima method we used the r package forecast hyndman 2018 for each sample the univariate arima model based on the pollution value was created with parameters suggested by the auto arima function available in the package these parameters are presented in table 8 parameters of the non seasonal part of the arima model are as follows p autoregressive order d the degree of differencing and q the moving average order analogously parameters of the seasonal part of arima are as follows p the autoregressive order d the degree of differencing and q the moving average order 6 2 results of clustering pollution level time series examples of ozone and pm10 time series clustering for the london bloomsbury monitoring site are shown in fig 12 and fig 13 respectively it can be noticed that each generated cluster of samples contains similar pollution levels time series and that its averaged pollution levels time series differs from averaged pollution levels time series of each other generated cluster it can be observed that the third cluster shown in fig 12 contains time series with the lowest values of ozone with the majority of the observations not exceeding the value of the pollutant of 40 μ g m 3 on the other hand cluster 2 contains time series with the highest values of ozone within 12 h period in the performed experiments the number of samples assigned to each cluster varies between 50 and 400 usually the cluster with the highest average pollution level contains the smallest number of samples similarly in fig 13 cluster 3 contains time series with the lowest pm10 pollution value with most observations below 20 μ g m 3 and cluster 1 contains observatios with the highest pollution values table 9 contains all parameters of the tsclust function from package dtwclust used for the clustering process sarda espinosa 2018 the preprocessing technique refers to z score normalization and is not used in the clustering 6 3 quality prediction measures in this subsection we first provide definitions of quality prediction measures which we then use to validate our ceesnn model against three other prediction models plain neucube arima mlp 6 3 1 definitions of quality prediction measures similarly to the previous studies in kurt and oktay 2010 feng et al 2015 mao et al 2017 we assess the prediction quality by means of the following measures mean absolute error mae root mean square error rmse index of agreement ia square of pearson s correlation coefficient r2 and band error both mae given in eq 7 and rmse given in eq 8 are absolute values smaller values of which indicate better prediction quality index of agreement given by eq 9 is a relative measure expressed in percentages for this type of error greater values indicate better prediction of the model in particular if ia is equal to 100 then for each training sample its predicted pollution value is the same as the real one pearson s correlation coefficient expresses the linear dependency between predicted and observed values in equations 7 10 n denotes the number of observations n 1000 in our experiments o i and p i are observed and predicted values respectively and o is the average of the observed values 7 m a e i 1 n o i p i n 8 r m s e i 1 n o i p i 2 n 9 i a 100 i 1 n o i p i 2 i 1 n p i o o i o 2 10 r 2 n i 1 n o i p i i 1 n o i i 1 n p i 2 i 1 n p i 2 i 1 n p i 2 n i 1 n o i 2 i 1 n o i 2 calculation of the band error is illustrated in fig 14 the range of values of a pollutant is divided into a given number of intervals in our experiments 5 intervals were used if the predicted and observed values fall into the same interval the prediction is reported as correct and 0 is returned as the band error otherwise the band error is calculated as one plus the number of intervals between the interval containing the predicted value and the interval containing the observed value 6 3 2 scatter plots of predicted vs observed pollution values in fig 15 we present scatter plots for the london bloomsbury station illustrating the observed and predicted values for ozone and pm10 pollution obtained using our ceesnn ensemble model similar scatter plots for london marylebone road monitoring site are shown in fig 16 and for london north kensington site in fig 17 for all scatter plots the overall dispersion of points around diagonal increases with the increasing prediction time for the london bloomsbury and london north kensington monitoring sites the scatter between observed values and values predicted with the ceesnn ensemble model is smaller for pm10 pollution than for ozone pollution so the air pollution quality prediction of ceesnn for these sites is better for pm10 than for ozone in the case of the london marylebone road monitoring site the results for both pollutants are comparable in figs 15 17 black continuous lines express linear dependency of the predicted values on the observed values the exact values of parameters of linear functions are given as well in these figures 6 3 3 quality prediction comparison values of four error indicators mae rmse ia and r2 were calculated for the proposed ceesnn model as well as for the plain neucube model the mlp network and the arima model and are shown in figs 18 and 19 for both ozone and pm10 pollution the ceesnn ensemble model outperforms the plain neucube model with respect to all these four quality measures additionally it can be noticed that the prediction results obtained with mlp network are always worse than the results obtained from other three used models based on the obtained results we conclude that while the arima model outperforms all other models for 1 h ahead prediction our ceesnn ensemble model gives better results for 3 and 6 h ahead predictions as follows from fig 20 for both ozone and pm10 pollution prediction the ceesnn model always provides larger number of correct predictions than the plain neucube model the total number of correct predictions is greater for ceesnn for 1 3 and 6 h ahead and for each monitoring site we will now compare the band error for the two best models ceesnn and arima in table 10 we show detailed results of ozone prediction for london bloomsbury london marylebone road and london north kensington monitoring sites it can be noticed that band 2 error and band 3 error occur very rarely band 4 error never occurs for any of the used models the overall correct number of predictions decreases with the increasing prediction time in particular for the london bloomsbury monitoring site the percentages of correct predictions for the ceesnn model for 1 3 and 6 h ahead forecasting are 77 4 76 7 and 71 0 respectively whereas for the arima model the respective percentages are 87 8 74 0 and 62 6 similarly for the london marylebone road monitoring site the percentages of correct predictions for 1 3 and 6 h are 80 3 76 9 and 77 5 respectively for the ceesnn model and the respective percentages for the arima model are 88 3 77 5 71 2 for the london north kensington site the percentages of correct predictions for 1 3 and 6 h for ceesnn model are 80 1 75 9 72 2 respectively while for the arima model the respective percentages are 89 6 73 4 and 64 3 the band errors for pm10 pollution are presented in table 11 as follows from this table for the london bloomsbury monitoring site the percentages of correct predictions for 1 3 and 6 h are very good for both models and are 96 2 96 3 94 9 respectively for the ceesnn model and 98 9 96 1 94 6 for the arima model for the london marylebone road monitoring site the percentages of correct predictions for 1 3 and 6 h are 83 8 81 7 79 6 respectively for the ceesnn model and 87 6 81 0 and 70 1 for the arima model for the london north kensington monitoring site the percentages of correct predictions for 1 3 and 6 h are 93 9 93 0 93 0 respectively for the ceesnn model and 96 7 94 2 and 88 6 for the arima model in fig 20 we show the bar plots illustrating correct predictions number versus total band error for both selected pollutants and all three monitoring sites 7 conclusions and future work in the article we proposed a new model clustering based ensemble of evolving spiking neural networks ceesnn for air pollution prediction in the proposed model each esnn of the ensemble is trained on a separate set of time series in our approach we proposed to generate training sets by clustering an initial set of pollution level time series each obtained cluster of time series is used to build a single esnn network as a result the ensemble prediction model consists of as many esnn networks as the number of created time series clusters in our research esnn networks were implemented as instances of neucube which is a neuromorphic tool for processing spatio temporal data the distinctive novelty in our approach is application of clustering of pollution level time series in order to get appropriate sets of training time series samples to be used for training esnn networks in the experimental part of the paper we forecasted ozone and pm10 pollution for greater london area based on data from three monitoring sites located there the pollution forceasting was carried out for 1 3 and 6 h ahead the quality of the prediction was measured by means of mean absolute error root mean square error index of agreement person s correlation coefficient and band error for comparative purposes the experimental evaluation was performed for both the proposed ceesnn ensemble prediction model as well as plain neucube model in which only a single essn network is used and two other models used in recent studies for air pollution prediction a multilayer perceptron and autoregressive integrated moving average model the ceesnn ensemble model turned out to be the predominant winner in the case of forecasting for 3 and 6 h ahead in addition the quality prediction of the ceesnn turned out significantly better than the plain neucube model for all experimental scenarios in the future studies we intend to concentrate on adapting a moving window technique to the proposed model for long time prediction of pollution levels acknowledgements this work was supported by eu project pacific atlantic network for technical higher education and research panther grant number 2013 5659 004 001 ema2 and by the institute of computer science of warsaw university of technology grant no ii 2018 ds 1 the authors are grateful to dr maryam gholami doborjeh and dr elisa capecci from knowledge engineering and discovery research institute auckland university of technology for providing us with the matlab implementation of neucube 
26209,a major challenge in environmental modeling is to identify structural changes in the ecosystem across time i e changes in the underlying process that generates the data in this paper we analyze the baltic sea food web in order to 1 examine potential unobserved processes that could affect the ecosystem and 2 make predictions on some variables of interest to do so dynamic bayesian networks with different setups of hidden variables hvs were built and validated applying two techniques rolling origin and rolling window moreover two statistical inference approaches were compared at regime shift detection fully bayesian and maximum likelihood estimation our results confirm that from the predictive accuracy point of view more data help to improve the predictions whereas the different setups of hvs did not make a critical difference in the predictions finally the different hvs picked up patterns in the data which revealed changes in different parts of the ecosystem keywords baltic sea ecosystem model model comparison regime shift structural change hidden variable 1 introduction ecosystems are constantly changing in response to both gradual and abrupt natural and human induced changes such as changes in climate land use or new species arrival in some cases these major changes can lead to abrupt shifts i e regime shifts affecting the structure and function of ecosystem dynamics scheffer et al 2001 conversi et al 2014 that are often costly and hard to reverse selkoe et al 2015 for instance the central baltic sea has undergone at least two regime shifts one was induced by constant nutrient loading from land causing a change from an oligotrophic to a eutrophic state in the 1960s resulting in harmful algal blooms and anoxic bottoms österblom et al 2007 another regime shift occurred in the late 1980s alheit et al 2005 möllmann et al 2009 and was induced by overfishing of cod gadus morhua and climate causing a shift toward a sprat sprattus sprattus dominated state möllmann et al 2009 which affected the magnitude of food web processes yletyinen et al 2016 the latter regime shift shows a clear indication of a hysteresis effect as even after the reduction of cod fishing the cod biomass could not be recovered blenckner et al 2015 casini et al 2016 such non stationary changes in ecosystem dynamics pose a challenge to ecosystem modelers and data analysts since it may be that the functional forms describing the relationships between the variables change bayesian networks bns which belong to the probabilistic graphical models are compact representations of the joint probability distribution over a set of variables whose independence relations are encoded by the structure of an underlying directed acyclic graph pearl 1988 since bns do not explicitly model changes over time a dynamic approach could represent more realistic modeling uusitalo 2007 dynamic bns extend the concept of bn by explicitly modeling change over time i e they allow the representation of the relationship between variables at successive time steps korb and nicholson 2011 it is normally assumed that the model structure is the same in each time step and the parameters over time do not change i e the model is assumed to be time invariant however it is possible to add hidden nodes to represent non stationary processes murphy 2012 hidden variables are unobserved variables that might represent relevant processes in the system that can help explain some observed variables of interest trifonova et al 2015 generally speaking the observed variables are not the only ones that affect the system i e there are a number of unobserved variables and processes that could have an effect but have not been identified or no data are available uusitalo et al 2018 the value of the hvs can be inferred from the observed variables linked to them so that a change in the hv pattern reflects a change in the system therefore dynamic models with hidden variables are one way of trying to find the signal of change amongst the multiple ecosystem variables and their interactions uusitalo et al 2018 trifonova et al 2017 the work by uusitalo et al 2018 evaluated the potential of dbns with hidden variables in the regime shift analysis on the baltic sea food web by linking different configurations of hidden variables to the core structure of the dbn in their study they built three versions of the model which differed in the hidden variable setup with the core structure being the same in order to analyze the pattern of the different hvs this work allowed to investigate whether or not the pattern of the hvs reflecting specific parts of the system such as the fish dynamics could be separated from the global ecosystem dynamic they found out that the different model setups showed the same general patterns they discussed the relative scarcity of data but did not assess how much data is needed in order to implement this kind of model or whether some setup of hvs performs better predictions than others our new study extends this work by answering the aforementioned questions and considering new configurations of hidden variables on the one hand we analyzed the amount of data needed to discover the hv pattern by fitting the models multiple times with an increasing amount of data in addition we compared the ability of two different statistical inference approaches to detect changes in the ecosystem bayesian and maximum likelihood estimation on the other hand we explored the predictive power of our expert based structure models and compared the results with the ones obtained by a fixed structure model moreover we examined the model s predictive performance across time by i increasing the sample size in order to detect if any of the models constantly outperforms the others and ii discarding old data as new information is available in order to determine either if the variations in the model s performance are due to the amount of data available or if some parts of the time series are easier to predict than others 2 material and methods 2 1 bayesian networks and dynamic bayesian networks a bayesian network bn is a statistical multivariate model for a set of variables x x 1 x n which is defined in terms of two components qualitative component a directed acyclic graph dag where each vertex represents one of the variables in the model and so that the presence of an edge linking two variables indicates the existence of statistical dependence between them quantitative component a conditional distribution p x i p a x i for each variable x i i 1 n given its parents in the graph denoted as p a x i the joint distribution of the variables in the network is therefore represented in a factorized way as 1 p x i 1 n p x i p a x i x 1 x n ω x 1 x n where x x 1 x n ω x i represents the set of all possible values of variable x i and p a x i denotes an instantiation of the parents of x i fig 1 shows an example of a dag and its joint probability distribution dynamic bayesian networks dbn extend the concept of bns by relating variables across time dbns are defined as a pair b 1 b 2 t murphy 2002 where b 1 is a classical bn representing the first time slice t 0 and b 2 t represents the transition model i e a two slice dbn for t 0 therefore the joint probability distribution of t 0 is the same as equation 1 whereas the joint probability distribution of the following time slices t 0 is 2 p x t x t 1 i 1 n p x i t p a x i t where x i t is the i t h node at time t and p a x i t are the parents of x i t in the graph the parents p a x i t of a node x i t can either be in the same time slice or in the previous one assuming a first order markov process we are making two other assumptions 1 x t 1 x t 1 x t the markov property i e the future is independent of the past given the present and 2 the transition processes are time invariant i e the transition functions are the same for all time slices fig 2 shows an example of unrolled dbn 2 2 parameter learning the dataset used in this paper contains only continuous variables which were parameterized using linear gaussian distributions parameters can be learned from data using the expectation maximization em algorithm lauritzen 1995 the goal of the em algorithm is to find the maximum likelihood estimate of the model parameters when the data have missing values the em algorithm iteratively finds e step and maximizes m step a current approximation to the log likelihood function of the parameters of a model liu 1997 the algorithm must be initialized with an initial value of the parameters in this work the initial random values of the parameters were drawn from the standard normal distribution n 0 1 then the e step computes the expected sufficient statistics mean and variance using the current parameter values and the observed data afterward the m step maximizes the log likelihood of the parameters given the sufficient statistics obtaining an updated value of the parameter estimate these two steps iterate until convergence in our case until the difference between consecutive log likelihoods was small enough since the em algorithm can get stuck in a local optimum it was run 100 times for each model retaining the model with the highest log likelihood value for further analyses 2 3 the baltic sea food web model 2 3 1 data description the data originate from the gotland basin in the central baltic sea fig 3 covering the 38 year period from 1975 to 2012 table 1 they were obtained from different sources the fish data are derived from the fish stock assessment model called virtual population analysis hilborn and walters 1992 tuned using the extended survival analysis xsa method darby et al 1994 this model uses both fish catch and fish survey data from multiple years as input data and fits an age structured fish stock model that accounts for the amount and mean weight of fish in each year class it estimates the total biomass of the spawning stock as well as the number of fish in the next year class recruitment the reproductive volume of cod i e the volume of water in which the environmental factors in particular salinity and oxygen concentration are such that cod eggs survive is based on a spatial interpolation model köster et al 2016 water temperature zooplankton and chlorophyll variables are based on direct measurements taken during field sampling campaigns the data are highly variable both temporally and spatially and therefore the data tend to be noisy as the observation may vary considerably depending on the sampling date and the exact location of the sampling in order to avoid numerical instability the data were log transformed and standardized to mean 0 and standard deviation 1 2 3 2 expert knowledge based structure bns can be used to encode expert knowledge over a domain pearl 1986 uusitalo et al 2005 the graphical representation provided by bns makes them a transparent tool which is especially useful when expert elicitation is required landuyt et al 2013 aguilera et al 2011 a gotland basin food web model structure was built based on expert elicitation fig 4a including the key components of the food web and their interactions some of the variables in this model are only linked to variables in the contiguous time slices which is essential to transmit the temporal dynamic five non observed variables representing juvenile fish stages were included to act as placeholders of the fish stage from birth to maturation these unobserved variables include 0 year old herring sprat and cod and one year old and three year old cod herring and sprat mature and join the spawning stock at the age two and cod at age four report of the balti 2013 the dynamic model is defined in one year time steps the fish related variables have temporal dependencies across time steps the spawning stock sizes ssb of the three fish species are autoregressive as they consist of individual fish that live for multiple years and therefore their value in one time slice depends on their value in the previous one the juvenile fish are modeled separately they exhibit temporal dependency so that the k year old fish k 0 3 for cod and k 0 1 for herring and sprat are k 1 years old in the following time slice until they reach maturation and join the spawning stock ssb the remaining variables are assumed not to directly depend on variables in the preceding year though they may have temporal autocorrelation due to the fact that variables affecting them are temporally autocorrelated the variables described above form the core structure of the expert knowledge based model structure fig 4a however it is clear that there are other variables and processes that could affect the ecosystem dynamics but have not been identified or no data are available for this reason models with different configurations of generic hidden variables hvs were built a model with no general hvs m0 fig 4a a model with one hv m1 genhv linked to all other variables in each time slice and to itself in the consecutive time slices fig 4b a model with two semi generic hvs m2 one linked to all fish related variables fishv and another linked to all zooplankton related variables zoohv in each time slice and to themselves in the consecutive time slices fig 4c and a model m2r including fishv and zoohv fig 4d as in m2 but reversing the links between the variables ssbher and ssbspr and the zooplankton variables this was done for food web reasons energy flows from phytoplankton to zooplankton to fish and for computer science reasons in model m2 variables ssbher and ssbspr and hidden variable zoohv are conditionally dependent given the zooplankton variables due to the v structure by reversing the links they are conditionally independent given the zooplankton variables 2 3 3 naive bayes structure in order to explore how well a simple approach would perform we built a naive bayes model with a hv as the class naive bayes nb is a bn with a fixed structure in which one variable the class c is the parent of all remaining variables x 1 x n which are independent to each other given c the strong assumption of independence behind nb models is somehow compensated by the reduction on the number of parameters to be estimated from data since in this case it holds that 3 p c x 1 x n p c i 1 n p x i c which means that instead of one n dimensional conditional density n one dimensional conditional densities have to be estimated variables included in table 1 as well as a generic hidden variable g e n h v were used to build the model the hv takes the place of the class which is the only autoregressive variable in the model fig 5 the dynamic model is defined in one year time steps where the links between time slices only correspond to the autoregressive hidden variable g e n h v in what follows we derive the explicit equation of the expected value of the hv for model nb the equations for the rest of the models are derived in a similar way for short let s denote the variables in model nb see fig 5 as c genhv x 1 fher x 2 fspr x 3 fcod x 4 ssbher x 5 ssbspr x 6 ssbcod x 7 her1y x 8 spr1y x 9 cod2y x 10 tem x 11 ac x 12 ps x 13 rv x 14 tspring x 15 chla x 16 tsum let x t x 1 t x 16 t denote the observed variables at time step t let x t x 1 t x 16 t be any of the possible configurations of the observed variables at time step t our goal is to model the expected value of the hidden variable at time step t given all the previous observations i e 4 e c t x 1 x t c t p c t x 1 x t d c t where 5 p c t x 1 x t p c 1 c t x 1 x t d c 1 d c t 1 1 z p x 1 x t c 1 c t p c 1 c t d c 1 d c t 1 1 z p c 1 i 1 t p x i c i p c i c i 1 d c 1 d c t 1 1 z p c 1 i 1 t p c i c i 1 j 1 16 p x j i c i d c 1 d c t 1 with z being a normalization constant equal to p x 1 x t each density p c i c i 1 is a gaussian density for c i with mean equal to a linear function of c i 1 and each p x j i c i is a gaussian density for x j i with a linear function of c i as mean 2 4 approach comparison of regime shift detection as an alternative to the already proposed method to detect unobserved processes we also followed a fully bayesian approach i e including the parameters as random variables and then updating the model by making probabilistic inference belief update for each record in the dataset the goal of this comparison was to explore the ability of these two approaches at detecting changes in the ecosystem in order to build a fully bayesian model we used the naive bayes concept drift detector nbcd algorithm borchani et al 2015 from the r package ramidst which is an r interface to the amidst toolbox masegosa et al 2019 written in java the nbcd relies on the variational bayes framework a class of approximation methods for the inference and learning tasks and uses the nb structure as the base model with a variable of interest being the class and the remaining being predictive variables then a hidden variable h is added and linked to the predictive variables in the model the specific inference method used in our experiments was the streaming variational bayes svb algorithm broderick et al 2013 on the other hand we built a model with the same dag structure as the nbcd but using the em algorithm to learn the parameters as implemented in the learn params dbn em function of the bayes net toolbox in matlab since we are interested in six variables ssbcod ssbspr ssbher cod2y spr1y and her1y we built six models using the nbcd algorithm and six using the em algorithm fig 6 shows the dag used to build the model with her1y as the class variable note that the dag is analogous for the remaining variables of interest we will analyze the hidden variable h for each model for the sake of clarity we will refer to this hidden variable as h e m if it was learned using the em algorithm or h b if it was learned following the bayesian approach in the case of no ambiguity or no need to distinguish between them h will be used instead in all the models described here and in the previous sections the autocorrelated hidden variables try to capture the evolution of the model uncertainty over time furthermore the bayesian approach used in model nbcd takes into account the possible lack of independence in the data to some extent by updating the parameters each time a new data item arrives in other words the posterior distribution in a time step becomes the prior distribution in the next time step notice however that we only consider discrete time steps as can be seen in equation 4 finer granularity can be achieved by adopting a continuous time approach reichert and mieleitner 2009 in which time becomes a variable into the model equations 2 5 model validation in order to validate the predictive performance of the aforementioned models five step ahead predictions were carried out two cross validation cv techniques to train and test the models were used rolling origin and rolling window bergmeir and benítez 2012 in the first one the sample size increases in each fold with respect to the origin last observation of the training set fig 7a i e the data from the test set move to the train set sequentially and the model is recalibrated in the second one the sample size is kept constant in each fold except for the first one due to the total amount of data moving the training set as a window across time fig 7b i e data from the beginning of the time series are discarded as new data are available the reason for doing this was to explore whether the variation in the model s performance across time is due to the amount of data available or because some parts of the time series are easier to predict than others the goodness of the predictions was evaluated using the log likelihood of the observations given the predicted values we compared the log likelihoods of the pairs rolling origin rolling window of each model version using the wilcoxon signed rank test moreover for the rolling origin case the log likelihoods of the five year prediction of different sets of models m0 m1 m2 m2r nb were compared using either wilcoxon signed rank test for two groups or friedman test with maxt statistic hothorn et al 2008 for more than two groups in the case of applying friedman test in those cases where significant differences were found we deployed wilcoxon nemenyi mcdonald thompson s post hoc test hollander and wolfe 1999 3 results 3 1 regime shift detection models with different configurations of hvs were built in order to detect processes that could affect different parts of the system in particular three hvs were explored a general hv genhv a fish hv fishv and a zooplankton hv zoohv to analyze the hv patterns we fit the models following the rolling origin approach as shown in fig 7a with the difference that for each fold both train and test sets were used to learn the models fig 8 shows the evolution of the expected value of the hvs in each model m1 m2 m2r and nb for each fold computed as explained in equation 4 the three hvs genhv fishv zoohv showed different patterns among them and similar to themselves except for zoohv in the other models i e genhv shows similar behavior in models m1 and nb and so does fishv in models m2 and m2r regarding the last fold in which the complete dataset is used to fit the models genhv shows a decrease at the very beginning of the time series in both models m1 and nb followed by an increase from the period 1981 1991 point at which they stabilize till the end of the time series when they start decreasing again on the other hand fishv decreases at the beginning of the time series then increases from 1978 to 1998 and finally decreases to its first values of special interest is variable zoohv which shows a different pattern in models m2 and m2r with the former having a zigzag trend and higher variance and the latter showing two stable periods separated by an abrupt increase from the mid 1980s till 1990s the difference between zoohv m2 and zoohv m2r is a consequence of reversing the links between the zooplankton variables tem ac and ps which will be referred to as z and the ssbher and ssbspr variables which will be referred to as s by inverting these links we transformed the v structure involving these variables i e s z zoohv to a serial connection i e s z zoohv so that the s variables and zoohv are conditionally independent given z in other words given z new information about s does not influence the zoohv variable on the other hand variables tspring and chla are also involved in the v structure therefore given any of the variables in z is observed tspring chla and zoohv become conditionally dependent in both m2 and m2r models since the size of the training set is increased by five in each fold it is not surprising that the first and last folds show very different patterns the fold at which the pattern of the hv is revealed depends on the specific hv for instance the abrupt increase of variable zoohv model m2r can be seen from fold 2 onwards i e with sample size n 13 whereas the stabilization phase can be observed from fold 3 on the other hand variable genhv model m1 and nb takes a bit longer to reveal its final pattern which can be seen from fold 5 onwards finally variable fishv has a similar pattern to genhv until fold 5 unlike genhv variable fishv experiments a drastic decrease from fold 5 to 6 it appears that variable genhv could be capturing both fishv and zoohv trends combining them in a smoother pattern 3 1 1 approach comparison of regime shift the comparison of regime shift detection between the em algorithm and the fully bayesian approach can be seen in fig 9 each individual plot shows the observed variable of interest i e the class variable in the model and the expected value of the hidden variable h following the em h e m and bayesian h b approaches in general the bayesian approach picks up the changes in the observed variables at the end of the time series the main reason for this peculiar performance is the lack of data it should be kept in mind that our dataset consists of only 38 data points i e what we see at the beginning of the time series is the flat prior which rules out the data likelihood until sufficient data is explored on the other hand the em approach captures the trend of the observed variables earlier than the bayesian it can be seen that h e m picks up the pattern of the observed variable acting as the class even when they are noisy as in the case of spr1y and her1y in both approaches a change can be seen near the year 2000 in all models which coincides to the abrupt change seen in variable fishv in models m2 and m2r fig 8b and c when the entire dataset is used to train the models fold 6 moreover for models where spr1y and her1y are the class variable h e m shows a change near the year 1980 which coincides to the first drift identified in variable genhv in models m1 and nb fig 8a and d when the entire dataset is used to train the models fold 6 3 2 predictive performance the time series of the observed and predicted values for each variable of interest model and fold of both cross validation techniques are shown in fig 14a f in appendix a from these predictions we computed the performance of each model for each variable of interest in terms of its log likelihood fig 10b furthermore for the rolling origin case only we performed some hypothesis testing which will be further discussed in the upcoming sections 3 2 1 does the increase of data improve the performance of the models in order to detect whether or not more data help to predict the variables of interest two cross validation techniques were used to build the models rolling origin ro and rolling window rw fig 10b shows the performance of each model for each variable of interest in terms of log likelihood the results show that most variables get improvements in their predictions when more data are available to fit the models p value 0 05 i e when the rolling origin technique is used table 2 shows an overview of the hypothesis testing results in particular variable ssbcod fig 14a showed significant differences in all models except the m2 model with ro performing better than rw in all significant cases variable ssbspr fig 14b showed significant differences in 3 out of 5 models with ro performing better than rw in models m0 and nb and rw performing better in model m2r variable ssbher fig 14c showed significant differences in all models except the m0 model with ro performing better than rw in 3 out of the 4 significant cases rw performed better in the m2r model variable spr1y fig 14d showed significant differences only in the naive bayes nb model with ro performing better than rw variable her1y fig 14e did not show significant differences in any model and variable cod2y fig 14f showed significant differences in all models except the nb with ro performing better than rw in all significant cases 3 2 2 does the introduction of hvs improve the performance of the models in order to find out whether or not the introduction of hidden variables improves the predictive performance we compared the loglikelihood of the predictions of models m0 m1 and m2 with m0 being the model without hidden variables m1 the model with one generic hidden variable and m2 the model with two hidden variables we used the friedman test to carry out the comparison fig 11 shows the boxplot of the differences between pairs of models boxplots outlined with color orange indicate that significant differences were found between the corresponding pair of models the results of the tests show significant differences only for variables ssbher and cod2y with models m1 and m2 outperforming model m0 in the case of cod2y whereas model m2 outperformed model m1 in the case of ssbher 3 2 3 does reversing some links improve the performance of the models in order to test whether or not reversing the links between variables ssbher and ssbspr and the zooplankton variables would make any difference in the models prediction we compared the loglikelihoods of the predictions of models m2 and m2r using the wilcoxon signed rank test fig 12 shows the boxplots of the differences between the pairs of models for each variable of interest the results of the tests show significant differences only for variables ssbher and spr1y with model m2r outperforming model m2 in both cases 3 2 4 does the use of expert based structures improve the performance of the models in order to test whether or not expert based structures outperform simpler models such as naive bayes we compared each expert based model m0 m1 m2 and m2r with the naive bayes nb model using the wilcoxon signed rank test fig 13 shows the boxplots of the differences between the pairs of models for each variable of interest the results of the tests show significant differences for models ssbspr spr1y her1y and cod2y in most cases nb outperformed the expert based models on the other hand for variable cod2y the expert based models outperformed the nb model except in the case of the pair m0 nb where no significant differences were found 4 discussion in this paper we have analyzed the baltic sea food web aiming at 1 detecting changes in its pattern and 2 making reliable predictions on some variables of interest the hidden variables called genhv and fishv models m1 m2 m2r and nb showed a clear increase near the year 1980 with the former occurring slightly later variable zoohv model m2r shows an abrupt jump which is delayed in comparison to the increase of the two other hvs former studies have found that the baltic sea has undergone several periods of change with one of them being described in the 1980s shannon et al 2009 many publications have also described a regime shift in the north sea in the late 80s beaugrand 2004 weijerman et al 2005 deyoung et al 2008 dippner et al 2012 which is connected to the baltic sea through the danish straits where water exchange between the two areas occurs according to the suggestion of some authors this regime shift may have been partly caused as a response to a change of the north atlantic oscillation nao in winter alheit et al 2005 which led to an increase in the winter and early spring air and water temperature nao induced changes along with overfishing of cod triggered a regime shift shannon et al 2009 leading to a period dominated by clupeids i e sprat and herring österblom et al 2007 after 1990 the regime shift described in the literature coincides with the changes observed in variable zoohv in model m2r this is in in line with the research by alheit et al 2005 where they describe that the abundance of zooplankton varied in accordance with the variations in the nao therefore it seems reasonable that the model reflects these variations in the nao on the other hand variable fishv in model m2 and m2r shows an increase from the beginning of the 1980s till the beginning of the 2000s when its tendency drastically changes towards a decreasing trend this hidden variable is linked to 14 fish related variables therefore the hidden variable is trying to capture the global trend of all these variables the fishv could be partly reflecting the indirect change of zooplankton as they partly predate on those and partly the change in the large anoxic area carstensen et al 2014 along with the clupeid dominated period variable genhv models m1 an nb shows a gradual increase from the beginning of the 1980s till 1990 where a constant period begins the behavior of this hidden variable could be reflecting the underlying dynamic of the zooplankton and fish variables as a whole but more research is needed from the predictive accuracy point of view significant differences among the models were scarcely found in particular the similar results for the expert based models could be regarded as a positive finding in the sense that as long as the model structure is coherent the details will not make a critical difference in the predictions when significant differences are found our results suggest that models with a higher number of hidden variables are not outperformed section 3 2 2 moreover reversing some links increased the predictive accuracy of two variables only with the remaining variables not showing statistically significant differences section 3 2 3 finally the use of simple models such as the naive bayes helped with the prediction of some variables compared to more complex models based on expert elicitation section 3 2 4 the reason for that is that simple models need to estimate fewer parameters which is very convenient when few data are available on the other hand improvements were found rather often when comparing the rolling origin and rolling window approaches in most of the significant cases having more data improved the prediction of the outcomes 5 conclusions the methodology proposed in this paper managed to identify a major regime shift that occurred in the baltic sea during the 1980s the dbn methodology can identify changes in ecosystems where only limited data are available which is an additional difficulty to cope with not surprisingly the increase in data size improved the models predictions some systems understanding is needed to construct the model structure but the present experiments show that the exact model setup does not make a critical difference to the results from the predictive accuracy point of view however the use of different hv setups helped reveal changes in the different parts of the ecosystem therefore the presented approach can be a highly useful tool in the study of potentially critical changes in complex ecological interactions acknowledgments this work has been partly supported by the spanish ministry of economy and competitiveness through project tin2016 77902 c3 3 p in addition this work has partially resulted from the bonus bluewebs project which has received funding from bonus art 185 funded jointly by the eu the academy of finland projektträger jülich ptj germany the state education development agency of latvia the national centre for research and development poland and the swedish research council formas a d maldonado was supported by the spanish ministry of education culture and sport through an fpu research grant fpu2013 00547 and a research visit grant est16 00723 appendix a model predictions the time series of the observed and predicted values for each variable of interest model and fold of both cross validation techniques are shown in fig 14a f from these predictions we computed the performance of each model in terms of log likelihood for each variable of interest fig 10b furthermore for the rolling origin case only we performed some hypothesis testing to compare the predictive performance of the different models figs 11 13 fig 14 model predictions expected value and observed values used to fit the models following the rolling origin ro and rolling window rw cross validation techniques as shown in fig 7 black dark gray and light gray lines correspond to data belonging to the train set of the rw approach train set of the ro approach and test set respectively red and blue lines correspond to the predictions obtained following the rw and ro techniques respectively the shaded bands correspond to the standard deviation fig 14 appendix b code the code necessary to run the experiments is available as supplementary material in particular supplementary material contains two m files with the matlab code one tar gz file with the ramidst r package and four files containing simulated data three csv and one arff as the real dataset used in this work is protected and cannot be published we provide simulated data to run the code most of the experiments carried out in this work were performed using the bayes net toolbox bnt package in matlab we have created a wrapper function dbn foodwebmodel to reproduce our experiments we provide some examples of how to use this function for the simulated dataset the results of running this function are stored in txt files on the one hand the expected value of the hidden variables can be obtained by running the following piece of code image 2 on the other hand the predictions of the target variables can be obtained running the following piece of code image 3 note that the given examples provide the predictions for the rolling origin case to obtain the results for the rolling window approach the argument cv must be changed from rol org to rol wind also note that it is not necessary to specify which are the target variables since the function is specifically programmed to return the values of these variables nevertheless this function can be easily adapted to other datasets and model structures to compare the variational bayes and the em algorithms for the regime shift detection we used the ramidst r package and the bnt matlab package we have created another wrapper function nbcdd for the experiments carried out in matlab an example of how to use this function is provided to illustrate the example variable cod2y is used as the class variable note that we computed the expected value of the hidden variable h for 6 different models i e using 6 different class variables image 4 an example of how to use the nb concept drift detector from stream function of ramidst r package is also provided image 5 appendix c supplementary data the following is the supplementary data to this article code spl data code spl data appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 011 
26209,a major challenge in environmental modeling is to identify structural changes in the ecosystem across time i e changes in the underlying process that generates the data in this paper we analyze the baltic sea food web in order to 1 examine potential unobserved processes that could affect the ecosystem and 2 make predictions on some variables of interest to do so dynamic bayesian networks with different setups of hidden variables hvs were built and validated applying two techniques rolling origin and rolling window moreover two statistical inference approaches were compared at regime shift detection fully bayesian and maximum likelihood estimation our results confirm that from the predictive accuracy point of view more data help to improve the predictions whereas the different setups of hvs did not make a critical difference in the predictions finally the different hvs picked up patterns in the data which revealed changes in different parts of the ecosystem keywords baltic sea ecosystem model model comparison regime shift structural change hidden variable 1 introduction ecosystems are constantly changing in response to both gradual and abrupt natural and human induced changes such as changes in climate land use or new species arrival in some cases these major changes can lead to abrupt shifts i e regime shifts affecting the structure and function of ecosystem dynamics scheffer et al 2001 conversi et al 2014 that are often costly and hard to reverse selkoe et al 2015 for instance the central baltic sea has undergone at least two regime shifts one was induced by constant nutrient loading from land causing a change from an oligotrophic to a eutrophic state in the 1960s resulting in harmful algal blooms and anoxic bottoms österblom et al 2007 another regime shift occurred in the late 1980s alheit et al 2005 möllmann et al 2009 and was induced by overfishing of cod gadus morhua and climate causing a shift toward a sprat sprattus sprattus dominated state möllmann et al 2009 which affected the magnitude of food web processes yletyinen et al 2016 the latter regime shift shows a clear indication of a hysteresis effect as even after the reduction of cod fishing the cod biomass could not be recovered blenckner et al 2015 casini et al 2016 such non stationary changes in ecosystem dynamics pose a challenge to ecosystem modelers and data analysts since it may be that the functional forms describing the relationships between the variables change bayesian networks bns which belong to the probabilistic graphical models are compact representations of the joint probability distribution over a set of variables whose independence relations are encoded by the structure of an underlying directed acyclic graph pearl 1988 since bns do not explicitly model changes over time a dynamic approach could represent more realistic modeling uusitalo 2007 dynamic bns extend the concept of bn by explicitly modeling change over time i e they allow the representation of the relationship between variables at successive time steps korb and nicholson 2011 it is normally assumed that the model structure is the same in each time step and the parameters over time do not change i e the model is assumed to be time invariant however it is possible to add hidden nodes to represent non stationary processes murphy 2012 hidden variables are unobserved variables that might represent relevant processes in the system that can help explain some observed variables of interest trifonova et al 2015 generally speaking the observed variables are not the only ones that affect the system i e there are a number of unobserved variables and processes that could have an effect but have not been identified or no data are available uusitalo et al 2018 the value of the hvs can be inferred from the observed variables linked to them so that a change in the hv pattern reflects a change in the system therefore dynamic models with hidden variables are one way of trying to find the signal of change amongst the multiple ecosystem variables and their interactions uusitalo et al 2018 trifonova et al 2017 the work by uusitalo et al 2018 evaluated the potential of dbns with hidden variables in the regime shift analysis on the baltic sea food web by linking different configurations of hidden variables to the core structure of the dbn in their study they built three versions of the model which differed in the hidden variable setup with the core structure being the same in order to analyze the pattern of the different hvs this work allowed to investigate whether or not the pattern of the hvs reflecting specific parts of the system such as the fish dynamics could be separated from the global ecosystem dynamic they found out that the different model setups showed the same general patterns they discussed the relative scarcity of data but did not assess how much data is needed in order to implement this kind of model or whether some setup of hvs performs better predictions than others our new study extends this work by answering the aforementioned questions and considering new configurations of hidden variables on the one hand we analyzed the amount of data needed to discover the hv pattern by fitting the models multiple times with an increasing amount of data in addition we compared the ability of two different statistical inference approaches to detect changes in the ecosystem bayesian and maximum likelihood estimation on the other hand we explored the predictive power of our expert based structure models and compared the results with the ones obtained by a fixed structure model moreover we examined the model s predictive performance across time by i increasing the sample size in order to detect if any of the models constantly outperforms the others and ii discarding old data as new information is available in order to determine either if the variations in the model s performance are due to the amount of data available or if some parts of the time series are easier to predict than others 2 material and methods 2 1 bayesian networks and dynamic bayesian networks a bayesian network bn is a statistical multivariate model for a set of variables x x 1 x n which is defined in terms of two components qualitative component a directed acyclic graph dag where each vertex represents one of the variables in the model and so that the presence of an edge linking two variables indicates the existence of statistical dependence between them quantitative component a conditional distribution p x i p a x i for each variable x i i 1 n given its parents in the graph denoted as p a x i the joint distribution of the variables in the network is therefore represented in a factorized way as 1 p x i 1 n p x i p a x i x 1 x n ω x 1 x n where x x 1 x n ω x i represents the set of all possible values of variable x i and p a x i denotes an instantiation of the parents of x i fig 1 shows an example of a dag and its joint probability distribution dynamic bayesian networks dbn extend the concept of bns by relating variables across time dbns are defined as a pair b 1 b 2 t murphy 2002 where b 1 is a classical bn representing the first time slice t 0 and b 2 t represents the transition model i e a two slice dbn for t 0 therefore the joint probability distribution of t 0 is the same as equation 1 whereas the joint probability distribution of the following time slices t 0 is 2 p x t x t 1 i 1 n p x i t p a x i t where x i t is the i t h node at time t and p a x i t are the parents of x i t in the graph the parents p a x i t of a node x i t can either be in the same time slice or in the previous one assuming a first order markov process we are making two other assumptions 1 x t 1 x t 1 x t the markov property i e the future is independent of the past given the present and 2 the transition processes are time invariant i e the transition functions are the same for all time slices fig 2 shows an example of unrolled dbn 2 2 parameter learning the dataset used in this paper contains only continuous variables which were parameterized using linear gaussian distributions parameters can be learned from data using the expectation maximization em algorithm lauritzen 1995 the goal of the em algorithm is to find the maximum likelihood estimate of the model parameters when the data have missing values the em algorithm iteratively finds e step and maximizes m step a current approximation to the log likelihood function of the parameters of a model liu 1997 the algorithm must be initialized with an initial value of the parameters in this work the initial random values of the parameters were drawn from the standard normal distribution n 0 1 then the e step computes the expected sufficient statistics mean and variance using the current parameter values and the observed data afterward the m step maximizes the log likelihood of the parameters given the sufficient statistics obtaining an updated value of the parameter estimate these two steps iterate until convergence in our case until the difference between consecutive log likelihoods was small enough since the em algorithm can get stuck in a local optimum it was run 100 times for each model retaining the model with the highest log likelihood value for further analyses 2 3 the baltic sea food web model 2 3 1 data description the data originate from the gotland basin in the central baltic sea fig 3 covering the 38 year period from 1975 to 2012 table 1 they were obtained from different sources the fish data are derived from the fish stock assessment model called virtual population analysis hilborn and walters 1992 tuned using the extended survival analysis xsa method darby et al 1994 this model uses both fish catch and fish survey data from multiple years as input data and fits an age structured fish stock model that accounts for the amount and mean weight of fish in each year class it estimates the total biomass of the spawning stock as well as the number of fish in the next year class recruitment the reproductive volume of cod i e the volume of water in which the environmental factors in particular salinity and oxygen concentration are such that cod eggs survive is based on a spatial interpolation model köster et al 2016 water temperature zooplankton and chlorophyll variables are based on direct measurements taken during field sampling campaigns the data are highly variable both temporally and spatially and therefore the data tend to be noisy as the observation may vary considerably depending on the sampling date and the exact location of the sampling in order to avoid numerical instability the data were log transformed and standardized to mean 0 and standard deviation 1 2 3 2 expert knowledge based structure bns can be used to encode expert knowledge over a domain pearl 1986 uusitalo et al 2005 the graphical representation provided by bns makes them a transparent tool which is especially useful when expert elicitation is required landuyt et al 2013 aguilera et al 2011 a gotland basin food web model structure was built based on expert elicitation fig 4a including the key components of the food web and their interactions some of the variables in this model are only linked to variables in the contiguous time slices which is essential to transmit the temporal dynamic five non observed variables representing juvenile fish stages were included to act as placeholders of the fish stage from birth to maturation these unobserved variables include 0 year old herring sprat and cod and one year old and three year old cod herring and sprat mature and join the spawning stock at the age two and cod at age four report of the balti 2013 the dynamic model is defined in one year time steps the fish related variables have temporal dependencies across time steps the spawning stock sizes ssb of the three fish species are autoregressive as they consist of individual fish that live for multiple years and therefore their value in one time slice depends on their value in the previous one the juvenile fish are modeled separately they exhibit temporal dependency so that the k year old fish k 0 3 for cod and k 0 1 for herring and sprat are k 1 years old in the following time slice until they reach maturation and join the spawning stock ssb the remaining variables are assumed not to directly depend on variables in the preceding year though they may have temporal autocorrelation due to the fact that variables affecting them are temporally autocorrelated the variables described above form the core structure of the expert knowledge based model structure fig 4a however it is clear that there are other variables and processes that could affect the ecosystem dynamics but have not been identified or no data are available for this reason models with different configurations of generic hidden variables hvs were built a model with no general hvs m0 fig 4a a model with one hv m1 genhv linked to all other variables in each time slice and to itself in the consecutive time slices fig 4b a model with two semi generic hvs m2 one linked to all fish related variables fishv and another linked to all zooplankton related variables zoohv in each time slice and to themselves in the consecutive time slices fig 4c and a model m2r including fishv and zoohv fig 4d as in m2 but reversing the links between the variables ssbher and ssbspr and the zooplankton variables this was done for food web reasons energy flows from phytoplankton to zooplankton to fish and for computer science reasons in model m2 variables ssbher and ssbspr and hidden variable zoohv are conditionally dependent given the zooplankton variables due to the v structure by reversing the links they are conditionally independent given the zooplankton variables 2 3 3 naive bayes structure in order to explore how well a simple approach would perform we built a naive bayes model with a hv as the class naive bayes nb is a bn with a fixed structure in which one variable the class c is the parent of all remaining variables x 1 x n which are independent to each other given c the strong assumption of independence behind nb models is somehow compensated by the reduction on the number of parameters to be estimated from data since in this case it holds that 3 p c x 1 x n p c i 1 n p x i c which means that instead of one n dimensional conditional density n one dimensional conditional densities have to be estimated variables included in table 1 as well as a generic hidden variable g e n h v were used to build the model the hv takes the place of the class which is the only autoregressive variable in the model fig 5 the dynamic model is defined in one year time steps where the links between time slices only correspond to the autoregressive hidden variable g e n h v in what follows we derive the explicit equation of the expected value of the hv for model nb the equations for the rest of the models are derived in a similar way for short let s denote the variables in model nb see fig 5 as c genhv x 1 fher x 2 fspr x 3 fcod x 4 ssbher x 5 ssbspr x 6 ssbcod x 7 her1y x 8 spr1y x 9 cod2y x 10 tem x 11 ac x 12 ps x 13 rv x 14 tspring x 15 chla x 16 tsum let x t x 1 t x 16 t denote the observed variables at time step t let x t x 1 t x 16 t be any of the possible configurations of the observed variables at time step t our goal is to model the expected value of the hidden variable at time step t given all the previous observations i e 4 e c t x 1 x t c t p c t x 1 x t d c t where 5 p c t x 1 x t p c 1 c t x 1 x t d c 1 d c t 1 1 z p x 1 x t c 1 c t p c 1 c t d c 1 d c t 1 1 z p c 1 i 1 t p x i c i p c i c i 1 d c 1 d c t 1 1 z p c 1 i 1 t p c i c i 1 j 1 16 p x j i c i d c 1 d c t 1 with z being a normalization constant equal to p x 1 x t each density p c i c i 1 is a gaussian density for c i with mean equal to a linear function of c i 1 and each p x j i c i is a gaussian density for x j i with a linear function of c i as mean 2 4 approach comparison of regime shift detection as an alternative to the already proposed method to detect unobserved processes we also followed a fully bayesian approach i e including the parameters as random variables and then updating the model by making probabilistic inference belief update for each record in the dataset the goal of this comparison was to explore the ability of these two approaches at detecting changes in the ecosystem in order to build a fully bayesian model we used the naive bayes concept drift detector nbcd algorithm borchani et al 2015 from the r package ramidst which is an r interface to the amidst toolbox masegosa et al 2019 written in java the nbcd relies on the variational bayes framework a class of approximation methods for the inference and learning tasks and uses the nb structure as the base model with a variable of interest being the class and the remaining being predictive variables then a hidden variable h is added and linked to the predictive variables in the model the specific inference method used in our experiments was the streaming variational bayes svb algorithm broderick et al 2013 on the other hand we built a model with the same dag structure as the nbcd but using the em algorithm to learn the parameters as implemented in the learn params dbn em function of the bayes net toolbox in matlab since we are interested in six variables ssbcod ssbspr ssbher cod2y spr1y and her1y we built six models using the nbcd algorithm and six using the em algorithm fig 6 shows the dag used to build the model with her1y as the class variable note that the dag is analogous for the remaining variables of interest we will analyze the hidden variable h for each model for the sake of clarity we will refer to this hidden variable as h e m if it was learned using the em algorithm or h b if it was learned following the bayesian approach in the case of no ambiguity or no need to distinguish between them h will be used instead in all the models described here and in the previous sections the autocorrelated hidden variables try to capture the evolution of the model uncertainty over time furthermore the bayesian approach used in model nbcd takes into account the possible lack of independence in the data to some extent by updating the parameters each time a new data item arrives in other words the posterior distribution in a time step becomes the prior distribution in the next time step notice however that we only consider discrete time steps as can be seen in equation 4 finer granularity can be achieved by adopting a continuous time approach reichert and mieleitner 2009 in which time becomes a variable into the model equations 2 5 model validation in order to validate the predictive performance of the aforementioned models five step ahead predictions were carried out two cross validation cv techniques to train and test the models were used rolling origin and rolling window bergmeir and benítez 2012 in the first one the sample size increases in each fold with respect to the origin last observation of the training set fig 7a i e the data from the test set move to the train set sequentially and the model is recalibrated in the second one the sample size is kept constant in each fold except for the first one due to the total amount of data moving the training set as a window across time fig 7b i e data from the beginning of the time series are discarded as new data are available the reason for doing this was to explore whether the variation in the model s performance across time is due to the amount of data available or because some parts of the time series are easier to predict than others the goodness of the predictions was evaluated using the log likelihood of the observations given the predicted values we compared the log likelihoods of the pairs rolling origin rolling window of each model version using the wilcoxon signed rank test moreover for the rolling origin case the log likelihoods of the five year prediction of different sets of models m0 m1 m2 m2r nb were compared using either wilcoxon signed rank test for two groups or friedman test with maxt statistic hothorn et al 2008 for more than two groups in the case of applying friedman test in those cases where significant differences were found we deployed wilcoxon nemenyi mcdonald thompson s post hoc test hollander and wolfe 1999 3 results 3 1 regime shift detection models with different configurations of hvs were built in order to detect processes that could affect different parts of the system in particular three hvs were explored a general hv genhv a fish hv fishv and a zooplankton hv zoohv to analyze the hv patterns we fit the models following the rolling origin approach as shown in fig 7a with the difference that for each fold both train and test sets were used to learn the models fig 8 shows the evolution of the expected value of the hvs in each model m1 m2 m2r and nb for each fold computed as explained in equation 4 the three hvs genhv fishv zoohv showed different patterns among them and similar to themselves except for zoohv in the other models i e genhv shows similar behavior in models m1 and nb and so does fishv in models m2 and m2r regarding the last fold in which the complete dataset is used to fit the models genhv shows a decrease at the very beginning of the time series in both models m1 and nb followed by an increase from the period 1981 1991 point at which they stabilize till the end of the time series when they start decreasing again on the other hand fishv decreases at the beginning of the time series then increases from 1978 to 1998 and finally decreases to its first values of special interest is variable zoohv which shows a different pattern in models m2 and m2r with the former having a zigzag trend and higher variance and the latter showing two stable periods separated by an abrupt increase from the mid 1980s till 1990s the difference between zoohv m2 and zoohv m2r is a consequence of reversing the links between the zooplankton variables tem ac and ps which will be referred to as z and the ssbher and ssbspr variables which will be referred to as s by inverting these links we transformed the v structure involving these variables i e s z zoohv to a serial connection i e s z zoohv so that the s variables and zoohv are conditionally independent given z in other words given z new information about s does not influence the zoohv variable on the other hand variables tspring and chla are also involved in the v structure therefore given any of the variables in z is observed tspring chla and zoohv become conditionally dependent in both m2 and m2r models since the size of the training set is increased by five in each fold it is not surprising that the first and last folds show very different patterns the fold at which the pattern of the hv is revealed depends on the specific hv for instance the abrupt increase of variable zoohv model m2r can be seen from fold 2 onwards i e with sample size n 13 whereas the stabilization phase can be observed from fold 3 on the other hand variable genhv model m1 and nb takes a bit longer to reveal its final pattern which can be seen from fold 5 onwards finally variable fishv has a similar pattern to genhv until fold 5 unlike genhv variable fishv experiments a drastic decrease from fold 5 to 6 it appears that variable genhv could be capturing both fishv and zoohv trends combining them in a smoother pattern 3 1 1 approach comparison of regime shift the comparison of regime shift detection between the em algorithm and the fully bayesian approach can be seen in fig 9 each individual plot shows the observed variable of interest i e the class variable in the model and the expected value of the hidden variable h following the em h e m and bayesian h b approaches in general the bayesian approach picks up the changes in the observed variables at the end of the time series the main reason for this peculiar performance is the lack of data it should be kept in mind that our dataset consists of only 38 data points i e what we see at the beginning of the time series is the flat prior which rules out the data likelihood until sufficient data is explored on the other hand the em approach captures the trend of the observed variables earlier than the bayesian it can be seen that h e m picks up the pattern of the observed variable acting as the class even when they are noisy as in the case of spr1y and her1y in both approaches a change can be seen near the year 2000 in all models which coincides to the abrupt change seen in variable fishv in models m2 and m2r fig 8b and c when the entire dataset is used to train the models fold 6 moreover for models where spr1y and her1y are the class variable h e m shows a change near the year 1980 which coincides to the first drift identified in variable genhv in models m1 and nb fig 8a and d when the entire dataset is used to train the models fold 6 3 2 predictive performance the time series of the observed and predicted values for each variable of interest model and fold of both cross validation techniques are shown in fig 14a f in appendix a from these predictions we computed the performance of each model for each variable of interest in terms of its log likelihood fig 10b furthermore for the rolling origin case only we performed some hypothesis testing which will be further discussed in the upcoming sections 3 2 1 does the increase of data improve the performance of the models in order to detect whether or not more data help to predict the variables of interest two cross validation techniques were used to build the models rolling origin ro and rolling window rw fig 10b shows the performance of each model for each variable of interest in terms of log likelihood the results show that most variables get improvements in their predictions when more data are available to fit the models p value 0 05 i e when the rolling origin technique is used table 2 shows an overview of the hypothesis testing results in particular variable ssbcod fig 14a showed significant differences in all models except the m2 model with ro performing better than rw in all significant cases variable ssbspr fig 14b showed significant differences in 3 out of 5 models with ro performing better than rw in models m0 and nb and rw performing better in model m2r variable ssbher fig 14c showed significant differences in all models except the m0 model with ro performing better than rw in 3 out of the 4 significant cases rw performed better in the m2r model variable spr1y fig 14d showed significant differences only in the naive bayes nb model with ro performing better than rw variable her1y fig 14e did not show significant differences in any model and variable cod2y fig 14f showed significant differences in all models except the nb with ro performing better than rw in all significant cases 3 2 2 does the introduction of hvs improve the performance of the models in order to find out whether or not the introduction of hidden variables improves the predictive performance we compared the loglikelihood of the predictions of models m0 m1 and m2 with m0 being the model without hidden variables m1 the model with one generic hidden variable and m2 the model with two hidden variables we used the friedman test to carry out the comparison fig 11 shows the boxplot of the differences between pairs of models boxplots outlined with color orange indicate that significant differences were found between the corresponding pair of models the results of the tests show significant differences only for variables ssbher and cod2y with models m1 and m2 outperforming model m0 in the case of cod2y whereas model m2 outperformed model m1 in the case of ssbher 3 2 3 does reversing some links improve the performance of the models in order to test whether or not reversing the links between variables ssbher and ssbspr and the zooplankton variables would make any difference in the models prediction we compared the loglikelihoods of the predictions of models m2 and m2r using the wilcoxon signed rank test fig 12 shows the boxplots of the differences between the pairs of models for each variable of interest the results of the tests show significant differences only for variables ssbher and spr1y with model m2r outperforming model m2 in both cases 3 2 4 does the use of expert based structures improve the performance of the models in order to test whether or not expert based structures outperform simpler models such as naive bayes we compared each expert based model m0 m1 m2 and m2r with the naive bayes nb model using the wilcoxon signed rank test fig 13 shows the boxplots of the differences between the pairs of models for each variable of interest the results of the tests show significant differences for models ssbspr spr1y her1y and cod2y in most cases nb outperformed the expert based models on the other hand for variable cod2y the expert based models outperformed the nb model except in the case of the pair m0 nb where no significant differences were found 4 discussion in this paper we have analyzed the baltic sea food web aiming at 1 detecting changes in its pattern and 2 making reliable predictions on some variables of interest the hidden variables called genhv and fishv models m1 m2 m2r and nb showed a clear increase near the year 1980 with the former occurring slightly later variable zoohv model m2r shows an abrupt jump which is delayed in comparison to the increase of the two other hvs former studies have found that the baltic sea has undergone several periods of change with one of them being described in the 1980s shannon et al 2009 many publications have also described a regime shift in the north sea in the late 80s beaugrand 2004 weijerman et al 2005 deyoung et al 2008 dippner et al 2012 which is connected to the baltic sea through the danish straits where water exchange between the two areas occurs according to the suggestion of some authors this regime shift may have been partly caused as a response to a change of the north atlantic oscillation nao in winter alheit et al 2005 which led to an increase in the winter and early spring air and water temperature nao induced changes along with overfishing of cod triggered a regime shift shannon et al 2009 leading to a period dominated by clupeids i e sprat and herring österblom et al 2007 after 1990 the regime shift described in the literature coincides with the changes observed in variable zoohv in model m2r this is in in line with the research by alheit et al 2005 where they describe that the abundance of zooplankton varied in accordance with the variations in the nao therefore it seems reasonable that the model reflects these variations in the nao on the other hand variable fishv in model m2 and m2r shows an increase from the beginning of the 1980s till the beginning of the 2000s when its tendency drastically changes towards a decreasing trend this hidden variable is linked to 14 fish related variables therefore the hidden variable is trying to capture the global trend of all these variables the fishv could be partly reflecting the indirect change of zooplankton as they partly predate on those and partly the change in the large anoxic area carstensen et al 2014 along with the clupeid dominated period variable genhv models m1 an nb shows a gradual increase from the beginning of the 1980s till 1990 where a constant period begins the behavior of this hidden variable could be reflecting the underlying dynamic of the zooplankton and fish variables as a whole but more research is needed from the predictive accuracy point of view significant differences among the models were scarcely found in particular the similar results for the expert based models could be regarded as a positive finding in the sense that as long as the model structure is coherent the details will not make a critical difference in the predictions when significant differences are found our results suggest that models with a higher number of hidden variables are not outperformed section 3 2 2 moreover reversing some links increased the predictive accuracy of two variables only with the remaining variables not showing statistically significant differences section 3 2 3 finally the use of simple models such as the naive bayes helped with the prediction of some variables compared to more complex models based on expert elicitation section 3 2 4 the reason for that is that simple models need to estimate fewer parameters which is very convenient when few data are available on the other hand improvements were found rather often when comparing the rolling origin and rolling window approaches in most of the significant cases having more data improved the prediction of the outcomes 5 conclusions the methodology proposed in this paper managed to identify a major regime shift that occurred in the baltic sea during the 1980s the dbn methodology can identify changes in ecosystems where only limited data are available which is an additional difficulty to cope with not surprisingly the increase in data size improved the models predictions some systems understanding is needed to construct the model structure but the present experiments show that the exact model setup does not make a critical difference to the results from the predictive accuracy point of view however the use of different hv setups helped reveal changes in the different parts of the ecosystem therefore the presented approach can be a highly useful tool in the study of potentially critical changes in complex ecological interactions acknowledgments this work has been partly supported by the spanish ministry of economy and competitiveness through project tin2016 77902 c3 3 p in addition this work has partially resulted from the bonus bluewebs project which has received funding from bonus art 185 funded jointly by the eu the academy of finland projektträger jülich ptj germany the state education development agency of latvia the national centre for research and development poland and the swedish research council formas a d maldonado was supported by the spanish ministry of education culture and sport through an fpu research grant fpu2013 00547 and a research visit grant est16 00723 appendix a model predictions the time series of the observed and predicted values for each variable of interest model and fold of both cross validation techniques are shown in fig 14a f from these predictions we computed the performance of each model in terms of log likelihood for each variable of interest fig 10b furthermore for the rolling origin case only we performed some hypothesis testing to compare the predictive performance of the different models figs 11 13 fig 14 model predictions expected value and observed values used to fit the models following the rolling origin ro and rolling window rw cross validation techniques as shown in fig 7 black dark gray and light gray lines correspond to data belonging to the train set of the rw approach train set of the ro approach and test set respectively red and blue lines correspond to the predictions obtained following the rw and ro techniques respectively the shaded bands correspond to the standard deviation fig 14 appendix b code the code necessary to run the experiments is available as supplementary material in particular supplementary material contains two m files with the matlab code one tar gz file with the ramidst r package and four files containing simulated data three csv and one arff as the real dataset used in this work is protected and cannot be published we provide simulated data to run the code most of the experiments carried out in this work were performed using the bayes net toolbox bnt package in matlab we have created a wrapper function dbn foodwebmodel to reproduce our experiments we provide some examples of how to use this function for the simulated dataset the results of running this function are stored in txt files on the one hand the expected value of the hidden variables can be obtained by running the following piece of code image 2 on the other hand the predictions of the target variables can be obtained running the following piece of code image 3 note that the given examples provide the predictions for the rolling origin case to obtain the results for the rolling window approach the argument cv must be changed from rol org to rol wind also note that it is not necessary to specify which are the target variables since the function is specifically programmed to return the values of these variables nevertheless this function can be easily adapted to other datasets and model structures to compare the variational bayes and the em algorithms for the regime shift detection we used the ramidst r package and the bnt matlab package we have created another wrapper function nbcdd for the experiments carried out in matlab an example of how to use this function is provided to illustrate the example variable cod2y is used as the class variable note that we computed the expected value of the hidden variable h for 6 different models i e using 6 different class variables image 4 an example of how to use the nb concept drift detector from stream function of ramidst r package is also provided image 5 appendix c supplementary data the following is the supplementary data to this article code spl data code spl data appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 011 
