index,text
2815,river discharge integrates many water related processes over land it is crucial for understanding inland water unfortunately in situ measurements are very sparse at the global scale this paper presents a totally new approach for the mapping i e spatially continuous estimate of the river discharge based on satellite observation of hydrological variables and the water budget balance first continuous satellite estimate of three water components precipitation evapotranspiration and total water storage change are corrected at basin scale using river discharge from a few gauge measurements secondly the water budget is balanced at the grid level using flow direction for horizontal water exchange this new approach is therefore based solely on satellite products and in situ measurements without the use of any dynamical model except river map the methodology is evaluated in interpolation mode between successive gauges median kge is 0 8 and extrapolation mode over small confluent rivers mean kge is 0 6 to be compared to 0 6 and 0 4 for a river dynamic model such as cama flood the spatially continuous river discharge shows a good agreement with altimetric water surface level wsl median correlation of anomalies is 0 6 and surface water extent satellite estimate correlation of anomalies is up to 0 7 over rivers with floodplains this suggests that timing of flood and low flow is fine a comparison with the hydrological model cama flood shows the benefit of such a continuous product to investigate model differences at high or low flow as well as early flow peak modeling while the spatial pattern of extreme events cannot be well represented only by in situ gauges information our study shows the added value of the mapping to better describe these events as hydrological application our method can be used in synergy with all the altimetric stations to create discharge wsl pair data which will benefit to advanced applications graphical abstract msc 86a32 86a22 keywords satellite observations river discharge mapping surface hydrology data availability data will be made available on request 1 introduction river discharge is a key component of the water cycle on earth because it integrates the response of the entire landmass to atmospheric forcing it is also one of the most critical variable for human society as the availability of surface water river and lakes have being shaping and conditioning the development of villages town and cities in addition hydrologic hazards such as drought and floods threat man made infrastructures with an increased frequency due to climate change kreibich et al 2022 ward et al 2020 in this context reliable information on river discharge is a necessity as the only ground truth measurement the in situ gauges remain crucial for water sciences but their usefulness is limited by their spatial coverage and latency on a global scale rivers monitoring with in situ observations is unevenly distributed between developed and developing countries and their precise calibration is not always straightforward fekete et al 2012 hydrologists have relied on satellite observations to complete the record of in situ measurements and increase their coverage the use of radar nadir altimetry papa et al 2010 santos da silva et al 2014 paris et al 2016 has proven to be very useful as the water surface level wsl estimates from the altimeter can be converted into discharges however such instruments provide only pointwise measurements and they need in situ measurement for calibration the classical approach to obtain spatially continuous river discharge remains however the combination of a land surface model lsm with a routing scheme fig 1 top represents a schematic view of this two step procedure 1 the lsm simulates surface water related process uptake and infiltration based on the precipitation forcing and computes the amount of water that will leave the model grid at each time step i e runoff production 2 a routing scheme integrates the runoff to gridded river discharge through the river system thanks to auxiliary information such as the flow direction derived from topography simulation of the river discharge has benefited in the last decades from a long effort improvement of both lsms and routing scheme for which the amazon basin was a benchmark fassoni andrade et al 2021 in the framework of international projects such as the global land atmosphere system studies glass or the climate model intercomparison panel cmip lsms have been improved in the representation of the energy water carbon cycles thanks to a fine vegetation dynamics snowpack modelization and the introduction of human activity into the models van den hurk et al 2011 in parallel the routing has benefited from improved river connectivity due to new hydrography dataset yamazaki et al 2019b and fine up scaling munier and decharme 2022 simple river representation has moved toward complex hydrological models miguez macho and fan 2012 paiva et al 2013 getirana et al 2012 yamazaki et al 2011 with sub grid approximations to represent fine scale processes in particular the interactions between groundwater maxwell et al 2007 vergnes et al 2012 miguez macho and fan 2012 or floodplains decharme et al 2008 yamazaki et al 2011 de paiva et al 2013 with surface water have been improved which have enabled floods forecasts kauffeldt et al 2016 as an effort to prepare for the surface water and ocean topography swot mission assimilation techniques have been developed to correct the model outputs or parameters based on observation of swot like water surface level these assimilations can be performed on a lsm wongchuig correa et al 2020 a routing scheme wang et al 2018 emery et al 2020 on a hydrological model revel et al 2019 or even on a hydraulic model malou et al 2021 the swot mission to be launched in 2022 will provide high resolution 2d fields of water surface for the very first time alsdorf et al 2007 durand et al 2010 chelton 2019 in the context of this mission important efforts have been done to provide spatially continuous river discharge estimates independently from a dynamical model durand et al 2016 if these estimates can be spatially continuous they are temporally discontinuous spatially and temporally continuous river discharge estimate is however not easy to obtained point observations of the river discharge encapsulate information about hydrological processes upstream the location conditioned by rainfall and topography that can be described by 2d field variables regular spatial interpolation fail to reconstruct the river discharge as neighbors provide no direct information in order to overcome this bottleneck two main strategies have been found in the literature on the one hand krigging based method have been recently developed to interpolate river discharges over the stream using statistical ordinary krigging interpolation farmer 2016 yoon et al 2013 paiva et al 2015 or flow duration krigging interpolation castellarin et al 2018 these methods derive river discharge over a basin relying on the covariance of in situ measurement solely farmer 2016 or more rarely in explicitly adding the river network information paiva et al 2015 but none of these methods account for the water mass conservation on the other hand the 2d field to point nature of the river discharge has been investigated to generate spatially and temporally continuous river discharge fields from discrete observations fisher et al 2020 yang et al 2019 the core of the algorithm is the inverse streamflow routing method pan and wood 2013 that is based on a kalman filter that inverse a routing scheme parameters including the flow direction it allows to reconstruct the runoff field from the river discharge measurements at various time and location in a basin nevertheless this method relies on some model parameter such as flow velocity and cannot be considered independent the goal of this study is to provide and assess a new method for the continuous river discharge mapping from a totally new source of information based on the satellite observation of hydrological variables fig 1 bottom illustrates this innovating approach that uses no model compared to the classical modeling method top using satellite estimates for the three water components i e precipitation evapotranspiration and total water storage change and information on the flow direction allows to balance the water budget spatially between the pixels and to estimate river discharge on a continuous field section 2 details the methodology section 3 describes the materials the quality of the river discharge mapping is quantified through two experiments and evaluated using water surface level and water surface extent in section 4 section 5 illustrates the use of the continuous river discharge estimate to infer mean river height and to analyze extreme events finally the conclusion and perspectives are drawn in section 6 2 methodology for river discharge estimation 2 1 indirect river discharge measurement using water budget since the objective is to estimate gridded river discharge the following formalism uses gridded variables let us consider a whole river basin including n pixels one sub basin e g catchment c o including m pixel is defined from an outlet located on the pixel o o for outlet over the catchment the change in water storage d s is explained by the flux of water entering through precipitation p and leaving through evapotranspiration e or river discharge at the outlet q o no groundwater discharge is considered here the water budget can therefore be used to estimate q o 1 q o i c o p i e i d s i since this equation represents the water mass conservation all values on each pixel have been weighted by the pixel area this estimation stands for any temporal resolution in the following d s is measured by the grace satellite at its native monthly resolution therefore the monthly time step is utilized throughout the paper it is useful for the following to re written eq 1 using a linear operator g applied on all the n pixels of the basin 2 q o g o y with y p 1 p n e 1 e n d s 1 d s n y is the state vector 3 n 1 of p e and d s over the basin with n pixels g o is a 1 3 n vector so that g o i 1 respectively g o n i 1 g o 2 n i 1 if pixel i is in the catchment c o and g o i 0 everywhere else for some locations over the river network the river discharge is measured by in situ gauges let us call q o the available measured river discharge at the location of pixel o the goal of this section is to optimize y to this aim the satellite observations provide an a priori y s a t characterized by the a priori error covariance matrix b b is a 3 n 3 n block diagonal matrix defined as 3 b b p 0 0 0 b e 0 0 0 b d s where b p resp b e and b d s is the covariance matrix of p resp e and d s it has been shown in the literature that the water budget could not be easily closed using satellite estimate y s a t and observed in situ river discharge q o pellet et al 2021b dorigo et al 2021 4 q s a t o g o y s a t q o ϵ q q o the term ϵ q i e the in situ measurement error ϵ q n 0 σ q estimating such measurement error is not an easy task the value σ q 4 is a trade off prescribed by a recent literature review dorigo et al 2021 to account in situ measurement river discharge uncertainty at global scale the previous equation tells how the observation q o can be used to optimized the initial estimate y s a t an optimized solution y a can be found based on the optimal interpolation oi rodgers 2000 such that g o y a q o 5 y a y s a t k q o g o y s a t where k b 1 g o σ q 1 g o t 1 g o σ q 1 and a represents the analysis of the previous solution y s a t with this solution y a closes the water budget over the catchment while being closest to y s a t in a least square sense since eq 5 considers all the pixels at a times the error correlation of the observations need to be considered in previous wb based optimization e g pellet et al 2021b b was limited to a diagonal matrix with prescribed values from the literature or estimated uncertainties from the spread of the observations following yang et al 2019 off diagonal terms have been added to the covariance error matrix b a simple assumption is taken error correlations decay exponentially in space 6 b p i j ϵ p p i ϵ p p j e d i j d where d i j is the distance between pixel i and j and d is the e folding distance in pixel here d 1 2 is chosen this expression extends easily to b e and b d s the error base estimates is taken from dorigo et al 2021 ϵ p 7 ϵ e 5 and ϵ d s 5 no covariance terms have been considered between two different variables i e p vs e p vs d s and e vs d s it is worth noting that accounting for the off diagonal term in covariance matrix b is the first step toward optimizing the state vector y over ungauged basin using the spatial pattern of the variable itself yang et al 2019 and introducing regularization in q the oi framework allows estimating the a posteriori error covariance matrix s a i e uncertainty on y a 7 s a b 1 g o σ 1 g o t 1 2 2 water budget closure at the pixel scale the previous section in particular eq 2 considers the water budget at the catchment scale it is however possible to compute the water budget at the pixel scale but extra information is required because no river discharge measurement is available the flow direction defines the direction taken by the free surface water to leave a pixel it is assumed that rivers flow downhill due to gravity so topography is used to estimate the flow direction 1 d flow direction is used here each pixel has one and only one downstream pixel while o was previously defined as an outlet of a catchment its definition is extended here to any pixel on the river indeed all pixels have a drainage area considering an outlet o and its u nearest upstream pixels the water budget gives 8 q o p o e o d s o u u p s t r e a m o q u the network connectivity matrix f d represents the flow direction a sparse square matrix whose dimension is n n a value of one is used at row i and column j if pixel j flows into pixel i and zero is used everywhere else the row f d i indicates the upstream pixels of i each pixels have only one output with no loop 1 d flow direction the previous equation can be re written using the connectivity matrix f d q o 1 1 1 p o e o d s o f d o q where vector q q 1 q n t is the river discharge vector over all pixels of the basin this last equation will be used in the following as a constrain on q o 2 3 two step mapping scheme the river discharge q o at a location o can be estimated from satellite observation y s a t accounting for the water mass conservation derived in the two eqs 5 and 9 this can be used to estimate the river discharge q across all pixels of the river network the mapping is a two step procedure 1 it first corrects the original satellite estimate of p e and d s at sub basin scale using the gauges observation 2 it then globally optimizes all the pixel fluxes using the flow direction to constrain the inter pixel exchanges using the river network connectivity to interpolate river discharge has already been achieved sauquet 2006 fisher et al 2020 but the key element of the proposed methods here is to use the water budget along with the satellite products for three of the water components in this way the method can be applied at global scale and the resulting river discharge estimate are independent from any model fig 2 illustrates the two step procedure 2 3 1 upstream propagation of in situ gauges the framework detailed in section 3 1 is here extended by the used of p in situ gauges q i to correct the satellite estimate of p e and d s over the respective drainage area using eq 2 we have the linear system which can be written using matrices 9 g y s a t q ϵ with g g 1 g p q q 1 q p ϵ ϵ 1 ϵ p n 0 σ following eq 5 the solution y a is a correction of y s a t to match the river discharge measurement q 10 y a y s a t k q g y s a t with k b 1 g σ 1 g t 1 g σ 1 this first step is close to what has been introduced in pellet et al 2021b a but the state vector is extended here to all the pixels inside the basin not only the spatial average 2 3 2 global pixel wise optimization with flow direction constrain once the three spatially continuous water components have been corrected to balance all in situ river discharge the water budget can be closed at pixel scale to estimate 2d field river discharge following eq 9 we can obtain the equation for river discharge over all the pixels using the water budget and flow direction q c y a f d q i n f d q c y a 11 h q y a with c a sparse n 3 n matrix with c i i 1 respectively c i n i 1 and c i 2 n i 1 and c u v 0 everywhere else c compute the flux difference only on particular location i e pixel contrary to g a l l that consider flux everywhere upstream a location using 12 h c t c c t 1 i n f d eq 12 allows estimating pixel wise river discharge q by balancing the water budget between pixels the global integration ensures the inter dependent correction at pixel scale following the river network connectivity over the entire spatial domain if no a priori information is used for q it is a pure interpolation of the river gauge measurement q though the use of y a if an a priori is used it can be seen as an optimal interpolation in our experiment the use of a priori q a p helps obtaining a stable solution several sources of information exist for the a priori e g climatology modeled runoff if modeled discharges were used this should be seen as assimilation to keep a pure observation based estimate we used the a priori q a p based on original satellite observation 13 q a p g a l l y s a t where g a l l is the matrix g defined in eq 2 extended for the n pixels of the basin the final solution is 14 q q a p k 2 y a h q a p 15 k 2 s a p 1 h s a 1 h t 1 h s a 1 where s a is the a posteriori error covariance matrix obtained at the first step of the procedure see eq 7 s a p is the a priori covariance of q a p to avoid negative river discharges a positiveness constrain is added on q this is achieved using an iterative algorithm for constrained linear least squares problems based on the lagrange multipliers at this stage of the optimization other sources of constraint such as regularization constraints e g tikhonov regularization could be added for estimating q also accounting for dams at pixel level may be introduced as additional constrains to optimize the solution 3 study area and data source 3 1 the amazon basin the amazon basin plays a major role in the global water and energy cycle it is characterized by complex hydrological processes forced by heavy precipitation the surface water creates extensive floodplains under dense tropical forests shapes complex topography and shows large variations through the basin the basin is now facing great climatic risk marengo et al 2018 the satellite observations have played a great role in supporting research in amazon hydrology fassoni andrade et al 2021 in the modeling community satellite datasets are usually adopted as either forcings e g precipitation a priori information to estimate parameter values e g topographic data validation calibration or assimilation data e g discharge river water levels 3 2 dataset used in this study river discharge q monthly water discharge data are obtained from 110 stations collected throughout 2000 2015 at these stations agência nacional de águas ana brazilian national water agency provides accurate river discharge measurements based on observations of water stages that enable representing accurate local rating curves paiva et al 2013 precipitation p the tropical rainfall measuring mission multi satellite precipitation analysis tmpa huffman et al 2007 is a quasi global 50 n to 50 s precipitation product with a spatial resolution of 0 25 and which covers the 1998 to present period huffman et al 2007 the retrieval estimates instantaneous precipitation from microwave imager passive microwave observations as well as infrared data from a geosynchronous earth orbit the version 3b42 v7 used here is obtained by combining satellite estimates with gauge measurements from the global precipitation climatology center schneider et al 2011 2014 using inverse random error variance weighting of the gauge data this product has been evaluated globally huffman et al 2007 su et al 2008 and used in many hydrological analysis the choice of tmpa has been made based on its original spatial resolution that matches the river map grid i e 0 25 see next sub section rather that its performance compared to other well used calibrated precipitation data set beck et al 2017 this has prevented the introduction of additional remapping error due to the spatial interpolation evapotranspiration e the global land evaporation amsterdam model gleam v3b miralles et al 2011 martens et al 2016 uses an empirical energy based equation priestley and taylor 1972 to calculate a reference evapotranspiration value which is converted to actual e based on land cover and an evaporative stress factor this method estimates separately the components of land evaporation transpiration bare soil evaporation interception loss open water evaporation and sublimation it does so by considering four different types of land cover bare soil sparse vegetation dense vegetation and open water in each grid pixel the vb version of gleam used here relies on tmpa as an inputs to produce a daily dataset at a spatial resolution of 0 25 if several e product exist in the literature mueller et al 2011 zhang et al 2017 with broadly equivalent global accuracy michel et al 2016 miralles et al 2016 the choice of the gleam has been made on its spatial resolution 0 25 so that the product matches the river map grid resolution and its temporal coverage of the grace period 2002 2018 gleam provides an et estimate at 0 25 that covered the 2002 2015 period as well as other et estimates total water storage change d s the twin satellites grace tapley et al 2004 offer a unique opportunity to monitor the water stored in land d s estimates are based on grace and grace follow on satellites tapley et al 2004 these estimates include surface water wetlands floodplains lakes rivers and artificial reservoirs soil moisture snowpack glaciers and groundwater the retrieval is based on the mass concentration solution developed by the center for space research csr known as mascon msc the csr msc solution is initially based on a classic sphherical decomposition sh of the inter satellite range rate measurements which is then spatially truncated at the location of the mass concentration save et al 2016 scanlon et al 2016 have shown the many advantages of the grace csr msc solutions relative to traditional sh solutions including reduced leakage perturbation increased seasonal signal amplitude and little or no postprocessing csr msc solution is provided on a 0 25 0 25 grid globally over the grace 2002 2017 and grace fo 2018 present periods 3 3 flow direction based on merit hydro the flow direction used to describe pixel connectivity inside the river system is based on merit hydro yamazaki et al 2019a a global hydrography map raster flow direction map based on the latest water layer data and spaceborne digital elevation models yamazaki et al 2017 the flow direction is 1d meaning that each pixel has only one downstream pixel the flow direction originally at the resolution of 3 is upscaled to 0 25 for running the catchment based macro scale floodplain cama flood model globally yamazaki et al 2011 we used this 0 25 river network map for satellite discharge mapping in this study 3 4 datasets used for evaluation modeled river discharge the simulated river discharges are obtained from the hydrological model cama flood zhou et al 2021a cama flood computes physically based hydrodynamics at continental scale along a prescribed river network based on the merit hydro flow direction map in this study we used the default setup of cama flood version v4 0 as an input runoff for cama flood we used the earth2observe runoff data produced by the land surface hydrological model htessel forced with modified era interim balsamo et al 2009 runoff and discharge are both provided on a 0 25 grid the choice of the modeling dataset relies on previous analysis zhou et al 2021b for the runoff input as well as a fine tuning of the cama flood model by its developers for a better representation of the amazon discharge detailed model settings can refer to zhou et al 2021a cama flood simulates daily river discharge that have been averaged on the monthly basis in order to be compared to our satellite based estimate water level the altimetric dataset is created by santos da silva et al 2014 and used in paris et al 2016 this dataset consists of 920 stations that provide a water height time series over the amazon basin from the envisat 2002 2010 mission envisat provides repeat measurements every 35 days with a height accuracy of a few tens of centimeters details regarding altimetry data processing and quality assessment can be found in santos da silva et al 2014 or paris et al 2016 these estimate are snapshots of the water level which may not be representative of the whole month especially if the observation occurs at the very end or the very beginning of the month on highly dynamic river reaches the evaluation is based on similarities with the estimated river discharge in terms of temporal variation i e correlation surface water extent the global inundation extent from multi satellites giems is a dataset representing the global surface water extent and dynamics over a long period 1993 2007 based on a collection of satellite observations the percentage inundation is estimated on monthly basis over an equal area grid 0 25 at the equator prigent et al 2007a 4 river discharge assessment the main objective of the methodology is to obtain an estimate of the river discharge that matches in situ measurement while being spatially coherent with the satellite observations it is therefore a compromise found between all the sources of information and constrain three metrics are used for the evaluation the correlation of anomalies the nash sutcliffe efficiency nse and the kling gupta efficiency kge nse of 1 indicates that the river discharge estimates matches perfectly the observations nse of 0 that the reconstruction is as good as the observation mean and nse lower than 0 that the reconstruction is a worse predictor than the observation mean similarly kge of 1 indicates perfect agreement between reconstructed river discharge and observations but no specific meaning attached to a kge of 0 a recent benchmark highlights that kge lower than 0 41 indicates performance equivalent to the observation mean knoben et al 2019 4 1 evaluation of the method in interpolation and in extrapolation modes in order to test the robustness of the estimation a leave one out loo validation technique is applied this technique is widely used when few data here the gauges measurements are available from a real word problem in this section the methods is tested in two various modes and compare to the cama flood simulation it should be noted that the cama flood simulation does not use any in situ discharge in assimilation mode we therefor compare our methods with a baseline model not with a reanalysis in the first experiment one gauge is withdrawn from the ensemble in the mapping procedure the obtained river discharge estimate is then compared to the gauge measurements on its location this operation is repeated for all the available gauges this experiment evaluates the capability to accurately estimate river discharge over a given pixel in testing the interpolation between two in situ stations based on the satellite observations table 1 summarizes the performance in the unseen gauges data it compares the interpolation to two other estimations the a priori estimate q a p based on the original satellite observation solely eq 13 and simulated river discharge from the cama flood model the a priori estimate q a p shows reasonable median nse 0 46 it can therefore be used as a good first guess for large river with unknown discharge its performance is still limited by the raw satellite products errors while obtaining very good performance over the big rivers the simulation q c a m a accuracy is degraded over small confluents not shown overall the simulated river discharge shows a better median nse 0 58 than q a p the nse obtained for the interpolation q l o o is greatly higher in the loo experiment 0 84 than for the two other estimates the mapping satisfactorily matches in situ measurement when neighboring gauges are used the correlation and kge median follows the same conclusion than nse for the three estimates nse and kge are slightly lower than the correlation with a larger spread among the in situ gauge ensemble not shown stressing bias or variance issues kge as well as anomalies error nse issues on some locations in the second loo experiment several gauges located on the same stream have been removed at once the objective is to estimate an entire stream using only its most downstream in situ station in the integration this can be seen as testing the capability of extrapolating the upstream river discharge over a basin not well constrained by gauges data the kge efficiency metric is shown in fig 3 for this experiment and for the model our estimate can reproduce satisfactorily river discharges over an entire stream based on only one station of the stream mean kge is 0 6 not surprisingly a negative gradient can be observed for the kge index along the streams stressing the difficulty to estimate river discharge on remote upstream stations this shows that the higher the number of in situ river discharge measurements used in the integration the better our gridded river discharge will be this is to be expected the methodology is able to exploit these points estimates and the satellite corrected observations are used to interpolate extrapolate these values overall the proposed integration scheme allows for a better estimation of the monthly river discharge on small confluents than the cama flood model particularly on southern rivers where the model outputs are characterized by a strong bias 4 2 evaluation using wsl from satellite altimetry altimeter measurements are used to estimate the wsl that can be converted into discharge via a rating curve equation paris et al 2016 see benefits of the methodology in ungauged rivers in paris et al 2020 and andriambeloson et al 2020 we evaluate here our gridded river discharge estimate by comparing them with the wsl observation from envisat fig 4 shows the correlation of anomalies between them over the envisat virtual stations climatological monthly mean over 2002 2010 have been removed from the two datasets to truly measure the link between these two quantities the correlation of anomalies goes from 0 3 in the most upstream area to 0 9 over the largest rivers negro solimoes amazon the overall agreement is higher in the northern western part of the basin the correlation slightly decrease in the most downstream of the large tributaries e g purus madeira this is due to the backwater effect from the mainstream when water level of mainstream is high discharge from tributaries is halted because it cannot flow into the mainstream this process cannot be represented by our method nor be resolved by satellite datasets as the relationship between wsl and river discharge is non linear but defined as a power law equation nor always univocal this simple evaluation is not perfect and will not permit to detect biases or over evaluation of the amplitude of variation of the river discharge but the agreement in terms of anomalies between them is promising and shows a potential synergy for the use of these two sources of information the degradation of the correlation near the confluences of the main rivers and the amazon river evidences this not univocal relationship and the limitations of this analysis 4 3 evaluation using surface water extent river discharge and flood extent are highly correlated in the amazon basin where the floodplain is much more caused by the elevation of the river than any other processes e g precipitation fed flooding or paddy irrigation the giems database provides an estimation of the surface water extent from 1993 to 2007 on a 0 25 resolution equal area grid at the global scale prigent et al 2007 fully assessed over the amazon basin papa et al 2013 it is possible to analyze the relationship between river discharge and surface water change over the giems at pixel scale over the 1993 2007 period correlation left and correlation of anomalies right are jointly represented fig 5 for all the pixels included in giems since rivers can only explain a small part of the surface water extent at 0 25 we did not expect a perfect match between these two variables the correlation is high for the main streams inter annual variability can be measured by seasonal anomalies their correlations anomalies in fig 5 bottom are significant in the central amazon upstream branco and madeira river this indicates that these pixels store much of the surface water at their surface this suggests that water balance can be useful in estimating pixel wise river discharge to investigate wetland hydrological regimes in areas with significant canopy coverage the comparison between q and water surface change is not a direct evaluation but the fact that there was coherence between the two completely independent measurements related to the water cycle demonstrates the strength of the river discharge reconstruction 4 4 comparison to the cama flood hydrological simulation in this section the pure observation based gridded river discharge estimate is compared with simulated river discharge cama floods model has been chosen here it is based on the same flow direction map merit hydro as the integration framework this avoids differences related to a different river network three types of comparison are shown spatial pattern river segment comparison and pixel time series comparison the spatial pattern of correlation correlation of anomalies and kge index between simulated and observation based river discharge are shown on the right panel of fig 6 the correlation coefficient is high over all the rivers 0 6 floodplain areas are characterized by a negative correlation between the two datasets while cama flood model can represent floodplains hydrology and in particular the backwater effect discharge is halted or reversed from the mainstream to surrounding tributaries due to its higher water level improvements are still needed to account for these processes in the integration framework when looking at the kge the overall proximity of the two datasets is clear but a difference with the correlation coefficient over the altamira river basin most southern western basin of the amazon is noticeable contrarily to what was seen on the correlation map this means that there is a large bias or variance difference see next paragraph for the pixel scale comparison on the map of the correlation of anomalies two river paths are shown in red through the madeira from a and the negro from b rivers the long term mean is shown along the path on the left bottom panel of fig 6 this evaluation allows to identify larger differences in the long term mean river discharge due to particular confluence rivers even if the simulated discharge is close to the observational based estimate at end of the water path defined from a on the madeira river differences can be high and compensate each other along the path over the river path on the negro river defined from b the two estimates show a continuous bias all along the second part of the path comparison between simulated and gridded observation based discharge can be achieved at pixel scale by comparing their time series two pixels represented by a rectangle on the background map have been chosen here and their location are indicated in the plots the outlet of the altamira has been chosen to discuss the bias issue pointed out by the previous intercomparison of the correlation and kge a scaling error of cama flood impacts high flow simulation for this river over the negro river for particularly dry year a bias at low flow can be observed in the simulation 2003 and 2005 this could be related to runoff input bias and or soil water content issue in the model fig 6 illustrates the opportunity to use the pure observation based representation of the river discharge to improve the model simulations 5 potential applications 5 1 spatially distributed mapping of extreme events the river discharge integrates all the surface and underground water related processes the analysis of extreme events allow to characterize the behavior of a river system under particular stresses the better the characterization of past events the better the forcasts under climate change will be such analyses therefore plays a key role in the hydrology using modeled river discharges is often limited in term of accuracy due to significant model biases so our river discharge mapping is an interesting alternative to study extreme regimes fig 7 top represents the climatological anomalies at the outlet 1 4 52 1 of the amazon basin important floods 2006 and 2009 and droughts 2005 and 2010 picked by the outlet river discharge and reported by the literature have strong positive and negative pattern of their seasonal anomalies fig 7 bottom it is difficult to choose appropriate scale to analyze river discharge at pixel scale as the variability is huge from a small to large river the anomalies are computed as percentage of the climatological mean for each pixel during the drought in 2005 the entire river system displays strong negative anomalies rare upstream rivers have positive anomalies the spatial pattern for negative value is vaster than for the drought in 2010 for which southern rivers xingu and tapajos are not dried comparing these two drought events it can be seen from the outlet river discharge anomalies top that the temporal scale is shorter in 2006 than in 2010 the drought in 2010 is stronger not particularly in space but in time with a long period of value from march to september the amazon mainstream was impacted mostly in september and october 2010 with discharge anomalies exceeding 30 the drought mainly impacts first southern and eastern then central regions of the river system during floods in 2006 the high anomaly appearing on the time series top is mainly due to the strong positive anomalies visible in the northern basins negro and bianco rivers of the amazon on the map bottom note that the southern basins are not impacted by this flood the outlet is finally impacted because the northern basin provides the largest amount of water to the mainstream and its discharge is very sensitive to the discharge of these confluent rivers in 2009 the flood peaked is in march the spatial pattern of this flood is wider than in 2006 only xingu river does not show positive anomalies and solimoes river have a positive value in the very upstream area floods and droughts reported by literature are analyzed at stations scale if co investigation of stations located on different confluent give insight on the spatial scale of the event wongchuig correa et al 2020 the analysis of extreme events benefits here from the spatially continuous aspect for the gridded river discharge estimate droughts are characterized by a wider spatial scale than floods in impacting most of the river system floods can be caused by one particular confluent the northern ones gridded river discharge estimates show the benefit of observation based spatial analysis of extreme events which have previously only been conducted using lsms the use of p e and d s observations estimate to reconstruct r provides realistic estimates of discharge anomalies 5 2 mean river depth estimation the results in section 4 2 have shown that envisat altimeter observation and the gridded river discharge estimate agree very well it represents a unique opportunity to use these two types of information for a pure observation based hydrological analysis one of the critical parameters of a hydrological model is the river bed elevation river bed elevation along with river width define the river storage capacity and finally the simulated river discharge while some remote sensing based products exist for the river width yamazaki et al 2014 river bed information is often derived from a simple parametrized power law equation here we estimate the mean water depth in a two steps method following paris et al 2016 first we estimate the rating curve parameter z 0 a b that link the river discharge q to the altimetric wsl h for a particular location 16 q t a h t z 0 b the three parameters are found using a nonlinear curve fitting data fitting solver in a least squares sense when the parameter z 0 is found the mean water depth is derived from the mean altimetric water surface level and z 0 parameter h e h t z 0 the fig 8 shows the river bed estimate at the altimetric virtual station location using the observed based gridded discharge estimate compared with the original paper paris et al 2016 the river channel depth parameter used in cama flood zhou et al 2021a is shown for illustration only as it is defined as the depth below the bankfull elevation and cannot thus represent the identical variable as the two other estimate of mean water depth so no direct comparison can be made between cama flood channel depth parameter and estimated mean water depth nevertheless this illustrates the potential benefit to use continuous river discharge estimate with altimetric wsl to better parametrize hydrological model while the two altimeter based estimates show a deeper river in the central amazon river confluent of the negro in the amazon at manaus cama floods parameter presents a gradient following flow accumulation pattern the critical difference between paris et al 2016 and our analysis is the source of information for the discharge paris et al 2016 have used simulated river discharge at the virtual station to find the rating curve parameter z 0 a b in our experiment the river discharge is purely estimated using satellite products if the two river bed elevation patterns agree the observation based discharge results in a deeper river upstream in the solimoes river the use of observed discharge ensures that no model bias is propagated to the river bed estimate such river bed elevation derived from altimeter and gridded river discharge is an opportunity to calibrate models ensuring the consistency between such information and altimeter river height that is the only observation assimilate into the hydrological model so far 6 conclusion a totally new integration framework is proposed to estimate a spatially continuous mapping of monthly river discharge using the water budget equation with satellite estimates of the precipitation evapotranspiration and water storage change a few in situ river measurements and the river network information the river discharge estimated over the amazon system has been validated against in situ gauge measurements and evaluated against two related water variables the surface water elevation and the water surface extent and compared to the cama flood simulation the overall performance of our new product is promising it is a step forward to quantify river discharge over sparsely monitored basin from the application the spatial patterns associated to extreme events are well represented by the gridded river discharge allowing discrimination between large scale versus localized extreme event are the main floods related to one particular confluent do droughts always impact all river at once these question could be investigated using gridded river discharge over longer period as a new source of information for hydrological studies the river discharge mapping can be used with the altimetric wsl data to create pair of river discharge and wsl this synergy could leverage bathymetry estimation in section 5 one application is showcased in estimating the mean water height based on the rating curve as stated before envisat provides repeated snapshot of the water surface elevation the difference of temporal resolution between monthly river discharge and snapshot of water surface elevation might limit the rating curve but raises the opportunity to temporally downscale the gridded river discharge using wsl data it could be possible to translate instantaneous wsl measurement into instantaneous discharge after calibrating the rating curve at monthly scale our estimate independent from any dynamical model could benefit to the river models in calibration validation experiments in particular this pure observation based representation of the river discharge will be useful to evaluate model simulations to quantify their biases and allow for their calibration it should also highly facilitate the assimilation in a single surface variable i e the river discharge a complex integrated satellite observation such as grace data that is not routinely been assimilated as already stated the cama flood simulation used as a comparison in this paper does not use any in situ discharge a comparison to the assimilated discharge product rather than open loop simulations is needed however assimilation for large scale river model is still underdevelopment and there is no available assimilated discharge products at the stage of this research currently the availability of in situ measurement is still critical for the accuracy of our product a better characterization of the satellite product uncertainties over particular regions should allow for a better estimation of the river discharge from the satellite observation another limitation is the size of the catchment due to the coarse resolution of grace the mapping strategy is suitable for large catchment with enough spatial information from grace furthermore due to the high uncertainty of the water component over the mountain and snowy areas the mapping might not be suitable for these regions our mapping method reconstructs a distributed representation of discharge ensures the hydrological coherency over a full river system while being independent from model but the river network map itself indeed our method is dependent to the conceptualization of real rivers with approximation such as one downstream direction assumptions this is still different from the reality and contains some uncertainties nevertheless this representation can be considered as a complementary pillar to modeling for a better knowledge of hydrology starting from observations this data driven approach can be seen as a simple model for along river network water budget closure it allows building a representation that takes into account the impact of human activity such as irrigation or dam management that can be sensed from space through evapotranspiration and total water storage change estimates for this perspective merging statistical relationships along with simple physical constrain into a hybrid data driven model allow alternative from the classical modeling as any data driven method the river discharge mapping is not exempt of uncertainty in particular the uncertainty on p and e can translate into an uncertainty in r decharme and douville 2006 nasonova et al 2011 gelati et al 2018 zhou et al 2018 david et al 2019 following what has been done in the modeling community concerning the runoff error propagation analysis schellekens et al 2017 a perspective of the study is to assess the river mapping with uncertainties subject to different combinations of water component inputs to investigate the error propagation mapping the river discharge with various inputs to estimate a pdf of the river mapping and a proxy of its error from the spread this should be consistent with the model inter comparison framework getirana et al 2014 and allow comparison between model and the proposed satellite based mapping uncertainties another source of improvement is to better estimate input uncertainties with the data producer and use these uncertainties in the mapping strategy instead of the prescribed value and reduce bias issues through an optimized merging of several datasets for each water component under such hypotheses final uncertainty on the river product can be derived too through the oi strategy the temporal resolution of the gridded river discharge estimate will be improved in the future using sub monthly solutions of the grace d s e g the cnes grgs grace solution at 10 day biancale et al 2005 or daily resolutions solutions ramillien et al 2020b kvas et al 2019 to benefit from the swot mission alsdorf et al 2007 durand et al 2010 chelton 2019 the next step is to obtain a global pure observation based continuous and sub monthly river discharge estimate a future task is to integrate our river discharge estimate into the upcoming swot based river discharge retrieval durand et al 2016 our methodology could also help the swot mission by extending its estimation of the river discharges back in time by a few decades by using a reference over the swot era to obtain reliable river discharges over the grace period 2002 2017 furthermore the assimilation experiments currently developed for swot yang et al 2019 emery et al 2020 could benefit from independent river discharge estimations credit authorship contribution statement victor pellet conceptualization methodology software writing reviewing filipe aires conceptualization writing reviewing dai yamazaki visualization investigation editing xudong zhou visualization investigation editing adrien paris visualization investigation editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments victor pellet is founded by the centre national d etudes spatiales cnes postdoctoral fellowship the author would like to thank ayan santos fleischmann for providing the ana river discharge measurements the authors thank the two anonymous reviewers who help to improve this manuscript 
2815,river discharge integrates many water related processes over land it is crucial for understanding inland water unfortunately in situ measurements are very sparse at the global scale this paper presents a totally new approach for the mapping i e spatially continuous estimate of the river discharge based on satellite observation of hydrological variables and the water budget balance first continuous satellite estimate of three water components precipitation evapotranspiration and total water storage change are corrected at basin scale using river discharge from a few gauge measurements secondly the water budget is balanced at the grid level using flow direction for horizontal water exchange this new approach is therefore based solely on satellite products and in situ measurements without the use of any dynamical model except river map the methodology is evaluated in interpolation mode between successive gauges median kge is 0 8 and extrapolation mode over small confluent rivers mean kge is 0 6 to be compared to 0 6 and 0 4 for a river dynamic model such as cama flood the spatially continuous river discharge shows a good agreement with altimetric water surface level wsl median correlation of anomalies is 0 6 and surface water extent satellite estimate correlation of anomalies is up to 0 7 over rivers with floodplains this suggests that timing of flood and low flow is fine a comparison with the hydrological model cama flood shows the benefit of such a continuous product to investigate model differences at high or low flow as well as early flow peak modeling while the spatial pattern of extreme events cannot be well represented only by in situ gauges information our study shows the added value of the mapping to better describe these events as hydrological application our method can be used in synergy with all the altimetric stations to create discharge wsl pair data which will benefit to advanced applications graphical abstract msc 86a32 86a22 keywords satellite observations river discharge mapping surface hydrology data availability data will be made available on request 1 introduction river discharge is a key component of the water cycle on earth because it integrates the response of the entire landmass to atmospheric forcing it is also one of the most critical variable for human society as the availability of surface water river and lakes have being shaping and conditioning the development of villages town and cities in addition hydrologic hazards such as drought and floods threat man made infrastructures with an increased frequency due to climate change kreibich et al 2022 ward et al 2020 in this context reliable information on river discharge is a necessity as the only ground truth measurement the in situ gauges remain crucial for water sciences but their usefulness is limited by their spatial coverage and latency on a global scale rivers monitoring with in situ observations is unevenly distributed between developed and developing countries and their precise calibration is not always straightforward fekete et al 2012 hydrologists have relied on satellite observations to complete the record of in situ measurements and increase their coverage the use of radar nadir altimetry papa et al 2010 santos da silva et al 2014 paris et al 2016 has proven to be very useful as the water surface level wsl estimates from the altimeter can be converted into discharges however such instruments provide only pointwise measurements and they need in situ measurement for calibration the classical approach to obtain spatially continuous river discharge remains however the combination of a land surface model lsm with a routing scheme fig 1 top represents a schematic view of this two step procedure 1 the lsm simulates surface water related process uptake and infiltration based on the precipitation forcing and computes the amount of water that will leave the model grid at each time step i e runoff production 2 a routing scheme integrates the runoff to gridded river discharge through the river system thanks to auxiliary information such as the flow direction derived from topography simulation of the river discharge has benefited in the last decades from a long effort improvement of both lsms and routing scheme for which the amazon basin was a benchmark fassoni andrade et al 2021 in the framework of international projects such as the global land atmosphere system studies glass or the climate model intercomparison panel cmip lsms have been improved in the representation of the energy water carbon cycles thanks to a fine vegetation dynamics snowpack modelization and the introduction of human activity into the models van den hurk et al 2011 in parallel the routing has benefited from improved river connectivity due to new hydrography dataset yamazaki et al 2019b and fine up scaling munier and decharme 2022 simple river representation has moved toward complex hydrological models miguez macho and fan 2012 paiva et al 2013 getirana et al 2012 yamazaki et al 2011 with sub grid approximations to represent fine scale processes in particular the interactions between groundwater maxwell et al 2007 vergnes et al 2012 miguez macho and fan 2012 or floodplains decharme et al 2008 yamazaki et al 2011 de paiva et al 2013 with surface water have been improved which have enabled floods forecasts kauffeldt et al 2016 as an effort to prepare for the surface water and ocean topography swot mission assimilation techniques have been developed to correct the model outputs or parameters based on observation of swot like water surface level these assimilations can be performed on a lsm wongchuig correa et al 2020 a routing scheme wang et al 2018 emery et al 2020 on a hydrological model revel et al 2019 or even on a hydraulic model malou et al 2021 the swot mission to be launched in 2022 will provide high resolution 2d fields of water surface for the very first time alsdorf et al 2007 durand et al 2010 chelton 2019 in the context of this mission important efforts have been done to provide spatially continuous river discharge estimates independently from a dynamical model durand et al 2016 if these estimates can be spatially continuous they are temporally discontinuous spatially and temporally continuous river discharge estimate is however not easy to obtained point observations of the river discharge encapsulate information about hydrological processes upstream the location conditioned by rainfall and topography that can be described by 2d field variables regular spatial interpolation fail to reconstruct the river discharge as neighbors provide no direct information in order to overcome this bottleneck two main strategies have been found in the literature on the one hand krigging based method have been recently developed to interpolate river discharges over the stream using statistical ordinary krigging interpolation farmer 2016 yoon et al 2013 paiva et al 2015 or flow duration krigging interpolation castellarin et al 2018 these methods derive river discharge over a basin relying on the covariance of in situ measurement solely farmer 2016 or more rarely in explicitly adding the river network information paiva et al 2015 but none of these methods account for the water mass conservation on the other hand the 2d field to point nature of the river discharge has been investigated to generate spatially and temporally continuous river discharge fields from discrete observations fisher et al 2020 yang et al 2019 the core of the algorithm is the inverse streamflow routing method pan and wood 2013 that is based on a kalman filter that inverse a routing scheme parameters including the flow direction it allows to reconstruct the runoff field from the river discharge measurements at various time and location in a basin nevertheless this method relies on some model parameter such as flow velocity and cannot be considered independent the goal of this study is to provide and assess a new method for the continuous river discharge mapping from a totally new source of information based on the satellite observation of hydrological variables fig 1 bottom illustrates this innovating approach that uses no model compared to the classical modeling method top using satellite estimates for the three water components i e precipitation evapotranspiration and total water storage change and information on the flow direction allows to balance the water budget spatially between the pixels and to estimate river discharge on a continuous field section 2 details the methodology section 3 describes the materials the quality of the river discharge mapping is quantified through two experiments and evaluated using water surface level and water surface extent in section 4 section 5 illustrates the use of the continuous river discharge estimate to infer mean river height and to analyze extreme events finally the conclusion and perspectives are drawn in section 6 2 methodology for river discharge estimation 2 1 indirect river discharge measurement using water budget since the objective is to estimate gridded river discharge the following formalism uses gridded variables let us consider a whole river basin including n pixels one sub basin e g catchment c o including m pixel is defined from an outlet located on the pixel o o for outlet over the catchment the change in water storage d s is explained by the flux of water entering through precipitation p and leaving through evapotranspiration e or river discharge at the outlet q o no groundwater discharge is considered here the water budget can therefore be used to estimate q o 1 q o i c o p i e i d s i since this equation represents the water mass conservation all values on each pixel have been weighted by the pixel area this estimation stands for any temporal resolution in the following d s is measured by the grace satellite at its native monthly resolution therefore the monthly time step is utilized throughout the paper it is useful for the following to re written eq 1 using a linear operator g applied on all the n pixels of the basin 2 q o g o y with y p 1 p n e 1 e n d s 1 d s n y is the state vector 3 n 1 of p e and d s over the basin with n pixels g o is a 1 3 n vector so that g o i 1 respectively g o n i 1 g o 2 n i 1 if pixel i is in the catchment c o and g o i 0 everywhere else for some locations over the river network the river discharge is measured by in situ gauges let us call q o the available measured river discharge at the location of pixel o the goal of this section is to optimize y to this aim the satellite observations provide an a priori y s a t characterized by the a priori error covariance matrix b b is a 3 n 3 n block diagonal matrix defined as 3 b b p 0 0 0 b e 0 0 0 b d s where b p resp b e and b d s is the covariance matrix of p resp e and d s it has been shown in the literature that the water budget could not be easily closed using satellite estimate y s a t and observed in situ river discharge q o pellet et al 2021b dorigo et al 2021 4 q s a t o g o y s a t q o ϵ q q o the term ϵ q i e the in situ measurement error ϵ q n 0 σ q estimating such measurement error is not an easy task the value σ q 4 is a trade off prescribed by a recent literature review dorigo et al 2021 to account in situ measurement river discharge uncertainty at global scale the previous equation tells how the observation q o can be used to optimized the initial estimate y s a t an optimized solution y a can be found based on the optimal interpolation oi rodgers 2000 such that g o y a q o 5 y a y s a t k q o g o y s a t where k b 1 g o σ q 1 g o t 1 g o σ q 1 and a represents the analysis of the previous solution y s a t with this solution y a closes the water budget over the catchment while being closest to y s a t in a least square sense since eq 5 considers all the pixels at a times the error correlation of the observations need to be considered in previous wb based optimization e g pellet et al 2021b b was limited to a diagonal matrix with prescribed values from the literature or estimated uncertainties from the spread of the observations following yang et al 2019 off diagonal terms have been added to the covariance error matrix b a simple assumption is taken error correlations decay exponentially in space 6 b p i j ϵ p p i ϵ p p j e d i j d where d i j is the distance between pixel i and j and d is the e folding distance in pixel here d 1 2 is chosen this expression extends easily to b e and b d s the error base estimates is taken from dorigo et al 2021 ϵ p 7 ϵ e 5 and ϵ d s 5 no covariance terms have been considered between two different variables i e p vs e p vs d s and e vs d s it is worth noting that accounting for the off diagonal term in covariance matrix b is the first step toward optimizing the state vector y over ungauged basin using the spatial pattern of the variable itself yang et al 2019 and introducing regularization in q the oi framework allows estimating the a posteriori error covariance matrix s a i e uncertainty on y a 7 s a b 1 g o σ 1 g o t 1 2 2 water budget closure at the pixel scale the previous section in particular eq 2 considers the water budget at the catchment scale it is however possible to compute the water budget at the pixel scale but extra information is required because no river discharge measurement is available the flow direction defines the direction taken by the free surface water to leave a pixel it is assumed that rivers flow downhill due to gravity so topography is used to estimate the flow direction 1 d flow direction is used here each pixel has one and only one downstream pixel while o was previously defined as an outlet of a catchment its definition is extended here to any pixel on the river indeed all pixels have a drainage area considering an outlet o and its u nearest upstream pixels the water budget gives 8 q o p o e o d s o u u p s t r e a m o q u the network connectivity matrix f d represents the flow direction a sparse square matrix whose dimension is n n a value of one is used at row i and column j if pixel j flows into pixel i and zero is used everywhere else the row f d i indicates the upstream pixels of i each pixels have only one output with no loop 1 d flow direction the previous equation can be re written using the connectivity matrix f d q o 1 1 1 p o e o d s o f d o q where vector q q 1 q n t is the river discharge vector over all pixels of the basin this last equation will be used in the following as a constrain on q o 2 3 two step mapping scheme the river discharge q o at a location o can be estimated from satellite observation y s a t accounting for the water mass conservation derived in the two eqs 5 and 9 this can be used to estimate the river discharge q across all pixels of the river network the mapping is a two step procedure 1 it first corrects the original satellite estimate of p e and d s at sub basin scale using the gauges observation 2 it then globally optimizes all the pixel fluxes using the flow direction to constrain the inter pixel exchanges using the river network connectivity to interpolate river discharge has already been achieved sauquet 2006 fisher et al 2020 but the key element of the proposed methods here is to use the water budget along with the satellite products for three of the water components in this way the method can be applied at global scale and the resulting river discharge estimate are independent from any model fig 2 illustrates the two step procedure 2 3 1 upstream propagation of in situ gauges the framework detailed in section 3 1 is here extended by the used of p in situ gauges q i to correct the satellite estimate of p e and d s over the respective drainage area using eq 2 we have the linear system which can be written using matrices 9 g y s a t q ϵ with g g 1 g p q q 1 q p ϵ ϵ 1 ϵ p n 0 σ following eq 5 the solution y a is a correction of y s a t to match the river discharge measurement q 10 y a y s a t k q g y s a t with k b 1 g σ 1 g t 1 g σ 1 this first step is close to what has been introduced in pellet et al 2021b a but the state vector is extended here to all the pixels inside the basin not only the spatial average 2 3 2 global pixel wise optimization with flow direction constrain once the three spatially continuous water components have been corrected to balance all in situ river discharge the water budget can be closed at pixel scale to estimate 2d field river discharge following eq 9 we can obtain the equation for river discharge over all the pixels using the water budget and flow direction q c y a f d q i n f d q c y a 11 h q y a with c a sparse n 3 n matrix with c i i 1 respectively c i n i 1 and c i 2 n i 1 and c u v 0 everywhere else c compute the flux difference only on particular location i e pixel contrary to g a l l that consider flux everywhere upstream a location using 12 h c t c c t 1 i n f d eq 12 allows estimating pixel wise river discharge q by balancing the water budget between pixels the global integration ensures the inter dependent correction at pixel scale following the river network connectivity over the entire spatial domain if no a priori information is used for q it is a pure interpolation of the river gauge measurement q though the use of y a if an a priori is used it can be seen as an optimal interpolation in our experiment the use of a priori q a p helps obtaining a stable solution several sources of information exist for the a priori e g climatology modeled runoff if modeled discharges were used this should be seen as assimilation to keep a pure observation based estimate we used the a priori q a p based on original satellite observation 13 q a p g a l l y s a t where g a l l is the matrix g defined in eq 2 extended for the n pixels of the basin the final solution is 14 q q a p k 2 y a h q a p 15 k 2 s a p 1 h s a 1 h t 1 h s a 1 where s a is the a posteriori error covariance matrix obtained at the first step of the procedure see eq 7 s a p is the a priori covariance of q a p to avoid negative river discharges a positiveness constrain is added on q this is achieved using an iterative algorithm for constrained linear least squares problems based on the lagrange multipliers at this stage of the optimization other sources of constraint such as regularization constraints e g tikhonov regularization could be added for estimating q also accounting for dams at pixel level may be introduced as additional constrains to optimize the solution 3 study area and data source 3 1 the amazon basin the amazon basin plays a major role in the global water and energy cycle it is characterized by complex hydrological processes forced by heavy precipitation the surface water creates extensive floodplains under dense tropical forests shapes complex topography and shows large variations through the basin the basin is now facing great climatic risk marengo et al 2018 the satellite observations have played a great role in supporting research in amazon hydrology fassoni andrade et al 2021 in the modeling community satellite datasets are usually adopted as either forcings e g precipitation a priori information to estimate parameter values e g topographic data validation calibration or assimilation data e g discharge river water levels 3 2 dataset used in this study river discharge q monthly water discharge data are obtained from 110 stations collected throughout 2000 2015 at these stations agência nacional de águas ana brazilian national water agency provides accurate river discharge measurements based on observations of water stages that enable representing accurate local rating curves paiva et al 2013 precipitation p the tropical rainfall measuring mission multi satellite precipitation analysis tmpa huffman et al 2007 is a quasi global 50 n to 50 s precipitation product with a spatial resolution of 0 25 and which covers the 1998 to present period huffman et al 2007 the retrieval estimates instantaneous precipitation from microwave imager passive microwave observations as well as infrared data from a geosynchronous earth orbit the version 3b42 v7 used here is obtained by combining satellite estimates with gauge measurements from the global precipitation climatology center schneider et al 2011 2014 using inverse random error variance weighting of the gauge data this product has been evaluated globally huffman et al 2007 su et al 2008 and used in many hydrological analysis the choice of tmpa has been made based on its original spatial resolution that matches the river map grid i e 0 25 see next sub section rather that its performance compared to other well used calibrated precipitation data set beck et al 2017 this has prevented the introduction of additional remapping error due to the spatial interpolation evapotranspiration e the global land evaporation amsterdam model gleam v3b miralles et al 2011 martens et al 2016 uses an empirical energy based equation priestley and taylor 1972 to calculate a reference evapotranspiration value which is converted to actual e based on land cover and an evaporative stress factor this method estimates separately the components of land evaporation transpiration bare soil evaporation interception loss open water evaporation and sublimation it does so by considering four different types of land cover bare soil sparse vegetation dense vegetation and open water in each grid pixel the vb version of gleam used here relies on tmpa as an inputs to produce a daily dataset at a spatial resolution of 0 25 if several e product exist in the literature mueller et al 2011 zhang et al 2017 with broadly equivalent global accuracy michel et al 2016 miralles et al 2016 the choice of the gleam has been made on its spatial resolution 0 25 so that the product matches the river map grid resolution and its temporal coverage of the grace period 2002 2018 gleam provides an et estimate at 0 25 that covered the 2002 2015 period as well as other et estimates total water storage change d s the twin satellites grace tapley et al 2004 offer a unique opportunity to monitor the water stored in land d s estimates are based on grace and grace follow on satellites tapley et al 2004 these estimates include surface water wetlands floodplains lakes rivers and artificial reservoirs soil moisture snowpack glaciers and groundwater the retrieval is based on the mass concentration solution developed by the center for space research csr known as mascon msc the csr msc solution is initially based on a classic sphherical decomposition sh of the inter satellite range rate measurements which is then spatially truncated at the location of the mass concentration save et al 2016 scanlon et al 2016 have shown the many advantages of the grace csr msc solutions relative to traditional sh solutions including reduced leakage perturbation increased seasonal signal amplitude and little or no postprocessing csr msc solution is provided on a 0 25 0 25 grid globally over the grace 2002 2017 and grace fo 2018 present periods 3 3 flow direction based on merit hydro the flow direction used to describe pixel connectivity inside the river system is based on merit hydro yamazaki et al 2019a a global hydrography map raster flow direction map based on the latest water layer data and spaceborne digital elevation models yamazaki et al 2017 the flow direction is 1d meaning that each pixel has only one downstream pixel the flow direction originally at the resolution of 3 is upscaled to 0 25 for running the catchment based macro scale floodplain cama flood model globally yamazaki et al 2011 we used this 0 25 river network map for satellite discharge mapping in this study 3 4 datasets used for evaluation modeled river discharge the simulated river discharges are obtained from the hydrological model cama flood zhou et al 2021a cama flood computes physically based hydrodynamics at continental scale along a prescribed river network based on the merit hydro flow direction map in this study we used the default setup of cama flood version v4 0 as an input runoff for cama flood we used the earth2observe runoff data produced by the land surface hydrological model htessel forced with modified era interim balsamo et al 2009 runoff and discharge are both provided on a 0 25 grid the choice of the modeling dataset relies on previous analysis zhou et al 2021b for the runoff input as well as a fine tuning of the cama flood model by its developers for a better representation of the amazon discharge detailed model settings can refer to zhou et al 2021a cama flood simulates daily river discharge that have been averaged on the monthly basis in order to be compared to our satellite based estimate water level the altimetric dataset is created by santos da silva et al 2014 and used in paris et al 2016 this dataset consists of 920 stations that provide a water height time series over the amazon basin from the envisat 2002 2010 mission envisat provides repeat measurements every 35 days with a height accuracy of a few tens of centimeters details regarding altimetry data processing and quality assessment can be found in santos da silva et al 2014 or paris et al 2016 these estimate are snapshots of the water level which may not be representative of the whole month especially if the observation occurs at the very end or the very beginning of the month on highly dynamic river reaches the evaluation is based on similarities with the estimated river discharge in terms of temporal variation i e correlation surface water extent the global inundation extent from multi satellites giems is a dataset representing the global surface water extent and dynamics over a long period 1993 2007 based on a collection of satellite observations the percentage inundation is estimated on monthly basis over an equal area grid 0 25 at the equator prigent et al 2007a 4 river discharge assessment the main objective of the methodology is to obtain an estimate of the river discharge that matches in situ measurement while being spatially coherent with the satellite observations it is therefore a compromise found between all the sources of information and constrain three metrics are used for the evaluation the correlation of anomalies the nash sutcliffe efficiency nse and the kling gupta efficiency kge nse of 1 indicates that the river discharge estimates matches perfectly the observations nse of 0 that the reconstruction is as good as the observation mean and nse lower than 0 that the reconstruction is a worse predictor than the observation mean similarly kge of 1 indicates perfect agreement between reconstructed river discharge and observations but no specific meaning attached to a kge of 0 a recent benchmark highlights that kge lower than 0 41 indicates performance equivalent to the observation mean knoben et al 2019 4 1 evaluation of the method in interpolation and in extrapolation modes in order to test the robustness of the estimation a leave one out loo validation technique is applied this technique is widely used when few data here the gauges measurements are available from a real word problem in this section the methods is tested in two various modes and compare to the cama flood simulation it should be noted that the cama flood simulation does not use any in situ discharge in assimilation mode we therefor compare our methods with a baseline model not with a reanalysis in the first experiment one gauge is withdrawn from the ensemble in the mapping procedure the obtained river discharge estimate is then compared to the gauge measurements on its location this operation is repeated for all the available gauges this experiment evaluates the capability to accurately estimate river discharge over a given pixel in testing the interpolation between two in situ stations based on the satellite observations table 1 summarizes the performance in the unseen gauges data it compares the interpolation to two other estimations the a priori estimate q a p based on the original satellite observation solely eq 13 and simulated river discharge from the cama flood model the a priori estimate q a p shows reasonable median nse 0 46 it can therefore be used as a good first guess for large river with unknown discharge its performance is still limited by the raw satellite products errors while obtaining very good performance over the big rivers the simulation q c a m a accuracy is degraded over small confluents not shown overall the simulated river discharge shows a better median nse 0 58 than q a p the nse obtained for the interpolation q l o o is greatly higher in the loo experiment 0 84 than for the two other estimates the mapping satisfactorily matches in situ measurement when neighboring gauges are used the correlation and kge median follows the same conclusion than nse for the three estimates nse and kge are slightly lower than the correlation with a larger spread among the in situ gauge ensemble not shown stressing bias or variance issues kge as well as anomalies error nse issues on some locations in the second loo experiment several gauges located on the same stream have been removed at once the objective is to estimate an entire stream using only its most downstream in situ station in the integration this can be seen as testing the capability of extrapolating the upstream river discharge over a basin not well constrained by gauges data the kge efficiency metric is shown in fig 3 for this experiment and for the model our estimate can reproduce satisfactorily river discharges over an entire stream based on only one station of the stream mean kge is 0 6 not surprisingly a negative gradient can be observed for the kge index along the streams stressing the difficulty to estimate river discharge on remote upstream stations this shows that the higher the number of in situ river discharge measurements used in the integration the better our gridded river discharge will be this is to be expected the methodology is able to exploit these points estimates and the satellite corrected observations are used to interpolate extrapolate these values overall the proposed integration scheme allows for a better estimation of the monthly river discharge on small confluents than the cama flood model particularly on southern rivers where the model outputs are characterized by a strong bias 4 2 evaluation using wsl from satellite altimetry altimeter measurements are used to estimate the wsl that can be converted into discharge via a rating curve equation paris et al 2016 see benefits of the methodology in ungauged rivers in paris et al 2020 and andriambeloson et al 2020 we evaluate here our gridded river discharge estimate by comparing them with the wsl observation from envisat fig 4 shows the correlation of anomalies between them over the envisat virtual stations climatological monthly mean over 2002 2010 have been removed from the two datasets to truly measure the link between these two quantities the correlation of anomalies goes from 0 3 in the most upstream area to 0 9 over the largest rivers negro solimoes amazon the overall agreement is higher in the northern western part of the basin the correlation slightly decrease in the most downstream of the large tributaries e g purus madeira this is due to the backwater effect from the mainstream when water level of mainstream is high discharge from tributaries is halted because it cannot flow into the mainstream this process cannot be represented by our method nor be resolved by satellite datasets as the relationship between wsl and river discharge is non linear but defined as a power law equation nor always univocal this simple evaluation is not perfect and will not permit to detect biases or over evaluation of the amplitude of variation of the river discharge but the agreement in terms of anomalies between them is promising and shows a potential synergy for the use of these two sources of information the degradation of the correlation near the confluences of the main rivers and the amazon river evidences this not univocal relationship and the limitations of this analysis 4 3 evaluation using surface water extent river discharge and flood extent are highly correlated in the amazon basin where the floodplain is much more caused by the elevation of the river than any other processes e g precipitation fed flooding or paddy irrigation the giems database provides an estimation of the surface water extent from 1993 to 2007 on a 0 25 resolution equal area grid at the global scale prigent et al 2007 fully assessed over the amazon basin papa et al 2013 it is possible to analyze the relationship between river discharge and surface water change over the giems at pixel scale over the 1993 2007 period correlation left and correlation of anomalies right are jointly represented fig 5 for all the pixels included in giems since rivers can only explain a small part of the surface water extent at 0 25 we did not expect a perfect match between these two variables the correlation is high for the main streams inter annual variability can be measured by seasonal anomalies their correlations anomalies in fig 5 bottom are significant in the central amazon upstream branco and madeira river this indicates that these pixels store much of the surface water at their surface this suggests that water balance can be useful in estimating pixel wise river discharge to investigate wetland hydrological regimes in areas with significant canopy coverage the comparison between q and water surface change is not a direct evaluation but the fact that there was coherence between the two completely independent measurements related to the water cycle demonstrates the strength of the river discharge reconstruction 4 4 comparison to the cama flood hydrological simulation in this section the pure observation based gridded river discharge estimate is compared with simulated river discharge cama floods model has been chosen here it is based on the same flow direction map merit hydro as the integration framework this avoids differences related to a different river network three types of comparison are shown spatial pattern river segment comparison and pixel time series comparison the spatial pattern of correlation correlation of anomalies and kge index between simulated and observation based river discharge are shown on the right panel of fig 6 the correlation coefficient is high over all the rivers 0 6 floodplain areas are characterized by a negative correlation between the two datasets while cama flood model can represent floodplains hydrology and in particular the backwater effect discharge is halted or reversed from the mainstream to surrounding tributaries due to its higher water level improvements are still needed to account for these processes in the integration framework when looking at the kge the overall proximity of the two datasets is clear but a difference with the correlation coefficient over the altamira river basin most southern western basin of the amazon is noticeable contrarily to what was seen on the correlation map this means that there is a large bias or variance difference see next paragraph for the pixel scale comparison on the map of the correlation of anomalies two river paths are shown in red through the madeira from a and the negro from b rivers the long term mean is shown along the path on the left bottom panel of fig 6 this evaluation allows to identify larger differences in the long term mean river discharge due to particular confluence rivers even if the simulated discharge is close to the observational based estimate at end of the water path defined from a on the madeira river differences can be high and compensate each other along the path over the river path on the negro river defined from b the two estimates show a continuous bias all along the second part of the path comparison between simulated and gridded observation based discharge can be achieved at pixel scale by comparing their time series two pixels represented by a rectangle on the background map have been chosen here and their location are indicated in the plots the outlet of the altamira has been chosen to discuss the bias issue pointed out by the previous intercomparison of the correlation and kge a scaling error of cama flood impacts high flow simulation for this river over the negro river for particularly dry year a bias at low flow can be observed in the simulation 2003 and 2005 this could be related to runoff input bias and or soil water content issue in the model fig 6 illustrates the opportunity to use the pure observation based representation of the river discharge to improve the model simulations 5 potential applications 5 1 spatially distributed mapping of extreme events the river discharge integrates all the surface and underground water related processes the analysis of extreme events allow to characterize the behavior of a river system under particular stresses the better the characterization of past events the better the forcasts under climate change will be such analyses therefore plays a key role in the hydrology using modeled river discharges is often limited in term of accuracy due to significant model biases so our river discharge mapping is an interesting alternative to study extreme regimes fig 7 top represents the climatological anomalies at the outlet 1 4 52 1 of the amazon basin important floods 2006 and 2009 and droughts 2005 and 2010 picked by the outlet river discharge and reported by the literature have strong positive and negative pattern of their seasonal anomalies fig 7 bottom it is difficult to choose appropriate scale to analyze river discharge at pixel scale as the variability is huge from a small to large river the anomalies are computed as percentage of the climatological mean for each pixel during the drought in 2005 the entire river system displays strong negative anomalies rare upstream rivers have positive anomalies the spatial pattern for negative value is vaster than for the drought in 2010 for which southern rivers xingu and tapajos are not dried comparing these two drought events it can be seen from the outlet river discharge anomalies top that the temporal scale is shorter in 2006 than in 2010 the drought in 2010 is stronger not particularly in space but in time with a long period of value from march to september the amazon mainstream was impacted mostly in september and october 2010 with discharge anomalies exceeding 30 the drought mainly impacts first southern and eastern then central regions of the river system during floods in 2006 the high anomaly appearing on the time series top is mainly due to the strong positive anomalies visible in the northern basins negro and bianco rivers of the amazon on the map bottom note that the southern basins are not impacted by this flood the outlet is finally impacted because the northern basin provides the largest amount of water to the mainstream and its discharge is very sensitive to the discharge of these confluent rivers in 2009 the flood peaked is in march the spatial pattern of this flood is wider than in 2006 only xingu river does not show positive anomalies and solimoes river have a positive value in the very upstream area floods and droughts reported by literature are analyzed at stations scale if co investigation of stations located on different confluent give insight on the spatial scale of the event wongchuig correa et al 2020 the analysis of extreme events benefits here from the spatially continuous aspect for the gridded river discharge estimate droughts are characterized by a wider spatial scale than floods in impacting most of the river system floods can be caused by one particular confluent the northern ones gridded river discharge estimates show the benefit of observation based spatial analysis of extreme events which have previously only been conducted using lsms the use of p e and d s observations estimate to reconstruct r provides realistic estimates of discharge anomalies 5 2 mean river depth estimation the results in section 4 2 have shown that envisat altimeter observation and the gridded river discharge estimate agree very well it represents a unique opportunity to use these two types of information for a pure observation based hydrological analysis one of the critical parameters of a hydrological model is the river bed elevation river bed elevation along with river width define the river storage capacity and finally the simulated river discharge while some remote sensing based products exist for the river width yamazaki et al 2014 river bed information is often derived from a simple parametrized power law equation here we estimate the mean water depth in a two steps method following paris et al 2016 first we estimate the rating curve parameter z 0 a b that link the river discharge q to the altimetric wsl h for a particular location 16 q t a h t z 0 b the three parameters are found using a nonlinear curve fitting data fitting solver in a least squares sense when the parameter z 0 is found the mean water depth is derived from the mean altimetric water surface level and z 0 parameter h e h t z 0 the fig 8 shows the river bed estimate at the altimetric virtual station location using the observed based gridded discharge estimate compared with the original paper paris et al 2016 the river channel depth parameter used in cama flood zhou et al 2021a is shown for illustration only as it is defined as the depth below the bankfull elevation and cannot thus represent the identical variable as the two other estimate of mean water depth so no direct comparison can be made between cama flood channel depth parameter and estimated mean water depth nevertheless this illustrates the potential benefit to use continuous river discharge estimate with altimetric wsl to better parametrize hydrological model while the two altimeter based estimates show a deeper river in the central amazon river confluent of the negro in the amazon at manaus cama floods parameter presents a gradient following flow accumulation pattern the critical difference between paris et al 2016 and our analysis is the source of information for the discharge paris et al 2016 have used simulated river discharge at the virtual station to find the rating curve parameter z 0 a b in our experiment the river discharge is purely estimated using satellite products if the two river bed elevation patterns agree the observation based discharge results in a deeper river upstream in the solimoes river the use of observed discharge ensures that no model bias is propagated to the river bed estimate such river bed elevation derived from altimeter and gridded river discharge is an opportunity to calibrate models ensuring the consistency between such information and altimeter river height that is the only observation assimilate into the hydrological model so far 6 conclusion a totally new integration framework is proposed to estimate a spatially continuous mapping of monthly river discharge using the water budget equation with satellite estimates of the precipitation evapotranspiration and water storage change a few in situ river measurements and the river network information the river discharge estimated over the amazon system has been validated against in situ gauge measurements and evaluated against two related water variables the surface water elevation and the water surface extent and compared to the cama flood simulation the overall performance of our new product is promising it is a step forward to quantify river discharge over sparsely monitored basin from the application the spatial patterns associated to extreme events are well represented by the gridded river discharge allowing discrimination between large scale versus localized extreme event are the main floods related to one particular confluent do droughts always impact all river at once these question could be investigated using gridded river discharge over longer period as a new source of information for hydrological studies the river discharge mapping can be used with the altimetric wsl data to create pair of river discharge and wsl this synergy could leverage bathymetry estimation in section 5 one application is showcased in estimating the mean water height based on the rating curve as stated before envisat provides repeated snapshot of the water surface elevation the difference of temporal resolution between monthly river discharge and snapshot of water surface elevation might limit the rating curve but raises the opportunity to temporally downscale the gridded river discharge using wsl data it could be possible to translate instantaneous wsl measurement into instantaneous discharge after calibrating the rating curve at monthly scale our estimate independent from any dynamical model could benefit to the river models in calibration validation experiments in particular this pure observation based representation of the river discharge will be useful to evaluate model simulations to quantify their biases and allow for their calibration it should also highly facilitate the assimilation in a single surface variable i e the river discharge a complex integrated satellite observation such as grace data that is not routinely been assimilated as already stated the cama flood simulation used as a comparison in this paper does not use any in situ discharge a comparison to the assimilated discharge product rather than open loop simulations is needed however assimilation for large scale river model is still underdevelopment and there is no available assimilated discharge products at the stage of this research currently the availability of in situ measurement is still critical for the accuracy of our product a better characterization of the satellite product uncertainties over particular regions should allow for a better estimation of the river discharge from the satellite observation another limitation is the size of the catchment due to the coarse resolution of grace the mapping strategy is suitable for large catchment with enough spatial information from grace furthermore due to the high uncertainty of the water component over the mountain and snowy areas the mapping might not be suitable for these regions our mapping method reconstructs a distributed representation of discharge ensures the hydrological coherency over a full river system while being independent from model but the river network map itself indeed our method is dependent to the conceptualization of real rivers with approximation such as one downstream direction assumptions this is still different from the reality and contains some uncertainties nevertheless this representation can be considered as a complementary pillar to modeling for a better knowledge of hydrology starting from observations this data driven approach can be seen as a simple model for along river network water budget closure it allows building a representation that takes into account the impact of human activity such as irrigation or dam management that can be sensed from space through evapotranspiration and total water storage change estimates for this perspective merging statistical relationships along with simple physical constrain into a hybrid data driven model allow alternative from the classical modeling as any data driven method the river discharge mapping is not exempt of uncertainty in particular the uncertainty on p and e can translate into an uncertainty in r decharme and douville 2006 nasonova et al 2011 gelati et al 2018 zhou et al 2018 david et al 2019 following what has been done in the modeling community concerning the runoff error propagation analysis schellekens et al 2017 a perspective of the study is to assess the river mapping with uncertainties subject to different combinations of water component inputs to investigate the error propagation mapping the river discharge with various inputs to estimate a pdf of the river mapping and a proxy of its error from the spread this should be consistent with the model inter comparison framework getirana et al 2014 and allow comparison between model and the proposed satellite based mapping uncertainties another source of improvement is to better estimate input uncertainties with the data producer and use these uncertainties in the mapping strategy instead of the prescribed value and reduce bias issues through an optimized merging of several datasets for each water component under such hypotheses final uncertainty on the river product can be derived too through the oi strategy the temporal resolution of the gridded river discharge estimate will be improved in the future using sub monthly solutions of the grace d s e g the cnes grgs grace solution at 10 day biancale et al 2005 or daily resolutions solutions ramillien et al 2020b kvas et al 2019 to benefit from the swot mission alsdorf et al 2007 durand et al 2010 chelton 2019 the next step is to obtain a global pure observation based continuous and sub monthly river discharge estimate a future task is to integrate our river discharge estimate into the upcoming swot based river discharge retrieval durand et al 2016 our methodology could also help the swot mission by extending its estimation of the river discharges back in time by a few decades by using a reference over the swot era to obtain reliable river discharges over the grace period 2002 2017 furthermore the assimilation experiments currently developed for swot yang et al 2019 emery et al 2020 could benefit from independent river discharge estimations credit authorship contribution statement victor pellet conceptualization methodology software writing reviewing filipe aires conceptualization writing reviewing dai yamazaki visualization investigation editing xudong zhou visualization investigation editing adrien paris visualization investigation editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments victor pellet is founded by the centre national d etudes spatiales cnes postdoctoral fellowship the author would like to thank ayan santos fleischmann for providing the ana river discharge measurements the authors thank the two anonymous reviewers who help to improve this manuscript 
2816,quantification and assessment of large wood lw accumulations in fluvial systems is still considered difficult due to the complex nature of wooden deposits although knowledge about volumetric measures and porosity parameters of lw accumulations is crucial for the prediction of hydraulic and geomorphic effects it has not yet been possible to obtain accurate measurements these limitations are mainly based on a lack of applicable sensing technologies available in the past in the present study a close range aerial surveying technique structure from motion sfm photogrammetry is applied for generating 3d replicates digital twin models of wooden deposits enabling their volumetric assessment in addition manually conducted volumetric measurements of corresponding prototype lw accumulations help to improve and calibrate the sfm derived estimates for the first time precise porosity parameters for lw accumulations ranging from 52 5 to 83 2 are provided in addition a novel parameter the packing arrangement is used which describes the structural alignment of individual elements in the lw accumulation and benefits porosity estimates based on the applied 2 5d and 3d photogrammetric approach accordingly randomly and loosely organised lw accumulations allow for a high penetration depth of the 3d approach resulting in a more accurate estimate of the actual porosity as the 3d volumetric estimate approaches the solid wood volume of the corresponding lw accumulation an empirical approach has been developed for future approximation of lw accumulation porosity without the need of knowing the solid wood volume with the present work a significant improvement of our understanding in employing a non intrusive sensing technique is provided linked with manually conducted field measurements of the solid wood volume of lw accumulations our study contributes to an improved data acquisition and processing plan which represents a further important step towards a systematic assessment framework that is urgently needed by river managers and engineers to better evaluate and manage lw in fluvial systems keywords accumulation volume close range sensing digital twin models large wood lw wood jam porosity data availability data will be made available on request 1 introduction 1 1 surveying large wood lw in rivers the need for developing innovative surveying methods and more refined assessment tools is continuously addressed throughout large wood lw research harmon et al 1986 huang et al 2009 vaughan et al 2009 ruiz villanueva et al 2016b lw assessment evolved from first visual estimates and manual measurements in rivers van wagner 1968 tritton 1981 over the categorisation of wooden deposits in fluvial systems e g key member individual lw piece framework mix of lw pieces lw accumulation mix of lw and finer materials with open space between wood pieces and complete jam compressed lw accumulation showing little open space between wood pieces by manners and doyle 2008 all the way to automated image and video monitoring of transported and deposited lw in rivers macvicar et al 2009 ghaffarian et al 2020 as well as the detection and quantification of lw in forests using highly advanced sensing techniques huang et al 2009 milani et al 2018 huang et al 2019 although major progress has been made in estimating potential wood volumes available for recruitment rickenmann 1997 steeb et al 2017 and developing as well as applying mapping techniques for characterising lw in rivers piégay and marston 1998 galia et al 2018 quantification measures of wooden deposits are still severely limited at field scale wohl and scott 2017 and associated with great uncertainties livers et al 2020 these limitations are mainly related to difficulties in data acquisition which often contain unintentionally influenced human impaired measurements e g bulk volume assumed parameters e g density porosity or rough transect estimates which all reflect on the quality of gained results in recent years a range of non intrusive surveying tools such as laser scanning and structure from motion photogrammetry sfm have strongly developed and increased in popularity for the application in geoscience westoby et al 2012 anderson et al 2019 pyörälä et al 2019 the emerging technologies have also created new opportunities and interest for digital mapping and budgeting of lw in fluvial systems pioneer studies used multispectral imagery polewski et al 2021 laser scanning tonon et al 2014 magnussen et al 2018 yrttimaa et al 2019 fayad et al 2021 as well as sfm photogrammetry sanhueza et al 2019 spreitzer et al 2019 windrim et al 2019 for mapping wooden deposits and determination of specific characteristics e g log geometries and volume both laser scanning as well as sfm photogrammetry are capable of producing highly accurate 3d replicates digital twin models of topography goetz et al 2018 backes et al 2020 and objects cunliffe et al 2016 sylvain et al 2019 moyano et al 2020 swayze et al 2021 given that the surveyed scene is behaving static mind vegetation in windy conditions or turbulent water accordingly sfm photogrammetry represents a well suited alternative to the more expensive laser scanning technology wallace et al 2016 anderson et al 2019 as well as to traditionally applied lw mapping methods such as the parallelepiped approach thevenet et al 1998 boivin et al 2017 discrete point and transect sampling böhl and brändli 2007 ritter and saborowski 2012 or manual measurement of individual lw elements wyżga et al 2015 tonon et al 2018 with the transition from traditionally applied lw mapping practices over to better performing techniques such as sfm photogrammetry a whole new level of detail can be acquired within a single field survey potentially benefitting lw research substantially 1 2 structure from motion sfm photogrammetry sfm photogrammetry can infer a 3d point cloud from overlapping 2d images carrivick et al 2016 the generated point cloud is meshed meaning that each point vertices is connected to its neighbouring points forming a continuous wire mesh of defined geometric shapes edges while the resulting surface of all shapes faces yields a first replicate of the mapped reach or object digital twin model the suitability for sfm photogrammetry in geoscience is benefiting from rapid advances in drone technology the acquisition of ground control points gcps for georegistration and model scaling represents a large barrier for drone based photogrammetry both from a cost and time perspective carbonneau and dietrich 2017 however for the assessment of transient lw accumulations the acquisition of gcps can be waived while chequerboards or alternatively gps data from the drone s flight record file can be used for model scaling dji 2021 spreitzer et al 2021a this approach is particularly useful for one time data acquisition and modelling as often needed in lw research sanhueza et al 2019 spreitzer et al 2019 to date a variety of sfm photogrammetry and post processing software packages are either freely or commercially available each of the software packages offer individual tools that are needed for the generation of most practical lw accumulation models for this reason a series of software packages are currently required for the digital assessment of lw accumulations spreitzer et al 2020a however generating digital twin models of prototype wooden deposits shows great potential for improving our current knowledge about the quantification of the lw accumulation characteristics and their effects of lw accumulations on both flow hydraulics and sediment transport processes 1 3 lw accumulation assessment highly resolved point cloud and mesh models can be used for the computation of lw accumulation volume and porosity depending on sorting and packing matrix spreitzer et al 2020a of wooden elements in a lw accumulation livers et al 2020 spreitzer et al 2020b the use of two volumetric measures may be necessary the computation of the 2 5d volume provides an initial volumetric measurement of lw accumulations sanhueza et al 2019 spreitzer et al 2019 determination of 2 5d stockpile volume via dense point network rather than single transects or cross sectional profiles represents a well established method to date draeyer and strecha 2014 the 2 5d volume computation is based on the vertical projection of a surface model onto a 2d raster grid also known as base surface remondino 2003 sanhueza et al 2019 the finest possible raster grid distance is defined by the ground sampling distance gsd which represents a measure for model resolution for applications in mining and construction but also in lw research 2 5d volumetric measurements from sfm photogrammetry were shown to produce highly accurate measurements compared to real metrics reflecting on errors of 1 in the generated surface model tucci et al 2019 spreitzer et al 2020a however there are challenges arising with the 2 5d approach in particular for the assessment of poorly sorted and loosely packed lw accumulations where voids at the surface and space beneath protruding elements are considered as lw volume and thus not taken into account as void space spreitzer et al 2019 by contrast the volume computed from 3d models accounts for all accessible void space from the outside showing a higher penetration depth and thus returning a more accurate lw accumulation volume than the 2 5d approach spreitzer et al 2020a however with decreasing penetration depths well sorted densely packed the 3d approach also considers the occluded void volume at the inside of the lw accumulation the difference between 2 5d and 3d volumetric measurements resolves void space that is accessible from the outside and may be of use as a measure for the performance of the 3d approach e g penetration depth as well as the structural alignment of individual wooden elements within a lw accumulation recent studies applying aerial sfm photogrammetry sanhueza et al 2019 windrim et al 2019 and close range sfm photogrammetry spreitzer et al 2019 for the quantification of lw emphasised the great potential and promising perspectives for the application of an image based surveying technique in lw research previous studies verified the sfm photogrammetry approach by means of scaled wooden dowels and organic fine material ofm in a laboratory setup and concluded that sfm photogrammetry represents a cost and time effective yet powerful tool for lw accumulation assessment spreitzer et al 2020a spreitzer et al 2020b sanhueza et al 2019 presented a sfm approach based on nadir images and reported limitations for accumulation assessment in the presence of living vegetation which may cover parts or entire lw deposits however these limitations can be overcome with a set of close range nadir and oblique imagery spreitzer et al 2019 spreitzer et al 2020a sfm photogrammetry could therefore be an applicable technique for assessing accumulation characteristics such as volume composition structural alignment and porosity which cannot be captured at satisfactory quality to date 1 4 challenges and objectives information on lw characteristics governs how lw affects both flow conditions and sediment transport processes thereby affecting potential resulting flood hazards due to lw the lack of applicable sensing technologies and assessment methodologies impedes the accurate determination of lw characteristics wyżga et al 2015 livers et al 2020 with the development of innovative sensing technologies new opportunities arise for an improved lw assessment apart from recent laboratory studies spreitzer et al 2020a spreitzer et al 2020b no reference data e g solid cubic meter volume of examined lw accumulations are available such reference data are urgently needed for calibration of the computed data gained from the sfm photogrammetry technique obtained 3d volumetric data for example still account for all void spaces of the wooden deposit that are not accessible from the outside thus actual volumetric measurements of prototype wooden deposits are required to better control for generated data from aerial and close range sfm photogrammetry surveys of lw accumulations however there are several challenges involved in obtaining precise volumetric measurements of prototype lw accumulations at field scale due to temporal and spatial constraints of field sites and logistical efforts the present study focuses on the manual quantification of the solid cubic meter volume of lw accumulations at field scale to calibrate computed lw volumes from the sfm photogrammetry method in addition the structural alignment of individual elements in the lw accumulation is estimated to further advance the assessment of lw accumulations furthermore field measurements of solid cubic meter volumes are needed to calibrate a correction factor based on the structural alignment of the wooden deposit that translates sfm derived volumes to porosity this study will help to improve the rapid assessment of lw accumulations at field scale the applied quantification method and results for lw accumulation volume and porosity may then be of help for establishing relations between flow sediment wood interaction processes and benefit river restoration as well as flood protection projects 2 methodology mapping of lw accumulations in the field was conducted using close range aerial sfm photogrammetry the image based methodology enables the non intrusive assessment of prototype lw accumulations extendingpreviously conducted laboratory experiments spreitzer et al 2019 spreitzer et al 2020a for calibration of the produced digital twin models with regard to the gained volumetric results the prototype lw accumulations were either weighed or processed to wood chips after digitalisation to manually obtain the corresponding solid cubic meter volume which in the present work is referred to as the solid wood volume samples of the prototype lw accumulations and wood chips were taken from the field and brought to the laboratory of hydraulics hydrology and glaciology vaw for density analysis based on the density analysis in the laboratory and obtained measurements in the field the solid wood volume could be estimated 2 1 close range aerial surveying imagery of selected lw accumulations was acquired using a dji phantom 4 pro p4p drone the p4p drone is equipped with a 1 cmos image sensor which was employed at an aspect ratio of 3 2 5472 3648 pixels in order to exploit its highest possible resolution of about 20 mp focal length was 8 8 mm representing a 24 mm lens with regard to the 35 mm format equivalent p4p 2017 and resulting in a field of view of 84 which is well suited for close range sfm photogrammetry in terms of capturing great detail without image distortion and producing satisfactory overlap for sfm photogrammetry furthermore the p4p allows gimbal tilt of the camera within 120 30 up and 90 down facing pitch p4p 2017 enabling the view below structures e g at bridges and occlusions in lw accumulations the average flight time for moderate breeze wind conditions i e 25 km h and temperatures above freezing is 25 min in this study close range aerial imagery was acquired along a manually controlled flight trajectory comprising a dense dataset of manual shoots from oblique and nadir perspectives the flight trajectory was significantly shaped by nearby structures and riparian vegetation which had to be evaded camera orientation pitch was adjusted during the flight in order to maintain shallow depth of field while keeping focus on the lw accumulation to be surveyed at the time of image capture the drone was operated hovering still to avoid the effect of rolling shutter the entire data acquisition for close range aerial sfm photogrammetry was conducted in 25 min per prototype lw accumulation for scaling purposes chequerboards with a raster grid of 30 mm were placed clearly visible on and around the lw accumulation prior to the flights 2 2 point cloud and mesh generation after image acquisition in the field sfm photogrammetry was applied for generating 3d digital twin models from heavily overlapping 2d images of the prototype lw accumulations a commercially available sfm software package pix4dmapper 2021 was used on a standard desktop pc cpu intelxeon e3 1245 v5 3 50 ghz ram 16 gb gpu nvidia geforce gtx 1050 ti os windows 10 64 bit pix4dmapper offers a well established processing pipeline for point cloud and mesh generation poisson surface reconstruction by kazhdan et al 2006 as well as 2 5d volume computation draeyer and strecha 2014 the computation of the 2 5d volume requires the set of a triangulated base surface which was adjusted according to observations in the field to best fit the layer between ground and lw accumulation the present study solely considers wood volume above the base surface cut volume while computed volume below the base surface fill volume is not considered for lw accumulations resting on a coarse channel bed the base surface was aligned with the upper level of the coarse gravel particles to rather compromise on a small fraction of lw volume than cutting into the ground and mistakenly considering soil material as wood volume the processing pipeline for obtaining both 2 5d and 3d volumetric measures required a minimum of three software packages namely i pix4dmapper 2021 version 4 5 6 ii autodesk meshmixer 2017 version 3 5 474 and iii meshlab version v2020 07 by cignoni et al 2008 while the 2 5d volume can be computed directly from pix4dmapper after digital surface model dsm generation some further processing steps are required for computation of the 3d volume as a watertight 3d model is needed pix4dmapper produces highly accurate meshed surface models using screened poisson surface reconstruction psr kazhdan and hoppe 2013 although the surface of the generated mesh in pix4dmapper is watertight no 3d volume can be computed straight from the output mesh as the bottom of the surface model is not closed yet in order to close the bottom of the surface model and produce a fully enclosed and watertight manifold 3d model the mesh is exported to autodesk meshmixer 2017 the inspector tool provided in meshmixer offers an effective method for repairing irregularities non manifold edges or cracks in the mesh as well as for closing the bottom hole of the surface model in connecting the mesh boundary via a smooth triangulated mesh to produce a manifold 3d mesh model in parallel to the inspector tool the outline of holes can be selected and closed manually with the erase fill tool returning highest possible model resolution alternatively the make solid tool can be applied under consideration of some compromises to be made with respect to model resolution and face count as the tool allows for the computation of a maximum number of roughly 1 m faces once the manifold 3d mesh model is created the digital twin models were exported to meshlab where the models were filtered for isolated pieces 10 and 3d volumes computed the detailed processing pipeline for pix4dmapper meshmixer and meshlab is shown in fig 1 2 3 prototype lw accumulation measurements the present study acquired data from three field sites in switzerland lw accumulations at the bacino di val malvaglia bdvm hydroelectric reservoir were surveyed along with wood deposits at two lw retention racks at the river grosse melchaa gm and at a bridge pier on the site of the hydroelectric power plant tavanasa operated by axpo power ag at the river anterior rhine ar all of the catchments are heavily forested with a mix of european spruce picea abies common beech fagus sylvatica and black alder alnus glutinosa 2 3 1 analysis at a hydroelectric reservoir bacino di val malvaglia the bacino di val malvaglia bdvm reservoir is located in the canton ticino in southern switzerland 46 25 28 n 9 01 18 e 993 m a s l and operated by the officine idroelettriche di blenio sa the reservoir is fed by the orino river a 15 km long and steep alpine stream with its source at 2 770 m a s l schweizerfluss 2021b a significant amount of lw accumulated at the bdvm dam and had to be removed from the reservoir using a mobile crane fig 2 a and b for the duration of the clearing works the reservoir surface was impounded to the maximum water level to ease the extraction of lw the cleared material was transported to a site nearby the dam fig 2c and d at the bdvm reservoir fig 2 a set of three differently sized lw accumulations were prepared on a flat surface to acquire imagery for close range aerial sfm photogrammetry of a small lwa1 medium lwa2 and large lwa3 lw accumulation fig 3 a d and g the datasets comprised 326 lwa1 416 lwa2 and 507 lwa3 images for generating digital twin models table 1 which enabled 3d fig 3a d and g and 2 5d fig 3b e and h volumetric analysis once digital mapping of the lw accumulations was completed each of the lw accumulations fig 2e was processed to wood chips fig 2f the piles of wood chips where then individually mapped via close range aerial imagery for stockpile analysis fig 3c f and i image count for the small medium and large stockpile models of processed wood chips amounted to 186 109 and 131 site photos respectively the assessment of stockpile volume using the 2 5d approach represents a well established method to obtain bulk volume measures draeyer and strecha 2014 liu et al 2020 son et al 2020 samples of wood chips and a variety of wood samples ofm small twigs and branches lw were brought to the laboratory for determination of bulk density and solid wet density multiplying the obtained bulk volume with bulk density results in the mass of the wood chips the solid wood volume is then estimated from the ratio of wood mass to mean solid wet density 2 3 2 analysis at lw retention racks river grosse melchaa the river grosse melchaa gm is located in central switzerland canton obwalden from the source at 1 025 m a s l the gm finds its roughly 18 5 km long way through a forested catchment and a 100 m deep gorge ranftschlucht down to its mouth at 469 m a s l schweizerfluss 2021a upstream of lake sarnen a series of lw retention racks are installed at the outer bend of the channel the vertical retention racks are part of the flood mitigation concept grosse melchaa chalcheren pieren et al 2009 and have to be cleared after floods in the present study a small lw accumulation at a retention rack located about 600 m downstream of the gorge fig 4 a 46 53 27 1 n 8 15 38 3 e 486 m a s l and a larger amount of lw at the retention rack immediately downstream of the gorge fig 4b 46 53 10 4 n 8 15 42 9 e 496 m a s l were digitally mapped removed and analysed for solid wood volume the collected datasets from close range aerial mapping comprised 394 site photos of the first lw accumulation lwa4 fig 4a and 328 site photos of the second lw accumulation lwa5 fig 4b representing satisfactory image count for sfm photogrammetry and model generation fig 5 a mid sized forwarder johndeere 1210g 2019 was employed for clearing deposited wood from the lw retention racks fig 4c the available load space of the forwarder incl subdivision board for reducing the v shape was given with 4 5 2 7 1 4 m lxwxh the forwarder load from lwa4 was estimated to 3 m3 stere volume pile volume for lwa5 two forwarder loads with an estimated 8 and 5 5 m3 stere volume were removed for later estimation of the solid wood volume the stere volume measurement from the forwarder is reduced by a factor of 0 7 resulting from the swedish nation board of forestry stacked measuring guidelines for debarked wood vmf nord 1999 and commonly used for estimating solid cubic meter from stere volume vmf nord 1999 pásztory et al 2018 united nations 2020 the accumulated lw was transported to an accessible site near the gm where samples were taken for solid wet density analysis in the laboratory while the remaining material was processed into wood chips the wood chips were directly stored on a container truck fig 4d with known dimensions of 5 5 2 4 2 5 m lxwxh while the weight of the shredded wood was determined by an on board scale in addition to the bulk volume and mass of the wood chips the mean solid wet density was used for the estimation of the solid wood volume 2 3 3 analysis at a bridge pier river anterior rhine at the site of the hydroelectric power plant tavanasa a bridge with two pier walls 0 6 m wide and 5 1 m long at a distance of approximately 15 m from each other crosses the river anterior rhine ar 46 45 05 n 9 02 45 e 800 m a s l lw accumulated at both pier walls while the lw accumulation at the right pier wall was about three times bigger than the lw accumulation at the left pier wall due to the limited accessibility of the lw at the left pier wall located in the active channel at the time only the lw accumulation at the right pier wall could be recorded and analysed in the present study the considered lw accumulation fig 6 a c measured roughly 3 5 m in height and was tightly wrapped around the pier wall based on the close range aerial imagery a total of 682 images were acquired from a single drone flight and used for sfm photogrammetry fig 7 data acquisition also considered the collection of images below the bridge deck meaning that the drone flight took place under the bridge to capture and resolve the entire lw accumulation at best detail the determination of the solid wood volume of the lw accumulation lwa6 fig 6a c required all wooden material to be cleared from the bridge pier a container truck with loader crane and on board scale was used to determine the total mass of the lw accumulation fig 6d samples of the wooden material were analysed in the laboratory to determine the solid wet density based on the obtained mass and mean solid wet density the solid wood volume of the lw accumulation was estimated 2 4 laboratory analysis of wood samples collected wooden samples of all lw accumulations in the field were analysed at the vaw laboratory fig 8 the wooden samples comprised ofm diameter d 10 mm twigs d 10 30 mm branches d 30 60 mm d 60 100 mm lw d 100 mm and wood chips and were required for the estimation of the solid wood volume vs based on mass and density analysis the density of the wooden material was determined using the water displacement method fig 8c h a cylindrical container 500 mm high 190 mm diameter was filled to an initial water level the sample was portioned and weighed before put into the cylindrical container to guarantee that all floating wooden material is submerged in water a lid with matching diameter to the cylinder was used to immerse all particles to be in line with the water level fig 8c h the final water level was measured and the difference between initial and final water level was calculated based on the displaced volume and weight of the samples wet density of the wooden material was calculated in parallel another portion of the samples was weighed dried for 24 h at 105 celsius and then weighed again to determine the moisture content fig 8i l 2 5 lw accumulation porosity lw accumulation porosity was estimated by applying an empirically derived correction factor to the measured porosity based on the 2 5d and 3d volumes using the results of the digital twin models porosity was determined according to eqs 1 and 2 and was defined as the ratio between the void volume vv and the bulk volume vb pagliara and carnacina 2010 the computed 2 5d v2 5d and 3d volumes v3d represent a proxy for vb and the obtained solid wood volumes vs were used to determine vv with vv v2 5d vs or vv v3d vs the maximum accumulation porosity n max was estimated based on v2 5d eq 1 1 n max v 2 5 d v s v 2 5 d 100 while the minimum porosity n min results from v3d eq 2 2 n min v 3 d v s v 3 d 100 in addition the difference of v2 5d and v3d yields the accessible void volume subscript vo from the outside of the lw accumulation vvo eq 3 3 v vo v 2 5 d v 3 d m 3 another parameter describing the structural alignment of the wooden elements within a lw accumulation using v2 5d and vvo is herein defined as packing arrangement pa eq 4 4 pa v 2 5 d v vo v 2 5 d 100 v 3 d v 2 5 d 100 of further relevance is the ratio of vvo with respect to v2 5d which has been introduced by spreitzer et al 2020a as a porosity parameter in the present study this porosity parameter is defined as the proportional penetration depth pd eq 5 5 pd v 2 5 d v 3 d v 2 5 d 100 v vo v 2 5 d 100 pd represents a first indicator for the lowest possible porosity of the assessed lw accumulation as v3d approaches vs i e the performance of the 3d approach allows for the resolution of all void space inside the lw accumulation pd yields the actual porosity n of the examined lw accumulation pa eq 4 enables the approximation of the lw accumulation porosity n calc without prior knowledge of vs eq 6 based on an empirically derived correction factor cf 6 n calc 100 c f p a the respective cf values were determined to best fit the data of actual porosity measurements n max eq 1 from the present field studies as well as previously conducted physical model experiments spreitzer et al 2020a spreitzer et al 2020b spreitzer et al 2021b 3 results 3 1 3d point cloud and mesh model generation in total nine close range aerial surveys of prototype lw accumulations and processed wood chips were conducted the majority of processing was conducted using pix4dmapper 2021 which resulted in point cloud models comprising between 5 m stockpile models of wood chips and 107 m 3d points lw accumulation replicates good camera optimisation 99 was achieved using the pre installed camera settings of the p4d drone in pix4dmapper in a further processing step pix4dmapper produced neat mesh models with 20 m faces each representing the maximum number of faces to be computed in pix4dmapper the reprojection error for all generated digital twin models ranged between 0 105 and 0 214 with an average of 0 165 pixels processing times for point cloud and mesh generation of the lw accumulation models varied from 6 5 to 15 h while the stockpile models of wood chips required 0 5 to 1 h for model generation the generation of corresponding manifold mesh models for the computation of v3d resulted in 20 m faces for the reservoir accumulation models planar bottom surface while all other manifold mesh models were automatically reduced to a maximum of roughly 1 m faces by autodesk meshmixer table 1 provides an overview of computational results with regard to the produced point cloud models generated from close range aerial sfm photogrammetry 3 2 density and moisture content of lw the results of the density and moisture content of the wooden samples are summarised in fig 9 for different categories e g ofm branches and lw samples taken from the reservoir bdvm have shown a very high wet density ranging from 0 66 to 0 98 g cm 3 by contrast dry density varied between 0 28 and 0 48 g cm 3 across all tested categories fig 9 the difference between wet and dry density indicates a high moisture content which was ranging from 99 to 179 the mean wet density of the wooden material excluding wood chips resulted in 0 84 g cm 3 across all lw accumulations at bdvm fig 9 with a mean dry density of 0 36 g cm 3 and a mean moisture content of 140 processed wood chips at the bdvm yielded a solid wet density of 0 96 g cm 3 0 43 g cm 3 solid dry density and 123 moisture content while wet bulk density of the wood chips was estimated to 0 37 g cm 3 0 18 g cm 3 dry bulk density and 136 moisture content density analysis of the partly submerged lw and ofm at the river gm ranged from 0 73 to 1 0 g cm 3 dry density ranged from 0 22 g cm 3 for ofm to 0 61 g cm 3 for branches with a diameter between 30 and 60 mm moisture content varied between 298 for ofm and 31 for lw elements bulk density of the wood chips ranged from 0 38 to 0 42 g cm 3 with an average of 0 40 g cm 3 the mean wet density of the examined lw resulted in 0 83 g cm 3 at a mean moisture content of 46 and a corresponding dry density of 0 57 g cm 3 without the consideration of ofm or wood chips laboratory analysis of samples from the lw accumulation at the bridge pier river ar have shown moderate wet densities ranging from 0 68 to 0 74 g cm 3 dry density 0 41 to 0 49 with a moderate moisture content between 51 and 66 the mean wet density was estimated to 0 71 g cm 3 at a mean moisture content of 59 fig 9 and was used for volumetric quantification of the lw accumulation the corresponding dry density resulted in 0 45 g cm 3 3 3 volume and porosity results the photogrammetric assessment of prototype lw accumulations included the computation of v2 5d and v3d table 2 accordingly lwa1 was determined as the smallest of all examined lw accumulations with a computed v2 5d of 4 39 m3 and a corresponding v3d of 2 53 m3 by contrast lwa6 yielded the largest v2 5d 42 8 m3 as well as v3d 29 48 stockpile volumes of the processed wood chips from the lw accumulations yielded 1 68 m3 lwa1 up to 22 82 m3 lwa3 on basis of field as well as laboratory analyses the smallest vs of all examined lw accumulations was estimated to 0 74 m3 lwa1 the largest vs was estimated to 10 05 m3 lwa3 due to variations in the structural alignment of lw accumulations computed volumes form digital twin models are not directly corresponding to the estimated vs this means that large v2 5d or v3d are not necessarily corresponding to large vs of the lw accumulation compare lwa6 and lwa3 in table 2 the accumulation porosity n calc can be approximated for elementary and complex lw accumulations based on pa and a correction factor cf equation 6 the respective cf values were determined to best fit the data of measured n max this study and spreitzer et al 2020b fig 10 the mean cf across 22 complex lw accumulations was estimated to 0 39 fig 10 with cf 0 29 for lwa1 and cf 0 47 for lwa5 table 2 data from elementary wooden deposits spreitzer et al 2020a spreitzer et al 2021b resulted in a mean cf of 0 95 fig 10 the root mean square error rmse of equation 6 applied to the data amounts to 4 1 based on equation 6 and the empirically derived cf fig 10 the porosity of a lw accumulation can be estimated using pa and without the prior knowledge of vs note that v2 5d was applied for estimating n max equation 1 while v3d was used for n min equation 2 specifically lwa3 a tightly packed lw accumulation at the bdvm has shown the lowest porosity 69 9 according to the 2 5d approach in contrast the wooden deposit at the retention rack gm lwa5 yielded the lowest porosity 52 5 from the 3d approach although the deposit was showing a loose structural alignment of individual wooden elements fig 5c to f for the smallest of all assessed lw accumulations lwa1 n max was estimated to 83 2 while n min yielded 70 8 the smallest lw accumulation thus corresponded to both the highest calculated n max and n min while the lowest pa 57 6 was observed the highest pa was calculated to 84 3 for lwa2 with the lowest pd of the 3d approach throughout this study on average n min was 63 6 5 7 compared to n max 74 0 4 7 calculated porosity parameters n calc via correction factor cf equation 6 were ranging between 67 2 lwa2 to 77 5 lwa1 table 2 4 discussion 4 1 close range aerial sfm photogrammetry an efficient processing pipeline for the generation of high quality point cloud and mesh models from close range drone imagery has been applied in the present study which is capable of producing accurate digital twin models of prototype lw accumulations pix4d support 2021 suggests threshold requirements for high quality models which are based on camera optimisation 95 and reprojection error equal or less than one pixel at model generation the computed models exceeded the present standards for high quality point cloud and mesh models in terms of camera optimisation 99 and reprojection errors 0 105 to 0 214 pixels besides meeting the common quality standards the models were additionally scaled via chequerboards which ensured a model accuracy in the sub millimetre range similar to results achieved via laser scanning boivin et al 2010 grigillo et al 2015 as well as other sfm photogrammetry studies smith et al 2016 and thus suggesting the models to be well suited for lw assessment the results further provide volumetric measures information about packing arrangement and a first estimate of the maximum and minimum lw accumulation porosity which are affecting flow hydraulics manners et al 2007 pagliara and carnacina 2010 although a total of three individual software packages are used the processing pipeline is simple and straight forward applying i pix4dmapper 2021 for point cloud and mesh generation as well as 2 5d volumetric computation ii autodesk meshmixer 2017 for generating a bottom surface for the mesh models in order to gain a manifold 3d mesh model and iii meshlab by cignoni et al 2008 for filtering isolated pieces and 3d volumetric computation while data acquisition in the field could be conducted within minutes 25 min per lw accumulation sfm photogrammetry processing time strongly varied between 6 5 and 15 h per prototype lw accumulation computational time for point cloud and mesh models may vary with complexity of the scene bianco et al 2018 and generated number of 3d points table 1 for complex models such as lw accumulations a large point count but less post processing steps are beneficial for maintaining a high level of detail e g protruding branches twigs ofm as previous studies revealed fua and sander 1992 khoshelham and elberink 2012 carrivick et al 2016 however processing time is also significantly affected by parallel processing tasks such as mesh closing and estimation of volumetric measures using meshlab and meshmixer the average computational time for processing 3d point cloud and mesh models considering multi tasking of the cpu and gpu with parallel mesh editing and processing tasks worst case scenario resulted in roughly 10 h per digital twin model except for the generation of point cloud and mesh in pix4dmapper the required processing times resulted in a few minutes the make solid tool has strongly reduced face count of lw accumulation models which were resting on non planar terrain fig 11 yet the achieved model quality was still satisfactory for volumetric computations although the quality of the digital twin model decreased little quantitative effects on the computed v3d are expected in fig 11a individual protruding branches and model resolution are depicted based on the make solid tool in meshmixer compared to the initial mesh model using pix4dmapper in fig 11b while the decrease in model accuracy shows negligible effects on volumetric estimates details of lw accumulations such as individual protruding logs and branches should be properly reconstructed in future applications as they represent key features for the retention of further wooden material and effects on flow hydraulics currently there is a lack of applicable tools for closing the mesh bottom in order to achieve a fully enclosed manifold 3d mesh model at the time of this study only meshmixer offered a practical solution for smoothly closing the bottom of the lw accumulation mesh model by using either the inspector or make solid tool this indicates that there are currently limited meshing algorithms available for the application in lw research which requires further improvements 4 2 density and moisture content of lw samples of prototype lw and processed wood chips were analysed for density and moisture content while the solid wet density was most relevant for volume estimates with regard to mass measurements from the field bulk density was required for the assessment of the stockpile volume where wood chips were produced attempts for estimating solid wet density of processed wood chips were conducted yet considered as not successful due to significant deviations of the solid wet density of wood chips 0 96 g cm 3 from the solid wet density of lw 0 84 g cm 3 resulting from an increased absorbability and wettability of wood chips when exposed to water xu et al 2016 although the immersing time of wood chips in water was kept at a minimum during density analysis absorption of water and swelling of the wood chips occurred promptly the drying process of the wood chip samples resulted in volumetric shrinkage of the wood chips in a range of 10 to 15 with regard to the initial bulk volume of the moist wood chip samples volumetric shrinkage of dried wood chips typically ranges from 9 to 17 glass and zelinka 2010 cousin and lekounougou 2019 which reflects on deviations observed for solid wet density estimates bulk density for the processed wood chips in the present study were 0 37 and 0 40 g cm 3 considering an increased moisture content of the wood chips in our study bulk density values are still in line with measurements of fresh wood chips examined in previous studies kofman 2006 gendek et al 2016 the estimated ratio between bulk volume of wood chips and solid wood volume amounted to 2 3 which is situated at the lower bound of frequently reported ratios ranging from 2 2 to 2 9 in previous studies nylinder and tornmarck 1986 johnson 1989 price 2012 united nations 2020 estimates of the solid lw density from partly submerged accumulations at the gm revealed similar results to lw cleared from the reservoir bdvm with 0 83 and 0 84 g cm 3 respectively this seems surprising considering lw recovered from the reservoir was soaked in water while only parts of the retained lw at the gm were submerged the similar solid wet densities at the two different locations are the effect of specific weight dry density with regard to the moisture content of different wood species simpson 1993 when comparing the estimates of mean moisture content 140 and mean dry density 0 36 g cm 3 from the bdvm field site along with results provided in a previous study conducted by glass and zelinka 2010 a neat match of the solid wet densities 0 84 g cm 3 from the present study and 0 86 g cm 3 according to tables 4 6a by glass and zelinka 2010 can be observed lw cleared from the retention structures at the gm and the bridge pier at the ar yielded a mean moisture content of 46 and 59 with a mean dry density of 0 57 and 0 45 g cm 3 which corresponds to reported wet densities by glass and zelinka 2010 of 84 and 70 respectively estimates of the mean solid wet density from the gm and ar resulted in 83 and 71 respectively fig 9 given that our density analysis align with results reported throughout previously conducted studies zanne et al 2009 glass and zelinka 2010 price 2012 ruiz villanueva et al 2016a united nations 2020 vs could be estimated with great confidence 4 3 accumulation volume and porosity computed volumes from close range aerial mapping overestimated vs obtained from the manually conducted volumetric assessment in the field for all examined prototype lw accumulations fig 12 provides an illustration of the volumetric measurements obtained from the digital twin models for each of the computations v2 5d exceeded v3d table 2 and fig 13 representing a first indicator of appropriate model reconstruction throughout the sfm photogrammetry pipeline and volume computation verbree and van oosterom 2003 spreitzer et al 2020a v2 5d typically overestimates v3d as any protruding branches or voids at varying depths in one direction typically z axis cannot be accounted for with the 2 5d onset remondino 2003 spreitzer et al 2020a consequently v2 5d claims more space than v3d fig 12 although the 2 5d approach is associated with limitations in most computer aided modelling applications remondino 2011 reichinger et al 2012 zhang et al 2019 it represents a valuable tool for bulk volume estimation in lw research sanhueza et al 2019 spreitzer et al 2019 and a significant improvement to previously applied bulk volume estimates such as the parallelepiped approach thevenet et al 1998 boivin et al 2017 and visual estimates dixon 2016 scott et al 2019 livers et al 2020 the difference between the 2 5d or 3d bulk volumes and vs represents a measure for vv and thus an important parameter for the estimation of lw accumulation porosity pagliara and carnacina 2010 spreitzer et al 2020a porosity estimates of the present field study varied between 52 5 and 83 2 this aligns with findings from previous studies reporting lw accumulation porosities ranging from 50 to 80 wallerstein et al 1997 manners and doyle 2008 livers et al 2020 spreitzer et al 2020b fig 13 illustrates the estimated porosity range for each lw accumulation spreitzer et al 2020a suggested that the ratio of the void volume accessible from the outside vvo to v 2 5d may be of use for porosity estimates indeed this definition results in the lowest possible porosity for the assessed wooden deposit unless v3d obtained by the non intrusive method approaches vs and thus allows for the estimation of the actual porosity this can be achieved for loosely packed lw deposits with uniform component mixture and a high penetration depth of the 3d approach spreitzer et al 2021b a randomly and loosely packed lw accumulation e g lwa5 results in a larger pd and enables the reconstruction of individual logs and branches which are protruding the surface fig 5c f this corresponds to a larger vvo and thus a better approximation to the actual porosity of the wooden deposit lwa1 5 and 6 in fig 13 in contrast very compact lw accumulations can result in dispersed porosity estimates for increasing accumulation size according to the suggested approach by spreitzer et al 2020a accordingly as v3d approaches v2 5d a lower pd results lwa2 3 and 4 in fig 13 new findings by the present study reveal that the ratio of v3d blue ellipse in fig 12 to v2 5d red ellipse represents a measure for the pa equation 4 it is important to note that v3d still comprises all void space of the wooden deposit that is not accessible from the outside based on pa and an empirically derived correction factor equation 6 and fig 10 lw accumulation porosity can be calculated solely via the sfm photogrammetry approach without prior knowledge of vs the results demonstrated that n calc describes the measured n max of lw accumulations in the field well with a rmse of 4 1 fig 13 when applying the determined cf 0 39 fig 10 a slight overestimation 5 3 of n calc is observed for lwa5 the remaining lw accumulations have shown lower n calc than n max with a maximal underestimation of lwa1 5 7 and thus representing a conservative approach in terms of flood risk management for the assessment of resulting flow hydraulics due to lw accumulations as lower porosities reportedly lead to a larger backwater rise manners et al 2007 pagliara and carnacina 2010 dixon 2016 schalko 2018 to account for these findings a more conservative correction factor cf 0 5 is recommended which better fits the lower bound of the estimated porosity range in the present work fig 13 the applied correction factor is only recommended for complex lw accumulations fig 14 such as categorised by manners and doyle 2008 framework deposits and complete log jams may require larger correction factors compare categories of lw deposits in fig 14 for the sake of completeness correction factors for elementary wooden deposits e g key members framework deposits were estimated to 1 0 and 0 9 respectively while complete jams result in cf 0 5 to 1 0 to estimate most realistic porosity parameters note that the provided correction factors for key members framework deposits and complete log jams may be subject to future studies for optimisation overall the obtained and calculated porosity parameters from the present study align with previously estimated porosity parameters of lw deposits from field wallerstein et al 1997 manners and doyle 2008 livers et al 2020 as well as physical model studies spreitzer et al 2020b spreitzer et al 2021b a field study by bezzola and hegg 2007 assessed the bulk volume 109 800 m3 and weight 27 450 t of deposited lw during the 2005 flood event in switzerland yielding a calculated porosity of 72 2 and pa of 71 4 that align with observations from the present study fig 14 in general laboratory studies show lower porosities than field studies the lowest possible pa of the present study considering a v3d equal to vs of 0 74 m3 for lwa1 would yield 16 9 this parameter was applied for closing the range of complex lw accumulations towards the lower bound of pa fig 14 the lower boundary reach for lw accumulations according to fig 14 results from wooden deposits assessed by spreitzer et al 2020a and spreitzer et al 2021b showing porosities higher than 40 as well as a pa of 60 framework deposits range all across the pa scale and within 9 to 91 at the porosity axis with minimum and maximum porosity parameters for cylinder arrangements trovato et al 2007 liu et al 2018 while key members and complete jams are situated in the higher pa range but lower porosity range fig 14 accordingly a pa higher than 70 describes tightly packed and neatly aligned lw accumulations while a lower pa indicates loosely and randomly distributed wooden deposits previous studies suggested that lw accumulation porosity may be the main driver for flow alteration wallerstein et al 1997 manners and doyle 2008 dixon 2016 schalko 2018 livers et al 2020 ismail et al 2021 however with the introduction of the pa parameter in the present study we believe that pa could have a significant impact on flow hydraulics rather than solely lw accumulation porosity recently ismail et al 2021 reported difficulties of existing lw accumulation assessment methods to estimate the resulting flow depth and local scour due to porous structures this may be due to the current focus on porosity parameters rather than the consideration of the structural alignment of the wooden obstruction the present study reveals a trend of decreasing lw accumulation porosity with increasing packing arrangement pa fig 14 thus lw deposits with a higher pa could have greater impact on flow hydraulics and changes in channel morphology according to fig 14 lw accumulations exhibit a mean porosity of 64 0 while pa varies between 57 6 and 99 5 in addition a higher impact on the flow may result from tightly packed lw accumulations n 70 and pa 70 than for loosely packed framework deposits n 70 and pa 70 while the data for these two structures show similar n the structural alignment remarkably varies with pa furthermore drag forces acting on the lw deposits may be significantly affected by the structural alignment of the obstruction manners et al 2007 pagliara and carnacina 2010 follett et al 2020 the effect of pa may be subject to future flume studies investigating the effects of lw accumulations with similar porosities yet significantly varying structural alignment e g comparing engineered logjams with uniform spacing compared to natural wooden deposits with heterogeneous log alignment on flow hydraulics and channel morphology 4 4 application and enhancement optical remote sensing is following a strong interdisciplinary trend towards computer vision e g sfm photogrammetry geosciences and digital twin generation aiming for an efficient application and better data interoperability beamish et al 2020 emilien et al 2021 polewski et al 2021 in the present work digital twin models were shown to be of significant value for lw accumulation volume and porosity estimation which are urgently needed parameters to expand the current understanding of lw related processes in rivers schalko et al 2018 follett et al 2020 livers et al 2020 ismail et al 2021 spreitzer et al 2021b the organisation of individual elements in the accumulation i e packing arrangement may represent an important parameter for the quantification of scour processes melville and dongol 1992 diehl 1997 schalko et al 2019b but also for the estimation of blocking ratios pagliara and carnacina 2010 ismail et al 2021 and backwater effects schalko et al 2019a follett et al 2020 furthermore digital twin models offer potential for the application in computational fluid dynamic cfd modelling hedger et al 2002 spreitzer et al 2019 the quantification of lw volume and porosity by means of 3d models is therefore of great interest for the establishment of relations between flow sediment wood interaction processes the proposed sfm photogrammetry method is moreover applicable for lw budgeting along river reaches and the assessment of hazard potential for inhabitants and infrastructure such as addressed by wohl et al 2016 but also for the monitoring of lw volumes in restored water courses kail et al 2007 grabowski et al 2019 the present study revealed that wooden deposits can show similar porosities for a varying range of packing arrangements e g framework deposits versus lw accumulations a finding which is of significant importance for numerical modelling and the assessment of the associated flow hydraulics due to lw accumulations however further field studies are needed for the verification of the presented model along with actual solid cubic meter volumes of prototype wooden deposits the generated knowledge will be of use for river restoration design of engineered log jams elj and flood protection projects such as often addressed wohl and scott 2017 ismail et al 2021 but also benefit numerical modelling which rely on prototype data from generated digital twin models and gained parameters such as the packing arrangement or roughness parameters allowing algorithms to compute most realistic outcome while 2 5d estimates may be sufficient for lw budgeting more accurate 3d volumes and digital twin models are required for the prediction of hydraulic effects and cfd applications the present work contributes to an improved data acquisition and processing plan which represents a further step towards a systematic lw assessment framework wohl et al 2016 keys et al 2018 needed by river managers and engineers to account for lw in rivers and maintain good ecological conditions in fluvial systems 5 conclusions non intrusive computation of lw accumulation volumes requires the generation of digital twin models in the present study an innovative close range aerial sensing technique sfm photogrammetry was employed for the assessment of prototype lw accumulations a straightforward processing pipeline was used for the generation of highly accurate point cloud and mesh modes the computed 2 5d and 3d volumes were used along with manual measurements of the solid wood volume gained results demonstrated graduated quantities of 2 5d 3d and solid wood volume for corresponding lw deposits and thus allowing for the most accurate lw accumulation assessment to date based on the acquired data from three swiss field sites porosity parameters in the range of 52 5 to 83 2 were estimated from bulk and solid wood volumes a novel approach for the calculation of lw accumulation porosity has been presented solely requiring non intrusive 2 5d and 3d volume estimates however additional reference studies focusing on the volumetric assessment of larger prototype wooden deposits 10 m3 solid wood volume are needed to verify the empirically derived correction factors and to expand the application to bigger lw accumulations as well as for an improved approximation of accumulation porosity novel insights into the structural alignment of wooden elements in lw accumulations have been provided by means of an introduced packing arrangement parameter computed from the ratio of 3d to 2 5d volume it was shown that wooden deposits can have similar porosities for significantly varying structural alignment packing arrangement leading to flow alteration which cannot be accounted for solely with porosity estimates the present study contributes to an advanced knowledge about lw deposits by providing non intrusive measures of 2 5d and 3d volume obtained from digital twin models and solid wood volume obtained from field measurements which are essential for the determination of lw accumulation porosity as well as packing arrangement to derive potential effects of lw accumulations on flow hydraulic and channel morphology with the present study we demonstrate that sfm photogrammetry represents a robust tool for the non intrusive assessment of lw deposits by significantly reducing human introduced errors and providing higher credibility of computed results the major processing was conducted with one software package pix4dmapper 2021 however the computation of 3d volumes is still severely restricted in most available software packages therefore we strongly encourage the cooperation with software developers to meet the needs of current research and to create new opportunities for the employment of non intrusive surveying techniques declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dr gabriel spreitzer reports financial support was provided by european commission marie sklodowska curie actions acknowledgements we are grateful for the support received from local councils and power providers in particular we want to thank mr sepp berchtold from the department of natural hazards in sarnen mr arnold flepp and mr luregn caspescha from axpo hydro surselva ag tavanasa as well as mr anto domislic from officine idroelettriche di blenio sa and his team at the bacino di val malvaglia field site for their support throughout this study we appreciate support received from the swiss pix4d team furthermore we thank our technical staff at the laboratory of hydraulics hydrology and glaciology vaw eth zurich for assisting the experiments we thank the anonymous reviewers and dr dan scott for their many insightful comments and detailed feedback provided funding this project has gratefully received funding from the european union s horizon 2020 research and innovation programme in form of a marie skłodowska curie individual fellowship msca if under grant agreement number 887254 
2816,quantification and assessment of large wood lw accumulations in fluvial systems is still considered difficult due to the complex nature of wooden deposits although knowledge about volumetric measures and porosity parameters of lw accumulations is crucial for the prediction of hydraulic and geomorphic effects it has not yet been possible to obtain accurate measurements these limitations are mainly based on a lack of applicable sensing technologies available in the past in the present study a close range aerial surveying technique structure from motion sfm photogrammetry is applied for generating 3d replicates digital twin models of wooden deposits enabling their volumetric assessment in addition manually conducted volumetric measurements of corresponding prototype lw accumulations help to improve and calibrate the sfm derived estimates for the first time precise porosity parameters for lw accumulations ranging from 52 5 to 83 2 are provided in addition a novel parameter the packing arrangement is used which describes the structural alignment of individual elements in the lw accumulation and benefits porosity estimates based on the applied 2 5d and 3d photogrammetric approach accordingly randomly and loosely organised lw accumulations allow for a high penetration depth of the 3d approach resulting in a more accurate estimate of the actual porosity as the 3d volumetric estimate approaches the solid wood volume of the corresponding lw accumulation an empirical approach has been developed for future approximation of lw accumulation porosity without the need of knowing the solid wood volume with the present work a significant improvement of our understanding in employing a non intrusive sensing technique is provided linked with manually conducted field measurements of the solid wood volume of lw accumulations our study contributes to an improved data acquisition and processing plan which represents a further important step towards a systematic assessment framework that is urgently needed by river managers and engineers to better evaluate and manage lw in fluvial systems keywords accumulation volume close range sensing digital twin models large wood lw wood jam porosity data availability data will be made available on request 1 introduction 1 1 surveying large wood lw in rivers the need for developing innovative surveying methods and more refined assessment tools is continuously addressed throughout large wood lw research harmon et al 1986 huang et al 2009 vaughan et al 2009 ruiz villanueva et al 2016b lw assessment evolved from first visual estimates and manual measurements in rivers van wagner 1968 tritton 1981 over the categorisation of wooden deposits in fluvial systems e g key member individual lw piece framework mix of lw pieces lw accumulation mix of lw and finer materials with open space between wood pieces and complete jam compressed lw accumulation showing little open space between wood pieces by manners and doyle 2008 all the way to automated image and video monitoring of transported and deposited lw in rivers macvicar et al 2009 ghaffarian et al 2020 as well as the detection and quantification of lw in forests using highly advanced sensing techniques huang et al 2009 milani et al 2018 huang et al 2019 although major progress has been made in estimating potential wood volumes available for recruitment rickenmann 1997 steeb et al 2017 and developing as well as applying mapping techniques for characterising lw in rivers piégay and marston 1998 galia et al 2018 quantification measures of wooden deposits are still severely limited at field scale wohl and scott 2017 and associated with great uncertainties livers et al 2020 these limitations are mainly related to difficulties in data acquisition which often contain unintentionally influenced human impaired measurements e g bulk volume assumed parameters e g density porosity or rough transect estimates which all reflect on the quality of gained results in recent years a range of non intrusive surveying tools such as laser scanning and structure from motion photogrammetry sfm have strongly developed and increased in popularity for the application in geoscience westoby et al 2012 anderson et al 2019 pyörälä et al 2019 the emerging technologies have also created new opportunities and interest for digital mapping and budgeting of lw in fluvial systems pioneer studies used multispectral imagery polewski et al 2021 laser scanning tonon et al 2014 magnussen et al 2018 yrttimaa et al 2019 fayad et al 2021 as well as sfm photogrammetry sanhueza et al 2019 spreitzer et al 2019 windrim et al 2019 for mapping wooden deposits and determination of specific characteristics e g log geometries and volume both laser scanning as well as sfm photogrammetry are capable of producing highly accurate 3d replicates digital twin models of topography goetz et al 2018 backes et al 2020 and objects cunliffe et al 2016 sylvain et al 2019 moyano et al 2020 swayze et al 2021 given that the surveyed scene is behaving static mind vegetation in windy conditions or turbulent water accordingly sfm photogrammetry represents a well suited alternative to the more expensive laser scanning technology wallace et al 2016 anderson et al 2019 as well as to traditionally applied lw mapping methods such as the parallelepiped approach thevenet et al 1998 boivin et al 2017 discrete point and transect sampling böhl and brändli 2007 ritter and saborowski 2012 or manual measurement of individual lw elements wyżga et al 2015 tonon et al 2018 with the transition from traditionally applied lw mapping practices over to better performing techniques such as sfm photogrammetry a whole new level of detail can be acquired within a single field survey potentially benefitting lw research substantially 1 2 structure from motion sfm photogrammetry sfm photogrammetry can infer a 3d point cloud from overlapping 2d images carrivick et al 2016 the generated point cloud is meshed meaning that each point vertices is connected to its neighbouring points forming a continuous wire mesh of defined geometric shapes edges while the resulting surface of all shapes faces yields a first replicate of the mapped reach or object digital twin model the suitability for sfm photogrammetry in geoscience is benefiting from rapid advances in drone technology the acquisition of ground control points gcps for georegistration and model scaling represents a large barrier for drone based photogrammetry both from a cost and time perspective carbonneau and dietrich 2017 however for the assessment of transient lw accumulations the acquisition of gcps can be waived while chequerboards or alternatively gps data from the drone s flight record file can be used for model scaling dji 2021 spreitzer et al 2021a this approach is particularly useful for one time data acquisition and modelling as often needed in lw research sanhueza et al 2019 spreitzer et al 2019 to date a variety of sfm photogrammetry and post processing software packages are either freely or commercially available each of the software packages offer individual tools that are needed for the generation of most practical lw accumulation models for this reason a series of software packages are currently required for the digital assessment of lw accumulations spreitzer et al 2020a however generating digital twin models of prototype wooden deposits shows great potential for improving our current knowledge about the quantification of the lw accumulation characteristics and their effects of lw accumulations on both flow hydraulics and sediment transport processes 1 3 lw accumulation assessment highly resolved point cloud and mesh models can be used for the computation of lw accumulation volume and porosity depending on sorting and packing matrix spreitzer et al 2020a of wooden elements in a lw accumulation livers et al 2020 spreitzer et al 2020b the use of two volumetric measures may be necessary the computation of the 2 5d volume provides an initial volumetric measurement of lw accumulations sanhueza et al 2019 spreitzer et al 2019 determination of 2 5d stockpile volume via dense point network rather than single transects or cross sectional profiles represents a well established method to date draeyer and strecha 2014 the 2 5d volume computation is based on the vertical projection of a surface model onto a 2d raster grid also known as base surface remondino 2003 sanhueza et al 2019 the finest possible raster grid distance is defined by the ground sampling distance gsd which represents a measure for model resolution for applications in mining and construction but also in lw research 2 5d volumetric measurements from sfm photogrammetry were shown to produce highly accurate measurements compared to real metrics reflecting on errors of 1 in the generated surface model tucci et al 2019 spreitzer et al 2020a however there are challenges arising with the 2 5d approach in particular for the assessment of poorly sorted and loosely packed lw accumulations where voids at the surface and space beneath protruding elements are considered as lw volume and thus not taken into account as void space spreitzer et al 2019 by contrast the volume computed from 3d models accounts for all accessible void space from the outside showing a higher penetration depth and thus returning a more accurate lw accumulation volume than the 2 5d approach spreitzer et al 2020a however with decreasing penetration depths well sorted densely packed the 3d approach also considers the occluded void volume at the inside of the lw accumulation the difference between 2 5d and 3d volumetric measurements resolves void space that is accessible from the outside and may be of use as a measure for the performance of the 3d approach e g penetration depth as well as the structural alignment of individual wooden elements within a lw accumulation recent studies applying aerial sfm photogrammetry sanhueza et al 2019 windrim et al 2019 and close range sfm photogrammetry spreitzer et al 2019 for the quantification of lw emphasised the great potential and promising perspectives for the application of an image based surveying technique in lw research previous studies verified the sfm photogrammetry approach by means of scaled wooden dowels and organic fine material ofm in a laboratory setup and concluded that sfm photogrammetry represents a cost and time effective yet powerful tool for lw accumulation assessment spreitzer et al 2020a spreitzer et al 2020b sanhueza et al 2019 presented a sfm approach based on nadir images and reported limitations for accumulation assessment in the presence of living vegetation which may cover parts or entire lw deposits however these limitations can be overcome with a set of close range nadir and oblique imagery spreitzer et al 2019 spreitzer et al 2020a sfm photogrammetry could therefore be an applicable technique for assessing accumulation characteristics such as volume composition structural alignment and porosity which cannot be captured at satisfactory quality to date 1 4 challenges and objectives information on lw characteristics governs how lw affects both flow conditions and sediment transport processes thereby affecting potential resulting flood hazards due to lw the lack of applicable sensing technologies and assessment methodologies impedes the accurate determination of lw characteristics wyżga et al 2015 livers et al 2020 with the development of innovative sensing technologies new opportunities arise for an improved lw assessment apart from recent laboratory studies spreitzer et al 2020a spreitzer et al 2020b no reference data e g solid cubic meter volume of examined lw accumulations are available such reference data are urgently needed for calibration of the computed data gained from the sfm photogrammetry technique obtained 3d volumetric data for example still account for all void spaces of the wooden deposit that are not accessible from the outside thus actual volumetric measurements of prototype wooden deposits are required to better control for generated data from aerial and close range sfm photogrammetry surveys of lw accumulations however there are several challenges involved in obtaining precise volumetric measurements of prototype lw accumulations at field scale due to temporal and spatial constraints of field sites and logistical efforts the present study focuses on the manual quantification of the solid cubic meter volume of lw accumulations at field scale to calibrate computed lw volumes from the sfm photogrammetry method in addition the structural alignment of individual elements in the lw accumulation is estimated to further advance the assessment of lw accumulations furthermore field measurements of solid cubic meter volumes are needed to calibrate a correction factor based on the structural alignment of the wooden deposit that translates sfm derived volumes to porosity this study will help to improve the rapid assessment of lw accumulations at field scale the applied quantification method and results for lw accumulation volume and porosity may then be of help for establishing relations between flow sediment wood interaction processes and benefit river restoration as well as flood protection projects 2 methodology mapping of lw accumulations in the field was conducted using close range aerial sfm photogrammetry the image based methodology enables the non intrusive assessment of prototype lw accumulations extendingpreviously conducted laboratory experiments spreitzer et al 2019 spreitzer et al 2020a for calibration of the produced digital twin models with regard to the gained volumetric results the prototype lw accumulations were either weighed or processed to wood chips after digitalisation to manually obtain the corresponding solid cubic meter volume which in the present work is referred to as the solid wood volume samples of the prototype lw accumulations and wood chips were taken from the field and brought to the laboratory of hydraulics hydrology and glaciology vaw for density analysis based on the density analysis in the laboratory and obtained measurements in the field the solid wood volume could be estimated 2 1 close range aerial surveying imagery of selected lw accumulations was acquired using a dji phantom 4 pro p4p drone the p4p drone is equipped with a 1 cmos image sensor which was employed at an aspect ratio of 3 2 5472 3648 pixels in order to exploit its highest possible resolution of about 20 mp focal length was 8 8 mm representing a 24 mm lens with regard to the 35 mm format equivalent p4p 2017 and resulting in a field of view of 84 which is well suited for close range sfm photogrammetry in terms of capturing great detail without image distortion and producing satisfactory overlap for sfm photogrammetry furthermore the p4p allows gimbal tilt of the camera within 120 30 up and 90 down facing pitch p4p 2017 enabling the view below structures e g at bridges and occlusions in lw accumulations the average flight time for moderate breeze wind conditions i e 25 km h and temperatures above freezing is 25 min in this study close range aerial imagery was acquired along a manually controlled flight trajectory comprising a dense dataset of manual shoots from oblique and nadir perspectives the flight trajectory was significantly shaped by nearby structures and riparian vegetation which had to be evaded camera orientation pitch was adjusted during the flight in order to maintain shallow depth of field while keeping focus on the lw accumulation to be surveyed at the time of image capture the drone was operated hovering still to avoid the effect of rolling shutter the entire data acquisition for close range aerial sfm photogrammetry was conducted in 25 min per prototype lw accumulation for scaling purposes chequerboards with a raster grid of 30 mm were placed clearly visible on and around the lw accumulation prior to the flights 2 2 point cloud and mesh generation after image acquisition in the field sfm photogrammetry was applied for generating 3d digital twin models from heavily overlapping 2d images of the prototype lw accumulations a commercially available sfm software package pix4dmapper 2021 was used on a standard desktop pc cpu intelxeon e3 1245 v5 3 50 ghz ram 16 gb gpu nvidia geforce gtx 1050 ti os windows 10 64 bit pix4dmapper offers a well established processing pipeline for point cloud and mesh generation poisson surface reconstruction by kazhdan et al 2006 as well as 2 5d volume computation draeyer and strecha 2014 the computation of the 2 5d volume requires the set of a triangulated base surface which was adjusted according to observations in the field to best fit the layer between ground and lw accumulation the present study solely considers wood volume above the base surface cut volume while computed volume below the base surface fill volume is not considered for lw accumulations resting on a coarse channel bed the base surface was aligned with the upper level of the coarse gravel particles to rather compromise on a small fraction of lw volume than cutting into the ground and mistakenly considering soil material as wood volume the processing pipeline for obtaining both 2 5d and 3d volumetric measures required a minimum of three software packages namely i pix4dmapper 2021 version 4 5 6 ii autodesk meshmixer 2017 version 3 5 474 and iii meshlab version v2020 07 by cignoni et al 2008 while the 2 5d volume can be computed directly from pix4dmapper after digital surface model dsm generation some further processing steps are required for computation of the 3d volume as a watertight 3d model is needed pix4dmapper produces highly accurate meshed surface models using screened poisson surface reconstruction psr kazhdan and hoppe 2013 although the surface of the generated mesh in pix4dmapper is watertight no 3d volume can be computed straight from the output mesh as the bottom of the surface model is not closed yet in order to close the bottom of the surface model and produce a fully enclosed and watertight manifold 3d model the mesh is exported to autodesk meshmixer 2017 the inspector tool provided in meshmixer offers an effective method for repairing irregularities non manifold edges or cracks in the mesh as well as for closing the bottom hole of the surface model in connecting the mesh boundary via a smooth triangulated mesh to produce a manifold 3d mesh model in parallel to the inspector tool the outline of holes can be selected and closed manually with the erase fill tool returning highest possible model resolution alternatively the make solid tool can be applied under consideration of some compromises to be made with respect to model resolution and face count as the tool allows for the computation of a maximum number of roughly 1 m faces once the manifold 3d mesh model is created the digital twin models were exported to meshlab where the models were filtered for isolated pieces 10 and 3d volumes computed the detailed processing pipeline for pix4dmapper meshmixer and meshlab is shown in fig 1 2 3 prototype lw accumulation measurements the present study acquired data from three field sites in switzerland lw accumulations at the bacino di val malvaglia bdvm hydroelectric reservoir were surveyed along with wood deposits at two lw retention racks at the river grosse melchaa gm and at a bridge pier on the site of the hydroelectric power plant tavanasa operated by axpo power ag at the river anterior rhine ar all of the catchments are heavily forested with a mix of european spruce picea abies common beech fagus sylvatica and black alder alnus glutinosa 2 3 1 analysis at a hydroelectric reservoir bacino di val malvaglia the bacino di val malvaglia bdvm reservoir is located in the canton ticino in southern switzerland 46 25 28 n 9 01 18 e 993 m a s l and operated by the officine idroelettriche di blenio sa the reservoir is fed by the orino river a 15 km long and steep alpine stream with its source at 2 770 m a s l schweizerfluss 2021b a significant amount of lw accumulated at the bdvm dam and had to be removed from the reservoir using a mobile crane fig 2 a and b for the duration of the clearing works the reservoir surface was impounded to the maximum water level to ease the extraction of lw the cleared material was transported to a site nearby the dam fig 2c and d at the bdvm reservoir fig 2 a set of three differently sized lw accumulations were prepared on a flat surface to acquire imagery for close range aerial sfm photogrammetry of a small lwa1 medium lwa2 and large lwa3 lw accumulation fig 3 a d and g the datasets comprised 326 lwa1 416 lwa2 and 507 lwa3 images for generating digital twin models table 1 which enabled 3d fig 3a d and g and 2 5d fig 3b e and h volumetric analysis once digital mapping of the lw accumulations was completed each of the lw accumulations fig 2e was processed to wood chips fig 2f the piles of wood chips where then individually mapped via close range aerial imagery for stockpile analysis fig 3c f and i image count for the small medium and large stockpile models of processed wood chips amounted to 186 109 and 131 site photos respectively the assessment of stockpile volume using the 2 5d approach represents a well established method to obtain bulk volume measures draeyer and strecha 2014 liu et al 2020 son et al 2020 samples of wood chips and a variety of wood samples ofm small twigs and branches lw were brought to the laboratory for determination of bulk density and solid wet density multiplying the obtained bulk volume with bulk density results in the mass of the wood chips the solid wood volume is then estimated from the ratio of wood mass to mean solid wet density 2 3 2 analysis at lw retention racks river grosse melchaa the river grosse melchaa gm is located in central switzerland canton obwalden from the source at 1 025 m a s l the gm finds its roughly 18 5 km long way through a forested catchment and a 100 m deep gorge ranftschlucht down to its mouth at 469 m a s l schweizerfluss 2021a upstream of lake sarnen a series of lw retention racks are installed at the outer bend of the channel the vertical retention racks are part of the flood mitigation concept grosse melchaa chalcheren pieren et al 2009 and have to be cleared after floods in the present study a small lw accumulation at a retention rack located about 600 m downstream of the gorge fig 4 a 46 53 27 1 n 8 15 38 3 e 486 m a s l and a larger amount of lw at the retention rack immediately downstream of the gorge fig 4b 46 53 10 4 n 8 15 42 9 e 496 m a s l were digitally mapped removed and analysed for solid wood volume the collected datasets from close range aerial mapping comprised 394 site photos of the first lw accumulation lwa4 fig 4a and 328 site photos of the second lw accumulation lwa5 fig 4b representing satisfactory image count for sfm photogrammetry and model generation fig 5 a mid sized forwarder johndeere 1210g 2019 was employed for clearing deposited wood from the lw retention racks fig 4c the available load space of the forwarder incl subdivision board for reducing the v shape was given with 4 5 2 7 1 4 m lxwxh the forwarder load from lwa4 was estimated to 3 m3 stere volume pile volume for lwa5 two forwarder loads with an estimated 8 and 5 5 m3 stere volume were removed for later estimation of the solid wood volume the stere volume measurement from the forwarder is reduced by a factor of 0 7 resulting from the swedish nation board of forestry stacked measuring guidelines for debarked wood vmf nord 1999 and commonly used for estimating solid cubic meter from stere volume vmf nord 1999 pásztory et al 2018 united nations 2020 the accumulated lw was transported to an accessible site near the gm where samples were taken for solid wet density analysis in the laboratory while the remaining material was processed into wood chips the wood chips were directly stored on a container truck fig 4d with known dimensions of 5 5 2 4 2 5 m lxwxh while the weight of the shredded wood was determined by an on board scale in addition to the bulk volume and mass of the wood chips the mean solid wet density was used for the estimation of the solid wood volume 2 3 3 analysis at a bridge pier river anterior rhine at the site of the hydroelectric power plant tavanasa a bridge with two pier walls 0 6 m wide and 5 1 m long at a distance of approximately 15 m from each other crosses the river anterior rhine ar 46 45 05 n 9 02 45 e 800 m a s l lw accumulated at both pier walls while the lw accumulation at the right pier wall was about three times bigger than the lw accumulation at the left pier wall due to the limited accessibility of the lw at the left pier wall located in the active channel at the time only the lw accumulation at the right pier wall could be recorded and analysed in the present study the considered lw accumulation fig 6 a c measured roughly 3 5 m in height and was tightly wrapped around the pier wall based on the close range aerial imagery a total of 682 images were acquired from a single drone flight and used for sfm photogrammetry fig 7 data acquisition also considered the collection of images below the bridge deck meaning that the drone flight took place under the bridge to capture and resolve the entire lw accumulation at best detail the determination of the solid wood volume of the lw accumulation lwa6 fig 6a c required all wooden material to be cleared from the bridge pier a container truck with loader crane and on board scale was used to determine the total mass of the lw accumulation fig 6d samples of the wooden material were analysed in the laboratory to determine the solid wet density based on the obtained mass and mean solid wet density the solid wood volume of the lw accumulation was estimated 2 4 laboratory analysis of wood samples collected wooden samples of all lw accumulations in the field were analysed at the vaw laboratory fig 8 the wooden samples comprised ofm diameter d 10 mm twigs d 10 30 mm branches d 30 60 mm d 60 100 mm lw d 100 mm and wood chips and were required for the estimation of the solid wood volume vs based on mass and density analysis the density of the wooden material was determined using the water displacement method fig 8c h a cylindrical container 500 mm high 190 mm diameter was filled to an initial water level the sample was portioned and weighed before put into the cylindrical container to guarantee that all floating wooden material is submerged in water a lid with matching diameter to the cylinder was used to immerse all particles to be in line with the water level fig 8c h the final water level was measured and the difference between initial and final water level was calculated based on the displaced volume and weight of the samples wet density of the wooden material was calculated in parallel another portion of the samples was weighed dried for 24 h at 105 celsius and then weighed again to determine the moisture content fig 8i l 2 5 lw accumulation porosity lw accumulation porosity was estimated by applying an empirically derived correction factor to the measured porosity based on the 2 5d and 3d volumes using the results of the digital twin models porosity was determined according to eqs 1 and 2 and was defined as the ratio between the void volume vv and the bulk volume vb pagliara and carnacina 2010 the computed 2 5d v2 5d and 3d volumes v3d represent a proxy for vb and the obtained solid wood volumes vs were used to determine vv with vv v2 5d vs or vv v3d vs the maximum accumulation porosity n max was estimated based on v2 5d eq 1 1 n max v 2 5 d v s v 2 5 d 100 while the minimum porosity n min results from v3d eq 2 2 n min v 3 d v s v 3 d 100 in addition the difference of v2 5d and v3d yields the accessible void volume subscript vo from the outside of the lw accumulation vvo eq 3 3 v vo v 2 5 d v 3 d m 3 another parameter describing the structural alignment of the wooden elements within a lw accumulation using v2 5d and vvo is herein defined as packing arrangement pa eq 4 4 pa v 2 5 d v vo v 2 5 d 100 v 3 d v 2 5 d 100 of further relevance is the ratio of vvo with respect to v2 5d which has been introduced by spreitzer et al 2020a as a porosity parameter in the present study this porosity parameter is defined as the proportional penetration depth pd eq 5 5 pd v 2 5 d v 3 d v 2 5 d 100 v vo v 2 5 d 100 pd represents a first indicator for the lowest possible porosity of the assessed lw accumulation as v3d approaches vs i e the performance of the 3d approach allows for the resolution of all void space inside the lw accumulation pd yields the actual porosity n of the examined lw accumulation pa eq 4 enables the approximation of the lw accumulation porosity n calc without prior knowledge of vs eq 6 based on an empirically derived correction factor cf 6 n calc 100 c f p a the respective cf values were determined to best fit the data of actual porosity measurements n max eq 1 from the present field studies as well as previously conducted physical model experiments spreitzer et al 2020a spreitzer et al 2020b spreitzer et al 2021b 3 results 3 1 3d point cloud and mesh model generation in total nine close range aerial surveys of prototype lw accumulations and processed wood chips were conducted the majority of processing was conducted using pix4dmapper 2021 which resulted in point cloud models comprising between 5 m stockpile models of wood chips and 107 m 3d points lw accumulation replicates good camera optimisation 99 was achieved using the pre installed camera settings of the p4d drone in pix4dmapper in a further processing step pix4dmapper produced neat mesh models with 20 m faces each representing the maximum number of faces to be computed in pix4dmapper the reprojection error for all generated digital twin models ranged between 0 105 and 0 214 with an average of 0 165 pixels processing times for point cloud and mesh generation of the lw accumulation models varied from 6 5 to 15 h while the stockpile models of wood chips required 0 5 to 1 h for model generation the generation of corresponding manifold mesh models for the computation of v3d resulted in 20 m faces for the reservoir accumulation models planar bottom surface while all other manifold mesh models were automatically reduced to a maximum of roughly 1 m faces by autodesk meshmixer table 1 provides an overview of computational results with regard to the produced point cloud models generated from close range aerial sfm photogrammetry 3 2 density and moisture content of lw the results of the density and moisture content of the wooden samples are summarised in fig 9 for different categories e g ofm branches and lw samples taken from the reservoir bdvm have shown a very high wet density ranging from 0 66 to 0 98 g cm 3 by contrast dry density varied between 0 28 and 0 48 g cm 3 across all tested categories fig 9 the difference between wet and dry density indicates a high moisture content which was ranging from 99 to 179 the mean wet density of the wooden material excluding wood chips resulted in 0 84 g cm 3 across all lw accumulations at bdvm fig 9 with a mean dry density of 0 36 g cm 3 and a mean moisture content of 140 processed wood chips at the bdvm yielded a solid wet density of 0 96 g cm 3 0 43 g cm 3 solid dry density and 123 moisture content while wet bulk density of the wood chips was estimated to 0 37 g cm 3 0 18 g cm 3 dry bulk density and 136 moisture content density analysis of the partly submerged lw and ofm at the river gm ranged from 0 73 to 1 0 g cm 3 dry density ranged from 0 22 g cm 3 for ofm to 0 61 g cm 3 for branches with a diameter between 30 and 60 mm moisture content varied between 298 for ofm and 31 for lw elements bulk density of the wood chips ranged from 0 38 to 0 42 g cm 3 with an average of 0 40 g cm 3 the mean wet density of the examined lw resulted in 0 83 g cm 3 at a mean moisture content of 46 and a corresponding dry density of 0 57 g cm 3 without the consideration of ofm or wood chips laboratory analysis of samples from the lw accumulation at the bridge pier river ar have shown moderate wet densities ranging from 0 68 to 0 74 g cm 3 dry density 0 41 to 0 49 with a moderate moisture content between 51 and 66 the mean wet density was estimated to 0 71 g cm 3 at a mean moisture content of 59 fig 9 and was used for volumetric quantification of the lw accumulation the corresponding dry density resulted in 0 45 g cm 3 3 3 volume and porosity results the photogrammetric assessment of prototype lw accumulations included the computation of v2 5d and v3d table 2 accordingly lwa1 was determined as the smallest of all examined lw accumulations with a computed v2 5d of 4 39 m3 and a corresponding v3d of 2 53 m3 by contrast lwa6 yielded the largest v2 5d 42 8 m3 as well as v3d 29 48 stockpile volumes of the processed wood chips from the lw accumulations yielded 1 68 m3 lwa1 up to 22 82 m3 lwa3 on basis of field as well as laboratory analyses the smallest vs of all examined lw accumulations was estimated to 0 74 m3 lwa1 the largest vs was estimated to 10 05 m3 lwa3 due to variations in the structural alignment of lw accumulations computed volumes form digital twin models are not directly corresponding to the estimated vs this means that large v2 5d or v3d are not necessarily corresponding to large vs of the lw accumulation compare lwa6 and lwa3 in table 2 the accumulation porosity n calc can be approximated for elementary and complex lw accumulations based on pa and a correction factor cf equation 6 the respective cf values were determined to best fit the data of measured n max this study and spreitzer et al 2020b fig 10 the mean cf across 22 complex lw accumulations was estimated to 0 39 fig 10 with cf 0 29 for lwa1 and cf 0 47 for lwa5 table 2 data from elementary wooden deposits spreitzer et al 2020a spreitzer et al 2021b resulted in a mean cf of 0 95 fig 10 the root mean square error rmse of equation 6 applied to the data amounts to 4 1 based on equation 6 and the empirically derived cf fig 10 the porosity of a lw accumulation can be estimated using pa and without the prior knowledge of vs note that v2 5d was applied for estimating n max equation 1 while v3d was used for n min equation 2 specifically lwa3 a tightly packed lw accumulation at the bdvm has shown the lowest porosity 69 9 according to the 2 5d approach in contrast the wooden deposit at the retention rack gm lwa5 yielded the lowest porosity 52 5 from the 3d approach although the deposit was showing a loose structural alignment of individual wooden elements fig 5c to f for the smallest of all assessed lw accumulations lwa1 n max was estimated to 83 2 while n min yielded 70 8 the smallest lw accumulation thus corresponded to both the highest calculated n max and n min while the lowest pa 57 6 was observed the highest pa was calculated to 84 3 for lwa2 with the lowest pd of the 3d approach throughout this study on average n min was 63 6 5 7 compared to n max 74 0 4 7 calculated porosity parameters n calc via correction factor cf equation 6 were ranging between 67 2 lwa2 to 77 5 lwa1 table 2 4 discussion 4 1 close range aerial sfm photogrammetry an efficient processing pipeline for the generation of high quality point cloud and mesh models from close range drone imagery has been applied in the present study which is capable of producing accurate digital twin models of prototype lw accumulations pix4d support 2021 suggests threshold requirements for high quality models which are based on camera optimisation 95 and reprojection error equal or less than one pixel at model generation the computed models exceeded the present standards for high quality point cloud and mesh models in terms of camera optimisation 99 and reprojection errors 0 105 to 0 214 pixels besides meeting the common quality standards the models were additionally scaled via chequerboards which ensured a model accuracy in the sub millimetre range similar to results achieved via laser scanning boivin et al 2010 grigillo et al 2015 as well as other sfm photogrammetry studies smith et al 2016 and thus suggesting the models to be well suited for lw assessment the results further provide volumetric measures information about packing arrangement and a first estimate of the maximum and minimum lw accumulation porosity which are affecting flow hydraulics manners et al 2007 pagliara and carnacina 2010 although a total of three individual software packages are used the processing pipeline is simple and straight forward applying i pix4dmapper 2021 for point cloud and mesh generation as well as 2 5d volumetric computation ii autodesk meshmixer 2017 for generating a bottom surface for the mesh models in order to gain a manifold 3d mesh model and iii meshlab by cignoni et al 2008 for filtering isolated pieces and 3d volumetric computation while data acquisition in the field could be conducted within minutes 25 min per lw accumulation sfm photogrammetry processing time strongly varied between 6 5 and 15 h per prototype lw accumulation computational time for point cloud and mesh models may vary with complexity of the scene bianco et al 2018 and generated number of 3d points table 1 for complex models such as lw accumulations a large point count but less post processing steps are beneficial for maintaining a high level of detail e g protruding branches twigs ofm as previous studies revealed fua and sander 1992 khoshelham and elberink 2012 carrivick et al 2016 however processing time is also significantly affected by parallel processing tasks such as mesh closing and estimation of volumetric measures using meshlab and meshmixer the average computational time for processing 3d point cloud and mesh models considering multi tasking of the cpu and gpu with parallel mesh editing and processing tasks worst case scenario resulted in roughly 10 h per digital twin model except for the generation of point cloud and mesh in pix4dmapper the required processing times resulted in a few minutes the make solid tool has strongly reduced face count of lw accumulation models which were resting on non planar terrain fig 11 yet the achieved model quality was still satisfactory for volumetric computations although the quality of the digital twin model decreased little quantitative effects on the computed v3d are expected in fig 11a individual protruding branches and model resolution are depicted based on the make solid tool in meshmixer compared to the initial mesh model using pix4dmapper in fig 11b while the decrease in model accuracy shows negligible effects on volumetric estimates details of lw accumulations such as individual protruding logs and branches should be properly reconstructed in future applications as they represent key features for the retention of further wooden material and effects on flow hydraulics currently there is a lack of applicable tools for closing the mesh bottom in order to achieve a fully enclosed manifold 3d mesh model at the time of this study only meshmixer offered a practical solution for smoothly closing the bottom of the lw accumulation mesh model by using either the inspector or make solid tool this indicates that there are currently limited meshing algorithms available for the application in lw research which requires further improvements 4 2 density and moisture content of lw samples of prototype lw and processed wood chips were analysed for density and moisture content while the solid wet density was most relevant for volume estimates with regard to mass measurements from the field bulk density was required for the assessment of the stockpile volume where wood chips were produced attempts for estimating solid wet density of processed wood chips were conducted yet considered as not successful due to significant deviations of the solid wet density of wood chips 0 96 g cm 3 from the solid wet density of lw 0 84 g cm 3 resulting from an increased absorbability and wettability of wood chips when exposed to water xu et al 2016 although the immersing time of wood chips in water was kept at a minimum during density analysis absorption of water and swelling of the wood chips occurred promptly the drying process of the wood chip samples resulted in volumetric shrinkage of the wood chips in a range of 10 to 15 with regard to the initial bulk volume of the moist wood chip samples volumetric shrinkage of dried wood chips typically ranges from 9 to 17 glass and zelinka 2010 cousin and lekounougou 2019 which reflects on deviations observed for solid wet density estimates bulk density for the processed wood chips in the present study were 0 37 and 0 40 g cm 3 considering an increased moisture content of the wood chips in our study bulk density values are still in line with measurements of fresh wood chips examined in previous studies kofman 2006 gendek et al 2016 the estimated ratio between bulk volume of wood chips and solid wood volume amounted to 2 3 which is situated at the lower bound of frequently reported ratios ranging from 2 2 to 2 9 in previous studies nylinder and tornmarck 1986 johnson 1989 price 2012 united nations 2020 estimates of the solid lw density from partly submerged accumulations at the gm revealed similar results to lw cleared from the reservoir bdvm with 0 83 and 0 84 g cm 3 respectively this seems surprising considering lw recovered from the reservoir was soaked in water while only parts of the retained lw at the gm were submerged the similar solid wet densities at the two different locations are the effect of specific weight dry density with regard to the moisture content of different wood species simpson 1993 when comparing the estimates of mean moisture content 140 and mean dry density 0 36 g cm 3 from the bdvm field site along with results provided in a previous study conducted by glass and zelinka 2010 a neat match of the solid wet densities 0 84 g cm 3 from the present study and 0 86 g cm 3 according to tables 4 6a by glass and zelinka 2010 can be observed lw cleared from the retention structures at the gm and the bridge pier at the ar yielded a mean moisture content of 46 and 59 with a mean dry density of 0 57 and 0 45 g cm 3 which corresponds to reported wet densities by glass and zelinka 2010 of 84 and 70 respectively estimates of the mean solid wet density from the gm and ar resulted in 83 and 71 respectively fig 9 given that our density analysis align with results reported throughout previously conducted studies zanne et al 2009 glass and zelinka 2010 price 2012 ruiz villanueva et al 2016a united nations 2020 vs could be estimated with great confidence 4 3 accumulation volume and porosity computed volumes from close range aerial mapping overestimated vs obtained from the manually conducted volumetric assessment in the field for all examined prototype lw accumulations fig 12 provides an illustration of the volumetric measurements obtained from the digital twin models for each of the computations v2 5d exceeded v3d table 2 and fig 13 representing a first indicator of appropriate model reconstruction throughout the sfm photogrammetry pipeline and volume computation verbree and van oosterom 2003 spreitzer et al 2020a v2 5d typically overestimates v3d as any protruding branches or voids at varying depths in one direction typically z axis cannot be accounted for with the 2 5d onset remondino 2003 spreitzer et al 2020a consequently v2 5d claims more space than v3d fig 12 although the 2 5d approach is associated with limitations in most computer aided modelling applications remondino 2011 reichinger et al 2012 zhang et al 2019 it represents a valuable tool for bulk volume estimation in lw research sanhueza et al 2019 spreitzer et al 2019 and a significant improvement to previously applied bulk volume estimates such as the parallelepiped approach thevenet et al 1998 boivin et al 2017 and visual estimates dixon 2016 scott et al 2019 livers et al 2020 the difference between the 2 5d or 3d bulk volumes and vs represents a measure for vv and thus an important parameter for the estimation of lw accumulation porosity pagliara and carnacina 2010 spreitzer et al 2020a porosity estimates of the present field study varied between 52 5 and 83 2 this aligns with findings from previous studies reporting lw accumulation porosities ranging from 50 to 80 wallerstein et al 1997 manners and doyle 2008 livers et al 2020 spreitzer et al 2020b fig 13 illustrates the estimated porosity range for each lw accumulation spreitzer et al 2020a suggested that the ratio of the void volume accessible from the outside vvo to v 2 5d may be of use for porosity estimates indeed this definition results in the lowest possible porosity for the assessed wooden deposit unless v3d obtained by the non intrusive method approaches vs and thus allows for the estimation of the actual porosity this can be achieved for loosely packed lw deposits with uniform component mixture and a high penetration depth of the 3d approach spreitzer et al 2021b a randomly and loosely packed lw accumulation e g lwa5 results in a larger pd and enables the reconstruction of individual logs and branches which are protruding the surface fig 5c f this corresponds to a larger vvo and thus a better approximation to the actual porosity of the wooden deposit lwa1 5 and 6 in fig 13 in contrast very compact lw accumulations can result in dispersed porosity estimates for increasing accumulation size according to the suggested approach by spreitzer et al 2020a accordingly as v3d approaches v2 5d a lower pd results lwa2 3 and 4 in fig 13 new findings by the present study reveal that the ratio of v3d blue ellipse in fig 12 to v2 5d red ellipse represents a measure for the pa equation 4 it is important to note that v3d still comprises all void space of the wooden deposit that is not accessible from the outside based on pa and an empirically derived correction factor equation 6 and fig 10 lw accumulation porosity can be calculated solely via the sfm photogrammetry approach without prior knowledge of vs the results demonstrated that n calc describes the measured n max of lw accumulations in the field well with a rmse of 4 1 fig 13 when applying the determined cf 0 39 fig 10 a slight overestimation 5 3 of n calc is observed for lwa5 the remaining lw accumulations have shown lower n calc than n max with a maximal underestimation of lwa1 5 7 and thus representing a conservative approach in terms of flood risk management for the assessment of resulting flow hydraulics due to lw accumulations as lower porosities reportedly lead to a larger backwater rise manners et al 2007 pagliara and carnacina 2010 dixon 2016 schalko 2018 to account for these findings a more conservative correction factor cf 0 5 is recommended which better fits the lower bound of the estimated porosity range in the present work fig 13 the applied correction factor is only recommended for complex lw accumulations fig 14 such as categorised by manners and doyle 2008 framework deposits and complete log jams may require larger correction factors compare categories of lw deposits in fig 14 for the sake of completeness correction factors for elementary wooden deposits e g key members framework deposits were estimated to 1 0 and 0 9 respectively while complete jams result in cf 0 5 to 1 0 to estimate most realistic porosity parameters note that the provided correction factors for key members framework deposits and complete log jams may be subject to future studies for optimisation overall the obtained and calculated porosity parameters from the present study align with previously estimated porosity parameters of lw deposits from field wallerstein et al 1997 manners and doyle 2008 livers et al 2020 as well as physical model studies spreitzer et al 2020b spreitzer et al 2021b a field study by bezzola and hegg 2007 assessed the bulk volume 109 800 m3 and weight 27 450 t of deposited lw during the 2005 flood event in switzerland yielding a calculated porosity of 72 2 and pa of 71 4 that align with observations from the present study fig 14 in general laboratory studies show lower porosities than field studies the lowest possible pa of the present study considering a v3d equal to vs of 0 74 m3 for lwa1 would yield 16 9 this parameter was applied for closing the range of complex lw accumulations towards the lower bound of pa fig 14 the lower boundary reach for lw accumulations according to fig 14 results from wooden deposits assessed by spreitzer et al 2020a and spreitzer et al 2021b showing porosities higher than 40 as well as a pa of 60 framework deposits range all across the pa scale and within 9 to 91 at the porosity axis with minimum and maximum porosity parameters for cylinder arrangements trovato et al 2007 liu et al 2018 while key members and complete jams are situated in the higher pa range but lower porosity range fig 14 accordingly a pa higher than 70 describes tightly packed and neatly aligned lw accumulations while a lower pa indicates loosely and randomly distributed wooden deposits previous studies suggested that lw accumulation porosity may be the main driver for flow alteration wallerstein et al 1997 manners and doyle 2008 dixon 2016 schalko 2018 livers et al 2020 ismail et al 2021 however with the introduction of the pa parameter in the present study we believe that pa could have a significant impact on flow hydraulics rather than solely lw accumulation porosity recently ismail et al 2021 reported difficulties of existing lw accumulation assessment methods to estimate the resulting flow depth and local scour due to porous structures this may be due to the current focus on porosity parameters rather than the consideration of the structural alignment of the wooden obstruction the present study reveals a trend of decreasing lw accumulation porosity with increasing packing arrangement pa fig 14 thus lw deposits with a higher pa could have greater impact on flow hydraulics and changes in channel morphology according to fig 14 lw accumulations exhibit a mean porosity of 64 0 while pa varies between 57 6 and 99 5 in addition a higher impact on the flow may result from tightly packed lw accumulations n 70 and pa 70 than for loosely packed framework deposits n 70 and pa 70 while the data for these two structures show similar n the structural alignment remarkably varies with pa furthermore drag forces acting on the lw deposits may be significantly affected by the structural alignment of the obstruction manners et al 2007 pagliara and carnacina 2010 follett et al 2020 the effect of pa may be subject to future flume studies investigating the effects of lw accumulations with similar porosities yet significantly varying structural alignment e g comparing engineered logjams with uniform spacing compared to natural wooden deposits with heterogeneous log alignment on flow hydraulics and channel morphology 4 4 application and enhancement optical remote sensing is following a strong interdisciplinary trend towards computer vision e g sfm photogrammetry geosciences and digital twin generation aiming for an efficient application and better data interoperability beamish et al 2020 emilien et al 2021 polewski et al 2021 in the present work digital twin models were shown to be of significant value for lw accumulation volume and porosity estimation which are urgently needed parameters to expand the current understanding of lw related processes in rivers schalko et al 2018 follett et al 2020 livers et al 2020 ismail et al 2021 spreitzer et al 2021b the organisation of individual elements in the accumulation i e packing arrangement may represent an important parameter for the quantification of scour processes melville and dongol 1992 diehl 1997 schalko et al 2019b but also for the estimation of blocking ratios pagliara and carnacina 2010 ismail et al 2021 and backwater effects schalko et al 2019a follett et al 2020 furthermore digital twin models offer potential for the application in computational fluid dynamic cfd modelling hedger et al 2002 spreitzer et al 2019 the quantification of lw volume and porosity by means of 3d models is therefore of great interest for the establishment of relations between flow sediment wood interaction processes the proposed sfm photogrammetry method is moreover applicable for lw budgeting along river reaches and the assessment of hazard potential for inhabitants and infrastructure such as addressed by wohl et al 2016 but also for the monitoring of lw volumes in restored water courses kail et al 2007 grabowski et al 2019 the present study revealed that wooden deposits can show similar porosities for a varying range of packing arrangements e g framework deposits versus lw accumulations a finding which is of significant importance for numerical modelling and the assessment of the associated flow hydraulics due to lw accumulations however further field studies are needed for the verification of the presented model along with actual solid cubic meter volumes of prototype wooden deposits the generated knowledge will be of use for river restoration design of engineered log jams elj and flood protection projects such as often addressed wohl and scott 2017 ismail et al 2021 but also benefit numerical modelling which rely on prototype data from generated digital twin models and gained parameters such as the packing arrangement or roughness parameters allowing algorithms to compute most realistic outcome while 2 5d estimates may be sufficient for lw budgeting more accurate 3d volumes and digital twin models are required for the prediction of hydraulic effects and cfd applications the present work contributes to an improved data acquisition and processing plan which represents a further step towards a systematic lw assessment framework wohl et al 2016 keys et al 2018 needed by river managers and engineers to account for lw in rivers and maintain good ecological conditions in fluvial systems 5 conclusions non intrusive computation of lw accumulation volumes requires the generation of digital twin models in the present study an innovative close range aerial sensing technique sfm photogrammetry was employed for the assessment of prototype lw accumulations a straightforward processing pipeline was used for the generation of highly accurate point cloud and mesh modes the computed 2 5d and 3d volumes were used along with manual measurements of the solid wood volume gained results demonstrated graduated quantities of 2 5d 3d and solid wood volume for corresponding lw deposits and thus allowing for the most accurate lw accumulation assessment to date based on the acquired data from three swiss field sites porosity parameters in the range of 52 5 to 83 2 were estimated from bulk and solid wood volumes a novel approach for the calculation of lw accumulation porosity has been presented solely requiring non intrusive 2 5d and 3d volume estimates however additional reference studies focusing on the volumetric assessment of larger prototype wooden deposits 10 m3 solid wood volume are needed to verify the empirically derived correction factors and to expand the application to bigger lw accumulations as well as for an improved approximation of accumulation porosity novel insights into the structural alignment of wooden elements in lw accumulations have been provided by means of an introduced packing arrangement parameter computed from the ratio of 3d to 2 5d volume it was shown that wooden deposits can have similar porosities for significantly varying structural alignment packing arrangement leading to flow alteration which cannot be accounted for solely with porosity estimates the present study contributes to an advanced knowledge about lw deposits by providing non intrusive measures of 2 5d and 3d volume obtained from digital twin models and solid wood volume obtained from field measurements which are essential for the determination of lw accumulation porosity as well as packing arrangement to derive potential effects of lw accumulations on flow hydraulic and channel morphology with the present study we demonstrate that sfm photogrammetry represents a robust tool for the non intrusive assessment of lw deposits by significantly reducing human introduced errors and providing higher credibility of computed results the major processing was conducted with one software package pix4dmapper 2021 however the computation of 3d volumes is still severely restricted in most available software packages therefore we strongly encourage the cooperation with software developers to meet the needs of current research and to create new opportunities for the employment of non intrusive surveying techniques declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dr gabriel spreitzer reports financial support was provided by european commission marie sklodowska curie actions acknowledgements we are grateful for the support received from local councils and power providers in particular we want to thank mr sepp berchtold from the department of natural hazards in sarnen mr arnold flepp and mr luregn caspescha from axpo hydro surselva ag tavanasa as well as mr anto domislic from officine idroelettriche di blenio sa and his team at the bacino di val malvaglia field site for their support throughout this study we appreciate support received from the swiss pix4d team furthermore we thank our technical staff at the laboratory of hydraulics hydrology and glaciology vaw eth zurich for assisting the experiments we thank the anonymous reviewers and dr dan scott for their many insightful comments and detailed feedback provided funding this project has gratefully received funding from the european union s horizon 2020 research and innovation programme in form of a marie skłodowska curie individual fellowship msca if under grant agreement number 887254 
2817,short term rainfall forecasting using machine learning based approaches of pso svr lstm and cnn fatemeh rezaie adaryani methodology software validation formal analysis investigation resources writing original draft visualization s jamshid mousavi conceptualization methodology validation formal analysis investigation resources data curation writing review editing supervision fatemeh jafari conceptualization methodology validation investigation resources data curation writing original draft department of civil and environmental engineering amirkabir university of technology tehran iran department of civil and environmental engineering amirkabir university of technology tehran iran department of civil and environmental engineering amirkabir university of technology tehran iran corresponding author at amirkabir university of technology tehran polytechnic tehran tehran iran amirkabir university of technology tehran polytechnic tehran tehran iran this manuscript was handled by corrado corradini editor in chief with the assistance of vahid nourani associate editor short term rainfall forecasting plays an important role in hydrologic modeling and water resource management problems such as flood warning and real time control of urban drainage systems this paper compares the performances of three machine and deep learning based rainfall forecasting approaches including a hybrid optimized by pso support vector regression pso svr long short term memory lstm and convolutional neural network cnn the approaches are used to develop both 5 minute and 15 minute ahead forecast models of rainfall depth based on datasets of niavaran station tehran iran results of applying the models to all data points indicated that pso svr and lstm approaches performed almost the same and better than cnn subsequently rainfall events were divided into four classes depending on their severity and duration using k nearest neighbor method and a separate forecast model was built for each of the classes classification of the events improved the forecast models accuracy where pso svr and lstm were the best approaches for the 15 minute and 5 minute ahead rainfall forecast models respectively investigating the impact of more predictors on the forecast quality adding differences of rainfall depths to model predictors improved the accuracy of pso svr approach for the 5 minute ahead forecast model up to 13 furthermore depending on the rainfall event additional input variables considering rainfall depth fluctuations over shorter time periods than the forecast lead time increased the performances of the pso svr and lstm approaches between 3 15 and 2 10 respectively keywords rainfall forecasting machine learning deep learning particle swarm optimization data availability data will be made available on request 1 introduction precipitation being a key important process of hydrologic cycle plays a vital role in maintaining the balance between freshwater and saltwater resources of the globe it is one of the most studied component in hydrology and climate sciences as it directly or indirectly affects our society duong et al 2018 most hydrological processes usually show a high degree of temporal and spatial variety which induces non stationarity in the measured hydrologic data jhong et al 2019 precise forecasting of rainfall is necessary for proper estimation of water budget as well as to plan strategize maintain the water storage options during extreme weather conditions and design of flood warning systems hammad et al 2021 rainfall forecasting is challenging because of chaotic weather behavior and has been studied by many researchers globally mehr et al 2019 chong et al 2020 note that some statistical methods may not be suitable for rainfall forecasting because historical data usually changes dramatically in a short time there are several works applying artificial neural network ann and soft computing techniques for time series modeling and prediction of hydrologic processes in general and precipitation in particular e g luk et al 2000 fotovatikhah et al 2018 moazenzadeh et al 2018 e lucas et al 2020 gao et al 2020 a number of research works such as zhang et al 2016 have discussed the application of artificial intelligence ai methods in rainfall forecasting although ann based approaches have been integrated into modeling systems they have revealed evidence of gaps that have limited their application in certain areas for example ann is lacking in precision and may produce unexpectedly poor results in non stationary time series modeling apart from its relative novelty it is this tantalizing mixture of successes and failures that add to the lure of anns govindaraju and rao 2013 however significant modifications and improvements have been made to classical anns since their introduction with numerous applications in hydrologic and hydrosystems modeling for instance nourani 2017 showed the superiority of emotional ann eann over a conventional feed forward neural network ffnn in a daily rainfall runoff modeling application there are several other studies where relatively more advanced approaches than conventional ann such as ga anfis least square support vector machine lssvm and multi strategy ensemble learning have improved the performance of the classical fann approach e g ganjidoost et al 2016 nourani et al 2019 ravichandran et al 2021 different prediction models can be developed using machine learning ml techniques that is a branch of ai emphasizing learning from data identifying patterns and making decisions with minimal human interference nayak and ghosh 2013 developed a support vector machine svm algorithm to predict extreme rainfalls they observed that weather patterns before the extreme rainfall events during nighttime are different from those during daytime so they introduced a two phase support vector classifier for extreme rainfall predictions the predictive model ability is impacted substantially by noises wu et al 2009 gupta and gupta 2019 furthermore strong correlations found in rainfall time series data leads to time lagged predictions for instance this problem has been investigated by de vos and rientjes 2005 for an ann based rainfall forecasting model one of the most important developments in modern computational science is the interaction between optimization and machine learning algorithms an optimization algorithm is aprocedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found formulations and methods of optimization are becoming increasingly important to designing algorithms to mine information from vast amounts of data for instance ravansalar et al 2017 used a hybrid wavelet linear genetic programming wlgp model to predict the monthly streamflow in two gauging stations they compared the wlgp model with a single lgp ann a hybrid wavelet ann wann and multi linear regression mlr models and showed that the wlgp models could significantly improve the accuracy of streamflow and other hydrological processes forecasting approaches with the advent of general purpose graphics processing units gpu and their use in ml researchers can now train large scale neural networks such as deep learning models that have previously been restricted due to high computational cost for their training long short term memory lstm and convolutional neural network cnn are among well known algorithms used for time series modeling and prediction cnn is a class of a nonlinear dynamical system that holistically models a time series of data through keeping track of the temporal relationships over time poornima and pushpalatha 2019 for example in the recurrent neural network rnn paradigm every point in a time series is assumed to be linked to every single time instance however that is not always the case especially when a different set of time instances emerges consequently the cnn architecture reduces time dependence structure in comparison to rnn and models a time series simultaneously chong et al 2020 bandara et al 2020 presented a prediction model that can be used with different types of rnn models on subgroups of similar time series which are identified by time series clustering techniques this procedure has also been used to improve baseline lstm networks essien and giannetti 2019 and cnn kim et al 2017 zhang et al 2021 first used the weighted k means clustering method to select the meteorological data of the surrounding stations related to a target station next the high altitude shear value of the target station was calculated then to reduce dimensions of the high altitude shear value and the surface factors they used the principal component analysis pca method finally a cnn was implemented to forecast rainfall this multi component forecasting approach outperformed the other existing approaches such as multilayer perceptron mlp autoregressive integrated moving average arima and backpropagation neural network bpnn with respect to threat rating ts and mean square error mse measures roy and singh 2020 proposed a hybrid approach benefiting from particle swarm optimization pso and grey wolf optimization gwo integrated with two classical ml models namely ann and adaptive neuro fuzzy system anfis for one day ahead streamflow forecasting in a catchment the results demonstrated that connecting optimization algorithm improved the forecast accuracy ni et al 2019 developed two hybrid lstm models for monthly rainfall and streamflow forecasting using a coupled wavelet transform algorithm wlstm and a coupled convolutional neural network clstm respectively the forecast accuracies of these models were compared with those of multi layer perceptron mlp ann and lstm results showed the superiority of the wlstm and clstm models over the other alternatives hammad et al 2021 developed a novel coupled wavelet multi order time lagged neural network wmtlnn model to forecast rainfall by using daily rainfall records at three meteorological stations and compared it with time lagged neural network tlnn models wavelet coupled time lagged neural network tlnn and lstm models results indicated that wmtlnn outperformed the other models although rainfall forecasting can provide a precise estimate of the associated risk in design and operation of hydrosystems and aids in implementing effective mitigation measures in either short or long term planning afshin et al 2011 zambrano et al 2018 chong et al 2020 the comprehensive comparison of their applicability particularly in short term predictions under heavy rainfall events has rarely been discussed zhang et al 2016 short term rainfall forecasting is a complex important issue as it impacts people s lives afshin et al 2011 reported that short term predictions were more sensitive than long term ones thus more difficult to reach the desired accuracy burlando et al 1993 investigated the performance of an autoregressive moving average arma in short term rainfall forecasting to provide estimates of flood hydrographs at key points nasseri et al 2008 developed an ann network to forecast rainfall for a number of time series and investigate the impacts of different parameters on the short term rainfall forecast accuracy in urbanized catchments rainfall forecasts at short intervals are notable as they demonstrate a fast hydrologic response luk et al 2000 improving urban drainage systems operation during severe flood events is one of the applications requiring short term rainfall forecasts that has been investigated by several researchers for instance chiang and willems 2015 combined the genetic algorithm ga optimization approach and a model predictive control mpc technique to develop a real time flood control model jafari et al 2020 investigated the role of short term rainfall forecasts in predictive real time optimal operation prtop where five adaptive prtop models were compared in this study we develop and test three forecasting approaches of pso svr lstm and cnn in short term rainfall forecasting two 5 and 15 minute ahead rainfall depth forecasting models are examined finally the role of additional rainfall features used as the model predictors including differences of rainfall depths and rainfall depth fluctuations within the forecast lead time is also investigated as a novel aspect of the study 2 case study and data precipitation time series data between years 1974 to 2014 measured at niyavaran station of tehran capital of iran is used see fig 1 it consists of 368 precipitation events and a total number of 29 039 and 10 129 data rows for 5 and 15 minute time intervals respectively the duration of investigated precipitation events varies from 5 to 2225 min complexity is an obstacle to find first the right model for forecasting and second for setting up the parameters of the model properly sadaei et al 2019 model parameters need to be estimated including those of ml forecasting models model complexity increases when the number of its parameters is large such as in deep learning dl ml models where the computation process required for training and parameter estimation may become troublesome roelofs et al 2019 the performance of dl models depends upon a number of parameters called hyper parameters hyper parameters are user set training variables which are determined before the model training automated techniques have also been proposed for tuning hyper parameters but these methods may not provide insight to the end users about the interactions among different hyper parameters and their relative importance hutter et al 2014 on the other hand in absence of enough sample data supporting the parameter set estimation an increased number of parameters does not necessarily help and there will be more potentiality for the model performance to decrease due to for example overfitting in this study the dataset has been divided to train test and validation sets with relative percentages of 70 20 and 10 respectively in other words a part of training dataset is used for cross validation while applying an iterative process each time the process is repeated a part of the data is used to train the forecast model and the rest is used to validate the model the process is a resampling method to estimate the model error the validation dataset serves for the purpose of preventing the trained model from overfitting see fig 2 for the dl lstm and cnn models the number of data used in every epoch a hyper parameter named batch size to train the network kandel and castelli 2020 has been parametrized based on a sensitivity analysis 2 1 data preprocessing to help the training process of the forecast models investigated in this study one can perform data normalization as a preprocessing approach using the following equation 1 xref i x i m i n xref i m a x xref i m i n xref i where x i is the prediction value xref i is the observed value and xref i is the observed normalized value all for event i random processing is a theory in modeling formed based on statistics and probability and is used to analyze data in most cases random processes are indexed by time markov process is a model to show a sequence of random variables in which each event s probable occurrence highly depends on the prior one and other events do not get involved foufoula georgiou and lettenmaier 1987 to develop a rainfall prediction model using anns a general markovian process is utilized as follows 2 p t 1 f p t p t 1 p t 2 p t k 1 e t in the above equation t is time step p t is depth of rainfall in period t f is generally a nonlinear function mapping previous rainfall depths to that of the current rime e t is an error random term and k is effective lag time of the prediction model representing what number of previous rainfall depths would affect the current rainfall depth markov 1987 according to a sensitivity analysis with respect to the number of previous rainfall depths as the predictor vector the best model performances were respectively resulted for lag t i m e 1 3 and 3 for pso svr lstm and cnn forecasting approaches details of which are presented in the next section additionally analysis of autocorrelation function acf for different lags showed meaningful autocorrelations between rainfall time series data especially for lags 1 to 3 with an average value of 0 689 and 0 498 for 5 and 15 minute normalized data respectively finally evaluation of the mentioned forecast models requires applying some illustrative steps and performance measures such as displaying the predicted and the observed values compared to line 45 coefficient of determination r 2 and root mean square error rmse has been used in this study to evaluate the performances of the forecast models as follows 3 rmse m m 1 n i 1 n e i 2 4 r 2 1 i e i 2 i x i m e a n xref i 2 where n is the number of data and e i is the difference between observed and predicted data 3 methodology 3 1 modeling tools and model setup 3 1 1 particle swarm optimization pso kennedy and eberhart kennedy and eberhart 1995 introduced pso as a population based optimization algorithm driven by birds flocking and schooling fish patterns pso works generally similar to other evolutionary computation techniques angeline 1998 where a population of random solutions is created to initialize the system and the generation of optimal solutions occurs after it over successive iterations pso does not come with the evolution operators which are presented in ga wu et al 2015 within the problem space potential solutions or particles move by following the two best solutions found so far among all generated solutions gbest and among the solutions of the current generation pbest pso is computationally efficient both in speed and memory requirements compared to other derivative free methods pso reduces the number of parameters to tune details on the pso algorithm formulation how it works and its several successful applications in hydrosystems modeling and optimization and hydrologic model calibration can be found elsewhere such as mousavi and shourian 2010 shourian et al 2008 alizadeh and mousavi 2013 kamali et al 2013 in this study we use pso to determine parameter values of the support vector regression svr forecast model optimally 3 1 2 support vector regression in svr a nonlinear mapping ϕ is used to map the data x i into a higher dimensional feature space and the multidimensional nonlinear problem is formulated as a high dimensional linear problem that is to determine the regression function below 5 f x t w t φ x t b estimation of an svr model parameters is formulated as a quadratic optimization problem the objective of which is minimizing the structural risk unlike the traditional statistical learning theory this overcomes the overfitting problem and the underlying optimization can be solved effectively so the generalization power of the approach is high using vapnik s ε insensitive loss function bottou and vapnik 1992 one can solve the following optimization problem to minimize the structural risk function 6 m i n 1 2 w t w c i 1 t ξ i c i 1 t ξ i subject to 7 y t w t φ x t b ε ξ t where w r n b r and ϕ represents a high dimensional feature mapping from the input space to the feature space note that indicates a vector symbol the dual optimization problem using the lagrange function can be written as follows 8 m i n 1 2 i j 1 t α i α i α j α j k x i x j ε i 1 t α i α i i 1 t α i α i subject to 9 i 1 t α i α i 0 0 α j c i 1 2 t where k x i x j is the kernel function on which the solution of the dual problem depends c is referred to as regularization parameter ε is the error sensitivity parameter and sigma σ controls the level of model non linearity wu and wang 2009 for the svr model the percentages of training and testing data sets were considered as 80 and 20 percent respectively the pso algorithm has been used to optimize five parameters of svr including 1 type of the kernel function from three options of polynomial linear and radial basis functions 2 sigma 3 c 4 epsilon and 5 degree of the polynomial function in case the optimal kernel function selected is of polynomial type the procedure used to optimize parameter values of the svr model using pso has been illustrated in fig 3 table 1 reports the best parameter values of the pso svr model developed in this study 3 1 3 long short term memory lstm networks lstm networks are among well known dl based models the operations inside lstm cells are similar to those in traditional rnns but they are more complex enabling an lstm model to learn long term and short term dependencies from data information can be stored in an additional state in lstm networks compared to traditional rnns malhotra et al 2015 as shown in fig 4 x t is the input vector h t represents the hidden state of the lstm cell at time t and c t represents the cell state of the lstm at time t the structure of an lstm cell indicates that it transfers its hidden state h t and its state c t to the next time step there are three gates in an lstm cell that maintain and adjust the cell state and the hidden state including a forget gate f t an input gate i t and an output gate o t fischer and krauss 2018 these gates serve different functions forget gates f t determine what information is removed from a cell state input gates i t determine how much information ought to be retained in the cell state c t and in output gates o t specific information from the state of the cell is used as output o t lstm cells are able to capture the complex correlations in both short term and long term time series via the functions of the three gates a major improvement over traditional rnn zhao et al 2017 more details on updating cell state c t and hidden state h t and how the lstm model works can be found elsewhere such as hochreiter and schmidhuber 1997 lstm has a number of parameters to be fine tuned li et al 2019 and hutter et al 2014 used adam learning algorithm to set the most appropriate parameter values of an lstm network adam is a computationally efficient optimization algorithm that exhibits invariance to diagonal rescaling of gradients two recently popular methods i e adagrad duchi et al 2011 and rmsprop tieleman and hinton 2012 are combined and used in this method the parameter values of the lstm forecast model including training initial rate ratio reduction value at each period cycle and maximum repetition value of each category were set to 0 01 0 9 and 20 respectively using the adam algorithm implemented according to the recommendations of kingma and ba 2014 other parameters of the lstm forecast model were determined based on a sensitivity analysis as presented in table 2 3 1 4 convolutional neural network cnn cnns are built up of different combinations of three main layers the convolutional pooling and fully connected layers fig 5 shows the general architecture of a deep cnn it has two convolution layers conv1 conv2 two pooling layers pool 1 pool 2 and a fully connected layer fcl haidar and verma 2018 a cnn s main building block is the convolutional layer neurons of a layer in a network are typically connected to all of their neighboring neurons of a previous layer when convolutional layers are used neurons are connected to local regions of earlier layers cnns learn from data through their hierarchical layer structure while going deeper with layers it increases the capability of understanding complex relationships within input features so number of hidden unit is one of the parameters which affects model capability as illustrated in fig 6 dropout is usually incorporated to avoid overfitting aswin et al 2018 estimation of the cnn parameters has been dealt with in previous studies such as noh et al 2016 and zhang et al 2019 again we used the adam algorithm adopted under recommendations of kingma and ba 2014 to determine the cnn model parameters where an initial learning rate equal to 0 001 ratio reduction value at each period cycle of 0 9 and a maximum repetition value of each category equal to 100 were resulted a random removal layer dropout layer was included to prevent the cnn network from overfitting so that 50 of the prior layer hidden layer was set to zero in each learning iteration a sensitivity analysis was conducted to determine the other parameter values of the cnn forecast model such as lag time minimum batch size and the number of hidden unit as presented in table 3 4 results this section compares the results of three mentioned forecasting approaches of pso svr lstm and cnn for both 5 minute and 15 minute ahead models in the first step results are presented considering all the rainfall events together excluding zero depth rainfalls by choosing 80 of data as training and the rest as testing data subsequently the three forecasting approaches are compared while forecasting eight selected rainfall events finally additional forecast models are considered to investigate the effect of increasing the number of model predictors that may improve the model performances table 4 presents what kind of data has been considered for various models the results obtained in the first step are presented in table 5 according to table 5 the performances of the models in predicting next 5 and 15 minute rainfall depths are close but the dl based lstm approach has performed a bit better than the others 4 1 event based forecasting since the approaches performed more or less the same in this stage we investigate if data clustering would affect their performances rainfall classification can be used for the purpose of declining nonlinearity of existing patterns in data minns and hall 1996 so enhancing the model prediction accuracy which can be done based on factors such as occurrence time of events seasons or months duration of events and so forth table 6 presents clustering results of the events based on their durations for the 15 minute ahead forecast models according to which four classes a b c and d are identified class a includes events having the lowest durations moving from class a towards d both rainfall duration and depth have increased the classification task has been done by using k means method that is an iterative clustering algorithm in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters likas et al 2003 the most important parameter in a classification system based on knn is k in the classification process k nearest neighbors of a test example in the training set are identified first then based on these k nearest neighbors labels the prediction can be made sun and huang 2010 it is common for each class in the training set to have an uneven distribution of examples some classes may have more examples than others due to this the performance of classification depends greatly on the choice of parameter k zeng et al 2009 one separate prediction model has been built for each class for both 5 minute and 15 minute ahead prediction models results show that the most accurate prediction model is the one associated with class c results of which are presented in table 7 for class c according to table 7 data clustering has resulted to 21 7 12 improvement for 15 minute 5 minute ahead svr forecast model in terms of r 2 measure the least accurate model is that of class a associated with more severe rainfall events where not only classification has not improved the performance of prediction models but also it has decreased the cnn model s accuracy data clustering has led to 10 4 and 15 7 improvement of r 2 for 15 minute 5 minute ahead svr forecast models for classes b and d respectively next based on the rainfall depth and duration one severe rainfall event and one average event of each class were chosen characteristics of which are presented in table 8 consequently the maximum rainfall depth of the first selected event a in each class is more than that of the second event b table 9 and table 10 compares the performances of both 5 minute and 15 minute ahead prediction models for the selected events fig 7 illustrates each event s predicted rainfall depths of the investigated models compared to the observed values for both 5 minute and 15 minute ahead forecast models according to the results presented above the best model for the case of 15 minute ahead forecasts is the pso svr followed by the lstm model whereas for the case of 5 minute ahead forecasts the lstm deep learning model making use of a larger dataset has performed better than cnn and pso svr additionally the lstm prediction accuracy has improved while increasing rainfall duration for both 5 and 15 minute forecast models generally cnn and pso svr have been the least accurate model for 15 minute and 5 minute ahead forecast cases respectively the main success of cnn has been in image captioning applications jose 2019 and it is less sensitive to modeling time dependent variations compared to lstm networks chong et al 2020 karevan and suykens 2020 finally according to fig 7 and table 10 all three approaches have been less accurate in predicting higher peak flows of events 1 3 5 and 7 compared to lower peak flows of events 2 4 6 and 8 4 2 assessing the role of additional predictors in this section on the availability of no other influencing variables than rainfall depths such as air temperature humidity and pressure we aim to investigate if the performances of the models presented can be enhanced by using additional information on the past rainfall fluctuations in the first scenario we test what would be the impact of adding the differences of rainfall depths to the model predictors on the performances of the forecasting approaches two additional difference variables of δ p t 2 p t 2 p t 3 and δ p t 1 p t 1 p t 2 were added to the input variables already chosen as predictors for both 5 and 15 minute ahead forecast models of the three studied forecasting approaches therefore six new updated forecast models are generated results are presented and shown in table 11 and fig 8 for the pso svr approach which performed not very well in the case of 5 minute ahead forecasts previously one can see that the model performance has improved significantly for all events belonging to four classes especially with respect to predicting peak flows rmse the same procedure was applied to the 15 minute ahead pso svr forecasting approach that had already performed well enough and it was found that adding these additional difference input variables had no significant impact on the model performance fig 9 compares the two models with updated model and without original additional input variables for two sample events 1 and 7 belonging respectively to classes a and d table 12 also reports r 2 and rmse measures for all eight events investigated another set of forecast models has been tested where preceding 15 and 5 minute rainfall depths as well as 5 minute difference depths are considered as predictors in forecasting the next 15 minute rainfall depth in other words in addition to three original past rainfall depths over previous 15 30 and 45 min nine rainfall depths of previous 5 minute time steps each triple of which corresponds to one original input variable 3 3 9 and eight 5 minute difference rainfalls 8 difference values for nine previous 5 minute time intervals are added resulting in 20 input forecast models the basis for testing such f updated models is to test whether or not accounting for finer rainfall fluctuations 5 minute depths within the selected lead time of the forecast model 15 minutes can improve the accuracy of forecasts this is because observed hyetographs of rainfall events show significant variations of rainfall intensity over shorter time periods than the forecast lead time these shorter fluctuations over for example 5 minute periods could influence the next 15 minute rainfall depth which are not sensed by a normal forecast model utilizing rainfall variations over previous 15 minute time steps according to the table 12 and fig 9 the prediction accuracy of the updated model is very close to that of original model showing that adding only the difference variables has not improved the pso svr approach performance for the case of 15 minute ahead forecast model however the f updated model benefiting from both additional difference variables and rainfall fluctuations over finer periods than the forecast lead time has improved the forecast accuracy r 2 from 3 to 15 for different events the same versions of the models updated and f updated were tested for the lstm and cnn approaches where no significant change compared to the associated original models were observed while using any types of difference variables as additional predictors the only improvement achieved was for the case of 15 minute ahead rainfall forecasts of the f updated version of the lstm forecasting approach as illustrated in fig 10 for the two sample events 1 and 7 we also checked that the f updated versions of both the pso svr and lstm approaches forecasting the next 15 minute rainfall depth outperformed those of associated 5 minute forecast models that were applied recursively over next three 5 minute time steps this is because another alternative approach to forecast the next 15 minute rainfall depth is to recursively apply a 5 minute ahead forecast model three times 5 conclusion this study investigated the performances of three ml and dl based approaches of support vector regression optimized by particle swarm optimization pso svr long short term memory lstm and convolutional neural network cnn in short term rainfall forecasting two 5 minute and 15 minute ahead forecast models of rainfall depth were developed for each of three approaches the choice of the forecast model s time step or lead time depends on for what purpose the forecast model results will be used for instance the decision time interval of a forecast based operational model where the rainfall forecast model is linked to a real time operation model of urban drainage systems see for example jafari et al 2020 although cnns have shown promising results in image captioning applications and modeling spatially varying processes represented by the related maps the pso svr and lstm approaches performed better in this study than cnn for the case of event based modeling the knn method was used to classify rainfall events with respect to their severity and duration and specific forecast models were developed for each class of events rainfall classification resulted in improved forecast models of the pso svr and lstm type depending on the forecast model s lead time further improvements were achieved through using additional predictors i e rainfall depth differences and rainfall depth fluctuations over shorter time spans than the forecast lead time these additional input variables helped the forecast model sense rainfall fluctuations over shorter time periods δ t than the forecast lead time δ t k δ t better than a model having forecast lead time equal to δ t that is applied recursively k times note that making use of these additional input variables needs no more measurements and information on other processes than rainfall influencing rainfall variations such as temperature humidity etc however there is still room for benefiting from additional sources of data on these influential processes in case of data availability using lstm or other deep learning based ml approaches capable of handling a large set of data representing spatio temporal interdependent patterns credit authorship contribution statement fatemeh rezaie adaryani methodology software validation formal analysis investigation resources writing original draft visualization s jamshid mousavi conceptualization methodology validation formal analysis investigation resources data curation writing review editing supervision fatemeh jafari conceptualization methodology validation investigation resources data curation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2817,short term rainfall forecasting using machine learning based approaches of pso svr lstm and cnn fatemeh rezaie adaryani methodology software validation formal analysis investigation resources writing original draft visualization s jamshid mousavi conceptualization methodology validation formal analysis investigation resources data curation writing review editing supervision fatemeh jafari conceptualization methodology validation investigation resources data curation writing original draft department of civil and environmental engineering amirkabir university of technology tehran iran department of civil and environmental engineering amirkabir university of technology tehran iran department of civil and environmental engineering amirkabir university of technology tehran iran corresponding author at amirkabir university of technology tehran polytechnic tehran tehran iran amirkabir university of technology tehran polytechnic tehran tehran iran this manuscript was handled by corrado corradini editor in chief with the assistance of vahid nourani associate editor short term rainfall forecasting plays an important role in hydrologic modeling and water resource management problems such as flood warning and real time control of urban drainage systems this paper compares the performances of three machine and deep learning based rainfall forecasting approaches including a hybrid optimized by pso support vector regression pso svr long short term memory lstm and convolutional neural network cnn the approaches are used to develop both 5 minute and 15 minute ahead forecast models of rainfall depth based on datasets of niavaran station tehran iran results of applying the models to all data points indicated that pso svr and lstm approaches performed almost the same and better than cnn subsequently rainfall events were divided into four classes depending on their severity and duration using k nearest neighbor method and a separate forecast model was built for each of the classes classification of the events improved the forecast models accuracy where pso svr and lstm were the best approaches for the 15 minute and 5 minute ahead rainfall forecast models respectively investigating the impact of more predictors on the forecast quality adding differences of rainfall depths to model predictors improved the accuracy of pso svr approach for the 5 minute ahead forecast model up to 13 furthermore depending on the rainfall event additional input variables considering rainfall depth fluctuations over shorter time periods than the forecast lead time increased the performances of the pso svr and lstm approaches between 3 15 and 2 10 respectively keywords rainfall forecasting machine learning deep learning particle swarm optimization data availability data will be made available on request 1 introduction precipitation being a key important process of hydrologic cycle plays a vital role in maintaining the balance between freshwater and saltwater resources of the globe it is one of the most studied component in hydrology and climate sciences as it directly or indirectly affects our society duong et al 2018 most hydrological processes usually show a high degree of temporal and spatial variety which induces non stationarity in the measured hydrologic data jhong et al 2019 precise forecasting of rainfall is necessary for proper estimation of water budget as well as to plan strategize maintain the water storage options during extreme weather conditions and design of flood warning systems hammad et al 2021 rainfall forecasting is challenging because of chaotic weather behavior and has been studied by many researchers globally mehr et al 2019 chong et al 2020 note that some statistical methods may not be suitable for rainfall forecasting because historical data usually changes dramatically in a short time there are several works applying artificial neural network ann and soft computing techniques for time series modeling and prediction of hydrologic processes in general and precipitation in particular e g luk et al 2000 fotovatikhah et al 2018 moazenzadeh et al 2018 e lucas et al 2020 gao et al 2020 a number of research works such as zhang et al 2016 have discussed the application of artificial intelligence ai methods in rainfall forecasting although ann based approaches have been integrated into modeling systems they have revealed evidence of gaps that have limited their application in certain areas for example ann is lacking in precision and may produce unexpectedly poor results in non stationary time series modeling apart from its relative novelty it is this tantalizing mixture of successes and failures that add to the lure of anns govindaraju and rao 2013 however significant modifications and improvements have been made to classical anns since their introduction with numerous applications in hydrologic and hydrosystems modeling for instance nourani 2017 showed the superiority of emotional ann eann over a conventional feed forward neural network ffnn in a daily rainfall runoff modeling application there are several other studies where relatively more advanced approaches than conventional ann such as ga anfis least square support vector machine lssvm and multi strategy ensemble learning have improved the performance of the classical fann approach e g ganjidoost et al 2016 nourani et al 2019 ravichandran et al 2021 different prediction models can be developed using machine learning ml techniques that is a branch of ai emphasizing learning from data identifying patterns and making decisions with minimal human interference nayak and ghosh 2013 developed a support vector machine svm algorithm to predict extreme rainfalls they observed that weather patterns before the extreme rainfall events during nighttime are different from those during daytime so they introduced a two phase support vector classifier for extreme rainfall predictions the predictive model ability is impacted substantially by noises wu et al 2009 gupta and gupta 2019 furthermore strong correlations found in rainfall time series data leads to time lagged predictions for instance this problem has been investigated by de vos and rientjes 2005 for an ann based rainfall forecasting model one of the most important developments in modern computational science is the interaction between optimization and machine learning algorithms an optimization algorithm is aprocedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found formulations and methods of optimization are becoming increasingly important to designing algorithms to mine information from vast amounts of data for instance ravansalar et al 2017 used a hybrid wavelet linear genetic programming wlgp model to predict the monthly streamflow in two gauging stations they compared the wlgp model with a single lgp ann a hybrid wavelet ann wann and multi linear regression mlr models and showed that the wlgp models could significantly improve the accuracy of streamflow and other hydrological processes forecasting approaches with the advent of general purpose graphics processing units gpu and their use in ml researchers can now train large scale neural networks such as deep learning models that have previously been restricted due to high computational cost for their training long short term memory lstm and convolutional neural network cnn are among well known algorithms used for time series modeling and prediction cnn is a class of a nonlinear dynamical system that holistically models a time series of data through keeping track of the temporal relationships over time poornima and pushpalatha 2019 for example in the recurrent neural network rnn paradigm every point in a time series is assumed to be linked to every single time instance however that is not always the case especially when a different set of time instances emerges consequently the cnn architecture reduces time dependence structure in comparison to rnn and models a time series simultaneously chong et al 2020 bandara et al 2020 presented a prediction model that can be used with different types of rnn models on subgroups of similar time series which are identified by time series clustering techniques this procedure has also been used to improve baseline lstm networks essien and giannetti 2019 and cnn kim et al 2017 zhang et al 2021 first used the weighted k means clustering method to select the meteorological data of the surrounding stations related to a target station next the high altitude shear value of the target station was calculated then to reduce dimensions of the high altitude shear value and the surface factors they used the principal component analysis pca method finally a cnn was implemented to forecast rainfall this multi component forecasting approach outperformed the other existing approaches such as multilayer perceptron mlp autoregressive integrated moving average arima and backpropagation neural network bpnn with respect to threat rating ts and mean square error mse measures roy and singh 2020 proposed a hybrid approach benefiting from particle swarm optimization pso and grey wolf optimization gwo integrated with two classical ml models namely ann and adaptive neuro fuzzy system anfis for one day ahead streamflow forecasting in a catchment the results demonstrated that connecting optimization algorithm improved the forecast accuracy ni et al 2019 developed two hybrid lstm models for monthly rainfall and streamflow forecasting using a coupled wavelet transform algorithm wlstm and a coupled convolutional neural network clstm respectively the forecast accuracies of these models were compared with those of multi layer perceptron mlp ann and lstm results showed the superiority of the wlstm and clstm models over the other alternatives hammad et al 2021 developed a novel coupled wavelet multi order time lagged neural network wmtlnn model to forecast rainfall by using daily rainfall records at three meteorological stations and compared it with time lagged neural network tlnn models wavelet coupled time lagged neural network tlnn and lstm models results indicated that wmtlnn outperformed the other models although rainfall forecasting can provide a precise estimate of the associated risk in design and operation of hydrosystems and aids in implementing effective mitigation measures in either short or long term planning afshin et al 2011 zambrano et al 2018 chong et al 2020 the comprehensive comparison of their applicability particularly in short term predictions under heavy rainfall events has rarely been discussed zhang et al 2016 short term rainfall forecasting is a complex important issue as it impacts people s lives afshin et al 2011 reported that short term predictions were more sensitive than long term ones thus more difficult to reach the desired accuracy burlando et al 1993 investigated the performance of an autoregressive moving average arma in short term rainfall forecasting to provide estimates of flood hydrographs at key points nasseri et al 2008 developed an ann network to forecast rainfall for a number of time series and investigate the impacts of different parameters on the short term rainfall forecast accuracy in urbanized catchments rainfall forecasts at short intervals are notable as they demonstrate a fast hydrologic response luk et al 2000 improving urban drainage systems operation during severe flood events is one of the applications requiring short term rainfall forecasts that has been investigated by several researchers for instance chiang and willems 2015 combined the genetic algorithm ga optimization approach and a model predictive control mpc technique to develop a real time flood control model jafari et al 2020 investigated the role of short term rainfall forecasts in predictive real time optimal operation prtop where five adaptive prtop models were compared in this study we develop and test three forecasting approaches of pso svr lstm and cnn in short term rainfall forecasting two 5 and 15 minute ahead rainfall depth forecasting models are examined finally the role of additional rainfall features used as the model predictors including differences of rainfall depths and rainfall depth fluctuations within the forecast lead time is also investigated as a novel aspect of the study 2 case study and data precipitation time series data between years 1974 to 2014 measured at niyavaran station of tehran capital of iran is used see fig 1 it consists of 368 precipitation events and a total number of 29 039 and 10 129 data rows for 5 and 15 minute time intervals respectively the duration of investigated precipitation events varies from 5 to 2225 min complexity is an obstacle to find first the right model for forecasting and second for setting up the parameters of the model properly sadaei et al 2019 model parameters need to be estimated including those of ml forecasting models model complexity increases when the number of its parameters is large such as in deep learning dl ml models where the computation process required for training and parameter estimation may become troublesome roelofs et al 2019 the performance of dl models depends upon a number of parameters called hyper parameters hyper parameters are user set training variables which are determined before the model training automated techniques have also been proposed for tuning hyper parameters but these methods may not provide insight to the end users about the interactions among different hyper parameters and their relative importance hutter et al 2014 on the other hand in absence of enough sample data supporting the parameter set estimation an increased number of parameters does not necessarily help and there will be more potentiality for the model performance to decrease due to for example overfitting in this study the dataset has been divided to train test and validation sets with relative percentages of 70 20 and 10 respectively in other words a part of training dataset is used for cross validation while applying an iterative process each time the process is repeated a part of the data is used to train the forecast model and the rest is used to validate the model the process is a resampling method to estimate the model error the validation dataset serves for the purpose of preventing the trained model from overfitting see fig 2 for the dl lstm and cnn models the number of data used in every epoch a hyper parameter named batch size to train the network kandel and castelli 2020 has been parametrized based on a sensitivity analysis 2 1 data preprocessing to help the training process of the forecast models investigated in this study one can perform data normalization as a preprocessing approach using the following equation 1 xref i x i m i n xref i m a x xref i m i n xref i where x i is the prediction value xref i is the observed value and xref i is the observed normalized value all for event i random processing is a theory in modeling formed based on statistics and probability and is used to analyze data in most cases random processes are indexed by time markov process is a model to show a sequence of random variables in which each event s probable occurrence highly depends on the prior one and other events do not get involved foufoula georgiou and lettenmaier 1987 to develop a rainfall prediction model using anns a general markovian process is utilized as follows 2 p t 1 f p t p t 1 p t 2 p t k 1 e t in the above equation t is time step p t is depth of rainfall in period t f is generally a nonlinear function mapping previous rainfall depths to that of the current rime e t is an error random term and k is effective lag time of the prediction model representing what number of previous rainfall depths would affect the current rainfall depth markov 1987 according to a sensitivity analysis with respect to the number of previous rainfall depths as the predictor vector the best model performances were respectively resulted for lag t i m e 1 3 and 3 for pso svr lstm and cnn forecasting approaches details of which are presented in the next section additionally analysis of autocorrelation function acf for different lags showed meaningful autocorrelations between rainfall time series data especially for lags 1 to 3 with an average value of 0 689 and 0 498 for 5 and 15 minute normalized data respectively finally evaluation of the mentioned forecast models requires applying some illustrative steps and performance measures such as displaying the predicted and the observed values compared to line 45 coefficient of determination r 2 and root mean square error rmse has been used in this study to evaluate the performances of the forecast models as follows 3 rmse m m 1 n i 1 n e i 2 4 r 2 1 i e i 2 i x i m e a n xref i 2 where n is the number of data and e i is the difference between observed and predicted data 3 methodology 3 1 modeling tools and model setup 3 1 1 particle swarm optimization pso kennedy and eberhart kennedy and eberhart 1995 introduced pso as a population based optimization algorithm driven by birds flocking and schooling fish patterns pso works generally similar to other evolutionary computation techniques angeline 1998 where a population of random solutions is created to initialize the system and the generation of optimal solutions occurs after it over successive iterations pso does not come with the evolution operators which are presented in ga wu et al 2015 within the problem space potential solutions or particles move by following the two best solutions found so far among all generated solutions gbest and among the solutions of the current generation pbest pso is computationally efficient both in speed and memory requirements compared to other derivative free methods pso reduces the number of parameters to tune details on the pso algorithm formulation how it works and its several successful applications in hydrosystems modeling and optimization and hydrologic model calibration can be found elsewhere such as mousavi and shourian 2010 shourian et al 2008 alizadeh and mousavi 2013 kamali et al 2013 in this study we use pso to determine parameter values of the support vector regression svr forecast model optimally 3 1 2 support vector regression in svr a nonlinear mapping ϕ is used to map the data x i into a higher dimensional feature space and the multidimensional nonlinear problem is formulated as a high dimensional linear problem that is to determine the regression function below 5 f x t w t φ x t b estimation of an svr model parameters is formulated as a quadratic optimization problem the objective of which is minimizing the structural risk unlike the traditional statistical learning theory this overcomes the overfitting problem and the underlying optimization can be solved effectively so the generalization power of the approach is high using vapnik s ε insensitive loss function bottou and vapnik 1992 one can solve the following optimization problem to minimize the structural risk function 6 m i n 1 2 w t w c i 1 t ξ i c i 1 t ξ i subject to 7 y t w t φ x t b ε ξ t where w r n b r and ϕ represents a high dimensional feature mapping from the input space to the feature space note that indicates a vector symbol the dual optimization problem using the lagrange function can be written as follows 8 m i n 1 2 i j 1 t α i α i α j α j k x i x j ε i 1 t α i α i i 1 t α i α i subject to 9 i 1 t α i α i 0 0 α j c i 1 2 t where k x i x j is the kernel function on which the solution of the dual problem depends c is referred to as regularization parameter ε is the error sensitivity parameter and sigma σ controls the level of model non linearity wu and wang 2009 for the svr model the percentages of training and testing data sets were considered as 80 and 20 percent respectively the pso algorithm has been used to optimize five parameters of svr including 1 type of the kernel function from three options of polynomial linear and radial basis functions 2 sigma 3 c 4 epsilon and 5 degree of the polynomial function in case the optimal kernel function selected is of polynomial type the procedure used to optimize parameter values of the svr model using pso has been illustrated in fig 3 table 1 reports the best parameter values of the pso svr model developed in this study 3 1 3 long short term memory lstm networks lstm networks are among well known dl based models the operations inside lstm cells are similar to those in traditional rnns but they are more complex enabling an lstm model to learn long term and short term dependencies from data information can be stored in an additional state in lstm networks compared to traditional rnns malhotra et al 2015 as shown in fig 4 x t is the input vector h t represents the hidden state of the lstm cell at time t and c t represents the cell state of the lstm at time t the structure of an lstm cell indicates that it transfers its hidden state h t and its state c t to the next time step there are three gates in an lstm cell that maintain and adjust the cell state and the hidden state including a forget gate f t an input gate i t and an output gate o t fischer and krauss 2018 these gates serve different functions forget gates f t determine what information is removed from a cell state input gates i t determine how much information ought to be retained in the cell state c t and in output gates o t specific information from the state of the cell is used as output o t lstm cells are able to capture the complex correlations in both short term and long term time series via the functions of the three gates a major improvement over traditional rnn zhao et al 2017 more details on updating cell state c t and hidden state h t and how the lstm model works can be found elsewhere such as hochreiter and schmidhuber 1997 lstm has a number of parameters to be fine tuned li et al 2019 and hutter et al 2014 used adam learning algorithm to set the most appropriate parameter values of an lstm network adam is a computationally efficient optimization algorithm that exhibits invariance to diagonal rescaling of gradients two recently popular methods i e adagrad duchi et al 2011 and rmsprop tieleman and hinton 2012 are combined and used in this method the parameter values of the lstm forecast model including training initial rate ratio reduction value at each period cycle and maximum repetition value of each category were set to 0 01 0 9 and 20 respectively using the adam algorithm implemented according to the recommendations of kingma and ba 2014 other parameters of the lstm forecast model were determined based on a sensitivity analysis as presented in table 2 3 1 4 convolutional neural network cnn cnns are built up of different combinations of three main layers the convolutional pooling and fully connected layers fig 5 shows the general architecture of a deep cnn it has two convolution layers conv1 conv2 two pooling layers pool 1 pool 2 and a fully connected layer fcl haidar and verma 2018 a cnn s main building block is the convolutional layer neurons of a layer in a network are typically connected to all of their neighboring neurons of a previous layer when convolutional layers are used neurons are connected to local regions of earlier layers cnns learn from data through their hierarchical layer structure while going deeper with layers it increases the capability of understanding complex relationships within input features so number of hidden unit is one of the parameters which affects model capability as illustrated in fig 6 dropout is usually incorporated to avoid overfitting aswin et al 2018 estimation of the cnn parameters has been dealt with in previous studies such as noh et al 2016 and zhang et al 2019 again we used the adam algorithm adopted under recommendations of kingma and ba 2014 to determine the cnn model parameters where an initial learning rate equal to 0 001 ratio reduction value at each period cycle of 0 9 and a maximum repetition value of each category equal to 100 were resulted a random removal layer dropout layer was included to prevent the cnn network from overfitting so that 50 of the prior layer hidden layer was set to zero in each learning iteration a sensitivity analysis was conducted to determine the other parameter values of the cnn forecast model such as lag time minimum batch size and the number of hidden unit as presented in table 3 4 results this section compares the results of three mentioned forecasting approaches of pso svr lstm and cnn for both 5 minute and 15 minute ahead models in the first step results are presented considering all the rainfall events together excluding zero depth rainfalls by choosing 80 of data as training and the rest as testing data subsequently the three forecasting approaches are compared while forecasting eight selected rainfall events finally additional forecast models are considered to investigate the effect of increasing the number of model predictors that may improve the model performances table 4 presents what kind of data has been considered for various models the results obtained in the first step are presented in table 5 according to table 5 the performances of the models in predicting next 5 and 15 minute rainfall depths are close but the dl based lstm approach has performed a bit better than the others 4 1 event based forecasting since the approaches performed more or less the same in this stage we investigate if data clustering would affect their performances rainfall classification can be used for the purpose of declining nonlinearity of existing patterns in data minns and hall 1996 so enhancing the model prediction accuracy which can be done based on factors such as occurrence time of events seasons or months duration of events and so forth table 6 presents clustering results of the events based on their durations for the 15 minute ahead forecast models according to which four classes a b c and d are identified class a includes events having the lowest durations moving from class a towards d both rainfall duration and depth have increased the classification task has been done by using k means method that is an iterative clustering algorithm in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters likas et al 2003 the most important parameter in a classification system based on knn is k in the classification process k nearest neighbors of a test example in the training set are identified first then based on these k nearest neighbors labels the prediction can be made sun and huang 2010 it is common for each class in the training set to have an uneven distribution of examples some classes may have more examples than others due to this the performance of classification depends greatly on the choice of parameter k zeng et al 2009 one separate prediction model has been built for each class for both 5 minute and 15 minute ahead prediction models results show that the most accurate prediction model is the one associated with class c results of which are presented in table 7 for class c according to table 7 data clustering has resulted to 21 7 12 improvement for 15 minute 5 minute ahead svr forecast model in terms of r 2 measure the least accurate model is that of class a associated with more severe rainfall events where not only classification has not improved the performance of prediction models but also it has decreased the cnn model s accuracy data clustering has led to 10 4 and 15 7 improvement of r 2 for 15 minute 5 minute ahead svr forecast models for classes b and d respectively next based on the rainfall depth and duration one severe rainfall event and one average event of each class were chosen characteristics of which are presented in table 8 consequently the maximum rainfall depth of the first selected event a in each class is more than that of the second event b table 9 and table 10 compares the performances of both 5 minute and 15 minute ahead prediction models for the selected events fig 7 illustrates each event s predicted rainfall depths of the investigated models compared to the observed values for both 5 minute and 15 minute ahead forecast models according to the results presented above the best model for the case of 15 minute ahead forecasts is the pso svr followed by the lstm model whereas for the case of 5 minute ahead forecasts the lstm deep learning model making use of a larger dataset has performed better than cnn and pso svr additionally the lstm prediction accuracy has improved while increasing rainfall duration for both 5 and 15 minute forecast models generally cnn and pso svr have been the least accurate model for 15 minute and 5 minute ahead forecast cases respectively the main success of cnn has been in image captioning applications jose 2019 and it is less sensitive to modeling time dependent variations compared to lstm networks chong et al 2020 karevan and suykens 2020 finally according to fig 7 and table 10 all three approaches have been less accurate in predicting higher peak flows of events 1 3 5 and 7 compared to lower peak flows of events 2 4 6 and 8 4 2 assessing the role of additional predictors in this section on the availability of no other influencing variables than rainfall depths such as air temperature humidity and pressure we aim to investigate if the performances of the models presented can be enhanced by using additional information on the past rainfall fluctuations in the first scenario we test what would be the impact of adding the differences of rainfall depths to the model predictors on the performances of the forecasting approaches two additional difference variables of δ p t 2 p t 2 p t 3 and δ p t 1 p t 1 p t 2 were added to the input variables already chosen as predictors for both 5 and 15 minute ahead forecast models of the three studied forecasting approaches therefore six new updated forecast models are generated results are presented and shown in table 11 and fig 8 for the pso svr approach which performed not very well in the case of 5 minute ahead forecasts previously one can see that the model performance has improved significantly for all events belonging to four classes especially with respect to predicting peak flows rmse the same procedure was applied to the 15 minute ahead pso svr forecasting approach that had already performed well enough and it was found that adding these additional difference input variables had no significant impact on the model performance fig 9 compares the two models with updated model and without original additional input variables for two sample events 1 and 7 belonging respectively to classes a and d table 12 also reports r 2 and rmse measures for all eight events investigated another set of forecast models has been tested where preceding 15 and 5 minute rainfall depths as well as 5 minute difference depths are considered as predictors in forecasting the next 15 minute rainfall depth in other words in addition to three original past rainfall depths over previous 15 30 and 45 min nine rainfall depths of previous 5 minute time steps each triple of which corresponds to one original input variable 3 3 9 and eight 5 minute difference rainfalls 8 difference values for nine previous 5 minute time intervals are added resulting in 20 input forecast models the basis for testing such f updated models is to test whether or not accounting for finer rainfall fluctuations 5 minute depths within the selected lead time of the forecast model 15 minutes can improve the accuracy of forecasts this is because observed hyetographs of rainfall events show significant variations of rainfall intensity over shorter time periods than the forecast lead time these shorter fluctuations over for example 5 minute periods could influence the next 15 minute rainfall depth which are not sensed by a normal forecast model utilizing rainfall variations over previous 15 minute time steps according to the table 12 and fig 9 the prediction accuracy of the updated model is very close to that of original model showing that adding only the difference variables has not improved the pso svr approach performance for the case of 15 minute ahead forecast model however the f updated model benefiting from both additional difference variables and rainfall fluctuations over finer periods than the forecast lead time has improved the forecast accuracy r 2 from 3 to 15 for different events the same versions of the models updated and f updated were tested for the lstm and cnn approaches where no significant change compared to the associated original models were observed while using any types of difference variables as additional predictors the only improvement achieved was for the case of 15 minute ahead rainfall forecasts of the f updated version of the lstm forecasting approach as illustrated in fig 10 for the two sample events 1 and 7 we also checked that the f updated versions of both the pso svr and lstm approaches forecasting the next 15 minute rainfall depth outperformed those of associated 5 minute forecast models that were applied recursively over next three 5 minute time steps this is because another alternative approach to forecast the next 15 minute rainfall depth is to recursively apply a 5 minute ahead forecast model three times 5 conclusion this study investigated the performances of three ml and dl based approaches of support vector regression optimized by particle swarm optimization pso svr long short term memory lstm and convolutional neural network cnn in short term rainfall forecasting two 5 minute and 15 minute ahead forecast models of rainfall depth were developed for each of three approaches the choice of the forecast model s time step or lead time depends on for what purpose the forecast model results will be used for instance the decision time interval of a forecast based operational model where the rainfall forecast model is linked to a real time operation model of urban drainage systems see for example jafari et al 2020 although cnns have shown promising results in image captioning applications and modeling spatially varying processes represented by the related maps the pso svr and lstm approaches performed better in this study than cnn for the case of event based modeling the knn method was used to classify rainfall events with respect to their severity and duration and specific forecast models were developed for each class of events rainfall classification resulted in improved forecast models of the pso svr and lstm type depending on the forecast model s lead time further improvements were achieved through using additional predictors i e rainfall depth differences and rainfall depth fluctuations over shorter time spans than the forecast lead time these additional input variables helped the forecast model sense rainfall fluctuations over shorter time periods δ t than the forecast lead time δ t k δ t better than a model having forecast lead time equal to δ t that is applied recursively k times note that making use of these additional input variables needs no more measurements and information on other processes than rainfall influencing rainfall variations such as temperature humidity etc however there is still room for benefiting from additional sources of data on these influential processes in case of data availability using lstm or other deep learning based ml approaches capable of handling a large set of data representing spatio temporal interdependent patterns credit authorship contribution statement fatemeh rezaie adaryani methodology software validation formal analysis investigation resources writing original draft visualization s jamshid mousavi conceptualization methodology validation formal analysis investigation resources data curation writing review editing supervision fatemeh jafari conceptualization methodology validation investigation resources data curation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2818,conceptual snowmelt runoff hydrological models involve uncertainties originating from various sources for example uncertainties related to the structure of hydrological models robustness parameters calibration approaches and input and output datasets may lead to considerably uncertain hydrological predictions quantifying such uncertainties is an essential engineering and scientific endeavor in this study the newly developed weather research and forecasting model hydrological glacier wrf hydro glacier modeling framework was applied over the snow fed astore catchment in the upper indus basin the hydrological processes were simulated more effectively while demonstrating their uncertainties as well as the overall model robustness three precipitation datasets were used for calibrating and validating the wrf hydro glacier the model parameters with different meteorological forcings were robust for simulating the hydrological processes in astore furthermore the uncertainty contributed by the optimized parameters and precipitation inputs was segregated to simulate the daily streamflow snow cover area sca and evapotranspiration et for streamflow the optimized parameters were the primary source of uncertainty for sca and et the optimized parameters were a major source of uncertainty in spring and summer whereas input precipitation was the key uncertainty in fall and winter overall the robustness of the wrf hydro glacier was demonstrated using multiple optimized parameters from satellite based precipitation datasets while also demonstrating the uncertainties in the prediction of hydrological processes in astore keywords upper indus basin wrf hydro glacier snow covered area dynamically dimensioned search precipitation datasets streamflow prediction abbreviations et evapotranspiration dds dynamically dimensioned search gleam global land evaporation amsterdam model gpcp global precipitation climatology project gpcp modis moderate resolution imaging spectrometer persiann precipitation estimation from remotely sensed information using artificial neural networks sca snow cover area uib upper indus basin wapda water and power development authority pakistan data availability data will be made available on request 1 introduction pakistan s agricultural sector is heavily reliant on water generated by melting snow and glaciers from the mountains of the upper indus basin uib often described as the breadbasket of pakistan clarke 2015 the hydrology of this snow and glacier dominated zone is prone to severe shifts owing to the adverse impacts of climatic variability ul hasson 2016 which may potentially alter the streamflow dynamics of the uib mukhopadhyay 2012 many researchers have been drawn to this complicated climatic feedback system to investigate the effects of climate change on water management and the sustainability of water supplies young and hewitt 1990 archer et al 2010 ismail et al 2020 mehboob and kim 2021 in the uib several models have been used to simulate hydrological processes across the uib for example latif et al 2020 employed a simple degree day hydrological model the snow runoff model srm martinec 1975 within gilgit a densely populated catchment in the uib to project hydrological changes similarly this simple lumped model has been employed to simulate regional hydrological processes in other catchments within the uib such as astore naeem et al 2016 gilgit adnan et al 2017 immerzeel et al 2009 hunza tahir et al 2011 and shigar and shyoke immerzeel et al 2009 lutz et al 2016 also employed a bucket type model spatial processes in hydrology sphy developed by terink et al 2015 in the uib to simulate streamflow ul hasson et al 2019 employed a semi distributed energy balance hydrological model from the university of british columbia ubc quick and pipes 1976 to understand hydrology in the uib most hydrological models employed over the uib or its sub catchments are either lumped or semi distributed models the lack of multiphysics multiscale and fully distributed hydrological models limits our understanding of the hydrology of the region recently ismail et al 2020 employed two hydrological models over the uib the snowmelt runoffmodel srm with glaciers srm g schaper et al 1999 which is an enhanced version of the srm that incorporates glaciers and a process based semi distributed variable infiltration capacity vic model liang et al 1994 incorporated with a glacier model vic glacier the authors investigated the change in streamflow using two calibration approaches the conventional approach wherein calibration was conducted only at the basin outlet and the enhanced approach wherein the models were calibrated with intermediate gauges along with the glacier mass they concluded that an enhanced modeling approach reduced the uncertainties involved in simulating future streamflows however it should be noted that the authors used 25 km grid pacing in their domain which is relatively coarse for a sub catchment study with a simple manual calibration approach for example one of the sub basins of the uib astore is covered by one or two grid cells which cannot accurately represent the physical processes in the basin among the fine scaled hydrological models weather research and forecasting wrf hydro gochis et al 2015 has been employed successfully for several catchments to simulate hydrological processes e g arnault et al 2018 cho and kim 2022 fersch et al 2020 kerandi et al 2018 lee et al 2022 li et al 2020 liu et al 2021 naabil et al 2017 fersch et al 2020 employed the classic wrf and fully coupled wrf hydro over two medium sized catchments in germany and showed that the coupled wrf hydro better captures the water budget li et al 2020 also suggested that the calibrated wrf hydro performs well in capturing the hydrology of a poorly gauged catchment liu et al 2021 successfully evaluated the performance of wrf hydro over the xijiang river basin china which is characterized by a subtropical climate zone the results showed that the model performance was equally high in both the small and large catchments within the basin lee et al 2022 showed that the wrf hydro model can reasonably capture observed streamflow and other hydrological variables in several basins of south korea located in the temperate climate zone they showed that the model can help understand drought propagation from meteorological to hydrological and agricultural droughts however for a snow fed glaciated region eidhammer et al 2021 pointed out that in wrf hydro the glacier is represented as a land class only and once the accumulated snow over glacier grid points melts off in summer it does not contribute to the streamflow thus the streamflow is underestimated to overcome this limitation the authors developed wrf hydro glacier by integrating a snowpack model into the wrf hydro modeling framework see section 2 2 in their study of a glacier in southern norway eidhammer et al 2021 showed that wrf hydro glacier better captures the observed streamflow and glacier mass balance than wrf hydro does a robust hydrological model should perform equally well under the selected optimized parameter set input datasets and calibration or validation periods an efficient hydrological simulation requires an accurate input precipitation dataset beven 2011 as precipitation errors influence the estimation of optimized parameter sets oudin et al 2006 and may lead to uncertainties in the hydrological simulation output bisselink et al 2016 showed that errors in precipitation can change the optimized parameter estimates and introduce uncertainty in the model output model calibration can reduce the uncertainty in hydrological model parameter estimation however such uncertainties are difficult to clarify uncertainties in hydrological models arise mainly from the model structure or poor robustness input datasets validation datasets and model parameters renard et al 2010 weaknesses in model robustness for hydrological model parameters transfer uncertainties to the model outputs qi et al 2019 most previous studies have reported on decreased model performances due to hydrological model parameters and different climatic conditions merz et al 2011 showed that the parameters controlling snow and soil moisture were strongly affected by climatic conditions in most austrian basins thereby altering the model s efficiency similarly li et al 2012 observed distinct optimized model parameters under different climatic conditions implying decreased model robustness and enhanced model uncertainty therefore it is essential to investigate the robustness of hydrological model parameters while considering simulation uncertainties the main objectives of this study were to analyze the impact of the input precipitation dataset in estimating optimized parameters to evaluate the robustness of wrf hydro glacier gochis et al 2015 eidhammer et al 2021 under multiple optimized parameter sets from multiple satellite based precipitation datasets and to quantify and segregate hydrological simulation uncertainty from optimized parameter sets and precipitation datasets to this end we set up wrf hydro glacier for astore a climate sensitive catchment in the uib mehboob and kim 2021 with calibration and validation using three precipitation datasets a calibrated set of parameters from each precipitation dataset was obtained using the dynamically dimensioned search dds approach tolson and shoemaker 2007 for uncertainty analysis we used the analysis of variance anova method déqué et al 2007 yip et al 2011 to segregate the uncertainty from the optimized parameters and precipitation datasets note that this study is the first attempt to employ a fine scale wrf hydro glacier over astore and quantify the multiple sources of uncertainties related to hydrological modeling within the region indeed it can contribute to better understanding of robustness and uncertainties of wrf hydro over snow covered areas 2 materials and methods 2 1 study area the study area astore is a climatologically critical snow fed catchment in the uib stretching from 74 4 75 25 e to 34 78 35 64 n fig 1 located in the high altitude northwest himalayan region the astore catchment has a total area of approximately 4000 km2 of which 14 is glaciated tahir et al 2015 two meteorological stations namely rama and rattu are situated at different altitudes i e 3179 m asl and 2718 m asl respectively and one streamflow gauge doyian 1583 m asl is installed by the water and power development authority wapda pakistan as shown in fig 1 according to the available data the catchment receives approximately 600 750 mm of total annual precipitation and the mean annual flow is 137 m3s 1 however owing to the lack of gauge stations at high altitudes the available observed precipitation dataset cannot lead to the actual streamflow signal at the catchment outlet moreover winter precipitation i e snow is underestimated and gauging errors have been cited in previous studies e g talchabhadel et al 2021 gauge measurements provide only point measurements that cannot adequately describe the terrestrial distribution of precipitation over the complex topography of the himalayas sevruk 1985 sevruk 1989 førland et al 1996 talchabhadel et al 2021 emphasized that in situ precipitation measurements within the himalayan region are inadequate and do not accurately represent precipitation therefore satellite based precipitation datasets are the best available alternatives to in situ datasets for simulating and predicting hydrological processes recent studies within the himalayan region have utilized satellite based precipitation datasets and found them to be effective for simulating hydrology and making climate change impact assessments khan and koch 2018 sharma et al 2020 khatakho et al 2021 nazeer et al 2021 moreover to drive a fully distributed hydrological model a gridded dataset is required thus a satellite based precipitation dataset was utilized in this study as an alternative to in situ data as detailed in section 2 3 1 2 2 model description the wrf hydro model gochis et al 2015 is an extension of the widely used noah and noah land surface models with multi parameterization options noah mp it is a physical based fully distributed and multi parameterized model that provides a routing routine along with channel routing surface subsurface and base flow processes multiphysics options are available in wrf hydro to simulate several hydrological processes e g vegetation dynamics soil moisture runoff and groundwater snow surface albedo precipitation partitioning glacier module and snow dynamics the resolutions of the land surface model lsm grid and routing grid are flexible i e lsm grids of kilometers and routing grids of hundreds of meters respectively allowing a fine resolution for the routing process for snow dynamics wrf hydro uses the three layer snow scheme of niu et al 2011 but cannot reflect streamflow in the summer season in glacier covered regions it includes glaciers only as a land cover category therefore it allows accumulated snow in the summer season to melt after which the underlying ice i e glacier melts or freezes however the glacier does not contribute streamflow or runoff to wrf hydro eidhammer et al 2021 recently eidhammer et al 2021 integrated the crocus snow model brun et al 1989 brun et al 1992 in a wrf hydro modeling framework to develop a wrf hydro glacier model that is activated only over glacier grid points whereas the default three layer snowpack scheme is activated over non glacier grid points the crocus snow model is a multilayer energy and mass balance model that can be forced with meteorological data and was developed to simulate snow cover at high altitudes brun et al 1989 brun et al 1992 in this physically based model the snow layer is flexible and the snow layers can be adjusted to a maximum of 50 layers gerbaux et al 2005 used crocus for the first time to study glacier mass balance and recent studies réveillet et al 2018 revuelto et al 2016 vionnet et al 2019 have also used crocus in the french surfex model to study the mass balance of glaciers in wrf hydro glacier eidhammer et al 2021 the crocus outputs runoff from snow and glacier melt combined as snow and ice can both be located within the same vertical dimension this runoff is then supplied to the terrain routing model which allows the generation of streamflow from glacier ice during the summer season 2 3 data 2 3 1 meteorological forcing and land data as described earlier there are only two meteorological stations in the astore basin and a gridded dataset is required to drive an offline wrf hydro glacier model we used precipitation estimates from the global land data assimilation system gldas version 2 1 rodell et al 2004 the gldas dataset is available at the global scale at spatial and temporal resolutions of 0 25 and 3 h respectively from 2000 to present meteorological forcing data including downward shortwave and longwave radiation wind speed specific humidity surface pressure and near surface air temperature except for precipitation were obtained from the national oceanic and atmospheric administration global data assimilation system gdas derber et al 1991 whereas precipitation data were obtained from the global precipitation climatology project gpcp the gdas data and gpcp precipitation data were merged to acquire gldas to create 3 hourly precipitation estimates for gldas a daily precipitation dataset from gpcp huffman et al 2001 and a disaggregated routine of gdas precipitation were used until november 2015 thereafter only gdas was used gldas version 2 1 can be assessed and downloaded from nasa goddard earth sciences data and information services center http disc sci gsfc nasa gov this product has been used for driving land surface models e g wrf hydro and simulating streamflow in several watersheds sun et al 2020 chao et al 2021 for simplicity we shall henceforth refer to the precipitation data from gldas v2 1 as gpcp precipitation the second precipitation dataset originated from tropical rainfall measuring mission trmm multi satellite precipitation analysis tmpa two versions are available 3b42rt is a real time version whereas 3b42 is a post real time research version with differences in the gauges used for bias correction tong et al 2014 moreover in the 3b42 version the trmm combined instrument tci was used as a calibrator whereas 3b42rt used the trmm microwave imager tmi huffman and bolvin 2013 for this study we used trmm 3b42 which is available at spatial and temporal resolutions of 0 25 and 3 h respectively huffman et al 2007 a detailed description of the dataset was provided by huffman and bolvin 2015 lastly we used the third precipitation dataset namely precipitation estimated from remotely sensed information using artificial neural networks persiann hsu et al 1997 which is available at spatial and temporal resolutions of 0 25 and 3 h respectively from 2000 to present the persiann product uses an artificial neural network technique to estimate the precipitation rate from satellite observations in this study we used this bias adjusted persiann dataset note that the 25 km precipitation dataset from three different products was regridded to the resolution of wrf hydro glacier in this study using the earth system modeling framework esmf regridding tool and the bilinear method land data including land use vegetation cover soil type terrestrial elevation and slope were constructed using the wrf preprocessing system wps archive which is a collection of global terrestrial data it provides two options for terrestrial classification 1 moderate resolution imaging spectrometer modis and 2 united states geological survey usgs anderson 1976 for land cover and vegetation classification usgs and soil category state the geographic soil archive https www2 mmm ucar edu wrf users download get sources wps geog html was used in this study moreover a high resolution digital elevation model dem of 15 arc seconds from the hydrological data and maps based on the shuttle elevation derivatives at multiple scales hydrosheds archive publicly available at http www hydrosheds org was used to create a routing geogrid within the study area defining the terrestrial overland flow subsurface flow and channel routing processes required for wrf hydro glacier modeling 2 3 2 calibration and evaluation data in this study wrf hydro glacier was calibrated for streamflow at the outlet doyian gauging station of the catchment to maximize the nash sutcliffe efficiency nse nash and sutcliffe 1970 using the dds to evaluate the performance of wrf hydro glacier and quantify the uncertainty under the three precipitation datasets we focused on three hydrological variables streamflow snow cover area sca and evapotranspiration et for streamflow data observed at the doyian gauging station installed by the wapda were used for sca we used the nasa modis snow cover product mod10a1 hall et al 2006 at a resolution of 500 m which can be accessed from the national snow and ice data center nsidc www nsidc org the dataset is based on a snow mapping process that mainly uses a normalized difference snow index ndsi with the ndsi value determining whether a cell is snowy or non snowy in this study we used the daily average sca for the study area modis based snow products have been frequently used for the estimation and analysis of sca in the indus basin regions which lie within the himalayan region and are consistent with in situ data singh et al 2021 while the product shows variations in accuracy particularly at high elevations pu et al 2007 tang et al 2013 it has been validated within the himalayas negi et al 2007 mishra et al 2014 and the tibet plateau wang et al 2007 2008 qin et al 2009 moreover modis data have been used to analyze the geospatial dynamics of snow cover ahmad et al 2018 and glacio hydrologic studies in astore farhan et al 2015 tahir et al 2015 farhan et al 2020 ismail et al 2020 owing to the harsh environment observational et data was not available in astore therefore we compared the simulated et with that of the global land evaporation amsterdam model version 3 5a gleam gleam is a series of algorithms that uses multi satellite measurements to approximate the various elements of evapotranspiration individually miralles et al 2011 and has a higher accuracy than other products martens et al 2017 khan and koch 2018 showed that gleam exhibited the lowest error estimates within the range of 0 0 5 mm day within asia pellet et al 2020 also presented gleam to be more reliable in estimating evapotranspiration within five main basins in asia mekong ganges brahmaputra godavari and irrawaddy daily gleam data are available from 1980 to the present at a spatial resolution of 25 km and can be freely downloaded from the gleam website https www gleam eu in this study daily evapotranspiration was derived from gleam maps by taking the average within the study area and comparing it with the simulated evapotranspiration from wrf hydro glacier descriptions of the datasets are provided in table 1 2 4 model calibration for model calibration with streamflow we used the dds algorithm tolson and shoemaker 2007 which is designed to automatically calibrate and search for an optimal solution within user defined iterations this is ideal for a hydrological model in which a large number of parameters must be calibrated specifically the dds algorithm begins searching for the optimal solution globally and narrows to a local search with an increase in the number of iterations this change from global to local solutions is accomplished by dynamically and probabilistically decreasing the size of the parameters from their best estimates and the set of decision variables parameter optimization was performed against daily streamflow using dds which efficiently finds the optimized parameter sets with a smaller number of iterations lespinas et al 2018 compared with the shuffled complex evolution function e g duan et al 1992 which requires thousands of iterations to attain an optimized parameter set the dds algorithm showed the greatest improvements between 100 and 500 iterations and the improvement after 500 iterations was negligible lespinas et al 2018 these characteristics of dds make it suitable for calibrating a fully distributed process based and computationally expensive hydrologic model such as wrf hydro glacier we calibrated wrf hydro glacier for 17 sensitive parameters selected from previous studies within a feasible range table 2 for calibration exercises with streamflow we used 200 iterations of dds for each precipitation dataset all calibrations reported in this study are based on the nse nash and sutcliffe 1970 performance metric which considers water balance weight correlations and variance errors the nse is ideal when its value is 1 and negative values indicate a lower level of skill the nse is expressed by eq 1 1 nse 1 ts 1 n q sim ts q obs ts ts 1 n q sim ts q obs where q sim ts represents the simulated value at the time step t s q obs ts represents the observed value at the time step t s and q obs is the mean observed value we also used kling gupta efficiency kge kling et al 2012 which includes correlation variability and bias terms kge is increasingly applied in model calibration and is expressed by eq 2 2 kge 1 r 1 2 cv sim cv obs 1 2 μ sim μ obs 1 2 here r 1 2 represents the correlation term cv sim cv obs 1 2 represents the variability term and μ s μ o 1 2 represents the bias term r is the pearson correlation coefficient cv sim and cv obs are the coefficients of variance for the simulated and observed values respectively μ sim and μ obs are the mean of the simulated and observed values respectively the range for kge is to 1 0 and the model performance is ideal when its value is 1 whereas negative values indicate poor model performance 2 5 uncertainty quantification in this study we quantified the total uncertainty arising from two primary sources optimized parameters pram and input precipitation datasets pd to quantify the sources of uncertainties for simulating the hydrological processes within astore a multifactor anova model was built with three precipitation datasets and three optimized sets of parameters together with the interaction between these factors the total sum of squares sst was calculated using eq 3 3 sst ss pram ss pd ss pram p d here ss scores are the sums of the square of the difference with respect to the mean and are given by 4 ss pram i a y i y 2 5 ss pd j b y j y 2 6 ss pram p d i a j b y i j y i y j y 2 where a and b represent the categories in pram and pd respectively for example gpcp tmpa and persiann y i is the average of all the observations of category i of variable pram y j is the average of all the observations of category j of variable pd and y is the overall average the uncertainty from each source was computed as the ratio between sst and the sum of squares of the individual sources the value has a range of 0 100 where 0 indicates no contribution and 100 indicates that the given source is fully responsible for uncertainty for streamflow the analysis was conducted using low flow 5th quantile q5 medium flow 50th quantile q50 high flow 95th quantile q95 and mean annual streamflow for sca and et we quantified uncertainties over four seasons winter december january and february djf spring march april and may mam summer june july and august jja and fall september october and november son 2 6 experimental design in this study we constructed a wrf hydro glacier model with domain data from the lsm at a resolution of 4 400 m and routing and overland flow domains at a resolution of 440 m along with the default 50 snow layers in astore then the wrf hydro glacier model was calibrated with 3 h precipitation datasets see section 2 3 1 for details using the dds algorithm see section 2 4 for details to achieve three sets of optimized parameters for each precipitation dataset the model was calibrated for two years from 2009 to 2010 this period was selected based on the total volume of precipitation received in a year among the available observed datasets we defined 2009 as a dry year because it received low precipitation conversely 2010 was a wet year not only was it characterized by the highest for 2009 2013 received precipitation but the precipitation also led to floods in pakistan webster et al 2011 rehman et al 2016 the model was validated for the 2006 2008 and 2011 2013 periods the period from 2006 to 2008 was comparatively dry except for 2006 whereas in the period from 2011 to 2013 there were two consecutive flood years 2012 and 2013 rehman et al 2016 this approach enabled us to validate the model for dry intermediate and wet years furthermore we performed additional calibrations with each precipitation dataset for 2012 2013 to examine the possible impact of non stationary climate conditions on the optimized parameters however the trend of the optimized parameters was qualitatively the same during the two different calibration periods fig 2 and thus the optimized sets of parameters from 2009 to 2010 were used for validation and analysis in this study to investigate the robustness of the model parameters and uncertainties see section 2 5 for details we transferred optimized parameter sets calibrated over the 2009 2010 period only from one precipitation dataset to the other two that is gpcp based optimized parameters were forced with tmpa and persiann tmpa based optimized parameters were forced with gpcp and persiann and persiann based optimized parameters were forced with gpcp and tmpa over the entire time period from 2006 to 2013 lastly we repeated nine runs using three different precipitation datasets and three calibrated parameter sets each optimized parameter set reflected the characteristics of the individual precipitation dataset thereby enabling us to capture the model parameter robustness and model output uncertainty 3 results and discussion 3 1 calibration and validation of wrf hydro glacier wrf hydro glacier was calibrated using dds as detailed in section 2 4 fig 2 depicts the nse value with the number of iterations for the individual precipitation dataset over two different calibration periods and notably the nse value almost stabilized after the 180th iteration within 200 iterations nse values of 0 73 0 70 and 0 66 were achieved for gpcp tmpa and persiann respectively during the calibration period of 2009 2010 similarly nse values of 0 75 0 74 and 0 69 were achieved for gpcp tmpa and persiann respectively during the calibration period of 2012 2013 in both calibration periods the trends of nse values were the same that is highest for gpcp followed by tmpa and persiann this may be attributed to the fact that the gpcp data are known to sufficiently capture the spatiotemporal trend of monsoon precipitation within pakistan palazzi et al 2013 compared to various satellite based precipitation products kanda et al 2020 found that gpcp and tmpa were more reliable while persiann exhibited deviations from the observed values yamamoto 2011 over the himalayas furthermore by showing similar calibration results between the two independent calibration periods we suggest that our calibration results were robust we also found that the optimized parameters from the three parameter sets yielded different optimized values this was recorded as a state of equifinality wherein non unique parameter sets showed equally good or satisfactory model outputs in a hydrological model equifinality is considered one of the primary sources of total uncertainty table 2 shows all the 17 parameters with default upper and lower values during the calibration period the model was validated with only the corresponding precipitation dataset using the optimized parameters from each precipitation dataset and two metrics nse and kge fig 3 were used to evaluate the performance of the model the results indicate that in the calibration period the nse and kge were 0 73 and 0 79 respectively for gpcp forced simulation whereas the scores were 0 70 and 0 83 respectively in the validation period which were best scores among the three precipitation datasets for the tmpa forced simulation the nse and kge were 0 70 and 0 73 respectively in the calibration period and 0 66 and 0 81 respectively in the validation period the persian forced simulation produced the lowest evaluation scores the nse and kge scores were 0 66 and 0 69 respectively in the calibration period and 0 61 and 0 76 respectively in the validation period moreover the rising and falling streamflow trends were successfully captured with high r values in all three simulations 3 2 model evaluation and robustness 3 2 1 streamflow to evaluate the model performance and examine the robustness of the model parameters the optimized parameter set from one precipitation dataset was transferred to the other two datasets as briefly explained in section 2 6 fig 5 a shows a comparison of the observed mean monthly streamflow with the simulated streamflow from the nine runs from november to march the streamflow fell to its lowest value and the peak was attained in june august during the ice and snow melt season the statistical performance of the streamflow simulation for all nine runs is shown in fig 5a model performance was 0 60 for all metrics i e nse kge and r and for all three precipitation datasets forced with their corresponding optimized parameter sets see minor diagonal values in fig 5a when the gpcp based parameters were forced with the other two precipitation datasets the evaluation metrics exhibited the same trends as those captured by the precipitation datasets see bottom row of fig 5a and table 3 i e high with tmpa and comparatively low with persiann since all precipitation datasets were consistent with one another the streamflow output was relatively insensitive a similar pattern was captured when tpma or persian based parameters were forced with the other three precipitation datasets see the middle and top rows of fig 5a respectively note that the parameters from gpcp tmpa and persiann were expected to show the best nses with the forcings with gpcp tmpa and persiann for streamflow as the optimization was performed by maximizing the nses in each case fig 5a with one exception when persiann based parameters were forced with persian forcing the nse value was 0 62 which was the second best with the best case being when persiann based parameters were forced with gpcp the difference was only 0 01 which is very small and may be attributed to the consistency between gpcp and persiann which upholds the consistency of gpcp s monthly total precipitation liu et al 2015 overall the optimized parameters from all precipitation datasets were deemed robust for simulating streamflow the performance of the model was noted to be either good or satisfactory except when the tmpa based parameters were transferred to persiann in this situation the model performance was unsatisfactory in terms of nse but remained satisfactory for kge this effect may be associated with biases in the precipitation datasets hydrological phenomena are non linear and any bias in the input dataset can cause a substantial bias in the outputs of the hydrological model suet al 2008 moreover in all nine runs the model successfully captured the streamflow trends with a strong correlation coefficient r between the simulated and observed streamflow fig 5a furthermore we examined the impact of the ice glacier component on the streamflow in wrf hydro glacier with meteorological forcings and parameters derived using gpcp as an example by comparing two simulations with wrf hydro and wrf hydro glacier fig 6 note that the runoff from snow and the combined ice glacier melt are supplied to the terrain routing model within wrf hydro glacier thus we compared the difference between the two models to quantify the contribution of ice glacier to streamflow first it is clear that wrf hydro glacier follows the observations more closely than wrf hydro does furthermore fig 6 suggests that the ice glacier contribution to the total streamflow is more than 75 which is in line with previous studies e g farhan et al 2015 ismail et al 2020 3 2 2 snow cover area in astore snow and glacier melt contribute the most to streamflow and the snow cover trend can alter it tahir et al 2015 therefore the robustness of the model parameters and their evaluation were not restricted to streamflow but we also examined the model in relation to the sca modis snow cover data were selected to evaluate the sca prediction ability of the model see section 2 3 2 the mean daily sca fluctuated from 97 in winter to 3 in summer as quantified from modis snow images for astore snow accumulation began in october and reached 80 97 until february march fig 5b the snowmelt begins in early april and the lowest sca occurs in august september at 3 10 the comparison of the modis estimated sca with the simulated sca produced acceptable statistical metrics and showed that the model successfully captured the sca trend in the astore basin fig 5b considering nse and kge when the gpcp based parameters were forced with either tmpa or persiann the average nse and kge scores were higher considering the tpma or persian based parameters when forced with the other three precipitation products the average nse and kge values were above 0 59 and 0 70 respectively see the mean values at the middle and top rows corresponding to nse and kge in fig 5b the model performance was also outstanding in terms of r with an average r greater than 0 86 for all nine combinations the simulated sca from the gpcp or persiann based parameters showed better r scores than those from the tmpa based parameters when forcing was applied to the tmpa precipitation dataset the results were unsatisfactory the reason why gpcp and persiann capture sca better than tmpa is associated with the optimized value of mfsno a parameter that controls sca as shown in table 2 i e 3 0 and 3 2 respectively while that for tmpa was 3 8 thus gpcp and persiann have similarities in simulating sca another reason might be the consistency between gpcp and persiann as persiann upholds the consistency of the monthly total precipitation of gpcp liu et al 2015 nevertheless our results found the model parameters to be robust in simulating sca for all precipitation datasets 3 2 3 evapotranspiration in this section we evaluate wrf hydro glacier and discuss the robustness of the model parameters by considering et aspects fig 5c shows a comparison of the simulated et from the nine runs with the et estimates from gleam this comparison shows that wrf hydro glacier can successfully capture the et trend in astore in most runs the et was overestimated from november to may and underestimated from june to october regarding seasonal variations the et estimated from gleam was higher in summer and lower in winter than that simulated by the wrf hydro glacier this difference can be attributed to the different approaches used by wrf hydro glacier and gleam in wrf hydro glacier the energy balance method is used for accounting for sublimation whereas gleam computes sublimation in snow and ice regions through the priestley taylor equation martens et al 2017 which depends more on the accuracy of satellite based datasets in general the evaluation metrics were within an acceptable range indicating the robustness of the model in simulating et again high evaluation metrics were observed when gpcp based parameters were forced either with tmpa or persiann see bottom row of fig 5c however tmpa based parameters had comparatively lower nse and kge scores than gpcp or persiann based parameters see middle row of fig 5c in summary the optimized set of parameters estimated from all precipitation datasets in the study area was regarded as robust for capturing variations in et 3 3 quantification of uncertainty 3 3 1 streamflow although the model was found to be robust in simulating the hydrology of the study area uncertainties in the output results could not be eliminated the robustness of a model indicates its ability to reproduce satisfactory results in time and space under different conditions whereas uncertainties define the changes in the model under various conditions the robustness of the model alone does not ensure the elimination of uncertainty qi et al 2019 thus we also focused on the uncertainties in the streamflow simulation fig 7 a depicting the optimized parameters as the dominant contributor of uncertainty across all types of flows the overall uncertainty contributions from the optimized parameters and precipitation dataset were 79 2 and 13 respectively and their mutual interaction contributed only 7 8 the contributions of the optimized parameters to the total uncertainty were 68 7 79 7 and 89 3 for low medium and high flow conditions respectively the contribution of the optimized parameters increased from low to high flow which can be explained by the fact that during winter the melting process ceased fig 4b resulting in a reduction in streamflow fig 4a therefore the calibrated parameters would be less sensitive to low flow hence the optimized parameter factors contributed less at low flows than at medium or high flows during medium or high flow seasons the optimized parameters become sensitive to streamflow and dominant contributors to the total uncertainty the contribution of uncertainty from the precipitation datasets continues to decrease from low to high flows among the three flow types the contribution of the uncertainty from the precipitation dataset was the highest under low flow conditions this may be attributed to the fact that low flows occur during winter when the catchment is fully covered by snow this flow reflects the base flow that is the melting of ice and snow and any uncertain rain on snow can generate surface runoff therefore under low flow conditions the uncertainty contribution from the precipitation dataset was relatively high compared to that under medium or low flow conditions note that astore is a region where approximately 75 of streamflow is caused by snow and glacier melt mainly controlled by the optimized parameters thus the precipitation datasets are not a major contributor to the total uncertainty 3 3 2 snow cover area for the sca fig 7b the uncertainty contribution of the precipitation datasets was higher in djf and son whereas the optimized parameters were a dominant contributor in mam and jja for djf and son the uncertainty contributions from the precipitation datasets were 94 2 and 90 8 respectively this is the period when the sca may reach up to 100 therefore precipitation primarily controls the sca this may be because snow increases the sca and rain can accelerate the melting process by transferring energy cohen et al 2015 thereby changing the sca however in mma and jja the optimized parameters were dominant contributors with uncertainty contributions of 90 9 and 90 6 respectively in mam the optimized parameters determined the trends of melting and streamflow which made them the dominant contributors to uncertainty in jja sca fell to its lowest value 3 and most of the precipitation was rainfall hence variations in the sca were unlikely causing precipitation datasets to be minor contributors to the total uncertainty such seasonal variations in the dominant contributor to uncertainty suggest that the precipitation datasets and optimized parameters should be collectively considered when estimating annual water resources in astore 3 3 3 evapotranspiration uncertainties in simulating et were quantified according to season as in the sca fig 7c the precipitation dataset is the main contributor to the total uncertainty in djf and son contributing 65 1 and 93 2 respectively while optimized parameters are major contributors in mam and jja contributing 43 and 53 respectively in djf and son the temperature fell below 0 c and the basin was fully covered with snow and ice during this period very little surface water was available to generate et and et was thus mostly dependent on precipitation events however in mam and jja the snow and glaciers started to melt producing surface flows hence the parameters linked to surface runoff soil or vegetation determined et therefore the optimized parameters significantly contributed to the total uncertainty it is worth noting that in mam when snow and ice start to melt the contribution of the mutual interaction of uncertainty sources is the second highest leading to greater uncertainty in mam 4 conclusions this study sought to address three main objectives to investigate the impact of input precipitation datasets in estimating optimized parameters to evaluate the model robustness under multiple parameter sets from multiple satellite based precipitations and to quantify and segregate the contribution of optimized parameters to model and satellite based precipitation datasets to model uncertainty by focusing on streamflow sca and et globally various hydrological models from lumped to distributed can comprehensively replicate hydrological processes but parametrization and input datasets can lead to ambiguity and unreliability her and chaubey 2015 associated with the lack of robustness of model parameters to examine the robustness of wrf hydro glacier we adopted an approach wherein the fully distributed hydrological model wrf hydro glacier was calibrated with three different precipitation datasets and an optimized set of parameters from each precipitation dataset was estimated the results showed that the three different precipitation datasets produced different optimized parameter sets reflecting the precipitation characteristics from the results obtained it can be concluded that the wrf hydro glacier model was robust in terms of simulating hydrological processes in the study area overall among the three precipitation datasets gpcp showed better results followed by persiann and tmpa however all three datasets can be used to drive a fully distributed model in this snow dominated data scarce region note that kanda et al 2020 suggested that satellite based precipitation datasets are vaguer compared to the temperature over the himalayas thus in this study we considered precipitation datasets to be the only source of uncertainty our uncertainty analysis for streamflow shows that the optimized parameters are major contributors to the total uncertainty in all flow types for sca and et the precipitation datasets are the primary contributors in the winter and fall seasons while the optimized parameter set is the major source of uncertainty in spring and summer therefore if the objective of hydrological modeling is to simulate streamflow the optimized parameter set should be given prime importance whereas for the sca or et both factors that is the optimized parameter set and precipitation dataset should be given equal importance developing the wrf hydro glacier model with fine scale meteorological datasets removes regional uncertainties that may affect coarser resolution or lumped models astore was selected for this study because it is a climate affected and water scarce catchment in the uib mehboob and kim 2021 in our view this advanced hydrological model can help water planners develop policies on water use and aid in the development of hydraulic infrastructure to boost agricultural production in the region declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the basic science research program through the national research foundation of korea funded by the ministry of science ict future planning 2020r1a2c2007670 the technology advancement research program through the korea agency for infrastructure technology advancement kaia funded by the ministry of land infrastructure and transport 22ctap c163540 02 and the korea environmental industry technology institute funded by the ministry of environment 2022003640002 we also acknowledge the water and power development authority wapda of pakistan and pakistan meteorological department pmd for providing the streamflow and meteorological datasets 
2818,conceptual snowmelt runoff hydrological models involve uncertainties originating from various sources for example uncertainties related to the structure of hydrological models robustness parameters calibration approaches and input and output datasets may lead to considerably uncertain hydrological predictions quantifying such uncertainties is an essential engineering and scientific endeavor in this study the newly developed weather research and forecasting model hydrological glacier wrf hydro glacier modeling framework was applied over the snow fed astore catchment in the upper indus basin the hydrological processes were simulated more effectively while demonstrating their uncertainties as well as the overall model robustness three precipitation datasets were used for calibrating and validating the wrf hydro glacier the model parameters with different meteorological forcings were robust for simulating the hydrological processes in astore furthermore the uncertainty contributed by the optimized parameters and precipitation inputs was segregated to simulate the daily streamflow snow cover area sca and evapotranspiration et for streamflow the optimized parameters were the primary source of uncertainty for sca and et the optimized parameters were a major source of uncertainty in spring and summer whereas input precipitation was the key uncertainty in fall and winter overall the robustness of the wrf hydro glacier was demonstrated using multiple optimized parameters from satellite based precipitation datasets while also demonstrating the uncertainties in the prediction of hydrological processes in astore keywords upper indus basin wrf hydro glacier snow covered area dynamically dimensioned search precipitation datasets streamflow prediction abbreviations et evapotranspiration dds dynamically dimensioned search gleam global land evaporation amsterdam model gpcp global precipitation climatology project gpcp modis moderate resolution imaging spectrometer persiann precipitation estimation from remotely sensed information using artificial neural networks sca snow cover area uib upper indus basin wapda water and power development authority pakistan data availability data will be made available on request 1 introduction pakistan s agricultural sector is heavily reliant on water generated by melting snow and glaciers from the mountains of the upper indus basin uib often described as the breadbasket of pakistan clarke 2015 the hydrology of this snow and glacier dominated zone is prone to severe shifts owing to the adverse impacts of climatic variability ul hasson 2016 which may potentially alter the streamflow dynamics of the uib mukhopadhyay 2012 many researchers have been drawn to this complicated climatic feedback system to investigate the effects of climate change on water management and the sustainability of water supplies young and hewitt 1990 archer et al 2010 ismail et al 2020 mehboob and kim 2021 in the uib several models have been used to simulate hydrological processes across the uib for example latif et al 2020 employed a simple degree day hydrological model the snow runoff model srm martinec 1975 within gilgit a densely populated catchment in the uib to project hydrological changes similarly this simple lumped model has been employed to simulate regional hydrological processes in other catchments within the uib such as astore naeem et al 2016 gilgit adnan et al 2017 immerzeel et al 2009 hunza tahir et al 2011 and shigar and shyoke immerzeel et al 2009 lutz et al 2016 also employed a bucket type model spatial processes in hydrology sphy developed by terink et al 2015 in the uib to simulate streamflow ul hasson et al 2019 employed a semi distributed energy balance hydrological model from the university of british columbia ubc quick and pipes 1976 to understand hydrology in the uib most hydrological models employed over the uib or its sub catchments are either lumped or semi distributed models the lack of multiphysics multiscale and fully distributed hydrological models limits our understanding of the hydrology of the region recently ismail et al 2020 employed two hydrological models over the uib the snowmelt runoffmodel srm with glaciers srm g schaper et al 1999 which is an enhanced version of the srm that incorporates glaciers and a process based semi distributed variable infiltration capacity vic model liang et al 1994 incorporated with a glacier model vic glacier the authors investigated the change in streamflow using two calibration approaches the conventional approach wherein calibration was conducted only at the basin outlet and the enhanced approach wherein the models were calibrated with intermediate gauges along with the glacier mass they concluded that an enhanced modeling approach reduced the uncertainties involved in simulating future streamflows however it should be noted that the authors used 25 km grid pacing in their domain which is relatively coarse for a sub catchment study with a simple manual calibration approach for example one of the sub basins of the uib astore is covered by one or two grid cells which cannot accurately represent the physical processes in the basin among the fine scaled hydrological models weather research and forecasting wrf hydro gochis et al 2015 has been employed successfully for several catchments to simulate hydrological processes e g arnault et al 2018 cho and kim 2022 fersch et al 2020 kerandi et al 2018 lee et al 2022 li et al 2020 liu et al 2021 naabil et al 2017 fersch et al 2020 employed the classic wrf and fully coupled wrf hydro over two medium sized catchments in germany and showed that the coupled wrf hydro better captures the water budget li et al 2020 also suggested that the calibrated wrf hydro performs well in capturing the hydrology of a poorly gauged catchment liu et al 2021 successfully evaluated the performance of wrf hydro over the xijiang river basin china which is characterized by a subtropical climate zone the results showed that the model performance was equally high in both the small and large catchments within the basin lee et al 2022 showed that the wrf hydro model can reasonably capture observed streamflow and other hydrological variables in several basins of south korea located in the temperate climate zone they showed that the model can help understand drought propagation from meteorological to hydrological and agricultural droughts however for a snow fed glaciated region eidhammer et al 2021 pointed out that in wrf hydro the glacier is represented as a land class only and once the accumulated snow over glacier grid points melts off in summer it does not contribute to the streamflow thus the streamflow is underestimated to overcome this limitation the authors developed wrf hydro glacier by integrating a snowpack model into the wrf hydro modeling framework see section 2 2 in their study of a glacier in southern norway eidhammer et al 2021 showed that wrf hydro glacier better captures the observed streamflow and glacier mass balance than wrf hydro does a robust hydrological model should perform equally well under the selected optimized parameter set input datasets and calibration or validation periods an efficient hydrological simulation requires an accurate input precipitation dataset beven 2011 as precipitation errors influence the estimation of optimized parameter sets oudin et al 2006 and may lead to uncertainties in the hydrological simulation output bisselink et al 2016 showed that errors in precipitation can change the optimized parameter estimates and introduce uncertainty in the model output model calibration can reduce the uncertainty in hydrological model parameter estimation however such uncertainties are difficult to clarify uncertainties in hydrological models arise mainly from the model structure or poor robustness input datasets validation datasets and model parameters renard et al 2010 weaknesses in model robustness for hydrological model parameters transfer uncertainties to the model outputs qi et al 2019 most previous studies have reported on decreased model performances due to hydrological model parameters and different climatic conditions merz et al 2011 showed that the parameters controlling snow and soil moisture were strongly affected by climatic conditions in most austrian basins thereby altering the model s efficiency similarly li et al 2012 observed distinct optimized model parameters under different climatic conditions implying decreased model robustness and enhanced model uncertainty therefore it is essential to investigate the robustness of hydrological model parameters while considering simulation uncertainties the main objectives of this study were to analyze the impact of the input precipitation dataset in estimating optimized parameters to evaluate the robustness of wrf hydro glacier gochis et al 2015 eidhammer et al 2021 under multiple optimized parameter sets from multiple satellite based precipitation datasets and to quantify and segregate hydrological simulation uncertainty from optimized parameter sets and precipitation datasets to this end we set up wrf hydro glacier for astore a climate sensitive catchment in the uib mehboob and kim 2021 with calibration and validation using three precipitation datasets a calibrated set of parameters from each precipitation dataset was obtained using the dynamically dimensioned search dds approach tolson and shoemaker 2007 for uncertainty analysis we used the analysis of variance anova method déqué et al 2007 yip et al 2011 to segregate the uncertainty from the optimized parameters and precipitation datasets note that this study is the first attempt to employ a fine scale wrf hydro glacier over astore and quantify the multiple sources of uncertainties related to hydrological modeling within the region indeed it can contribute to better understanding of robustness and uncertainties of wrf hydro over snow covered areas 2 materials and methods 2 1 study area the study area astore is a climatologically critical snow fed catchment in the uib stretching from 74 4 75 25 e to 34 78 35 64 n fig 1 located in the high altitude northwest himalayan region the astore catchment has a total area of approximately 4000 km2 of which 14 is glaciated tahir et al 2015 two meteorological stations namely rama and rattu are situated at different altitudes i e 3179 m asl and 2718 m asl respectively and one streamflow gauge doyian 1583 m asl is installed by the water and power development authority wapda pakistan as shown in fig 1 according to the available data the catchment receives approximately 600 750 mm of total annual precipitation and the mean annual flow is 137 m3s 1 however owing to the lack of gauge stations at high altitudes the available observed precipitation dataset cannot lead to the actual streamflow signal at the catchment outlet moreover winter precipitation i e snow is underestimated and gauging errors have been cited in previous studies e g talchabhadel et al 2021 gauge measurements provide only point measurements that cannot adequately describe the terrestrial distribution of precipitation over the complex topography of the himalayas sevruk 1985 sevruk 1989 førland et al 1996 talchabhadel et al 2021 emphasized that in situ precipitation measurements within the himalayan region are inadequate and do not accurately represent precipitation therefore satellite based precipitation datasets are the best available alternatives to in situ datasets for simulating and predicting hydrological processes recent studies within the himalayan region have utilized satellite based precipitation datasets and found them to be effective for simulating hydrology and making climate change impact assessments khan and koch 2018 sharma et al 2020 khatakho et al 2021 nazeer et al 2021 moreover to drive a fully distributed hydrological model a gridded dataset is required thus a satellite based precipitation dataset was utilized in this study as an alternative to in situ data as detailed in section 2 3 1 2 2 model description the wrf hydro model gochis et al 2015 is an extension of the widely used noah and noah land surface models with multi parameterization options noah mp it is a physical based fully distributed and multi parameterized model that provides a routing routine along with channel routing surface subsurface and base flow processes multiphysics options are available in wrf hydro to simulate several hydrological processes e g vegetation dynamics soil moisture runoff and groundwater snow surface albedo precipitation partitioning glacier module and snow dynamics the resolutions of the land surface model lsm grid and routing grid are flexible i e lsm grids of kilometers and routing grids of hundreds of meters respectively allowing a fine resolution for the routing process for snow dynamics wrf hydro uses the three layer snow scheme of niu et al 2011 but cannot reflect streamflow in the summer season in glacier covered regions it includes glaciers only as a land cover category therefore it allows accumulated snow in the summer season to melt after which the underlying ice i e glacier melts or freezes however the glacier does not contribute streamflow or runoff to wrf hydro eidhammer et al 2021 recently eidhammer et al 2021 integrated the crocus snow model brun et al 1989 brun et al 1992 in a wrf hydro modeling framework to develop a wrf hydro glacier model that is activated only over glacier grid points whereas the default three layer snowpack scheme is activated over non glacier grid points the crocus snow model is a multilayer energy and mass balance model that can be forced with meteorological data and was developed to simulate snow cover at high altitudes brun et al 1989 brun et al 1992 in this physically based model the snow layer is flexible and the snow layers can be adjusted to a maximum of 50 layers gerbaux et al 2005 used crocus for the first time to study glacier mass balance and recent studies réveillet et al 2018 revuelto et al 2016 vionnet et al 2019 have also used crocus in the french surfex model to study the mass balance of glaciers in wrf hydro glacier eidhammer et al 2021 the crocus outputs runoff from snow and glacier melt combined as snow and ice can both be located within the same vertical dimension this runoff is then supplied to the terrain routing model which allows the generation of streamflow from glacier ice during the summer season 2 3 data 2 3 1 meteorological forcing and land data as described earlier there are only two meteorological stations in the astore basin and a gridded dataset is required to drive an offline wrf hydro glacier model we used precipitation estimates from the global land data assimilation system gldas version 2 1 rodell et al 2004 the gldas dataset is available at the global scale at spatial and temporal resolutions of 0 25 and 3 h respectively from 2000 to present meteorological forcing data including downward shortwave and longwave radiation wind speed specific humidity surface pressure and near surface air temperature except for precipitation were obtained from the national oceanic and atmospheric administration global data assimilation system gdas derber et al 1991 whereas precipitation data were obtained from the global precipitation climatology project gpcp the gdas data and gpcp precipitation data were merged to acquire gldas to create 3 hourly precipitation estimates for gldas a daily precipitation dataset from gpcp huffman et al 2001 and a disaggregated routine of gdas precipitation were used until november 2015 thereafter only gdas was used gldas version 2 1 can be assessed and downloaded from nasa goddard earth sciences data and information services center http disc sci gsfc nasa gov this product has been used for driving land surface models e g wrf hydro and simulating streamflow in several watersheds sun et al 2020 chao et al 2021 for simplicity we shall henceforth refer to the precipitation data from gldas v2 1 as gpcp precipitation the second precipitation dataset originated from tropical rainfall measuring mission trmm multi satellite precipitation analysis tmpa two versions are available 3b42rt is a real time version whereas 3b42 is a post real time research version with differences in the gauges used for bias correction tong et al 2014 moreover in the 3b42 version the trmm combined instrument tci was used as a calibrator whereas 3b42rt used the trmm microwave imager tmi huffman and bolvin 2013 for this study we used trmm 3b42 which is available at spatial and temporal resolutions of 0 25 and 3 h respectively huffman et al 2007 a detailed description of the dataset was provided by huffman and bolvin 2015 lastly we used the third precipitation dataset namely precipitation estimated from remotely sensed information using artificial neural networks persiann hsu et al 1997 which is available at spatial and temporal resolutions of 0 25 and 3 h respectively from 2000 to present the persiann product uses an artificial neural network technique to estimate the precipitation rate from satellite observations in this study we used this bias adjusted persiann dataset note that the 25 km precipitation dataset from three different products was regridded to the resolution of wrf hydro glacier in this study using the earth system modeling framework esmf regridding tool and the bilinear method land data including land use vegetation cover soil type terrestrial elevation and slope were constructed using the wrf preprocessing system wps archive which is a collection of global terrestrial data it provides two options for terrestrial classification 1 moderate resolution imaging spectrometer modis and 2 united states geological survey usgs anderson 1976 for land cover and vegetation classification usgs and soil category state the geographic soil archive https www2 mmm ucar edu wrf users download get sources wps geog html was used in this study moreover a high resolution digital elevation model dem of 15 arc seconds from the hydrological data and maps based on the shuttle elevation derivatives at multiple scales hydrosheds archive publicly available at http www hydrosheds org was used to create a routing geogrid within the study area defining the terrestrial overland flow subsurface flow and channel routing processes required for wrf hydro glacier modeling 2 3 2 calibration and evaluation data in this study wrf hydro glacier was calibrated for streamflow at the outlet doyian gauging station of the catchment to maximize the nash sutcliffe efficiency nse nash and sutcliffe 1970 using the dds to evaluate the performance of wrf hydro glacier and quantify the uncertainty under the three precipitation datasets we focused on three hydrological variables streamflow snow cover area sca and evapotranspiration et for streamflow data observed at the doyian gauging station installed by the wapda were used for sca we used the nasa modis snow cover product mod10a1 hall et al 2006 at a resolution of 500 m which can be accessed from the national snow and ice data center nsidc www nsidc org the dataset is based on a snow mapping process that mainly uses a normalized difference snow index ndsi with the ndsi value determining whether a cell is snowy or non snowy in this study we used the daily average sca for the study area modis based snow products have been frequently used for the estimation and analysis of sca in the indus basin regions which lie within the himalayan region and are consistent with in situ data singh et al 2021 while the product shows variations in accuracy particularly at high elevations pu et al 2007 tang et al 2013 it has been validated within the himalayas negi et al 2007 mishra et al 2014 and the tibet plateau wang et al 2007 2008 qin et al 2009 moreover modis data have been used to analyze the geospatial dynamics of snow cover ahmad et al 2018 and glacio hydrologic studies in astore farhan et al 2015 tahir et al 2015 farhan et al 2020 ismail et al 2020 owing to the harsh environment observational et data was not available in astore therefore we compared the simulated et with that of the global land evaporation amsterdam model version 3 5a gleam gleam is a series of algorithms that uses multi satellite measurements to approximate the various elements of evapotranspiration individually miralles et al 2011 and has a higher accuracy than other products martens et al 2017 khan and koch 2018 showed that gleam exhibited the lowest error estimates within the range of 0 0 5 mm day within asia pellet et al 2020 also presented gleam to be more reliable in estimating evapotranspiration within five main basins in asia mekong ganges brahmaputra godavari and irrawaddy daily gleam data are available from 1980 to the present at a spatial resolution of 25 km and can be freely downloaded from the gleam website https www gleam eu in this study daily evapotranspiration was derived from gleam maps by taking the average within the study area and comparing it with the simulated evapotranspiration from wrf hydro glacier descriptions of the datasets are provided in table 1 2 4 model calibration for model calibration with streamflow we used the dds algorithm tolson and shoemaker 2007 which is designed to automatically calibrate and search for an optimal solution within user defined iterations this is ideal for a hydrological model in which a large number of parameters must be calibrated specifically the dds algorithm begins searching for the optimal solution globally and narrows to a local search with an increase in the number of iterations this change from global to local solutions is accomplished by dynamically and probabilistically decreasing the size of the parameters from their best estimates and the set of decision variables parameter optimization was performed against daily streamflow using dds which efficiently finds the optimized parameter sets with a smaller number of iterations lespinas et al 2018 compared with the shuffled complex evolution function e g duan et al 1992 which requires thousands of iterations to attain an optimized parameter set the dds algorithm showed the greatest improvements between 100 and 500 iterations and the improvement after 500 iterations was negligible lespinas et al 2018 these characteristics of dds make it suitable for calibrating a fully distributed process based and computationally expensive hydrologic model such as wrf hydro glacier we calibrated wrf hydro glacier for 17 sensitive parameters selected from previous studies within a feasible range table 2 for calibration exercises with streamflow we used 200 iterations of dds for each precipitation dataset all calibrations reported in this study are based on the nse nash and sutcliffe 1970 performance metric which considers water balance weight correlations and variance errors the nse is ideal when its value is 1 and negative values indicate a lower level of skill the nse is expressed by eq 1 1 nse 1 ts 1 n q sim ts q obs ts ts 1 n q sim ts q obs where q sim ts represents the simulated value at the time step t s q obs ts represents the observed value at the time step t s and q obs is the mean observed value we also used kling gupta efficiency kge kling et al 2012 which includes correlation variability and bias terms kge is increasingly applied in model calibration and is expressed by eq 2 2 kge 1 r 1 2 cv sim cv obs 1 2 μ sim μ obs 1 2 here r 1 2 represents the correlation term cv sim cv obs 1 2 represents the variability term and μ s μ o 1 2 represents the bias term r is the pearson correlation coefficient cv sim and cv obs are the coefficients of variance for the simulated and observed values respectively μ sim and μ obs are the mean of the simulated and observed values respectively the range for kge is to 1 0 and the model performance is ideal when its value is 1 whereas negative values indicate poor model performance 2 5 uncertainty quantification in this study we quantified the total uncertainty arising from two primary sources optimized parameters pram and input precipitation datasets pd to quantify the sources of uncertainties for simulating the hydrological processes within astore a multifactor anova model was built with three precipitation datasets and three optimized sets of parameters together with the interaction between these factors the total sum of squares sst was calculated using eq 3 3 sst ss pram ss pd ss pram p d here ss scores are the sums of the square of the difference with respect to the mean and are given by 4 ss pram i a y i y 2 5 ss pd j b y j y 2 6 ss pram p d i a j b y i j y i y j y 2 where a and b represent the categories in pram and pd respectively for example gpcp tmpa and persiann y i is the average of all the observations of category i of variable pram y j is the average of all the observations of category j of variable pd and y is the overall average the uncertainty from each source was computed as the ratio between sst and the sum of squares of the individual sources the value has a range of 0 100 where 0 indicates no contribution and 100 indicates that the given source is fully responsible for uncertainty for streamflow the analysis was conducted using low flow 5th quantile q5 medium flow 50th quantile q50 high flow 95th quantile q95 and mean annual streamflow for sca and et we quantified uncertainties over four seasons winter december january and february djf spring march april and may mam summer june july and august jja and fall september october and november son 2 6 experimental design in this study we constructed a wrf hydro glacier model with domain data from the lsm at a resolution of 4 400 m and routing and overland flow domains at a resolution of 440 m along with the default 50 snow layers in astore then the wrf hydro glacier model was calibrated with 3 h precipitation datasets see section 2 3 1 for details using the dds algorithm see section 2 4 for details to achieve three sets of optimized parameters for each precipitation dataset the model was calibrated for two years from 2009 to 2010 this period was selected based on the total volume of precipitation received in a year among the available observed datasets we defined 2009 as a dry year because it received low precipitation conversely 2010 was a wet year not only was it characterized by the highest for 2009 2013 received precipitation but the precipitation also led to floods in pakistan webster et al 2011 rehman et al 2016 the model was validated for the 2006 2008 and 2011 2013 periods the period from 2006 to 2008 was comparatively dry except for 2006 whereas in the period from 2011 to 2013 there were two consecutive flood years 2012 and 2013 rehman et al 2016 this approach enabled us to validate the model for dry intermediate and wet years furthermore we performed additional calibrations with each precipitation dataset for 2012 2013 to examine the possible impact of non stationary climate conditions on the optimized parameters however the trend of the optimized parameters was qualitatively the same during the two different calibration periods fig 2 and thus the optimized sets of parameters from 2009 to 2010 were used for validation and analysis in this study to investigate the robustness of the model parameters and uncertainties see section 2 5 for details we transferred optimized parameter sets calibrated over the 2009 2010 period only from one precipitation dataset to the other two that is gpcp based optimized parameters were forced with tmpa and persiann tmpa based optimized parameters were forced with gpcp and persiann and persiann based optimized parameters were forced with gpcp and tmpa over the entire time period from 2006 to 2013 lastly we repeated nine runs using three different precipitation datasets and three calibrated parameter sets each optimized parameter set reflected the characteristics of the individual precipitation dataset thereby enabling us to capture the model parameter robustness and model output uncertainty 3 results and discussion 3 1 calibration and validation of wrf hydro glacier wrf hydro glacier was calibrated using dds as detailed in section 2 4 fig 2 depicts the nse value with the number of iterations for the individual precipitation dataset over two different calibration periods and notably the nse value almost stabilized after the 180th iteration within 200 iterations nse values of 0 73 0 70 and 0 66 were achieved for gpcp tmpa and persiann respectively during the calibration period of 2009 2010 similarly nse values of 0 75 0 74 and 0 69 were achieved for gpcp tmpa and persiann respectively during the calibration period of 2012 2013 in both calibration periods the trends of nse values were the same that is highest for gpcp followed by tmpa and persiann this may be attributed to the fact that the gpcp data are known to sufficiently capture the spatiotemporal trend of monsoon precipitation within pakistan palazzi et al 2013 compared to various satellite based precipitation products kanda et al 2020 found that gpcp and tmpa were more reliable while persiann exhibited deviations from the observed values yamamoto 2011 over the himalayas furthermore by showing similar calibration results between the two independent calibration periods we suggest that our calibration results were robust we also found that the optimized parameters from the three parameter sets yielded different optimized values this was recorded as a state of equifinality wherein non unique parameter sets showed equally good or satisfactory model outputs in a hydrological model equifinality is considered one of the primary sources of total uncertainty table 2 shows all the 17 parameters with default upper and lower values during the calibration period the model was validated with only the corresponding precipitation dataset using the optimized parameters from each precipitation dataset and two metrics nse and kge fig 3 were used to evaluate the performance of the model the results indicate that in the calibration period the nse and kge were 0 73 and 0 79 respectively for gpcp forced simulation whereas the scores were 0 70 and 0 83 respectively in the validation period which were best scores among the three precipitation datasets for the tmpa forced simulation the nse and kge were 0 70 and 0 73 respectively in the calibration period and 0 66 and 0 81 respectively in the validation period the persian forced simulation produced the lowest evaluation scores the nse and kge scores were 0 66 and 0 69 respectively in the calibration period and 0 61 and 0 76 respectively in the validation period moreover the rising and falling streamflow trends were successfully captured with high r values in all three simulations 3 2 model evaluation and robustness 3 2 1 streamflow to evaluate the model performance and examine the robustness of the model parameters the optimized parameter set from one precipitation dataset was transferred to the other two datasets as briefly explained in section 2 6 fig 5 a shows a comparison of the observed mean monthly streamflow with the simulated streamflow from the nine runs from november to march the streamflow fell to its lowest value and the peak was attained in june august during the ice and snow melt season the statistical performance of the streamflow simulation for all nine runs is shown in fig 5a model performance was 0 60 for all metrics i e nse kge and r and for all three precipitation datasets forced with their corresponding optimized parameter sets see minor diagonal values in fig 5a when the gpcp based parameters were forced with the other two precipitation datasets the evaluation metrics exhibited the same trends as those captured by the precipitation datasets see bottom row of fig 5a and table 3 i e high with tmpa and comparatively low with persiann since all precipitation datasets were consistent with one another the streamflow output was relatively insensitive a similar pattern was captured when tpma or persian based parameters were forced with the other three precipitation datasets see the middle and top rows of fig 5a respectively note that the parameters from gpcp tmpa and persiann were expected to show the best nses with the forcings with gpcp tmpa and persiann for streamflow as the optimization was performed by maximizing the nses in each case fig 5a with one exception when persiann based parameters were forced with persian forcing the nse value was 0 62 which was the second best with the best case being when persiann based parameters were forced with gpcp the difference was only 0 01 which is very small and may be attributed to the consistency between gpcp and persiann which upholds the consistency of gpcp s monthly total precipitation liu et al 2015 overall the optimized parameters from all precipitation datasets were deemed robust for simulating streamflow the performance of the model was noted to be either good or satisfactory except when the tmpa based parameters were transferred to persiann in this situation the model performance was unsatisfactory in terms of nse but remained satisfactory for kge this effect may be associated with biases in the precipitation datasets hydrological phenomena are non linear and any bias in the input dataset can cause a substantial bias in the outputs of the hydrological model suet al 2008 moreover in all nine runs the model successfully captured the streamflow trends with a strong correlation coefficient r between the simulated and observed streamflow fig 5a furthermore we examined the impact of the ice glacier component on the streamflow in wrf hydro glacier with meteorological forcings and parameters derived using gpcp as an example by comparing two simulations with wrf hydro and wrf hydro glacier fig 6 note that the runoff from snow and the combined ice glacier melt are supplied to the terrain routing model within wrf hydro glacier thus we compared the difference between the two models to quantify the contribution of ice glacier to streamflow first it is clear that wrf hydro glacier follows the observations more closely than wrf hydro does furthermore fig 6 suggests that the ice glacier contribution to the total streamflow is more than 75 which is in line with previous studies e g farhan et al 2015 ismail et al 2020 3 2 2 snow cover area in astore snow and glacier melt contribute the most to streamflow and the snow cover trend can alter it tahir et al 2015 therefore the robustness of the model parameters and their evaluation were not restricted to streamflow but we also examined the model in relation to the sca modis snow cover data were selected to evaluate the sca prediction ability of the model see section 2 3 2 the mean daily sca fluctuated from 97 in winter to 3 in summer as quantified from modis snow images for astore snow accumulation began in october and reached 80 97 until february march fig 5b the snowmelt begins in early april and the lowest sca occurs in august september at 3 10 the comparison of the modis estimated sca with the simulated sca produced acceptable statistical metrics and showed that the model successfully captured the sca trend in the astore basin fig 5b considering nse and kge when the gpcp based parameters were forced with either tmpa or persiann the average nse and kge scores were higher considering the tpma or persian based parameters when forced with the other three precipitation products the average nse and kge values were above 0 59 and 0 70 respectively see the mean values at the middle and top rows corresponding to nse and kge in fig 5b the model performance was also outstanding in terms of r with an average r greater than 0 86 for all nine combinations the simulated sca from the gpcp or persiann based parameters showed better r scores than those from the tmpa based parameters when forcing was applied to the tmpa precipitation dataset the results were unsatisfactory the reason why gpcp and persiann capture sca better than tmpa is associated with the optimized value of mfsno a parameter that controls sca as shown in table 2 i e 3 0 and 3 2 respectively while that for tmpa was 3 8 thus gpcp and persiann have similarities in simulating sca another reason might be the consistency between gpcp and persiann as persiann upholds the consistency of the monthly total precipitation of gpcp liu et al 2015 nevertheless our results found the model parameters to be robust in simulating sca for all precipitation datasets 3 2 3 evapotranspiration in this section we evaluate wrf hydro glacier and discuss the robustness of the model parameters by considering et aspects fig 5c shows a comparison of the simulated et from the nine runs with the et estimates from gleam this comparison shows that wrf hydro glacier can successfully capture the et trend in astore in most runs the et was overestimated from november to may and underestimated from june to october regarding seasonal variations the et estimated from gleam was higher in summer and lower in winter than that simulated by the wrf hydro glacier this difference can be attributed to the different approaches used by wrf hydro glacier and gleam in wrf hydro glacier the energy balance method is used for accounting for sublimation whereas gleam computes sublimation in snow and ice regions through the priestley taylor equation martens et al 2017 which depends more on the accuracy of satellite based datasets in general the evaluation metrics were within an acceptable range indicating the robustness of the model in simulating et again high evaluation metrics were observed when gpcp based parameters were forced either with tmpa or persiann see bottom row of fig 5c however tmpa based parameters had comparatively lower nse and kge scores than gpcp or persiann based parameters see middle row of fig 5c in summary the optimized set of parameters estimated from all precipitation datasets in the study area was regarded as robust for capturing variations in et 3 3 quantification of uncertainty 3 3 1 streamflow although the model was found to be robust in simulating the hydrology of the study area uncertainties in the output results could not be eliminated the robustness of a model indicates its ability to reproduce satisfactory results in time and space under different conditions whereas uncertainties define the changes in the model under various conditions the robustness of the model alone does not ensure the elimination of uncertainty qi et al 2019 thus we also focused on the uncertainties in the streamflow simulation fig 7 a depicting the optimized parameters as the dominant contributor of uncertainty across all types of flows the overall uncertainty contributions from the optimized parameters and precipitation dataset were 79 2 and 13 respectively and their mutual interaction contributed only 7 8 the contributions of the optimized parameters to the total uncertainty were 68 7 79 7 and 89 3 for low medium and high flow conditions respectively the contribution of the optimized parameters increased from low to high flow which can be explained by the fact that during winter the melting process ceased fig 4b resulting in a reduction in streamflow fig 4a therefore the calibrated parameters would be less sensitive to low flow hence the optimized parameter factors contributed less at low flows than at medium or high flows during medium or high flow seasons the optimized parameters become sensitive to streamflow and dominant contributors to the total uncertainty the contribution of uncertainty from the precipitation datasets continues to decrease from low to high flows among the three flow types the contribution of the uncertainty from the precipitation dataset was the highest under low flow conditions this may be attributed to the fact that low flows occur during winter when the catchment is fully covered by snow this flow reflects the base flow that is the melting of ice and snow and any uncertain rain on snow can generate surface runoff therefore under low flow conditions the uncertainty contribution from the precipitation dataset was relatively high compared to that under medium or low flow conditions note that astore is a region where approximately 75 of streamflow is caused by snow and glacier melt mainly controlled by the optimized parameters thus the precipitation datasets are not a major contributor to the total uncertainty 3 3 2 snow cover area for the sca fig 7b the uncertainty contribution of the precipitation datasets was higher in djf and son whereas the optimized parameters were a dominant contributor in mam and jja for djf and son the uncertainty contributions from the precipitation datasets were 94 2 and 90 8 respectively this is the period when the sca may reach up to 100 therefore precipitation primarily controls the sca this may be because snow increases the sca and rain can accelerate the melting process by transferring energy cohen et al 2015 thereby changing the sca however in mma and jja the optimized parameters were dominant contributors with uncertainty contributions of 90 9 and 90 6 respectively in mam the optimized parameters determined the trends of melting and streamflow which made them the dominant contributors to uncertainty in jja sca fell to its lowest value 3 and most of the precipitation was rainfall hence variations in the sca were unlikely causing precipitation datasets to be minor contributors to the total uncertainty such seasonal variations in the dominant contributor to uncertainty suggest that the precipitation datasets and optimized parameters should be collectively considered when estimating annual water resources in astore 3 3 3 evapotranspiration uncertainties in simulating et were quantified according to season as in the sca fig 7c the precipitation dataset is the main contributor to the total uncertainty in djf and son contributing 65 1 and 93 2 respectively while optimized parameters are major contributors in mam and jja contributing 43 and 53 respectively in djf and son the temperature fell below 0 c and the basin was fully covered with snow and ice during this period very little surface water was available to generate et and et was thus mostly dependent on precipitation events however in mam and jja the snow and glaciers started to melt producing surface flows hence the parameters linked to surface runoff soil or vegetation determined et therefore the optimized parameters significantly contributed to the total uncertainty it is worth noting that in mam when snow and ice start to melt the contribution of the mutual interaction of uncertainty sources is the second highest leading to greater uncertainty in mam 4 conclusions this study sought to address three main objectives to investigate the impact of input precipitation datasets in estimating optimized parameters to evaluate the model robustness under multiple parameter sets from multiple satellite based precipitations and to quantify and segregate the contribution of optimized parameters to model and satellite based precipitation datasets to model uncertainty by focusing on streamflow sca and et globally various hydrological models from lumped to distributed can comprehensively replicate hydrological processes but parametrization and input datasets can lead to ambiguity and unreliability her and chaubey 2015 associated with the lack of robustness of model parameters to examine the robustness of wrf hydro glacier we adopted an approach wherein the fully distributed hydrological model wrf hydro glacier was calibrated with three different precipitation datasets and an optimized set of parameters from each precipitation dataset was estimated the results showed that the three different precipitation datasets produced different optimized parameter sets reflecting the precipitation characteristics from the results obtained it can be concluded that the wrf hydro glacier model was robust in terms of simulating hydrological processes in the study area overall among the three precipitation datasets gpcp showed better results followed by persiann and tmpa however all three datasets can be used to drive a fully distributed model in this snow dominated data scarce region note that kanda et al 2020 suggested that satellite based precipitation datasets are vaguer compared to the temperature over the himalayas thus in this study we considered precipitation datasets to be the only source of uncertainty our uncertainty analysis for streamflow shows that the optimized parameters are major contributors to the total uncertainty in all flow types for sca and et the precipitation datasets are the primary contributors in the winter and fall seasons while the optimized parameter set is the major source of uncertainty in spring and summer therefore if the objective of hydrological modeling is to simulate streamflow the optimized parameter set should be given prime importance whereas for the sca or et both factors that is the optimized parameter set and precipitation dataset should be given equal importance developing the wrf hydro glacier model with fine scale meteorological datasets removes regional uncertainties that may affect coarser resolution or lumped models astore was selected for this study because it is a climate affected and water scarce catchment in the uib mehboob and kim 2021 in our view this advanced hydrological model can help water planners develop policies on water use and aid in the development of hydraulic infrastructure to boost agricultural production in the region declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the basic science research program through the national research foundation of korea funded by the ministry of science ict future planning 2020r1a2c2007670 the technology advancement research program through the korea agency for infrastructure technology advancement kaia funded by the ministry of land infrastructure and transport 22ctap c163540 02 and the korea environmental industry technology institute funded by the ministry of environment 2022003640002 we also acknowledge the water and power development authority wapda of pakistan and pakistan meteorological department pmd for providing the streamflow and meteorological datasets 
2819,as a result of global warming compound extreme events become more frequent across the world compound drought and hot events cdhes are receiving increasing attention because their social and environmental impacts are greater than those of individual extreme events this study uses the standardized precipitation index spi and maximum temperature to define cdhes based on the two dimensional definition on the daily scale in eastern china during the warm season may october from 1961 to 2018 identify hot spot areas of cdhes and explore the multiple characteristics and variations of cdhes the main results are as follows north china nc and southwest china swc were selected as hot spot areas due to their high frequency of compound days and the increasing trend in the number of days and land area ratio affected by compound days cdhes have become more frequent long lasting and extreme in eastern china especially in nc although the change in swc is not as great as that in nc the proportion of extreme events in all the severities and the proportion of long duration events in all the durations are larger in swc than in the other regions the probability distribution of occurrence date was unimodal in the entire eastern china and it was similar in nc whereas the probability distribution was bimodal in swc the occurrence dates of cdhes with different severities and durations postponed in 1991 2018 compared to 1961 1990 and this feature was the most pronounced in swc this study contributes to raising awareness of cdhes and informs the mitigation of their adverse effects keywords compound drought and hot events eastern china severity duration occurrence dates data availability data will be made available on request 1 introduction global climate change accelerates water cycle at global and regional scales ma et al 2018 and alters relevant processes such as precipitation evapotranspiration and runoff hattermann et al 2015 many studies show that extreme weather and climate events are likely to increase in the future dosio et al 2018 naumann et al 2018 jiang and wang 2021 traditionally the research on extreme weather and climate mainly focuses on single process or variable such as heavy precipitation or maximum temperature rahmstorf and coumou 2011 horton et al 2016 however climate variables are interrelated thus focusing on individual extreme variables may not be sufficient to fully describe the impacts of extreme events compound extreme events that is the concurrent or continuous occurrence of multiple extreme events can exacerbate adverse impacts and result in more severe consequences on human society and natural environment than those individual extreme events do leonard et al 2014 drought and hot extreme events are two severe climate hazards in the world which can lead to major natural disasters with serious social and environmental impacts ciais et al 2005 drought affects the growth and development of crops and vegetation and causes reduced river flows and lower lake and reservoir levels leading to problems such as crop shortages and water shortages wilhite 2000 hao and aghakouchak 2013 hot extreme events have adverse effects on human health crop and vegetation growth air quality and so on vautard et al 2005 lesk et al 2016 with the changes of global climate the occurrence of drought and hot events has increased in both global and regional areas dai 2011 drought and hot events often occur simultaneously compound drought and hot events cdhes are typical of compound extremes such as the simultaneous occurrence of drought and heat waves in europe in 2003 russia in 2010 and california usa in 2014 fink et al 2004 trenberth and fasullo 2012 aghakouchak et al 2014 accompanied by high mortality rates and huge economic losses making them one of the most catastrophic compound events in many regions of the world the frequency number of days and severity of cdhes have increased mukherjee and mishra 2020 hao et al 2018 assessed changes in the severity of cdhes on a global scale based on the standardized dry hot index sdhi their research showed that cdhes in the western usa northern south america western europe africa western asia southeastern asia southern india northeastern china and eastern australia increased significantly in severity during the warm season recent studies indicate that cdhes occur more frequently from northeast to southwest china wu et al 2019 kong et al 2019 yu and zhai 2020 in china drought and extreme hot events are extensively studied as individual extreme events in recent decades there has been an increasing trend of drought across china especially over the northern regions which have become more frequent more serious and longer lasting yu et al 2014 chen and sun 2015 shao et al 2018 extreme hot events rose significantly across china over the last few decades but has dropped slightly in the central regions ding et al 2010 wei and chen 2011 there is an important research gap in the study of concurrent droughts and heatwaves which have heavier impacts on physical health social economy and ecological environment at present several studies of cdhes in china have been conducted li et al 2021 kong et al 2019 however most studies of cdhes are based on relatively coarse resolution at the monthly scale and focus on the few characteristics such as the frequency and severity of occurrence being heavily populated economically developed and vulnerable to climate changes eastern china will suffer from high social and economic losses for compound drought and hot extremes more detailed information such as severity duration and starting time is helpful to enhance the understanding of cdhes and cope with the adverse effects which is important for disaster prevention and reduction and regional sustainable development frequency severity duration and occurrence date are the key attributes of cdhes therefore it is essential to identify the cdhes on the daily scale and explore the multifaceted characteristics and changes of cdhes to mitigate their adverse effects this study uses the standardized precipitation index spi and maximum temperature to define cdhes based on the two dimensional definition on the daily scale in eastern china during the warm season may october from 1961 to 2018 first the general characteristics of compound drought and hot days cdhds were analysed in eastern china secondly the study area was partitioned and the two most severe regions were selected as hot spot areas for further study thirdly the spatio temporal variation of the cdhes was investigated for different severities and durations lastly the study focused on the probabilities of occurrence dates of cdhes to explore their possible temporal migration 2 data and methods 2 1 data cn05 1 high resolution gridded 0 25 0 25 daily precipitation and maximum temperature tmax datasets wu and gao 2013 from 1961 to 2018 was used in this study cn05 1 is a gridded dataset based on the interpolation of daily observation of more than 2400 stations in china obtained from the national meteorological information center the warm season may to october is considered to cover most of the high temperature and low precipitation extreme events in this study 2 2 study area the selected area for this study is eastern china mainland china within 14 75 n 55 25 n 97 e 140 25 e including north china nc northeast china nec east china ec south china sc central china cc part of southwest china swc and part of northwest china nwc of china s geographic divisions the entire region has complex topographic and climatic conditions moreover most of the population in china concentrates in eastern china there is growing concern about the impacts of extreme weather events on human society and the natural environment especially in densely populated areas 2 3 definitions of cdhds and cdhes drought is defined according to meteorological drought which refers to the phenomenon of insufficient precipitation in which the precipitation is lower than the climate average over a period of time the standardized precipitation index spi mckee et al 1993 was used to quantify drought in this study spi is a normalization of precipitation which is usually obtained by summing the precipitation over several months cumulative period then fitting the cumulative precipitation to a probability distribution function parametric statistical distribution and converting the probabilities to a standard normal distribution in this study spi was calculated at a daily temporal resolution and the cumulative period was 30 days the drought threshold of this study is set to be relatively high thus focusing on more severe drought events drought day is defined as spi smaller than 1 3 and is further classified into three categories moderate drought 1 6 spi 1 3 severe drought 2 spi 1 6 and extreme drought spi 2 hot day is defined as the daily tmax higher than the 85th percentile of the daily tmax during the warm season it is divided into three categories moderate tmax85 tmax tmax90 severe tmax90 tmax tmax95 and extreme tmax tmax95 compound drought and hot events cdhes refer to drought and hot extreme events occurring simultaneously the day is named compound drought and hot day cdhd when tmax tmax85 and spi 1 3 in this study cdhes normally consist of consecutive or individual cdhds the severity of cdhd is divided into three categories according to the two dimensional definition shown in fig 1 extreme tmax tmax95 and spi 2 severe tmax tmax90 and spi 1 6 tmax tmax90 and spi 1 6 tmax tmax95 and spi 2 moderate tmax tmax85 and spi 1 3 tmax tmax85 and spi 1 3 tmax tmax90 and spi 1 6 in this study cdhes are characterized in several aspects including 1 cdhes frequency is defined as the number of annual compound events 2 cdhes duration is defined as the number of days that a cdhe lasts cdhes are classified into four kinds according to different durations with 1 2 days being isolated events 3 5 days being short duration events 6 10 days being medium duration events and over 10 days being long duration events 3 cdhes severity refers to the severity of the day with the highest severity of a compound event lasting for n days 4 land area ratio is defined as the ratio of the number of grids affected by cdhes to the total number of the study area 5 cdhes occurrence date is the date on which a cdhe begins different from previous studies hao et al 2018 hao et al 2020 this study provides a new two dimensional definition of daily scale cdhes meanwhile the severity of cdhes is determined by the severity of drought events and high temperature events at the same time avoiding the case when a combination of a less severe high temperature drought and a more severe drought high temperature is identified as a more severe compound event 2 4 trend analysis long term trends are estimated by the slope of the linear regression model mann kendall mk trend test kendall 1948 mann 1945 is applied to test the significance of the trend which is a non parametric approach and one of the most frequently applied methods for detecting changes in hydrology and climatology 2 5 normalized anomaly normalized anomalies x i of the frequency of cdhes for different durations are calculated and derived as 1 x i x i x σ i where x i x and σ i are the frequency frequency mean and frequency standard deviation of cdhes for a particular duration respectively if the normalized anomaly is positive negative it indicates that the frequency is greater smaller than normal 2 6 two sample kolmogorov smirnov ks test the two sample kolmogorov smirnov ks test is used to assess whether two samples come from the same or different distribution families ks is a nonparametric test that can evaluate two distribution functions samples based on the distance between their empirical distribution functions the null hypothesis is that the two distribution functions are drawn from the same distribution at a certain significance level here at 95 confidence level here the two sample ks test is employed to assess differences in the probability distributions of occurrence date in the two time periods the test indicates whether the data from the two periods come from the same distribution at 95 confidence level 3 results 3 1 variation in drought days hot days and cdhds based on daily spi and tmax the changes in the number of drought days hot days and cdhds are first analyzed as shown in fig 2 the trends displayed spatial inhomogeneity from 1961 to 2018 the spatial distribution of the trends in the number of drought days of different severities was generally consistent across the study area fig 2a d there was an upward trend in most areas from the north to the southwest of the study area and a downward trend in most of the other regions with some areas gray shaded area showing a statistically significant trend 95 confidence this change pattern of drought days is consistent with that of precipitation reduction indicated by su et al 2020 moreover the upward trend of drought days in the southwestern region increased with the severity of drought indicating an increasing risk of drought xu 2020 for the number of hot days most areas showed a significant upward trend at 95 confidence level and only some parts of central and eastern region showed a downward trend fig 2e h as the severity of high temperature increased the increasing trends in most regions were greater and the area with a decreasing trend was smaller the change pattern of hot days is generally consistent with the daytime heat wave change results of chen and li 2017 for the number of cdhds the spatial distribution of the trends was similar to that of the number of drought days fig 2i l which is consistent with the results of previous studies yu and zhai 2020 the upward trend in the number of cdhds in northern region decreased as the severity of cdhd increased the trend of cdhds in southern region changed from upward trend to downward trend with the increase of severity in addition fig 3 exhibited the multi year average of the number of cdhds overall the number of cdhds was the highest in southwestern region followed by northern region showing that cdhds occur frequently in these two regions for moderate cdhds those in southwestern and northern region were about the same however as the severity increased the number of days became much greater in southwestern region than that in northern region as a result of the vast area of the entire eastern china and the complex and varied topography and climate the study area was divided into seven sub regions based on geography and climate nc nec ec sc cc swc and nwc fig 4 we further evaluate the number of cdhds and the land area ratio affected by cdhds the sum of the unclassified three categories in sub regions fig 4a shows the time series of regional average cdhds the number of days had shown an upward trend in the entire eastern china significant increase in the cdhds of nc and swc were observed with a significant trend of 0 0559 days year at 99 confidence level and a significant trend of 0 0484 days year at 90 confidence level respectively there was no significant change in most of the other regions fig 4b shows the land area ratio affected by cdhds in each region the area ratio didn t show a significant trend in the entire eastern china nc and swc showed an increasing trend and the trend in nc 0 0023 year was significant at at 90 confidence level whereas cc and ec showed a significant decreasing trend reaching 95 and 99 confidence levels respectively nc and swc were high frequency regions of cdhds and the number of cdhds and land area ratio affected by cdhds were increasing which may have greater adverse effects therefore we chose these two regions as hot spot areas for more detailed analysis next we explore the relationship between compound days and drought and high temperature through the time evolution of drought days hot days and cdhds fig 5 shows the distribution of occurrence of cdhds associated with the occurrence of drought and hot days from 1961 to 2018 the change in the number of cdhds was consistent with the change in the number of drought and hot days and all of them increased with time fig 5a c in order to quantitatively investigate the relationship between compound events and individual events the correlation coefficient between cdhds and drought days hot days is given in table 1 cdhds were strongly correlated with drought days hot days α 0 01 thus the change of compound events was generally closely related to individual changes in the droughts and heat waves regional variations in precipitation and temperature can have an impact on cdhes the correlation between cdhds and drought days was generally the same as the correlation between cdhds and hot days in nc while the correlation between cdhds and drought days was greater than the correlation between cdhds and hot days in swc the decrease in precipitation may be the main reason for the increase in compound drought and hot events in swc the evolution of the number of cdhds with different severities were further revealed fig 5d f in eastern china the proportion of compound days with extreme severity tended to increase in line with the evolution of the total number of unclassified three types of cdhds the proportion of severe cdhds also tended to increase while the proportion of moderate cdhds showed a decreasing trend besides the trends of extreme cdhds and moderate cdhds were significant at 95 confidence level table 2 the proportion of cdhds in different severities in nc and swc was consistent with that in eastern china it indicated that cdhds would become more extreme potentially causing more significant risks 3 2 variation in the frequency duration and severity of cdhes the above section focuses on the analysis of the number of cdhds on a daily scale and next we explore the variation characteristics of non consecutive and consecutive cdhes fig 6 displays the spatial distribution of trend and the multi year average of frequency of cdhes in warm season from 1961 to 2018 the trend distribution of the frequency of cdhes was very similar to that of the number of cdhds showing an increasing trend in most areas of northern and southwestern regions and a decreasing trend in the northwestern and southeastern regions for the multi year average of the frequency of cdhes the frequency in most areas of the southwest and north was higher than that in the other areas the characteristics of higher frequency in southwestern regions than in other regions become more evident with the severity subsequently fig 7 exhibits the spatial distribution of trend and the multi year average of duration of cdhes in warm season from 1961 to 2018 the trend distribution of duration was similar to that of frequency multi year averages of duration were generally longer in the south than in the north the distribution of multi year averages of duration was similar to that of frequency for different severities as well there was a trend towards increasing frequency and duration in most of the northern and southwestern regions and their multi year averages were also higher relative to the other regions the increasing frequency persistence and severity of compound events in both regions nc and swc confirms the necessity to select them for more in depth analysis to further explore the change of cdhes with different durations the time evolution and trend of normalized anomalies of cdhes frequency with different durations are provided in fig 8 overall the frequency was mostly below normal until the 1990 s and then increased to above normal after the early 1990 s the frequency of cdhes with all durations had changed since the early 1990 s in addition we obtained similar results by using mann kendall test to detect the break point figure omitted most of the significant break points at 95 confidence level were around the early 1990 s from 1961 to 2018 the normalized anomalies of each duration showed an increasing trend in eastern china and nc the trend of normalized anomalies generally increased monotonously with the increment of duration whereas the trend was more evident for shorter and longer durations and less evident for medium durations in swc for moderate severe and extreme cdhes fig a 1 fig a 3 their temporal changes were similar to those of the unclassified total cdhes however as the severity increases the characteristics of the events became more pronounced whose frequency tended to be below normal until the early 1990 s and above normal thereafter in eastern china trends in the frequency of compound events of all durations were insignificant for both moderate and severe severity with decreasing trends for shorter events and increasing trends for longer ones the trend for the frequency of extreme cdhes of all durations was increasing with some cases being significant in nc the frequency of compound events of all durations of different severities tended to increase as severity increases the frequency of cdhes of different durations increased significantly in swc trend in the frequency of moderate cdhes of all durations was either increasing or decreasing but none of them was significant the trend was increasing for both severe and extreme events with the frequency of more events showing a significant increase with increasing severity in a word the frequency of cdhes of all durations and severities generally tended to increase with the characteristics of being mostly below normal until the early 1990 s and mostly above normal thereafter due to the fact that the frequency of cdhes is negative before the early 1990 s and positive after the early 1990 s we further analyze the change of various characteristics in the two periods of 1961 1990 and 1991 2018 to evaluate the changes in compound events since cdhes of different severities and durations all show an increasing trend we cannot directly compare the changes of events with different severities durations hence we next analyze the contribution of cdhes with different severities durations to the total cdhes fig 9 exhibits the percentage of cdhes with different severities to the total cdhes in eastern china nc and swc the moderate events accounted for the largest proportion of all cdhes in the two periods followed by the severe events and the extreme events accounted for the minimum proportion with the increase of duration the proportion of moderate events decreased the proportion of severe events first increased and then decreased and the proportion of extreme events increased for the extreme cdhes with the highest severity the proportion of extreme cdhes of each duration was greater in swc than in eastern china and nc suggesting that swc is more prone to extreme cdhes and its disaster risk was greater the proportion of extreme cdhes in 1991 2018 was always larger than that in 1961 1990 a larger proportion of cdhes with extreme severity indicated that cdhes become more extreme furthermore fig 10 demonstrates the proportion of cdhes with different durations to the total cdhes in eastern china the stacked histogram is the ratio of the multi year average occurrence frequency of the two periods to the sum of the two periods in the two periods all cdhes ranked in descending order of duration length as isolated events short duration events medium duration events and long duration events as the severity increases the proportion of consecutive cdhes increased for all three categories in both time periods and the increase was greater for cdhes of longer duration the proportion of isolated cdhes decreases as the severity increases the same characteristics were observed in nc fig a 4 and swc fig a 5 in eastern china and nc the proportion of long duration events and medium duration events with different severities in 1991 to 2018 was always greater than that in 1961 to 1990 the proportion of isolated events in 1991 2018 was always less than that in 1961 1990 the increase in the proportion of cdhes with longer duration indicates that cdhes become more lasting there is no obvious difference in the proportion of events with different durations between the two time periods in swc the multi year average frequency was greater in 1991 2018 than that in 1961 1990 in eastern china and nc with the increase of duration the multi year average frequency from 1991 to 2018 accounted for a greater percentage of the sum of the two periods in most cases confirming the development of cdhes toward longer duration differences between different durations of the same severity were more pronounced in nc while there were no apparent differences in the multi year average frequency of different durations in swc the above suggests that the more persistent characteristics of the cdhes are obvious in nc while there is no such obvious characteristics in swc 3 3 variation in the occurrence date of cdhes events in this section we investigate the changes in the occurrence date of cdhes fig 11 showed the trend in the five year running mean of occurrence probabilities in each date the trends with different severities were generally consistent in the entire eastern china nc and swc most of the dates indicated a decreasing trend in the occurrence probability during the months from may to july and an increasing trend in august and thereafter this change means that cdhes were postponed in the warm season the probability distribution results of cdhes occurrence date in 1961 1990 and 1991 2018 were used to further analyze the characteristics and changes of occurrence date more visually fig 12 illustrates the probability distribution of cdhes occurrence date with the orange and blue bars showing the probabilities in 1961 1990 and 1991 2018 respectively we used two sample ks test to assess the differences in the probability distributions of occurrence date of the two time periods the test results showed that the data for the two periods came from the same distribution at 95 confidence level however we can still observe some changes to get some useful information although they are not significant the shape of the probability distribution was similar for different severities the possible occurrence dates of cdhes in the three regions during the warm season were from early may to early october eastern china early may to late september nc and early may to mid october swc respectively the possible time range of cdhes in swc was the longest the probability distributions of occurrence date in eastern china and nc were similar in shape with a single peak from 1961 to 1990 the probability of occurrence in july middle june to early july was the highest in eastern china in nc while the date with the highest probability of occurrence from 1991 to 2018 postponed to late july to middle august nc was the same but the peaks of extreme severity in both regions had not moved the probability distribution in swc is bimodal from 1961 to 1990 there were two obvious peaks in the probability distribution with a greater probability of occurrence from middle may to early june and from middle july to early august whereas from 1991 to 2018 the double peaks in the probability distribution became less obvious with the former peak weakening and the latter peak strengthening moreover most of the probabilities before late july middle july were greater in eastern china and nc in swc during 1961 1990 than during 1991 2018 while the differences between the two time periods was reversed for most of the probabilities after late july middle july in eastern china the differences in cumulative probability before and after late july were about 0 06 between 1961 and 1990 and 1991 2018 with few differences in different severities the differences were within 0 11 0 21 in nc and the differences decreased with the increase of severity however the differences in swc increased with severity and the differences in cumulative probability before and after middle july were 0 06 0 24 between 1961 and 1990 and 1991 2018 the above results demonstrated that most of the occurrence dates postponed in 1991 2018 relative to 1961 1990 and this change was most pronounced in swc we further investigate the probability of cdhes with different durations as the duration increases the kurtosis of most probability distributions increases table 3 and the shape of distribution changes from short and fat to tall and thin becoming more concentrated for instance in eastern china fig a 6 the dates on which cdhes were concentrated occurring with approximately 70 of the total probability are early june late august 1 2 days mid june mid august 3 5 days late june mid august 6 10 days and late june early august 10 days the probability distributions with different durations in nc were very similar to those in the entire eastern china fig a 7 in swc fig a 8 the bimodal character of the probability distribution became increasingly evident as the duration increases 4 discussion the changing characteristics of cdhes are closely related to the changes in individual events of drought and high temperature cdhds occurred frequently throughout eastern china from the 1960 s to the early 1970 s and after the 1990 s fig 4a fig 5a c it seems to be related to the fact that several regions of china experienced severe summer droughts in the 1960 s and 1970 s yao et al 2017 the increase in drought and hot events after the 1990 s may have led to an increase in compound events yu and zhai 2020 ding and qian 2011 meanwhile droughts and heat waves have become more frequent in southwestern and northern china gong et al 2017 jia and hu 2017 which is generally consistent with the increase in compound events in these regions moreover more hot events are predicted to occur in the context of a warming climate in the future resulting in relatively higher mortality wang et al 2017 wang et al 2019 with the further intensification of temperature rise in the future the frequency and risk of compound extreme events will increase ipcc 2021 the increase in multiple features of cdhes strengthens local disaster risk and may pose a greater threat than a single extreme event sedlmeier et al 2018 schumacher et al 2019 cdhes have caused serious social and economic losses in china zhang et al 2022 feng et al 2020 therefore improving the understanding of cdhes and providing early warnings is an urgent issue to be addressed furthermore we found that the probability distribution of the occurrence dates of cdhes in different regions is quite different in this study the distribution is unimodal in nc while it is bimodal in swc this may be due to the fact that the rainy season in swc has not yet started in may yan et al 2013 when the probability of drought is higher and the probability of high temperatures is also higher resulting in a peak in may the unimodal distribution in nc is because the probability of high temperature is quite small in may although the probability of drought is high at this time meanwhile most of the occurrence dates postponed in 1991 2018 relative to 1961 1990 this may be related to the delay of rainy season which is affected by monsoon activity south asia high water vapor transport and the like hao et al 2021 yan et al 2013 however there are still some points need to be improved in this study 1 the definition of compound events in this study only considers the daily precipitation and daily maximum temperature however meteorological variables such as evapotranspiration wind speed relative humidity and radiation also have impacts on compound events russo et al 2019 and some studies have added evapotranspiration to define cdhes jena et al 2020 kong et al 2019 moreover extreme high temperatures include not only daytime temperatures but also nighttime temperatures wang et al 2020 2 in addition to the characteristics and changes of cdhes future research should concentrate on the physical mechanism of cdhes the changes of compound events are affected by the changes of precipitation and temperature and their dependence kong et al 2019 for instance sea surface temperature sst plays an important role in the variations and trends of extreme temperature and precipitation dittus et al 2018 what is more sustained large scale circulation anomalies can trigger the occurrence of cdhes and land atmosphere feedbacks can exacerbate and propagate these events miralles et al 2019 besides global warming can lead to the enhancement of regional soil moisture temperature coupling in some water limited areas thus amplifying the intensity of heat waves during droughts cheng et al 2019 therefore cdhes are fundamentally influenced by sst anomalies atmospheric circulations land atmosphere interactions and climate warming and the physical mechanism of cdhes will be explored in the future 5 conclusions the social and environmental impacts of cdhes in which drought and hot events occur simultaneously are greater than the impacts of individual events in this study the spi and maximum temperature were used to define cdhes on the daily scale hot spot areas were identified and the characteristics and changes in the multiple aspects of cdhes were explored in order to mitigate their adverse effects the main conclusions are as follows there was an upward trend in most areas from the north to the southwest and a downward trend in most of the northwestern and southeastern regions for the number of cdhds nc and swc are high frequency regions of cdhds and the number of cdhds and land area ratio affected by cdhds were increasing which may have greater adverse effects thus these two regions were selected as hot spot areas for more detailed analysis cdhes have become more frequent long lasting and extreme in eastern china the frequency of cdhes tended to rise in most regions from the north to the southwest and was higher than that in the other regions the duration of cdhes was longer in the southern region than in the northern region with longer duration from the northern to southwestern regions cdhes of all durations and severities generally showed an increasing trend with the characteristics of being mostly below normal until the early 1990 s and mostly above normal thereafter comparing the contribution of cdhes with different severities durations to the total cdhes in the two periods it was found that the cdhes become more extreme and persistent especially in nc although the change in swc is not as great as that in nc both extreme and long duration events contribute more to the total cdhes in swc than in the other regions the possible occurrence dates of cdhes during the warm season were from early may to early october the entire eastern china early may to late september nc and early may to mid october swc respectively the shapes of the probability distribution were similar for the different severities the probability distribution of occurrence date was unimodal in the entire eastern china and it was similar in nc whereas the probability distribution was bimodal in swc compared with 1961 1990 most of the occurrence dates of cdhes with different severities in 1991 2018 postponed and this feature in swc was the most pronounced credit authorship contribution statement mengyang liu conceptualization data curation formal analysis methodology software validation writing original draft yixing yin software investigation project administration resources supervision writing review editing xiaojun wang funding acquisition investigation project administration resources supervision xieyao ma funding acquisition investigation project administration resources supervision ying chen supervision writing review editing weilin chen writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by national natural science foundation of china no 52121006 no 41877158 no 41875098 young top notch talent support program of national high level talents special support plan research project of ministry of natural resources no 20210103 six talents peak project of jiangsu province no jnhb 068 333 high level talents cultivation project of jiangsu province research project of jiangsu water conservancy research institute no 2022019 water science and technology project of department of water resources of zhejiang province no ra1704 and china scholarship council we are also thankful to anonymous reviewers and editors for their helpful comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128499 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2819,as a result of global warming compound extreme events become more frequent across the world compound drought and hot events cdhes are receiving increasing attention because their social and environmental impacts are greater than those of individual extreme events this study uses the standardized precipitation index spi and maximum temperature to define cdhes based on the two dimensional definition on the daily scale in eastern china during the warm season may october from 1961 to 2018 identify hot spot areas of cdhes and explore the multiple characteristics and variations of cdhes the main results are as follows north china nc and southwest china swc were selected as hot spot areas due to their high frequency of compound days and the increasing trend in the number of days and land area ratio affected by compound days cdhes have become more frequent long lasting and extreme in eastern china especially in nc although the change in swc is not as great as that in nc the proportion of extreme events in all the severities and the proportion of long duration events in all the durations are larger in swc than in the other regions the probability distribution of occurrence date was unimodal in the entire eastern china and it was similar in nc whereas the probability distribution was bimodal in swc the occurrence dates of cdhes with different severities and durations postponed in 1991 2018 compared to 1961 1990 and this feature was the most pronounced in swc this study contributes to raising awareness of cdhes and informs the mitigation of their adverse effects keywords compound drought and hot events eastern china severity duration occurrence dates data availability data will be made available on request 1 introduction global climate change accelerates water cycle at global and regional scales ma et al 2018 and alters relevant processes such as precipitation evapotranspiration and runoff hattermann et al 2015 many studies show that extreme weather and climate events are likely to increase in the future dosio et al 2018 naumann et al 2018 jiang and wang 2021 traditionally the research on extreme weather and climate mainly focuses on single process or variable such as heavy precipitation or maximum temperature rahmstorf and coumou 2011 horton et al 2016 however climate variables are interrelated thus focusing on individual extreme variables may not be sufficient to fully describe the impacts of extreme events compound extreme events that is the concurrent or continuous occurrence of multiple extreme events can exacerbate adverse impacts and result in more severe consequences on human society and natural environment than those individual extreme events do leonard et al 2014 drought and hot extreme events are two severe climate hazards in the world which can lead to major natural disasters with serious social and environmental impacts ciais et al 2005 drought affects the growth and development of crops and vegetation and causes reduced river flows and lower lake and reservoir levels leading to problems such as crop shortages and water shortages wilhite 2000 hao and aghakouchak 2013 hot extreme events have adverse effects on human health crop and vegetation growth air quality and so on vautard et al 2005 lesk et al 2016 with the changes of global climate the occurrence of drought and hot events has increased in both global and regional areas dai 2011 drought and hot events often occur simultaneously compound drought and hot events cdhes are typical of compound extremes such as the simultaneous occurrence of drought and heat waves in europe in 2003 russia in 2010 and california usa in 2014 fink et al 2004 trenberth and fasullo 2012 aghakouchak et al 2014 accompanied by high mortality rates and huge economic losses making them one of the most catastrophic compound events in many regions of the world the frequency number of days and severity of cdhes have increased mukherjee and mishra 2020 hao et al 2018 assessed changes in the severity of cdhes on a global scale based on the standardized dry hot index sdhi their research showed that cdhes in the western usa northern south america western europe africa western asia southeastern asia southern india northeastern china and eastern australia increased significantly in severity during the warm season recent studies indicate that cdhes occur more frequently from northeast to southwest china wu et al 2019 kong et al 2019 yu and zhai 2020 in china drought and extreme hot events are extensively studied as individual extreme events in recent decades there has been an increasing trend of drought across china especially over the northern regions which have become more frequent more serious and longer lasting yu et al 2014 chen and sun 2015 shao et al 2018 extreme hot events rose significantly across china over the last few decades but has dropped slightly in the central regions ding et al 2010 wei and chen 2011 there is an important research gap in the study of concurrent droughts and heatwaves which have heavier impacts on physical health social economy and ecological environment at present several studies of cdhes in china have been conducted li et al 2021 kong et al 2019 however most studies of cdhes are based on relatively coarse resolution at the monthly scale and focus on the few characteristics such as the frequency and severity of occurrence being heavily populated economically developed and vulnerable to climate changes eastern china will suffer from high social and economic losses for compound drought and hot extremes more detailed information such as severity duration and starting time is helpful to enhance the understanding of cdhes and cope with the adverse effects which is important for disaster prevention and reduction and regional sustainable development frequency severity duration and occurrence date are the key attributes of cdhes therefore it is essential to identify the cdhes on the daily scale and explore the multifaceted characteristics and changes of cdhes to mitigate their adverse effects this study uses the standardized precipitation index spi and maximum temperature to define cdhes based on the two dimensional definition on the daily scale in eastern china during the warm season may october from 1961 to 2018 first the general characteristics of compound drought and hot days cdhds were analysed in eastern china secondly the study area was partitioned and the two most severe regions were selected as hot spot areas for further study thirdly the spatio temporal variation of the cdhes was investigated for different severities and durations lastly the study focused on the probabilities of occurrence dates of cdhes to explore their possible temporal migration 2 data and methods 2 1 data cn05 1 high resolution gridded 0 25 0 25 daily precipitation and maximum temperature tmax datasets wu and gao 2013 from 1961 to 2018 was used in this study cn05 1 is a gridded dataset based on the interpolation of daily observation of more than 2400 stations in china obtained from the national meteorological information center the warm season may to october is considered to cover most of the high temperature and low precipitation extreme events in this study 2 2 study area the selected area for this study is eastern china mainland china within 14 75 n 55 25 n 97 e 140 25 e including north china nc northeast china nec east china ec south china sc central china cc part of southwest china swc and part of northwest china nwc of china s geographic divisions the entire region has complex topographic and climatic conditions moreover most of the population in china concentrates in eastern china there is growing concern about the impacts of extreme weather events on human society and the natural environment especially in densely populated areas 2 3 definitions of cdhds and cdhes drought is defined according to meteorological drought which refers to the phenomenon of insufficient precipitation in which the precipitation is lower than the climate average over a period of time the standardized precipitation index spi mckee et al 1993 was used to quantify drought in this study spi is a normalization of precipitation which is usually obtained by summing the precipitation over several months cumulative period then fitting the cumulative precipitation to a probability distribution function parametric statistical distribution and converting the probabilities to a standard normal distribution in this study spi was calculated at a daily temporal resolution and the cumulative period was 30 days the drought threshold of this study is set to be relatively high thus focusing on more severe drought events drought day is defined as spi smaller than 1 3 and is further classified into three categories moderate drought 1 6 spi 1 3 severe drought 2 spi 1 6 and extreme drought spi 2 hot day is defined as the daily tmax higher than the 85th percentile of the daily tmax during the warm season it is divided into three categories moderate tmax85 tmax tmax90 severe tmax90 tmax tmax95 and extreme tmax tmax95 compound drought and hot events cdhes refer to drought and hot extreme events occurring simultaneously the day is named compound drought and hot day cdhd when tmax tmax85 and spi 1 3 in this study cdhes normally consist of consecutive or individual cdhds the severity of cdhd is divided into three categories according to the two dimensional definition shown in fig 1 extreme tmax tmax95 and spi 2 severe tmax tmax90 and spi 1 6 tmax tmax90 and spi 1 6 tmax tmax95 and spi 2 moderate tmax tmax85 and spi 1 3 tmax tmax85 and spi 1 3 tmax tmax90 and spi 1 6 in this study cdhes are characterized in several aspects including 1 cdhes frequency is defined as the number of annual compound events 2 cdhes duration is defined as the number of days that a cdhe lasts cdhes are classified into four kinds according to different durations with 1 2 days being isolated events 3 5 days being short duration events 6 10 days being medium duration events and over 10 days being long duration events 3 cdhes severity refers to the severity of the day with the highest severity of a compound event lasting for n days 4 land area ratio is defined as the ratio of the number of grids affected by cdhes to the total number of the study area 5 cdhes occurrence date is the date on which a cdhe begins different from previous studies hao et al 2018 hao et al 2020 this study provides a new two dimensional definition of daily scale cdhes meanwhile the severity of cdhes is determined by the severity of drought events and high temperature events at the same time avoiding the case when a combination of a less severe high temperature drought and a more severe drought high temperature is identified as a more severe compound event 2 4 trend analysis long term trends are estimated by the slope of the linear regression model mann kendall mk trend test kendall 1948 mann 1945 is applied to test the significance of the trend which is a non parametric approach and one of the most frequently applied methods for detecting changes in hydrology and climatology 2 5 normalized anomaly normalized anomalies x i of the frequency of cdhes for different durations are calculated and derived as 1 x i x i x σ i where x i x and σ i are the frequency frequency mean and frequency standard deviation of cdhes for a particular duration respectively if the normalized anomaly is positive negative it indicates that the frequency is greater smaller than normal 2 6 two sample kolmogorov smirnov ks test the two sample kolmogorov smirnov ks test is used to assess whether two samples come from the same or different distribution families ks is a nonparametric test that can evaluate two distribution functions samples based on the distance between their empirical distribution functions the null hypothesis is that the two distribution functions are drawn from the same distribution at a certain significance level here at 95 confidence level here the two sample ks test is employed to assess differences in the probability distributions of occurrence date in the two time periods the test indicates whether the data from the two periods come from the same distribution at 95 confidence level 3 results 3 1 variation in drought days hot days and cdhds based on daily spi and tmax the changes in the number of drought days hot days and cdhds are first analyzed as shown in fig 2 the trends displayed spatial inhomogeneity from 1961 to 2018 the spatial distribution of the trends in the number of drought days of different severities was generally consistent across the study area fig 2a d there was an upward trend in most areas from the north to the southwest of the study area and a downward trend in most of the other regions with some areas gray shaded area showing a statistically significant trend 95 confidence this change pattern of drought days is consistent with that of precipitation reduction indicated by su et al 2020 moreover the upward trend of drought days in the southwestern region increased with the severity of drought indicating an increasing risk of drought xu 2020 for the number of hot days most areas showed a significant upward trend at 95 confidence level and only some parts of central and eastern region showed a downward trend fig 2e h as the severity of high temperature increased the increasing trends in most regions were greater and the area with a decreasing trend was smaller the change pattern of hot days is generally consistent with the daytime heat wave change results of chen and li 2017 for the number of cdhds the spatial distribution of the trends was similar to that of the number of drought days fig 2i l which is consistent with the results of previous studies yu and zhai 2020 the upward trend in the number of cdhds in northern region decreased as the severity of cdhd increased the trend of cdhds in southern region changed from upward trend to downward trend with the increase of severity in addition fig 3 exhibited the multi year average of the number of cdhds overall the number of cdhds was the highest in southwestern region followed by northern region showing that cdhds occur frequently in these two regions for moderate cdhds those in southwestern and northern region were about the same however as the severity increased the number of days became much greater in southwestern region than that in northern region as a result of the vast area of the entire eastern china and the complex and varied topography and climate the study area was divided into seven sub regions based on geography and climate nc nec ec sc cc swc and nwc fig 4 we further evaluate the number of cdhds and the land area ratio affected by cdhds the sum of the unclassified three categories in sub regions fig 4a shows the time series of regional average cdhds the number of days had shown an upward trend in the entire eastern china significant increase in the cdhds of nc and swc were observed with a significant trend of 0 0559 days year at 99 confidence level and a significant trend of 0 0484 days year at 90 confidence level respectively there was no significant change in most of the other regions fig 4b shows the land area ratio affected by cdhds in each region the area ratio didn t show a significant trend in the entire eastern china nc and swc showed an increasing trend and the trend in nc 0 0023 year was significant at at 90 confidence level whereas cc and ec showed a significant decreasing trend reaching 95 and 99 confidence levels respectively nc and swc were high frequency regions of cdhds and the number of cdhds and land area ratio affected by cdhds were increasing which may have greater adverse effects therefore we chose these two regions as hot spot areas for more detailed analysis next we explore the relationship between compound days and drought and high temperature through the time evolution of drought days hot days and cdhds fig 5 shows the distribution of occurrence of cdhds associated with the occurrence of drought and hot days from 1961 to 2018 the change in the number of cdhds was consistent with the change in the number of drought and hot days and all of them increased with time fig 5a c in order to quantitatively investigate the relationship between compound events and individual events the correlation coefficient between cdhds and drought days hot days is given in table 1 cdhds were strongly correlated with drought days hot days α 0 01 thus the change of compound events was generally closely related to individual changes in the droughts and heat waves regional variations in precipitation and temperature can have an impact on cdhes the correlation between cdhds and drought days was generally the same as the correlation between cdhds and hot days in nc while the correlation between cdhds and drought days was greater than the correlation between cdhds and hot days in swc the decrease in precipitation may be the main reason for the increase in compound drought and hot events in swc the evolution of the number of cdhds with different severities were further revealed fig 5d f in eastern china the proportion of compound days with extreme severity tended to increase in line with the evolution of the total number of unclassified three types of cdhds the proportion of severe cdhds also tended to increase while the proportion of moderate cdhds showed a decreasing trend besides the trends of extreme cdhds and moderate cdhds were significant at 95 confidence level table 2 the proportion of cdhds in different severities in nc and swc was consistent with that in eastern china it indicated that cdhds would become more extreme potentially causing more significant risks 3 2 variation in the frequency duration and severity of cdhes the above section focuses on the analysis of the number of cdhds on a daily scale and next we explore the variation characteristics of non consecutive and consecutive cdhes fig 6 displays the spatial distribution of trend and the multi year average of frequency of cdhes in warm season from 1961 to 2018 the trend distribution of the frequency of cdhes was very similar to that of the number of cdhds showing an increasing trend in most areas of northern and southwestern regions and a decreasing trend in the northwestern and southeastern regions for the multi year average of the frequency of cdhes the frequency in most areas of the southwest and north was higher than that in the other areas the characteristics of higher frequency in southwestern regions than in other regions become more evident with the severity subsequently fig 7 exhibits the spatial distribution of trend and the multi year average of duration of cdhes in warm season from 1961 to 2018 the trend distribution of duration was similar to that of frequency multi year averages of duration were generally longer in the south than in the north the distribution of multi year averages of duration was similar to that of frequency for different severities as well there was a trend towards increasing frequency and duration in most of the northern and southwestern regions and their multi year averages were also higher relative to the other regions the increasing frequency persistence and severity of compound events in both regions nc and swc confirms the necessity to select them for more in depth analysis to further explore the change of cdhes with different durations the time evolution and trend of normalized anomalies of cdhes frequency with different durations are provided in fig 8 overall the frequency was mostly below normal until the 1990 s and then increased to above normal after the early 1990 s the frequency of cdhes with all durations had changed since the early 1990 s in addition we obtained similar results by using mann kendall test to detect the break point figure omitted most of the significant break points at 95 confidence level were around the early 1990 s from 1961 to 2018 the normalized anomalies of each duration showed an increasing trend in eastern china and nc the trend of normalized anomalies generally increased monotonously with the increment of duration whereas the trend was more evident for shorter and longer durations and less evident for medium durations in swc for moderate severe and extreme cdhes fig a 1 fig a 3 their temporal changes were similar to those of the unclassified total cdhes however as the severity increases the characteristics of the events became more pronounced whose frequency tended to be below normal until the early 1990 s and above normal thereafter in eastern china trends in the frequency of compound events of all durations were insignificant for both moderate and severe severity with decreasing trends for shorter events and increasing trends for longer ones the trend for the frequency of extreme cdhes of all durations was increasing with some cases being significant in nc the frequency of compound events of all durations of different severities tended to increase as severity increases the frequency of cdhes of different durations increased significantly in swc trend in the frequency of moderate cdhes of all durations was either increasing or decreasing but none of them was significant the trend was increasing for both severe and extreme events with the frequency of more events showing a significant increase with increasing severity in a word the frequency of cdhes of all durations and severities generally tended to increase with the characteristics of being mostly below normal until the early 1990 s and mostly above normal thereafter due to the fact that the frequency of cdhes is negative before the early 1990 s and positive after the early 1990 s we further analyze the change of various characteristics in the two periods of 1961 1990 and 1991 2018 to evaluate the changes in compound events since cdhes of different severities and durations all show an increasing trend we cannot directly compare the changes of events with different severities durations hence we next analyze the contribution of cdhes with different severities durations to the total cdhes fig 9 exhibits the percentage of cdhes with different severities to the total cdhes in eastern china nc and swc the moderate events accounted for the largest proportion of all cdhes in the two periods followed by the severe events and the extreme events accounted for the minimum proportion with the increase of duration the proportion of moderate events decreased the proportion of severe events first increased and then decreased and the proportion of extreme events increased for the extreme cdhes with the highest severity the proportion of extreme cdhes of each duration was greater in swc than in eastern china and nc suggesting that swc is more prone to extreme cdhes and its disaster risk was greater the proportion of extreme cdhes in 1991 2018 was always larger than that in 1961 1990 a larger proportion of cdhes with extreme severity indicated that cdhes become more extreme furthermore fig 10 demonstrates the proportion of cdhes with different durations to the total cdhes in eastern china the stacked histogram is the ratio of the multi year average occurrence frequency of the two periods to the sum of the two periods in the two periods all cdhes ranked in descending order of duration length as isolated events short duration events medium duration events and long duration events as the severity increases the proportion of consecutive cdhes increased for all three categories in both time periods and the increase was greater for cdhes of longer duration the proportion of isolated cdhes decreases as the severity increases the same characteristics were observed in nc fig a 4 and swc fig a 5 in eastern china and nc the proportion of long duration events and medium duration events with different severities in 1991 to 2018 was always greater than that in 1961 to 1990 the proportion of isolated events in 1991 2018 was always less than that in 1961 1990 the increase in the proportion of cdhes with longer duration indicates that cdhes become more lasting there is no obvious difference in the proportion of events with different durations between the two time periods in swc the multi year average frequency was greater in 1991 2018 than that in 1961 1990 in eastern china and nc with the increase of duration the multi year average frequency from 1991 to 2018 accounted for a greater percentage of the sum of the two periods in most cases confirming the development of cdhes toward longer duration differences between different durations of the same severity were more pronounced in nc while there were no apparent differences in the multi year average frequency of different durations in swc the above suggests that the more persistent characteristics of the cdhes are obvious in nc while there is no such obvious characteristics in swc 3 3 variation in the occurrence date of cdhes events in this section we investigate the changes in the occurrence date of cdhes fig 11 showed the trend in the five year running mean of occurrence probabilities in each date the trends with different severities were generally consistent in the entire eastern china nc and swc most of the dates indicated a decreasing trend in the occurrence probability during the months from may to july and an increasing trend in august and thereafter this change means that cdhes were postponed in the warm season the probability distribution results of cdhes occurrence date in 1961 1990 and 1991 2018 were used to further analyze the characteristics and changes of occurrence date more visually fig 12 illustrates the probability distribution of cdhes occurrence date with the orange and blue bars showing the probabilities in 1961 1990 and 1991 2018 respectively we used two sample ks test to assess the differences in the probability distributions of occurrence date of the two time periods the test results showed that the data for the two periods came from the same distribution at 95 confidence level however we can still observe some changes to get some useful information although they are not significant the shape of the probability distribution was similar for different severities the possible occurrence dates of cdhes in the three regions during the warm season were from early may to early october eastern china early may to late september nc and early may to mid october swc respectively the possible time range of cdhes in swc was the longest the probability distributions of occurrence date in eastern china and nc were similar in shape with a single peak from 1961 to 1990 the probability of occurrence in july middle june to early july was the highest in eastern china in nc while the date with the highest probability of occurrence from 1991 to 2018 postponed to late july to middle august nc was the same but the peaks of extreme severity in both regions had not moved the probability distribution in swc is bimodal from 1961 to 1990 there were two obvious peaks in the probability distribution with a greater probability of occurrence from middle may to early june and from middle july to early august whereas from 1991 to 2018 the double peaks in the probability distribution became less obvious with the former peak weakening and the latter peak strengthening moreover most of the probabilities before late july middle july were greater in eastern china and nc in swc during 1961 1990 than during 1991 2018 while the differences between the two time periods was reversed for most of the probabilities after late july middle july in eastern china the differences in cumulative probability before and after late july were about 0 06 between 1961 and 1990 and 1991 2018 with few differences in different severities the differences were within 0 11 0 21 in nc and the differences decreased with the increase of severity however the differences in swc increased with severity and the differences in cumulative probability before and after middle july were 0 06 0 24 between 1961 and 1990 and 1991 2018 the above results demonstrated that most of the occurrence dates postponed in 1991 2018 relative to 1961 1990 and this change was most pronounced in swc we further investigate the probability of cdhes with different durations as the duration increases the kurtosis of most probability distributions increases table 3 and the shape of distribution changes from short and fat to tall and thin becoming more concentrated for instance in eastern china fig a 6 the dates on which cdhes were concentrated occurring with approximately 70 of the total probability are early june late august 1 2 days mid june mid august 3 5 days late june mid august 6 10 days and late june early august 10 days the probability distributions with different durations in nc were very similar to those in the entire eastern china fig a 7 in swc fig a 8 the bimodal character of the probability distribution became increasingly evident as the duration increases 4 discussion the changing characteristics of cdhes are closely related to the changes in individual events of drought and high temperature cdhds occurred frequently throughout eastern china from the 1960 s to the early 1970 s and after the 1990 s fig 4a fig 5a c it seems to be related to the fact that several regions of china experienced severe summer droughts in the 1960 s and 1970 s yao et al 2017 the increase in drought and hot events after the 1990 s may have led to an increase in compound events yu and zhai 2020 ding and qian 2011 meanwhile droughts and heat waves have become more frequent in southwestern and northern china gong et al 2017 jia and hu 2017 which is generally consistent with the increase in compound events in these regions moreover more hot events are predicted to occur in the context of a warming climate in the future resulting in relatively higher mortality wang et al 2017 wang et al 2019 with the further intensification of temperature rise in the future the frequency and risk of compound extreme events will increase ipcc 2021 the increase in multiple features of cdhes strengthens local disaster risk and may pose a greater threat than a single extreme event sedlmeier et al 2018 schumacher et al 2019 cdhes have caused serious social and economic losses in china zhang et al 2022 feng et al 2020 therefore improving the understanding of cdhes and providing early warnings is an urgent issue to be addressed furthermore we found that the probability distribution of the occurrence dates of cdhes in different regions is quite different in this study the distribution is unimodal in nc while it is bimodal in swc this may be due to the fact that the rainy season in swc has not yet started in may yan et al 2013 when the probability of drought is higher and the probability of high temperatures is also higher resulting in a peak in may the unimodal distribution in nc is because the probability of high temperature is quite small in may although the probability of drought is high at this time meanwhile most of the occurrence dates postponed in 1991 2018 relative to 1961 1990 this may be related to the delay of rainy season which is affected by monsoon activity south asia high water vapor transport and the like hao et al 2021 yan et al 2013 however there are still some points need to be improved in this study 1 the definition of compound events in this study only considers the daily precipitation and daily maximum temperature however meteorological variables such as evapotranspiration wind speed relative humidity and radiation also have impacts on compound events russo et al 2019 and some studies have added evapotranspiration to define cdhes jena et al 2020 kong et al 2019 moreover extreme high temperatures include not only daytime temperatures but also nighttime temperatures wang et al 2020 2 in addition to the characteristics and changes of cdhes future research should concentrate on the physical mechanism of cdhes the changes of compound events are affected by the changes of precipitation and temperature and their dependence kong et al 2019 for instance sea surface temperature sst plays an important role in the variations and trends of extreme temperature and precipitation dittus et al 2018 what is more sustained large scale circulation anomalies can trigger the occurrence of cdhes and land atmosphere feedbacks can exacerbate and propagate these events miralles et al 2019 besides global warming can lead to the enhancement of regional soil moisture temperature coupling in some water limited areas thus amplifying the intensity of heat waves during droughts cheng et al 2019 therefore cdhes are fundamentally influenced by sst anomalies atmospheric circulations land atmosphere interactions and climate warming and the physical mechanism of cdhes will be explored in the future 5 conclusions the social and environmental impacts of cdhes in which drought and hot events occur simultaneously are greater than the impacts of individual events in this study the spi and maximum temperature were used to define cdhes on the daily scale hot spot areas were identified and the characteristics and changes in the multiple aspects of cdhes were explored in order to mitigate their adverse effects the main conclusions are as follows there was an upward trend in most areas from the north to the southwest and a downward trend in most of the northwestern and southeastern regions for the number of cdhds nc and swc are high frequency regions of cdhds and the number of cdhds and land area ratio affected by cdhds were increasing which may have greater adverse effects thus these two regions were selected as hot spot areas for more detailed analysis cdhes have become more frequent long lasting and extreme in eastern china the frequency of cdhes tended to rise in most regions from the north to the southwest and was higher than that in the other regions the duration of cdhes was longer in the southern region than in the northern region with longer duration from the northern to southwestern regions cdhes of all durations and severities generally showed an increasing trend with the characteristics of being mostly below normal until the early 1990 s and mostly above normal thereafter comparing the contribution of cdhes with different severities durations to the total cdhes in the two periods it was found that the cdhes become more extreme and persistent especially in nc although the change in swc is not as great as that in nc both extreme and long duration events contribute more to the total cdhes in swc than in the other regions the possible occurrence dates of cdhes during the warm season were from early may to early october the entire eastern china early may to late september nc and early may to mid october swc respectively the shapes of the probability distribution were similar for the different severities the probability distribution of occurrence date was unimodal in the entire eastern china and it was similar in nc whereas the probability distribution was bimodal in swc compared with 1961 1990 most of the occurrence dates of cdhes with different severities in 1991 2018 postponed and this feature in swc was the most pronounced credit authorship contribution statement mengyang liu conceptualization data curation formal analysis methodology software validation writing original draft yixing yin software investigation project administration resources supervision writing review editing xiaojun wang funding acquisition investigation project administration resources supervision xieyao ma funding acquisition investigation project administration resources supervision ying chen supervision writing review editing weilin chen writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by national natural science foundation of china no 52121006 no 41877158 no 41875098 young top notch talent support program of national high level talents special support plan research project of ministry of natural resources no 20210103 six talents peak project of jiangsu province no jnhb 068 333 high level talents cultivation project of jiangsu province research project of jiangsu water conservancy research institute no 2022019 water science and technology project of department of water resources of zhejiang province no ra1704 and china scholarship council we are also thankful to anonymous reviewers and editors for their helpful comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128499 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
