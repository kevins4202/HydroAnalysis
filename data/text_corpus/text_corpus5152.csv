index,text
25760,in this study a spatiotemporal estimation method based on funk singular value decomposition f svd that considers the spatiotemporal correlation of rainfall is proposed to improve estimations from gauge observations hourly rainfall data of several flood events are selected to verify the proposed method by comparing with inverse distance weighting idw and ordinary kriging ok in hanjiang basin china the results show that 1 f svd has the best performance in rainfall estimation the larger the amount of rainfall event the greater the improvement of f svd method as compared to ok and idw 2 through the combination integration with f svd the accuracy of idw and ok can be greatly improved therefore f svd can be employed as a practical method to estimate rainfall spatial distribution which is essential data for regional hydrological modelling and water resource analysis keywords rainfall estimation spatiotemporal interpolation recommendation system singular value decomposition matrix factorization 1 introduction rainfall is one of the main sources of the water system and a key component of the water cycle on the earth diez sierra and del jesus 2017 due to the influence of natural climate terrain and underlying surface the spatial and temporal distributions of rainfall on the earth s surface are uneven where apparent characteristics of regional and temporal variation exist the spatial and temporal distribution of rainfall is of great significance for maintaining the life and health of all biological communities dai et al 2020 it is also one of the most critical data sources for hydrological scientific research water resources management drought and flood disaster management and ecological environment governance sivakumar and woldemeskel 2015 currently the most common way of rainfall observation and collection is a rainfall gauge network of which the main features are convenient real time and accurate however since the gauges are discrete spatial calculation methods are needed to obtain the spatially continuous rainfall distribution the spatial interpolation method plays a vital role in the calculation of the spatial distribution of rainfall data and has been widely concerned by many scholars ahrens 2006 garcia et al 2008 kumari et al 2016 morris et al 2016 the spatial interpolation methods for rainfall are based on tobler s first law of geography tobler 1970 that points closer in space are more likely to have similar eigenvalues and points farther away are less likely to have similar eigenvalues such as tyson polygon thiessen 1911 inverse distance weight idw shepard 1968 and kriging delhomme 1978 are among the most widely used methods in the spatial estimation for rainfall cai et al 2018 carrera hernández and gaskin 2007 foehn et al 2018 goovaerts 2000 plouffe et al 2015 ryu et al 2021 zhang et al 2018 while during the process of rainfall not only points adjacent in space are more likely to have similar characteristics but also points contiguous in time are more likely to have a consistent variation trend therefore to obtain more accurate results both the spatial dimension and the time dimension should be considered during interpolating many researches have been carried out on the spatiotemporal estimation method for rainfall such as space time autoregressive moving average model starma and kriging interpolation method with time extension cliff and ord 1975 dalezios and adamowski 1995 pfeifer and deutrch 1980 dalezios and adamowski 1995 applied starma models in spatiotemporal rainfall modelling bargaoui and chebbi 2009 proposed a 3 dimensional variogram location duration intensity to replace the traditional 2 dimensional variogram location intensity which can effectively consider the spatial variability of the maximum rainfall intensity in a given duration range and significantly reduce the rainfall prediction error spadavecchia and williams 2009 compared simple kriging sk ordinary kriging ok and space time kriging with an external drift using a residual variogram with spatiotemporal lags in the interpolation of meteorological variables besides some scholars combined the time machine learning method with spatial simulation to realize the spatiotemporal interpolation of rainfall xu et al 2019 proposed a novel spatiotemporal prediction model based on the cubic spline method and the spatiotemporal echo state networks which showed advantages in predicting meteorological series over other spatial estimation models in general some progress has been made in the spatiotemporal estimation method for rainfall and the existing results have shown that the spatial temporal interpolation methods have higher accuracies than those spatial interpolation methods without considering the time dimension the superiority of spatial temporal interpolation methods for rainfall has been proved however compared with extensive application of the spatial estimation methods for rainfall the existing spatiotemporal estimation methods for rainfall are too complex to be widely applied due to strong randomness and complexity of the rainfall process in order to more conveniently and widely use the spatial temporal estimation methods for rainfall there are still exploration works to be worth doing for them if the rainfall data at different times of each gauge are putting together it can be found that an enormous two dimensional spatiotemporal matrix is formed where the rows are the time dimension and the columns are the spatial dimension as shown in fig 1 a from fig 1 a it can be seen that the rainfall process has a clear correlation in the time t and space dimension s it is possible to transform the spatial temporal interpolation problem into a two dimensional matrix solution problem for example if a certain value in a matrix is missing it can be calculated through matrix factorization technology which has been widely used in the e commerce recommender system fig 1 b a common task of the recommender system is to improve customer experience through personalized recommendations based on historical interactions and prior implicit feedback hu et al 2008 these interactions are stored in the so called user item interactions matrix fig 1 b which are used in many famous e commerce platforms to recommend relevant products to users such as amazon taobao joybuy youtube and netflix etc its algorithms are mainly divided into two categories collaborative filtering methods and content based methods and the collaborative filtering cf algorithm gains an advantage due to its insensitivity to content yu et al 2018 which predicts user preferences for products by learning known user item relationships bell and koren 2007 matrix factorization mf technique is one of the most popular approaches for solving the problem of cf which views user preference ratings of items as a user item matrix and uses known user ratings of items to predict user preferences in item selection takacs et al 2009 as mf in the recommender system has high prediction accuracy it has become recognized as a mature method in environmental science biomedicine and many other fields xie and berkowitz 2006 xue et al 2014 zhang et al 2019 gonzález macías et al 2014 used the positive matrix factorization approach in identification and source apportionment of the anthropogenic heavy metals in the sediments of sea lee et al 2012 applied non negative matrix factorization to new gene expression data quantifying the molecular changes in four tissue types due to different dosages of an experimental panppar agonist in mouse yeh et al 2018 proposed a rain removal method based on non negative matrix factorization to improve image quality funk singular value decomposition model f svd proposed by funk 2006 is a variant of mf that outperforms other models in the netflix prize competition the essential idea incorporated in f svd of mf is that users and items can be described by their latent features vectors inferred from rating matrix and the high correspondence between user and item features leads to recommendation koren et al 2009 as rainfall data can be viewed as an intrinsically related matrix f svd is a good way to estimate an unknown point in the spatiotemporal rainfall matrix from fig 1 it can be seen that there are similar interactions between spatiotemporal rainfall matrix and the user item interactions matrix in this study f svd in the recommender system is regarded as a potential spatiotemporal method to estimate the rainfall for the first time and its performance is evaluated by compared with idw and ok methods this paper aims to present a new approach using information of points both adjacent in space and contiguous in time to estimate rainfall more accurately and obtain continuous spatial distribution of rainfall which is helpful for regional hydrological modelling and spatial statistical analysis the rest of this paper is structured as follows section 2 introduces the proposed spatiotemporal interpolation method based on f svd and its implementation steps section 3 introduces the study area data and evaluation indicators section 4 analyzes and discusses the results of spatiotemporal interpolation finally section 5 summarizes the results of the study and presents existing problems and suggestions 2 spatiotemporal estimation method based on f svd 2 1 rainfall spatiotemporal estimation expressed by f svd rainfall data can be viewed as an intrinsically related matrix with columns representing time and rows representing space respectively there is an implicit interactions relationship between time and space which cannot be figured out directly thus latent factors are needed to establish an indirect relationship there is an example to illustrate the basic ideas of f svd to achieve spatiotemporal estimation of rainfall as shown in fig 2 the rainfall records of 5 gauges with 5 moments form a spatiotemporal two dimensional matrix r and the rainfall at gauge s 5 at moment t 5 is supposed to be unknown marked with red box in r firstly assuming there are latent factors affecting the spatiotemporal distribution of the rainfall at each gauge and each moment they can t be observed directly from r while they can be decomposed from r by using f svd in this case the derived latent spatial factors matrix x and temporal factors matrix y consist of 5 row vectors x i and column vectors y i i 1 5 which represent the interactions relationships of latent factors in space and time then it can be assumed that rainfall values in r can be derived from latent factors x and y by multiplying them for example there exist latent factors x 5 and y 5 for gauge s5 and moment t5 marked with red box in x and y which can be deduced from spatiotemporal interactions relationships in r by using f svd the supposed unknown values at gauge s 5 and moment t 5 can be estimated by multiplying latent factors x 5 and y 5 whose estimation is 5 1 mm by comparing the estimation matrix r and original matrix r it can be seen that there are estimation errors between them while they are relatively small for example for unknown points the relative error is 8 9 which indicates the high accuracy of f svd in rainfall estimation of course this is a specific case and the applicability of the f svd method needs to be further verified in the following sections 2 2 proposed spatiotemporal estimation model based on f svd the objective of the proposed spatiotemporal estimation method based on f svd is to use the current time and historical information to interpolate rainfall at the target points the steps are as follows where the transformation of variables involved is shown in fig 3 1 for the estimation of rainfall at position i at moment j a spatiotemporal interactions matrix r sized of m n which consists of rainfall of m positions at n moments is needed the positions and moments corresponding to the rows and columns of the matrix have to be figured out first and the method is shown below in matrix r the m positions consists of target position and rainfall gauges around which belong to the optimal set chosen by the spatial uniformity l after running through all the possible permutations randomly select m 1 gauges from all available gauges and combine them with the target position to form a set of m points for the formed set l is calculate based on the spatial distribution of points drawn according to the latitudes and longitudes l is a measure of spatial relationship of point set and the larger the l value the more evenly distributed the points by listing all possible combinations and calculating the spatial uniformity the point set with the largest l is the optimal set l can be defined as 1 l 4 a π a in the equation above a represents the area of the grid rectangle that contains all the points and a indicates the total area of exclusive circles which is defined for each point in the set as a circle with the center of itself and a radius of half the distance from the nearest adjacent point as for the set of previous moments t n involved considering the efficiency and accuracy of matrix factorization it is determined by n according to the following rules 2 t n n t s t a r t t s t a r t 1 j j t s t a r t n n j n j n 1 j j s t a r t n where t s t a r t denotes the starting time of rainfall event if the rainfall event does not last long then n ranges from the starting time t s t a r t to the interpolated time j else if the event lasts longer than n for moment j to which over n hours passed from the starting time n includes n moments before j and j itself 2 after obtaining the correspondence between matrix rows and positions matrix columns and moments the rainfall data used for interpolation need to be filled into the matrix accordingly for the m 1 rainfall gauges involved the rainfall data before and at the interpolation time are filled directly into the related locations in the matrix for the target position if there are observation records before the interpolation time then the observed data are directly filled into the corresponding locations in which way only f svd is used for interpolation else the traditional method such as idw has to be used to interpolate the historical rainfall first and then the historical interpolation result is filled into the matrix in which way the f svd is integrated with traditional method after the data filling is completed only the position corresponding to the target point and the interpolated time in the matrix is a null value 3 based on the f svd model the matrix r is decomposed into spatial feature matrix x and temporal feature matrix y by computing the relationships of q latent features in time and space through minimizing squared error on all known rainfall moreover in case of the phenomenon of over fitting regularization method is introduced to the objective function 3 e 2 i j r i j r i j 2 r i j q 1 q x i q y q j 2 4 min s s e i 1 m 1 j 1 n e i j 2 λ i q x i q 2 λ q j y q j 2 where s s e denotes the loss function and λ is a hyper parameter that controls the degree of regularization in order to minimize sse stochastic gradient descent sgd algorithm is chosen to solve the optimization problem above a summary of this method is available as a flowchart here in fig 4 x i q and y q j will decrease in the direction of the fastest one in which the gradient descent and therefore the optimal solution can be inferred to learn the optimum value of spatial feature vector x i rainfall of all known time p i j j 1 2 n are used to factorized that is to say the value of each component in spatial feature vector x i is related to all temporal feature vector y j j 1 2 n that extracted from historical rainfall information the equations are as follow 5 x i q x i q α e i j 2 x i q x i q 2 α e i j y q j λ x i q 6 y q j y q j α e i j 2 y q j y q j 2 α e i j x i q λ y q j in the equations above α indicates the learning rate in machine learning 4 the spatial feature matrix x and temporal feature matrix y are multiplied to obtain the optimal reconstructed interactions matrix r and each element in it has a value a one to one correspondence exists between the elements in r and r that is the value of the element in spatiotemporal interactions matrix r is equal to that of reconstructed interactions matrix r plus reconstruction error matrix hence the value of row i column j in r is the estimated rainfall of point i at moment j 2 3 evaluation methods in this study two widely used spatial interpolation approaches including idw and ok are adopted as benchmark methods for comparison without considering the temporal change trend f svd can estimate the rainfall value of one site by using the spatiotemporal matrix of rainfall which can be applied to the estimation of missing rainfall value or the test of rainfall abnormal value for sites when it is applied to the interpolation of unknown points in space it needs to be combined with the existing spatial interpolation methods to obtain more accurate spatial interpolation results in order to evaluate the performance of the combination of f svd with the spatial interpolation methods this study considered the combined use of f svd with idw and ok respectively named f svd idw and f svd ok the leave one out cross validation method was adopted to assess the accuracy in this process each time a record of one gauge from the dataset was removed and then be assumed using the information of all the gauges left then the interpolation results were compared to the observations to evaluate the estimation error using four statistical measures namely root mean square error rmse mean average error mae percentage error perc and two sample kolmogorov smirnov test statistic ks among these statistical measures the two sample kolmogorov smirnov test is a non parametric test that compares whether there is a significant difference between two samples based on the empirical distribution function and it is applicable and even for small sample sizes engmann and cousineau 2011 the calculation formulas of each indicator are as follows 7 r s m e 1 n i 1 n z i s i m z i o b s 2 8 m a e 1 n i 1 n z i s i m z i o b s 9 p e r c 1 n 1 n 2 i 1 n 1 z i s i m z i o b s z i o b s n 2 10 sup x r f 1 x f 2 x d p k s i 0 sup x r f 1 x f 2 x d p k s i 1 k s 1 n i 1 n k s i where z i o b s and z i s i m denote the observed value and the interpolated value at the i th gauge n 1 and n 2 represent the number of records which are non zero and records where a measured zero is not predicted f 1 and f 2 indicate the distribution functions of calculation sequence and observation sequence of the i th gauge d p is the critical value at the significance level p 5 for all measures the smaller the value the better the results the low values of the first three measures indicate that the errors of the interpolation results are small and the closer the fourth measure is to 0 the fewer gauges with significant errors in the interpolation and the measured sequence 3 study region and data the study region is the upstream of the hanjiang basin fig 5 which is the largest tributary in the yangtze river and the water source of the middle route project of south to north water transfer china chen et al 2007 it originates from qinling mountain and is located in the southeast of china between east longitude of 106 15 112 00 and north latitude of 31 40 34 20 the entire drainage area of the study region is about 96 000 km2 influenced by geographical factors the basin has a subtropical monsoon climate with humid air and abundant rainfall the annual average rainfall is approximately 830 mm decreasing from south to north the spatial distribution of 176 rainfall gauges is shown in fig 5 hourly rainfall data from several flood events were selected during the period from 2012 to 2018 under different meteorological conditions in hanjiang basin the starting and finishing time of rainfall events were determined according to the division of flood event based on hydrological hydrographs observed at the outlet streamflow gauge of the study area the 20 selected events as well as some statistics are listed in table 1 it can be seen from the table that the selected rainfall events are distributed from april to october spanning the three seasons of spring summer and autumn including the main rainfall season in the hanjiang river basin may to october thus have a good representation the duration of different rainfall events spans a wide range with the shortest being only 16 h and the longest reaching 106 h the percentage of gauges with a cumulative observed rainfall of zero varies from less than 5 to more than 20 which represents the different spatial concentration of rainfall 4 results and discussions 4 1 sensitivity analysis on the value of m and n the f svd method requires two parameters m and n to determine the size of the matrix when interpolating where m is the number of gauges involved in interpolation and is related to the number of rows of the matrix combining n and interpolation time j can determine the number of columns in the matrix to obtain the best interpolation result various values of these two parameters are selected for calculation and comparison table 2 lists the average spatial uniformity of all stations when m takes different values as the value of m increases the spatial uniformity increases first and then decreases when the value of m is 20 l reaches the maximum value which means the distribution of the surrounding stations is the most uniform and can well reflect the spatial information in all directions around the interpolation point in this case therefore the value of m in this study is assigned 20 for the determination of the value of n to ensure the efficiency of the calculation three typical long term events were selected and the accuracy of interpolation is calculated in different situations these three events are no 1 no 14 and no 19 respectively and their duration exceeds 50 h which are long lasting rainfall events here rsme is used as the evaluation indicator and the result is shown in fig 6 it is found that as the value of n becomes larger rsme decreases first and then increases reaching the minimum value that is the highest accuracy when n is 24 so n is assigned 24 in this study 4 2 models evaluation on all rainfall events the overall estimation results of the five models in 20 rainfall events are comprehensively evaluated using the four different indicators and shown in table 3 it can be seen from table 3 that the accuracy of the results of the five methods is within a reasonable range indicating that they can be well applied to rainfall estimation in the hanjiang river basin the evaluation results using four selected indicators are consistent with the accuracy of f svd being the highest f svd idw and f svd ok being the second and third highest idw being the fourth highest and ok being the lowest the result of idw is better than ok with a small gap which is similar to previous research hadi and tombul 2018 yang et al 2015 besides the difference in accuracy between the five methods is more obvious judged by mae and rsme since these two indicators are directly related to the amount of rainfall and heavy rainfall usually has a great impact on the values of them and perc and ks are less likely to be influenced by the magnitude of rainfall thus the difference is smaller wasko et al 2013 the cumulative rainfall of 20 events in the basin and the cross validation interpolation results of the five methods are shown in fig 7 it can be seen that rainfall is mainly concentrated in the southwest of the basin this is due to the high terrain in the southwest the warm and humid air flows along the windward slope of the mountain range plus the influence of the local climate forming a strong rainfall center in the southwest of the basin with rsme as the evaluation index the sites with large cumulative rainfall mainly distributed in the southwest have large interpolation errors as the value of rsme is directly affected by the amount of rainfall the sites with small interpolation errors are mainly distributed in the northern part of the basin since the rainfall in the north is very small or even zero comparing the distribution of gauges errors using the five methods methods that combined with f svd have more yellow points in the northern part and less blue points in the southern part of the basin than ok and idw indicating that the use of f svd will improve the accuracy no matter the rainfall is large or small the areal rainfall characters of 20 events calculated by the interpolation results are shown in table 4 among them the calculation results that are closest to the actual rainfall records are marked with a green background and the farthest ones are marked with orange it can be seen that the average of areal rainfall ranges from 0 5 mm to 3 mm the standard deviation reflects the fluctuation of rainfall at different times no 5 event has an average areal rainfall of 1 22 mm which is of medium scale but the standard deviation is the lowest indicating that the rainfall process is relatively smooth with few cases of sudden increase and decrease the average areal rainfall of the no 9 event is 1 86 mm and it only lasts for 16 h but the standard deviation reaches 2 01 indicating that rainfall mainly concentrated in several periods the characteristic values of the events calculated by the five interpolation methods are not much different from the measured values among the 20 selected rainfalls for both rainfall characteristic indicators at least 16 of the rainfall characteristic values calculated by f svd are the closest to the actual measurement and that calculated by ok are the farthest besides through combination with f svd both idw and ok perform better and generate more accurate rainfall characteristics than before 4 3 models evaluation on representative rainfall events to better evaluate the interpolation ability of the f svd model the magnitude of the event is sorted and three typical events of heavy moderate and small rain are selected for comparative analysis they are in turn the no 7 small rain event 7 may 8 may 2015 no 20 moderate rain event 17 jun 19 jun 2018 and no 18 heavy rain event 23 sep 27 sep 2017 the results are shown in fig 8 in the figure a is the cumulative observed rainfall at each site b c and d are the mae of f svd idw and ok respectively and e and f are the difference in accuracy between svd and the other two methods where the blue dots indicate the accuracy of f svd is higher and the red dot indicates that of f svd is lower it can be seen that for rainfall events of different magnitudes the interpolation error of gauges with heavy rainfall is large besides the blue dots in figures e and f are more than the red points indicating the number of gauges whose interpolation accuracy using f svd is higher than using idw and ok is more but for the gauges with small cumulative rainfall 5 mm the accuracy is not improved the difference in accuracy between f svd and the other two methods is small during small rain but as the rainfall magnitude becomes larger the difference gradually increases in the heavy rain event the historical rainfall at most gauges is not zero thus the f svd method can effectively extract the spatial and temporal feature information from the historical rainfall for interpolation resulting in a noticeable improvement in accuracy the results of three representative rainfall events using all five interpolation methods evaluated by different indicators are listed in table 5 it can be seen that the accuracy of idw is higher than that of ok and through combination with f svd the accuracy of idw and ok are greatly improved the accuracy of f svd is highest which shows good estimation ability than traditional interpolators as the magnitude of rainfall increases the difference in accuracy between the five methods becomes larger the interpolation error of no 7 small rain event is the lowest judging from mae rsme and perc besides the evaluation result of no 20 moderate rain event using all indicators is worser than no 18 heavy rain event as can be seen from table 1 n0 20 event lasts for 106 h and no 18 event only lasts for 44 h for n0 20 event the rainfall is very scattered and is extremely low in many moments and the trend and regularity are not obvious contributing to a larger interpolation error 4 4 estimation error distribution of interpolation methods the average gauge error in different selected events using five interpolation methods is shown in fig 9 it is found that except for few events most of them produce the largest error using ok and the smallest error using f svd through combination with f svd the accuracy of both idw and ok is improved the evaluation results by rsme and mae are similar since they are both greatly affected by the rainfall amount the results of perc and ks are not the same some rainfall events such as no 11 have low rsme mae and high ks and perc as the areal total rainfall in the no 11 event is only 15 9 mm the accumulated rainfall at each gauge is relatively low so the rsme and mae values are not too large the calculation of perc concerns more with the relative error than the absolute error for gauges with little rainfall although the interpolated rainfall is also very low the relative error may be large causing perc to become large to evaluate the uncertainty of the five methods two indicators mae and rsme are selected and a box plot of the errors of all stations in 20 events is shown in fig 10 it can be seen from the figure that the variation trends of mae and rsme are consistent the median values of ok and idw are higher than other methods and the confidence intervals of them are wider ok has a rather large uncertainty as its confidence interval varies in different events for most rainfall events the median value and the width of the confidence interval of f svd ok and f svd idw are very close and sometimes the median value of f svd idw is slightly lower than that of f svd ok through combination the width of the confidence interval of them is greatly shorter than the previous two methods and the median is also lower the error distribution of f svd is the best as it directly uses observed records for estimation it shows that the f svd method not only improves the interpolation accuracy but also has higher stability among the 20 rainfall events interpolated by f svd the relatively large errors mainly occur in no 2 no 4 no 9 and no 13 events and their durations are 30 32 16 and 38 h respectively thus the f svd method may perform less well in short duration rainfall events 5 conclusions in this paper a new method based on f svd to improve rainfall estimation is proposed unlike traditional interpolators that only focus on the spatial relationships of gauges the proposed method incorporates rainfall information at the current and historical moments into the estimation process which results in a more accurate result through combination with traditional interpolators it can be applied to interpolate rainfall at unknown points and obtain continuous spatial distribution of rainfall thus it is a practical method to process rainfall data for spatial pattern analysis and prepare input data for distributed hydrological models twenty rainfall events are selected from the hourly rainfall data of the rainfall gauges in the hanjiang basin to verify this method by cross validation using four indicators and idw and kriging are included as benchmarks for accuracy comparison the study concludes that 1 according to the interpolation results of 20 rainfalls f svd has the highest accuracy and ok has the lowest accuracy through combination of ok and idw with f svd the accuracy of ok and idw can be greatly improved due to the different emphasis of the indicators the gap between five methods in rsme and mae is larger than that in perc and ks 2 in cross validation the rainfall magnitude has an effect on the interpolation accuracy for different rainfall events the larger the rainfall magnitude the more gauges of which the accuracy can be improved by the f svd method 3 according to the error distribution of all stations in each rainfall event f svd not only improves the interpolation accuracy but also reduces the uncertainty of the error so f svd has better stability however there are also some limitations in this study f svd is a latent factor model and its algorithmic meaning is to build relationships between time and space through latent factors which cannot correspond to physical concepts in reality thus it has a poor interpretability besides only two widely used interpolation methods ok and idw are used for comparison and one basin hanjiang basin is considered in this study the conclusions may not be generalized therefore more basins with different distribution of gauges and more methods for comparison will be helpful to validate the proposed interpolation method declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is financially supported by the national key research and development program 2018yfc0407904 2017yfa0603702 
25760,in this study a spatiotemporal estimation method based on funk singular value decomposition f svd that considers the spatiotemporal correlation of rainfall is proposed to improve estimations from gauge observations hourly rainfall data of several flood events are selected to verify the proposed method by comparing with inverse distance weighting idw and ordinary kriging ok in hanjiang basin china the results show that 1 f svd has the best performance in rainfall estimation the larger the amount of rainfall event the greater the improvement of f svd method as compared to ok and idw 2 through the combination integration with f svd the accuracy of idw and ok can be greatly improved therefore f svd can be employed as a practical method to estimate rainfall spatial distribution which is essential data for regional hydrological modelling and water resource analysis keywords rainfall estimation spatiotemporal interpolation recommendation system singular value decomposition matrix factorization 1 introduction rainfall is one of the main sources of the water system and a key component of the water cycle on the earth diez sierra and del jesus 2017 due to the influence of natural climate terrain and underlying surface the spatial and temporal distributions of rainfall on the earth s surface are uneven where apparent characteristics of regional and temporal variation exist the spatial and temporal distribution of rainfall is of great significance for maintaining the life and health of all biological communities dai et al 2020 it is also one of the most critical data sources for hydrological scientific research water resources management drought and flood disaster management and ecological environment governance sivakumar and woldemeskel 2015 currently the most common way of rainfall observation and collection is a rainfall gauge network of which the main features are convenient real time and accurate however since the gauges are discrete spatial calculation methods are needed to obtain the spatially continuous rainfall distribution the spatial interpolation method plays a vital role in the calculation of the spatial distribution of rainfall data and has been widely concerned by many scholars ahrens 2006 garcia et al 2008 kumari et al 2016 morris et al 2016 the spatial interpolation methods for rainfall are based on tobler s first law of geography tobler 1970 that points closer in space are more likely to have similar eigenvalues and points farther away are less likely to have similar eigenvalues such as tyson polygon thiessen 1911 inverse distance weight idw shepard 1968 and kriging delhomme 1978 are among the most widely used methods in the spatial estimation for rainfall cai et al 2018 carrera hernández and gaskin 2007 foehn et al 2018 goovaerts 2000 plouffe et al 2015 ryu et al 2021 zhang et al 2018 while during the process of rainfall not only points adjacent in space are more likely to have similar characteristics but also points contiguous in time are more likely to have a consistent variation trend therefore to obtain more accurate results both the spatial dimension and the time dimension should be considered during interpolating many researches have been carried out on the spatiotemporal estimation method for rainfall such as space time autoregressive moving average model starma and kriging interpolation method with time extension cliff and ord 1975 dalezios and adamowski 1995 pfeifer and deutrch 1980 dalezios and adamowski 1995 applied starma models in spatiotemporal rainfall modelling bargaoui and chebbi 2009 proposed a 3 dimensional variogram location duration intensity to replace the traditional 2 dimensional variogram location intensity which can effectively consider the spatial variability of the maximum rainfall intensity in a given duration range and significantly reduce the rainfall prediction error spadavecchia and williams 2009 compared simple kriging sk ordinary kriging ok and space time kriging with an external drift using a residual variogram with spatiotemporal lags in the interpolation of meteorological variables besides some scholars combined the time machine learning method with spatial simulation to realize the spatiotemporal interpolation of rainfall xu et al 2019 proposed a novel spatiotemporal prediction model based on the cubic spline method and the spatiotemporal echo state networks which showed advantages in predicting meteorological series over other spatial estimation models in general some progress has been made in the spatiotemporal estimation method for rainfall and the existing results have shown that the spatial temporal interpolation methods have higher accuracies than those spatial interpolation methods without considering the time dimension the superiority of spatial temporal interpolation methods for rainfall has been proved however compared with extensive application of the spatial estimation methods for rainfall the existing spatiotemporal estimation methods for rainfall are too complex to be widely applied due to strong randomness and complexity of the rainfall process in order to more conveniently and widely use the spatial temporal estimation methods for rainfall there are still exploration works to be worth doing for them if the rainfall data at different times of each gauge are putting together it can be found that an enormous two dimensional spatiotemporal matrix is formed where the rows are the time dimension and the columns are the spatial dimension as shown in fig 1 a from fig 1 a it can be seen that the rainfall process has a clear correlation in the time t and space dimension s it is possible to transform the spatial temporal interpolation problem into a two dimensional matrix solution problem for example if a certain value in a matrix is missing it can be calculated through matrix factorization technology which has been widely used in the e commerce recommender system fig 1 b a common task of the recommender system is to improve customer experience through personalized recommendations based on historical interactions and prior implicit feedback hu et al 2008 these interactions are stored in the so called user item interactions matrix fig 1 b which are used in many famous e commerce platforms to recommend relevant products to users such as amazon taobao joybuy youtube and netflix etc its algorithms are mainly divided into two categories collaborative filtering methods and content based methods and the collaborative filtering cf algorithm gains an advantage due to its insensitivity to content yu et al 2018 which predicts user preferences for products by learning known user item relationships bell and koren 2007 matrix factorization mf technique is one of the most popular approaches for solving the problem of cf which views user preference ratings of items as a user item matrix and uses known user ratings of items to predict user preferences in item selection takacs et al 2009 as mf in the recommender system has high prediction accuracy it has become recognized as a mature method in environmental science biomedicine and many other fields xie and berkowitz 2006 xue et al 2014 zhang et al 2019 gonzález macías et al 2014 used the positive matrix factorization approach in identification and source apportionment of the anthropogenic heavy metals in the sediments of sea lee et al 2012 applied non negative matrix factorization to new gene expression data quantifying the molecular changes in four tissue types due to different dosages of an experimental panppar agonist in mouse yeh et al 2018 proposed a rain removal method based on non negative matrix factorization to improve image quality funk singular value decomposition model f svd proposed by funk 2006 is a variant of mf that outperforms other models in the netflix prize competition the essential idea incorporated in f svd of mf is that users and items can be described by their latent features vectors inferred from rating matrix and the high correspondence between user and item features leads to recommendation koren et al 2009 as rainfall data can be viewed as an intrinsically related matrix f svd is a good way to estimate an unknown point in the spatiotemporal rainfall matrix from fig 1 it can be seen that there are similar interactions between spatiotemporal rainfall matrix and the user item interactions matrix in this study f svd in the recommender system is regarded as a potential spatiotemporal method to estimate the rainfall for the first time and its performance is evaluated by compared with idw and ok methods this paper aims to present a new approach using information of points both adjacent in space and contiguous in time to estimate rainfall more accurately and obtain continuous spatial distribution of rainfall which is helpful for regional hydrological modelling and spatial statistical analysis the rest of this paper is structured as follows section 2 introduces the proposed spatiotemporal interpolation method based on f svd and its implementation steps section 3 introduces the study area data and evaluation indicators section 4 analyzes and discusses the results of spatiotemporal interpolation finally section 5 summarizes the results of the study and presents existing problems and suggestions 2 spatiotemporal estimation method based on f svd 2 1 rainfall spatiotemporal estimation expressed by f svd rainfall data can be viewed as an intrinsically related matrix with columns representing time and rows representing space respectively there is an implicit interactions relationship between time and space which cannot be figured out directly thus latent factors are needed to establish an indirect relationship there is an example to illustrate the basic ideas of f svd to achieve spatiotemporal estimation of rainfall as shown in fig 2 the rainfall records of 5 gauges with 5 moments form a spatiotemporal two dimensional matrix r and the rainfall at gauge s 5 at moment t 5 is supposed to be unknown marked with red box in r firstly assuming there are latent factors affecting the spatiotemporal distribution of the rainfall at each gauge and each moment they can t be observed directly from r while they can be decomposed from r by using f svd in this case the derived latent spatial factors matrix x and temporal factors matrix y consist of 5 row vectors x i and column vectors y i i 1 5 which represent the interactions relationships of latent factors in space and time then it can be assumed that rainfall values in r can be derived from latent factors x and y by multiplying them for example there exist latent factors x 5 and y 5 for gauge s5 and moment t5 marked with red box in x and y which can be deduced from spatiotemporal interactions relationships in r by using f svd the supposed unknown values at gauge s 5 and moment t 5 can be estimated by multiplying latent factors x 5 and y 5 whose estimation is 5 1 mm by comparing the estimation matrix r and original matrix r it can be seen that there are estimation errors between them while they are relatively small for example for unknown points the relative error is 8 9 which indicates the high accuracy of f svd in rainfall estimation of course this is a specific case and the applicability of the f svd method needs to be further verified in the following sections 2 2 proposed spatiotemporal estimation model based on f svd the objective of the proposed spatiotemporal estimation method based on f svd is to use the current time and historical information to interpolate rainfall at the target points the steps are as follows where the transformation of variables involved is shown in fig 3 1 for the estimation of rainfall at position i at moment j a spatiotemporal interactions matrix r sized of m n which consists of rainfall of m positions at n moments is needed the positions and moments corresponding to the rows and columns of the matrix have to be figured out first and the method is shown below in matrix r the m positions consists of target position and rainfall gauges around which belong to the optimal set chosen by the spatial uniformity l after running through all the possible permutations randomly select m 1 gauges from all available gauges and combine them with the target position to form a set of m points for the formed set l is calculate based on the spatial distribution of points drawn according to the latitudes and longitudes l is a measure of spatial relationship of point set and the larger the l value the more evenly distributed the points by listing all possible combinations and calculating the spatial uniformity the point set with the largest l is the optimal set l can be defined as 1 l 4 a π a in the equation above a represents the area of the grid rectangle that contains all the points and a indicates the total area of exclusive circles which is defined for each point in the set as a circle with the center of itself and a radius of half the distance from the nearest adjacent point as for the set of previous moments t n involved considering the efficiency and accuracy of matrix factorization it is determined by n according to the following rules 2 t n n t s t a r t t s t a r t 1 j j t s t a r t n n j n j n 1 j j s t a r t n where t s t a r t denotes the starting time of rainfall event if the rainfall event does not last long then n ranges from the starting time t s t a r t to the interpolated time j else if the event lasts longer than n for moment j to which over n hours passed from the starting time n includes n moments before j and j itself 2 after obtaining the correspondence between matrix rows and positions matrix columns and moments the rainfall data used for interpolation need to be filled into the matrix accordingly for the m 1 rainfall gauges involved the rainfall data before and at the interpolation time are filled directly into the related locations in the matrix for the target position if there are observation records before the interpolation time then the observed data are directly filled into the corresponding locations in which way only f svd is used for interpolation else the traditional method such as idw has to be used to interpolate the historical rainfall first and then the historical interpolation result is filled into the matrix in which way the f svd is integrated with traditional method after the data filling is completed only the position corresponding to the target point and the interpolated time in the matrix is a null value 3 based on the f svd model the matrix r is decomposed into spatial feature matrix x and temporal feature matrix y by computing the relationships of q latent features in time and space through minimizing squared error on all known rainfall moreover in case of the phenomenon of over fitting regularization method is introduced to the objective function 3 e 2 i j r i j r i j 2 r i j q 1 q x i q y q j 2 4 min s s e i 1 m 1 j 1 n e i j 2 λ i q x i q 2 λ q j y q j 2 where s s e denotes the loss function and λ is a hyper parameter that controls the degree of regularization in order to minimize sse stochastic gradient descent sgd algorithm is chosen to solve the optimization problem above a summary of this method is available as a flowchart here in fig 4 x i q and y q j will decrease in the direction of the fastest one in which the gradient descent and therefore the optimal solution can be inferred to learn the optimum value of spatial feature vector x i rainfall of all known time p i j j 1 2 n are used to factorized that is to say the value of each component in spatial feature vector x i is related to all temporal feature vector y j j 1 2 n that extracted from historical rainfall information the equations are as follow 5 x i q x i q α e i j 2 x i q x i q 2 α e i j y q j λ x i q 6 y q j y q j α e i j 2 y q j y q j 2 α e i j x i q λ y q j in the equations above α indicates the learning rate in machine learning 4 the spatial feature matrix x and temporal feature matrix y are multiplied to obtain the optimal reconstructed interactions matrix r and each element in it has a value a one to one correspondence exists between the elements in r and r that is the value of the element in spatiotemporal interactions matrix r is equal to that of reconstructed interactions matrix r plus reconstruction error matrix hence the value of row i column j in r is the estimated rainfall of point i at moment j 2 3 evaluation methods in this study two widely used spatial interpolation approaches including idw and ok are adopted as benchmark methods for comparison without considering the temporal change trend f svd can estimate the rainfall value of one site by using the spatiotemporal matrix of rainfall which can be applied to the estimation of missing rainfall value or the test of rainfall abnormal value for sites when it is applied to the interpolation of unknown points in space it needs to be combined with the existing spatial interpolation methods to obtain more accurate spatial interpolation results in order to evaluate the performance of the combination of f svd with the spatial interpolation methods this study considered the combined use of f svd with idw and ok respectively named f svd idw and f svd ok the leave one out cross validation method was adopted to assess the accuracy in this process each time a record of one gauge from the dataset was removed and then be assumed using the information of all the gauges left then the interpolation results were compared to the observations to evaluate the estimation error using four statistical measures namely root mean square error rmse mean average error mae percentage error perc and two sample kolmogorov smirnov test statistic ks among these statistical measures the two sample kolmogorov smirnov test is a non parametric test that compares whether there is a significant difference between two samples based on the empirical distribution function and it is applicable and even for small sample sizes engmann and cousineau 2011 the calculation formulas of each indicator are as follows 7 r s m e 1 n i 1 n z i s i m z i o b s 2 8 m a e 1 n i 1 n z i s i m z i o b s 9 p e r c 1 n 1 n 2 i 1 n 1 z i s i m z i o b s z i o b s n 2 10 sup x r f 1 x f 2 x d p k s i 0 sup x r f 1 x f 2 x d p k s i 1 k s 1 n i 1 n k s i where z i o b s and z i s i m denote the observed value and the interpolated value at the i th gauge n 1 and n 2 represent the number of records which are non zero and records where a measured zero is not predicted f 1 and f 2 indicate the distribution functions of calculation sequence and observation sequence of the i th gauge d p is the critical value at the significance level p 5 for all measures the smaller the value the better the results the low values of the first three measures indicate that the errors of the interpolation results are small and the closer the fourth measure is to 0 the fewer gauges with significant errors in the interpolation and the measured sequence 3 study region and data the study region is the upstream of the hanjiang basin fig 5 which is the largest tributary in the yangtze river and the water source of the middle route project of south to north water transfer china chen et al 2007 it originates from qinling mountain and is located in the southeast of china between east longitude of 106 15 112 00 and north latitude of 31 40 34 20 the entire drainage area of the study region is about 96 000 km2 influenced by geographical factors the basin has a subtropical monsoon climate with humid air and abundant rainfall the annual average rainfall is approximately 830 mm decreasing from south to north the spatial distribution of 176 rainfall gauges is shown in fig 5 hourly rainfall data from several flood events were selected during the period from 2012 to 2018 under different meteorological conditions in hanjiang basin the starting and finishing time of rainfall events were determined according to the division of flood event based on hydrological hydrographs observed at the outlet streamflow gauge of the study area the 20 selected events as well as some statistics are listed in table 1 it can be seen from the table that the selected rainfall events are distributed from april to october spanning the three seasons of spring summer and autumn including the main rainfall season in the hanjiang river basin may to october thus have a good representation the duration of different rainfall events spans a wide range with the shortest being only 16 h and the longest reaching 106 h the percentage of gauges with a cumulative observed rainfall of zero varies from less than 5 to more than 20 which represents the different spatial concentration of rainfall 4 results and discussions 4 1 sensitivity analysis on the value of m and n the f svd method requires two parameters m and n to determine the size of the matrix when interpolating where m is the number of gauges involved in interpolation and is related to the number of rows of the matrix combining n and interpolation time j can determine the number of columns in the matrix to obtain the best interpolation result various values of these two parameters are selected for calculation and comparison table 2 lists the average spatial uniformity of all stations when m takes different values as the value of m increases the spatial uniformity increases first and then decreases when the value of m is 20 l reaches the maximum value which means the distribution of the surrounding stations is the most uniform and can well reflect the spatial information in all directions around the interpolation point in this case therefore the value of m in this study is assigned 20 for the determination of the value of n to ensure the efficiency of the calculation three typical long term events were selected and the accuracy of interpolation is calculated in different situations these three events are no 1 no 14 and no 19 respectively and their duration exceeds 50 h which are long lasting rainfall events here rsme is used as the evaluation indicator and the result is shown in fig 6 it is found that as the value of n becomes larger rsme decreases first and then increases reaching the minimum value that is the highest accuracy when n is 24 so n is assigned 24 in this study 4 2 models evaluation on all rainfall events the overall estimation results of the five models in 20 rainfall events are comprehensively evaluated using the four different indicators and shown in table 3 it can be seen from table 3 that the accuracy of the results of the five methods is within a reasonable range indicating that they can be well applied to rainfall estimation in the hanjiang river basin the evaluation results using four selected indicators are consistent with the accuracy of f svd being the highest f svd idw and f svd ok being the second and third highest idw being the fourth highest and ok being the lowest the result of idw is better than ok with a small gap which is similar to previous research hadi and tombul 2018 yang et al 2015 besides the difference in accuracy between the five methods is more obvious judged by mae and rsme since these two indicators are directly related to the amount of rainfall and heavy rainfall usually has a great impact on the values of them and perc and ks are less likely to be influenced by the magnitude of rainfall thus the difference is smaller wasko et al 2013 the cumulative rainfall of 20 events in the basin and the cross validation interpolation results of the five methods are shown in fig 7 it can be seen that rainfall is mainly concentrated in the southwest of the basin this is due to the high terrain in the southwest the warm and humid air flows along the windward slope of the mountain range plus the influence of the local climate forming a strong rainfall center in the southwest of the basin with rsme as the evaluation index the sites with large cumulative rainfall mainly distributed in the southwest have large interpolation errors as the value of rsme is directly affected by the amount of rainfall the sites with small interpolation errors are mainly distributed in the northern part of the basin since the rainfall in the north is very small or even zero comparing the distribution of gauges errors using the five methods methods that combined with f svd have more yellow points in the northern part and less blue points in the southern part of the basin than ok and idw indicating that the use of f svd will improve the accuracy no matter the rainfall is large or small the areal rainfall characters of 20 events calculated by the interpolation results are shown in table 4 among them the calculation results that are closest to the actual rainfall records are marked with a green background and the farthest ones are marked with orange it can be seen that the average of areal rainfall ranges from 0 5 mm to 3 mm the standard deviation reflects the fluctuation of rainfall at different times no 5 event has an average areal rainfall of 1 22 mm which is of medium scale but the standard deviation is the lowest indicating that the rainfall process is relatively smooth with few cases of sudden increase and decrease the average areal rainfall of the no 9 event is 1 86 mm and it only lasts for 16 h but the standard deviation reaches 2 01 indicating that rainfall mainly concentrated in several periods the characteristic values of the events calculated by the five interpolation methods are not much different from the measured values among the 20 selected rainfalls for both rainfall characteristic indicators at least 16 of the rainfall characteristic values calculated by f svd are the closest to the actual measurement and that calculated by ok are the farthest besides through combination with f svd both idw and ok perform better and generate more accurate rainfall characteristics than before 4 3 models evaluation on representative rainfall events to better evaluate the interpolation ability of the f svd model the magnitude of the event is sorted and three typical events of heavy moderate and small rain are selected for comparative analysis they are in turn the no 7 small rain event 7 may 8 may 2015 no 20 moderate rain event 17 jun 19 jun 2018 and no 18 heavy rain event 23 sep 27 sep 2017 the results are shown in fig 8 in the figure a is the cumulative observed rainfall at each site b c and d are the mae of f svd idw and ok respectively and e and f are the difference in accuracy between svd and the other two methods where the blue dots indicate the accuracy of f svd is higher and the red dot indicates that of f svd is lower it can be seen that for rainfall events of different magnitudes the interpolation error of gauges with heavy rainfall is large besides the blue dots in figures e and f are more than the red points indicating the number of gauges whose interpolation accuracy using f svd is higher than using idw and ok is more but for the gauges with small cumulative rainfall 5 mm the accuracy is not improved the difference in accuracy between f svd and the other two methods is small during small rain but as the rainfall magnitude becomes larger the difference gradually increases in the heavy rain event the historical rainfall at most gauges is not zero thus the f svd method can effectively extract the spatial and temporal feature information from the historical rainfall for interpolation resulting in a noticeable improvement in accuracy the results of three representative rainfall events using all five interpolation methods evaluated by different indicators are listed in table 5 it can be seen that the accuracy of idw is higher than that of ok and through combination with f svd the accuracy of idw and ok are greatly improved the accuracy of f svd is highest which shows good estimation ability than traditional interpolators as the magnitude of rainfall increases the difference in accuracy between the five methods becomes larger the interpolation error of no 7 small rain event is the lowest judging from mae rsme and perc besides the evaluation result of no 20 moderate rain event using all indicators is worser than no 18 heavy rain event as can be seen from table 1 n0 20 event lasts for 106 h and no 18 event only lasts for 44 h for n0 20 event the rainfall is very scattered and is extremely low in many moments and the trend and regularity are not obvious contributing to a larger interpolation error 4 4 estimation error distribution of interpolation methods the average gauge error in different selected events using five interpolation methods is shown in fig 9 it is found that except for few events most of them produce the largest error using ok and the smallest error using f svd through combination with f svd the accuracy of both idw and ok is improved the evaluation results by rsme and mae are similar since they are both greatly affected by the rainfall amount the results of perc and ks are not the same some rainfall events such as no 11 have low rsme mae and high ks and perc as the areal total rainfall in the no 11 event is only 15 9 mm the accumulated rainfall at each gauge is relatively low so the rsme and mae values are not too large the calculation of perc concerns more with the relative error than the absolute error for gauges with little rainfall although the interpolated rainfall is also very low the relative error may be large causing perc to become large to evaluate the uncertainty of the five methods two indicators mae and rsme are selected and a box plot of the errors of all stations in 20 events is shown in fig 10 it can be seen from the figure that the variation trends of mae and rsme are consistent the median values of ok and idw are higher than other methods and the confidence intervals of them are wider ok has a rather large uncertainty as its confidence interval varies in different events for most rainfall events the median value and the width of the confidence interval of f svd ok and f svd idw are very close and sometimes the median value of f svd idw is slightly lower than that of f svd ok through combination the width of the confidence interval of them is greatly shorter than the previous two methods and the median is also lower the error distribution of f svd is the best as it directly uses observed records for estimation it shows that the f svd method not only improves the interpolation accuracy but also has higher stability among the 20 rainfall events interpolated by f svd the relatively large errors mainly occur in no 2 no 4 no 9 and no 13 events and their durations are 30 32 16 and 38 h respectively thus the f svd method may perform less well in short duration rainfall events 5 conclusions in this paper a new method based on f svd to improve rainfall estimation is proposed unlike traditional interpolators that only focus on the spatial relationships of gauges the proposed method incorporates rainfall information at the current and historical moments into the estimation process which results in a more accurate result through combination with traditional interpolators it can be applied to interpolate rainfall at unknown points and obtain continuous spatial distribution of rainfall thus it is a practical method to process rainfall data for spatial pattern analysis and prepare input data for distributed hydrological models twenty rainfall events are selected from the hourly rainfall data of the rainfall gauges in the hanjiang basin to verify this method by cross validation using four indicators and idw and kriging are included as benchmarks for accuracy comparison the study concludes that 1 according to the interpolation results of 20 rainfalls f svd has the highest accuracy and ok has the lowest accuracy through combination of ok and idw with f svd the accuracy of ok and idw can be greatly improved due to the different emphasis of the indicators the gap between five methods in rsme and mae is larger than that in perc and ks 2 in cross validation the rainfall magnitude has an effect on the interpolation accuracy for different rainfall events the larger the rainfall magnitude the more gauges of which the accuracy can be improved by the f svd method 3 according to the error distribution of all stations in each rainfall event f svd not only improves the interpolation accuracy but also reduces the uncertainty of the error so f svd has better stability however there are also some limitations in this study f svd is a latent factor model and its algorithmic meaning is to build relationships between time and space through latent factors which cannot correspond to physical concepts in reality thus it has a poor interpretability besides only two widely used interpolation methods ok and idw are used for comparison and one basin hanjiang basin is considered in this study the conclusions may not be generalized therefore more basins with different distribution of gauges and more methods for comparison will be helpful to validate the proposed interpolation method declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is financially supported by the national key research and development program 2018yfc0407904 2017yfa0603702 
25761,collecting and managing high temporal resolution residential water use data is challenging due to cost and technical requirements associated with the volume and velocity of data collected we developed an open source modular generalized architecture called cyberinfrastructure for intelligent water supply ciws to automate the process from data collection to analysis and presentation of high temporal residential water use data a prototype implementation was built using existing open source technologies including smart meters databases and services two case studies were selected to test functionalities of ciws including push and pull data models within single family and multi unit residential contexts respectively ciws was tested for scalability and performance within our design constraints and proved to be effective within both case studies all ciws elements and the case study data described are freely available for re use keywords residential water use data management smart metering cyberinfrastructure information and communication technology 1 introduction achieving higher efficiency in urban water management and planning requires understanding of how water is used at the household level daily patterns in consumption potential for water savings and distribution of water use across end uses are essential inputs to water demand estimation leak identification design of programs to manage water demand and water planning to ensure adequate supply giurco et al 2008 willis et al 2011 metering water use for billing purposes is a common practice in the united states where meters are typically read monthly or quarterly our ability to characterize water demand is limited by the temporal resolution of the data collected higher resolution data can increase the accuracy of peak demand estimation and reduce leak volumes that can go undetected sub minute resolution data is required to record and quantify end uses of water that have short duration cominola et al 2018 nguyen et al 2015 however obtaining this higher temporal resolution data at a scale larger than a few houses presents several challenges in terms of data collection storage management and processing cominola et al 2018 and doing it over an extended period of time can be unpractical cardell oliver 2013 collecting a month of 10 s resolution data for a single meter which is common in end uses of water studies deoreo et al 2011 2016 mayer et al 1999 2004 produces more than 250 000 observations doing so at a water utility or municipality scale which may have thousands of metered residential connections presents obvious challenges associated with the volume of data that would be produced many utilities lack a dedicated information technology or data management staff which means that new database management software deployment and data analysis tasks can be prohibitive in these cases and in the absence of sufficient cyberinfrastructure for automating data management tasks high resolution data could be more of a roadblock for a water provider than a benefit however with adequate data collection and management tools utilities may be able to realize more of the potential benefits associated with high temporal resolution data this includes quantifying water use behavior to better enable planning that ensures adequate supply the promotion of water conservation behavior among users liu et al 2015 improving customer service quality for utilities beal and flynn 2015 tipping the cost benefit balance in the smart metering adoption case which remains undefined cominola et al 2018 and enabling the proliferation of scientific work in this field the term cyberinfrastructure integrates hardware and software tools as well as data networks nsf 2007 cyberinfrastructure can help solve data management challenges and enable more widespread collection of higher temporal resolution water use data for utilities and researchers in a broader context cyberinfrastructure is improving the communication of results from hydrological models souffront alcantara et al 2017 helping monitor watershed health parameters szwilski et al 2018 assisting in the automation of comparing climate model results sun et al 2020 and it is now ubiquitous in multiple scientific domains hachmann et al 2018 shams et al 2018 wegrzyn et al 2020 smart meters have potential to solve one of the challenges in the pathway to an advanced water cyberinfrastructure high resolution measurement of water use the term smart meter can be ambiguous boyle et al 2013 within this article it is used to denote devices capable of recording water use with high resolution i e sub minute frequency that can be integrated in automated systems for data management nearly a decade ago it was anticipated that use of smart meters would grow over time boyle et al 2013 and they are in fact becoming more widely available and adopted with this emergence of smart meters there has been an increase in the number of scientific publications using the high resolution data they produce for water demand analysis cominola et al 2015 provide a comprehensive review however despite the increase in the number of publications using smart metering data to quantify end uses of water and water use behavior the data management procedures or tools used in these studies are not well described and most of the datasets used are not openly available di mauro et al 2020 in most of these studies the focus has been on the tools and algorithms used for identifying water end uses and user behavior other components of the data management process are not described available cyberinfrastructure for collecting managing and analyzing this type of data remains scarce and of proprietary nature with little available literature describing tools and procedures for data collection management and analysis meter manufacturers tend to have their own software systems designed for their metering technology which complicates synthesis or integration of data from multiple systems and may help explain why research in this field has been conducted in a limited number of countries using a limited number of datasets many of these studies have used the same data logging device for data collection and the same software tool for end use analysis beal and stewart 2011 deoreo et al 2011 2016 mayer et al 1999 2004 other studies have reused the same dataset to conduct different analyses for example beal at al 2013 present differences between perceived and actual water consumption willis et al 2013 studied the impact of socio demographic and efficient fixtures on water use and beal and stewart 2011 presented end uses of water characteristics all using the same dataset collected in southeast queensland australia the datalogging devices used in most high resolution data collection studies lack communication capabilities which limits the potential for automated integration with downstream cyberinfrastructure e g telemetry storage management and analysis applications more recently there has been increasing discussion around smart cities smart grids smart water networks and other related terms despite there not being a wide agreement about their definition what is meant by smart or the extension of their applications ardito et al 2013 hollands 2008 wissner 2011 it is generally agreed that smart cities make use of information and communication technologies ict in an attempt to assist cities in optimizing the use of their assets neirotti et al 2014 water being one of the most important connectedness of data collection and its application is important in this context advanced metering infrastructure ami and ict systems are vital for the successful deployment of a smart grid yan et al 2013 in the energy sector smart grids use smart technologies for metering communication and automation and make use of digital information to improve reliability u s congress 2007 the internet of things iot has also been described as a potential enabler of smart grids in the water sector alghamdi and shetty 2016 robles et al 2014 zanella et al 2014 and more recently smart solutions that use iot principles have been proposed amaxilatis et al 2020 stiri et al 2019 liu and nielsen 2016 discussed existing technologies to develop an ict system or cyberinfrastructure to enable smart meter analytics for the energy sector acknowledging the difficulties in processing and managing the large volumes of data generated similar systems have been proposed and discussed for water use analytics boyle et al 2013 li et al 2020 makropoulos 2017 moy de vitry et al 2019 but few implementations have been published due to the cost and complexity of these applications alvisi et al 2019 amaxilatis et al 2020 anda et al 2013 in one notable example chen et al 2011 conducted analysis using data collected on a smart water service architecture deployed for billing purposes on the city of dubuque ia this system collects data every 15 min providing more advanced analysis to water consumers and providers erickson et al 2012 while multiple high level designs of a smart water network have been described e g hauser et al 2016 li et al 2020 ye et al 2016 implementations are scarce most of the smart water systems designs we reviewed lacked a full demonstration or prototype implementation in some cases important elements such as performance metrics and implementation guidance were not fully described li et al 2020 when demonstrations were presented the focus was primarily on the results of the specific case study i e the lessons learned about water use and or behavior and not on the design and implementation of the tools used to complete the tasks the limited availability of data and tools for the water sector constitutes a significant barrier for the development of research and prevents the advancement and implementation of smarter water grids at a large scale mutchek and williams 2014 the closed source nature of existing data collection hardware and data management software creates accessibility and interoperability issues that prevent the progress of smart water grids while curtailing the adoption of open architectures hauser and roedler 2015 robles et al 2014 the development of open source cyberinfrastructure for managing high resolution data can lay the foundations for the development of newer and better tools for water utilities as well as standards for operations that result in increased interoperability all of these actions could pave the road for more water demand research and ultimately advance technologies for the development of smart water grids thus in order to achieve the full potential of smart meters cyberinfrastructure is needed to support utilization of the high resolution data they produce horsburgh et al 2019 mason et al 2014 developing effective cyberinfrastructure that can support both operational data collection and management e g for billing reporting and day to day management purposes and exploration of data for research aimed at better understanding water use behavior is expensive and challenging stocks et al 2019 indeed architectural designs and data structures for cyberinfrastructure supporting residential water use data must meet the needs of multiple users i e water providers water consumers researchers without disrupting a utility s necessary business functions the research described here focused on the following research questions to advance the cyberinfrastructure and availability of software tools for collecting managing and analyzing high resolution smart metering data a what is the general architecture for a cyberinfrastructure to support collection and management of high temporal resolution smart metering data and b how can that architecture be implemented to meet the needs of multiple potential users e g water utilities water consumers researchers in this paper we present a generalized architectural design for a cyberinfrastructure for intelligent water supply ciws and a prototype implementation of each of the components within the architecture in support of multiple data collection management and analysis case studies the prototypes we developed demonstrate tools that are not currently available for researchers or utility managers and include a a data collection layer consisting of datalogging devices with data transmission capabilities which are modifications from our previous work horsburgh et al 2017 bastidas pacheco et al 2020 b a data management and archival layer that receives processes and stores data and c a data analytics layer that enables calculation of common water use metrics e g average hourly water use instantaneous peak and end uses of water disaggregation and classification components within these layers demonstrate the entire workflow consisting of data collection communication storage management and archival and visualization and analysis while ciws was designed and implemented for research purposes including appropriate mechanisms for protecting the identities of research participants where necessary it facilitates implementation of high temporal residential water use analysis which is of interest to not only researchers in the field but also utility companies and water consumers and can provide information currently not available to them the data collected and managed using ciws is relevant for assessment and management of both water demand and for planning to ensure adequate water supply we first describe the requirements for the system along with the overall architecture we designed to meet these requirements section 2 we then describe a set of case studies in which this overall architecture was prototyped and implemented using both existing and new open source hardware and software components section 3 finally we close with discussion and conclusions section 4 2 methods 2 1 ciws design and overall software architecture our goal in developing ciws was to create a generalized modular architecture that can be used to automate the process from collection to analysis and visualization of high temporal resolution water use data in our case study applications of ciws we combined existing and developed new open source hardware devices and software tools to demonstrate an integrated solution for high resolution residential water use data collection management and analysis the ciws architecture and our prototype implementation were designed to address the following requirements while we present our prototype implementations in this paper there may be multiple implementations of the generalized architecture that meet these requirements a an open architecture that could be implemented using a variety of technologies b open source software development to facilitate its deployment and use by other users reduce costs and provide a platform for future improvement by others while advancing financial feasibility of larger scale implementations c a modular design so each component of ciws can be used or advanced independently d accept input data from different meters and measurement devices sensors to address heterogeneity in urban water meter technology e capacity to manage push and pull data retrieval from the metering devices depending on available communication technologies and storing of data in a centralized server f scalable to accommodate a large data volume while remaining responsive to queries for subsets of time series data of varying sizes g support production of analysis and insights that meet the needs of different audiences in our review of the literature we found that existing designs of smart components or cyberinfrastructure for managing water systems are not fully standardized however most systems described or implemented to date are composed of multiple layers working in connection to achieve the overall goal li et al 2020 we found that the number name and function of these layers was different in each design however we observed some similarities in practice the number of layers included in an architectural design comes down to tradeoffs between the benefits of modularity and separation of concerns that can be achieved versus the complexity and potential fragility introduced with a larger number of layers separate layers can be autonomous such that changes to one layer do not have to affect the other layers however a greater number of layers typically involves more components that can fail our overall architectural design for ciws adopts this multi layer paradigm fig 1 and is composed of three main layers the first layer is the data collection layer and includes the physical instruments and sensors used to monitor water use it has also been called the sensing layer ye et al 2016 the physical layer hauser et al 2016 or the instrument layer li et al 2020 the second layer is the data management and archival layer which handles data communication parsing and archival this layer has also been referred to as the network or function layer hauser et al 2016 li et al 2020 ye et al 2016 the final layer is the data analytics layer which handles all the steps between queries to retrieve data from the archival component to final visualizations analyses and presentations produced for utilities water consumers researchers etc i e the consumers of the data this layer has also been referred to as the application or the data fusion and analysis layer hauser et al 2016 li et al 2020 ye et al 2016 some of the other systems reviewed include elements for real time monitoring and control of observed variables and processes within the system resulting in architectural designs with a larger number of layers since these elements were not needed in our case study use cases a three layer model met all of the requirements listed above a system with more layers may become more fragile therefore our design includes the minimum needed to meet the design considerations the architecture for ciws and our prototype implementations were developed with a research focus e g collecting storing and managing high resolution water use data to enable advanced study of residential water use behavior this type of research may be carried out by utilities universities or other agencies involved in research related to or management of urban water supply and demand the typical deployment size in this type of work has been around 50 houses per city however some studies have analyzed up to 762 sites deoreo et al 2016 in the latter case the data was not collected simultaneously at all sites our aim was to develop a system that can handle at minimum the number of simultaneous data collection sites within the range of deployments observed in the past 40 60 houses in the following sections we describe in more detail the high level design for each of the architectural layers their key components and their basic functionality 2 1 1 data collection layer data collection refers to the actual measurement of the variable or variables of interest in this case high temporal resolution water use here we define high temporal resolution data as data collected at a sub minute resolution typical investigations of water use behavior such as separating and quantifying end uses of water within a home require data to be recorded at 10 s or even finer resolution over data collection periods of weeks to months with few exceptions high temporal resolution data cannot be collected using existing commercially available smart meters without adding additional hardware or software components cominola et al 2018 which can be expensive horsburgh et al 2017 water metering technology typically consists of a physical meter that uses one of several measurement techniques paired with an analog or digital register on which a totalized volume of water use is recorded some registers including those of commercially available smart meters are capable of storing volume readings within internal memory however this is usually constrained to relatively short periods of time e g weeks at recording intervals longer than 1 min other registers report only the most recent volume reading and are designed for periodic e g monthly or quarterly readings either manually or automatically via radio these practical limitations are driven by power local data storage and network bandwidth limitations of existing metering technology some water use studies have added flow metering sensors directly on the water pipe leading to each appliance in a residential house kofinas et al 2018 di mauro et al 2019 opting for this approach allows direct measurements of water use from each fixture and by placing the measuring element inside the property power and communications can be readily available however this approach is invasive and requires modifications to the plumbing in each home where data is collected which can increase costs and limit the applicability of this methodology at a medium or large scale therefore we opted to focus our efforts on datalogging devices that can be coupled with the existing water meter available at the property datalogging devices designed to couple with existing meters are available bastidas pacheco et al 2020 f s brainard company 2020 these dataloggers essentially perform the same function as the meter s register but have the capability of recording much more frequent observations over longer periods of time to be fully integrated in a data management system like ciws the datalogging devices must also have communication capabilities ciws was designed to handle both push and pull data communication making it adaptable for multiple scenarios the term push is used to denote systems where the data is sent by each datalogger client to a centralized server while pull refers to systems where a centralized server connects to each datalogger and requests data given the modular design of ciws it is possible to integrate dataloggers that lack communication capabilities such as those used in most residential studies in the past under this scenario a user can take advantage of the data management and archival and data analytics layers of ciws while using data files manually downloaded from the datalogging devices in the field 2 1 2 data management and archival layer the data management and archival layer is responsible for the work required to process the data logged by the devices the key component addressed in this layer relates to developing and using software elements to automate repetitive data management processes and enable an easier transition between large volumes of data collection and useful information generation this layer is composed of multiple working elements fig 1 for push based data transmission a listener service is required to receive the data sent by the dataloggers in pull based data transmission a request service is used to achieve the same task once the data is received it must be verified parsed and transferred to a database component the database component accepts and stores data for downstream analysis and decision making real time monitoring of water use is typically not of interest in most research scenarios where most data analysis happens after the data have been collected additionally given the frequency with which observations are recorded e g on the order of seconds it is not practical to push or pull data every time a new observation becomes available based on this ciws was designed to collect and send files containing many observations rather than sending observations individually this approach minimizes the communication load on the system because the data transfer process does not occur constantly and it can be scheduled to meet specific needs the request service for pull based data transmission must execute the following tasks a connect to a datalogging device b check for new data files c request and transfer new files d read and parse the files and e upload the data into the database remotely accessing devices can be achieved using a variety of communication protocols like secure shell ssh which is a widely used method for similar tasks due to its simplicity speed and security in this model the datalogging devices need to be powered on and connected to the network at the time the connection is established additionally a key requirement is that each datalogging device must be located addressed and accessed directly which also provides an opportunity for remote functionalities such as software updates troubleshooting changing data collection settings and others the listener service which manages the data transferring process under the push model must complete the following tasks a accept and validate the data sent from each datalogging device deployed b process incoming files including parsing the information they contain and c saving the data received into the database under this approach the communication elements of the datalogger only need to be powered up and functioning for the time it takes to send the desired information to the listener service which can contribute to lower power requirements additionally there is no requirement for data logging devices to be uniquely addressed on a network as they can identify themselves within the content of the message they push to the listener service multiple technologies that can potentially meet the data storage and accessibility design considerations i e the database requirement are available the database must be able to manage large volumes of data and provide a platform for generating analytics of such data the data managed by the system consist mainly of time series of flow observations which are constantly being collected and written into the database thus the databasing technology selected must provide a easy and fast querying between dates and times to enable manipulation of the data b high performance for read and write operations as the database is continuously being updated with new data and potentially accessed by multiple users and c scalability as the volume of data to be stored in the database increases quickly as the monitoring network and time period over which data are collected grow the database schema used to organize the data for ciws was designed to maximize query efficiency while maintaining the ability to protect the privacy of water consumers by storing personally identifiable information outside of the database common queries to be conducted in projects where ciws can be used include selecting all or part time constrained of the full resolution or time aggregated data for a single or multiple sites 2 1 3 data analytics layer the data analytics layer supports generalized interactions between data users and the database for the purposes of visualization and analysis of the data the necessary functions executed in this layer include a user authentication to access existing data b querying data from the data base c data manipulation and analysis and d generation of reports and visualizations of interest for different target audiences for the purposes of this research three main target audiences were identified as users of information produced by the data analytics layer water consumers utility managers and researchers while these categories of users are not necessarily exhaustive or mutually exclusive the information that would be useful to these different users and the methods used to interact with the data are not the same for instance an individual residential user would need to be able to access and interact with the data from their home in a practical and non technical way that does not require specialized software past studies have evaluated residential users preferences for water use feedback finding that information about their prior water consumption comparison of use with that of similar users and details about their consumption can increase user understanding erickson et al 2012 liu et al 2015 utility managers may want to access standardized plots or reports showing data from multiple users and researchers may need much more freedom to formulate their own custom queries to the database to subset aggregate or summarize data in useful ways this implies that the data analytics layer needs to support multiple mechanisms for accessing and interacting with the database authentication authorization and privacy for users with different privileges read or write data in a database to access online resources have been discussed for multiple applications christie et al 2020 heiland et al 2015 kim and lee 2017 high temporal resolution data products such as distribution and timing of end uses can raise privacy concerns among water consumers that must be considered when designing data presentation tools froehlich et al 2012 aggregation and summarization techniques can be used to present information for multiple water consumers while protecting privacy and authentication and authorization can be used to limit what data is available for different users ciws considers the use of anonymized datasets throughout the system by identifying water consumers with a unique identifier linkage with the personally identifiable information about each water consumer is stored separately and is only available to those who have appropriate privileges and are allowed match water consumers with their data 2 2 case study design and system testing in order to evaluate the overall architecture design we designed two case studies that demonstrate different aspects of the architecture presented in two distinct data collection environments the first case study demonstrates data collection at individual single family residential homes it uses an autonomous datalogger with communication capabilities to collect high resolution water use data and demonstrates push based transmission of the data to the data management and archival layer the second case study demonstrates data collection within multi unit residential structures on a university campus it uses dataloggers with dedicated power supplies and network registrations to demonstrate pull based transmission of the data to the data management and archival layer in the second case study we collected data for additional parameters needed to characterize the energy consumption related to hot water use the collection of data for these parameters provides an example of ciws flexibility both case studies share the same layers but we describe the different elements used by each case study we created a full prototype implementation of the design layers presented in fig 1 for each case study and deployed them in an operational environment these prototypes and deployments were created to demonstrate proof of concept for data collection and management components the shareability of components within the architecture regardless of the data transmission method and generalizability for our architectural design we tested the system developed for scalability by simulating an increased number of sites and larger volumes of data python 3 7 was chosen to develop all of the code and software associated with our case studies given that it is freely available and open source it is a high level programming language with a vast number of libraries available to complete an important number of functions required in our application and it could be used across all three layers of our architectural design using python also helped us meet the first three requirements described above as the code can be easily shared read and modified by other programmers and scientists and can be deployed in different operating systems which increases reuse possibilities 2 2 1 case study 1 description water use in single family residential homes is quantified to a large extent using analog positive displacement water meters the volume of water that has passed through the meter is usually the only variable recorded by this type of meter in most cases water meters are enclosed in underground pits of varying depth limiting power supply availability these meters are typically read monthly quarterly or at coarser resolutions by the utility for billing purposes either manually or via a roving radio that receives the most recent volume observation from each meter when the roving radio passes within range some more advanced networks include automated retrieval of the coarse resolution volume data but very few have the capability to record and transmit high resolution data given that the vast majority of residential water meters in use today share these constraints we chose this case study to demonstrate adding high resolution data collection and transmission capabilities to existing analog water meters 2 2 2 case study 2 description the living learning community center llc on utah state university s usu campus was selected as a second case study for deploying ciws within a set of multi unit residential buildings the llc is one of usu s newer student housing options and houses approximately 500 students distributed among six dormitory buildings labeled building a building f the objective of this implementation was to characterize water and water related energy use in five buildings b f the importance of the water energy nexus for optimizing conservation and sustainable management has been identified in the past hamiche et al 2016 kenway et al 2016 fang and chen 2017 however collecting water and energy consumption data combined at a sufficient temporal resolution to analyze their relation is uncommon and the methods for linking water and energy use are not well established this case study demonstrates a methodology for collecting water and water related energy data in a multi unit residential setting buildings b f host approximately 90 students each building a hosts administrative offices has a much lower student occupancy and was excluded from the study we chose a pull based model for this case study given the availability of dedicated power at each data collection site and the availability of usu s campus wi fi network to enable communications and data transmission three water meters are present in the water supply system for each of these buildings hot water supply cold water supply and hot water return to monitor water and water related energy use within each building two characteristics of each meter were measured flow and water temperature resulting in a total of six variables collected per building table 1 the hot water return is a feature of the llc s innovative hot water recirculation system hot water is continually circulated from three boilers to the llc buildings at a constant base flowrate of approximately 3 gallons per minute gpm or 11 4 liters per minute lpm increases from this base flowrate constitute hot water use unused hot water returns to the one of the three boilers for reheating and eventual recirculation cold water is supplied in a typical on demand basis 3 results and discussion 3 1 case study 1 push based data collection for single family residential homes we selected a single family residential property to test the ciws functionality under a push based data retrieval model we collected two weeks of data at this property between january 15 2021 and january 28 2021 for the implementation described all water use results presented are for this time period this home had five occupants three of ages between 10 and 25 and two between 40 and 60 during the data collection period it was built in 2006 has three bathrooms and a total parcel area of approximately 12 000 ft2 1114 8 m2 we chose push based data retrieval for this case study because it is enabled by heterogeneous networking i e any datalogger device capable of high resolution data collection and sending data over an available data network could be used without the need for each device to be uniquely addressable on a network additionally power requirements can be reduced given that data logging devices do not have to listen for connections and requests from a centralized server but rather wake to transmit data on a user configured schedule 3 1 1 data collection layer at the property selected a one inch 2 54 cm bottom load bl master meter with an analog register was being used by the water utility to record monthly water use transmit it to a roving receiver via a 3g radio and bill water usage we added high temporal resolution data collection and transmission capabilities without affecting the normal operation of the utility s meter by installing a ciws water meter node ciws wm node datalogger to measure water use at a 4 s temporal resolution on top of the existing meter the ciws wn node is an advanced modification of the ciws datalogger bastidas pacheco et al 2020 which is an open source arduino based datalogger that we designed to work with any magnetically driven water meter the ciws datalogger uses a magnetometer sensor to measure the magnetic field around magnetically driven residential water meters it counts peaks in the magnetic field associated with movement of the magnetically driven measurement element within the meter and registers peaks as pulses that represent a fixed volume of water passing through the meter these pulses are multiplied by a factor called the meter resolution 0 041619 gallons per pulse or 0 1575 liters per pulse for the case study meter which is specific to each meter type brand and size to obtain the volume of water that passed through the meter per unit of time meter pulse resolution values can be obtained from meter manufacturers or through a calibration procedure described by bastidas pacheco et al 2020 the ciws wm node we developed for this case study adds communication and computational capabilities to the ciws datalogger by coupling it with a raspberry pi model b or model b single board linux computer the components of the ciws datalogger control all of the datalogging functions whereas the raspberry pi computer can be powered on a user defined schedule to process and transmit data the raspberry pi runs a version of the linux operating system called raspberry pi os previously called raspbian although the raspberry pi is capable of interfacing with a number of different wireless communication options including wi fi radio frequency cellular 3g lte bluetooth and satellite we chose to use the raspberry pi s built in wi fi capabilities for this case study because the homeowner s wi fi network was easily accessible in broader application however any internet data connection compatible with a raspberry pi could be used the ciws wm node datalogger outputs a comma separated values csv file including a three line header with a unique identifier for the site at which the datalogger is installed a unique identifier for the datalogger and the meter resolution for the meter on which it is installed the datalogger records three variables during the logging process datetime record and pulses bastidas pacheco et al 2020 the ciws wm node datalogging device was configured to chunk the data files by day i e a new csv file is created for each day and send data files once per day to the data management and archival layer via an http post request this functionality was developed as a single python script data transfer py when the raspberry pi is powered on it can conduct any computation required and the data transfer py script is executed to send data files to the data management and archival layer for further processing after a file is successfully sent via http it is moved to a different folder in the datalogger s local storage for backup 3 1 2 data management and archival layer for our case studies the data management and archival layer components were deployed within a vmware esxi server environment hosted at utah state university on a single virtual machine vm running the ubuntu linux server version 18 04 bionic beaver operating system ubuntu is a free and open source linux distribution developed by canonical ltd it is well supported stable and offers reliable file security the vm was configured with a 64 bit architecture four 2 3 ghz processor cores eight gb of ram and 100 gb of hard disk space we refer to this vm as the data management and archival server we developed three main components to complete the tasks described for this layer the data posting service dps the data loading service dls and the operational database each of which is described in the sections that follow the dps and the dls were developed in a generalizable way to facilitate reuse and serve as the network listener shown in the center panel of fig 1 however some specific details were adapted to this implementation for example the data parsing works for the specific output format of the ciws datalogger the dps and the dls were deployed on the data management and archival server and then configured via settings stored in a user modifiable javascript object notation json file named configuration json that details the information needed for their operation for deployment the configuration file must be placed in the same folder with the dps and dls 3 1 2 1 data posting service dps the dps is a listener web service that receives and processes data files pushed to the data management and archival server from the ciws wm node dataloggers the dps works integrated with two common server technologies the web server software that processes http requests received by the server and a web server gateway interface wsgi that runs the dps application in response to the requests we chose nginx nginx 2021 which is a free open source http server to serve as the web server software because of its high performance stability simple configuration and low resource consumption the wsgi was implemented using gunicorn 2021 which is a python wsgi http server for unix like operating systems guidance for deploying the web server and wsgi software is available in the project s github repository the parameters included in the configuration files for the dps and the dls are described in table 2 the overall functioning of the dps is as follows dataloggers send an http post request to the server that contains a data file for our case study one day of high resolution water use data for that home these requests are received and handled by the nginx web server which passes them to the gunicorn wsgi gunicorn then invokes and executes the dps to authenticate the http post requests by using a token client token in table 2 verifying the file type csv and that the file does not already exist on the server before moving it to a local folder on the server source directory in table 2 for further processing by the dls the dps is composed of three pieces of code app py which lists the functions needed to read the application configuration file auth py that lists all the functions for file authentication and web service py which calls the previous two files and executes the tasks described fig 2 illustrates the processes described and lists the elements involved the dps was implemented using bottle hellkamp 2021 which is a wsgi micro web framework for python bottle is simple fast lightweight and works without additional dependencies making it ideal for running small applications like the dps bottle built in functionalities such as its simple url routing capabilities and the convenient access to file uploads were used to facilitate the development of the dps and avoid dealing with low level details of http requests handling and routing we implemented a very simple token based authentication for the http post requests in our prototype to avoid spam content being submitted to the dps more sophisticated and secure authentication and authorization processes could be integrated in the future if needed to provide greater security a log file keeps track of the requests received by the dps and actions executed the log file is located in a directory described in table 2 the log file records successful and unsuccessful e g a file that already exists is sent to the server multiple times a request that is rejected by not having appropriate authentication credentials posting attempts all events are logged in a single file named data poster log which is limited to 5 mb in size when a log file exceeds this size it is saved adding a sequential number at the end data poster 1 log initially and the current logging continues in the original log file 3 1 2 2 data loading service dls we developed the dls to read the files received from the dataloggers from the source directory on the server parse the unique site identifier information from the header of the csv file and insert the data into the database for archival and use by the data analytics layer the dls also verifies that the data received does not already exist in the database by checking the unique site identifier and datetime values of the data to avoid duplication of data in the database the dls uses the same configuration file as the dps described on table 2 the dls reads data files from a local source directory and moves them to a local target directory after successfully inserting the data into the operational database if an error occurs the files are moved to the quarantine directory a log file records all the activity executed by the dls including any error observed in the process such as invalid datetime stamps invalid site identifiers and attempts to load data that already exists in the database this log file is named data loader log and it is managed identically to the dps log file both are located in the same folder log directory in table 2 we chose this implementation for several reasons first it enables preservation archival of the original csv data files recorded by the dataloggers second the data are loaded into an operational database that is highly performant for querying and data retrieval in support of the data analytics layer third it enables all of the downstream components in the architecture to be used regardless of how the data files arrive on the server for example they can be automatically pushed to the server from the datalogger pulled from the datalogger by the server as in our second case study or manually copied to the server in the case where data transmission is not automated the dls was implemented in a single python script named loader py 3 1 2 3 operational database for the operational database component we chose to use an existing technology given the availability of mature and robust database software in our previous work related to investigating how to best manage large volumes of time series data we tested the performance of four commonly used open source database technologies including mongodb mysql postgresql and influxdb brewer 2020 based on our tests we chose to use influxdb influxdata 2021 due to its time series oriented data structure rapid query performance and favorable disk space requirements when compared to the other software technologies influxdb is a popular time series database designed specifically for time series data in applications that require handling high data write and query loads it provides a powerful structured query language sql like query language and has both open source distributions that can be installed and used for free e g as we did on our linux vm and cloud deployments that can be implemented with usage based pricing influxdb has been used in multiple iot and other applications where it has been tested for large datasets balis et al 2017 di martino et al 2019 rinaldi et al 2019 influxdb also offers extensive support for multiple programming languages including python and r which are commonly used for data science this made it straightforward for us to use python to insert data and to execute queries from the data analytics layer influxdb databases are organized around the concept of a measurement which can be thought of as a table that contains an indexed column named time containing the timestamp of each data point where each data point is a row in the table additional variables are stored in columns that can be tags or fields the main difference is that tags are indexed and are not required in a data structure whereas at least one field is required fields are not indexed the column names for tags and fields are defined as keys generally it is recommended that data values are stored as fields and metadata as tags to improve query performance in our design for storing data in influxdb the number of pulses recorded by the datalogger during each time interval is included as a field key pulses and the site identifier key siteid and the datalogger identifier key dataloggerid are included as tags table 3 the data for all sites are stored in a single measurement within the influx database raw data and quality controlled qc data are stored in separate measurements with the same structure qc data is a copy of the raw data that is created after verifying that the volume registered by the datalogger is within 5 of the volume registered by the meter estimated using subsequent readings of the meter s register conducted during installation during periodic site visits and at removal of the datalogger in some cases known bad data were trimmed from the beginning and end of a valid deployment where the volume recorded by the datalogger did not match the volume recorded by the meter s register the data were discarded and a new deployment was started during our case study deployments we did not observe any out of range anomalous or unreasonable pulse count values after this qc procedure in consequence additional qc modules were not implemented however additional qc procedures could be implemented in the future all queries and analysis are conducted using the qc data the database is the point of connection between the data analytics layer and the data management and archival layer and its design must meet requirements from both layers to write and read data typically database schemas are designed around the structure of the data to be stored and to facilitate the most common types of queries this is usually a tradeoff between making it easy to insert data into the database while still providing highly performant queries the simple database schema implemented in this case study table 3 mirrors the structure of the data files generated by the dataloggers making it straightforward to insert data but is also optimized to support the following queries 1 selecting all of the data for a particular siteid 2 selecting all of the data for a particular dataloggerid e g to track the performance of a datalogger which may be deployed at multiple sites at different times and identify correct any systematic errors and 3 querying data for a specific time frame e g between a beginning and ending date combining queries based on these three elements provides most of the functionality intended for ciws and met all of the needs of our case study additional queries intended to allow comparison of data across multiple sites may also be of interest our design separates the time series data which are stored anonymously in the influxdb database from household information which is stored in a separate csv file named sites csv the data stored in influxdb do not contain any identifiable information which removes privacy concerns from the time series data the separate sites csv file may include sensitive personally identifiable information e g names addresses etc along with any other descriptive characteristics the version of the sites csv file for this study published in hydroshare has been anonymized data managers may wish to maintain multiple versions of the sites csv file e g one with all personally identifiable information about data collection sites and one that has been anonymized and could be released to a broader set of users while this approach adds an additional step for certain types of queries e g selecting data for all houses within a certain geographic area or of a certain built age because the site information must be queried before the correct time series data can be retrieved it provides a mechanism for protecting personally identifiable information and more flexibility for managing metadata about the sites removing or adding tags to existing measurements is significantly restricted in influxdb in consequence anonymizing the data stored in influxdb for publication is not needed as the data stored is already anonymous queries against the time series data can always be executed using a siteid or set of siteids obtained via a prior query to the sites csv file it is also possible but currently not implemented to add all site metadata as tags in the influxdb measurement to eliminate this intermediate query step if that is more convenient in a specific application researchers and utility managers can access the data within the influxdb database with a non administrator user account influxdb allows for the creation of multiple non administrator users and at least one administrator user the administrator manages authorization for each non administrator user non administrator users can be restricted to write read or both the free version of influxdb does not allow fine grained authorization which would be needed to restrict users to view only part of the data in a measurement however we did not see this as a significant drawback as high level users like researchers and or utility managers would likely need to have unrestricted access to all of the data in an influxdb database furthermore it is unlikely that the full resolution data would be provided to water consumers rather a more likely scenario would be for a software application with a graphical user interface to be developed for presenting water consumers with feedback about their consumption authentication and authorization of users could be handled separately by the software application in future deployments erickson et al 2012 provide an example of an online water portal and discuss the privacy and user authorization concerns that impact the design of similar tools homeowners are typically presented with summary statistics and visualizations calculated for their property and may be provided with a summary level comparison with other properties however they generally would not have access to view raw data for their own or other properties 3 1 3 data analytics layer to illustrate the type of capabilities supported by the data analytics layer we developed python tools that provide an example of the main aspects involved in this process connection to the database user authentication and data retrieval via common queries once the data has been retrieved into a python environment it can be integrated with existing and more advanced data analysis and visualization tools while it is beyond the scope of this paper to demonstrate all of the possible ways in which data can be retrieved from the database component and used within analytical applications the tools we developed demonstrate the general patterns required for developing such tools and serve as a foundation on which others could be developed influxdb client programming libraries are available for several popular programming languages including python go c java php ruby scala javascript and r which simplifies software development using influxdb and facilitates desktop mobile and web application development using the python client library for influxdb influxdb 2020 we first developed a set of functions for interacting with the influxdb database these functions were implemented within a single python script called da functions py this script connects to the database using a set of configuration parameters that are included in a json file named configuration json which is similar to the one used by the dps and dls applications parameters in the json file include host port username password and database as defined in table 2 the functions we developed in da functions py table 4 use the existing capabilities of the influxdb python client library along with specific parameters provided by the user e g siteid time dataloggerid as defined in table 3 to provide a simple application programming interface api for querying data from the database we anticipate that these functions will meet many of the most common data requirements for most researchers and utilities the functions generate a pandas dataframe mckinney 2010 with the resulting data if a single siteid is provided and a python list of pandas dataframes when multiple siteids are provided if a start date or end date are not included the function will download the entire record available if only a start date is provided the function will return everything from that date to the end of the record in the opposite case it will retrieve data from the beginning of the record to the specified ending data if measurement is not provided the functions will query from the quality controlled data qcdata raw data can be downloaded by specifying measurement rawdata for time aggregated data the function parameter can include any influx supported aggregation function e g mean median max min sum the time resolution of the aggregated data supports any influxdb duration type e g 1m for 1 min data 1h for hourly data 1d for daily data 1w for weekly data all the arguments in both functions are python keyword arguments they must be preceded by their identifier or name when executing the functions i e get data site 1 to return all the quality controlled data for siteid 1 we then developed a python jupyter notebook called data analytics ipynb that loads the functions listed and implements a basic workflow to produce metrics and analysis from the data collected jupyter notebooks kluyver et al 2016 allow creation and sharing of documents that contain live code equations visualizations and narrative text which makes them ideal for prototyping visualizations and analyses for the data analytics layer the notebook we developed imports data using the defined functions and then generates visualizations of common metrics of residential water use for presentation to water consumers for example fig 3 shows the average hourly water use blue solid line and the boxplots show the distribution of hourly water use for the period of data collection at the residential home we monitored we can notice two periods of higher water usage one during the morning and the other early in the afternoon corresponding with patterns typically observed in hourly residential water use data during this period no outdoor water use occurred therefore the figure represents indoor water use only the notebook then demonstrates calculation of summary water use information for the data collection period for example average daily water use was 170 2 gallons 644 3 l leading to a per capita average daily water use of 34 gallons 128 7 l the maximum daily water usage observed during the period was 292 7 gallons 1077 9 l the instantaneous peak was 10 gpm or 37 95 l per minute lpm and the maximum hourly usage registered was 74 1 gallons 280 5 l another analysis of special interest using high temporal resolution data is the identification of end uses of water we used an open source algorithm developed by attallah et al 2021 available via the hydroshare repository attallah and bastidas pacheco 2021 within the data analytics layer to separate raw data into events and classify the resulting events into categories of end uses of water the algorithm filters the data collected using a low pass filter making it easier identify single or concurrent events concurrent events are separated into single events and the final table containing only single events is classified by using a combination of clustering to identify atypical or outlier events and a fully supervised machine learning methodology to assign labels to the remaining events the machine learning model uses a random forest classifier liaw and wiener 2002 trained using a set of user labeled and manually labeled events to classify new events for individual residential homes attallah et al 2021 we used the trained machine learning model to label the events generated during the data collection period at the residential home we monitored while a potentially large number of analytics visualization and information can be generated from the labeled events the jupyter notebook we developed presents a small subset of them fig 4 as an example of products that can be generated from the raw data at the observed home toilet events account for 36 1 of the total indoor volume used showers 26 3 clothes washer 13 faucets 12 4 and bathtub events 11 1 unclassified events defined as events lasting 4 s or less and consisting of a single pulse recorded by the meter approximately 5 ounces or 0 15 l of water account for approximately 1 of total use unclassified events include very short water use events e g ice making refrigerators short faucet events and leaks fig 4 shows the distribution of the volume a flow rate b and duration c for each category of indoor water use unclassified events were excluded from fig 4 faucet events had a median flow rate of approximately 0 8 gpm 3 lpm water efficient bathroom faucets as defined by the united states u s environmental and protection agency epa in their water sense program epa 2020 operate between 0 8 gpm at a pressure of 20 pounds per square inches psi or 137 9 kilopascals kpa and 1 5 gpm 5 7 lpm at 60 psi 413 7 kpa compared to this epa standard the flowrates we observed from the faucets at the study property are efficient a similar conclusion can be reached by comparing the median flow rate of shower heads at the study property approximately 1 8 gpm or 6 8 lpm with epa water sense standards limiting the maximum flow rate to 2 0 gpm or 7 6 lpm in previous studies from multiple u s cities shower durations averaged 7 8 min deoreo et al 2016 the average shower duration observed at the study property was approximately 8 min with a median value of 6 3 approximately 25 of the shower durations were longer than 9 5 min fig 4 the average gallons per flush gpf for toilets at the study property was 2 78 10 5 l significantly higher than the 1 28 4 8 l recommended by the epa epa 2020 indicating there is potential for reducing water usage by retrofitting the property with water efficient toilets there is relatively little variability in the durations of toilet and clothes washer events as observed in fig 4 c for these events the characteristics are dependent on the type brand and setting used shower events reflect the largest variability as expected due to personal preferences of the different occupants of the property code to reproduce the results in this section and the raw data collected are publicly available in hydroshare bastidas pacheco et al 2021 the workflow that can be used to reproduce the results presented in this section consists of the following a influxdb is installed locally with instructions provided b the database described in table 3 is created c the database is loaded with the raw data provided using influxdb loading ipynb and then d data analytics ipynb is executed on the database producing all the results described 3 2 case study 2 pull based data collection within multi unit residential buildings for results of this case study we present only the data collection and management infrastructure required the specifics details about estimating and water related energy use estimates using the data collected are reported elsewhere by brewer 2020 the functionality of the data analytics layer is independent of the selected data communication method push or pull because the data analytics layer interacts only with the operational database given that the data collected by both case studies and the resulting database are similar the considerations for implementing the data analytics layer are equivalent to those of the first case study presented e g ability to support queries data privacy etc and the technology of the implementation would follow the same process to avoid duplication of results we have chosen not to present an implementation of the data analytics layer with this case study however similar functionalities related to this case study are discussed in our previous work brewer 2020 and available in an online data resource brewer and horsburgh 2020 3 2 1 data collection layer an enhanced version of the water meter datalogger presented by horsburgh et al 2017 was used to collect data for the variables listed in table 1 this device was named the ciws ewm logger where ewm denotes electronic water meter for the electronic output signal of the meter types it works with the ciws ewm logger was designed to be installed on commercial water meters of the types typically used in multi unit residential buildings and where a dedicated power source is readily available at the meter s location the ciws ewm logger also uses a raspberry pi 3 model b or model b linux computer running raspberry pi os the raspberry pi in this device controls the functioning of the datalogger and has integrated ethernet and wi fi capabilities for connecting to a network while operating given the location of the water meters in utility closets with no wired ethernet ports we chose to use wi fi to enable communications with the dataloggers connecting a device to usu s wi fi network requires registration of the device s hardware address after which each device is assigned a unique host name that is routable on usu s network thus each datalogger could be located and connected to within the network which allowed for remote work interactions with the datalogger for example the firmware of the loggers could be updated their functioning could be evaluated in real time and data could be pulled from them via ssh at any time while this specific configuration relies on characteristics of usu s wi fi network we anticipate that wi fi networks like usu s would be available in many application contexts the functionality described here would function identically for wired ethernet connections the ciws ewm logger was specifically modified to read the output of each of the meters available on the llc buildings along with water temperature values from three separate sensors the ciws ewm loggers we deployed can be used with any water meter or sensor that has a 4 20 ma current loop output analog voltage output digital output readable by the raspberry pi via its general purpose input output gpio ports or pulsed output the master meter octave meters provide output through a 4 20 ma current loop module where the output current is directly proportional to the flow rate through the meter the necessary transformations from current to voltage and then to flow rate were performed by the ciws ewm logger brewer 2020 and a time series of water flow in gallons per minute at a user configurable temporal resolution was generated the blmj meter outputs a pulsed signal voltage where every pulse represents a gallon of water that has passed through the meter in this case the count of pulses which equals the number of gallons was registered by the ciws ewm logger at the same user configured temporal resolution the ds18b20 digital thermometers provided digital 9 bit to 12 bit celsius temperature measurements to an accuracy of 0 5 c and were wired directly to the raspberry pi with a single wire for each sensor and do not require an external power supply the ciws ewm logger in each building logged data to a csv file that was saved in a local directory within the raspberry pi s file system for this deployment data was collected at a 1 s time interval and includes the following columns time datetime of the measurement using the yyyy mm dd hh mm ss format buildingid b c d e or f coldinflowrate coldintemp hotinflowrate hotintemp hotoutflowrate and hotouttemp with units indicated in table 1 in the quality controlled data the hot water return flow was transformed to gallons per minute for uniformity 3 2 2 data management and archival layer to support pull based data retrieval we developed an application called the data transfer manager dtm to serve as the request service shown in fig 1 it was implemented as a single python script named transfer manager py and follows the same convention used by the dps and the dls reading configuration data from a json file as in the first case study the dtm and the operational database were deployed on a vm with similar characteristics to the one described in section 3 1 2 we used influxdb as the operational database for this case study as well given the similarity in the type of data and requirements among both case studies and to show generalizability the dtm manages all data communications under the pull based model operation of the dtm was scheduled using linux s native cron functionality which allows the user to specify how often the dtm program is executed upon being triggered by the scheduled cron job the dtm first reads the configuration file described in table 5 and then proceeds through a list of defined tasks to manage transfer of data from each remote data collection site to the data management and archival layer 1 connect to each datalogger listed in the configuration file using paramiko a python library that enables ssh connections for safely accessing network services over unsecured networks forcier 2021 2 parse the datalogger s linux file system for new datalog files and download them to the server with secure file transfer protocol sftp an extension of ssh that offers secure file transfer capabilities over any reliable data stream tasks 1 and 2 in this list are executed by a function named connect in the transfer manager py python script 3 upload new data into the influxdb database this task is completed by the write to db function in the transfer manager py python script an additional function in the dtm named send error was developed to inform data managers about errors in the data transfer process errors are sent via slack a cloud based instant messaging service slack technologies 2021 messages are formulated as a json payload that is sent to a unique url provided by slack as a webhook information detailing which datalogger file caused the error is included in the message fig 5 describes the overall functionality of the dtm indicating the key tasks mentioned for this case study data transferring and parsing are executed by a single element transfer manager py which requires fewer moving parts and minimizes the amount of time between the data being retrieved from the remote dataloggers and having them show up in the operational influxdb database this is a slightly different approach than the one presented for case study 1 which allows more flexibility in the system the dtm can work concurrently on a user defined number of datalogger devices at the same time connections in table 5 the optimal number of threads is dependent on the number of cpu cores of the server for our testing we set the number of threads to 6 matching the number of dataloggers in the llc buildings as in the first case study the raw data and quality controlled data were stored in the same influxdb database in different measurements brewer 2020 describes the quality control procedures for the data collected in this case study the database schema used for this case study is similar in structure to that of the first case study the data included in the database copies all columns from the csv files recorded by the dataloggers buildingid serves as the siteid and is the only column stored as a tag all additional variables the recorded data values for each variable are stored as fields 3 3 scalability and performance metrics while we experienced no performance issues in the case study deployments we performed scalability testing to investigate the performance of the system beyond the scale of our case studies we conducted individual tests of the dps the dls and the dtm simulating larger numbers of dataloggers and http post requests in the case of the dps and dls and a larger number of remote datalogger hosts in the case of the dtm to be processed by the system scalability of the dps is dependent upon its ability to handle many http post requests from many dataloggers posting data at the same time the dps was tested by sending multiple http post requests each with a csv file containing one day of randomly generated data with values recorded every 4 s for consistency with the implementation of case study 1 the files were sent using a python script implemented using the asyncio library python software foundation 2021a from a macbook pro laptop computer with a 2 3 ghz 8 core intel core i9 processor and 16 gb of memory asyncio is a library that can be used to write code that executes concurrently allowing the code to send multiple simultaneous or nearly simultaneous requests to the dps there are limitations in the number of concurrent requests that can be sent from the same computer as well as in the number of dataloggers that can send data at the exact same time in a filed deployment considering computing power speed of connection and synchronization we simulated an increasing number of concurrent http post requests to the dps 10 50 100 200 and finally 500 and each operation was repeated ten times to characterize server network variability the total duration of each repetition calculated as the end time of the last http post request minus the start time of the first request on average was 0 6 s 2 05 s 3 58 6 91 s and 16 7 s for 10 50 100 200 and 500 requests respectively we observed no transmission errors or requests rejected by the server during our testing process fig 6 shows the durations of http post requests separated by the batch size 10 50 100 200 and 500 for each one of the 10 repetitions conducted we observed that the median duration of post requests was larger for the 10 request batches compared to all other batches but longer durations were observed for some requests in larger batches which is expected as the dps is busy with an increasing number of requests median times are consistent for batches with more than 50 post requests these times are affected by the processing power of the machine sending the request the resources available on the remote server and the speed and quality of the internet connection but are provided here as an indicator of the performance of our prototype implementation these tests indicate that the dps can handle 500 nearly simultaneous post requests in under 20 s with most individual requests being handled in under 0 2 s to test the dls we simulated different data loading scenarios ranging from loading one csv file for a single site to loading one file for 500 sites the testing procedure consisted of placing csv files containing one day of data with values recorded every 4 s in the source directory and then executing the dls each operation was repeated ten times table 6 presents the mean and standard deviation of each scenario along with the average time for loading a single file to facilitate comparisons the dls can load 1 day of data from 100 different sites in less than 50 s there are differences between loading n files from the same site and loading 1 file from n sites which can be explained by the way data are organized within the influxdb database although all of the data values are stored in the same influxdb measurement influxdb logically groups data values by shared measurement tag set and field key writing data with multiple siteid tag values takes longer both scenarios are realistic applications the first scenario n files from 1 site simulates loading data collected from dataloggers lacking communication technologies the second scenario 1 file from n sites represents a deployment like the one described in case study 1 with a larger number of sites we used the six dataloggers described in case study 2 to test the dtm each data logger sent 1 day of data during all tests the functionality that allows the system to identify existing data or files was removed allowing the system to upload existing csv files and re write existing data to the influxdb without restrictions this configuration enabled us to simulate a larger number of connections by repeating dataloggers in the hosts list included in the dtm configuration file described in table 5 the number of dataloggers was gradually increased 6 48 96 and finally 480 and the dtm was executed ten times for each number of dataloggers processing one csv file containing one day of 1 s resolution data for each datalogger the dtm was set to execute six threads at a time meaning that it can be simultaneously connected to and downloading data from six dataloggers at a time for consistency with the application of case study 2 during our testing only 6 dataloggers were available which meant that it was possible for the dtm to attempt connecting to and processing data from the same logger multiple times simultaneously this can negatively affect the time reported if a host is not immediately available for processing when the system is trying to connect to it table 7 lists the duration and standard deviation after ten runs with an increasing number of datalogger hosts using our test configuration it took less than 50 min for the dtm to process data from 480 hosts we tested the system up to and with much larger numbers than the 40 60 sites in our design considerations and observed no real limitations for using ciws in deployments roughly an order of magnitude larger even with our relatively limited testing server the dls and the dtm include writing to the database as part of their tasks and the times observed satisfy the stated requirements for our application as a final test we tested the database by conducting standard queries from a python environment using the same laptop computer we observed the amount of time required to downloaded one day one week and one month of data for 1 5 and 10 sites along with the time required to load the data into a pandas dataframe object table 8 all queries were conducted using the function get data described on table 4 the timeit python module python software foundation 2021b was used to repeat each query 10 times and measure execution times downloading one month of data a common record length in studies collecting high resolution residential water use data for ten sites into a pandas dataframe takes less than 1 min the log files and code to reproduce all the results of this section are publicly available in hydroshare bastidas pacheco et al 2021 the cost of deploying ciws to support data collection at residential houses using the equipment described for case study 1 can be broken down as follows a the cost of ciws node datalogger devices which is approximately 180 multiplied by the number of houses to be enrolled simultaneously and b the cost of hosting a server with characteristics similar to our testing server 4 processor cores 8 gb of memory 100 gb of storage at the time of this writing hosting this machine using the amazon elastic compute cloud would cost approximately 57 per month amazon 2021 although there are multiple hosting alternatives for the server that could be used and that would impact the cost estimate provided the approximated cost of building the datalogger device used in case study 2 is 85 4 conclusions and future work a complete cyberinfrastructure system that uses a layered approach to collect and manage high temporal resolution water use data was developed and implemented the system was designed focusing on the scale of data collection that would be required for research projects conducted by utilities or other researchers having a standardized cyberinfrastructure like ciws can increase the value of the data collected by allowing more straightforward data collection and management as well as facilitating the analysis and understanding of data collected in different projects cities and utilities ciws can be used to manage data collected or used for multiple purposes e g collecting data to support estimates of design parameters for future home developments guiding the planning of water conservation campaigns assessing the effectiveness of rebate programs assisting in the definition of utility rates and defining future demand and infrastructure needs our case studies showed that ciws can work with any datalogging devices that generate csv files containing time series of water use data but it can also be used in the collection of other variables as demonstrated in experimental use case 2 by integrating low cost data collection devices and open source cyberinfrastructure we sought to increase the accessibility of tools for conducting high temporal resolution data collection in support of residential water use studies ciws can reduce not only the cost of such studies but also technical barriers by providing a framework to collect and manage the data ciws can manage push and pull based data communication since each functionality is implemented separately future users of ciws can select push or pull or a combination of both depending on the needs and settings of their application the work performed within the data management and archival layer depends on whether the push or pull model is used in the pull case the data is pulled from the device by a request service whereas in the push case the data is managed by a network listener web service that accepts incoming files and processes them both use the same database component which means that the data analytics layer can operate independent of how the data are transferred the demonstrations we presented of the data analytics layer serve as a proof of concept and show the foundation upon which more sophisticated tools could be built that can be used to communicate results with multiple interested parties we focused our design and implementation on a system that is capable of transferring high temporal resolution water use data from water meters to a centralized infrastructure for storage and subsequent analysis in a research context this is preferable as researchers may not know at the outset of a study all of the specific analyses they may want to perform with the data and thus keeping all of the data is necessary however transferring large volumes of data to a centralized data management system poses challenges when scaling a system like this to larger deployments while technically possible over wi fi or cellular data networks the availability of wi fi is limited and cost of data transfer over a cellular data network may be prohibitive as an alternative we are now investigating edge computing techniques using our ciws wm node datalogger to process the high resolution water use data on the logger to produce summary data products that are much smaller and can be transferred over a network with far less bandwidth and at lower cost the tradeoff is that the full resolution data are never transferred or saved in the long term ciws combines multiple open source technologies the modular design makes it easier to replace or update technology elements in the system if needed similarly additional tools can be added to system e g more advanced analytics tools and enhanced authentication protocols the analytics presented show potential for conservation programs and can assist in the design of future urban water infrastructure all of the components we developed are publicly available for reuse and we envision future improvements to the system once the tools are used in other studies the system testing performance metrics and deployment demonstrate that ciws can meet and significantly exceed the design considerations in terms of scale and performance we saw no impediment for using ciws or a similar system in larger deployments than the ones tested by increasing the processing power of the virtual machine or deploying multiple instances the server we used for testing had only moderate system specifications and could either be run on private server hardware or could easily be hosted within a commercial cloud service provider at a reasonable monthly cost software and data availability name of software cyberinfrastructure for intelligent water supply ciws developers camilo j bastidas pacheco joseph c brewer jeffery s horsburgh juan caraballo elijah west contact jeff horsburgh usu edu year first available 2021 required hardware and software we used open source dataloggers for the data collection efforts in this study datalogger hardware details are provided by bastidas pacheco et al 2020 and horsburgh et al 2017 data management and archival components of ciws were designed to run on a linux server and were tested using ubuntu the data analytics components we demonstrate require a computer running the windows linux or macintosh operating system instructions for how to deploy the system are available in the project s github repository availability source code for the data management and archival layer software components described in this manuscript is freely available and can be downloaded from the ciws server github repository https github com uchic ciws server the src folder in that repository contains a folder named ciws ci and a folder named data transfer manager where the elements related to case study 1 and case study 2 are located respectively the doc folder contains a deployment guide for ciws the data described in case study 1 and the source code of the data analytics layer software are publicly available in hydroshare bastidas pacheco et al 2021 with instructions for reproducing the results presented in that section the data described in case study 2 and tools used to analyze it are also publicly available in hydroshare brewer and horsburgh 2020 the log files from section 3 3 scalability and performance metrics and code used to generate the results presented are available in hydroshare bastidas pacheco et al 2021 design files instructions for assembly and firmware for the open source dataloggers are available on the github sites for the ciws water meter node datalogger https github com uchic ciws wm node and the ciws electronic output water meter datalogger https github com uchic ciws ewm logger declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the united states national science foundation under grant number 1552444 any opinions findings and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the national science foundation we would like to acknowledge providence city and utah state university housing and facilities for their cooperation and support in the data collection efforts the authors would also like to acknowledge support from nour atallah arle j beckwith and rob j tracy in the data collection efforts and elijah west for his contribution in software development we also acknowledge and thank the owner of the residential home and the students in the llc buildings that participated in the data collection campaign 
25761,collecting and managing high temporal resolution residential water use data is challenging due to cost and technical requirements associated with the volume and velocity of data collected we developed an open source modular generalized architecture called cyberinfrastructure for intelligent water supply ciws to automate the process from data collection to analysis and presentation of high temporal residential water use data a prototype implementation was built using existing open source technologies including smart meters databases and services two case studies were selected to test functionalities of ciws including push and pull data models within single family and multi unit residential contexts respectively ciws was tested for scalability and performance within our design constraints and proved to be effective within both case studies all ciws elements and the case study data described are freely available for re use keywords residential water use data management smart metering cyberinfrastructure information and communication technology 1 introduction achieving higher efficiency in urban water management and planning requires understanding of how water is used at the household level daily patterns in consumption potential for water savings and distribution of water use across end uses are essential inputs to water demand estimation leak identification design of programs to manage water demand and water planning to ensure adequate supply giurco et al 2008 willis et al 2011 metering water use for billing purposes is a common practice in the united states where meters are typically read monthly or quarterly our ability to characterize water demand is limited by the temporal resolution of the data collected higher resolution data can increase the accuracy of peak demand estimation and reduce leak volumes that can go undetected sub minute resolution data is required to record and quantify end uses of water that have short duration cominola et al 2018 nguyen et al 2015 however obtaining this higher temporal resolution data at a scale larger than a few houses presents several challenges in terms of data collection storage management and processing cominola et al 2018 and doing it over an extended period of time can be unpractical cardell oliver 2013 collecting a month of 10 s resolution data for a single meter which is common in end uses of water studies deoreo et al 2011 2016 mayer et al 1999 2004 produces more than 250 000 observations doing so at a water utility or municipality scale which may have thousands of metered residential connections presents obvious challenges associated with the volume of data that would be produced many utilities lack a dedicated information technology or data management staff which means that new database management software deployment and data analysis tasks can be prohibitive in these cases and in the absence of sufficient cyberinfrastructure for automating data management tasks high resolution data could be more of a roadblock for a water provider than a benefit however with adequate data collection and management tools utilities may be able to realize more of the potential benefits associated with high temporal resolution data this includes quantifying water use behavior to better enable planning that ensures adequate supply the promotion of water conservation behavior among users liu et al 2015 improving customer service quality for utilities beal and flynn 2015 tipping the cost benefit balance in the smart metering adoption case which remains undefined cominola et al 2018 and enabling the proliferation of scientific work in this field the term cyberinfrastructure integrates hardware and software tools as well as data networks nsf 2007 cyberinfrastructure can help solve data management challenges and enable more widespread collection of higher temporal resolution water use data for utilities and researchers in a broader context cyberinfrastructure is improving the communication of results from hydrological models souffront alcantara et al 2017 helping monitor watershed health parameters szwilski et al 2018 assisting in the automation of comparing climate model results sun et al 2020 and it is now ubiquitous in multiple scientific domains hachmann et al 2018 shams et al 2018 wegrzyn et al 2020 smart meters have potential to solve one of the challenges in the pathway to an advanced water cyberinfrastructure high resolution measurement of water use the term smart meter can be ambiguous boyle et al 2013 within this article it is used to denote devices capable of recording water use with high resolution i e sub minute frequency that can be integrated in automated systems for data management nearly a decade ago it was anticipated that use of smart meters would grow over time boyle et al 2013 and they are in fact becoming more widely available and adopted with this emergence of smart meters there has been an increase in the number of scientific publications using the high resolution data they produce for water demand analysis cominola et al 2015 provide a comprehensive review however despite the increase in the number of publications using smart metering data to quantify end uses of water and water use behavior the data management procedures or tools used in these studies are not well described and most of the datasets used are not openly available di mauro et al 2020 in most of these studies the focus has been on the tools and algorithms used for identifying water end uses and user behavior other components of the data management process are not described available cyberinfrastructure for collecting managing and analyzing this type of data remains scarce and of proprietary nature with little available literature describing tools and procedures for data collection management and analysis meter manufacturers tend to have their own software systems designed for their metering technology which complicates synthesis or integration of data from multiple systems and may help explain why research in this field has been conducted in a limited number of countries using a limited number of datasets many of these studies have used the same data logging device for data collection and the same software tool for end use analysis beal and stewart 2011 deoreo et al 2011 2016 mayer et al 1999 2004 other studies have reused the same dataset to conduct different analyses for example beal at al 2013 present differences between perceived and actual water consumption willis et al 2013 studied the impact of socio demographic and efficient fixtures on water use and beal and stewart 2011 presented end uses of water characteristics all using the same dataset collected in southeast queensland australia the datalogging devices used in most high resolution data collection studies lack communication capabilities which limits the potential for automated integration with downstream cyberinfrastructure e g telemetry storage management and analysis applications more recently there has been increasing discussion around smart cities smart grids smart water networks and other related terms despite there not being a wide agreement about their definition what is meant by smart or the extension of their applications ardito et al 2013 hollands 2008 wissner 2011 it is generally agreed that smart cities make use of information and communication technologies ict in an attempt to assist cities in optimizing the use of their assets neirotti et al 2014 water being one of the most important connectedness of data collection and its application is important in this context advanced metering infrastructure ami and ict systems are vital for the successful deployment of a smart grid yan et al 2013 in the energy sector smart grids use smart technologies for metering communication and automation and make use of digital information to improve reliability u s congress 2007 the internet of things iot has also been described as a potential enabler of smart grids in the water sector alghamdi and shetty 2016 robles et al 2014 zanella et al 2014 and more recently smart solutions that use iot principles have been proposed amaxilatis et al 2020 stiri et al 2019 liu and nielsen 2016 discussed existing technologies to develop an ict system or cyberinfrastructure to enable smart meter analytics for the energy sector acknowledging the difficulties in processing and managing the large volumes of data generated similar systems have been proposed and discussed for water use analytics boyle et al 2013 li et al 2020 makropoulos 2017 moy de vitry et al 2019 but few implementations have been published due to the cost and complexity of these applications alvisi et al 2019 amaxilatis et al 2020 anda et al 2013 in one notable example chen et al 2011 conducted analysis using data collected on a smart water service architecture deployed for billing purposes on the city of dubuque ia this system collects data every 15 min providing more advanced analysis to water consumers and providers erickson et al 2012 while multiple high level designs of a smart water network have been described e g hauser et al 2016 li et al 2020 ye et al 2016 implementations are scarce most of the smart water systems designs we reviewed lacked a full demonstration or prototype implementation in some cases important elements such as performance metrics and implementation guidance were not fully described li et al 2020 when demonstrations were presented the focus was primarily on the results of the specific case study i e the lessons learned about water use and or behavior and not on the design and implementation of the tools used to complete the tasks the limited availability of data and tools for the water sector constitutes a significant barrier for the development of research and prevents the advancement and implementation of smarter water grids at a large scale mutchek and williams 2014 the closed source nature of existing data collection hardware and data management software creates accessibility and interoperability issues that prevent the progress of smart water grids while curtailing the adoption of open architectures hauser and roedler 2015 robles et al 2014 the development of open source cyberinfrastructure for managing high resolution data can lay the foundations for the development of newer and better tools for water utilities as well as standards for operations that result in increased interoperability all of these actions could pave the road for more water demand research and ultimately advance technologies for the development of smart water grids thus in order to achieve the full potential of smart meters cyberinfrastructure is needed to support utilization of the high resolution data they produce horsburgh et al 2019 mason et al 2014 developing effective cyberinfrastructure that can support both operational data collection and management e g for billing reporting and day to day management purposes and exploration of data for research aimed at better understanding water use behavior is expensive and challenging stocks et al 2019 indeed architectural designs and data structures for cyberinfrastructure supporting residential water use data must meet the needs of multiple users i e water providers water consumers researchers without disrupting a utility s necessary business functions the research described here focused on the following research questions to advance the cyberinfrastructure and availability of software tools for collecting managing and analyzing high resolution smart metering data a what is the general architecture for a cyberinfrastructure to support collection and management of high temporal resolution smart metering data and b how can that architecture be implemented to meet the needs of multiple potential users e g water utilities water consumers researchers in this paper we present a generalized architectural design for a cyberinfrastructure for intelligent water supply ciws and a prototype implementation of each of the components within the architecture in support of multiple data collection management and analysis case studies the prototypes we developed demonstrate tools that are not currently available for researchers or utility managers and include a a data collection layer consisting of datalogging devices with data transmission capabilities which are modifications from our previous work horsburgh et al 2017 bastidas pacheco et al 2020 b a data management and archival layer that receives processes and stores data and c a data analytics layer that enables calculation of common water use metrics e g average hourly water use instantaneous peak and end uses of water disaggregation and classification components within these layers demonstrate the entire workflow consisting of data collection communication storage management and archival and visualization and analysis while ciws was designed and implemented for research purposes including appropriate mechanisms for protecting the identities of research participants where necessary it facilitates implementation of high temporal residential water use analysis which is of interest to not only researchers in the field but also utility companies and water consumers and can provide information currently not available to them the data collected and managed using ciws is relevant for assessment and management of both water demand and for planning to ensure adequate water supply we first describe the requirements for the system along with the overall architecture we designed to meet these requirements section 2 we then describe a set of case studies in which this overall architecture was prototyped and implemented using both existing and new open source hardware and software components section 3 finally we close with discussion and conclusions section 4 2 methods 2 1 ciws design and overall software architecture our goal in developing ciws was to create a generalized modular architecture that can be used to automate the process from collection to analysis and visualization of high temporal resolution water use data in our case study applications of ciws we combined existing and developed new open source hardware devices and software tools to demonstrate an integrated solution for high resolution residential water use data collection management and analysis the ciws architecture and our prototype implementation were designed to address the following requirements while we present our prototype implementations in this paper there may be multiple implementations of the generalized architecture that meet these requirements a an open architecture that could be implemented using a variety of technologies b open source software development to facilitate its deployment and use by other users reduce costs and provide a platform for future improvement by others while advancing financial feasibility of larger scale implementations c a modular design so each component of ciws can be used or advanced independently d accept input data from different meters and measurement devices sensors to address heterogeneity in urban water meter technology e capacity to manage push and pull data retrieval from the metering devices depending on available communication technologies and storing of data in a centralized server f scalable to accommodate a large data volume while remaining responsive to queries for subsets of time series data of varying sizes g support production of analysis and insights that meet the needs of different audiences in our review of the literature we found that existing designs of smart components or cyberinfrastructure for managing water systems are not fully standardized however most systems described or implemented to date are composed of multiple layers working in connection to achieve the overall goal li et al 2020 we found that the number name and function of these layers was different in each design however we observed some similarities in practice the number of layers included in an architectural design comes down to tradeoffs between the benefits of modularity and separation of concerns that can be achieved versus the complexity and potential fragility introduced with a larger number of layers separate layers can be autonomous such that changes to one layer do not have to affect the other layers however a greater number of layers typically involves more components that can fail our overall architectural design for ciws adopts this multi layer paradigm fig 1 and is composed of three main layers the first layer is the data collection layer and includes the physical instruments and sensors used to monitor water use it has also been called the sensing layer ye et al 2016 the physical layer hauser et al 2016 or the instrument layer li et al 2020 the second layer is the data management and archival layer which handles data communication parsing and archival this layer has also been referred to as the network or function layer hauser et al 2016 li et al 2020 ye et al 2016 the final layer is the data analytics layer which handles all the steps between queries to retrieve data from the archival component to final visualizations analyses and presentations produced for utilities water consumers researchers etc i e the consumers of the data this layer has also been referred to as the application or the data fusion and analysis layer hauser et al 2016 li et al 2020 ye et al 2016 some of the other systems reviewed include elements for real time monitoring and control of observed variables and processes within the system resulting in architectural designs with a larger number of layers since these elements were not needed in our case study use cases a three layer model met all of the requirements listed above a system with more layers may become more fragile therefore our design includes the minimum needed to meet the design considerations the architecture for ciws and our prototype implementations were developed with a research focus e g collecting storing and managing high resolution water use data to enable advanced study of residential water use behavior this type of research may be carried out by utilities universities or other agencies involved in research related to or management of urban water supply and demand the typical deployment size in this type of work has been around 50 houses per city however some studies have analyzed up to 762 sites deoreo et al 2016 in the latter case the data was not collected simultaneously at all sites our aim was to develop a system that can handle at minimum the number of simultaneous data collection sites within the range of deployments observed in the past 40 60 houses in the following sections we describe in more detail the high level design for each of the architectural layers their key components and their basic functionality 2 1 1 data collection layer data collection refers to the actual measurement of the variable or variables of interest in this case high temporal resolution water use here we define high temporal resolution data as data collected at a sub minute resolution typical investigations of water use behavior such as separating and quantifying end uses of water within a home require data to be recorded at 10 s or even finer resolution over data collection periods of weeks to months with few exceptions high temporal resolution data cannot be collected using existing commercially available smart meters without adding additional hardware or software components cominola et al 2018 which can be expensive horsburgh et al 2017 water metering technology typically consists of a physical meter that uses one of several measurement techniques paired with an analog or digital register on which a totalized volume of water use is recorded some registers including those of commercially available smart meters are capable of storing volume readings within internal memory however this is usually constrained to relatively short periods of time e g weeks at recording intervals longer than 1 min other registers report only the most recent volume reading and are designed for periodic e g monthly or quarterly readings either manually or automatically via radio these practical limitations are driven by power local data storage and network bandwidth limitations of existing metering technology some water use studies have added flow metering sensors directly on the water pipe leading to each appliance in a residential house kofinas et al 2018 di mauro et al 2019 opting for this approach allows direct measurements of water use from each fixture and by placing the measuring element inside the property power and communications can be readily available however this approach is invasive and requires modifications to the plumbing in each home where data is collected which can increase costs and limit the applicability of this methodology at a medium or large scale therefore we opted to focus our efforts on datalogging devices that can be coupled with the existing water meter available at the property datalogging devices designed to couple with existing meters are available bastidas pacheco et al 2020 f s brainard company 2020 these dataloggers essentially perform the same function as the meter s register but have the capability of recording much more frequent observations over longer periods of time to be fully integrated in a data management system like ciws the datalogging devices must also have communication capabilities ciws was designed to handle both push and pull data communication making it adaptable for multiple scenarios the term push is used to denote systems where the data is sent by each datalogger client to a centralized server while pull refers to systems where a centralized server connects to each datalogger and requests data given the modular design of ciws it is possible to integrate dataloggers that lack communication capabilities such as those used in most residential studies in the past under this scenario a user can take advantage of the data management and archival and data analytics layers of ciws while using data files manually downloaded from the datalogging devices in the field 2 1 2 data management and archival layer the data management and archival layer is responsible for the work required to process the data logged by the devices the key component addressed in this layer relates to developing and using software elements to automate repetitive data management processes and enable an easier transition between large volumes of data collection and useful information generation this layer is composed of multiple working elements fig 1 for push based data transmission a listener service is required to receive the data sent by the dataloggers in pull based data transmission a request service is used to achieve the same task once the data is received it must be verified parsed and transferred to a database component the database component accepts and stores data for downstream analysis and decision making real time monitoring of water use is typically not of interest in most research scenarios where most data analysis happens after the data have been collected additionally given the frequency with which observations are recorded e g on the order of seconds it is not practical to push or pull data every time a new observation becomes available based on this ciws was designed to collect and send files containing many observations rather than sending observations individually this approach minimizes the communication load on the system because the data transfer process does not occur constantly and it can be scheduled to meet specific needs the request service for pull based data transmission must execute the following tasks a connect to a datalogging device b check for new data files c request and transfer new files d read and parse the files and e upload the data into the database remotely accessing devices can be achieved using a variety of communication protocols like secure shell ssh which is a widely used method for similar tasks due to its simplicity speed and security in this model the datalogging devices need to be powered on and connected to the network at the time the connection is established additionally a key requirement is that each datalogging device must be located addressed and accessed directly which also provides an opportunity for remote functionalities such as software updates troubleshooting changing data collection settings and others the listener service which manages the data transferring process under the push model must complete the following tasks a accept and validate the data sent from each datalogging device deployed b process incoming files including parsing the information they contain and c saving the data received into the database under this approach the communication elements of the datalogger only need to be powered up and functioning for the time it takes to send the desired information to the listener service which can contribute to lower power requirements additionally there is no requirement for data logging devices to be uniquely addressed on a network as they can identify themselves within the content of the message they push to the listener service multiple technologies that can potentially meet the data storage and accessibility design considerations i e the database requirement are available the database must be able to manage large volumes of data and provide a platform for generating analytics of such data the data managed by the system consist mainly of time series of flow observations which are constantly being collected and written into the database thus the databasing technology selected must provide a easy and fast querying between dates and times to enable manipulation of the data b high performance for read and write operations as the database is continuously being updated with new data and potentially accessed by multiple users and c scalability as the volume of data to be stored in the database increases quickly as the monitoring network and time period over which data are collected grow the database schema used to organize the data for ciws was designed to maximize query efficiency while maintaining the ability to protect the privacy of water consumers by storing personally identifiable information outside of the database common queries to be conducted in projects where ciws can be used include selecting all or part time constrained of the full resolution or time aggregated data for a single or multiple sites 2 1 3 data analytics layer the data analytics layer supports generalized interactions between data users and the database for the purposes of visualization and analysis of the data the necessary functions executed in this layer include a user authentication to access existing data b querying data from the data base c data manipulation and analysis and d generation of reports and visualizations of interest for different target audiences for the purposes of this research three main target audiences were identified as users of information produced by the data analytics layer water consumers utility managers and researchers while these categories of users are not necessarily exhaustive or mutually exclusive the information that would be useful to these different users and the methods used to interact with the data are not the same for instance an individual residential user would need to be able to access and interact with the data from their home in a practical and non technical way that does not require specialized software past studies have evaluated residential users preferences for water use feedback finding that information about their prior water consumption comparison of use with that of similar users and details about their consumption can increase user understanding erickson et al 2012 liu et al 2015 utility managers may want to access standardized plots or reports showing data from multiple users and researchers may need much more freedom to formulate their own custom queries to the database to subset aggregate or summarize data in useful ways this implies that the data analytics layer needs to support multiple mechanisms for accessing and interacting with the database authentication authorization and privacy for users with different privileges read or write data in a database to access online resources have been discussed for multiple applications christie et al 2020 heiland et al 2015 kim and lee 2017 high temporal resolution data products such as distribution and timing of end uses can raise privacy concerns among water consumers that must be considered when designing data presentation tools froehlich et al 2012 aggregation and summarization techniques can be used to present information for multiple water consumers while protecting privacy and authentication and authorization can be used to limit what data is available for different users ciws considers the use of anonymized datasets throughout the system by identifying water consumers with a unique identifier linkage with the personally identifiable information about each water consumer is stored separately and is only available to those who have appropriate privileges and are allowed match water consumers with their data 2 2 case study design and system testing in order to evaluate the overall architecture design we designed two case studies that demonstrate different aspects of the architecture presented in two distinct data collection environments the first case study demonstrates data collection at individual single family residential homes it uses an autonomous datalogger with communication capabilities to collect high resolution water use data and demonstrates push based transmission of the data to the data management and archival layer the second case study demonstrates data collection within multi unit residential structures on a university campus it uses dataloggers with dedicated power supplies and network registrations to demonstrate pull based transmission of the data to the data management and archival layer in the second case study we collected data for additional parameters needed to characterize the energy consumption related to hot water use the collection of data for these parameters provides an example of ciws flexibility both case studies share the same layers but we describe the different elements used by each case study we created a full prototype implementation of the design layers presented in fig 1 for each case study and deployed them in an operational environment these prototypes and deployments were created to demonstrate proof of concept for data collection and management components the shareability of components within the architecture regardless of the data transmission method and generalizability for our architectural design we tested the system developed for scalability by simulating an increased number of sites and larger volumes of data python 3 7 was chosen to develop all of the code and software associated with our case studies given that it is freely available and open source it is a high level programming language with a vast number of libraries available to complete an important number of functions required in our application and it could be used across all three layers of our architectural design using python also helped us meet the first three requirements described above as the code can be easily shared read and modified by other programmers and scientists and can be deployed in different operating systems which increases reuse possibilities 2 2 1 case study 1 description water use in single family residential homes is quantified to a large extent using analog positive displacement water meters the volume of water that has passed through the meter is usually the only variable recorded by this type of meter in most cases water meters are enclosed in underground pits of varying depth limiting power supply availability these meters are typically read monthly quarterly or at coarser resolutions by the utility for billing purposes either manually or via a roving radio that receives the most recent volume observation from each meter when the roving radio passes within range some more advanced networks include automated retrieval of the coarse resolution volume data but very few have the capability to record and transmit high resolution data given that the vast majority of residential water meters in use today share these constraints we chose this case study to demonstrate adding high resolution data collection and transmission capabilities to existing analog water meters 2 2 2 case study 2 description the living learning community center llc on utah state university s usu campus was selected as a second case study for deploying ciws within a set of multi unit residential buildings the llc is one of usu s newer student housing options and houses approximately 500 students distributed among six dormitory buildings labeled building a building f the objective of this implementation was to characterize water and water related energy use in five buildings b f the importance of the water energy nexus for optimizing conservation and sustainable management has been identified in the past hamiche et al 2016 kenway et al 2016 fang and chen 2017 however collecting water and energy consumption data combined at a sufficient temporal resolution to analyze their relation is uncommon and the methods for linking water and energy use are not well established this case study demonstrates a methodology for collecting water and water related energy data in a multi unit residential setting buildings b f host approximately 90 students each building a hosts administrative offices has a much lower student occupancy and was excluded from the study we chose a pull based model for this case study given the availability of dedicated power at each data collection site and the availability of usu s campus wi fi network to enable communications and data transmission three water meters are present in the water supply system for each of these buildings hot water supply cold water supply and hot water return to monitor water and water related energy use within each building two characteristics of each meter were measured flow and water temperature resulting in a total of six variables collected per building table 1 the hot water return is a feature of the llc s innovative hot water recirculation system hot water is continually circulated from three boilers to the llc buildings at a constant base flowrate of approximately 3 gallons per minute gpm or 11 4 liters per minute lpm increases from this base flowrate constitute hot water use unused hot water returns to the one of the three boilers for reheating and eventual recirculation cold water is supplied in a typical on demand basis 3 results and discussion 3 1 case study 1 push based data collection for single family residential homes we selected a single family residential property to test the ciws functionality under a push based data retrieval model we collected two weeks of data at this property between january 15 2021 and january 28 2021 for the implementation described all water use results presented are for this time period this home had five occupants three of ages between 10 and 25 and two between 40 and 60 during the data collection period it was built in 2006 has three bathrooms and a total parcel area of approximately 12 000 ft2 1114 8 m2 we chose push based data retrieval for this case study because it is enabled by heterogeneous networking i e any datalogger device capable of high resolution data collection and sending data over an available data network could be used without the need for each device to be uniquely addressable on a network additionally power requirements can be reduced given that data logging devices do not have to listen for connections and requests from a centralized server but rather wake to transmit data on a user configured schedule 3 1 1 data collection layer at the property selected a one inch 2 54 cm bottom load bl master meter with an analog register was being used by the water utility to record monthly water use transmit it to a roving receiver via a 3g radio and bill water usage we added high temporal resolution data collection and transmission capabilities without affecting the normal operation of the utility s meter by installing a ciws water meter node ciws wm node datalogger to measure water use at a 4 s temporal resolution on top of the existing meter the ciws wn node is an advanced modification of the ciws datalogger bastidas pacheco et al 2020 which is an open source arduino based datalogger that we designed to work with any magnetically driven water meter the ciws datalogger uses a magnetometer sensor to measure the magnetic field around magnetically driven residential water meters it counts peaks in the magnetic field associated with movement of the magnetically driven measurement element within the meter and registers peaks as pulses that represent a fixed volume of water passing through the meter these pulses are multiplied by a factor called the meter resolution 0 041619 gallons per pulse or 0 1575 liters per pulse for the case study meter which is specific to each meter type brand and size to obtain the volume of water that passed through the meter per unit of time meter pulse resolution values can be obtained from meter manufacturers or through a calibration procedure described by bastidas pacheco et al 2020 the ciws wm node we developed for this case study adds communication and computational capabilities to the ciws datalogger by coupling it with a raspberry pi model b or model b single board linux computer the components of the ciws datalogger control all of the datalogging functions whereas the raspberry pi computer can be powered on a user defined schedule to process and transmit data the raspberry pi runs a version of the linux operating system called raspberry pi os previously called raspbian although the raspberry pi is capable of interfacing with a number of different wireless communication options including wi fi radio frequency cellular 3g lte bluetooth and satellite we chose to use the raspberry pi s built in wi fi capabilities for this case study because the homeowner s wi fi network was easily accessible in broader application however any internet data connection compatible with a raspberry pi could be used the ciws wm node datalogger outputs a comma separated values csv file including a three line header with a unique identifier for the site at which the datalogger is installed a unique identifier for the datalogger and the meter resolution for the meter on which it is installed the datalogger records three variables during the logging process datetime record and pulses bastidas pacheco et al 2020 the ciws wm node datalogging device was configured to chunk the data files by day i e a new csv file is created for each day and send data files once per day to the data management and archival layer via an http post request this functionality was developed as a single python script data transfer py when the raspberry pi is powered on it can conduct any computation required and the data transfer py script is executed to send data files to the data management and archival layer for further processing after a file is successfully sent via http it is moved to a different folder in the datalogger s local storage for backup 3 1 2 data management and archival layer for our case studies the data management and archival layer components were deployed within a vmware esxi server environment hosted at utah state university on a single virtual machine vm running the ubuntu linux server version 18 04 bionic beaver operating system ubuntu is a free and open source linux distribution developed by canonical ltd it is well supported stable and offers reliable file security the vm was configured with a 64 bit architecture four 2 3 ghz processor cores eight gb of ram and 100 gb of hard disk space we refer to this vm as the data management and archival server we developed three main components to complete the tasks described for this layer the data posting service dps the data loading service dls and the operational database each of which is described in the sections that follow the dps and the dls were developed in a generalizable way to facilitate reuse and serve as the network listener shown in the center panel of fig 1 however some specific details were adapted to this implementation for example the data parsing works for the specific output format of the ciws datalogger the dps and the dls were deployed on the data management and archival server and then configured via settings stored in a user modifiable javascript object notation json file named configuration json that details the information needed for their operation for deployment the configuration file must be placed in the same folder with the dps and dls 3 1 2 1 data posting service dps the dps is a listener web service that receives and processes data files pushed to the data management and archival server from the ciws wm node dataloggers the dps works integrated with two common server technologies the web server software that processes http requests received by the server and a web server gateway interface wsgi that runs the dps application in response to the requests we chose nginx nginx 2021 which is a free open source http server to serve as the web server software because of its high performance stability simple configuration and low resource consumption the wsgi was implemented using gunicorn 2021 which is a python wsgi http server for unix like operating systems guidance for deploying the web server and wsgi software is available in the project s github repository the parameters included in the configuration files for the dps and the dls are described in table 2 the overall functioning of the dps is as follows dataloggers send an http post request to the server that contains a data file for our case study one day of high resolution water use data for that home these requests are received and handled by the nginx web server which passes them to the gunicorn wsgi gunicorn then invokes and executes the dps to authenticate the http post requests by using a token client token in table 2 verifying the file type csv and that the file does not already exist on the server before moving it to a local folder on the server source directory in table 2 for further processing by the dls the dps is composed of three pieces of code app py which lists the functions needed to read the application configuration file auth py that lists all the functions for file authentication and web service py which calls the previous two files and executes the tasks described fig 2 illustrates the processes described and lists the elements involved the dps was implemented using bottle hellkamp 2021 which is a wsgi micro web framework for python bottle is simple fast lightweight and works without additional dependencies making it ideal for running small applications like the dps bottle built in functionalities such as its simple url routing capabilities and the convenient access to file uploads were used to facilitate the development of the dps and avoid dealing with low level details of http requests handling and routing we implemented a very simple token based authentication for the http post requests in our prototype to avoid spam content being submitted to the dps more sophisticated and secure authentication and authorization processes could be integrated in the future if needed to provide greater security a log file keeps track of the requests received by the dps and actions executed the log file is located in a directory described in table 2 the log file records successful and unsuccessful e g a file that already exists is sent to the server multiple times a request that is rejected by not having appropriate authentication credentials posting attempts all events are logged in a single file named data poster log which is limited to 5 mb in size when a log file exceeds this size it is saved adding a sequential number at the end data poster 1 log initially and the current logging continues in the original log file 3 1 2 2 data loading service dls we developed the dls to read the files received from the dataloggers from the source directory on the server parse the unique site identifier information from the header of the csv file and insert the data into the database for archival and use by the data analytics layer the dls also verifies that the data received does not already exist in the database by checking the unique site identifier and datetime values of the data to avoid duplication of data in the database the dls uses the same configuration file as the dps described on table 2 the dls reads data files from a local source directory and moves them to a local target directory after successfully inserting the data into the operational database if an error occurs the files are moved to the quarantine directory a log file records all the activity executed by the dls including any error observed in the process such as invalid datetime stamps invalid site identifiers and attempts to load data that already exists in the database this log file is named data loader log and it is managed identically to the dps log file both are located in the same folder log directory in table 2 we chose this implementation for several reasons first it enables preservation archival of the original csv data files recorded by the dataloggers second the data are loaded into an operational database that is highly performant for querying and data retrieval in support of the data analytics layer third it enables all of the downstream components in the architecture to be used regardless of how the data files arrive on the server for example they can be automatically pushed to the server from the datalogger pulled from the datalogger by the server as in our second case study or manually copied to the server in the case where data transmission is not automated the dls was implemented in a single python script named loader py 3 1 2 3 operational database for the operational database component we chose to use an existing technology given the availability of mature and robust database software in our previous work related to investigating how to best manage large volumes of time series data we tested the performance of four commonly used open source database technologies including mongodb mysql postgresql and influxdb brewer 2020 based on our tests we chose to use influxdb influxdata 2021 due to its time series oriented data structure rapid query performance and favorable disk space requirements when compared to the other software technologies influxdb is a popular time series database designed specifically for time series data in applications that require handling high data write and query loads it provides a powerful structured query language sql like query language and has both open source distributions that can be installed and used for free e g as we did on our linux vm and cloud deployments that can be implemented with usage based pricing influxdb has been used in multiple iot and other applications where it has been tested for large datasets balis et al 2017 di martino et al 2019 rinaldi et al 2019 influxdb also offers extensive support for multiple programming languages including python and r which are commonly used for data science this made it straightforward for us to use python to insert data and to execute queries from the data analytics layer influxdb databases are organized around the concept of a measurement which can be thought of as a table that contains an indexed column named time containing the timestamp of each data point where each data point is a row in the table additional variables are stored in columns that can be tags or fields the main difference is that tags are indexed and are not required in a data structure whereas at least one field is required fields are not indexed the column names for tags and fields are defined as keys generally it is recommended that data values are stored as fields and metadata as tags to improve query performance in our design for storing data in influxdb the number of pulses recorded by the datalogger during each time interval is included as a field key pulses and the site identifier key siteid and the datalogger identifier key dataloggerid are included as tags table 3 the data for all sites are stored in a single measurement within the influx database raw data and quality controlled qc data are stored in separate measurements with the same structure qc data is a copy of the raw data that is created after verifying that the volume registered by the datalogger is within 5 of the volume registered by the meter estimated using subsequent readings of the meter s register conducted during installation during periodic site visits and at removal of the datalogger in some cases known bad data were trimmed from the beginning and end of a valid deployment where the volume recorded by the datalogger did not match the volume recorded by the meter s register the data were discarded and a new deployment was started during our case study deployments we did not observe any out of range anomalous or unreasonable pulse count values after this qc procedure in consequence additional qc modules were not implemented however additional qc procedures could be implemented in the future all queries and analysis are conducted using the qc data the database is the point of connection between the data analytics layer and the data management and archival layer and its design must meet requirements from both layers to write and read data typically database schemas are designed around the structure of the data to be stored and to facilitate the most common types of queries this is usually a tradeoff between making it easy to insert data into the database while still providing highly performant queries the simple database schema implemented in this case study table 3 mirrors the structure of the data files generated by the dataloggers making it straightforward to insert data but is also optimized to support the following queries 1 selecting all of the data for a particular siteid 2 selecting all of the data for a particular dataloggerid e g to track the performance of a datalogger which may be deployed at multiple sites at different times and identify correct any systematic errors and 3 querying data for a specific time frame e g between a beginning and ending date combining queries based on these three elements provides most of the functionality intended for ciws and met all of the needs of our case study additional queries intended to allow comparison of data across multiple sites may also be of interest our design separates the time series data which are stored anonymously in the influxdb database from household information which is stored in a separate csv file named sites csv the data stored in influxdb do not contain any identifiable information which removes privacy concerns from the time series data the separate sites csv file may include sensitive personally identifiable information e g names addresses etc along with any other descriptive characteristics the version of the sites csv file for this study published in hydroshare has been anonymized data managers may wish to maintain multiple versions of the sites csv file e g one with all personally identifiable information about data collection sites and one that has been anonymized and could be released to a broader set of users while this approach adds an additional step for certain types of queries e g selecting data for all houses within a certain geographic area or of a certain built age because the site information must be queried before the correct time series data can be retrieved it provides a mechanism for protecting personally identifiable information and more flexibility for managing metadata about the sites removing or adding tags to existing measurements is significantly restricted in influxdb in consequence anonymizing the data stored in influxdb for publication is not needed as the data stored is already anonymous queries against the time series data can always be executed using a siteid or set of siteids obtained via a prior query to the sites csv file it is also possible but currently not implemented to add all site metadata as tags in the influxdb measurement to eliminate this intermediate query step if that is more convenient in a specific application researchers and utility managers can access the data within the influxdb database with a non administrator user account influxdb allows for the creation of multiple non administrator users and at least one administrator user the administrator manages authorization for each non administrator user non administrator users can be restricted to write read or both the free version of influxdb does not allow fine grained authorization which would be needed to restrict users to view only part of the data in a measurement however we did not see this as a significant drawback as high level users like researchers and or utility managers would likely need to have unrestricted access to all of the data in an influxdb database furthermore it is unlikely that the full resolution data would be provided to water consumers rather a more likely scenario would be for a software application with a graphical user interface to be developed for presenting water consumers with feedback about their consumption authentication and authorization of users could be handled separately by the software application in future deployments erickson et al 2012 provide an example of an online water portal and discuss the privacy and user authorization concerns that impact the design of similar tools homeowners are typically presented with summary statistics and visualizations calculated for their property and may be provided with a summary level comparison with other properties however they generally would not have access to view raw data for their own or other properties 3 1 3 data analytics layer to illustrate the type of capabilities supported by the data analytics layer we developed python tools that provide an example of the main aspects involved in this process connection to the database user authentication and data retrieval via common queries once the data has been retrieved into a python environment it can be integrated with existing and more advanced data analysis and visualization tools while it is beyond the scope of this paper to demonstrate all of the possible ways in which data can be retrieved from the database component and used within analytical applications the tools we developed demonstrate the general patterns required for developing such tools and serve as a foundation on which others could be developed influxdb client programming libraries are available for several popular programming languages including python go c java php ruby scala javascript and r which simplifies software development using influxdb and facilitates desktop mobile and web application development using the python client library for influxdb influxdb 2020 we first developed a set of functions for interacting with the influxdb database these functions were implemented within a single python script called da functions py this script connects to the database using a set of configuration parameters that are included in a json file named configuration json which is similar to the one used by the dps and dls applications parameters in the json file include host port username password and database as defined in table 2 the functions we developed in da functions py table 4 use the existing capabilities of the influxdb python client library along with specific parameters provided by the user e g siteid time dataloggerid as defined in table 3 to provide a simple application programming interface api for querying data from the database we anticipate that these functions will meet many of the most common data requirements for most researchers and utilities the functions generate a pandas dataframe mckinney 2010 with the resulting data if a single siteid is provided and a python list of pandas dataframes when multiple siteids are provided if a start date or end date are not included the function will download the entire record available if only a start date is provided the function will return everything from that date to the end of the record in the opposite case it will retrieve data from the beginning of the record to the specified ending data if measurement is not provided the functions will query from the quality controlled data qcdata raw data can be downloaded by specifying measurement rawdata for time aggregated data the function parameter can include any influx supported aggregation function e g mean median max min sum the time resolution of the aggregated data supports any influxdb duration type e g 1m for 1 min data 1h for hourly data 1d for daily data 1w for weekly data all the arguments in both functions are python keyword arguments they must be preceded by their identifier or name when executing the functions i e get data site 1 to return all the quality controlled data for siteid 1 we then developed a python jupyter notebook called data analytics ipynb that loads the functions listed and implements a basic workflow to produce metrics and analysis from the data collected jupyter notebooks kluyver et al 2016 allow creation and sharing of documents that contain live code equations visualizations and narrative text which makes them ideal for prototyping visualizations and analyses for the data analytics layer the notebook we developed imports data using the defined functions and then generates visualizations of common metrics of residential water use for presentation to water consumers for example fig 3 shows the average hourly water use blue solid line and the boxplots show the distribution of hourly water use for the period of data collection at the residential home we monitored we can notice two periods of higher water usage one during the morning and the other early in the afternoon corresponding with patterns typically observed in hourly residential water use data during this period no outdoor water use occurred therefore the figure represents indoor water use only the notebook then demonstrates calculation of summary water use information for the data collection period for example average daily water use was 170 2 gallons 644 3 l leading to a per capita average daily water use of 34 gallons 128 7 l the maximum daily water usage observed during the period was 292 7 gallons 1077 9 l the instantaneous peak was 10 gpm or 37 95 l per minute lpm and the maximum hourly usage registered was 74 1 gallons 280 5 l another analysis of special interest using high temporal resolution data is the identification of end uses of water we used an open source algorithm developed by attallah et al 2021 available via the hydroshare repository attallah and bastidas pacheco 2021 within the data analytics layer to separate raw data into events and classify the resulting events into categories of end uses of water the algorithm filters the data collected using a low pass filter making it easier identify single or concurrent events concurrent events are separated into single events and the final table containing only single events is classified by using a combination of clustering to identify atypical or outlier events and a fully supervised machine learning methodology to assign labels to the remaining events the machine learning model uses a random forest classifier liaw and wiener 2002 trained using a set of user labeled and manually labeled events to classify new events for individual residential homes attallah et al 2021 we used the trained machine learning model to label the events generated during the data collection period at the residential home we monitored while a potentially large number of analytics visualization and information can be generated from the labeled events the jupyter notebook we developed presents a small subset of them fig 4 as an example of products that can be generated from the raw data at the observed home toilet events account for 36 1 of the total indoor volume used showers 26 3 clothes washer 13 faucets 12 4 and bathtub events 11 1 unclassified events defined as events lasting 4 s or less and consisting of a single pulse recorded by the meter approximately 5 ounces or 0 15 l of water account for approximately 1 of total use unclassified events include very short water use events e g ice making refrigerators short faucet events and leaks fig 4 shows the distribution of the volume a flow rate b and duration c for each category of indoor water use unclassified events were excluded from fig 4 faucet events had a median flow rate of approximately 0 8 gpm 3 lpm water efficient bathroom faucets as defined by the united states u s environmental and protection agency epa in their water sense program epa 2020 operate between 0 8 gpm at a pressure of 20 pounds per square inches psi or 137 9 kilopascals kpa and 1 5 gpm 5 7 lpm at 60 psi 413 7 kpa compared to this epa standard the flowrates we observed from the faucets at the study property are efficient a similar conclusion can be reached by comparing the median flow rate of shower heads at the study property approximately 1 8 gpm or 6 8 lpm with epa water sense standards limiting the maximum flow rate to 2 0 gpm or 7 6 lpm in previous studies from multiple u s cities shower durations averaged 7 8 min deoreo et al 2016 the average shower duration observed at the study property was approximately 8 min with a median value of 6 3 approximately 25 of the shower durations were longer than 9 5 min fig 4 the average gallons per flush gpf for toilets at the study property was 2 78 10 5 l significantly higher than the 1 28 4 8 l recommended by the epa epa 2020 indicating there is potential for reducing water usage by retrofitting the property with water efficient toilets there is relatively little variability in the durations of toilet and clothes washer events as observed in fig 4 c for these events the characteristics are dependent on the type brand and setting used shower events reflect the largest variability as expected due to personal preferences of the different occupants of the property code to reproduce the results in this section and the raw data collected are publicly available in hydroshare bastidas pacheco et al 2021 the workflow that can be used to reproduce the results presented in this section consists of the following a influxdb is installed locally with instructions provided b the database described in table 3 is created c the database is loaded with the raw data provided using influxdb loading ipynb and then d data analytics ipynb is executed on the database producing all the results described 3 2 case study 2 pull based data collection within multi unit residential buildings for results of this case study we present only the data collection and management infrastructure required the specifics details about estimating and water related energy use estimates using the data collected are reported elsewhere by brewer 2020 the functionality of the data analytics layer is independent of the selected data communication method push or pull because the data analytics layer interacts only with the operational database given that the data collected by both case studies and the resulting database are similar the considerations for implementing the data analytics layer are equivalent to those of the first case study presented e g ability to support queries data privacy etc and the technology of the implementation would follow the same process to avoid duplication of results we have chosen not to present an implementation of the data analytics layer with this case study however similar functionalities related to this case study are discussed in our previous work brewer 2020 and available in an online data resource brewer and horsburgh 2020 3 2 1 data collection layer an enhanced version of the water meter datalogger presented by horsburgh et al 2017 was used to collect data for the variables listed in table 1 this device was named the ciws ewm logger where ewm denotes electronic water meter for the electronic output signal of the meter types it works with the ciws ewm logger was designed to be installed on commercial water meters of the types typically used in multi unit residential buildings and where a dedicated power source is readily available at the meter s location the ciws ewm logger also uses a raspberry pi 3 model b or model b linux computer running raspberry pi os the raspberry pi in this device controls the functioning of the datalogger and has integrated ethernet and wi fi capabilities for connecting to a network while operating given the location of the water meters in utility closets with no wired ethernet ports we chose to use wi fi to enable communications with the dataloggers connecting a device to usu s wi fi network requires registration of the device s hardware address after which each device is assigned a unique host name that is routable on usu s network thus each datalogger could be located and connected to within the network which allowed for remote work interactions with the datalogger for example the firmware of the loggers could be updated their functioning could be evaluated in real time and data could be pulled from them via ssh at any time while this specific configuration relies on characteristics of usu s wi fi network we anticipate that wi fi networks like usu s would be available in many application contexts the functionality described here would function identically for wired ethernet connections the ciws ewm logger was specifically modified to read the output of each of the meters available on the llc buildings along with water temperature values from three separate sensors the ciws ewm loggers we deployed can be used with any water meter or sensor that has a 4 20 ma current loop output analog voltage output digital output readable by the raspberry pi via its general purpose input output gpio ports or pulsed output the master meter octave meters provide output through a 4 20 ma current loop module where the output current is directly proportional to the flow rate through the meter the necessary transformations from current to voltage and then to flow rate were performed by the ciws ewm logger brewer 2020 and a time series of water flow in gallons per minute at a user configurable temporal resolution was generated the blmj meter outputs a pulsed signal voltage where every pulse represents a gallon of water that has passed through the meter in this case the count of pulses which equals the number of gallons was registered by the ciws ewm logger at the same user configured temporal resolution the ds18b20 digital thermometers provided digital 9 bit to 12 bit celsius temperature measurements to an accuracy of 0 5 c and were wired directly to the raspberry pi with a single wire for each sensor and do not require an external power supply the ciws ewm logger in each building logged data to a csv file that was saved in a local directory within the raspberry pi s file system for this deployment data was collected at a 1 s time interval and includes the following columns time datetime of the measurement using the yyyy mm dd hh mm ss format buildingid b c d e or f coldinflowrate coldintemp hotinflowrate hotintemp hotoutflowrate and hotouttemp with units indicated in table 1 in the quality controlled data the hot water return flow was transformed to gallons per minute for uniformity 3 2 2 data management and archival layer to support pull based data retrieval we developed an application called the data transfer manager dtm to serve as the request service shown in fig 1 it was implemented as a single python script named transfer manager py and follows the same convention used by the dps and the dls reading configuration data from a json file as in the first case study the dtm and the operational database were deployed on a vm with similar characteristics to the one described in section 3 1 2 we used influxdb as the operational database for this case study as well given the similarity in the type of data and requirements among both case studies and to show generalizability the dtm manages all data communications under the pull based model operation of the dtm was scheduled using linux s native cron functionality which allows the user to specify how often the dtm program is executed upon being triggered by the scheduled cron job the dtm first reads the configuration file described in table 5 and then proceeds through a list of defined tasks to manage transfer of data from each remote data collection site to the data management and archival layer 1 connect to each datalogger listed in the configuration file using paramiko a python library that enables ssh connections for safely accessing network services over unsecured networks forcier 2021 2 parse the datalogger s linux file system for new datalog files and download them to the server with secure file transfer protocol sftp an extension of ssh that offers secure file transfer capabilities over any reliable data stream tasks 1 and 2 in this list are executed by a function named connect in the transfer manager py python script 3 upload new data into the influxdb database this task is completed by the write to db function in the transfer manager py python script an additional function in the dtm named send error was developed to inform data managers about errors in the data transfer process errors are sent via slack a cloud based instant messaging service slack technologies 2021 messages are formulated as a json payload that is sent to a unique url provided by slack as a webhook information detailing which datalogger file caused the error is included in the message fig 5 describes the overall functionality of the dtm indicating the key tasks mentioned for this case study data transferring and parsing are executed by a single element transfer manager py which requires fewer moving parts and minimizes the amount of time between the data being retrieved from the remote dataloggers and having them show up in the operational influxdb database this is a slightly different approach than the one presented for case study 1 which allows more flexibility in the system the dtm can work concurrently on a user defined number of datalogger devices at the same time connections in table 5 the optimal number of threads is dependent on the number of cpu cores of the server for our testing we set the number of threads to 6 matching the number of dataloggers in the llc buildings as in the first case study the raw data and quality controlled data were stored in the same influxdb database in different measurements brewer 2020 describes the quality control procedures for the data collected in this case study the database schema used for this case study is similar in structure to that of the first case study the data included in the database copies all columns from the csv files recorded by the dataloggers buildingid serves as the siteid and is the only column stored as a tag all additional variables the recorded data values for each variable are stored as fields 3 3 scalability and performance metrics while we experienced no performance issues in the case study deployments we performed scalability testing to investigate the performance of the system beyond the scale of our case studies we conducted individual tests of the dps the dls and the dtm simulating larger numbers of dataloggers and http post requests in the case of the dps and dls and a larger number of remote datalogger hosts in the case of the dtm to be processed by the system scalability of the dps is dependent upon its ability to handle many http post requests from many dataloggers posting data at the same time the dps was tested by sending multiple http post requests each with a csv file containing one day of randomly generated data with values recorded every 4 s for consistency with the implementation of case study 1 the files were sent using a python script implemented using the asyncio library python software foundation 2021a from a macbook pro laptop computer with a 2 3 ghz 8 core intel core i9 processor and 16 gb of memory asyncio is a library that can be used to write code that executes concurrently allowing the code to send multiple simultaneous or nearly simultaneous requests to the dps there are limitations in the number of concurrent requests that can be sent from the same computer as well as in the number of dataloggers that can send data at the exact same time in a filed deployment considering computing power speed of connection and synchronization we simulated an increasing number of concurrent http post requests to the dps 10 50 100 200 and finally 500 and each operation was repeated ten times to characterize server network variability the total duration of each repetition calculated as the end time of the last http post request minus the start time of the first request on average was 0 6 s 2 05 s 3 58 6 91 s and 16 7 s for 10 50 100 200 and 500 requests respectively we observed no transmission errors or requests rejected by the server during our testing process fig 6 shows the durations of http post requests separated by the batch size 10 50 100 200 and 500 for each one of the 10 repetitions conducted we observed that the median duration of post requests was larger for the 10 request batches compared to all other batches but longer durations were observed for some requests in larger batches which is expected as the dps is busy with an increasing number of requests median times are consistent for batches with more than 50 post requests these times are affected by the processing power of the machine sending the request the resources available on the remote server and the speed and quality of the internet connection but are provided here as an indicator of the performance of our prototype implementation these tests indicate that the dps can handle 500 nearly simultaneous post requests in under 20 s with most individual requests being handled in under 0 2 s to test the dls we simulated different data loading scenarios ranging from loading one csv file for a single site to loading one file for 500 sites the testing procedure consisted of placing csv files containing one day of data with values recorded every 4 s in the source directory and then executing the dls each operation was repeated ten times table 6 presents the mean and standard deviation of each scenario along with the average time for loading a single file to facilitate comparisons the dls can load 1 day of data from 100 different sites in less than 50 s there are differences between loading n files from the same site and loading 1 file from n sites which can be explained by the way data are organized within the influxdb database although all of the data values are stored in the same influxdb measurement influxdb logically groups data values by shared measurement tag set and field key writing data with multiple siteid tag values takes longer both scenarios are realistic applications the first scenario n files from 1 site simulates loading data collected from dataloggers lacking communication technologies the second scenario 1 file from n sites represents a deployment like the one described in case study 1 with a larger number of sites we used the six dataloggers described in case study 2 to test the dtm each data logger sent 1 day of data during all tests the functionality that allows the system to identify existing data or files was removed allowing the system to upload existing csv files and re write existing data to the influxdb without restrictions this configuration enabled us to simulate a larger number of connections by repeating dataloggers in the hosts list included in the dtm configuration file described in table 5 the number of dataloggers was gradually increased 6 48 96 and finally 480 and the dtm was executed ten times for each number of dataloggers processing one csv file containing one day of 1 s resolution data for each datalogger the dtm was set to execute six threads at a time meaning that it can be simultaneously connected to and downloading data from six dataloggers at a time for consistency with the application of case study 2 during our testing only 6 dataloggers were available which meant that it was possible for the dtm to attempt connecting to and processing data from the same logger multiple times simultaneously this can negatively affect the time reported if a host is not immediately available for processing when the system is trying to connect to it table 7 lists the duration and standard deviation after ten runs with an increasing number of datalogger hosts using our test configuration it took less than 50 min for the dtm to process data from 480 hosts we tested the system up to and with much larger numbers than the 40 60 sites in our design considerations and observed no real limitations for using ciws in deployments roughly an order of magnitude larger even with our relatively limited testing server the dls and the dtm include writing to the database as part of their tasks and the times observed satisfy the stated requirements for our application as a final test we tested the database by conducting standard queries from a python environment using the same laptop computer we observed the amount of time required to downloaded one day one week and one month of data for 1 5 and 10 sites along with the time required to load the data into a pandas dataframe object table 8 all queries were conducted using the function get data described on table 4 the timeit python module python software foundation 2021b was used to repeat each query 10 times and measure execution times downloading one month of data a common record length in studies collecting high resolution residential water use data for ten sites into a pandas dataframe takes less than 1 min the log files and code to reproduce all the results of this section are publicly available in hydroshare bastidas pacheco et al 2021 the cost of deploying ciws to support data collection at residential houses using the equipment described for case study 1 can be broken down as follows a the cost of ciws node datalogger devices which is approximately 180 multiplied by the number of houses to be enrolled simultaneously and b the cost of hosting a server with characteristics similar to our testing server 4 processor cores 8 gb of memory 100 gb of storage at the time of this writing hosting this machine using the amazon elastic compute cloud would cost approximately 57 per month amazon 2021 although there are multiple hosting alternatives for the server that could be used and that would impact the cost estimate provided the approximated cost of building the datalogger device used in case study 2 is 85 4 conclusions and future work a complete cyberinfrastructure system that uses a layered approach to collect and manage high temporal resolution water use data was developed and implemented the system was designed focusing on the scale of data collection that would be required for research projects conducted by utilities or other researchers having a standardized cyberinfrastructure like ciws can increase the value of the data collected by allowing more straightforward data collection and management as well as facilitating the analysis and understanding of data collected in different projects cities and utilities ciws can be used to manage data collected or used for multiple purposes e g collecting data to support estimates of design parameters for future home developments guiding the planning of water conservation campaigns assessing the effectiveness of rebate programs assisting in the definition of utility rates and defining future demand and infrastructure needs our case studies showed that ciws can work with any datalogging devices that generate csv files containing time series of water use data but it can also be used in the collection of other variables as demonstrated in experimental use case 2 by integrating low cost data collection devices and open source cyberinfrastructure we sought to increase the accessibility of tools for conducting high temporal resolution data collection in support of residential water use studies ciws can reduce not only the cost of such studies but also technical barriers by providing a framework to collect and manage the data ciws can manage push and pull based data communication since each functionality is implemented separately future users of ciws can select push or pull or a combination of both depending on the needs and settings of their application the work performed within the data management and archival layer depends on whether the push or pull model is used in the pull case the data is pulled from the device by a request service whereas in the push case the data is managed by a network listener web service that accepts incoming files and processes them both use the same database component which means that the data analytics layer can operate independent of how the data are transferred the demonstrations we presented of the data analytics layer serve as a proof of concept and show the foundation upon which more sophisticated tools could be built that can be used to communicate results with multiple interested parties we focused our design and implementation on a system that is capable of transferring high temporal resolution water use data from water meters to a centralized infrastructure for storage and subsequent analysis in a research context this is preferable as researchers may not know at the outset of a study all of the specific analyses they may want to perform with the data and thus keeping all of the data is necessary however transferring large volumes of data to a centralized data management system poses challenges when scaling a system like this to larger deployments while technically possible over wi fi or cellular data networks the availability of wi fi is limited and cost of data transfer over a cellular data network may be prohibitive as an alternative we are now investigating edge computing techniques using our ciws wm node datalogger to process the high resolution water use data on the logger to produce summary data products that are much smaller and can be transferred over a network with far less bandwidth and at lower cost the tradeoff is that the full resolution data are never transferred or saved in the long term ciws combines multiple open source technologies the modular design makes it easier to replace or update technology elements in the system if needed similarly additional tools can be added to system e g more advanced analytics tools and enhanced authentication protocols the analytics presented show potential for conservation programs and can assist in the design of future urban water infrastructure all of the components we developed are publicly available for reuse and we envision future improvements to the system once the tools are used in other studies the system testing performance metrics and deployment demonstrate that ciws can meet and significantly exceed the design considerations in terms of scale and performance we saw no impediment for using ciws or a similar system in larger deployments than the ones tested by increasing the processing power of the virtual machine or deploying multiple instances the server we used for testing had only moderate system specifications and could either be run on private server hardware or could easily be hosted within a commercial cloud service provider at a reasonable monthly cost software and data availability name of software cyberinfrastructure for intelligent water supply ciws developers camilo j bastidas pacheco joseph c brewer jeffery s horsburgh juan caraballo elijah west contact jeff horsburgh usu edu year first available 2021 required hardware and software we used open source dataloggers for the data collection efforts in this study datalogger hardware details are provided by bastidas pacheco et al 2020 and horsburgh et al 2017 data management and archival components of ciws were designed to run on a linux server and were tested using ubuntu the data analytics components we demonstrate require a computer running the windows linux or macintosh operating system instructions for how to deploy the system are available in the project s github repository availability source code for the data management and archival layer software components described in this manuscript is freely available and can be downloaded from the ciws server github repository https github com uchic ciws server the src folder in that repository contains a folder named ciws ci and a folder named data transfer manager where the elements related to case study 1 and case study 2 are located respectively the doc folder contains a deployment guide for ciws the data described in case study 1 and the source code of the data analytics layer software are publicly available in hydroshare bastidas pacheco et al 2021 with instructions for reproducing the results presented in that section the data described in case study 2 and tools used to analyze it are also publicly available in hydroshare brewer and horsburgh 2020 the log files from section 3 3 scalability and performance metrics and code used to generate the results presented are available in hydroshare bastidas pacheco et al 2021 design files instructions for assembly and firmware for the open source dataloggers are available on the github sites for the ciws water meter node datalogger https github com uchic ciws wm node and the ciws electronic output water meter datalogger https github com uchic ciws ewm logger declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the united states national science foundation under grant number 1552444 any opinions findings and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the national science foundation we would like to acknowledge providence city and utah state university housing and facilities for their cooperation and support in the data collection efforts the authors would also like to acknowledge support from nour atallah arle j beckwith and rob j tracy in the data collection efforts and elijah west for his contribution in software development we also acknowledge and thank the owner of the residential home and the students in the llc buildings that participated in the data collection campaign 
25762,geographic models are becoming key methods for geographic environment research in addition to the progress in model sharing and reusing geographic simulation geo simulation knowledge sharing and application are increasingly attracting much attention this study aims to provide both models and geo simulation knowledge simultaneously to support web based simulation firstly we propose a geo simulation approach consisting of three modules concerning data model and knowledge with a conceptual framework secondly taking the storm water management model as instance a web based prototype system is implemented with geo simulation knowledge graph constructed based on literatures using bilateral long shortterm memory conditional random field bilstm crf thirdly application of the system in nanchong china is conducted with user experiences analysis the results demonstrate that the proposed approach can effectively provide geo simulation knowledge moreover it can alleviate the difficulty of the geographic simulation process for diverse modelers and further improve the use of the model simultaneously keywords web based geographic simulation geographic simulation knowledge knowledge graph model services storm water management model swmm 1 introduction the geographic environment is a comprehensive and sophisticated system consisting of various hydrological atmospheric ecologic geomorphic and human impacted dynamic processes and their interactions matthews and herbert 2008 walker and chapra 2014 kay et al 2015 pande and sivapalan 2017 tao et al 2019 to simulate and explore such a complex system dynamic model based geographic simulation i e called geo simulation as a new generation of geographic analysis tools has been widely applied lin et al 2013 goodchild 2018 lin et al 2020 li et al 2021 improvement of geo simulation capacity has been further extensively carried out to promote the reuse and sharing of geographic models and data an approach of service oriented architecture soa has become one of the significant methods for the model sharing work via the web goodall et al 2011 granell et al 2010 nativi et al 2013 belete et al 2017 subsequently in recent years a significant amount of interdisciplinary works in soa has been conducted to share geographic model resources on the web wang et al 2018 zhang et al 2018 yue et al 2019 choi et al 2021 however it is still challenging for modeler to use the developed interdisiciplinary models in the geo simulation effectively because the computational cost of simulation to acquire essential knowledge is relatively high sancho jiménez et al 2008 zhang et al 2018 chen et al 2020 nativi et al 2020 subsequently effective sharing and reusing of geo simulation knowledge e g model used simulation parameters and validation results is urgently needed to carry out geo simulation golledge 2002 mauser et al 2013 chen et al 2021 such geo simulation knowledge includes not only the model itself but also the simulation process information such as simulation parameters and simulation results palomino et al 2017 in addition to modeling management lu 2011 zhang et al 2015 2017 zhang et al 2017a b the collaborative geo simulation is increasingly accepted and recognized as an effective approach to promote sharing and reusing of geo simulation knowledge palomino et al 2017 chen et al 2020 several investigations have been carried out to develop collaborative geo simulation approaches for knowledge sharing and reuse e g chen et al 2019 bandaragoda et al 2019 and chen et al 2020 such collaborative geo simulation involves multidisciplinary experts to collaboratively contribute their knowledge to the entire geo simulation process for reaching a consensus on the specific geo problems schaeffer and foerster 2008 chen et al 2010 lin et al 2020 however following significant challenges are still existed first simultaneously involvement of multidisciplinary experts in a collaborative simulation event is difficult to be implemented specifically for the modelers having limited cooperation across disciplines athanasiadis 2017 elsawah et al 2020 in addition the involvement of experts may still produce bias in geo simulation due to different epistemological and knowledge backgrounds leading to different perceptions of geo problem verburg et al 2016 voinov et al 2018 to tack such an issue extensive literature research of geo simulation in model setting e g input data model parameters and model conditions relevant to the case study is carried out to reduce the uncertainty of geo simulation zhang et al 2016 elsawah et al 2020 w l li et al 2020 second the process of the collaborative geo simulation reflects multidisciplianry human expert knowledge and ideas whereas such unstructured knowledge is difficult to be described transformed and further reused by other researchers due to the lack of web based knowledge management zhang et al 2017a b fortunately knowledge graphs as a form of structured human knowledge have drawn great research attention from both academia and the industry nickel et al 2016 wang et al 2017 it has been gradually implemented into geographic modelling to reduce the complexity and difficulty of geo simulation j y li et al 2020 zhang et al 2020 however integration of web based models and structured geo simulation knowledge has not been investigated extensively to improve geo simulation models and knowledge sharing simultaneously subsequently this study aims to introduce an innovative integrating geo simulation approach of knowledge graph and model services to establish a geo simulation environment on the web the remainder of this paper is structured as follows the modules concerning data model and knowledge for the geo simulation approach with a conceptual framework is described in section 2 applying this approach section 3 introduces the design and implementation of the prototype system using storm water management model swmm rossman and huber 2016 and corresponding simulation knowledge graph in section 4 a related case study to domonstrate the feasibility and applicability of the approach and the system is then presented conclusions and pointers to future work are discussed in section 5 2 methodology 2 1 modules design of the geo simulation approach the approach is designed based on three modules concerning data model and knowledge fig 1 first model database mdb is designed to store and manage input data simulation parameters and simulation results second a collection of model services is provided to support geo simulation including model input configuration modification execution third geo simulation knowledge graph an innovative advantage of the approach provides modelers with structured simulation knowledge transformed from massive unstructured information e g published literature web page information and specialist experience such a process may alleviate the complexity and difficulty of geographic simulation furthermore three task flows fig 1 tightly integrate the modules through seamless data transmission to construct a geo simulation environment on the web fig 1 presents the procedures to carry out the geo simulation three tasks i e data acquisition and storage model services request and respond and simulation knowledge graphs request and respond are involved in the data acquisition and storage task pre processed input data is stored into mdb as simulation data task 1 1 storing input data in mbd is used to simulate when invoking model services that is responsible for providing simulation computing sub service of simulation environment when a modeler sends a hypertext transfer protocol http request to the simulation environment task 2 1 the simulation environment subsequently sends an http request to model services and the computing sub service will start immediately task 2 2 at this stage the modellers are allowed to obtain structured simulation knowledge through reviewing knowledge graph provided in simulation environment such a process will alleviate the complexity and difficulty of the simulation works task 3 the results and simulation parameters are further stored in the output data to ensure the traceability of the simulations task 1 2 those data can be obtained from the mdb when a user make request in the simulation environment task 1 1 in addition this proposed approach may have various physical implementations where different technologies may serve the specific needs for the geographic and environmental research field therefore we use the swmm model simulation as an implementation case study to establish a prototype system to demonstrate the feasibility of the approach in section 3 details of the conceptual framework are further described in the following section 2 2 a conceptual framework of the approach for geo simulation the framework of the proposed approach mainly consists of three layers model database layer knowledge graph layer and model services layer see fig 2 first the model database layer fig 2 is essential to support the comprehensive organization and management of the simulation resources in addition the model database pays more attention to the simulation parameters and the corresponding simulation output management than the classical spatial database systems such data and information is helpful for extending the application of the model services and simulation reproduction second the knowledge graph layer fig 2 is responsible for constructing and applying geo simulation knowledge in this study the geo simulation knowledge has been developed from extensive literature studies needing to be extracted and organized into structured knowledge for multidisciplinary modelers using symbols such a knowledge graph describes the relevant attribute value and their relationships in the geographic environment to carry out this task entities are connected through relationships forming a network of knowledge structures using a storm water management model swmm simulation as an example implementation the extraction of simulation knowledge and the construction of the corresponding knowledge graph are further described in section 3 third the model services layer fig 2 represents a series of web based methods that can be invoked through http requests using the model services layer modelers can use an http put method to send simulation parameters and request model to run several advantages can be acquired using such a method including 1 produce personalization and flexibility in functions customization to meet the needs of modelers 2 eliminate many unnecessary procedures in model use 3 reuse model code and computing resources to work more effectively finally the geo simulation environment fig 2 can be regarded as an integrated workspace for implementing the sharing and reuse of geo simulation knowledge and models in this approach the simulation knowledge and model resources of the integrated geo simulation are distributed in the internet and hence the geo simulation environment should first take the collaborative execution strategies of the knowledge and model services into consideration moreover web based geo simulation and visual methods should help modelers to control the simulation process and intuitively understand the simulation results last but equally important open knowledge exchange and sharing must be part of the design process to encourage broad participation e g students researchers domain experts and other people interested in specific geographic issues 3 implementation of the approach as a prototype system with swmm adopting from the case of frequently urban flood disasters storm water management model swmm version 5 1 is adopted in the prototype system to simulate precipitation events in urban areas three basic modules are generated to adopt such model fig 3 i a set of swmm model services ii urban hydrological simulation knowledge graphs and iii a model database the prototype system interface provides a user friendly gui for modelers from multiple disciplines with different knowledge backgrounds including students researchers stakeholders and the public with the help of the simulation knowledge graph the gui provides users with a panel to set the model simulation parameters in addition a simulation panel is developed to simulate and visualize various model output data to understand urban stormwater management 3 1 design and develop the swmm model web services in the swmm model the urban drainage system is abstracted into a set of physical components including junctions outlets conduits and subcatchments the hydrological behavior of urban areas is then simulated by those components in this study we designed and implemented a set of model web services to provide computing support for the prototype system by analyzing the swmm source code structure and workflow the swmm source code structure fig 4 can be divided into five types of modules 1 main program module 2 hydrodynamics module 3 water quality module 4 general function module and 5 other modules in addition the swmm consists of 53 c code files in total and each of the modules consists of a series of related c code files the main program module is used to control and record the entire swmm simulation process i e manipulating model configuration files and initializing module by calling swmm open swmm start method after the initialization the module is completed the main program module is responsible for the calculation of the hydrodynamics module and water quality module respectively using swmm run method once an swmm model run has been completed the output c and report c code files in the general function module are responsible for recording the simulation results of the hydrodynamics module and the water quality module to files with the file extensions out and rpt the out binary file records the results of swmm time series whereas the rpt file is an extension file to store the summary results of swmm simulation finalizing the above analysis of the swmm source code and workflow secondary development of the swmm source code is conducted to compile and encapsulate the source code into a dynamic link library swmm5 dll for developers to invoke a set of swmm web services is further implemented by invoking the external functions exposed by the swmm numerical engine swmm5 dll fig 5 shows eight web service methods for the prototype system to invoke the following information can be obtained from the invoking including 1 the summary information and swmm simulation results of swmm 2 the length of the simulation period 3 and the number of modeling objects e g junctions outfalls links and sub catchments in addition three basic functions can be implemented by invoking these methods as presented in fig 5 1 manipulate model configuration files to modify the input parameters 2 run swmm 3 obtain the simulation time series results from the binary output file to store in mdb the development frameworks and tools used to develop the swmm web services are listed in table 1 and the model database design is shown in fig 6 3 2 model database design for simulation management mdb is developed to storing and managing swmm input data simulation parameters and results in this study we designed the model database in physical mode based on the conceptual and logical modes the essential table linked by foreign keys of the database consists of several tables 1 expert simulation parameters 2 user simulation parameters and 3 user simulation results such design is valuable for the multidisciplinary modelers to acquire relevant urban hydrological simulation knowledge from experts through their simulation information in addition modelers can trace their history behaviors by accessing tables of user simulation parameters to further improve and adjust their geo simulation results 3 3 construction of the simulation knowledge graph construction of the simulation knowledge graph in the hydrological simulation is carried out using the following three steps fig 7 1 preparation and construction of knowledge source and ontology library respectively 2 acquisition of hydrological simulation knowledge and 3 construction of urban hydrological simulation knowledge graph in the first step the knowledge source for the construction of the simulation knowledge graph is collected using the semi structured information on the web pages and the existing literature related to urban hydrological simulation the ontology library of hydrological simulation is further constructed to defines the data pattern in the process of knowledge acquisition and graph construction the ontology mapping rules of the urban hydrological simulation knowledge graph consisting of 5 core elements are expressed as ontology literatureinformation parametermeaning parametersetting scenesetting resultevaluation relation literatureinformation stores the data of title authors institutions impact factor and keywords of the literature parametermeaning interpretates the parameters in the literature parametersetting refers to the parametric scheme of the simulation model scenesetting provides essential simulation information e g study area location simulation scopes simulation time and scene variables resultevaluation refers to the evaluations ways of the simulation result such as model error assessment and station based validation and relation refers to the semantic relationship of these elements in the second steps the web crawler was employed to parse the hypertext markup language html codes of the web pages to obtain the literature information and store them in the structured data sheet for knowledge graph construction for the case where the knowledge is unstructured in the literature a knowledge extraction model based on bilateral long shortterm memory conditional random field bilstm crf is introduced to recognize and extract the simulation knowledge referring to xiao and zhang 2021 for details subsequently the fusion of knowledge from diverse sources was conducted and the urban hydrological simulation knowledge graph was ultimately constructed and stored in neo4j https neo4j com an instance of urban hydrological simulation knowledge graph is shown in fig 8 it clearly shows the knowledge nodes belonging to the various elements and the relationship between them the simulation knowledge is classified into different stages e g literature information parameter meaning scene setting parameter setting according to different categories and roles to obtain simulation knowledge on demand moreover the knowledge retrieval function is provided in the system to enable users to quickly obtain the simulation knowledge that they need 4 application of the system in nanchong china 4 1 study area and data an urban area located in the shunqing district of nanchong city in china was selected as a test case for the prototype system it is the central city of nanchong located in the northeast of the sichuan basin on the west bank of the middle reaches of the jialing river the coordinate is between 106 03 e to 107 07 e latitude and 30 41 n to 30 51 n longitude precipitation mostly occurs in summer and autumn due to the influence of a monsoon climate the long term annual mean temperature and precipitation are 17 4 c and 1020 8 mm respectively the total study area is approximately 33 02 km 2 see fig 9 a and is classified into five categories including buildings rivers green spaces agricultural areas and city areas the city areas almost cover the entire study area with typical infrastructures of transportation networks municipal squares institutions and commercial districts the drainage system is a combined system consisting of links junction and outfall fig 9 b however the drainage system in the city areas is independent of other basins or hydrological systems this means that the drainage system only collects rainwater from the city areas and no external springs contributing to the surface runoff ideally moreover for the swmm simulation the meteorological data is collected and recorded by a weather station to carry out the swmm simulation in the case study four essential data i e land cover resolution of 30 m 30 m digital elevation model dem resolution of 30 m 30 m and rainfall data were collected from nanchong meteorological bureau primary data on drainage systems were also collected from the local data authorities provideing the records of drainage networks and related parameters i e pipe diameter material length and shape those data were then preprocessed to meet the requirements of swmm simulation first the voronoid diagram is adopted in processing the dem data and junction distribution to subdivide and define the sub catchments consequently each subarea discharges into a single junction or well for the subsequent simulations second junction outfall and link are in the form of cad computer aided design file therefore the data need to be digitized into geojson geographic javascript object notation format geojson is a lightweight and concise language for data analysis transmission and visualization on the web fig 9 b shows that the entire drainage system consists of 9971 junctions 57 outfalls and 9520 links finally several adjustments were made based on the actual engineering situation the rainfall event data fig 10 were collected at the weather station within the period from 8 00 a m on june 27 2020 to 00 00 a m on june 28 2020 and measured with time intervals of 1 h in this study the simulation results were macroscopically evaluated based on the approximate location of the overflow node during the rainfall event provided by the local meteorological office due to the limited data provided in addition the drainage system is assumed to have no inflows fromother basins and thus the model is simulated with only direct rainfall inputs 4 2 configuration and implementation the prototype system on the web was developed based on arcgis api for javascript 4 18 https developers arcgis com javascript latest the terminal was equipped with an intel core i7 7700hq cpu 2 8 ghz with 32 gb of memory and an nvidia geforce gtx1050 gpu with 2 gb of memory the software installed on the terminal included a windows 10 os chrome navigator iis server neo4j graph database and mysql https www mysql com the development environment was webstorm2020 ide https www jetbrains com webstorm and vs2017 ide among them mysql a relational database management system was used to store data from swmm simulation in addition neo4j graph database was used to store and visualize the knowledge graph of the geographic simulation knowledge 4 3 results and discussion 4 3 1 system validation fig 11 fully demonstrates the running process of the prototype system from three aspects data model and knowledge three essential lines in fig 11 i e the blue the red and the yellow dotted lines represent the data flow to show the conversion and transmission of model input and output during the running process of the system the model flow of the running process mainly responsible for input running and output of the model and the knowledge flow respectively the figure indicates that the prototype system shows a 17 h rain event with the simulation duration and time step of 23 h and 5 min respectively to run the prototype system first the rainfall fig 10 and drainage system data fig 9 b are obtained from the model database and then generated a model input file for driving the swmm the entire drainage system was also converted to geojson format data to display on the web after completing the data input preparation it was necessary to calibrate and understand the sensitivity of the hydrological parameters e g impermeability and roughness to improve the reliability of geo simulation work consequently the system needs to modify and reset the simulation parameters to update the model input file with the help of simulation knowledge illustrated by various simulation knowledge subgraphs such a task may produce ideal simulation results these knowledge subgraphs stored in the neo4j graph database were then exported as json javascript object notation format files the json file is a lightweight data format for data transmission and parsing this file is then visualized in the knowledge pane the above function was implemented by invoking service a modify input method in the model web services subsequently the system ran swmm through invoking service b run swmm method and then produce two output files of out and rpt thus the system needed to invoke service c get results method to parse the binary simulation results and finally stored them in the mdb fig 12 a presents a simulated snapshot of the study area at 9 00 a m on june 27 2020 fig 12 b exhibits the process of flow rate within nearly 17 h at each of the 5 conduits fig 12 c shows the process of water depth within nearly 17 h at each of the 5 junctions the results accurately reflect the real time status information of links junctions and outfalls in the drainage system such as the overflow and overload status and drainage capacity of the pipe network such data can provide guidance for the related stakeholder to define a reliable decision in optimizing and transforming the urban drainage system finally during the simulation the system performed a dynamic visualization of the flow and capacity output values of the drainage system using the simulation results such a visualization is performed during the rain event in addition with the help of scene setting subgraph the users load different scene objects e g link and junction and simulate various output values this is to construct swmm simulation scenes for better understanding and analyzing the simulation results the status report recording the junctions where the overflow and the overloaded links have occurred was also shown in the simulation pane this report provides sufficient data for modelers to conduct a macro verification of the simulation results thus if the simulation results cannot meet the requirements defined by the modelers they can trace the historical behaviors from the mdb and re adjust the work with the help of various simulation knowledge subgraphs to improve their swmm model at this point the prototype system had been successfully run once running results show that all tests are being performed following to the design scheme 4 3 2 user experience of the prototype system participants are from our college our college is a research university with geology resources and environment as its main characteristics and outside showing great interest in geographic simulation research or urban stormwater management a total of 30 participants with varying degrees of knowledge and backgrounds were recruited including 10 public people 10 undergraduates 8 postgraduates and 2 professors the authors briefly introduced the prototype system before allowing them to use the prototype system the questionnaire responses were then collected after the participants used the system the rating of the questionnaire is on 10 point scale i e 1 for don t agree and 10 for totally agree in addition the experiment recorded the selection preferences of each participant such as different knowledge subgraphs simulation processes and the experimental results for four types of users and are analyzed as follows first the postgraduate user type has clear research goals and hence so their selections are usually more focused this type of participant only selected the knowledge subgraphs related to their own needs e g parameter meaning and parameter setting the selection preferences of this user type are shown in fig 13 second the knowledge subgraphs selected by the public people user type are often confused because they are completely unfamiliar with the field this results in the selections of approximately three users at the beginning of the experiment being completely random as the experiment progresses such users can gradually perform the swmm simulation under the help of the parameter setting and scene setting subgraph the selection preferences of this user type are shown in fig 14 the selection preferences of the undergraduate user type are similar to the postgraduate user type as shown in fig 15 a such users conveniently complete the work according to the simulation knowledge provided by the simulation knowledge graph the users also paid more attention to the information e g the location and status of junction overflow to improve their cognition and understanding of urban rainwater management the professor user type has relatively little demand for the simulation knowledge because such users have professional simulation knowledge run excellent simulation the selection preferences of this user type are shown in fig 15 b selected key questions in our survey are further listed in table 2 developed from the survey results the table shows that 26 participants would first refer to the simulation knowledge graph provided by the prototype system to set their simulation schemes moreover the scores for questions 4 and 5 are still larger than 6 the average scores still support the conclusion that the structured simulation knowledge is useful for users who lack relevant knowledge to alleviate the complexity and difficulty of geo simulation and hence promote the sharing of models and knowledge simultaneously 5 conclusions and future work with the increasing complexity and comprehensiveness of geographic science issues the consideration of sharing and reuse of geo simulation knowledge in geo simulation has become extremely important such a consideration advances the multi domain sharing and application of the models this paper successfully developed an innovative approach using geo simulation knowledge from massive unstructured literature and web pages to help modelers quickly recognize models to support broader participation and applications we have demonstrated how geo simulation work can be effectively completed with the help of geo simulation knowledge through the implementation of the prototype system and the experimental study the main contributions of this study are concluded as follows proposing an integrated innovative approach of knowledge graph and model services for effectively sharing and reusing models and geo simulation knowledge such an approach also demonstrated the effectiveness and feasibility of the approach through designing a prototype system on the web taking the swmm model as an example the simulation knowledge graphs were constructed and stored by neo4j and the applicability and reliability of these simulation knowledge were verified through user experience experiment and questionnaire survey the system effectively alleviated the complexity and difficulty of swmm simulation by providing simulation knowledge reduced the threshold for model use which helps to improve the capabilities of geo simulation potentially despite the achievements described above the following aspects still need improvement in the future work first the procedures of geo simulation knowledge recommendation may need to be considered to provide modelers with more intelligent and friendly knowledge services in the future second in order to further support multi field knowledge exchange and sharing collaborative simulation by interdisciplinary and geographically separated users will be considered in the next stage of the modelling system zhang et al 2015 chen et al 2019 2020 furthermore we will conduct more comprehensive research based on this approach including a sensitivity analysis of simulation parameters and station based validation to provide a more open and intelligent geo simulation environment for modelers from different fields software and data availability program title the prototype system swmm developer heng li contact address 2004200036 cugb edu cn source code available and data https github com liheng gis the prototype system swmm git program required internet browser tested on chrome and firefox iis7 net 4 6 1 source code for the swmm 5 1 014 webstorm 2020 2 3 visual studio 2017 year first available 2021 program language javascript c availability and cost open source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the fundamental research funds for the central universities no 2652018082 and the national natural science foundation of china no 72033005 the data support from nanchong meteorologic bureau are gratefully acknowledged given their roles as environmental modelling software editor min chen was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor dr daniel p ames 
25762,geographic models are becoming key methods for geographic environment research in addition to the progress in model sharing and reusing geographic simulation geo simulation knowledge sharing and application are increasingly attracting much attention this study aims to provide both models and geo simulation knowledge simultaneously to support web based simulation firstly we propose a geo simulation approach consisting of three modules concerning data model and knowledge with a conceptual framework secondly taking the storm water management model as instance a web based prototype system is implemented with geo simulation knowledge graph constructed based on literatures using bilateral long shortterm memory conditional random field bilstm crf thirdly application of the system in nanchong china is conducted with user experiences analysis the results demonstrate that the proposed approach can effectively provide geo simulation knowledge moreover it can alleviate the difficulty of the geographic simulation process for diverse modelers and further improve the use of the model simultaneously keywords web based geographic simulation geographic simulation knowledge knowledge graph model services storm water management model swmm 1 introduction the geographic environment is a comprehensive and sophisticated system consisting of various hydrological atmospheric ecologic geomorphic and human impacted dynamic processes and their interactions matthews and herbert 2008 walker and chapra 2014 kay et al 2015 pande and sivapalan 2017 tao et al 2019 to simulate and explore such a complex system dynamic model based geographic simulation i e called geo simulation as a new generation of geographic analysis tools has been widely applied lin et al 2013 goodchild 2018 lin et al 2020 li et al 2021 improvement of geo simulation capacity has been further extensively carried out to promote the reuse and sharing of geographic models and data an approach of service oriented architecture soa has become one of the significant methods for the model sharing work via the web goodall et al 2011 granell et al 2010 nativi et al 2013 belete et al 2017 subsequently in recent years a significant amount of interdisciplinary works in soa has been conducted to share geographic model resources on the web wang et al 2018 zhang et al 2018 yue et al 2019 choi et al 2021 however it is still challenging for modeler to use the developed interdisiciplinary models in the geo simulation effectively because the computational cost of simulation to acquire essential knowledge is relatively high sancho jiménez et al 2008 zhang et al 2018 chen et al 2020 nativi et al 2020 subsequently effective sharing and reusing of geo simulation knowledge e g model used simulation parameters and validation results is urgently needed to carry out geo simulation golledge 2002 mauser et al 2013 chen et al 2021 such geo simulation knowledge includes not only the model itself but also the simulation process information such as simulation parameters and simulation results palomino et al 2017 in addition to modeling management lu 2011 zhang et al 2015 2017 zhang et al 2017a b the collaborative geo simulation is increasingly accepted and recognized as an effective approach to promote sharing and reusing of geo simulation knowledge palomino et al 2017 chen et al 2020 several investigations have been carried out to develop collaborative geo simulation approaches for knowledge sharing and reuse e g chen et al 2019 bandaragoda et al 2019 and chen et al 2020 such collaborative geo simulation involves multidisciplinary experts to collaboratively contribute their knowledge to the entire geo simulation process for reaching a consensus on the specific geo problems schaeffer and foerster 2008 chen et al 2010 lin et al 2020 however following significant challenges are still existed first simultaneously involvement of multidisciplinary experts in a collaborative simulation event is difficult to be implemented specifically for the modelers having limited cooperation across disciplines athanasiadis 2017 elsawah et al 2020 in addition the involvement of experts may still produce bias in geo simulation due to different epistemological and knowledge backgrounds leading to different perceptions of geo problem verburg et al 2016 voinov et al 2018 to tack such an issue extensive literature research of geo simulation in model setting e g input data model parameters and model conditions relevant to the case study is carried out to reduce the uncertainty of geo simulation zhang et al 2016 elsawah et al 2020 w l li et al 2020 second the process of the collaborative geo simulation reflects multidisciplianry human expert knowledge and ideas whereas such unstructured knowledge is difficult to be described transformed and further reused by other researchers due to the lack of web based knowledge management zhang et al 2017a b fortunately knowledge graphs as a form of structured human knowledge have drawn great research attention from both academia and the industry nickel et al 2016 wang et al 2017 it has been gradually implemented into geographic modelling to reduce the complexity and difficulty of geo simulation j y li et al 2020 zhang et al 2020 however integration of web based models and structured geo simulation knowledge has not been investigated extensively to improve geo simulation models and knowledge sharing simultaneously subsequently this study aims to introduce an innovative integrating geo simulation approach of knowledge graph and model services to establish a geo simulation environment on the web the remainder of this paper is structured as follows the modules concerning data model and knowledge for the geo simulation approach with a conceptual framework is described in section 2 applying this approach section 3 introduces the design and implementation of the prototype system using storm water management model swmm rossman and huber 2016 and corresponding simulation knowledge graph in section 4 a related case study to domonstrate the feasibility and applicability of the approach and the system is then presented conclusions and pointers to future work are discussed in section 5 2 methodology 2 1 modules design of the geo simulation approach the approach is designed based on three modules concerning data model and knowledge fig 1 first model database mdb is designed to store and manage input data simulation parameters and simulation results second a collection of model services is provided to support geo simulation including model input configuration modification execution third geo simulation knowledge graph an innovative advantage of the approach provides modelers with structured simulation knowledge transformed from massive unstructured information e g published literature web page information and specialist experience such a process may alleviate the complexity and difficulty of geographic simulation furthermore three task flows fig 1 tightly integrate the modules through seamless data transmission to construct a geo simulation environment on the web fig 1 presents the procedures to carry out the geo simulation three tasks i e data acquisition and storage model services request and respond and simulation knowledge graphs request and respond are involved in the data acquisition and storage task pre processed input data is stored into mdb as simulation data task 1 1 storing input data in mbd is used to simulate when invoking model services that is responsible for providing simulation computing sub service of simulation environment when a modeler sends a hypertext transfer protocol http request to the simulation environment task 2 1 the simulation environment subsequently sends an http request to model services and the computing sub service will start immediately task 2 2 at this stage the modellers are allowed to obtain structured simulation knowledge through reviewing knowledge graph provided in simulation environment such a process will alleviate the complexity and difficulty of the simulation works task 3 the results and simulation parameters are further stored in the output data to ensure the traceability of the simulations task 1 2 those data can be obtained from the mdb when a user make request in the simulation environment task 1 1 in addition this proposed approach may have various physical implementations where different technologies may serve the specific needs for the geographic and environmental research field therefore we use the swmm model simulation as an implementation case study to establish a prototype system to demonstrate the feasibility of the approach in section 3 details of the conceptual framework are further described in the following section 2 2 a conceptual framework of the approach for geo simulation the framework of the proposed approach mainly consists of three layers model database layer knowledge graph layer and model services layer see fig 2 first the model database layer fig 2 is essential to support the comprehensive organization and management of the simulation resources in addition the model database pays more attention to the simulation parameters and the corresponding simulation output management than the classical spatial database systems such data and information is helpful for extending the application of the model services and simulation reproduction second the knowledge graph layer fig 2 is responsible for constructing and applying geo simulation knowledge in this study the geo simulation knowledge has been developed from extensive literature studies needing to be extracted and organized into structured knowledge for multidisciplinary modelers using symbols such a knowledge graph describes the relevant attribute value and their relationships in the geographic environment to carry out this task entities are connected through relationships forming a network of knowledge structures using a storm water management model swmm simulation as an example implementation the extraction of simulation knowledge and the construction of the corresponding knowledge graph are further described in section 3 third the model services layer fig 2 represents a series of web based methods that can be invoked through http requests using the model services layer modelers can use an http put method to send simulation parameters and request model to run several advantages can be acquired using such a method including 1 produce personalization and flexibility in functions customization to meet the needs of modelers 2 eliminate many unnecessary procedures in model use 3 reuse model code and computing resources to work more effectively finally the geo simulation environment fig 2 can be regarded as an integrated workspace for implementing the sharing and reuse of geo simulation knowledge and models in this approach the simulation knowledge and model resources of the integrated geo simulation are distributed in the internet and hence the geo simulation environment should first take the collaborative execution strategies of the knowledge and model services into consideration moreover web based geo simulation and visual methods should help modelers to control the simulation process and intuitively understand the simulation results last but equally important open knowledge exchange and sharing must be part of the design process to encourage broad participation e g students researchers domain experts and other people interested in specific geographic issues 3 implementation of the approach as a prototype system with swmm adopting from the case of frequently urban flood disasters storm water management model swmm version 5 1 is adopted in the prototype system to simulate precipitation events in urban areas three basic modules are generated to adopt such model fig 3 i a set of swmm model services ii urban hydrological simulation knowledge graphs and iii a model database the prototype system interface provides a user friendly gui for modelers from multiple disciplines with different knowledge backgrounds including students researchers stakeholders and the public with the help of the simulation knowledge graph the gui provides users with a panel to set the model simulation parameters in addition a simulation panel is developed to simulate and visualize various model output data to understand urban stormwater management 3 1 design and develop the swmm model web services in the swmm model the urban drainage system is abstracted into a set of physical components including junctions outlets conduits and subcatchments the hydrological behavior of urban areas is then simulated by those components in this study we designed and implemented a set of model web services to provide computing support for the prototype system by analyzing the swmm source code structure and workflow the swmm source code structure fig 4 can be divided into five types of modules 1 main program module 2 hydrodynamics module 3 water quality module 4 general function module and 5 other modules in addition the swmm consists of 53 c code files in total and each of the modules consists of a series of related c code files the main program module is used to control and record the entire swmm simulation process i e manipulating model configuration files and initializing module by calling swmm open swmm start method after the initialization the module is completed the main program module is responsible for the calculation of the hydrodynamics module and water quality module respectively using swmm run method once an swmm model run has been completed the output c and report c code files in the general function module are responsible for recording the simulation results of the hydrodynamics module and the water quality module to files with the file extensions out and rpt the out binary file records the results of swmm time series whereas the rpt file is an extension file to store the summary results of swmm simulation finalizing the above analysis of the swmm source code and workflow secondary development of the swmm source code is conducted to compile and encapsulate the source code into a dynamic link library swmm5 dll for developers to invoke a set of swmm web services is further implemented by invoking the external functions exposed by the swmm numerical engine swmm5 dll fig 5 shows eight web service methods for the prototype system to invoke the following information can be obtained from the invoking including 1 the summary information and swmm simulation results of swmm 2 the length of the simulation period 3 and the number of modeling objects e g junctions outfalls links and sub catchments in addition three basic functions can be implemented by invoking these methods as presented in fig 5 1 manipulate model configuration files to modify the input parameters 2 run swmm 3 obtain the simulation time series results from the binary output file to store in mdb the development frameworks and tools used to develop the swmm web services are listed in table 1 and the model database design is shown in fig 6 3 2 model database design for simulation management mdb is developed to storing and managing swmm input data simulation parameters and results in this study we designed the model database in physical mode based on the conceptual and logical modes the essential table linked by foreign keys of the database consists of several tables 1 expert simulation parameters 2 user simulation parameters and 3 user simulation results such design is valuable for the multidisciplinary modelers to acquire relevant urban hydrological simulation knowledge from experts through their simulation information in addition modelers can trace their history behaviors by accessing tables of user simulation parameters to further improve and adjust their geo simulation results 3 3 construction of the simulation knowledge graph construction of the simulation knowledge graph in the hydrological simulation is carried out using the following three steps fig 7 1 preparation and construction of knowledge source and ontology library respectively 2 acquisition of hydrological simulation knowledge and 3 construction of urban hydrological simulation knowledge graph in the first step the knowledge source for the construction of the simulation knowledge graph is collected using the semi structured information on the web pages and the existing literature related to urban hydrological simulation the ontology library of hydrological simulation is further constructed to defines the data pattern in the process of knowledge acquisition and graph construction the ontology mapping rules of the urban hydrological simulation knowledge graph consisting of 5 core elements are expressed as ontology literatureinformation parametermeaning parametersetting scenesetting resultevaluation relation literatureinformation stores the data of title authors institutions impact factor and keywords of the literature parametermeaning interpretates the parameters in the literature parametersetting refers to the parametric scheme of the simulation model scenesetting provides essential simulation information e g study area location simulation scopes simulation time and scene variables resultevaluation refers to the evaluations ways of the simulation result such as model error assessment and station based validation and relation refers to the semantic relationship of these elements in the second steps the web crawler was employed to parse the hypertext markup language html codes of the web pages to obtain the literature information and store them in the structured data sheet for knowledge graph construction for the case where the knowledge is unstructured in the literature a knowledge extraction model based on bilateral long shortterm memory conditional random field bilstm crf is introduced to recognize and extract the simulation knowledge referring to xiao and zhang 2021 for details subsequently the fusion of knowledge from diverse sources was conducted and the urban hydrological simulation knowledge graph was ultimately constructed and stored in neo4j https neo4j com an instance of urban hydrological simulation knowledge graph is shown in fig 8 it clearly shows the knowledge nodes belonging to the various elements and the relationship between them the simulation knowledge is classified into different stages e g literature information parameter meaning scene setting parameter setting according to different categories and roles to obtain simulation knowledge on demand moreover the knowledge retrieval function is provided in the system to enable users to quickly obtain the simulation knowledge that they need 4 application of the system in nanchong china 4 1 study area and data an urban area located in the shunqing district of nanchong city in china was selected as a test case for the prototype system it is the central city of nanchong located in the northeast of the sichuan basin on the west bank of the middle reaches of the jialing river the coordinate is between 106 03 e to 107 07 e latitude and 30 41 n to 30 51 n longitude precipitation mostly occurs in summer and autumn due to the influence of a monsoon climate the long term annual mean temperature and precipitation are 17 4 c and 1020 8 mm respectively the total study area is approximately 33 02 km 2 see fig 9 a and is classified into five categories including buildings rivers green spaces agricultural areas and city areas the city areas almost cover the entire study area with typical infrastructures of transportation networks municipal squares institutions and commercial districts the drainage system is a combined system consisting of links junction and outfall fig 9 b however the drainage system in the city areas is independent of other basins or hydrological systems this means that the drainage system only collects rainwater from the city areas and no external springs contributing to the surface runoff ideally moreover for the swmm simulation the meteorological data is collected and recorded by a weather station to carry out the swmm simulation in the case study four essential data i e land cover resolution of 30 m 30 m digital elevation model dem resolution of 30 m 30 m and rainfall data were collected from nanchong meteorological bureau primary data on drainage systems were also collected from the local data authorities provideing the records of drainage networks and related parameters i e pipe diameter material length and shape those data were then preprocessed to meet the requirements of swmm simulation first the voronoid diagram is adopted in processing the dem data and junction distribution to subdivide and define the sub catchments consequently each subarea discharges into a single junction or well for the subsequent simulations second junction outfall and link are in the form of cad computer aided design file therefore the data need to be digitized into geojson geographic javascript object notation format geojson is a lightweight and concise language for data analysis transmission and visualization on the web fig 9 b shows that the entire drainage system consists of 9971 junctions 57 outfalls and 9520 links finally several adjustments were made based on the actual engineering situation the rainfall event data fig 10 were collected at the weather station within the period from 8 00 a m on june 27 2020 to 00 00 a m on june 28 2020 and measured with time intervals of 1 h in this study the simulation results were macroscopically evaluated based on the approximate location of the overflow node during the rainfall event provided by the local meteorological office due to the limited data provided in addition the drainage system is assumed to have no inflows fromother basins and thus the model is simulated with only direct rainfall inputs 4 2 configuration and implementation the prototype system on the web was developed based on arcgis api for javascript 4 18 https developers arcgis com javascript latest the terminal was equipped with an intel core i7 7700hq cpu 2 8 ghz with 32 gb of memory and an nvidia geforce gtx1050 gpu with 2 gb of memory the software installed on the terminal included a windows 10 os chrome navigator iis server neo4j graph database and mysql https www mysql com the development environment was webstorm2020 ide https www jetbrains com webstorm and vs2017 ide among them mysql a relational database management system was used to store data from swmm simulation in addition neo4j graph database was used to store and visualize the knowledge graph of the geographic simulation knowledge 4 3 results and discussion 4 3 1 system validation fig 11 fully demonstrates the running process of the prototype system from three aspects data model and knowledge three essential lines in fig 11 i e the blue the red and the yellow dotted lines represent the data flow to show the conversion and transmission of model input and output during the running process of the system the model flow of the running process mainly responsible for input running and output of the model and the knowledge flow respectively the figure indicates that the prototype system shows a 17 h rain event with the simulation duration and time step of 23 h and 5 min respectively to run the prototype system first the rainfall fig 10 and drainage system data fig 9 b are obtained from the model database and then generated a model input file for driving the swmm the entire drainage system was also converted to geojson format data to display on the web after completing the data input preparation it was necessary to calibrate and understand the sensitivity of the hydrological parameters e g impermeability and roughness to improve the reliability of geo simulation work consequently the system needs to modify and reset the simulation parameters to update the model input file with the help of simulation knowledge illustrated by various simulation knowledge subgraphs such a task may produce ideal simulation results these knowledge subgraphs stored in the neo4j graph database were then exported as json javascript object notation format files the json file is a lightweight data format for data transmission and parsing this file is then visualized in the knowledge pane the above function was implemented by invoking service a modify input method in the model web services subsequently the system ran swmm through invoking service b run swmm method and then produce two output files of out and rpt thus the system needed to invoke service c get results method to parse the binary simulation results and finally stored them in the mdb fig 12 a presents a simulated snapshot of the study area at 9 00 a m on june 27 2020 fig 12 b exhibits the process of flow rate within nearly 17 h at each of the 5 conduits fig 12 c shows the process of water depth within nearly 17 h at each of the 5 junctions the results accurately reflect the real time status information of links junctions and outfalls in the drainage system such as the overflow and overload status and drainage capacity of the pipe network such data can provide guidance for the related stakeholder to define a reliable decision in optimizing and transforming the urban drainage system finally during the simulation the system performed a dynamic visualization of the flow and capacity output values of the drainage system using the simulation results such a visualization is performed during the rain event in addition with the help of scene setting subgraph the users load different scene objects e g link and junction and simulate various output values this is to construct swmm simulation scenes for better understanding and analyzing the simulation results the status report recording the junctions where the overflow and the overloaded links have occurred was also shown in the simulation pane this report provides sufficient data for modelers to conduct a macro verification of the simulation results thus if the simulation results cannot meet the requirements defined by the modelers they can trace the historical behaviors from the mdb and re adjust the work with the help of various simulation knowledge subgraphs to improve their swmm model at this point the prototype system had been successfully run once running results show that all tests are being performed following to the design scheme 4 3 2 user experience of the prototype system participants are from our college our college is a research university with geology resources and environment as its main characteristics and outside showing great interest in geographic simulation research or urban stormwater management a total of 30 participants with varying degrees of knowledge and backgrounds were recruited including 10 public people 10 undergraduates 8 postgraduates and 2 professors the authors briefly introduced the prototype system before allowing them to use the prototype system the questionnaire responses were then collected after the participants used the system the rating of the questionnaire is on 10 point scale i e 1 for don t agree and 10 for totally agree in addition the experiment recorded the selection preferences of each participant such as different knowledge subgraphs simulation processes and the experimental results for four types of users and are analyzed as follows first the postgraduate user type has clear research goals and hence so their selections are usually more focused this type of participant only selected the knowledge subgraphs related to their own needs e g parameter meaning and parameter setting the selection preferences of this user type are shown in fig 13 second the knowledge subgraphs selected by the public people user type are often confused because they are completely unfamiliar with the field this results in the selections of approximately three users at the beginning of the experiment being completely random as the experiment progresses such users can gradually perform the swmm simulation under the help of the parameter setting and scene setting subgraph the selection preferences of this user type are shown in fig 14 the selection preferences of the undergraduate user type are similar to the postgraduate user type as shown in fig 15 a such users conveniently complete the work according to the simulation knowledge provided by the simulation knowledge graph the users also paid more attention to the information e g the location and status of junction overflow to improve their cognition and understanding of urban rainwater management the professor user type has relatively little demand for the simulation knowledge because such users have professional simulation knowledge run excellent simulation the selection preferences of this user type are shown in fig 15 b selected key questions in our survey are further listed in table 2 developed from the survey results the table shows that 26 participants would first refer to the simulation knowledge graph provided by the prototype system to set their simulation schemes moreover the scores for questions 4 and 5 are still larger than 6 the average scores still support the conclusion that the structured simulation knowledge is useful for users who lack relevant knowledge to alleviate the complexity and difficulty of geo simulation and hence promote the sharing of models and knowledge simultaneously 5 conclusions and future work with the increasing complexity and comprehensiveness of geographic science issues the consideration of sharing and reuse of geo simulation knowledge in geo simulation has become extremely important such a consideration advances the multi domain sharing and application of the models this paper successfully developed an innovative approach using geo simulation knowledge from massive unstructured literature and web pages to help modelers quickly recognize models to support broader participation and applications we have demonstrated how geo simulation work can be effectively completed with the help of geo simulation knowledge through the implementation of the prototype system and the experimental study the main contributions of this study are concluded as follows proposing an integrated innovative approach of knowledge graph and model services for effectively sharing and reusing models and geo simulation knowledge such an approach also demonstrated the effectiveness and feasibility of the approach through designing a prototype system on the web taking the swmm model as an example the simulation knowledge graphs were constructed and stored by neo4j and the applicability and reliability of these simulation knowledge were verified through user experience experiment and questionnaire survey the system effectively alleviated the complexity and difficulty of swmm simulation by providing simulation knowledge reduced the threshold for model use which helps to improve the capabilities of geo simulation potentially despite the achievements described above the following aspects still need improvement in the future work first the procedures of geo simulation knowledge recommendation may need to be considered to provide modelers with more intelligent and friendly knowledge services in the future second in order to further support multi field knowledge exchange and sharing collaborative simulation by interdisciplinary and geographically separated users will be considered in the next stage of the modelling system zhang et al 2015 chen et al 2019 2020 furthermore we will conduct more comprehensive research based on this approach including a sensitivity analysis of simulation parameters and station based validation to provide a more open and intelligent geo simulation environment for modelers from different fields software and data availability program title the prototype system swmm developer heng li contact address 2004200036 cugb edu cn source code available and data https github com liheng gis the prototype system swmm git program required internet browser tested on chrome and firefox iis7 net 4 6 1 source code for the swmm 5 1 014 webstorm 2020 2 3 visual studio 2017 year first available 2021 program language javascript c availability and cost open source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the fundamental research funds for the central universities no 2652018082 and the national natural science foundation of china no 72033005 the data support from nanchong meteorologic bureau are gratefully acknowledged given their roles as environmental modelling software editor min chen was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor dr daniel p ames 
25763,the reliability and applicability of hydrological models within ecohydrological frameworks are major concerns two multi objective model calibration strategies were formulated to achieve a balanced representation of ecologically relevant hydrologic indices two approaches were employed 1 a performance based method constraining the targeted hydrologic indices and 2 an unconstrained signature based method explicitly incorporating the targeted hydrologic indices into multiple objective functions both strategies were successful in representing most of the selected hydrologic indices within a 30 relative error acceptability threshold while yielding consistent runoff predictions in a watershed the performance based strategy was preferred since it showed a lower dispersion of near optimal pareto solutions when representing the selected indices based on water balance and flow duration curve characteristics still the overall representation of low flow magnitude and timing rise and fall rates and duration and frequency of extreme flows was limited in terms of interannual variability due to the hydrological model structural inadequacies keywords multi objective optimization streamflow regime environmental flows hydrologic indices swat 1 introduction the streamflow regime is widely acknowledged as a key determinant of the ecological integrity of riverine ecosystems poff et al 1997 sofi et al 2020 both climate and human driven alterations to natural streamflow fluctuations affect the structure and functioning of these ecosystems threatening biodiversity and restricting the provision of ecosystem services palmer and ruhi 2019 vörösmarty et al 2010 therefore understanding and evaluating the impacts of climate change and human interventions on the streamflow regime is critical to inform and prioritize environmental management alternatives hassanzadeh et al 2017 mittal et al 2016 a broadly accepted approach to characterizing streamflow regimes is to compute flow statistics from streamflow hydrographs these statistics also known as hydrologic signature metrics streamflow characteristics sfcs or ecologically relevant hydrologic indices erhis generally represent five fundamental facets magnitude frequency duration timing and rate of change of flows poff and zimmerman 2010 currently there are over 200 flow statistics relevant to stream ecology archfield et al 2014 olden and poff 2003 vogel et al 2007 these indices are usually employed in ecohydrological applications such as stream classification kennard et al 2010 mcmanamay et al 2014 prediction of stream health or distribution of riverine species hernandez suarez and nejadhashemi 2018 kakouei et al 2017 and environmental flow determination mathews and richter 2007 poff et al 2010 since these applications generally cover large spatial scales statistical and hydrological models have been increasingly used especially to predict regional changes in erhis due to climate and anthropogenic factors caldwell et al 2015 mittal et al 2016 yang et al 2016 hydrological models are usually preferred over regional statistical approaches because they can explicitly consider modifications in land use environmental conditions and management practices hall et al 2017 shrestha et al 2016 moreover some environmental flow frameworks recommend using hydrological models for predicting streamflow in poorly gauged or ungauged locations peters et al 2012 poff et al 2010 however there is a growing number of studies revealing important limitations of hydrological models in representing erhis especially when these models are calibrated based on traditional performance metrics such as the nash sutcliffe efficiency nse murphy et al 2013 shrestha et al 2014 vigiak et al 2018 vis et al 2015 these limitations include over or underprediction of low and high flow indices wenger et al 2010 high errors uncertainties when predicting erhis related to timing duration frequency and or rate of change of flows murphy et al 2013 shrestha et al 2014 vigiak et al 2018 and different sets of equally well performing model parameters in terms of traditional metrics yielding very different performances in terms of erhis vis et al 2015 current model calibration approaches for addressing limitations in erhis representation can be classified into two major categories in the first category hereafter referred to as performance based objective functions are formulated based on traditional performance metrics with different streamflow transformations e g square root logarithm inverse to stress or balance the importance of different flow conditions on the other hand calibration approaches in the second category hereafter referred to as signature based explicitly incorporate sfcs of interest into the objective functions hallouin et al 2020 kiesel et al 2017 2020 pool et al 2017 vis et al 2015 zhang et al 2016 in ecohydrological applications the choice of sfcs of interest has been mainly based on riverine species preferences hallouin et al 2020 kiesel et al 2017 2020 pool et al 2017 whereas hydrological applications usually target flow duration curve fdc features runoff ratios and basic discharge statistics chilkoti et al 2018 euser et al 2013 fernandez palomino et al 2020 pfannerstill et al 2014 2017 sahraei et al 2020 shafii and tolson 2015 yilmaz et al 2008 some applications using performance based approaches target specific flow conditions garcia et al 2017 mizukami et al 2019 whereas others use one or multiple objective functions to attain an acceptable overall representation of the streamflow regime hallouin et al 2020 when combining multiple objective functions studies either use aggregated single objective functions vis et al 2015 or pure multi objective approaches chilkoti et al 2018 hernandez suarez et al 2018 sahraei et al 2020 in general signature based approaches provide better predictions of pre selected sfcs compared to performance based approaches hallouin et al 2020 however those sfcs that are not included in the original objective function formulation are not necessarily well represented or better performing than traditional approaches using streamflow transformations hallouin et al 2020 during the last decade researchers have obtained a better understanding of the implications of model calibration into ehris replication for instance several studies have demonstrated that the objective function choice or formulation influences the prediction of flow statistics kiesel et al 2020 pool et al 2017 shafii and tolson 2015 vis et al 2015 also these studies showed that optimality in terms of traditional performance metrics does not necessarily result in optimal solutions for ecohydrological purposes hallouin et al 2020 kiesel et al 2020 in ecohydrological applications regardless of the optimization scheme for model calibration it is uncommon to find solutions yielding acceptable results for all erhis of interest also finding an individual simulation with acceptable results for both low and high flow conditions is unusual therefore simulation ensembles such as median or averages of optimal results or their clusters are recommended hernandez suarez et al 2018 vis et al 2015 it is worth noting that most of the calibration approaches used in previous ecohydrological studies have run on single objective mode i e multi metric aggregated functions hence those results depend on the weight assigned to each erhi or performance metric considered within the objective function zhang et al 2016 and tradeoffs among different indices performance metrics or regime facets are not fully explored the goal of this study was to develop calibration strategies providing a balanced streamflow regime representation among the different regime facets i e magnitude frequency duration timing and rate of change two strategies were developed to compare both performance and signature based calibration approaches the strategy using a performance based approach was improved by incorporating a novel constraint formulation to obtain simulations with targeted erhis within pre defined acceptability thresholds for the signature based strategy tradeoffs between different streamflow regime facets were explicitly considered these calibration strategies were implemented in an agriculture dominated watershed in michigan us using the recently developed evolutionary multi objective optimization algorithm called unified non dominated sorting genetic algorithm iii u nsga iii and the soil and water assessment tool swat to the best of our knowledge previous multi objective calibration approaches for ecohydrological applications have not explicitly considered optimization routines constraining the performance of erhis of interest likewise this is the first time that a multi objective calibration approach is applied to targeted erhis pursuing a balanced representation of the overall streamflow regime while explicitly considering different regime facets 2 materials and methods 2 1 overview two different strategies for multi objective calibration were evaluated to improve the representation of the overall streamflow regime in a watershed model strategy 1 employed a constrained performance based approach whereas strategy 2 used a constraint free signature based approach fig 1 strategy 1 consisted of three major steps in the first step the goal was to identify a reduced set of performance metrics that jointly represented a wide list of erhis then in the second step a tailored constraint was formulated to generate individual simulations with an acceptable replication of a reduced set of erhis of interest this formulation was based on pre defined acceptability criteria for erhi replication moreover the selection of erhis of interest was performed by targeting a balanced representation of different flow regime facets in the third step the outputs of the previous steps were used as inputs to formulate a multi objective optimization problem for model calibration meanwhile strategy 2 consisted of two major steps in the first step a reduced set of erhis was defined to provide a balanced representation of different regime facets then several objective functions representing different regime facets were formulated these objective functions were considered as inputs of the problem formulation in step 2 this formulation was intended to explore tradeoffs in the simulation of different regime facets for each strategy near optimal pareto solutions were obtained using an evolutionary multi objective optimization algorithm finally preferred tradeoff solutions were identified and compared using multicriteria decision making mcdm methods 2 2 study area the proposed strategies were evaluated in the honeyoey creek pine creek watershed hydrologic unit code 0408020203 located in east central michigan us fig 2 this watershed has a drainage area of 1010 km2 and is situated within the saginaw river watershed which drains into lake huron the saginaw river watershed is identified as an area of concern by the us environmental protection agency usepa due to water pollution wildlife habitat degradation loss of recreational values among others usepa 2015 according to data from the national agricultural statistics service nass of the us department of agriculture usda agriculture is the dominant land use 50 of the area followed by forests 24 wetlands 16 pasturelands 7 and urban development 3 usda nass 2012 2 3 watershed model the soil and water assessment tool swat 2012 rev 622 was used to simulate the streamflow regime in the study area swat is a semi distributed process based continuous time watershed model that can operate on a daily or sub daily time step swat is mainly used to evaluate the impact of land use and management practices on water sediments nutrients pesticides and bacteria yields at the watershed scale arnold et al 2012 when using swat a watershed is divided into subwatersheds which are further discretized into hydrologic response units hrus hrus are geographical units with homogeneous land use soil and topographical characteristics swat inputs controlling the water balance include daily or sub daily precipitation maximum and minimum air temperatures solar radiation wind speed and relative humidity swat simulates the watershed hydrology in two phases land loading and water network routing simulated hydrological processes include snow accumulation and melting canopy storage plant growth evapotranspiration infiltration surface runoff soil water redistribution lateral flow groundwater flows and channel routing neitsch et al 2011 in this study swat was used to obtain daily streamflow from 2003 to 2014 calibration period and from 1983 to 1994 validation period at the outlet of the honeyoey creek pine creek watershed fig 2 a warm up period of two years was used to minimize the effect of initial conditions on the simulations simulated streamflow values were compared against daily observations obtained from the pine river near midland us geological survey usgs gauging station id 04155500 usgs 2020 input daily precipitation and max min temperature data from 1981 to 2014 were collected from two weather stations provided by the national centers for environmental information ncei of the national oceanic and atmospheric administration noaa noaa ncei 2020 the missing weather input data were estimated using swat s stochastic weather generator wxgen neitsch et al 2011 the watershed was divided into 250 subwatersheds each consisting of a unique hru obtained from dominant land use soil and slope characteristics these subwatersheds were delineated using stream network data from the national hydrography dataset nhd and pre defined units obtained from the michigan institute for fisheries research einheuser et al 2012 elevation data with a 30 m resolution was obtained from the national elevation dataset provided by the usgs national map usgs 2018 land use was extracted from the 30 m resolution cropland data layer cdl which was obtained from usda nass 2012 soil characteristics were extracted from the soil survey geographic database ssurgo provided by the usda natural resources conservation service nrcs usda nrcs 2020 potential evapotranspiration was calculated using the penman monteith equation monteith 1965 whereas surface runoff was computed using the soil conservation service scs curve number method usda scs 1972 streamflow was routed through the channel network using the variable storage coefficient method williams 1969 the model was calibrated by adjusting 15 parameters whose description and calibration ranges are reported in table 1 2 4 strategy 1 constrained performance based model calibration 2 4 1 performance metrics selection a reduced set of performance metrics were used for objective functions formulation from a list of widely used measures see table 2 these measures included nse nash and sutcliffe 1970 original and modified versions of the kling gupta efficiency kge gupta et al 2009 kling et al 2012 the index of agreement ioa willmott 1981 and the coefficient of determination r 2 the fourth root mean quadrupled error r4ms4e was also considered in order to emphasize the largest residuals expected under high flow conditions since both nse and the root mean square error rmse vary only with the sum of squared model residuals just the former was contemplated in this study following gupta et al 2009 nse and kge can be expressed in terms of three components representing correlation bias and variability correlation relates to timing and hydrograph shape meanwhile bias and variability are aimed to reproduce the first and second moments of the distribution of observations which mainly affect magnitude related sfcs these three components interact differently under each performance measure for instance bias is scaled by the standard deviation of observations in nse thus in presence of high variability the bias component might be less important when obtaining optimal values in addition correlation and variability components interact with each other in nse which generally results in underestimation of the latter gupta et al 2009 as an alternative kge provides a more balanced representation of correlation bias and variability while avoiding interactions among these components gupta et al 2009 by considering r 2 as an additional measure we aimed to evaluate the role of the correlation component in erhis replication meanwhile ioa was included to consider a different way of normalizing the sum of square errors and its effects on erhis replication to accentuate different flow conditions i e low moderate and high relative errors and error transformations were considered except for r4ms4e all measures included their standard versions along with logarithmic inverse and square root transformations relative error versions were only used for nse and ioa it is worth mentioning that in general the standard versions favor high flows representation square root transform is used for highlighting moderate or average flow conditions whereas logarithmic inverse and relative error versions accentuate low flows bennett et al 2013 krause et al 2005 single objective model calibration was executed for each performance metric and transformation indicated above resulting in 23 individual optimization problems each minimization objective function f was defined as 1 p m where p m is the transformed performance metric to be maximized for r4ms4e f p m as this metric has to be minimized as a next step 171 erhis reported by henriksen et al 2006 and seven erhis proposed by archfield et al 2014 were computed for each of the 23 optimal solutions simulated erhis were compared against those obtained from streamflow observations by calculating relative errors for each index e r e l for a vector of model parameters θ as follows 1 e r e l i θ i i y θ i i y ˆ i i y ˆ 100 where i i is the i th hydrologic index evaluated for simulations y θ and observations y ˆ then erhis within a pre defined relative error threshold were identified for each optimal solution it was expected that optimal results from different performance metrics and transformations yield different well replicated erhis therefore the final choice of performance metrics was determined by selecting up to six transformed measures that jointly represented the maximum number of erhis within the pre defined relative error threshold for the selection procedure the transformed measure with the highest number of erhis within the acceptability threshold was selected then another transformed measure was identified based on the remaining erhis and added to the list of selected measures the previous step was repeated until either attaining the maximum number of well replicated erhis or the pre defined maximum number of objective functions it is worth noting that the fraction of non dominated solutions with respect to the total population increases with the number of objective functions slowing down the search process deb and jain 2014 likewise a higher population size is required to maintain a good exploration of large dimensional spaces which increases the number of function evaluations and the overall computational time for these reasons we decided to limit the number of objective functions to six in this study a real parameter genetic algorithm ga goldberg 1991 was used for single objective optimization particularly tournament selection simulated binary crossover sbx deb and agrawal 1994 and polynomial mutation deb 2001 were designated as ga operators the optimization algorithm ran for 250 generations with a population size of 100 resulting in a total of 25 000 model evaluations for each problem the crossover probability and distribution index for the sbx operator were defined as 0 9 and 10 respectively likewise the mutation probability and distribution index for the polynomial mutation operator were defined as 1 15 i e the reciprocal of the number of calibration parameters and 20 respectively on the other hand the relative error threshold for ehri replication was defined as 30 following uncertainty in the estimation of hydrologic indices reported by kennard et al 2009 when using 15 year time series this threshold has also been used in previous ecohydrological studies to evaluate the performance of erhis predictions caldwell et al 2015 hernandez suarez et al 2018 vis et al 2015 2 4 2 constraint definition traditionally hydrologic signatures have been used in model calibration either as objective functions or post calibration evaluation criteria shafii and tolson 2015 here we used a set of relevant signatures as constraints given a pre defined acceptability threshold this set can be identified by the modeler depending on the ecohydrological application needs in this study we used 32 indicators of hydrologic alteration iha the nature conservancy 2009 divided into five categories table 3 each representing specific streamflow regime facets in addition seven indices presented by archfield et al 2014 which describe fundamental stochastic properties of streamflow time series were included in the constraint definition the aforementioned 39 indices are described in table 3 for consistency an acceptability threshold of 30 relative error was used for constraining erhis prediction the constraint which was formulated as the sum of two components aggregates the performance of all ehris of interest into a single measure the first component is the number of indices with relative errors outside the pre defined acceptability threshold for ehris replication the second component is a weighted sum of relative violations by each index with respect to the pre defined acceptability threshold the constraint can be expressed as follows 2 c v θ i 1 m k i θ 1 w i 1 τ i i y θ i i y ˆ i i y ˆ 1 k i θ 0 if 1 τ i i y θ i i y ˆ i i y ˆ 1 0 1 otherwise w i 1 g h i where cv θ is the constraint violation for simulations y θ m is the total number of indices i e 39 in this study τ is the acceptability threshold expressed as the absolute value of a fraction between 0 and 1 0 30 is used for this study w i is the weighting factor for the i th index g is the number of index categories i e 6 in this study and h i is the total number of indices in the category that contains the i th index the weighing factor was explicitly incorporated to provide a balanced contribution from different streamflow regime facets a solution is considered feasible when cv θ attains a value of zero but for ease of handling the constraint with an optimization algorithm we convert it to an inequality constraint as c v θ 0 by introducing the constraint formulation presented above the optimization algorithm is forced to find streamflow simulations in which all erhis of interest are estimated within the acceptable range i e the relative error is within 30 it is worth noting that the constraint definition is flexible enough to designate different acceptability thresholds τ i for each index this might be necessary when it is needed to iteratively relax certain acceptability conditions to find feasible solutions 2 5 strategy 2 unconstrained signature based model calibration under this strategy an objective function was formulated for each index category presented in table 3 as follows 3 f j θ i g j i i y θ i i y ˆ i i y ˆ where f j θ is the objective function for the j th category and g j is the set of indices belonging to the j th category each objective function represents the total error obtained under each index category relative errors were used to normalize the contribution from different indices no constraints were formulated for this calibration strategy therefore opposite to strategy 1 no pre defined acceptability thresholds for erhis replication and no weighting factors were required in strategy 2 2 6 evolutionary multi objective optimization algorithm in both calibration strategies the goal was to determine the values for the vector of model parameters θ i e decision variables that minimize the objective functions formulated for each strategy each decision variable θ p p 1 2 15 could take a value within the ranges defined in table 1 in strategy 1 those model simulations with c v θ 0 were considered as feasible see eq 2 the remaining were infeasible an evolutionary multi objective optimization algorithm u nsga iii seada and deb 2016 was implemented to address the optimization problems resulting from each strategy u nsga iii is a population based algorithm that employs crossover and mutation operators along with non dominated sorting and reference directions to move towards near optimum pareto solutions reference directions are vectors evenly filling the objective space this algorithm can be used for single multi i e 2 or 3 objective functions and many objective i e 3 objective functions optimization problems and stems from the nsga iii algorithm deb and jain 2014 it is worth mentioning that u nsga iii can handle both unconstrained and constrained problems for unconstrained problems during the non domination sorting any two solutions are compared using just the objective function values a solution x1 dominates a solution x2 when 1 x1 is no worse than x2 in all objective functions and 2 x1 is better than x2 in at least one objective function deb 2001 in constrained problems the concept of constraint domination is used instead a solution x1 constraint dominates a solution x2 when 1 x1 is feasible and x2 is infeasible 2 both x1 and x2 are infeasible but x1 has a lower constraint violation cv or 3 both x1 and x2 are feasible and x1 dominates x2 using the traditional domination principle jain and deb 2014 in non domination sorting feasible solutions will always be on top of infeasible solutions likewise the selection operation when creating the offspring population is modified for constrained problems jain and deb 2014 nsga iii and u nsga iii have been implemented in previous water resources applications such as multivariate model calibration using streamflow and evapotranspiration data herman et al 2020 multi objective calibration targeting different flow conditions hernandez suarez et al 2018 irrigation scheduling kropp et al 2019 mwiya et al 2020 reservoir design and operation chen et al 2020 pourshahabi et al 2020 and optimization of land use practices raschke et al 2021 in this study an interface for modifying swat input files and executing the model was developed in python 3 7 this interface also included the computation of the erhis reported by henriksen et al 2006 and archfield et al 2014 and was coupled with the python library pymoo blank and deb 2020 to implement the u nsga iii algorithm the stopping criterion was set as a maximum of 1000 generations for the multi objective optimization with a number of reference directions assigned equal to 100 well spaced reference directions were generated using the recently developed riesz s energy method blank et al 2021 included in the pymoo library the operators and parameters chosen for crossover and mutation were the same as the ones presented in section 2 4 1 for the ga which are standard and recommended deb et al 2002 convergence to a near optimal solution was analyzed using the hypervolume indicator auger et al 2009 which is a measure of the collective volume of the region dominated by the pareto optimal solutions in the objective space 2 7 selection of preferred tradeoff solutions since we were interested in obtaining solutions providing balanced representations of different streamflow regime facets we compared a set of preferred solutions from different mcdm methods particularly two approaches were implemented compromise programming zeleny 2011 and the pseudo weight method deb 2001 the compromise programming approach identifies the closest pareto optimal solution to a reference point using a user defined distance metric usually the reference point is the ideal point representing the best expected objective function values in this study the ideal point was the origin of the objective space as distance metrics we used the ℓ p norm with p 2 euclidian distance and p chebyshev distance the latter is preferred for non convex pareto optimal solutions the metrics for a pareto optimal solution were computed as follows branke et al 2008 4 ℓ p m 1 m f m z m p 1 p 5 ℓ p max m f m z m where m is the number of objective functions f m is the value for the m th objective function and z m is the value of the m th component of the reference point before applying any distance metrics the objective functions were normalized to values between 0 and 1 meanwhile the pseudo weight method generates a vector for each pareto optimal solution representing the relative importance or weight of each objective function the sum of the different weights in each vector is forced to one the pseudo weight w i for the i th component in a pareto optimal solution was computed as follows deb 2001 6 w i f i max f i f i max f i min m 1 m f m max f m f m max f m min where f i max and f i min are the maximum and minimum values for the i th objective function among all pareto optimal solutions respectively the denominator in eq 6 guarantees that the sum of all pseudo weight vector components for a pareto solution is equal to one pseudo weights are proportional to the difference between the maximum objective function value and the solution s value for a particular component thus a higher pseudo weight indicates that the point is closer to the minimum objective function value for that component in other words a higher pseudo weight value indicates a higher preference for the corresponding objective function in this study we selected the most balanced pareto optimal solution as the one with the closest pseudo weight vector to the m dimension target vector 1 m 1 m different target vectors can be used to explore how a pareto solution changes when giving more relevance to a particular objective function 2 8 evaluation of calibration results using water balance flow duration curve characteristics and additional hydrologic indices the flow duration curve fdc is the complement of the streamflow cumulative distribution function vogel and fennessey 1994 fdcs are signatures of runoff variability and summarize a watershed s ability to generate streamflow values of different magnitude yilmaz et al 2008 fdcs have been widely used for model evaluation and calibration fenicia et al 2018 since a fdc is a frequency domain representation of a hydrograph information concerning to streamflow timing is lost limiting its utility to diagnose the overall streamflow regime however some characteristics extracted from fdcs are useful for understanding key hydrological processes and their ecohydrological significance mcmillan 2020a 2020b in this study we computed the percent bias pbias of four indices extracted from fdcs to evaluate the consistency between calibration results and sfcs that have been typically used in signature based model calibration the characteristics derived from fdcs were the very high segment volume fhv high segment volume fmv midsegment slope fms and low segment volume flv ley et al 2016 yilmaz et al 2008 the aforementioned segments were subjectively defined by yilmaz et al 2008 using the 2 20 and 70 flow exceedance probabilities fms is a signature of the vertical soil moisture redistribution and streamflow flashiness likewise fhv provides additional information regarding streamflow flashiness and quantifies watershed reactions to large precipitation events meanwhile fmv quantifies the watershed response to heavy rainfall finally flv which is related to long term baseflow was computed using the modification reported by casper et al 2012 to reduce the effect of the difference in lowest simulated and observed flows on the pbias computation long term water balance was also considered by computing the pbias in the overall runoff ratio rr yilmaz et al 2008 the iha indices that were selected in this study are computed from metrics obtained on an annual basis and represent the central tendency i e mean of annual metrics table 3 when setting environmental flows or evaluating streamflow regime alteration widely used methods such as the range of variability approach richter et al 1996 1997 also consider the associated interannual variability in those metrics these methods define streamflow alteration targets as a function of central tendency and variability metrics these targets are defined for each streamflow regime facet using meaningful indices and are further customized depending on the available ecological information of the study area poff et al 2010 richter et al 1997 given the relevance of streamflow variability in ecohydrological applications especially in the definition of limits of streamflow alteration we evaluated the impact of the two calibration strategies defined in this study which use only central tendency indices in the replication of associated variability indices these variability indices are expressed here in terms of coefficients of variation following henriksen et al 2006 3 results and discussion 3 1 performance of single objective model calibration using transformed metrics the relative errors for ehris replication under each optimal solution using the transformed measures indicated in section 2 4 1 are presented in fig 3 these results were obtained as part of the objective functions selection routine under strategy 1 using hierarchical clustering with euclidean distances and ward s method five groups of performance metrics were identified based on their similarity in replicating erhis these groups are presented in table 4 and can be visualized in fig 3 for the different categories of hydrologic indices performance metrics were arranged by similarity in the y axis these groups revealed that optimal solutions using r 2 and relative transformed metrics as objective functions behaved drastically different compared to the other evaluated metrics generally optimal simulations using the former metrics were able to represent those erhis that did not fall within the 30 relative error threshold using kge and sum of square errors based metrics for example in fig 3b indices ml9 and ml10 are better represented by r 2 and relative transformed metrics than any other metrics similar examples can be observed for dl6 and dl11 in fig 3d for fh5 in fig 3g or for ra3 and ra6 in fig 3k still the overall performance in erhis replication was very poor for r 2 and relative transformed metrics having less than 51 of erhis within the threshold according to table 4 other poor performing metrics included inverse and log transformed nse these results suggest that those measures should be used as complementary criteria rather than objective functions in single objective model calibration when targeting the overall streamflow regime representation different performance metrics or groups of metrics are more suitable in replicating specific index categories or streamflow regime facets see table 4 in terms of magnitude standard or square root transformed metrics are preferred when targeting average and high flows ma and mh respectively whereas low flows ml were best represented by optimal solutions when using r 2 for model calibration regarding duration kge and kge provided the best performing solutions for low flows dl whereas standard and square root transformed metrics were better suited for high flows dh for frequency most of the standard square root and inverse transformed metrics were better suited for low flows fl whereas kgesqrt yielded the highest proportion of well replicated high flow fh indices with respect to timing standard square root or most of the log transformed metrics are preferred when targeting average ta and high flows th meanwhile ioarel ioalog kge inv and r 2 were the best performing metrics when looking for optimal solutions in replicating low flows timing tl those indices representing the change of flow and reversals showed an acceptable replication under some r 2 based or relative transformed metrics however in general an acceptable representation of rate of change indices ra was quite difficult and none of the performance metrics that were employed in this study resulted in an outstanding performance finally all of the magnificent seven indices mag were well replicated by optimal results using standard square root or log transformed metrics it is worth noting that none of the 23 identified optimal solutions were able to represent five indices within the pre defined acceptability threshold of 30 relative error these indices were the mean duration of flows exceeded 25 of the time dh21 mean low flow pulse duration dl16 mean low flow pulse count fl1 mean number of high flow events using the flow exceeded 25 of the time as a threshold fh9 and mean high flow volume using the median annual flow as a threshold mh21 3 2 selected metrics for constrained performance based model calibration the selection process of objective functions under strategy 1 resulted in three different lists of six transformed measures jointly representing 168 out of 178 erhis within the 30 relative error range these lists had in common the first five measures standard and inverse kge and standard inverse and square root r 2 the sixth measure was either r 2 log kge inv or ioarel we decided to proceed with the list containing ioarel because opposite to the other two lists this one represented all rate of change indices within the 30 acceptability threshold the optimal solution using standard kge was able to provide the highest number of indices within the error threshold i e 128 indices or 72 of all erhis see table 4 note that the selected list of metrics includes most of the best performing measures for each group reported in table 4 i e metrics in bold however this list did not well represent five indices related to flow variability and high flow magnitude variability in annual minima of daily flows dl6 and ml21 variability in february and august flows ma25 and ma31 respectively and mean peak flows using the median annual flow as a threshold mh24 these indices were added to the five indices that were not represented by an optimal solution see section 3 1 3 3 overall performance of pareto optimal solutions each multi objective calibration strategy was executed using 20 threads in parallel on a machine equipped with two intel xeon cpu e5 2640 processor at 2 5 ghz with 64 gb ram running ubuntu 16 04 7 lts total computation time for strategies 1 and 2 were 32 43 and 30 86 h respectively strategy 1 successfully identified pareto solutions satisfying the defined constraint for all 39 erhis of interest the first feasible solution was found at generation 48 and convergence to a near optimal pareto front was achieved after 800 generations once the hypervolume indicator started to show a steady behavior fig 4 a pareto front sizes at the end of each generation over the u nsga iii search process did not exceed 35 solutions with 25 near optimal pareto solutions for the 1000th generation similarly strategy 2 converged to a near optimal pareto front after 800 generations in this case pareto front sizes at the end of each generation mostly varied between 20 and 40 solutions with 29 near optimal pareto solutions for the 1000th generation performance of near optimal pareto solutions for both strategies improved with respect to the initial random population sampled from uniform distributions of model calibration parameters fig 4b and c near optimal solutions from strategy 1 resulted in linear correlations r between 0 80 and 0 85 whereas strategy 2 provided results with a broader range for r between 0 70 and 0 85 all pareto solutions from strategy 1 overestimated up to 1 3 times the standard deviation in observations while showing a ratio between simulated and observed means between 0 95 and 1 05 meanwhile strategy 2 resulted in a more balanced and wider set of near optimal pareto solutions in terms of both simulated observed standard deviation and mean ratios α and β respectively under both strategies the standard deviation of model residuals was around 60 70 of the standard deviation of observations a summary of the metrics and objectives median interquartile range iqr maximum and minimum that were used to obtain the near optimal pareto and preferred tradeoffs solutions are presented in table 5 in both strategies the performance metric showing the highest variability as presented by iqr was kgeinv which emphasizes low flow conditions in general near optimal pareto solutions from strategy 2 showed a higher variability of objective function values compared to strategy 1 in terms of kgeinv strategy 2 provided an overall better performance but also had solutions with very low values minimum was 1 43 it is worth noting that none of the maximum values for the performance metrics chosen under strategy 1 were as high as those found when executing single objective model calibration for example the maximum kge of 0 83 obtained from the near optimal pareto set from strategy 1 reported in table 5 was below the near optimum value of 0 88 reported for kge in table 4 these results indicate that simulations with erhis of interest within 30 relative error are not necessarily close to an optimum in terms of a particular performance metric 3 4 replication of ecologically relevant hydrologic indices of interest fig 5 shows the distribution of relative errors for each erhi of interest and model calibration strategy during both calibration and validation periods during the calibration period which was defined between 2003 and 2014 all the indices computed from the near optimal pareto set from strategy 1 fell within the 30 relative error range meanwhile strategy 2 provided median values for almost all erhi of interest ma19 i e august mean flow was the exception within the same range with some near optimal pareto solutions generating index values outside this range in general strategy 1 resulted in a lower variability of ehris values compared to strategy 2 additionally median relative errors had a similar behavior among both calibration strategies some exceptions included dh1 to dh5 i e duration of annual maxima in fig 5b which exhibited opposite trends under both strategies i e overestimation for strategy 1 and underestimation for strategy 2 indices that showed the highest variability during the calibration period in both strategies were mostly related to low flow conditions these indices include dl1 to dl5 i e duration of annual minima and ml17 i e baseflow index in fig 5b row 1 dl16 and fl1 i e low flow pulse duration and frequency and ra3 i e fall rate in fig 5c row 1 and mag3 and mag4 i e skewness and kurtosis in fig 5d row 1 strategy 1 presented the most biased results for indices in the iha3 category representing the timing of annual extremes this is consistent with the f 3 median value of 26 4 for this strategy reported in table 5 which is the highest median value among both strategies and objectives used in strategy 2 it is worth noting that median values for objectives f 2 and f 4 related to duration and frequency respectively were lower and better for strategy 1 than strategy 2 near optimal pareto sets of model parameters obtained during model calibration were validated for a 12 year period between 1983 and 1994 validation results for the replication of erhis of interest are also presented in fig 5 from the list of 39 erhi of interest the median relative errors for three indices fell outside the acceptability range of 30 these indices were ra3 for both strategies fl1 for strategy 1 and dh15 for strategy 2 fig 5c row 2 ra3 relative error values were mostly distributed within 50 and 40 and median relative error values for fl1 and dh15 were very close to 30 and 30 limits respectively excepting dh15 for strategy 2 these results are indicative of the robustness of both calibration strategies variability of erhis of interest behaved similarly during the calibration and validation periods however dh15 and th1 related to duration and timing of high flow events drastically increased their variability during the validation period 3 5 performance of preferred tradeoff solutions we obtained three different solutions two from strategy 1 and one from strategy 2 targeting a balanced representation of the different streamflow regime facets both euclidean and chebyshev distances used for the compromise programming method selected the same preferred solution from the near optimal pareto sets under each calibration strategy for strategy 2 the pseudo weight method s preferred solution was the same as the compromise programming method the overall calibration performance for these preferred solutions is reported on the right side of table 5 there are no major differences between the three preferred solutions in terms of performance during the calibration period strategy 2 provided slightly better results for kge and kgeinv whereas strategy 1 solutions presented better r 2 and r 2 sqrt values it is worth noting that nse values were 0 61 and 0 60 for compromise programming and pseudo weight solutions in strategy 1 respectively the preferred solution under strategy 2 attained a lower nse of 0 56 regarding the replication of erhis of interest the strategy 2 preferred solution provided in average lower absolute relative errors for five of six categories of hydrologic indices during the calibration period meanwhile strategy 1 preferred solutions attained better replication results for the iha4 category which is related to frequency and duration of high and low pulses during the validation period preferred solutions from strategy 1 improved in terms of kge kgeinv and ioarel 7 42 and 1 respectively while slightly worsening in terms of r 2 r 2 sqrt and r 2 inv 3 4 and 6 respectively nse improved to 0 65 and 0 63 6 for the compromise programming and pseudo weight method solutions respectively average absolute relative errors improved for the three first erhi categories i e magnitude of monthly flows duration and timing of extremes and deteriorated for the remaining categories especially for the iha4 category regarding strategy 2 the preferred solution generally worsened in terms of both performance metrics and erhi replication especially for the iha4 category which exceeded the 30 threshold on average likewise the validation nse was reduced to 0 48 14 reduction 3 6 representation of water balance and flow duration curve characteristics percent bias for long term water balance and fdc characteristics are presented in fig 6 in the same figure fdcs for the preferred tradeoff solutions and near optimal pareto sets under each strategy are compared against the observed fdc during the calibration period generally absolute biases of fdc characteristics for the validation period were lower than the calibration period most of the near optimal pareto solutions over estimated fmv high segment volume and fms midsegment slope whereas flv low segment volume was mostly under estimated opposite to strategy 1 fhv very high segment volume was mostly under estimated in strategy 2 the maximum absolute bias for strategy 1 was below 30 for strategy 2 the maximum bias was just below 100 meanwhile the largest variability resulted from fms under both strategies whereas the highest variability for flv occurred under strategy 2 the overall rr runoff ratio showed a lower variability compared to the fdc characteristics moreover this index was mostly under estimated during both calibration and validation periods concerning the preferred tradeoff solutions none of them exceed the 30 absolute threshold for any water balance or fdc characteristic for these solutions the most biased fdc characteristic was fms and the least biased was fhv it is worth noting that during validation the minimum observed flow was at least 0 5 m3 s lower than the minimum simulated flow for any preferred tradeoff solution 3 7 relationship between water balance flow duration curve characteristics and ecologically relevant hydrologic indices of interest the results obtained in the previous section indicated that constraining or targeting erhis of interest during model calibration did not drastically worsen long term water balance and fdc representation instead calibration and validation results for erhis were relatively consistent with the behavior of the five sfcs addressed above for instance flv under estimation is related to the observed under estimation of most of the monthly mean flows indices ma12 23 showed in fig 5a likewise baseflow index behavior see ml17 in fig 5b under both calibration and validation periods was consistent with rr lower values in the latter i e lower simulated mean flow resulted in an increase of ml17 which is computed as the ratio between the minimum 7 day flow and the overall mean flow assuming the minimum 7 day flow does not change drastically another example is the under estimation of fhv which is related to the under estimation of dh indices fig 5b which can be caused by missing high flow events or low volume events the same logic applies to fhv over estimation on the other hand fms interpretation posed a different and remarkable case in this study simulations under both strategies were prone to yield steeper midsegment slopes an initial explanation for this behavior was that the chosen model structure and calibrated parameters favored flashiness i e abrupt ascendant and descendant streamflow changes after the occurrence of rainfall events however this explanation contradicted the observed underestimation of the fall rate see ra3 in fig 5c when explicitly considering the timing facet neglected when constructing fdcs we obtained a more consistent interpretation for this purpose it is worth noting that end of summer monthly flows i e ma18 and ma19 linked to july and august months respectively were drastically over predicted whereas september and october monthly flows were under predicted also the timing of flow minima tl1 which usually occurs during the summer season was generally over estimated the latter followed the lower simulated fall rate for the spring to summer transition the lagged timing prediction in annual minima resulted in the over estimation of summer flows likewise there was an additional delay in the transition towards the fall season this delay was one of the reasons for the observed under prediction in monthly flows for the fall season adding up this behavior across all the simulated years mainly explained the fms results given the consistency between erhis constraining targeting and fdc water balance characteristics model structure inadequacies in representing intermediate and baseflows were likely the main factors contributing to the previous inaccuracies 3 8 replication of variability in ecologically relevant hydrologic indices of interest fig 7 shows the distribution of relative errors for iha variability indices under each model calibration strategy opposite to strategy 2 many of the interannual variabilities of monthly flows were not captured by strategy 1 for the calibration period using the 30 relative error threshold however most of these indices were well represented during the validation period under both strategies according to fig 7a the variabilities of winter flows i e ma24 25 ma34 35 were over predicted with median relative errors as high as 110 variabilities of summer flows i e ma29 31 were generally under predicted with absolute median relative errors as high as 50 indices representing variabilities in magnitude and duration of annual extreme water conditions were mostly well represented under both strategies compared to strategy 1 strategy 2 resulted in more over predicted indices under this category outside the acceptability threshold especially those representing the duration of high flows fig 7b it is worth noting that median relative errors for the variability in the duration of annual 1 day minimum flows i e dl6 were slightly below 30 under both strategies and for both calibration and validation periods regarding other streamflow facets i e frequency rate of change and timing some calibration and validation results showed a contrasting behavior fig 7c for the calibration period most of variability indices median relative errors fell within the acceptability threshold regardless of the strategy the most problematic index was the coefficient of variation of the julian day of annual minimum i e tl2 which was over predicted with median relative errors around 100 meanwhile most indices of variability in frequency duration of flow pulses and variability in rate of change of flows were largely over predicted during the validation period with median relative errors as high as 120 therefore our results suggest that water resources managers must be particularly cautious when defining streamflow regime alteration limits based on simulated low flow timing rate of change and extreme events duration and frequency given the observed bias in both associated central tendency and variability indices during model validation 4 conclusions implementing the performance based calibration strategy confirmed that various performance metrics and transformations are better suited for particular streamflow regime facets also it was revealed that r 2 and relative transformed metrics behaved drastically different comparing to kge and sum of square errors based metrics when replicating hydrologic indices moreover results showed that a balanced representation of the streamflow regime is not directly related to the improvement of a particular performance metric instead it responded to tradeoffs among different performance based objective functions stressing different regime facets i e magnitude duration frequency rate of change and timing and flow conditions i e high moderate and low flows the successful implementation of the signature based calibration strategy demonstrated that it is possible to obtain consistent hydrological responses by simultaneously targeting multiple streamflow regime facets more importantly this was achieved without using any performance based objective function however compared to the latter the signature based strategy resulted in higher variability in the near optimal pareto solutions many of them with simulated indices falling outside the acceptability threshold 30 relative error similarly this strategy resulted in a highly variable representation of water balance and fdc characteristics compared to the performance based strategy therefore performance based calibration is preferable it is worth noting that the variability in the near optimal pareto solutions obtained under the two calibration strategies was driven mostly by the representation of low flows as revealed by the highly variable inverse transformed kge values and low flow related fdc characteristics among these solutions the model calibration framework developed here can also be used as a diagnosis tool for instance results revealed limitations of the swat model structure when representing the vertical redistribution of soil moisture fall rate and timing of annual extremes likewise the representation of low flow magnitude and timing rate of change of flows especially rise and fall rates and duration and frequency of extreme flows was limited in terms of interannual variability these limitations impact the definition of limits to hydrologic alteration which are relevant when defining environmental flows and managing social hydrological systems thus water managers and modelers must account for limitations in hydrologic indices replication when defining or selecting streamflow regime targets as part of broader ecohydrological frameworks and applications in ungauged or poorly gauged watersheds in this study we focused on analyzing the objective space and output variables of interest analyzing the near optimal decision variables i e model parameters and intermediate variables representing other water cycle components e g evapotranspiration soil moisture groundwater was out of the scope of this study our framework detected modeling limitations when representing various streamflow regime facets which is useful to address structural inadequacies and improving the overall modeling process future research should involve redesigning hydrological models and tailoring modeling practices e g input data processing model parameters selection choosing calibration validation time periods lengths to better represent ecologically relevant characteristics of riverine ecosystems likewise we recommend future studies to analyze model parameter behavior and other water cycle components when using any of the proposed calibration methods in this regard the proposed performance based method is flexible enough to implement multi variable and multi site model calibration software and data availability source code swat model text files and observed streamflow time series for the computation of 178 hydrologic indices and swat multi objective calibration in python are available at https github com jshernandezs swat pytools pareto optimal solutions and all the objective functions model paramerers and simulations obtained for each calibration strategy and optimization algorithm generation are available at http www hydroshare org resource 99d0dd48a2334679a5278602237e4314 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by the usda national institute of food and agriculture hatch project 1019654 j sebastian hernandez suarez is grateful to fulbright and the colombian ministry of science technology and innovation for the financial support of his doctoral studies 
25763,the reliability and applicability of hydrological models within ecohydrological frameworks are major concerns two multi objective model calibration strategies were formulated to achieve a balanced representation of ecologically relevant hydrologic indices two approaches were employed 1 a performance based method constraining the targeted hydrologic indices and 2 an unconstrained signature based method explicitly incorporating the targeted hydrologic indices into multiple objective functions both strategies were successful in representing most of the selected hydrologic indices within a 30 relative error acceptability threshold while yielding consistent runoff predictions in a watershed the performance based strategy was preferred since it showed a lower dispersion of near optimal pareto solutions when representing the selected indices based on water balance and flow duration curve characteristics still the overall representation of low flow magnitude and timing rise and fall rates and duration and frequency of extreme flows was limited in terms of interannual variability due to the hydrological model structural inadequacies keywords multi objective optimization streamflow regime environmental flows hydrologic indices swat 1 introduction the streamflow regime is widely acknowledged as a key determinant of the ecological integrity of riverine ecosystems poff et al 1997 sofi et al 2020 both climate and human driven alterations to natural streamflow fluctuations affect the structure and functioning of these ecosystems threatening biodiversity and restricting the provision of ecosystem services palmer and ruhi 2019 vörösmarty et al 2010 therefore understanding and evaluating the impacts of climate change and human interventions on the streamflow regime is critical to inform and prioritize environmental management alternatives hassanzadeh et al 2017 mittal et al 2016 a broadly accepted approach to characterizing streamflow regimes is to compute flow statistics from streamflow hydrographs these statistics also known as hydrologic signature metrics streamflow characteristics sfcs or ecologically relevant hydrologic indices erhis generally represent five fundamental facets magnitude frequency duration timing and rate of change of flows poff and zimmerman 2010 currently there are over 200 flow statistics relevant to stream ecology archfield et al 2014 olden and poff 2003 vogel et al 2007 these indices are usually employed in ecohydrological applications such as stream classification kennard et al 2010 mcmanamay et al 2014 prediction of stream health or distribution of riverine species hernandez suarez and nejadhashemi 2018 kakouei et al 2017 and environmental flow determination mathews and richter 2007 poff et al 2010 since these applications generally cover large spatial scales statistical and hydrological models have been increasingly used especially to predict regional changes in erhis due to climate and anthropogenic factors caldwell et al 2015 mittal et al 2016 yang et al 2016 hydrological models are usually preferred over regional statistical approaches because they can explicitly consider modifications in land use environmental conditions and management practices hall et al 2017 shrestha et al 2016 moreover some environmental flow frameworks recommend using hydrological models for predicting streamflow in poorly gauged or ungauged locations peters et al 2012 poff et al 2010 however there is a growing number of studies revealing important limitations of hydrological models in representing erhis especially when these models are calibrated based on traditional performance metrics such as the nash sutcliffe efficiency nse murphy et al 2013 shrestha et al 2014 vigiak et al 2018 vis et al 2015 these limitations include over or underprediction of low and high flow indices wenger et al 2010 high errors uncertainties when predicting erhis related to timing duration frequency and or rate of change of flows murphy et al 2013 shrestha et al 2014 vigiak et al 2018 and different sets of equally well performing model parameters in terms of traditional metrics yielding very different performances in terms of erhis vis et al 2015 current model calibration approaches for addressing limitations in erhis representation can be classified into two major categories in the first category hereafter referred to as performance based objective functions are formulated based on traditional performance metrics with different streamflow transformations e g square root logarithm inverse to stress or balance the importance of different flow conditions on the other hand calibration approaches in the second category hereafter referred to as signature based explicitly incorporate sfcs of interest into the objective functions hallouin et al 2020 kiesel et al 2017 2020 pool et al 2017 vis et al 2015 zhang et al 2016 in ecohydrological applications the choice of sfcs of interest has been mainly based on riverine species preferences hallouin et al 2020 kiesel et al 2017 2020 pool et al 2017 whereas hydrological applications usually target flow duration curve fdc features runoff ratios and basic discharge statistics chilkoti et al 2018 euser et al 2013 fernandez palomino et al 2020 pfannerstill et al 2014 2017 sahraei et al 2020 shafii and tolson 2015 yilmaz et al 2008 some applications using performance based approaches target specific flow conditions garcia et al 2017 mizukami et al 2019 whereas others use one or multiple objective functions to attain an acceptable overall representation of the streamflow regime hallouin et al 2020 when combining multiple objective functions studies either use aggregated single objective functions vis et al 2015 or pure multi objective approaches chilkoti et al 2018 hernandez suarez et al 2018 sahraei et al 2020 in general signature based approaches provide better predictions of pre selected sfcs compared to performance based approaches hallouin et al 2020 however those sfcs that are not included in the original objective function formulation are not necessarily well represented or better performing than traditional approaches using streamflow transformations hallouin et al 2020 during the last decade researchers have obtained a better understanding of the implications of model calibration into ehris replication for instance several studies have demonstrated that the objective function choice or formulation influences the prediction of flow statistics kiesel et al 2020 pool et al 2017 shafii and tolson 2015 vis et al 2015 also these studies showed that optimality in terms of traditional performance metrics does not necessarily result in optimal solutions for ecohydrological purposes hallouin et al 2020 kiesel et al 2020 in ecohydrological applications regardless of the optimization scheme for model calibration it is uncommon to find solutions yielding acceptable results for all erhis of interest also finding an individual simulation with acceptable results for both low and high flow conditions is unusual therefore simulation ensembles such as median or averages of optimal results or their clusters are recommended hernandez suarez et al 2018 vis et al 2015 it is worth noting that most of the calibration approaches used in previous ecohydrological studies have run on single objective mode i e multi metric aggregated functions hence those results depend on the weight assigned to each erhi or performance metric considered within the objective function zhang et al 2016 and tradeoffs among different indices performance metrics or regime facets are not fully explored the goal of this study was to develop calibration strategies providing a balanced streamflow regime representation among the different regime facets i e magnitude frequency duration timing and rate of change two strategies were developed to compare both performance and signature based calibration approaches the strategy using a performance based approach was improved by incorporating a novel constraint formulation to obtain simulations with targeted erhis within pre defined acceptability thresholds for the signature based strategy tradeoffs between different streamflow regime facets were explicitly considered these calibration strategies were implemented in an agriculture dominated watershed in michigan us using the recently developed evolutionary multi objective optimization algorithm called unified non dominated sorting genetic algorithm iii u nsga iii and the soil and water assessment tool swat to the best of our knowledge previous multi objective calibration approaches for ecohydrological applications have not explicitly considered optimization routines constraining the performance of erhis of interest likewise this is the first time that a multi objective calibration approach is applied to targeted erhis pursuing a balanced representation of the overall streamflow regime while explicitly considering different regime facets 2 materials and methods 2 1 overview two different strategies for multi objective calibration were evaluated to improve the representation of the overall streamflow regime in a watershed model strategy 1 employed a constrained performance based approach whereas strategy 2 used a constraint free signature based approach fig 1 strategy 1 consisted of three major steps in the first step the goal was to identify a reduced set of performance metrics that jointly represented a wide list of erhis then in the second step a tailored constraint was formulated to generate individual simulations with an acceptable replication of a reduced set of erhis of interest this formulation was based on pre defined acceptability criteria for erhi replication moreover the selection of erhis of interest was performed by targeting a balanced representation of different flow regime facets in the third step the outputs of the previous steps were used as inputs to formulate a multi objective optimization problem for model calibration meanwhile strategy 2 consisted of two major steps in the first step a reduced set of erhis was defined to provide a balanced representation of different regime facets then several objective functions representing different regime facets were formulated these objective functions were considered as inputs of the problem formulation in step 2 this formulation was intended to explore tradeoffs in the simulation of different regime facets for each strategy near optimal pareto solutions were obtained using an evolutionary multi objective optimization algorithm finally preferred tradeoff solutions were identified and compared using multicriteria decision making mcdm methods 2 2 study area the proposed strategies were evaluated in the honeyoey creek pine creek watershed hydrologic unit code 0408020203 located in east central michigan us fig 2 this watershed has a drainage area of 1010 km2 and is situated within the saginaw river watershed which drains into lake huron the saginaw river watershed is identified as an area of concern by the us environmental protection agency usepa due to water pollution wildlife habitat degradation loss of recreational values among others usepa 2015 according to data from the national agricultural statistics service nass of the us department of agriculture usda agriculture is the dominant land use 50 of the area followed by forests 24 wetlands 16 pasturelands 7 and urban development 3 usda nass 2012 2 3 watershed model the soil and water assessment tool swat 2012 rev 622 was used to simulate the streamflow regime in the study area swat is a semi distributed process based continuous time watershed model that can operate on a daily or sub daily time step swat is mainly used to evaluate the impact of land use and management practices on water sediments nutrients pesticides and bacteria yields at the watershed scale arnold et al 2012 when using swat a watershed is divided into subwatersheds which are further discretized into hydrologic response units hrus hrus are geographical units with homogeneous land use soil and topographical characteristics swat inputs controlling the water balance include daily or sub daily precipitation maximum and minimum air temperatures solar radiation wind speed and relative humidity swat simulates the watershed hydrology in two phases land loading and water network routing simulated hydrological processes include snow accumulation and melting canopy storage plant growth evapotranspiration infiltration surface runoff soil water redistribution lateral flow groundwater flows and channel routing neitsch et al 2011 in this study swat was used to obtain daily streamflow from 2003 to 2014 calibration period and from 1983 to 1994 validation period at the outlet of the honeyoey creek pine creek watershed fig 2 a warm up period of two years was used to minimize the effect of initial conditions on the simulations simulated streamflow values were compared against daily observations obtained from the pine river near midland us geological survey usgs gauging station id 04155500 usgs 2020 input daily precipitation and max min temperature data from 1981 to 2014 were collected from two weather stations provided by the national centers for environmental information ncei of the national oceanic and atmospheric administration noaa noaa ncei 2020 the missing weather input data were estimated using swat s stochastic weather generator wxgen neitsch et al 2011 the watershed was divided into 250 subwatersheds each consisting of a unique hru obtained from dominant land use soil and slope characteristics these subwatersheds were delineated using stream network data from the national hydrography dataset nhd and pre defined units obtained from the michigan institute for fisheries research einheuser et al 2012 elevation data with a 30 m resolution was obtained from the national elevation dataset provided by the usgs national map usgs 2018 land use was extracted from the 30 m resolution cropland data layer cdl which was obtained from usda nass 2012 soil characteristics were extracted from the soil survey geographic database ssurgo provided by the usda natural resources conservation service nrcs usda nrcs 2020 potential evapotranspiration was calculated using the penman monteith equation monteith 1965 whereas surface runoff was computed using the soil conservation service scs curve number method usda scs 1972 streamflow was routed through the channel network using the variable storage coefficient method williams 1969 the model was calibrated by adjusting 15 parameters whose description and calibration ranges are reported in table 1 2 4 strategy 1 constrained performance based model calibration 2 4 1 performance metrics selection a reduced set of performance metrics were used for objective functions formulation from a list of widely used measures see table 2 these measures included nse nash and sutcliffe 1970 original and modified versions of the kling gupta efficiency kge gupta et al 2009 kling et al 2012 the index of agreement ioa willmott 1981 and the coefficient of determination r 2 the fourth root mean quadrupled error r4ms4e was also considered in order to emphasize the largest residuals expected under high flow conditions since both nse and the root mean square error rmse vary only with the sum of squared model residuals just the former was contemplated in this study following gupta et al 2009 nse and kge can be expressed in terms of three components representing correlation bias and variability correlation relates to timing and hydrograph shape meanwhile bias and variability are aimed to reproduce the first and second moments of the distribution of observations which mainly affect magnitude related sfcs these three components interact differently under each performance measure for instance bias is scaled by the standard deviation of observations in nse thus in presence of high variability the bias component might be less important when obtaining optimal values in addition correlation and variability components interact with each other in nse which generally results in underestimation of the latter gupta et al 2009 as an alternative kge provides a more balanced representation of correlation bias and variability while avoiding interactions among these components gupta et al 2009 by considering r 2 as an additional measure we aimed to evaluate the role of the correlation component in erhis replication meanwhile ioa was included to consider a different way of normalizing the sum of square errors and its effects on erhis replication to accentuate different flow conditions i e low moderate and high relative errors and error transformations were considered except for r4ms4e all measures included their standard versions along with logarithmic inverse and square root transformations relative error versions were only used for nse and ioa it is worth mentioning that in general the standard versions favor high flows representation square root transform is used for highlighting moderate or average flow conditions whereas logarithmic inverse and relative error versions accentuate low flows bennett et al 2013 krause et al 2005 single objective model calibration was executed for each performance metric and transformation indicated above resulting in 23 individual optimization problems each minimization objective function f was defined as 1 p m where p m is the transformed performance metric to be maximized for r4ms4e f p m as this metric has to be minimized as a next step 171 erhis reported by henriksen et al 2006 and seven erhis proposed by archfield et al 2014 were computed for each of the 23 optimal solutions simulated erhis were compared against those obtained from streamflow observations by calculating relative errors for each index e r e l for a vector of model parameters θ as follows 1 e r e l i θ i i y θ i i y ˆ i i y ˆ 100 where i i is the i th hydrologic index evaluated for simulations y θ and observations y ˆ then erhis within a pre defined relative error threshold were identified for each optimal solution it was expected that optimal results from different performance metrics and transformations yield different well replicated erhis therefore the final choice of performance metrics was determined by selecting up to six transformed measures that jointly represented the maximum number of erhis within the pre defined relative error threshold for the selection procedure the transformed measure with the highest number of erhis within the acceptability threshold was selected then another transformed measure was identified based on the remaining erhis and added to the list of selected measures the previous step was repeated until either attaining the maximum number of well replicated erhis or the pre defined maximum number of objective functions it is worth noting that the fraction of non dominated solutions with respect to the total population increases with the number of objective functions slowing down the search process deb and jain 2014 likewise a higher population size is required to maintain a good exploration of large dimensional spaces which increases the number of function evaluations and the overall computational time for these reasons we decided to limit the number of objective functions to six in this study a real parameter genetic algorithm ga goldberg 1991 was used for single objective optimization particularly tournament selection simulated binary crossover sbx deb and agrawal 1994 and polynomial mutation deb 2001 were designated as ga operators the optimization algorithm ran for 250 generations with a population size of 100 resulting in a total of 25 000 model evaluations for each problem the crossover probability and distribution index for the sbx operator were defined as 0 9 and 10 respectively likewise the mutation probability and distribution index for the polynomial mutation operator were defined as 1 15 i e the reciprocal of the number of calibration parameters and 20 respectively on the other hand the relative error threshold for ehri replication was defined as 30 following uncertainty in the estimation of hydrologic indices reported by kennard et al 2009 when using 15 year time series this threshold has also been used in previous ecohydrological studies to evaluate the performance of erhis predictions caldwell et al 2015 hernandez suarez et al 2018 vis et al 2015 2 4 2 constraint definition traditionally hydrologic signatures have been used in model calibration either as objective functions or post calibration evaluation criteria shafii and tolson 2015 here we used a set of relevant signatures as constraints given a pre defined acceptability threshold this set can be identified by the modeler depending on the ecohydrological application needs in this study we used 32 indicators of hydrologic alteration iha the nature conservancy 2009 divided into five categories table 3 each representing specific streamflow regime facets in addition seven indices presented by archfield et al 2014 which describe fundamental stochastic properties of streamflow time series were included in the constraint definition the aforementioned 39 indices are described in table 3 for consistency an acceptability threshold of 30 relative error was used for constraining erhis prediction the constraint which was formulated as the sum of two components aggregates the performance of all ehris of interest into a single measure the first component is the number of indices with relative errors outside the pre defined acceptability threshold for ehris replication the second component is a weighted sum of relative violations by each index with respect to the pre defined acceptability threshold the constraint can be expressed as follows 2 c v θ i 1 m k i θ 1 w i 1 τ i i y θ i i y ˆ i i y ˆ 1 k i θ 0 if 1 τ i i y θ i i y ˆ i i y ˆ 1 0 1 otherwise w i 1 g h i where cv θ is the constraint violation for simulations y θ m is the total number of indices i e 39 in this study τ is the acceptability threshold expressed as the absolute value of a fraction between 0 and 1 0 30 is used for this study w i is the weighting factor for the i th index g is the number of index categories i e 6 in this study and h i is the total number of indices in the category that contains the i th index the weighing factor was explicitly incorporated to provide a balanced contribution from different streamflow regime facets a solution is considered feasible when cv θ attains a value of zero but for ease of handling the constraint with an optimization algorithm we convert it to an inequality constraint as c v θ 0 by introducing the constraint formulation presented above the optimization algorithm is forced to find streamflow simulations in which all erhis of interest are estimated within the acceptable range i e the relative error is within 30 it is worth noting that the constraint definition is flexible enough to designate different acceptability thresholds τ i for each index this might be necessary when it is needed to iteratively relax certain acceptability conditions to find feasible solutions 2 5 strategy 2 unconstrained signature based model calibration under this strategy an objective function was formulated for each index category presented in table 3 as follows 3 f j θ i g j i i y θ i i y ˆ i i y ˆ where f j θ is the objective function for the j th category and g j is the set of indices belonging to the j th category each objective function represents the total error obtained under each index category relative errors were used to normalize the contribution from different indices no constraints were formulated for this calibration strategy therefore opposite to strategy 1 no pre defined acceptability thresholds for erhis replication and no weighting factors were required in strategy 2 2 6 evolutionary multi objective optimization algorithm in both calibration strategies the goal was to determine the values for the vector of model parameters θ i e decision variables that minimize the objective functions formulated for each strategy each decision variable θ p p 1 2 15 could take a value within the ranges defined in table 1 in strategy 1 those model simulations with c v θ 0 were considered as feasible see eq 2 the remaining were infeasible an evolutionary multi objective optimization algorithm u nsga iii seada and deb 2016 was implemented to address the optimization problems resulting from each strategy u nsga iii is a population based algorithm that employs crossover and mutation operators along with non dominated sorting and reference directions to move towards near optimum pareto solutions reference directions are vectors evenly filling the objective space this algorithm can be used for single multi i e 2 or 3 objective functions and many objective i e 3 objective functions optimization problems and stems from the nsga iii algorithm deb and jain 2014 it is worth mentioning that u nsga iii can handle both unconstrained and constrained problems for unconstrained problems during the non domination sorting any two solutions are compared using just the objective function values a solution x1 dominates a solution x2 when 1 x1 is no worse than x2 in all objective functions and 2 x1 is better than x2 in at least one objective function deb 2001 in constrained problems the concept of constraint domination is used instead a solution x1 constraint dominates a solution x2 when 1 x1 is feasible and x2 is infeasible 2 both x1 and x2 are infeasible but x1 has a lower constraint violation cv or 3 both x1 and x2 are feasible and x1 dominates x2 using the traditional domination principle jain and deb 2014 in non domination sorting feasible solutions will always be on top of infeasible solutions likewise the selection operation when creating the offspring population is modified for constrained problems jain and deb 2014 nsga iii and u nsga iii have been implemented in previous water resources applications such as multivariate model calibration using streamflow and evapotranspiration data herman et al 2020 multi objective calibration targeting different flow conditions hernandez suarez et al 2018 irrigation scheduling kropp et al 2019 mwiya et al 2020 reservoir design and operation chen et al 2020 pourshahabi et al 2020 and optimization of land use practices raschke et al 2021 in this study an interface for modifying swat input files and executing the model was developed in python 3 7 this interface also included the computation of the erhis reported by henriksen et al 2006 and archfield et al 2014 and was coupled with the python library pymoo blank and deb 2020 to implement the u nsga iii algorithm the stopping criterion was set as a maximum of 1000 generations for the multi objective optimization with a number of reference directions assigned equal to 100 well spaced reference directions were generated using the recently developed riesz s energy method blank et al 2021 included in the pymoo library the operators and parameters chosen for crossover and mutation were the same as the ones presented in section 2 4 1 for the ga which are standard and recommended deb et al 2002 convergence to a near optimal solution was analyzed using the hypervolume indicator auger et al 2009 which is a measure of the collective volume of the region dominated by the pareto optimal solutions in the objective space 2 7 selection of preferred tradeoff solutions since we were interested in obtaining solutions providing balanced representations of different streamflow regime facets we compared a set of preferred solutions from different mcdm methods particularly two approaches were implemented compromise programming zeleny 2011 and the pseudo weight method deb 2001 the compromise programming approach identifies the closest pareto optimal solution to a reference point using a user defined distance metric usually the reference point is the ideal point representing the best expected objective function values in this study the ideal point was the origin of the objective space as distance metrics we used the ℓ p norm with p 2 euclidian distance and p chebyshev distance the latter is preferred for non convex pareto optimal solutions the metrics for a pareto optimal solution were computed as follows branke et al 2008 4 ℓ p m 1 m f m z m p 1 p 5 ℓ p max m f m z m where m is the number of objective functions f m is the value for the m th objective function and z m is the value of the m th component of the reference point before applying any distance metrics the objective functions were normalized to values between 0 and 1 meanwhile the pseudo weight method generates a vector for each pareto optimal solution representing the relative importance or weight of each objective function the sum of the different weights in each vector is forced to one the pseudo weight w i for the i th component in a pareto optimal solution was computed as follows deb 2001 6 w i f i max f i f i max f i min m 1 m f m max f m f m max f m min where f i max and f i min are the maximum and minimum values for the i th objective function among all pareto optimal solutions respectively the denominator in eq 6 guarantees that the sum of all pseudo weight vector components for a pareto solution is equal to one pseudo weights are proportional to the difference between the maximum objective function value and the solution s value for a particular component thus a higher pseudo weight indicates that the point is closer to the minimum objective function value for that component in other words a higher pseudo weight value indicates a higher preference for the corresponding objective function in this study we selected the most balanced pareto optimal solution as the one with the closest pseudo weight vector to the m dimension target vector 1 m 1 m different target vectors can be used to explore how a pareto solution changes when giving more relevance to a particular objective function 2 8 evaluation of calibration results using water balance flow duration curve characteristics and additional hydrologic indices the flow duration curve fdc is the complement of the streamflow cumulative distribution function vogel and fennessey 1994 fdcs are signatures of runoff variability and summarize a watershed s ability to generate streamflow values of different magnitude yilmaz et al 2008 fdcs have been widely used for model evaluation and calibration fenicia et al 2018 since a fdc is a frequency domain representation of a hydrograph information concerning to streamflow timing is lost limiting its utility to diagnose the overall streamflow regime however some characteristics extracted from fdcs are useful for understanding key hydrological processes and their ecohydrological significance mcmillan 2020a 2020b in this study we computed the percent bias pbias of four indices extracted from fdcs to evaluate the consistency between calibration results and sfcs that have been typically used in signature based model calibration the characteristics derived from fdcs were the very high segment volume fhv high segment volume fmv midsegment slope fms and low segment volume flv ley et al 2016 yilmaz et al 2008 the aforementioned segments were subjectively defined by yilmaz et al 2008 using the 2 20 and 70 flow exceedance probabilities fms is a signature of the vertical soil moisture redistribution and streamflow flashiness likewise fhv provides additional information regarding streamflow flashiness and quantifies watershed reactions to large precipitation events meanwhile fmv quantifies the watershed response to heavy rainfall finally flv which is related to long term baseflow was computed using the modification reported by casper et al 2012 to reduce the effect of the difference in lowest simulated and observed flows on the pbias computation long term water balance was also considered by computing the pbias in the overall runoff ratio rr yilmaz et al 2008 the iha indices that were selected in this study are computed from metrics obtained on an annual basis and represent the central tendency i e mean of annual metrics table 3 when setting environmental flows or evaluating streamflow regime alteration widely used methods such as the range of variability approach richter et al 1996 1997 also consider the associated interannual variability in those metrics these methods define streamflow alteration targets as a function of central tendency and variability metrics these targets are defined for each streamflow regime facet using meaningful indices and are further customized depending on the available ecological information of the study area poff et al 2010 richter et al 1997 given the relevance of streamflow variability in ecohydrological applications especially in the definition of limits of streamflow alteration we evaluated the impact of the two calibration strategies defined in this study which use only central tendency indices in the replication of associated variability indices these variability indices are expressed here in terms of coefficients of variation following henriksen et al 2006 3 results and discussion 3 1 performance of single objective model calibration using transformed metrics the relative errors for ehris replication under each optimal solution using the transformed measures indicated in section 2 4 1 are presented in fig 3 these results were obtained as part of the objective functions selection routine under strategy 1 using hierarchical clustering with euclidean distances and ward s method five groups of performance metrics were identified based on their similarity in replicating erhis these groups are presented in table 4 and can be visualized in fig 3 for the different categories of hydrologic indices performance metrics were arranged by similarity in the y axis these groups revealed that optimal solutions using r 2 and relative transformed metrics as objective functions behaved drastically different compared to the other evaluated metrics generally optimal simulations using the former metrics were able to represent those erhis that did not fall within the 30 relative error threshold using kge and sum of square errors based metrics for example in fig 3b indices ml9 and ml10 are better represented by r 2 and relative transformed metrics than any other metrics similar examples can be observed for dl6 and dl11 in fig 3d for fh5 in fig 3g or for ra3 and ra6 in fig 3k still the overall performance in erhis replication was very poor for r 2 and relative transformed metrics having less than 51 of erhis within the threshold according to table 4 other poor performing metrics included inverse and log transformed nse these results suggest that those measures should be used as complementary criteria rather than objective functions in single objective model calibration when targeting the overall streamflow regime representation different performance metrics or groups of metrics are more suitable in replicating specific index categories or streamflow regime facets see table 4 in terms of magnitude standard or square root transformed metrics are preferred when targeting average and high flows ma and mh respectively whereas low flows ml were best represented by optimal solutions when using r 2 for model calibration regarding duration kge and kge provided the best performing solutions for low flows dl whereas standard and square root transformed metrics were better suited for high flows dh for frequency most of the standard square root and inverse transformed metrics were better suited for low flows fl whereas kgesqrt yielded the highest proportion of well replicated high flow fh indices with respect to timing standard square root or most of the log transformed metrics are preferred when targeting average ta and high flows th meanwhile ioarel ioalog kge inv and r 2 were the best performing metrics when looking for optimal solutions in replicating low flows timing tl those indices representing the change of flow and reversals showed an acceptable replication under some r 2 based or relative transformed metrics however in general an acceptable representation of rate of change indices ra was quite difficult and none of the performance metrics that were employed in this study resulted in an outstanding performance finally all of the magnificent seven indices mag were well replicated by optimal results using standard square root or log transformed metrics it is worth noting that none of the 23 identified optimal solutions were able to represent five indices within the pre defined acceptability threshold of 30 relative error these indices were the mean duration of flows exceeded 25 of the time dh21 mean low flow pulse duration dl16 mean low flow pulse count fl1 mean number of high flow events using the flow exceeded 25 of the time as a threshold fh9 and mean high flow volume using the median annual flow as a threshold mh21 3 2 selected metrics for constrained performance based model calibration the selection process of objective functions under strategy 1 resulted in three different lists of six transformed measures jointly representing 168 out of 178 erhis within the 30 relative error range these lists had in common the first five measures standard and inverse kge and standard inverse and square root r 2 the sixth measure was either r 2 log kge inv or ioarel we decided to proceed with the list containing ioarel because opposite to the other two lists this one represented all rate of change indices within the 30 acceptability threshold the optimal solution using standard kge was able to provide the highest number of indices within the error threshold i e 128 indices or 72 of all erhis see table 4 note that the selected list of metrics includes most of the best performing measures for each group reported in table 4 i e metrics in bold however this list did not well represent five indices related to flow variability and high flow magnitude variability in annual minima of daily flows dl6 and ml21 variability in february and august flows ma25 and ma31 respectively and mean peak flows using the median annual flow as a threshold mh24 these indices were added to the five indices that were not represented by an optimal solution see section 3 1 3 3 overall performance of pareto optimal solutions each multi objective calibration strategy was executed using 20 threads in parallel on a machine equipped with two intel xeon cpu e5 2640 processor at 2 5 ghz with 64 gb ram running ubuntu 16 04 7 lts total computation time for strategies 1 and 2 were 32 43 and 30 86 h respectively strategy 1 successfully identified pareto solutions satisfying the defined constraint for all 39 erhis of interest the first feasible solution was found at generation 48 and convergence to a near optimal pareto front was achieved after 800 generations once the hypervolume indicator started to show a steady behavior fig 4 a pareto front sizes at the end of each generation over the u nsga iii search process did not exceed 35 solutions with 25 near optimal pareto solutions for the 1000th generation similarly strategy 2 converged to a near optimal pareto front after 800 generations in this case pareto front sizes at the end of each generation mostly varied between 20 and 40 solutions with 29 near optimal pareto solutions for the 1000th generation performance of near optimal pareto solutions for both strategies improved with respect to the initial random population sampled from uniform distributions of model calibration parameters fig 4b and c near optimal solutions from strategy 1 resulted in linear correlations r between 0 80 and 0 85 whereas strategy 2 provided results with a broader range for r between 0 70 and 0 85 all pareto solutions from strategy 1 overestimated up to 1 3 times the standard deviation in observations while showing a ratio between simulated and observed means between 0 95 and 1 05 meanwhile strategy 2 resulted in a more balanced and wider set of near optimal pareto solutions in terms of both simulated observed standard deviation and mean ratios α and β respectively under both strategies the standard deviation of model residuals was around 60 70 of the standard deviation of observations a summary of the metrics and objectives median interquartile range iqr maximum and minimum that were used to obtain the near optimal pareto and preferred tradeoffs solutions are presented in table 5 in both strategies the performance metric showing the highest variability as presented by iqr was kgeinv which emphasizes low flow conditions in general near optimal pareto solutions from strategy 2 showed a higher variability of objective function values compared to strategy 1 in terms of kgeinv strategy 2 provided an overall better performance but also had solutions with very low values minimum was 1 43 it is worth noting that none of the maximum values for the performance metrics chosen under strategy 1 were as high as those found when executing single objective model calibration for example the maximum kge of 0 83 obtained from the near optimal pareto set from strategy 1 reported in table 5 was below the near optimum value of 0 88 reported for kge in table 4 these results indicate that simulations with erhis of interest within 30 relative error are not necessarily close to an optimum in terms of a particular performance metric 3 4 replication of ecologically relevant hydrologic indices of interest fig 5 shows the distribution of relative errors for each erhi of interest and model calibration strategy during both calibration and validation periods during the calibration period which was defined between 2003 and 2014 all the indices computed from the near optimal pareto set from strategy 1 fell within the 30 relative error range meanwhile strategy 2 provided median values for almost all erhi of interest ma19 i e august mean flow was the exception within the same range with some near optimal pareto solutions generating index values outside this range in general strategy 1 resulted in a lower variability of ehris values compared to strategy 2 additionally median relative errors had a similar behavior among both calibration strategies some exceptions included dh1 to dh5 i e duration of annual maxima in fig 5b which exhibited opposite trends under both strategies i e overestimation for strategy 1 and underestimation for strategy 2 indices that showed the highest variability during the calibration period in both strategies were mostly related to low flow conditions these indices include dl1 to dl5 i e duration of annual minima and ml17 i e baseflow index in fig 5b row 1 dl16 and fl1 i e low flow pulse duration and frequency and ra3 i e fall rate in fig 5c row 1 and mag3 and mag4 i e skewness and kurtosis in fig 5d row 1 strategy 1 presented the most biased results for indices in the iha3 category representing the timing of annual extremes this is consistent with the f 3 median value of 26 4 for this strategy reported in table 5 which is the highest median value among both strategies and objectives used in strategy 2 it is worth noting that median values for objectives f 2 and f 4 related to duration and frequency respectively were lower and better for strategy 1 than strategy 2 near optimal pareto sets of model parameters obtained during model calibration were validated for a 12 year period between 1983 and 1994 validation results for the replication of erhis of interest are also presented in fig 5 from the list of 39 erhi of interest the median relative errors for three indices fell outside the acceptability range of 30 these indices were ra3 for both strategies fl1 for strategy 1 and dh15 for strategy 2 fig 5c row 2 ra3 relative error values were mostly distributed within 50 and 40 and median relative error values for fl1 and dh15 were very close to 30 and 30 limits respectively excepting dh15 for strategy 2 these results are indicative of the robustness of both calibration strategies variability of erhis of interest behaved similarly during the calibration and validation periods however dh15 and th1 related to duration and timing of high flow events drastically increased their variability during the validation period 3 5 performance of preferred tradeoff solutions we obtained three different solutions two from strategy 1 and one from strategy 2 targeting a balanced representation of the different streamflow regime facets both euclidean and chebyshev distances used for the compromise programming method selected the same preferred solution from the near optimal pareto sets under each calibration strategy for strategy 2 the pseudo weight method s preferred solution was the same as the compromise programming method the overall calibration performance for these preferred solutions is reported on the right side of table 5 there are no major differences between the three preferred solutions in terms of performance during the calibration period strategy 2 provided slightly better results for kge and kgeinv whereas strategy 1 solutions presented better r 2 and r 2 sqrt values it is worth noting that nse values were 0 61 and 0 60 for compromise programming and pseudo weight solutions in strategy 1 respectively the preferred solution under strategy 2 attained a lower nse of 0 56 regarding the replication of erhis of interest the strategy 2 preferred solution provided in average lower absolute relative errors for five of six categories of hydrologic indices during the calibration period meanwhile strategy 1 preferred solutions attained better replication results for the iha4 category which is related to frequency and duration of high and low pulses during the validation period preferred solutions from strategy 1 improved in terms of kge kgeinv and ioarel 7 42 and 1 respectively while slightly worsening in terms of r 2 r 2 sqrt and r 2 inv 3 4 and 6 respectively nse improved to 0 65 and 0 63 6 for the compromise programming and pseudo weight method solutions respectively average absolute relative errors improved for the three first erhi categories i e magnitude of monthly flows duration and timing of extremes and deteriorated for the remaining categories especially for the iha4 category regarding strategy 2 the preferred solution generally worsened in terms of both performance metrics and erhi replication especially for the iha4 category which exceeded the 30 threshold on average likewise the validation nse was reduced to 0 48 14 reduction 3 6 representation of water balance and flow duration curve characteristics percent bias for long term water balance and fdc characteristics are presented in fig 6 in the same figure fdcs for the preferred tradeoff solutions and near optimal pareto sets under each strategy are compared against the observed fdc during the calibration period generally absolute biases of fdc characteristics for the validation period were lower than the calibration period most of the near optimal pareto solutions over estimated fmv high segment volume and fms midsegment slope whereas flv low segment volume was mostly under estimated opposite to strategy 1 fhv very high segment volume was mostly under estimated in strategy 2 the maximum absolute bias for strategy 1 was below 30 for strategy 2 the maximum bias was just below 100 meanwhile the largest variability resulted from fms under both strategies whereas the highest variability for flv occurred under strategy 2 the overall rr runoff ratio showed a lower variability compared to the fdc characteristics moreover this index was mostly under estimated during both calibration and validation periods concerning the preferred tradeoff solutions none of them exceed the 30 absolute threshold for any water balance or fdc characteristic for these solutions the most biased fdc characteristic was fms and the least biased was fhv it is worth noting that during validation the minimum observed flow was at least 0 5 m3 s lower than the minimum simulated flow for any preferred tradeoff solution 3 7 relationship between water balance flow duration curve characteristics and ecologically relevant hydrologic indices of interest the results obtained in the previous section indicated that constraining or targeting erhis of interest during model calibration did not drastically worsen long term water balance and fdc representation instead calibration and validation results for erhis were relatively consistent with the behavior of the five sfcs addressed above for instance flv under estimation is related to the observed under estimation of most of the monthly mean flows indices ma12 23 showed in fig 5a likewise baseflow index behavior see ml17 in fig 5b under both calibration and validation periods was consistent with rr lower values in the latter i e lower simulated mean flow resulted in an increase of ml17 which is computed as the ratio between the minimum 7 day flow and the overall mean flow assuming the minimum 7 day flow does not change drastically another example is the under estimation of fhv which is related to the under estimation of dh indices fig 5b which can be caused by missing high flow events or low volume events the same logic applies to fhv over estimation on the other hand fms interpretation posed a different and remarkable case in this study simulations under both strategies were prone to yield steeper midsegment slopes an initial explanation for this behavior was that the chosen model structure and calibrated parameters favored flashiness i e abrupt ascendant and descendant streamflow changes after the occurrence of rainfall events however this explanation contradicted the observed underestimation of the fall rate see ra3 in fig 5c when explicitly considering the timing facet neglected when constructing fdcs we obtained a more consistent interpretation for this purpose it is worth noting that end of summer monthly flows i e ma18 and ma19 linked to july and august months respectively were drastically over predicted whereas september and october monthly flows were under predicted also the timing of flow minima tl1 which usually occurs during the summer season was generally over estimated the latter followed the lower simulated fall rate for the spring to summer transition the lagged timing prediction in annual minima resulted in the over estimation of summer flows likewise there was an additional delay in the transition towards the fall season this delay was one of the reasons for the observed under prediction in monthly flows for the fall season adding up this behavior across all the simulated years mainly explained the fms results given the consistency between erhis constraining targeting and fdc water balance characteristics model structure inadequacies in representing intermediate and baseflows were likely the main factors contributing to the previous inaccuracies 3 8 replication of variability in ecologically relevant hydrologic indices of interest fig 7 shows the distribution of relative errors for iha variability indices under each model calibration strategy opposite to strategy 2 many of the interannual variabilities of monthly flows were not captured by strategy 1 for the calibration period using the 30 relative error threshold however most of these indices were well represented during the validation period under both strategies according to fig 7a the variabilities of winter flows i e ma24 25 ma34 35 were over predicted with median relative errors as high as 110 variabilities of summer flows i e ma29 31 were generally under predicted with absolute median relative errors as high as 50 indices representing variabilities in magnitude and duration of annual extreme water conditions were mostly well represented under both strategies compared to strategy 1 strategy 2 resulted in more over predicted indices under this category outside the acceptability threshold especially those representing the duration of high flows fig 7b it is worth noting that median relative errors for the variability in the duration of annual 1 day minimum flows i e dl6 were slightly below 30 under both strategies and for both calibration and validation periods regarding other streamflow facets i e frequency rate of change and timing some calibration and validation results showed a contrasting behavior fig 7c for the calibration period most of variability indices median relative errors fell within the acceptability threshold regardless of the strategy the most problematic index was the coefficient of variation of the julian day of annual minimum i e tl2 which was over predicted with median relative errors around 100 meanwhile most indices of variability in frequency duration of flow pulses and variability in rate of change of flows were largely over predicted during the validation period with median relative errors as high as 120 therefore our results suggest that water resources managers must be particularly cautious when defining streamflow regime alteration limits based on simulated low flow timing rate of change and extreme events duration and frequency given the observed bias in both associated central tendency and variability indices during model validation 4 conclusions implementing the performance based calibration strategy confirmed that various performance metrics and transformations are better suited for particular streamflow regime facets also it was revealed that r 2 and relative transformed metrics behaved drastically different comparing to kge and sum of square errors based metrics when replicating hydrologic indices moreover results showed that a balanced representation of the streamflow regime is not directly related to the improvement of a particular performance metric instead it responded to tradeoffs among different performance based objective functions stressing different regime facets i e magnitude duration frequency rate of change and timing and flow conditions i e high moderate and low flows the successful implementation of the signature based calibration strategy demonstrated that it is possible to obtain consistent hydrological responses by simultaneously targeting multiple streamflow regime facets more importantly this was achieved without using any performance based objective function however compared to the latter the signature based strategy resulted in higher variability in the near optimal pareto solutions many of them with simulated indices falling outside the acceptability threshold 30 relative error similarly this strategy resulted in a highly variable representation of water balance and fdc characteristics compared to the performance based strategy therefore performance based calibration is preferable it is worth noting that the variability in the near optimal pareto solutions obtained under the two calibration strategies was driven mostly by the representation of low flows as revealed by the highly variable inverse transformed kge values and low flow related fdc characteristics among these solutions the model calibration framework developed here can also be used as a diagnosis tool for instance results revealed limitations of the swat model structure when representing the vertical redistribution of soil moisture fall rate and timing of annual extremes likewise the representation of low flow magnitude and timing rate of change of flows especially rise and fall rates and duration and frequency of extreme flows was limited in terms of interannual variability these limitations impact the definition of limits to hydrologic alteration which are relevant when defining environmental flows and managing social hydrological systems thus water managers and modelers must account for limitations in hydrologic indices replication when defining or selecting streamflow regime targets as part of broader ecohydrological frameworks and applications in ungauged or poorly gauged watersheds in this study we focused on analyzing the objective space and output variables of interest analyzing the near optimal decision variables i e model parameters and intermediate variables representing other water cycle components e g evapotranspiration soil moisture groundwater was out of the scope of this study our framework detected modeling limitations when representing various streamflow regime facets which is useful to address structural inadequacies and improving the overall modeling process future research should involve redesigning hydrological models and tailoring modeling practices e g input data processing model parameters selection choosing calibration validation time periods lengths to better represent ecologically relevant characteristics of riverine ecosystems likewise we recommend future studies to analyze model parameter behavior and other water cycle components when using any of the proposed calibration methods in this regard the proposed performance based method is flexible enough to implement multi variable and multi site model calibration software and data availability source code swat model text files and observed streamflow time series for the computation of 178 hydrologic indices and swat multi objective calibration in python are available at https github com jshernandezs swat pytools pareto optimal solutions and all the objective functions model paramerers and simulations obtained for each calibration strategy and optimization algorithm generation are available at http www hydroshare org resource 99d0dd48a2334679a5278602237e4314 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by the usda national institute of food and agriculture hatch project 1019654 j sebastian hernandez suarez is grateful to fulbright and the colombian ministry of science technology and innovation for the financial support of his doctoral studies 
25764,it is currently unclear how changes to modeled soil water simulations influence key predictions in complex ecohydrologic models particularly beyond hydrology we assessed sensitivity to soil moisture in the soil and water assessment tool by artificially perturbing simulated soil moisture and analyzing targeted prediction change results demonstrated a strong process dependent sensitivity of model predictions to perturbations in soil moisture at the basin scale the absolute values of differences from a baseline simulation for surface runoff evapotranspiration and nitrate loss ranged from 42 to 107 29 76 and 19 369 respectively for soil moisture changes between 0 5 and 2 standard deviations landscape scale results illustrated the impact of modeled land management actions notably fertilizer applications on determining the magnitude of the change the findings highlight and motivate the need to improve soil moisture simulations suggesting possible accuracy improvement for key model predictions by more precisely simulating soil moisture graphical abstract image 1 keywords soil water content soil moisture swat model sensitivity ecohydrologic modeling 1 introduction ecohydrologic models are critical for scenario analyses used to inform land management and policy decisions zalewski 2011 as a result it is critical to understand how well models represent known and predicted processes for a range of intended applications uncertainty in these predictions is attributed to a multitude of sources generally some combination of measurement and systematic error parameter uncertainty and inherent randomness song et al 2015 uusitalo et al 2015 van griensven and meixner 2006 one approach to measure the dependence of model accuracy on process representation is sensitivity analysis an estimate of the predicted output change relative to a change in inputs or system variables pianosi et al 2016 sensitivity analysis may examine model input data formulation or parameters where results can be used to subsequently target those that are the most impactful for improvement most practical applications of sensitivity analyses assess parameter sensitivity for model calibration mccuen 1973 van griensven et al 2006 approaches range from simple one at a time parameter changes to complex variance based methods e g saltelli et al 2010 requiring thousands of model iterations the sensitivity of soil water dynamics or soil moisture in ecohydrologic modeling is central in accurate predictions critically linking hydrology to climate plant and biogeochemical processes vereecken et al 2008 soil moisture is known to regulate key nutrient transformation processes in non point source pollution liu et al 2014 for instance enabling favorable anaerobic conditions for nitrogen gas emission denitrification schaufler et al 2010 accordingly several conceptual and physically based modeling approaches have been developed for soil moisture brocca et al 2017 vereecken et al 2016 often as components of a larger model previous efforts to improve soil moisture simulation rely on 1 direct updates to soil moisture via parameter calibration li et al 2018 ridler et al 2012 or data assimilation techniques houser et al 1998 reichle 2008 or 2 reformulating the model structure itself doro et al 2018 liang et al 1996 it is typically uncertain however whether a model is amenable to translating soil moisture updates to other variables or predictions or if they are evenly distributed as few studies have evaluated the impacts of changes to soil water content on model performance butts et al 2004 especially beyond hydrology establishing model sensitivity to soil moisture first can provide both motivation and justification for targeted model updates to inform future research in improving soil water simulations the sensitivity of soil moisture was tested for an ecohydrologic model the soil and water assessment tool swat arnold et al 1998 swat is a daily watershed scale model used globally that includes hydrology plant growth nutrient cycling and land management components gassman et al 2007 to date swat soil moisture dynamics have received little attention previous explorations of swat soil moisture simulations are narrow in scope only extending to hydrologic impacts havrylenko et al 2016 li et al 2010 mapfumo et al 2004 narasimhan et al 2005 uniyal et al 2017 there is therefore a need to focus on soil moisture sensitivity for a range of swat predictions koo et al 2020 previously identified structural uncertainty impacts on swat water quality predictions van griensven and meixner 2006 and known weaknesses in conceptual representation of nutrient processes pohlert et al 2005 highlights this need further while sensitivity analysis is commonly used with swat to identify parameters for model calibration e g cibin et al 2010 haas et al 2015 nossent and bauwens 2012 white and chaubey 2005 these approaches are not regularly adapted or applied to test model structural sensitivity the objective of this study was therefore to evaluate the sensitivity of targeted model predictions to soil moisture surface soil moisture simulations were perturbed for swat models at two geographically and climatically distinct us experimental watersheds 1 the cedar creek watershed in north eastern indiana and 2 the little river watershed in south western georgia baseline simulations were evaluated to ensure soil moisture values were in general agreement with observations the models were perturbed based on standard deviations of the baseline soil water content distribution soil moisture was both increased and decreased in separate perturbations effects of changes to soil water content were evaluated at the basin and landscape scales for three representative output processes related to hydrology plant growth and nutrient loss percent change from the baseline simulation for each target output was used to assess the magnitude of soil moisture sensitivity 2 methods 2 1 swat model description swat consists of hydrology weather crop growth soil nutrient pesticide and agricultural management components for the purpose of predicting the impact of various land management scenarios on crop production hydrology and water quality arnold et al 1998 basic modeling units are unique combinations of input land use soils and slopes known as hydrologic response units hrus simulations include a land phase where the hrus are used and a routing phase where landscape simulations are aggregated and transported in the catchment via a stream network soil water content is central to the model dynamics based upon the water balance 1 s w t s w i 1 t r i q i e t i p i q r i where sw t is the soil water content at time step day t r is precipitation q is surface runoff et is evapotranspiration p is percolation and qr is return flow all in units of mm each landscape unit consists of a possible snow layer soil profile that is divided into multiple layers according to input soil data 0 2 m and a shallow 2 20 m and deep 20 m aquifer the primary mechanisms of soil water movement in swat rely upon saturation conditions that is threshold based redistribution only saturated soil water flow is directly simulated in the model occurring when the water content of a soil layer exceeds its field capacity under this condition excess soil water greater than field capacity is available for percolation lateral flow or tile flow provided soil temperatures are above freezing 0 c unsaturated flow is indirectly accounted for by evapotranspiration demand from soil and plants evapotranspiration processes are controlled by two parameters epco and esco the plant uptake and soil evaporative compensation factors respectively together these parameters control the soil water extraction depth to meet soil evaporation and plant transpiration demands nietsch et al 2001 2 2 study sites two long term u s department of agriculture agricultural research service usda ars experimental watersheds were selected for this study the cedar creek watershed cc in north eastern indiana and the little river watershed lr in south western georgia fig 1 as usda ars experimental watersheds both are well instrumented including an array of in situ soil moisture sensors at multiple depths the cedar creek watershed 707 km2 is largely agricultural 80 dominated by rotational corn and soybean crops with poorly drained silt loam silty clay loam and clay loam soil types coupled with relatively flat topography 2 10 slopes the slow draining soils necessitate the prevalence of artificial tile drains throughout the watershed climate for cedar creek is classified as humid continental with an average temperature range of 1 to 28 c and mean annual precipitation of 940 mm han et al 2012a heathman et al 2012 larose et al 2007 there are currently 15 soil moisture monitoring sites with data collected every 10 or 15 min at depths of 50 200 400 and 600 mm made available for this study from 2005 to 2015 little river watershed 334 km2 is characterized by mixed land use with agriculture commonly cotton and peanuts in upland areas and forests and wetlands near the extensive riparian network soils are sands and sandy loams with high infiltration over gentle flat topography climate in little river is humid subtropical receiving 1200 mm of precipitation per year on average and temperatures ranging from 4 to 33 c soil moisture observations were available from 2003 to 2015 for 29 sites within the watershed at depths of 50 200 and 300 mm collected every 30 min bosch et al 2007a 2007b sullivan et al 2007 the u s geological survey usgs collects streamflow measurements at the outlets of both watersheds at station numbers usgs 04180000 and 02317797 for cedar creek and little river respectively 2 3 model set up and calibration swat models were delineated using the usgs streamflow monitoring sites as watershed outlets with 30 m digital elevation models usgs national elevation dataset gesch et al 2002 streams were defined using a threshold of 1 of expected total watershed area 700 ha cc 33 ha lr and burned in national hydrography dataset streamlines simley and carswell 2009 subbasins in little river were delineated to match experimental boundaries for compatibility with previous modeling research arnold et al 2010 bosch et al 2006 2010 for both sites hrus were defined with 10 ha area thresholds using soils data from the soil survey geographic database ssurgo nrcs 2005 land use from the 2014 national agricultural statistics service cropland data layer boryan et al 2011 and three slope classifications 2 2 to 5 and 5 while the threshold drops some combinations of soil type land use and slope in total cedar creek and little river had 57 and 8 subbassins with 2369 and 658 hrus respectively management was specified for the dominant crops at each watershed corn soybean for cc cotton peanut for lr including tile drains at cedar creek based on previous research bieger 2016 cibin et al 2012 her et al 2016 specific management operations and parameters are provided in the appendix tables a 1 a 2 a 3 model calibration was performed for streamflow at the watershed outlets using 11 sensitive parameters identified in previous swat modeling experiments for cedar creek her and chaubey 2015 larose et al 2007 rajib et al 2016 and little river cho et al 2010 rathjens et al 2014 van liew et al 2007 the genetically adaptive multi objective algorithm amalgam vrugt and robinson 2007 was used for 2880 simulations over 60 generations to maximize the daily kling gupta efficiency kge gupta et al 2009 kge combines measures of correlation mean bias and variance as values ranging from to 1 where larger positive values indicate good model performance for both watersheds models were run from 2003 to 2015 to maximize the temporal coverage of the in situ soil moisture observations the first four model simulation years were used as warm up and calibration and validation were from 2007 to 2011 and 2012 to 2015 respectively calibrated parameters and optimized values are shown in table 1 including kge calibration and validation statistics 2 4 estimating baseline soil moisture performance baseline soil moisture simulation accuracy was assessed at each watershed by comparing swat predicted soil water content to the in situ data the comparison ignored the 10 mm default soil layer created by swat as its purpose is primarily nutrient interaction with surface runoff and is not intended to accurately reflect soil moisture dynamics neitsch et al 2011 instead comparisons focused on the first upper swat soil layer as defined by the soils input data this layer was studied because it is important for rainfall infiltration runoff response robinson et al 2008 and secondarily because it is typically targeted for model calibration e g kundu et al 2017 or assimilation e g han et al 2012b using satellite soil moisture observations depth of 5 cm the average depths of these surface layers based on the ssurgo soils data were 225 mm and 424 mm for cedar creek and little river respectively in situ soil moisture was spatially averaged across the watersheds from all in situ sensors within these depths this included the upper two sensors at cedar creek and upper three sensors at little river watershed network averages were then compared to the swat surface layer soil moisture predictions weighted by hru area variances of simulated soil moisture distributions were further used to define perturbation values for sensitivity testing 2 5 testing output variable sensitivity to soil water content a simple perturbation approach was used to capture model response to change in soil water content for representative hydrologic nutrient and plant predictions daily simulated surface soil water content ignoring the 10 mm layer was directly perturbed by a range of values and the difference from the baseline model scenario was measured swat model source code version 664 was modified as shown in fig 2 to accommodate perturbations a new parameter sa val artificially increased or decreased soil water content for each hru this parameter specified a fractional value by which the surface soil moisture was multiplied at each time step perturbations to soil water content were specified such that they occurred prior to soil water redistribution and by effect routing and other landscape processes while this approach modifies the water balance it is necessary to determine soil moisture sensitivity additionally the perturbations used are analogous to change induced by direct soil moisture calibration or assimilation updates of which this study attempts to motivate perturbation values were derived from the standard deviations of baseline simulated surface soil moisture from the calibrated swat models for each watershed standard deviations of 0 5 1 and 2 were chosen for a total of seven perturbation model runs per watershed including the baseline according to chebyshev s inequality without any assumptions on statistical distribution it is expected that 75 of predicted soil water content values are within two standard deviations of the mean saw et al 1984 differences from the baseline run were calculated for three representative processes at the landscape hru and watershed basin scales average annual surface runoff plant yield and nitrate were evaluated at the landscape scale while evapotranspiration replaced plant yield at the watershed scale 3 results and discussion 3 1 baseline model performance calibration kge values for streamflow at the watershed outlets table 1 indicated acceptable model performance 0 84 cc 0 73 lr average observed and simulated flow values and standard deviations showed good agreement at both cedar creek 8 05 14 19 m3 s observed 7 75 14 29 m3 s simulated and little river 3 44 10 51 m3 s observed 3 55 10 19 m3 s simulated while soil water content was not calibrated directly the aggregated mean and standard deviations of the observed and simulated soil water contents for each watershed are shown in table 2 additionally the root mean square error rmse correlation coefficient r and percent bias pbias of the soil moisture simulations were calculated relative to the observations it is important to note that the values are site specific and averaged integrating heterogenous watersheds horizontally vertically and temporally nevertheless these estimates provide valuable information on basic model performance in predicting soil moisture overall the soil moisture predictions performed satisfactorily at both sites similar to other swat research li et al 2010 mapfumo et al 2004 uniyal et al 2017 soil water content values at cedar creek compared well based on the statistical distributions rmse and pbias but exhibited a lower than expected correlation r value 0 51 time series of daily observed and simulated soil water content fig 3 at cedar creek showed the model overpredicted soil water content in the winter months swat prevents soil water movement under frozen soil conditions soil temperature 0 c restricting drainage and thereby decreasing correlation with observed soil moisture which showed continued fluctuation at little river soil temperatures remained above freezing leading to a higher correlation 0 74 however average predicted soil water content was lower than observations pbias 19 3 previous research suggest that lower magnitude soil moisture predictions at little river are due to a mischaracterized upper limit field capacity derived from the ssurgo soils database pignotti et al 2021 although the model performance was satisfactory for this study rmse values 0 06 m3 m3 cc 0 03 m3 m3 lr generally suggest that future efforts to improve soil moisture simulations are warranted 3 2 model sensitivity to soil moisture perturbations 3 2 1 basin scale sensitivity swat surface soil moisture was perturbed by 0 5 1 and 2 standard deviations daily for each hru based on the simulated standard deviations at each watershed 0 067 cm3 cm3 cc 0 021 cm3 cm3 lr these soil water content changes at the basin scale demonstrated a strong effect on evaluated surface runoff evapotranspiration and nitrate export fig 4 examined and discussed individually in this section the percent differences from the baseline model simulation orig exhibited anisotropic behavior with respect to whether soil water content increased or decreased and were not uniform between output variables response behavior between the two test watersheds was however similar fig 4 for each target output prediction cedar creek had greater measured percent differences relative to baseline simulations similarly overall ranges for differences in model output units table 3 for surface runoff were higher at cedar creek 200 to 254 mm compared to little river 54 to 58 mm while evapotranspiration differences were greater at little river 694 to 396 mm than cedar creek 490 to 373 mm magnitude differences for target predictions directly related to watershed characteristics as surface runoff was a larger portion of the water balance at cedar creek than little river while evapotranspiration and subsurface flow were greater at little river nitrate loss varied greatly between the sites 4 8 to 25 kg ha cc 1 3 to 6 5 kg ha lr especially when soil water content was increased higher nitrate loss and variation at cedar creek was due to greater agricultural intensity that included more fertilization in land use management while these differences in watershed characteristics contributed to output prediction variance the complexity of process dependence upon soil moisture primarily governed specific process response surface runoff changed between an absolute range of 42 107 collectively between the two test sites for all perturbations to soil water content for the smallest perturbations of 0 5 standard deviations surface runoff exhibited between a 42 68 difference from the baseline directionally decreased soil water content reduced surface runoff up to 84 and 73 at cedar creek and little river respectively decreased soil water content necessitated additional abstraction from precipitation into the soil column based on the curve number methodology used in swat to generate infiltration excess runoff williams and laseur 1976 increased abstraction rates therefore reduced precipitation available for surface runoff conversely increased soil water content lowered abstraction to near zero effectively converting the majority of precipitation into surface runoff surface runoff accordingly increased to a maximum of 107 and 80 at cedar creek and little river respectively these results demonstrated the strong control of antecedent soil water content on the predicted amount of surface runoff other research has shown that updating swat soil moisture improves streamflow predictions kundu et al 2017 further highlighting the importance of accurate simulation compared to surface runoff evapotranspiration had lower sensitivity to soil water content evapotranspiration changed between an absolute range of 29 76 for all soil water content perturbations tested and 29 50 for the smallest change of 0 5 standard deviations as opposed to event based surface runoff evapotranspiration is demand based therefore when evapotranspiration demand is met by sufficient soil water extraction it reaches an upper limit potential evapotranspiration where it no longer increases see fig 4 this was demonstrated when soil water was increased for the simulations where evapotranspiration increased initially and then was nearly unchanged as soil moisture increased when soil water was decreased evapotranspiration approached 75 change for both watersheds this lower limit was maintained by soil esco and plant epco extraction depth swat parameters these parameters control the depth at which soil moisture is removed from the soil profile to meet evapotranspiration demand calibrated values table 1 indicated that deep plant extraction was parameterized at both watersheds allowing minimum evapotranspiration to occur even as soil moisture continually decreased in the upper layer nitrate loss exhibited the most complex relationship to soil water content affected by both flow related transport and saturation based transformation processes zhu et al 2018 perturbed soil moisture created a nitrate loss absolute difference of between 19 and 369 overall and 44 175 for 0 5 standard deviations nitrate loss showed the highest sensitivity among all directional soil moisture changes when soil moisture was increased up to 369 and 314 increases at cedar creek and little river respectively this sensitivity of water quality predictions to swat flow processes has been highlighted in previous research koo et al 2020 nossent and bauwens 2012 for example ouyang et al 2017 found that under similar precipitation total swat predicted nitrogen loss increased between 1 7 and 1 9 times for higher soil water content in this study increased soil moisture more readily transported nitrate nitrate loss was not conditioned upon precipitation occurrence as additional water was artificially introduced into the soil via the perturbation this meant that the upper limit to nitrate loss depended upon the available nitrate pool frequently replenished by fertilizer applications in the model decreased soil moisture led to a maximum decrease of nitrate loss of 69 and 63 for cedar creek and little river respectively for the largest soil moisture decrease 2 standard deviations nitrate loss surprisingly increased by 19 at cedar creek a similar pattern was detected at little river where the largest soil water decrease began to display an increasing nitrate loss trajectory although it remained a net decrease less initial transport of nitrate out of the soil from less available soil water immediate decreased nitrate loss followed by increased water stress and decreased plant nutrient uptake increased nitrate available for loss likely explains this trend because model applied fertilizers continue to provide a nitrogen pool competing water transport and plant demand processes then dictate the potential movement of this pool transformation of nitrogen between various compound pools also presumably plays a role but is more complex as changes to soil moisture affect redox conditions skopp et al 1990 with the potential to alter nitrogen fluxes castellano et al 2010 zhu et al 2013 3 2 2 landscape scale sensitivity the impacts of soil water perturbations by land use for surface runoff plant yield and nitrate loss are shown in figs 5 and 6 values are plotted to show differences in target predictions where the original orig baseline simulation is displayed as a black bar the figures generally indicate that differences from perturbed soil water content follow patterns observed at the basin scale predictions demonstrated strong sensitivity to soil moisture change where from the baseline to 0 5 standard deviations had the largest step change while surface runoff was relevant for all land use types yield and nitrate loss largely pertained to agricultural and urban land use table 4 shows the percent change of each target output prediction relative to the baseline simulation an important distinction was made between whether agriculture was managed by user specified practices user managed agriculture or automatically by swat swat managed agriculture the significance of this designation is the difference in how management actions were applied in the model e g fertilizer application rates and timing the dominant two crops for each watershed corn and soybeans cc cotton and peanuts lr were managed according to user specifications where actions were approximated by expert knowledge appendix tables a1 and a 2 all other crops were automatically managed by swat user managed agriculture followed a strict timing and rate for fertilizer applications from fixed estimated values while swat managed agriculture was flexible and changed according to crop conditions land use categories were less relevant for surface runoff where it increased with increasing soil moisture and vice versa a similar range of surface runoff percent differences was found for both watersheds 95 131 cc 91 138 lr however the difference in surface runoff change between watersheds appears greater in figs 5 and 6 because the baseline magnitude of surface runoff at cedar creek is relatively greater 263 to 312 mm cc vs 94 to 86 mm lr this difference in magnitude is likely due to fine textured slower draining soils leading to relatively higher surface runoff at cedar creek surface runoff predictions for urban and barren land use were the least sensitive to soil water changes their surface runoff was initially relatively high due to larger curve number values and therefore was less dependent on the amount of soil moisture needed to generate runoff yield values apart from agriculture included urban lawn grass clippings and pasture where for little river only the pasture classification included hay yield predictions decreased at higher levels of soil water content perturbations regardless of the directionality of the change this was an important difference from basin scale evapotranspiration response which did not decrease when soil water increased this suggested that at higher soil moisture values other factors besides water demand limited growth optimal plant growth requires adequate water nutrients and aeration when soil moisture decreases water stress and possibly nutrient stress increase conversely when soil moisture increases nutrient stress and aeration stress may increase due to increased transport and lack of soil oxygen respectively average annual plant stress days evaluated for both watersheds confirmed these expected trends decreased soil moisture increased the number of water stress days with a corresponding decrease in yields maximum of 97 cc 92 lr when soil water was initially increased the number of water stress days was reduced and yield increased moderately 25 cc 13 lr this was followed by an increase in nutrient and aeration stress days as soil moisture continued to increase yield effects were more pronounced for user managed agriculture as fertilization rate and timing were fixed in the model the additional nutrient loss could therefore not be compensated potentially during critical growth periods thus user managed agriculture displayed a greater decrease in yield than swat managed agriculture when soil water content increased nitrate loss had the greatest sensitivity of the three target output variables evaluated while site characteristics between watersheds such as climate and soil type led to a difference in the magnitude of change for target predictions patterns by land use type were generally similar not surprisingly the amount of fertilizer applied to each land use type largely determined the magnitude of nitrate loss for example increased soil moisture at little river demonstrated higher nitrate loss for crops to which more fertilizer corn was applied than those that required less peanuts previous swat research has confirmed these findings showing nitrogen loss sensitivity to fertilizer application rates arabi et al 2007 in this study increased nitrogen loss is attributed to increased water flow haas et al 2015 as a result of soil moisture increase the continuous application of fertilizer for swat managed agriculture under increased soil moisture conditions in the simulations led to greater nitrate loss 1221 cc 218 lr compared to the fixed application of user managed agriculture land use 140 cc 41 lr water not nutrients limited crop growth when soil water decreased thus agriculture automatically managed by swat did not apply additional fertilizers under these dry conditions user managed agriculture however continued to apply the fixed amount of fertilizer these fertilizer applications allowed large pools of nitrogen to accumulate in the soil with insufficient water for plant uptake under high stormflow events this nitrogen was readily available for transport therefore nitrate loss was higher for user managed agriculture 642 cc 141 lr when soil moisture decreased compared to swat managed agriculture 82 cc 13 lr these findings underscore the importance of understanding model management actions in any scenario analysis that updates the water balance with observations 4 conclusions in this study the impacts of changes to swat simulated soil water content were assessed to motivate future efforts to update and improve predictions daily swat simulated surface soil moisture was perturbed by 0 5 1 and 2 standard deviations for two distinct experimental watersheds baseline soil water simulations were evaluated to establish accuracy using in situ soil moisture observations these simulations were then compared to simulations with perturbed soil moisture for three target model predictions at both the basin surface runoff evapotranspiration nitrate loss and landscape scales surface runoff crop yield nitrate loss results showed satisfactory baseline soil moisture performance but with the potential for future improvement soil water perturbation suggested a strong impact on model output predictions that was process dependent at both scales tested at the basin scale surface runoff and evapotranspiration sensitivity related to changes in saturation conditions that did or did not more easily meet event or demand based requirements respectively nitrate loss response was the most sensitive and complex with loss over three times greater than the baseline simulation for maximum soil water perturbations of 2 standard deviations landscape scale sensitivity generally followed the patterns at the basin scale with the ability to distinguish effects by land use type prescribed management played a significant role in determining output sensitivity to soil water change notably the quantity and timing of fertilizer applications markedly affected nitrate loss greater nitrate losses were found for swat automatically managed land when soil water content was increased due to continual application of fertilizers to compensate for increasing nutrient loss user specified management conversely had greater losses when soil water was decreased this was due to constant fixed application likely during induced plant stressed limited uptake conditions these changes to nutrient availability for both cases in addition to water or aeration stress led to decreases in crop yield regardless of whether soil water increased or decreased the broader implications of these findings highlight the influential nature of soil moisture in swat simulations and indicate the need to accurately capture soil moisture dynamics this study determined there is both potential and justification in updating soil moisture simulations and motivates future research in model structural improvements and use of data assimilation updating techniques further research could help to fully explore the effects of changes to soil water content for both swat and other models especially when extended to several other diverse locations future efforts can better define the potential improvements for soil moisture simulations in modeling and present an opportunity to reduce uncertainty and increase confidence in model predictions on which management actions critically depend with expected continued pressures on natural and human modified systems modeling is a vital tool for testing and surveying optimal land management strategies that maximize benefits while minimizing degradation software and data availability modified swat source code version 664 and a compiled executable 13 2 mb used in this study are freely available at https github com gpignotti swat soil moisture sensitivity original swat source code and documentation are available from the model developers at https swat tamu edu all swat code is written in the fortran language and is open source input data used to develop the models used in this study are from publicly available sources listed in the methods funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank cibin raj younggu her and keith cherkauer for their helpful discussion and insight in developing this research additionally we acknowledge support from the usda ars for use of soil moisture data over both experimental watersheds appendix table a 1 agricultural practices used for cedar creek table a 1 management strategy date operation corn year 22 apr nitrogen application1 22 apr atrazine application2 6 may field cultivator till 6 may planting 6 jun nitrogen application3 14 oct harvest 15 oct plant kill soybean year 24 may zero till 24 may planting 7 oct harvest 8 oct plant kill 15 oct phosphorous application4 1 nov chisel plow till 1 anhydrous 53 kg ha n of 43 kg ha 2 atrazine 2 2 kg ha 3 urea 284 kg ha n of 131 kg ha 4 dap p2o5 123 kg ha p of 54 kg ha table a 2 tile drainage parameters for cedar creek table a 2 parameter definition value ddrain mm depth from surface to tile drains 1000 tdrain hr time to drain to field capacity 24 gdrain hr drainage lag time 48 dep imp mm depth from surface to impervious layer 1200 itdrn flag drainage routine 1 indicates new routine 1 re mm effective drain radius 20 sdrain mm distance between tile drains 20000 drain co mm day drainage coefficient 10 pc mm hr pump capacity 0 latksatf multiplication factor for lateral conductivity 1 2 table a 3 agricultural practices used for little river table a 3 management strategy date operation cotton year 15 apr chisel plow till 1 may field cultivator till 2 may planting 3 may phosphorous application1 3 may nitrogen application2 11 nov harvest peanut year 1 apr chisel plow till 15 apr chisel plow till 1 may field cultivator till 2 may planting 1 oct harvest cotton year 15 mar chisel plow till 1 apr chisel plow till 15 apr chisel plow till 1 may field cultivator till 2 may planting 3 may phosphorous application1 3 may nitrogen application2 11 nov harvest 1 elemental phosphorous 12 3 kg ha 2 elemental nitrogen 92 1 kg ha 
25764,it is currently unclear how changes to modeled soil water simulations influence key predictions in complex ecohydrologic models particularly beyond hydrology we assessed sensitivity to soil moisture in the soil and water assessment tool by artificially perturbing simulated soil moisture and analyzing targeted prediction change results demonstrated a strong process dependent sensitivity of model predictions to perturbations in soil moisture at the basin scale the absolute values of differences from a baseline simulation for surface runoff evapotranspiration and nitrate loss ranged from 42 to 107 29 76 and 19 369 respectively for soil moisture changes between 0 5 and 2 standard deviations landscape scale results illustrated the impact of modeled land management actions notably fertilizer applications on determining the magnitude of the change the findings highlight and motivate the need to improve soil moisture simulations suggesting possible accuracy improvement for key model predictions by more precisely simulating soil moisture graphical abstract image 1 keywords soil water content soil moisture swat model sensitivity ecohydrologic modeling 1 introduction ecohydrologic models are critical for scenario analyses used to inform land management and policy decisions zalewski 2011 as a result it is critical to understand how well models represent known and predicted processes for a range of intended applications uncertainty in these predictions is attributed to a multitude of sources generally some combination of measurement and systematic error parameter uncertainty and inherent randomness song et al 2015 uusitalo et al 2015 van griensven and meixner 2006 one approach to measure the dependence of model accuracy on process representation is sensitivity analysis an estimate of the predicted output change relative to a change in inputs or system variables pianosi et al 2016 sensitivity analysis may examine model input data formulation or parameters where results can be used to subsequently target those that are the most impactful for improvement most practical applications of sensitivity analyses assess parameter sensitivity for model calibration mccuen 1973 van griensven et al 2006 approaches range from simple one at a time parameter changes to complex variance based methods e g saltelli et al 2010 requiring thousands of model iterations the sensitivity of soil water dynamics or soil moisture in ecohydrologic modeling is central in accurate predictions critically linking hydrology to climate plant and biogeochemical processes vereecken et al 2008 soil moisture is known to regulate key nutrient transformation processes in non point source pollution liu et al 2014 for instance enabling favorable anaerobic conditions for nitrogen gas emission denitrification schaufler et al 2010 accordingly several conceptual and physically based modeling approaches have been developed for soil moisture brocca et al 2017 vereecken et al 2016 often as components of a larger model previous efforts to improve soil moisture simulation rely on 1 direct updates to soil moisture via parameter calibration li et al 2018 ridler et al 2012 or data assimilation techniques houser et al 1998 reichle 2008 or 2 reformulating the model structure itself doro et al 2018 liang et al 1996 it is typically uncertain however whether a model is amenable to translating soil moisture updates to other variables or predictions or if they are evenly distributed as few studies have evaluated the impacts of changes to soil water content on model performance butts et al 2004 especially beyond hydrology establishing model sensitivity to soil moisture first can provide both motivation and justification for targeted model updates to inform future research in improving soil water simulations the sensitivity of soil moisture was tested for an ecohydrologic model the soil and water assessment tool swat arnold et al 1998 swat is a daily watershed scale model used globally that includes hydrology plant growth nutrient cycling and land management components gassman et al 2007 to date swat soil moisture dynamics have received little attention previous explorations of swat soil moisture simulations are narrow in scope only extending to hydrologic impacts havrylenko et al 2016 li et al 2010 mapfumo et al 2004 narasimhan et al 2005 uniyal et al 2017 there is therefore a need to focus on soil moisture sensitivity for a range of swat predictions koo et al 2020 previously identified structural uncertainty impacts on swat water quality predictions van griensven and meixner 2006 and known weaknesses in conceptual representation of nutrient processes pohlert et al 2005 highlights this need further while sensitivity analysis is commonly used with swat to identify parameters for model calibration e g cibin et al 2010 haas et al 2015 nossent and bauwens 2012 white and chaubey 2005 these approaches are not regularly adapted or applied to test model structural sensitivity the objective of this study was therefore to evaluate the sensitivity of targeted model predictions to soil moisture surface soil moisture simulations were perturbed for swat models at two geographically and climatically distinct us experimental watersheds 1 the cedar creek watershed in north eastern indiana and 2 the little river watershed in south western georgia baseline simulations were evaluated to ensure soil moisture values were in general agreement with observations the models were perturbed based on standard deviations of the baseline soil water content distribution soil moisture was both increased and decreased in separate perturbations effects of changes to soil water content were evaluated at the basin and landscape scales for three representative output processes related to hydrology plant growth and nutrient loss percent change from the baseline simulation for each target output was used to assess the magnitude of soil moisture sensitivity 2 methods 2 1 swat model description swat consists of hydrology weather crop growth soil nutrient pesticide and agricultural management components for the purpose of predicting the impact of various land management scenarios on crop production hydrology and water quality arnold et al 1998 basic modeling units are unique combinations of input land use soils and slopes known as hydrologic response units hrus simulations include a land phase where the hrus are used and a routing phase where landscape simulations are aggregated and transported in the catchment via a stream network soil water content is central to the model dynamics based upon the water balance 1 s w t s w i 1 t r i q i e t i p i q r i where sw t is the soil water content at time step day t r is precipitation q is surface runoff et is evapotranspiration p is percolation and qr is return flow all in units of mm each landscape unit consists of a possible snow layer soil profile that is divided into multiple layers according to input soil data 0 2 m and a shallow 2 20 m and deep 20 m aquifer the primary mechanisms of soil water movement in swat rely upon saturation conditions that is threshold based redistribution only saturated soil water flow is directly simulated in the model occurring when the water content of a soil layer exceeds its field capacity under this condition excess soil water greater than field capacity is available for percolation lateral flow or tile flow provided soil temperatures are above freezing 0 c unsaturated flow is indirectly accounted for by evapotranspiration demand from soil and plants evapotranspiration processes are controlled by two parameters epco and esco the plant uptake and soil evaporative compensation factors respectively together these parameters control the soil water extraction depth to meet soil evaporation and plant transpiration demands nietsch et al 2001 2 2 study sites two long term u s department of agriculture agricultural research service usda ars experimental watersheds were selected for this study the cedar creek watershed cc in north eastern indiana and the little river watershed lr in south western georgia fig 1 as usda ars experimental watersheds both are well instrumented including an array of in situ soil moisture sensors at multiple depths the cedar creek watershed 707 km2 is largely agricultural 80 dominated by rotational corn and soybean crops with poorly drained silt loam silty clay loam and clay loam soil types coupled with relatively flat topography 2 10 slopes the slow draining soils necessitate the prevalence of artificial tile drains throughout the watershed climate for cedar creek is classified as humid continental with an average temperature range of 1 to 28 c and mean annual precipitation of 940 mm han et al 2012a heathman et al 2012 larose et al 2007 there are currently 15 soil moisture monitoring sites with data collected every 10 or 15 min at depths of 50 200 400 and 600 mm made available for this study from 2005 to 2015 little river watershed 334 km2 is characterized by mixed land use with agriculture commonly cotton and peanuts in upland areas and forests and wetlands near the extensive riparian network soils are sands and sandy loams with high infiltration over gentle flat topography climate in little river is humid subtropical receiving 1200 mm of precipitation per year on average and temperatures ranging from 4 to 33 c soil moisture observations were available from 2003 to 2015 for 29 sites within the watershed at depths of 50 200 and 300 mm collected every 30 min bosch et al 2007a 2007b sullivan et al 2007 the u s geological survey usgs collects streamflow measurements at the outlets of both watersheds at station numbers usgs 04180000 and 02317797 for cedar creek and little river respectively 2 3 model set up and calibration swat models were delineated using the usgs streamflow monitoring sites as watershed outlets with 30 m digital elevation models usgs national elevation dataset gesch et al 2002 streams were defined using a threshold of 1 of expected total watershed area 700 ha cc 33 ha lr and burned in national hydrography dataset streamlines simley and carswell 2009 subbasins in little river were delineated to match experimental boundaries for compatibility with previous modeling research arnold et al 2010 bosch et al 2006 2010 for both sites hrus were defined with 10 ha area thresholds using soils data from the soil survey geographic database ssurgo nrcs 2005 land use from the 2014 national agricultural statistics service cropland data layer boryan et al 2011 and three slope classifications 2 2 to 5 and 5 while the threshold drops some combinations of soil type land use and slope in total cedar creek and little river had 57 and 8 subbassins with 2369 and 658 hrus respectively management was specified for the dominant crops at each watershed corn soybean for cc cotton peanut for lr including tile drains at cedar creek based on previous research bieger 2016 cibin et al 2012 her et al 2016 specific management operations and parameters are provided in the appendix tables a 1 a 2 a 3 model calibration was performed for streamflow at the watershed outlets using 11 sensitive parameters identified in previous swat modeling experiments for cedar creek her and chaubey 2015 larose et al 2007 rajib et al 2016 and little river cho et al 2010 rathjens et al 2014 van liew et al 2007 the genetically adaptive multi objective algorithm amalgam vrugt and robinson 2007 was used for 2880 simulations over 60 generations to maximize the daily kling gupta efficiency kge gupta et al 2009 kge combines measures of correlation mean bias and variance as values ranging from to 1 where larger positive values indicate good model performance for both watersheds models were run from 2003 to 2015 to maximize the temporal coverage of the in situ soil moisture observations the first four model simulation years were used as warm up and calibration and validation were from 2007 to 2011 and 2012 to 2015 respectively calibrated parameters and optimized values are shown in table 1 including kge calibration and validation statistics 2 4 estimating baseline soil moisture performance baseline soil moisture simulation accuracy was assessed at each watershed by comparing swat predicted soil water content to the in situ data the comparison ignored the 10 mm default soil layer created by swat as its purpose is primarily nutrient interaction with surface runoff and is not intended to accurately reflect soil moisture dynamics neitsch et al 2011 instead comparisons focused on the first upper swat soil layer as defined by the soils input data this layer was studied because it is important for rainfall infiltration runoff response robinson et al 2008 and secondarily because it is typically targeted for model calibration e g kundu et al 2017 or assimilation e g han et al 2012b using satellite soil moisture observations depth of 5 cm the average depths of these surface layers based on the ssurgo soils data were 225 mm and 424 mm for cedar creek and little river respectively in situ soil moisture was spatially averaged across the watersheds from all in situ sensors within these depths this included the upper two sensors at cedar creek and upper three sensors at little river watershed network averages were then compared to the swat surface layer soil moisture predictions weighted by hru area variances of simulated soil moisture distributions were further used to define perturbation values for sensitivity testing 2 5 testing output variable sensitivity to soil water content a simple perturbation approach was used to capture model response to change in soil water content for representative hydrologic nutrient and plant predictions daily simulated surface soil water content ignoring the 10 mm layer was directly perturbed by a range of values and the difference from the baseline model scenario was measured swat model source code version 664 was modified as shown in fig 2 to accommodate perturbations a new parameter sa val artificially increased or decreased soil water content for each hru this parameter specified a fractional value by which the surface soil moisture was multiplied at each time step perturbations to soil water content were specified such that they occurred prior to soil water redistribution and by effect routing and other landscape processes while this approach modifies the water balance it is necessary to determine soil moisture sensitivity additionally the perturbations used are analogous to change induced by direct soil moisture calibration or assimilation updates of which this study attempts to motivate perturbation values were derived from the standard deviations of baseline simulated surface soil moisture from the calibrated swat models for each watershed standard deviations of 0 5 1 and 2 were chosen for a total of seven perturbation model runs per watershed including the baseline according to chebyshev s inequality without any assumptions on statistical distribution it is expected that 75 of predicted soil water content values are within two standard deviations of the mean saw et al 1984 differences from the baseline run were calculated for three representative processes at the landscape hru and watershed basin scales average annual surface runoff plant yield and nitrate were evaluated at the landscape scale while evapotranspiration replaced plant yield at the watershed scale 3 results and discussion 3 1 baseline model performance calibration kge values for streamflow at the watershed outlets table 1 indicated acceptable model performance 0 84 cc 0 73 lr average observed and simulated flow values and standard deviations showed good agreement at both cedar creek 8 05 14 19 m3 s observed 7 75 14 29 m3 s simulated and little river 3 44 10 51 m3 s observed 3 55 10 19 m3 s simulated while soil water content was not calibrated directly the aggregated mean and standard deviations of the observed and simulated soil water contents for each watershed are shown in table 2 additionally the root mean square error rmse correlation coefficient r and percent bias pbias of the soil moisture simulations were calculated relative to the observations it is important to note that the values are site specific and averaged integrating heterogenous watersheds horizontally vertically and temporally nevertheless these estimates provide valuable information on basic model performance in predicting soil moisture overall the soil moisture predictions performed satisfactorily at both sites similar to other swat research li et al 2010 mapfumo et al 2004 uniyal et al 2017 soil water content values at cedar creek compared well based on the statistical distributions rmse and pbias but exhibited a lower than expected correlation r value 0 51 time series of daily observed and simulated soil water content fig 3 at cedar creek showed the model overpredicted soil water content in the winter months swat prevents soil water movement under frozen soil conditions soil temperature 0 c restricting drainage and thereby decreasing correlation with observed soil moisture which showed continued fluctuation at little river soil temperatures remained above freezing leading to a higher correlation 0 74 however average predicted soil water content was lower than observations pbias 19 3 previous research suggest that lower magnitude soil moisture predictions at little river are due to a mischaracterized upper limit field capacity derived from the ssurgo soils database pignotti et al 2021 although the model performance was satisfactory for this study rmse values 0 06 m3 m3 cc 0 03 m3 m3 lr generally suggest that future efforts to improve soil moisture simulations are warranted 3 2 model sensitivity to soil moisture perturbations 3 2 1 basin scale sensitivity swat surface soil moisture was perturbed by 0 5 1 and 2 standard deviations daily for each hru based on the simulated standard deviations at each watershed 0 067 cm3 cm3 cc 0 021 cm3 cm3 lr these soil water content changes at the basin scale demonstrated a strong effect on evaluated surface runoff evapotranspiration and nitrate export fig 4 examined and discussed individually in this section the percent differences from the baseline model simulation orig exhibited anisotropic behavior with respect to whether soil water content increased or decreased and were not uniform between output variables response behavior between the two test watersheds was however similar fig 4 for each target output prediction cedar creek had greater measured percent differences relative to baseline simulations similarly overall ranges for differences in model output units table 3 for surface runoff were higher at cedar creek 200 to 254 mm compared to little river 54 to 58 mm while evapotranspiration differences were greater at little river 694 to 396 mm than cedar creek 490 to 373 mm magnitude differences for target predictions directly related to watershed characteristics as surface runoff was a larger portion of the water balance at cedar creek than little river while evapotranspiration and subsurface flow were greater at little river nitrate loss varied greatly between the sites 4 8 to 25 kg ha cc 1 3 to 6 5 kg ha lr especially when soil water content was increased higher nitrate loss and variation at cedar creek was due to greater agricultural intensity that included more fertilization in land use management while these differences in watershed characteristics contributed to output prediction variance the complexity of process dependence upon soil moisture primarily governed specific process response surface runoff changed between an absolute range of 42 107 collectively between the two test sites for all perturbations to soil water content for the smallest perturbations of 0 5 standard deviations surface runoff exhibited between a 42 68 difference from the baseline directionally decreased soil water content reduced surface runoff up to 84 and 73 at cedar creek and little river respectively decreased soil water content necessitated additional abstraction from precipitation into the soil column based on the curve number methodology used in swat to generate infiltration excess runoff williams and laseur 1976 increased abstraction rates therefore reduced precipitation available for surface runoff conversely increased soil water content lowered abstraction to near zero effectively converting the majority of precipitation into surface runoff surface runoff accordingly increased to a maximum of 107 and 80 at cedar creek and little river respectively these results demonstrated the strong control of antecedent soil water content on the predicted amount of surface runoff other research has shown that updating swat soil moisture improves streamflow predictions kundu et al 2017 further highlighting the importance of accurate simulation compared to surface runoff evapotranspiration had lower sensitivity to soil water content evapotranspiration changed between an absolute range of 29 76 for all soil water content perturbations tested and 29 50 for the smallest change of 0 5 standard deviations as opposed to event based surface runoff evapotranspiration is demand based therefore when evapotranspiration demand is met by sufficient soil water extraction it reaches an upper limit potential evapotranspiration where it no longer increases see fig 4 this was demonstrated when soil water was increased for the simulations where evapotranspiration increased initially and then was nearly unchanged as soil moisture increased when soil water was decreased evapotranspiration approached 75 change for both watersheds this lower limit was maintained by soil esco and plant epco extraction depth swat parameters these parameters control the depth at which soil moisture is removed from the soil profile to meet evapotranspiration demand calibrated values table 1 indicated that deep plant extraction was parameterized at both watersheds allowing minimum evapotranspiration to occur even as soil moisture continually decreased in the upper layer nitrate loss exhibited the most complex relationship to soil water content affected by both flow related transport and saturation based transformation processes zhu et al 2018 perturbed soil moisture created a nitrate loss absolute difference of between 19 and 369 overall and 44 175 for 0 5 standard deviations nitrate loss showed the highest sensitivity among all directional soil moisture changes when soil moisture was increased up to 369 and 314 increases at cedar creek and little river respectively this sensitivity of water quality predictions to swat flow processes has been highlighted in previous research koo et al 2020 nossent and bauwens 2012 for example ouyang et al 2017 found that under similar precipitation total swat predicted nitrogen loss increased between 1 7 and 1 9 times for higher soil water content in this study increased soil moisture more readily transported nitrate nitrate loss was not conditioned upon precipitation occurrence as additional water was artificially introduced into the soil via the perturbation this meant that the upper limit to nitrate loss depended upon the available nitrate pool frequently replenished by fertilizer applications in the model decreased soil moisture led to a maximum decrease of nitrate loss of 69 and 63 for cedar creek and little river respectively for the largest soil moisture decrease 2 standard deviations nitrate loss surprisingly increased by 19 at cedar creek a similar pattern was detected at little river where the largest soil water decrease began to display an increasing nitrate loss trajectory although it remained a net decrease less initial transport of nitrate out of the soil from less available soil water immediate decreased nitrate loss followed by increased water stress and decreased plant nutrient uptake increased nitrate available for loss likely explains this trend because model applied fertilizers continue to provide a nitrogen pool competing water transport and plant demand processes then dictate the potential movement of this pool transformation of nitrogen between various compound pools also presumably plays a role but is more complex as changes to soil moisture affect redox conditions skopp et al 1990 with the potential to alter nitrogen fluxes castellano et al 2010 zhu et al 2013 3 2 2 landscape scale sensitivity the impacts of soil water perturbations by land use for surface runoff plant yield and nitrate loss are shown in figs 5 and 6 values are plotted to show differences in target predictions where the original orig baseline simulation is displayed as a black bar the figures generally indicate that differences from perturbed soil water content follow patterns observed at the basin scale predictions demonstrated strong sensitivity to soil moisture change where from the baseline to 0 5 standard deviations had the largest step change while surface runoff was relevant for all land use types yield and nitrate loss largely pertained to agricultural and urban land use table 4 shows the percent change of each target output prediction relative to the baseline simulation an important distinction was made between whether agriculture was managed by user specified practices user managed agriculture or automatically by swat swat managed agriculture the significance of this designation is the difference in how management actions were applied in the model e g fertilizer application rates and timing the dominant two crops for each watershed corn and soybeans cc cotton and peanuts lr were managed according to user specifications where actions were approximated by expert knowledge appendix tables a1 and a 2 all other crops were automatically managed by swat user managed agriculture followed a strict timing and rate for fertilizer applications from fixed estimated values while swat managed agriculture was flexible and changed according to crop conditions land use categories were less relevant for surface runoff where it increased with increasing soil moisture and vice versa a similar range of surface runoff percent differences was found for both watersheds 95 131 cc 91 138 lr however the difference in surface runoff change between watersheds appears greater in figs 5 and 6 because the baseline magnitude of surface runoff at cedar creek is relatively greater 263 to 312 mm cc vs 94 to 86 mm lr this difference in magnitude is likely due to fine textured slower draining soils leading to relatively higher surface runoff at cedar creek surface runoff predictions for urban and barren land use were the least sensitive to soil water changes their surface runoff was initially relatively high due to larger curve number values and therefore was less dependent on the amount of soil moisture needed to generate runoff yield values apart from agriculture included urban lawn grass clippings and pasture where for little river only the pasture classification included hay yield predictions decreased at higher levels of soil water content perturbations regardless of the directionality of the change this was an important difference from basin scale evapotranspiration response which did not decrease when soil water increased this suggested that at higher soil moisture values other factors besides water demand limited growth optimal plant growth requires adequate water nutrients and aeration when soil moisture decreases water stress and possibly nutrient stress increase conversely when soil moisture increases nutrient stress and aeration stress may increase due to increased transport and lack of soil oxygen respectively average annual plant stress days evaluated for both watersheds confirmed these expected trends decreased soil moisture increased the number of water stress days with a corresponding decrease in yields maximum of 97 cc 92 lr when soil water was initially increased the number of water stress days was reduced and yield increased moderately 25 cc 13 lr this was followed by an increase in nutrient and aeration stress days as soil moisture continued to increase yield effects were more pronounced for user managed agriculture as fertilization rate and timing were fixed in the model the additional nutrient loss could therefore not be compensated potentially during critical growth periods thus user managed agriculture displayed a greater decrease in yield than swat managed agriculture when soil water content increased nitrate loss had the greatest sensitivity of the three target output variables evaluated while site characteristics between watersheds such as climate and soil type led to a difference in the magnitude of change for target predictions patterns by land use type were generally similar not surprisingly the amount of fertilizer applied to each land use type largely determined the magnitude of nitrate loss for example increased soil moisture at little river demonstrated higher nitrate loss for crops to which more fertilizer corn was applied than those that required less peanuts previous swat research has confirmed these findings showing nitrogen loss sensitivity to fertilizer application rates arabi et al 2007 in this study increased nitrogen loss is attributed to increased water flow haas et al 2015 as a result of soil moisture increase the continuous application of fertilizer for swat managed agriculture under increased soil moisture conditions in the simulations led to greater nitrate loss 1221 cc 218 lr compared to the fixed application of user managed agriculture land use 140 cc 41 lr water not nutrients limited crop growth when soil water decreased thus agriculture automatically managed by swat did not apply additional fertilizers under these dry conditions user managed agriculture however continued to apply the fixed amount of fertilizer these fertilizer applications allowed large pools of nitrogen to accumulate in the soil with insufficient water for plant uptake under high stormflow events this nitrogen was readily available for transport therefore nitrate loss was higher for user managed agriculture 642 cc 141 lr when soil moisture decreased compared to swat managed agriculture 82 cc 13 lr these findings underscore the importance of understanding model management actions in any scenario analysis that updates the water balance with observations 4 conclusions in this study the impacts of changes to swat simulated soil water content were assessed to motivate future efforts to update and improve predictions daily swat simulated surface soil moisture was perturbed by 0 5 1 and 2 standard deviations for two distinct experimental watersheds baseline soil water simulations were evaluated to establish accuracy using in situ soil moisture observations these simulations were then compared to simulations with perturbed soil moisture for three target model predictions at both the basin surface runoff evapotranspiration nitrate loss and landscape scales surface runoff crop yield nitrate loss results showed satisfactory baseline soil moisture performance but with the potential for future improvement soil water perturbation suggested a strong impact on model output predictions that was process dependent at both scales tested at the basin scale surface runoff and evapotranspiration sensitivity related to changes in saturation conditions that did or did not more easily meet event or demand based requirements respectively nitrate loss response was the most sensitive and complex with loss over three times greater than the baseline simulation for maximum soil water perturbations of 2 standard deviations landscape scale sensitivity generally followed the patterns at the basin scale with the ability to distinguish effects by land use type prescribed management played a significant role in determining output sensitivity to soil water change notably the quantity and timing of fertilizer applications markedly affected nitrate loss greater nitrate losses were found for swat automatically managed land when soil water content was increased due to continual application of fertilizers to compensate for increasing nutrient loss user specified management conversely had greater losses when soil water was decreased this was due to constant fixed application likely during induced plant stressed limited uptake conditions these changes to nutrient availability for both cases in addition to water or aeration stress led to decreases in crop yield regardless of whether soil water increased or decreased the broader implications of these findings highlight the influential nature of soil moisture in swat simulations and indicate the need to accurately capture soil moisture dynamics this study determined there is both potential and justification in updating soil moisture simulations and motivates future research in model structural improvements and use of data assimilation updating techniques further research could help to fully explore the effects of changes to soil water content for both swat and other models especially when extended to several other diverse locations future efforts can better define the potential improvements for soil moisture simulations in modeling and present an opportunity to reduce uncertainty and increase confidence in model predictions on which management actions critically depend with expected continued pressures on natural and human modified systems modeling is a vital tool for testing and surveying optimal land management strategies that maximize benefits while minimizing degradation software and data availability modified swat source code version 664 and a compiled executable 13 2 mb used in this study are freely available at https github com gpignotti swat soil moisture sensitivity original swat source code and documentation are available from the model developers at https swat tamu edu all swat code is written in the fortran language and is open source input data used to develop the models used in this study are from publicly available sources listed in the methods funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank cibin raj younggu her and keith cherkauer for their helpful discussion and insight in developing this research additionally we acknowledge support from the usda ars for use of soil moisture data over both experimental watersheds appendix table a 1 agricultural practices used for cedar creek table a 1 management strategy date operation corn year 22 apr nitrogen application1 22 apr atrazine application2 6 may field cultivator till 6 may planting 6 jun nitrogen application3 14 oct harvest 15 oct plant kill soybean year 24 may zero till 24 may planting 7 oct harvest 8 oct plant kill 15 oct phosphorous application4 1 nov chisel plow till 1 anhydrous 53 kg ha n of 43 kg ha 2 atrazine 2 2 kg ha 3 urea 284 kg ha n of 131 kg ha 4 dap p2o5 123 kg ha p of 54 kg ha table a 2 tile drainage parameters for cedar creek table a 2 parameter definition value ddrain mm depth from surface to tile drains 1000 tdrain hr time to drain to field capacity 24 gdrain hr drainage lag time 48 dep imp mm depth from surface to impervious layer 1200 itdrn flag drainage routine 1 indicates new routine 1 re mm effective drain radius 20 sdrain mm distance between tile drains 20000 drain co mm day drainage coefficient 10 pc mm hr pump capacity 0 latksatf multiplication factor for lateral conductivity 1 2 table a 3 agricultural practices used for little river table a 3 management strategy date operation cotton year 15 apr chisel plow till 1 may field cultivator till 2 may planting 3 may phosphorous application1 3 may nitrogen application2 11 nov harvest peanut year 1 apr chisel plow till 15 apr chisel plow till 1 may field cultivator till 2 may planting 1 oct harvest cotton year 15 mar chisel plow till 1 apr chisel plow till 15 apr chisel plow till 1 may field cultivator till 2 may planting 3 may phosphorous application1 3 may nitrogen application2 11 nov harvest 1 elemental phosphorous 12 3 kg ha 2 elemental nitrogen 92 1 kg ha 
