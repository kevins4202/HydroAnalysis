index,text
25705,there is a need to determine and quantify global change induced phenological asynchrony because of possible loss of biodiversity and implications for the food web phenological asynchrony in freshwater lakes can be studied retrospectively by analysing historical data and prospectively by scenario analysis based on historical data models allowing to study phenology by scenario analysis need to be valid for multiple populations over decades limiting the suitability of process based models that are structural rigid and calibratable only for a limited period of time here we applied models inferred from historical data by evolutionary computation to simulate food web dynamics of the plankton community of lake müggelsee from 2002 to 2012 the models were driven by nutrient concentrations water temperature wt and endogenous interrelationships within the plankton community the validated models simulated seasonal and inter annual dynamics of seven phyto and zooplankton groups in response to scenarios of prospective global warming nutrient enrichments as well as combinations of warming with nutrient enrichments and warming with nutrient reductions phenological wt sensitivities resulting from the warming scenario indicated substantial shifts towards earlier timing of cyanobacteria peaks and delayed timing of cladocerans peaks in summer suggesting significant phenological asynchrony in the plankton community of lake müggelsee the phenological sensitivities of cyanobacteria and cladocerans towards phosphorus and nitrogen enrichments revealed similar trends of summer peaks as identified for warming most likely contributing to phenological asynchrony the combination of warming and nutrient reductions showed increased spring maxima but almost unchanged summer peaks of cyanobacteria demonstrating that gradually decreasing phosphorus and nitrogen concentrations may outweigh warming effects on phytoplankton growth scenarios that simulated wt and nutrient changes as gradual processes rather than immediately imposed events proved to be more realistic and credible the proposed ensemble of complementary inferential models proved to be a viable tool for determining long term dynamics and phenological asynchronies in plankton communities under the impact global changes keywords eutrophic polymictic lake plankton hybrid evolutionary algorithm hea inferential modelling scenario analysis warming eutrophication phenological sensitivities phenological asynchrony 1 introduction there is strong evidence that global change induces phenological changes on various system levels adrian et al 2006 which may lead to asynchrony among species functional groups and trophic levels causing biodiversity loss e g stenseth and mysterud 2002 edwards and richardson 2004 winder and schindler 2004 rosenzweig et al 2007 thackeray et al 2016 henson et al 2018 long term monitoring of phyto and zooplankton communities in conjunction with physical and chemical habitat properties are a prerequisite for determining and quantifying phenological asynchrony in aquatic ecosystems taking advantage of long term phytoplankton data collected by the continuous plankton recorder richardson et al 2006 edwards and richardson 2004 identified an average shift in the timing of spring blooms of key diatom species in the northeast atlantic from 1976 to 2005 by 0 3 days per decade in lakes of the north temperate zone the timing of the spring phytoplankton and zooplankton abundance peaks coherently advanced by about 4 weeks in the past decades following changes in ice phenology earlier ice off and enhanced water temperature blenckner et al 2007 adrian et al 2006 straile et al 2012 analysing limnological time series of lake müggelsee germany from 1979 to 2003 adrian et al 2006 concluded that fast growing plankton species such as algae cladocerans and rotifers responded coherently as a direct response to warming trends whereby slow growing species such as copepods going through multiple larvae stages were potentially affected by multiple warming events in the course of a year which did not necessarily lead to coherent changes in their phenologies here the timing of the emergence of larval stages from the sediment in spring as initiated by water temperature and light conditions loss of ice was advanced by about 1 month adrian et al 2006 thackeray et al 2010 estimated an advanced average timing of phytoplankton blooms in british freshwater lakes from 1976 to 2005 by 0 23 d yr 1 thackeray et al 2016 assessed the relative impact of climate change on 812 marine freshwater and terrestrial taxa over the period 1960 2012 in the uk by means of phenological sensitivity in days per c and days per mm rainfall since the above mentioned studies focused primarily on water temperature as driving force for phenological asynchrony thackeray et al 2008 and maberly et al 2018 suggested that varying nutrient availability over time can also have an important influence upon seasonality in view of the fact that changes in water temperature and nutrient concentrations don t occur in isolation but cause synergistic effects there is a need to analyse simultaneously effects of both on phenological synchrony respectively asynchrony in plankton communities meta analysis of long term plankton data is one approach that has been successfully applied to reveal phenology of populations and communities across diverse lakes with different morphology climate and trophic status e g blenckner et al 2007 feuchtmayr et al 2012 thackeray et al 2016 maberly et al 2018 recknagel et al 2019 even though applied retrospectively meta analysis allows to reveal phenological differences of plankton populations between different trophic states and climate zones by contrast scenario analysis based on models that are valid for the simulation of plankton dynamics enables to analyse prospectively long term responses of plankton to well defined global changes a variety of process based models have been applied for scenario analysis such as schladow and hamilton 1997 arhonditsis and brett 2005 trolle et al 2014 chen et al 2014 dou et al 2019 specific scenarios were addressed by simulating plankton population dynamics and nutrient cycles with ad hoc modified driving forces such as p and n loads water temperature and transparency shortcomings of these studies arose from the rigidity of process based models relying on rationally bounded algebraic equations that could be calibrated and validated only for a short period of time cf recknagel et al 2018 thomas et al 2018 scenarios therefore did not analyse the gradual variations of seasonal and inter annual habitat conditions such as temperature and nutrient concentrations over consecutive years but simulated snapshot like an abrupt instantaneous change imposed on the same year or two in contrast lake ecosystems respond to global changes over time differently due to year by year varying meteorological and ecological constellations as rigler 1982 articulated it our concern is to develop theories that will predict for years in the future the abundance of species that are part of the food web of an ecosystem here we developed an ensemble of complementary inferential models for seven key phytoplankton phyla and zooplankton groups of the polymictic eutrophic lake müggelsee by means of the hybrid evolutionary algorithm hea in contrast to process based models that define rate equations from theoretical and empirical assumptions inferential models apply equations learned and acquired from long term data patterns thus resulting inferential models reflect specific local conditions over a long term period whilst process based models though more theoretical sound fail to simulate specific local conditions over a longer period of time the plankton models developed by hea utilised solely exogenous drivers relevant for warming and eutrophication scenarios such as concentrations of dissolve inorganic nitrogen din dissolved inorganic phosphorus dip total phosphorus tp and silica sio2 and water temperature wt in addition endogenous drivers reflected trophic interrelationships between phyto and zooplankton groups both exo and endogenous drivers allowed to simulate simultaneously daily abundances of the seven plankton groups calibrated and validated for the years 2002 2012 a period with high inter annual variability but insignificant trends in water temperature and trophic state hea evolved tens of thousands of model generations by means of repeated cross over mutation and reproduction to determine the seven fittest models assuming that the fittest models learnt seasonal and inter annual patterns of abundances in conjunction with varying wt and nutrient concentrations along the 11 years we applied them to analyse phenological asynchrony by following scenarios 1 warming of wt in c by 2 per year 2 p enrichment by 5 per year 3 n enrichment by 5 per year 4 warming and n p enrichment and 5 warming and n p reduction by 5 per year scenarios 1 to 3 allowed to assess phenological asynchrony by means of phenological sensitivities of diatoms cyanobacteria copepods and cladocerans in relation to wt dip tp and din the scenarios 4 and 5 enabled us to estimate combined effects of changing wt din tp and dip on seasonal asynchrony between diatoms cyanobacteria copepods and cladocerans the study focused on following research questions 1 does the ensemble of inferential models simulate realistically long term plankton dynamics of lake müggelsee as prerequisite for scenario analysis and do the if conditions of the inferential models correspond with rational thresholds of key drivers of the seven plankton populations 2 does the scenario analysis indicate signs of phenological asynchrony between key populations of the plankton community of lake müggelsee in response to warming and nutrient changes and do results differ by simulating scenarios as gradual processes rather than immediate events 2 materials and methods 2 1 limnological data of lake müggelsee lake müggelsee is a eutrophic polymictic lake in southeastern berlin germany that has a mean hydraulic retention time of 6 12 weeks during the 22 years of investigation 1991 2012 it has been sampled weekly biweekly during winter where 21 volumetrically weighed subsamples from five stations have been integrated concentrations of dissolved inorganic phosphorus dip total phosphorus tp dissolved inorganic nitrogen din and soluble reactive silica sio2 were quantified using standard protocols apha 2005 water temperature was measured by a multiprobe sonde ysi 6600 yellow springs instruments usa at 0 5 m volumetric integrated samples accounting for the volume of the different depth strata were taken with a 5 l friedinger vertical tube sampler at 1 m intervals from the surface to the bottom phytoplankton was counted in sedimentation chambers using an inverted microscope utermoehl 1958 cell volumes were obtained from each algal species taking into account their specific geometric shape zooplankton 20 l of the integrated sample were collected on a 30 μm mesh net hydrobios fixed with formaldehyde to a final concentration of 4 and enumerated under a light microscope table 1 summarises limnological variables of lake müggelsee utilised in this study whilst data from 2002 to 2012 were used for training and validation data from 1991 to 2001 served as unseen data for testing 2 2 inferential modelling by the hybrid evolutionary algorithm hea evolutionary computation infers models from data based on principles of natural selection and evolution holland et al 1986 the hybrid evolutionary algorithm hea cao et al 2014 2016 has been designed to evolve fittest if then else models from ecological data by integrating genetic programming gp and differential evolution de according to fig 1 it applies gp according to koza 1992 to evolve the optimum structure of if then else models and de according to storn and price 1997 to optimize the parameters of rule models since gp typically operates on parse trees rather than on bit strings it suits well to evolve if then else models for multivariate relationships the way in which genetic programming applies cross over mutation and reproduction to create if then else models by means of logic functions fl and or comparison functions fc and arithmetic functions fa exp ln has been illustrated by recknagel et al 2017 differential evolution extracts information on the distance and direction of current solutions models towards global optimum guiding the search for optimal parameters in the if then else models as outlined by cao et al 2014 the source code of hea is written in c and is activated by parameters specifying numbers of data points driving and response variables as well as the ratio between training and testing data for boot strapping information for selecting alternative parameter optimisation methods for details see cao et al 2014 and levels of if then else rules to take into account its intensive cpu time it is advisable to run hea on supercomputers in cloud mode e g to model ten years of daily data of one response and five driving variables lasts approx for 75 h information on hea can also be found at netlake toolbox 11 hea pdf dkit ie fig 2 illustrates the concept of developing an ensemble of complementary models driven not only by the exogenous inputs wt tp dip din and sio2 but also by the endogenous inputs calculated for the abundances of phytoplankton and zooplankton one time step before as shown in fig 2 all exogenous and endogenous inputs were provided during the modelling process however specific inputs for each of the seven plankton models were selected by inference from the data though inferred from long term data patterns the input selection of the models reflected basic theoretical assumptions typical for local conditions of the shallow polymictic lake müggelsee with relative high abundances of diatoms and cyanobacteria compared to low abundances of motile chloro and dinophytes a cross validation of the plankton models has been carried out in relation to the measured data of the study period from 2002 to 2012 as well as to eleven years of unseen data from 1991 to 2001 2 3 scenario analysis phenological parameters and delta differences following scenarios were simulated for 11 years from 2002 to 2012 1 warming gradually increasing wt by 2 per year justified by an observed decadal increase of wt by 0 52 c in lake müggelsee from 1976 to 2015 2 p enrichment gradually increasing dip and tp concentration by 5 per year 3 n enrichment gradually increasing din concentration by 5 per year 4 warming and n p enrichment by 5 per year and 5 warming and n p reduction by 5 per year all five scenarios were defined to test consequences of theoretically possible changes on the plankton community imposed by global warming and eutrophication over 11 years of the eleven years of scenario results the maxima and days of occurrences of abundance peaks were averaged for the spring months march till may and the summer months june till august based on these averages phenological sensitivities were calculated for spring and summer related to delta differences of water temperature for the scenario warming delta differences of dip and tp concentrations for the scenario p enrichment and delta differences of din concentrations for the scenario n enrichment 3 results 3 1 model design and validation fig 3 illustrates the input selection of the plankton models rather than focusing on one model only the hybrid evolutionary algorithm hea evaluates successive generations of mutated and optimised models as guide for the search for the overall fittest model within highly complex and diverse input relationships across the long term data as a result the models learn and select the key drivers by their weight within the numerous data patterns over 1000s of model generations fig 3a summarises the input selection frequencies of the 100 fittest models for each of the four phytoplankton groups indicating that more than 90 of the diatoms models selected dip and sio2 as drivers and the majority of cyanobacteria models selected wt tp and rotifers as drivers the examples in fig 3b and c shows the sensitivity curves of the 3 fittest diatoms models indicating negative logarithmic relationships with dip and sio2 fig 3d displays the input selection frequencies of the 100 fittest zooplankton models suggesting that all models for the three zooplankton groups selected wt and all copepods models selected diatoms as drivers fig 3e and f shows the positive linear sensitivity curves for wt and diatoms of the 3 fittest copepods models fig 4 illustrates the input selection of the overall fittest models for the seven plankton groups models of chlorophytes and diatoms appeared to be only driven by wt and nutrients with sio2 being solely selected by the diatom model rotifers were selected as endogenous driver by the cyanobacteria model while cladocerans by the dinophytes model diatoms and wt have been selected as drivers for all three zooplankton models while cyanobacteria appeared as endogenous driver for rotifers and cladocerans amongst nutrients only tp was offered as input for the zooplankton models as a substitute for detritus and total algal mass that has been selected as driver by cladocerans and copepods figs 5 and 6 document the fittest phytoplankton models that showed p values 0 05 the if condition of the model for diatoms fig 5a suggested that low abundances occurred in most of the years when tp concentrations exceeded 283 8 μg l 1 which coincided with extended periods of thermal stratification of lake müggelsee initiating internal din loading from sediments the cross validation achieved a coefficient of determination r2 0 52 and the aggregated cross validation an r2 0 77 fig 5b and c the model for chlorophytes fig 5d e f forecasted correctly the high abundances only observed in 2008 and 2009 and achieved an r2 0 83 for both cross and aggregated cross validation fig 6a b c display the modelling results for dinophytes suggesting that a wt 18 6 c may have caused the high abundances observed in 2007 both cross and aggregated cross validation reached an r2 0 71 the if condition of the cyanobacteria model fig 6d e f indicated that the coincidence of wt 19 8 c and dip 66 2 μg l 1 may be linked to highest abundances observed in 2006 2010 and 2012 the model s cross validation attained an r2 0 58 and the aggregated cross validation an r2 0 83 the fittest models for zooplankton are documented in fig 7 that showed p values 0 055 whilst the if condition of the copepods model fig 7 a b c demonstrated that a wt 18 4 c may have caused highest abundances observed in 2003 and 2005 the model s cross validation reached an r2 0 45 and the aggregated cross validation an r2 0 65 the coefficients of determination for the cross and aggregated cross validation of the cladocerans model fig 7 d e f resulted in r2 0 65 and r2 0 71 respectively the rotifers model fig 7 g h i revealed that higher abundances across the 11 years occurred when wt 14 5 c while cross validation achieved an r2 0 51 aggregated cross validation resulted in r2 0 59 fig 8 illustrates the aggregated validation of the seven plankton models by means of 11 years of unseen data from 1991 to 2001 even though the models match the typical seasonal trends of the plankton populations their accuracy in timing and magnitudes differ as reflected by their coefficient of determination r2 the models for the four phytoplankton phyla achieved an averaged r2 0 54 and for the three zooplankton groups an averaged r2 0 67 3 2 scenario analysis and phenology fig 9 illustrates estimated effects of combinations of warming by 20 and p n reductions by 50 whereby the changes were simulated either as imposed fully at once for all years red lines or gradually in annual rates green line the results showed declining abundances in spring for diatoms fig 9a and in summer and autumn for cyanobacteria fig 9c and copepods fig 9e that were much more severe for immediately simulated changes rather than gradually simulated changes in case of cladocerans fig 9g warming combined with nutrient reductions caused slightly increased abundances during summer and autumn that again were more distinct for immediately simulated changes than for gradual changes when simulating the scenario by immediate full changes fig 9b and d showed drastic direct effects on concentrations of diatoms and cyanobacteria during summer from the first year onwards causing artefacts for consecutive years copepods and cladocerans showed similar effects fig 9 f and 9g although being driven primarily by wt and only indirectly affected by nutrient changes in contrast gradually developing changes resulted in more realistic long term results the phenological sensitivities and delta differences of abundance peaks of diatoms cyanobacteria cladocerans and copepods in spring and in summer in response to gradual increases of wt by 2 per year are represented in fig 10 a b whilst the phenological wt sensitivities of cyanobacteria of approx 3 days c in spring and 5 days c in summer indicate much earlier abundance peaks the peak magnitudes increased by 20 in contrast peaks of diatoms were 22 lower and delayed by 1 8 days c in spring and only slightly decreased but delayed by 4 6 days c in summer the response of cladocerans indicated abundance peaks that were 95 higher and delayed by 0 9 days c in spring and 39 higher and delayed by approx 9 days c in summer peaks of copepods appeared to be delayed by 4 3 days c and decreased by 24 in spring whereby summer peaks were only slightly delayed but declined by 38 fig 10 c d display the phenological sensitivities and delta differences of abundance peaks of diatoms and cyanobacteria to gradual increases of dip and of cladocerans and copepods to gradual increases of tp by 5 per year the phenological dip sensitivities of cyanobacteria displayed 15 higher spring peaks that were delayed by 0 9 days per μg dip l 1 and 40 higher summer peaks advanced by 0 35 days per μg dip l 1 the diatoms showed a phenological dip sensitivity of 1 6 days per μg dip l 1 in spring and of 0 05 days per μg dip l 1 in summer with 3 higher abundance peaks in spring but 10 lower peaks in summer whilst phenological tp sensitivities of cladocerans and copepods showed slightly delayed summer peaks cladocerans displayed spring peaks advanced by 1 8 days per μg tp l 1 both cladocerans and copepods showed spring peaks declined by approximately 50 and summer peaks increased by 30 for cladocerans and by 10 for copepods the phenological din sensitivities represented in fig 10e and f suggested earlier and increased peaks of cyanobacteria in summer whilst diatoms copepods and cladocerans displayed declining peaks with slight delays both in spring and summer fig 11 summarises results of the scenario analysis by comparing timing and magnitudes of measured and simulated spring and summer peaks for diatoms fig 11 a b and cyanobacteria fig 11 c d peaks of diatoms responded strongly in spring to the scenario warming by being reduced by 25 and to the scenario warming and n p reduction by being lowered by 40 but were only slightly affected by the scenario warming and n p enrichment the timing of spring maxima of diatoms showed delays for all three scenario between 2 and 7 days the summer peaks of diatoms displayed similar trends of declining abundances for the three scenarios whilst occurring 1 day in advance at warming and 13 days delayed at warming and n p reduction cyanobacteria responded in spring to all three scenarios with peak maxima slightly increased by 7 18 whilst occurring 3 days earlier by warming but being delayed by 13 days for warming and n p enrichment and by 8 days for warming and n p reduction however summer peaks by warming and warming and n p enrichment exceeded measured abundances by 22 and 43 respectively while summer maxima for warming and n p reduction appeared to be only slightly reduced by 3 the summer peaks of cyanobacteria occurred earlier for all three scenarios by 2 days for warming and n p reduction and by up to 12 days for warming and n p enrichment results of the scenario analysis for zooplankton are represented in fig 12 whilst the scenarios warming and warming and n p reduction caused a 50 decline of the spring peaks of cladocerans warming and n p enrichment resulted in a 80 increase of the spring peaks summer peaks of cladocerans at warming and n p reduction declined by 30 and were delayed by 27 days but increased by 80 with a delay of 36 days for warming and n p enrichment compared to 45 and a delay by 17 days for warming peak abundances of copepods declined by all three scenarios in spring with delays by 2 5 days but declined in summer only by warming and n p reduction with a delay of 37 days 4 discussion the seven complementary inferential models allowed to simulate realistically seasonal and inter annual dynamics of key phyto and zooplankton populations cross validated for the years from 2002 to 2012 and tested for unseen data from 1991 to 2001 by learning from patterns of the observed data from 2002 to 2012 the if conditions of the models determined thresholds of key drivers explaining plankton population dynamics the models matched largely timing and magnitudes of observed low and peak events justifying their use for simulating scenarios on warming and nutrient changes for the years from 2002 to 2012 as gradual processes rather than as instantaneous events running scenarios for warming and nutrient enrichments separately allowed to better understand the prospective specific effects of temperature and nutrient changes on the phenology of plankton populations whilst combined scenarios for warming and nutrient changes exposed reinforcing and mitigating effects on plankton phenology it became evident that results of the scenarios simulated as gradually developing processes were more realistic compared to those simulated as immediately imposed events that over or underestimated plankton abundances by more than 50 the thresholds discovered for high abundances of dinophytes wt 18 6 c and cyanobacteria wt 19 8 c and dip 66 2 μg l 1 corresponded well with optimum growth conditions of these phyla observed by reynolds 1984 and wagner and adrian 2009 peaks predicted by the diatom model were solely related to the threshold tp 283 8 μg l 1 suggesting that diatoms may be outcompeted by cyanobacteria at higher tp concentrations such high tp concentrations were caused by strong remobilization of redox sensitive bound p from sediments during extended periods of thermal stratification resulting in high sedimentation losses of diatoms and favouring condition for cyanobacteria nevertheless the distinct spring peaks of diatoms contribute a prominent fraction of the total algal mass throughout the year the differences in the water temperature thresholds of rotifers wt 14 5 c and cladocerans wt 19 7 c refer to their seasonal succession with rotifer abundances peaking prior to the daphnids the threshold wt 18 4 c for copepods is hinting at their different life stages and species succession throughout the year the models for cladocerans and rotifers selected cyanobacteria dino and chlorophytes as drivers indicating their diverse food preferences in contrast the model for copepods selected only diatoms as endogenous driver corresponding with the fact that cyclopoid copepods being known as omnivores that prey heavily on diatoms adrian 1987 adrian and frost 1993 the phenological sensitivities of phyto and zooplankton to water temperature demonstrated that spring and summer peaks of cyanobacteria increased by about 20 and occurred approx 3 and 5 days c in advance taking into account that delta differences of water temperatures in summer exceed 2 c warming by 2 suggests summer peaks may occur at least 10 days earlier in contrast diatoms displayed declined spring and summer peaks with delays by 2 and 5 days c since growth of diatoms populations in temperate lakes is not favoured by higher temperatures e g reynolds 1984 anderson 2000 shatwell et al 2008 and intensifies grazing by zooplankton cladocerans populations increased substantially in spring and summer most likely driven by both increased water temperatures and high food supply by phytoplankton populations the mean summer peak was delayed by 9 days c resulting in more than 18 days for a delta difference of wt that exceeds 2 c both spring and summer peaks of copepods declined possibly affected by competition with cladocerans that benefited from shorter life cycles and therefore earlier grazing of phytoplankton e g mott 1989 overall the scenario warming indicated that increasing water temperatures may not only lead to earlier timing of increased abundances of cyanobacteria and delayed timing of increased abundances of cladocerans but also stimulate phytoplankton grazing by cladocerans during the spring to summer period as observed in mesocosms experiments by velthuis et al 2017 the phenological p sensitivities indicated slightly delayed but higher spring peaks of both cyanobacteria and diatoms but earlier and 40 higher summer peaks of cyanobacteria whilst diatoms declined in summer as demonstrated by shatwell and köhler 2019 phytoplankton in lake müggelsee is limited by low dip concentrations between march and june during this study period that may explain higher modelled abundances of diatoms and cyanobacteria in response to p enrichment in spring as a consequence cyanobacteria start with a higher biomass in the summer season allowing them higher summer peaks the growth of diatoms is not favoured by summer temperatures and less limited by dip demott 1989 the phenological din sensitivities revealed in spring declining peaks of diatoms cladocerans and copepods but hardly noticeable responses by cyanobacteria however increased cyanobacteria abundances occurred slightly earlier in summer with further declining abundances of diatoms and cladocerans according to shatwell and köhler 2019 din concentrations in lake müggelsee were consistently low from june to august during this study period possibly explaining the positive response of cyanobacteria in summer to din enrichment these results agree with pearl et al 2016 that in the light of rising anthropogenic n and p loads instead of considering traditionally p as the only driver of cyanobacteria growth the combined n and p enrichment needs to be taken into account diatoms responded to the scenario warming and n p enrichment primarily by declined summer peaks however cyanobacteria showed higher but delayed peaks in spring and strongly increased summer peaks occurring 12 days earlier the increased abundance of cyanobacteria in spring corresponds with findings by shatwell et al 2008 that filamentous cyanobacteria are favoured by warming over diatoms cladocerans developed much higher peaks in spring and with a delay by 35 days in summer this significant gap of 47 days between summer peaks of cyanobacteria and cladocerans is another indication of likely phenological asynchrony within the plankton community of lake müggelsee under the influence of global warming and eutrophication results of the scenario warming and n p reduction suggested slightly delayed but strongly declined spring and summer peaks of diatoms but higher and delayed spring peaks and earlier but almost unchanged summer peaks for cyanobacteria these results demonstrate that gradually decreasing phosphorus and nitrogen concentrations may cause a trade off with the water temperature driven cyanobacteria growth these results correspond with findings by stich and brinker 2010 who concluded that oligotrophication effects are likely to outweigh climate effects on phytoplankton productivity in the foreseeable future 5 conclusions inferential modelling by means of evolutionary algorithms offers a vital alternative to process based modelling in limnology evolutionary algorithms induce multivariate condition action rules from complex data patterns following the cognitive principles of generative creation and choices over open ended possibilities holland et al 1986 the fittest models evolved by the hybrid evolutionary algorithm hea simulated seasonal and inter annual patterns in plankton dynamics concisely and quantitatively and are as empirical and simple as anticipated by peters 1986 as requirements for predictive limnology following conclusions can be drawn from this study 1 the ensemble of complementary inferential models proved to be predictive for plankton community dynamics of lake müggelsee across two decades 2 simulating global change scenarios as events fully imposed to all years at once affects results by cumulated artefacts simulating scenarios as gradual processes avoids such artefacts resulting in more realistic forecasts for consecutive years 3 the scenario analysis has revealed that warming of the lake by 2 per year decreased spring and summer maxima of diatoms advanced higher spring and summer maxima of cyanobacteria delayed higher summer maxima of cladocerans and delayed summer maxima of copepods corresponding well with findings from retrospective data analysis of lake müggelsee by adrian et al 2006 warming and n p enrichment suggested that the gap between increased summer peaks of cyanobacteria and cladocerans to be widened to 47 days causing severe phenological asynchrony within the plankton community warming and n p reduction increased spring maxima but kept summer peaks of cyanobacteria almost unchanged demonstrating that gradually decreasing phosphorus and nitrogen concentrations may trade off effects of water temperature driven cyanobacteria growth 4 a comparison of phenological sensitivities derived from the warming din and p enrichment scenarios suggested that cyanobacteria tend to have delayed but higher spring peaks and earlier and higher summer peaks whilst cladocerans are likely to have delayed summer peaks to be higher in response to warming and p enrichment but slightly lower to din enrichment these results gave further evidence of distinct phenological asynchrony in the plankton community of lake müggelsee under the influence of warming and eutrophication the fact that copepods displayed only minor phenological sensitivities in summer 0 2 days c and 0 25 days per μg tp l 1 confirms their resilience to habitat changes due to changing life stages and species dominance throughout the years e g adrian et al 2006 future work aims to extend the ensemble of inferential models by endogenous drivers reflecting competition and predation between the phyto and zooplankton groups another ongoing task is to verify outcomes of this research by applying the proposed modelling approach to a variety of lakes with different climates and trophic states declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to steven c maberly for his valuable comments on the first draft of the manuscript we are also thankful to the leibniz institute of freshwater ecology and inland fishery berlin for providing 22 years of unique limnological time series of lake müggelsee we acknowledge helgard täuscher katrin preuβ phytoplankton and renate rusche and uschi newen zooplankton for taxonomic analysis this work was partially supported by jst sicosrp grant number jpmjsc1804 
25705,there is a need to determine and quantify global change induced phenological asynchrony because of possible loss of biodiversity and implications for the food web phenological asynchrony in freshwater lakes can be studied retrospectively by analysing historical data and prospectively by scenario analysis based on historical data models allowing to study phenology by scenario analysis need to be valid for multiple populations over decades limiting the suitability of process based models that are structural rigid and calibratable only for a limited period of time here we applied models inferred from historical data by evolutionary computation to simulate food web dynamics of the plankton community of lake müggelsee from 2002 to 2012 the models were driven by nutrient concentrations water temperature wt and endogenous interrelationships within the plankton community the validated models simulated seasonal and inter annual dynamics of seven phyto and zooplankton groups in response to scenarios of prospective global warming nutrient enrichments as well as combinations of warming with nutrient enrichments and warming with nutrient reductions phenological wt sensitivities resulting from the warming scenario indicated substantial shifts towards earlier timing of cyanobacteria peaks and delayed timing of cladocerans peaks in summer suggesting significant phenological asynchrony in the plankton community of lake müggelsee the phenological sensitivities of cyanobacteria and cladocerans towards phosphorus and nitrogen enrichments revealed similar trends of summer peaks as identified for warming most likely contributing to phenological asynchrony the combination of warming and nutrient reductions showed increased spring maxima but almost unchanged summer peaks of cyanobacteria demonstrating that gradually decreasing phosphorus and nitrogen concentrations may outweigh warming effects on phytoplankton growth scenarios that simulated wt and nutrient changes as gradual processes rather than immediately imposed events proved to be more realistic and credible the proposed ensemble of complementary inferential models proved to be a viable tool for determining long term dynamics and phenological asynchronies in plankton communities under the impact global changes keywords eutrophic polymictic lake plankton hybrid evolutionary algorithm hea inferential modelling scenario analysis warming eutrophication phenological sensitivities phenological asynchrony 1 introduction there is strong evidence that global change induces phenological changes on various system levels adrian et al 2006 which may lead to asynchrony among species functional groups and trophic levels causing biodiversity loss e g stenseth and mysterud 2002 edwards and richardson 2004 winder and schindler 2004 rosenzweig et al 2007 thackeray et al 2016 henson et al 2018 long term monitoring of phyto and zooplankton communities in conjunction with physical and chemical habitat properties are a prerequisite for determining and quantifying phenological asynchrony in aquatic ecosystems taking advantage of long term phytoplankton data collected by the continuous plankton recorder richardson et al 2006 edwards and richardson 2004 identified an average shift in the timing of spring blooms of key diatom species in the northeast atlantic from 1976 to 2005 by 0 3 days per decade in lakes of the north temperate zone the timing of the spring phytoplankton and zooplankton abundance peaks coherently advanced by about 4 weeks in the past decades following changes in ice phenology earlier ice off and enhanced water temperature blenckner et al 2007 adrian et al 2006 straile et al 2012 analysing limnological time series of lake müggelsee germany from 1979 to 2003 adrian et al 2006 concluded that fast growing plankton species such as algae cladocerans and rotifers responded coherently as a direct response to warming trends whereby slow growing species such as copepods going through multiple larvae stages were potentially affected by multiple warming events in the course of a year which did not necessarily lead to coherent changes in their phenologies here the timing of the emergence of larval stages from the sediment in spring as initiated by water temperature and light conditions loss of ice was advanced by about 1 month adrian et al 2006 thackeray et al 2010 estimated an advanced average timing of phytoplankton blooms in british freshwater lakes from 1976 to 2005 by 0 23 d yr 1 thackeray et al 2016 assessed the relative impact of climate change on 812 marine freshwater and terrestrial taxa over the period 1960 2012 in the uk by means of phenological sensitivity in days per c and days per mm rainfall since the above mentioned studies focused primarily on water temperature as driving force for phenological asynchrony thackeray et al 2008 and maberly et al 2018 suggested that varying nutrient availability over time can also have an important influence upon seasonality in view of the fact that changes in water temperature and nutrient concentrations don t occur in isolation but cause synergistic effects there is a need to analyse simultaneously effects of both on phenological synchrony respectively asynchrony in plankton communities meta analysis of long term plankton data is one approach that has been successfully applied to reveal phenology of populations and communities across diverse lakes with different morphology climate and trophic status e g blenckner et al 2007 feuchtmayr et al 2012 thackeray et al 2016 maberly et al 2018 recknagel et al 2019 even though applied retrospectively meta analysis allows to reveal phenological differences of plankton populations between different trophic states and climate zones by contrast scenario analysis based on models that are valid for the simulation of plankton dynamics enables to analyse prospectively long term responses of plankton to well defined global changes a variety of process based models have been applied for scenario analysis such as schladow and hamilton 1997 arhonditsis and brett 2005 trolle et al 2014 chen et al 2014 dou et al 2019 specific scenarios were addressed by simulating plankton population dynamics and nutrient cycles with ad hoc modified driving forces such as p and n loads water temperature and transparency shortcomings of these studies arose from the rigidity of process based models relying on rationally bounded algebraic equations that could be calibrated and validated only for a short period of time cf recknagel et al 2018 thomas et al 2018 scenarios therefore did not analyse the gradual variations of seasonal and inter annual habitat conditions such as temperature and nutrient concentrations over consecutive years but simulated snapshot like an abrupt instantaneous change imposed on the same year or two in contrast lake ecosystems respond to global changes over time differently due to year by year varying meteorological and ecological constellations as rigler 1982 articulated it our concern is to develop theories that will predict for years in the future the abundance of species that are part of the food web of an ecosystem here we developed an ensemble of complementary inferential models for seven key phytoplankton phyla and zooplankton groups of the polymictic eutrophic lake müggelsee by means of the hybrid evolutionary algorithm hea in contrast to process based models that define rate equations from theoretical and empirical assumptions inferential models apply equations learned and acquired from long term data patterns thus resulting inferential models reflect specific local conditions over a long term period whilst process based models though more theoretical sound fail to simulate specific local conditions over a longer period of time the plankton models developed by hea utilised solely exogenous drivers relevant for warming and eutrophication scenarios such as concentrations of dissolve inorganic nitrogen din dissolved inorganic phosphorus dip total phosphorus tp and silica sio2 and water temperature wt in addition endogenous drivers reflected trophic interrelationships between phyto and zooplankton groups both exo and endogenous drivers allowed to simulate simultaneously daily abundances of the seven plankton groups calibrated and validated for the years 2002 2012 a period with high inter annual variability but insignificant trends in water temperature and trophic state hea evolved tens of thousands of model generations by means of repeated cross over mutation and reproduction to determine the seven fittest models assuming that the fittest models learnt seasonal and inter annual patterns of abundances in conjunction with varying wt and nutrient concentrations along the 11 years we applied them to analyse phenological asynchrony by following scenarios 1 warming of wt in c by 2 per year 2 p enrichment by 5 per year 3 n enrichment by 5 per year 4 warming and n p enrichment and 5 warming and n p reduction by 5 per year scenarios 1 to 3 allowed to assess phenological asynchrony by means of phenological sensitivities of diatoms cyanobacteria copepods and cladocerans in relation to wt dip tp and din the scenarios 4 and 5 enabled us to estimate combined effects of changing wt din tp and dip on seasonal asynchrony between diatoms cyanobacteria copepods and cladocerans the study focused on following research questions 1 does the ensemble of inferential models simulate realistically long term plankton dynamics of lake müggelsee as prerequisite for scenario analysis and do the if conditions of the inferential models correspond with rational thresholds of key drivers of the seven plankton populations 2 does the scenario analysis indicate signs of phenological asynchrony between key populations of the plankton community of lake müggelsee in response to warming and nutrient changes and do results differ by simulating scenarios as gradual processes rather than immediate events 2 materials and methods 2 1 limnological data of lake müggelsee lake müggelsee is a eutrophic polymictic lake in southeastern berlin germany that has a mean hydraulic retention time of 6 12 weeks during the 22 years of investigation 1991 2012 it has been sampled weekly biweekly during winter where 21 volumetrically weighed subsamples from five stations have been integrated concentrations of dissolved inorganic phosphorus dip total phosphorus tp dissolved inorganic nitrogen din and soluble reactive silica sio2 were quantified using standard protocols apha 2005 water temperature was measured by a multiprobe sonde ysi 6600 yellow springs instruments usa at 0 5 m volumetric integrated samples accounting for the volume of the different depth strata were taken with a 5 l friedinger vertical tube sampler at 1 m intervals from the surface to the bottom phytoplankton was counted in sedimentation chambers using an inverted microscope utermoehl 1958 cell volumes were obtained from each algal species taking into account their specific geometric shape zooplankton 20 l of the integrated sample were collected on a 30 μm mesh net hydrobios fixed with formaldehyde to a final concentration of 4 and enumerated under a light microscope table 1 summarises limnological variables of lake müggelsee utilised in this study whilst data from 2002 to 2012 were used for training and validation data from 1991 to 2001 served as unseen data for testing 2 2 inferential modelling by the hybrid evolutionary algorithm hea evolutionary computation infers models from data based on principles of natural selection and evolution holland et al 1986 the hybrid evolutionary algorithm hea cao et al 2014 2016 has been designed to evolve fittest if then else models from ecological data by integrating genetic programming gp and differential evolution de according to fig 1 it applies gp according to koza 1992 to evolve the optimum structure of if then else models and de according to storn and price 1997 to optimize the parameters of rule models since gp typically operates on parse trees rather than on bit strings it suits well to evolve if then else models for multivariate relationships the way in which genetic programming applies cross over mutation and reproduction to create if then else models by means of logic functions fl and or comparison functions fc and arithmetic functions fa exp ln has been illustrated by recknagel et al 2017 differential evolution extracts information on the distance and direction of current solutions models towards global optimum guiding the search for optimal parameters in the if then else models as outlined by cao et al 2014 the source code of hea is written in c and is activated by parameters specifying numbers of data points driving and response variables as well as the ratio between training and testing data for boot strapping information for selecting alternative parameter optimisation methods for details see cao et al 2014 and levels of if then else rules to take into account its intensive cpu time it is advisable to run hea on supercomputers in cloud mode e g to model ten years of daily data of one response and five driving variables lasts approx for 75 h information on hea can also be found at netlake toolbox 11 hea pdf dkit ie fig 2 illustrates the concept of developing an ensemble of complementary models driven not only by the exogenous inputs wt tp dip din and sio2 but also by the endogenous inputs calculated for the abundances of phytoplankton and zooplankton one time step before as shown in fig 2 all exogenous and endogenous inputs were provided during the modelling process however specific inputs for each of the seven plankton models were selected by inference from the data though inferred from long term data patterns the input selection of the models reflected basic theoretical assumptions typical for local conditions of the shallow polymictic lake müggelsee with relative high abundances of diatoms and cyanobacteria compared to low abundances of motile chloro and dinophytes a cross validation of the plankton models has been carried out in relation to the measured data of the study period from 2002 to 2012 as well as to eleven years of unseen data from 1991 to 2001 2 3 scenario analysis phenological parameters and delta differences following scenarios were simulated for 11 years from 2002 to 2012 1 warming gradually increasing wt by 2 per year justified by an observed decadal increase of wt by 0 52 c in lake müggelsee from 1976 to 2015 2 p enrichment gradually increasing dip and tp concentration by 5 per year 3 n enrichment gradually increasing din concentration by 5 per year 4 warming and n p enrichment by 5 per year and 5 warming and n p reduction by 5 per year all five scenarios were defined to test consequences of theoretically possible changes on the plankton community imposed by global warming and eutrophication over 11 years of the eleven years of scenario results the maxima and days of occurrences of abundance peaks were averaged for the spring months march till may and the summer months june till august based on these averages phenological sensitivities were calculated for spring and summer related to delta differences of water temperature for the scenario warming delta differences of dip and tp concentrations for the scenario p enrichment and delta differences of din concentrations for the scenario n enrichment 3 results 3 1 model design and validation fig 3 illustrates the input selection of the plankton models rather than focusing on one model only the hybrid evolutionary algorithm hea evaluates successive generations of mutated and optimised models as guide for the search for the overall fittest model within highly complex and diverse input relationships across the long term data as a result the models learn and select the key drivers by their weight within the numerous data patterns over 1000s of model generations fig 3a summarises the input selection frequencies of the 100 fittest models for each of the four phytoplankton groups indicating that more than 90 of the diatoms models selected dip and sio2 as drivers and the majority of cyanobacteria models selected wt tp and rotifers as drivers the examples in fig 3b and c shows the sensitivity curves of the 3 fittest diatoms models indicating negative logarithmic relationships with dip and sio2 fig 3d displays the input selection frequencies of the 100 fittest zooplankton models suggesting that all models for the three zooplankton groups selected wt and all copepods models selected diatoms as drivers fig 3e and f shows the positive linear sensitivity curves for wt and diatoms of the 3 fittest copepods models fig 4 illustrates the input selection of the overall fittest models for the seven plankton groups models of chlorophytes and diatoms appeared to be only driven by wt and nutrients with sio2 being solely selected by the diatom model rotifers were selected as endogenous driver by the cyanobacteria model while cladocerans by the dinophytes model diatoms and wt have been selected as drivers for all three zooplankton models while cyanobacteria appeared as endogenous driver for rotifers and cladocerans amongst nutrients only tp was offered as input for the zooplankton models as a substitute for detritus and total algal mass that has been selected as driver by cladocerans and copepods figs 5 and 6 document the fittest phytoplankton models that showed p values 0 05 the if condition of the model for diatoms fig 5a suggested that low abundances occurred in most of the years when tp concentrations exceeded 283 8 μg l 1 which coincided with extended periods of thermal stratification of lake müggelsee initiating internal din loading from sediments the cross validation achieved a coefficient of determination r2 0 52 and the aggregated cross validation an r2 0 77 fig 5b and c the model for chlorophytes fig 5d e f forecasted correctly the high abundances only observed in 2008 and 2009 and achieved an r2 0 83 for both cross and aggregated cross validation fig 6a b c display the modelling results for dinophytes suggesting that a wt 18 6 c may have caused the high abundances observed in 2007 both cross and aggregated cross validation reached an r2 0 71 the if condition of the cyanobacteria model fig 6d e f indicated that the coincidence of wt 19 8 c and dip 66 2 μg l 1 may be linked to highest abundances observed in 2006 2010 and 2012 the model s cross validation attained an r2 0 58 and the aggregated cross validation an r2 0 83 the fittest models for zooplankton are documented in fig 7 that showed p values 0 055 whilst the if condition of the copepods model fig 7 a b c demonstrated that a wt 18 4 c may have caused highest abundances observed in 2003 and 2005 the model s cross validation reached an r2 0 45 and the aggregated cross validation an r2 0 65 the coefficients of determination for the cross and aggregated cross validation of the cladocerans model fig 7 d e f resulted in r2 0 65 and r2 0 71 respectively the rotifers model fig 7 g h i revealed that higher abundances across the 11 years occurred when wt 14 5 c while cross validation achieved an r2 0 51 aggregated cross validation resulted in r2 0 59 fig 8 illustrates the aggregated validation of the seven plankton models by means of 11 years of unseen data from 1991 to 2001 even though the models match the typical seasonal trends of the plankton populations their accuracy in timing and magnitudes differ as reflected by their coefficient of determination r2 the models for the four phytoplankton phyla achieved an averaged r2 0 54 and for the three zooplankton groups an averaged r2 0 67 3 2 scenario analysis and phenology fig 9 illustrates estimated effects of combinations of warming by 20 and p n reductions by 50 whereby the changes were simulated either as imposed fully at once for all years red lines or gradually in annual rates green line the results showed declining abundances in spring for diatoms fig 9a and in summer and autumn for cyanobacteria fig 9c and copepods fig 9e that were much more severe for immediately simulated changes rather than gradually simulated changes in case of cladocerans fig 9g warming combined with nutrient reductions caused slightly increased abundances during summer and autumn that again were more distinct for immediately simulated changes than for gradual changes when simulating the scenario by immediate full changes fig 9b and d showed drastic direct effects on concentrations of diatoms and cyanobacteria during summer from the first year onwards causing artefacts for consecutive years copepods and cladocerans showed similar effects fig 9 f and 9g although being driven primarily by wt and only indirectly affected by nutrient changes in contrast gradually developing changes resulted in more realistic long term results the phenological sensitivities and delta differences of abundance peaks of diatoms cyanobacteria cladocerans and copepods in spring and in summer in response to gradual increases of wt by 2 per year are represented in fig 10 a b whilst the phenological wt sensitivities of cyanobacteria of approx 3 days c in spring and 5 days c in summer indicate much earlier abundance peaks the peak magnitudes increased by 20 in contrast peaks of diatoms were 22 lower and delayed by 1 8 days c in spring and only slightly decreased but delayed by 4 6 days c in summer the response of cladocerans indicated abundance peaks that were 95 higher and delayed by 0 9 days c in spring and 39 higher and delayed by approx 9 days c in summer peaks of copepods appeared to be delayed by 4 3 days c and decreased by 24 in spring whereby summer peaks were only slightly delayed but declined by 38 fig 10 c d display the phenological sensitivities and delta differences of abundance peaks of diatoms and cyanobacteria to gradual increases of dip and of cladocerans and copepods to gradual increases of tp by 5 per year the phenological dip sensitivities of cyanobacteria displayed 15 higher spring peaks that were delayed by 0 9 days per μg dip l 1 and 40 higher summer peaks advanced by 0 35 days per μg dip l 1 the diatoms showed a phenological dip sensitivity of 1 6 days per μg dip l 1 in spring and of 0 05 days per μg dip l 1 in summer with 3 higher abundance peaks in spring but 10 lower peaks in summer whilst phenological tp sensitivities of cladocerans and copepods showed slightly delayed summer peaks cladocerans displayed spring peaks advanced by 1 8 days per μg tp l 1 both cladocerans and copepods showed spring peaks declined by approximately 50 and summer peaks increased by 30 for cladocerans and by 10 for copepods the phenological din sensitivities represented in fig 10e and f suggested earlier and increased peaks of cyanobacteria in summer whilst diatoms copepods and cladocerans displayed declining peaks with slight delays both in spring and summer fig 11 summarises results of the scenario analysis by comparing timing and magnitudes of measured and simulated spring and summer peaks for diatoms fig 11 a b and cyanobacteria fig 11 c d peaks of diatoms responded strongly in spring to the scenario warming by being reduced by 25 and to the scenario warming and n p reduction by being lowered by 40 but were only slightly affected by the scenario warming and n p enrichment the timing of spring maxima of diatoms showed delays for all three scenario between 2 and 7 days the summer peaks of diatoms displayed similar trends of declining abundances for the three scenarios whilst occurring 1 day in advance at warming and 13 days delayed at warming and n p reduction cyanobacteria responded in spring to all three scenarios with peak maxima slightly increased by 7 18 whilst occurring 3 days earlier by warming but being delayed by 13 days for warming and n p enrichment and by 8 days for warming and n p reduction however summer peaks by warming and warming and n p enrichment exceeded measured abundances by 22 and 43 respectively while summer maxima for warming and n p reduction appeared to be only slightly reduced by 3 the summer peaks of cyanobacteria occurred earlier for all three scenarios by 2 days for warming and n p reduction and by up to 12 days for warming and n p enrichment results of the scenario analysis for zooplankton are represented in fig 12 whilst the scenarios warming and warming and n p reduction caused a 50 decline of the spring peaks of cladocerans warming and n p enrichment resulted in a 80 increase of the spring peaks summer peaks of cladocerans at warming and n p reduction declined by 30 and were delayed by 27 days but increased by 80 with a delay of 36 days for warming and n p enrichment compared to 45 and a delay by 17 days for warming peak abundances of copepods declined by all three scenarios in spring with delays by 2 5 days but declined in summer only by warming and n p reduction with a delay of 37 days 4 discussion the seven complementary inferential models allowed to simulate realistically seasonal and inter annual dynamics of key phyto and zooplankton populations cross validated for the years from 2002 to 2012 and tested for unseen data from 1991 to 2001 by learning from patterns of the observed data from 2002 to 2012 the if conditions of the models determined thresholds of key drivers explaining plankton population dynamics the models matched largely timing and magnitudes of observed low and peak events justifying their use for simulating scenarios on warming and nutrient changes for the years from 2002 to 2012 as gradual processes rather than as instantaneous events running scenarios for warming and nutrient enrichments separately allowed to better understand the prospective specific effects of temperature and nutrient changes on the phenology of plankton populations whilst combined scenarios for warming and nutrient changes exposed reinforcing and mitigating effects on plankton phenology it became evident that results of the scenarios simulated as gradually developing processes were more realistic compared to those simulated as immediately imposed events that over or underestimated plankton abundances by more than 50 the thresholds discovered for high abundances of dinophytes wt 18 6 c and cyanobacteria wt 19 8 c and dip 66 2 μg l 1 corresponded well with optimum growth conditions of these phyla observed by reynolds 1984 and wagner and adrian 2009 peaks predicted by the diatom model were solely related to the threshold tp 283 8 μg l 1 suggesting that diatoms may be outcompeted by cyanobacteria at higher tp concentrations such high tp concentrations were caused by strong remobilization of redox sensitive bound p from sediments during extended periods of thermal stratification resulting in high sedimentation losses of diatoms and favouring condition for cyanobacteria nevertheless the distinct spring peaks of diatoms contribute a prominent fraction of the total algal mass throughout the year the differences in the water temperature thresholds of rotifers wt 14 5 c and cladocerans wt 19 7 c refer to their seasonal succession with rotifer abundances peaking prior to the daphnids the threshold wt 18 4 c for copepods is hinting at their different life stages and species succession throughout the year the models for cladocerans and rotifers selected cyanobacteria dino and chlorophytes as drivers indicating their diverse food preferences in contrast the model for copepods selected only diatoms as endogenous driver corresponding with the fact that cyclopoid copepods being known as omnivores that prey heavily on diatoms adrian 1987 adrian and frost 1993 the phenological sensitivities of phyto and zooplankton to water temperature demonstrated that spring and summer peaks of cyanobacteria increased by about 20 and occurred approx 3 and 5 days c in advance taking into account that delta differences of water temperatures in summer exceed 2 c warming by 2 suggests summer peaks may occur at least 10 days earlier in contrast diatoms displayed declined spring and summer peaks with delays by 2 and 5 days c since growth of diatoms populations in temperate lakes is not favoured by higher temperatures e g reynolds 1984 anderson 2000 shatwell et al 2008 and intensifies grazing by zooplankton cladocerans populations increased substantially in spring and summer most likely driven by both increased water temperatures and high food supply by phytoplankton populations the mean summer peak was delayed by 9 days c resulting in more than 18 days for a delta difference of wt that exceeds 2 c both spring and summer peaks of copepods declined possibly affected by competition with cladocerans that benefited from shorter life cycles and therefore earlier grazing of phytoplankton e g mott 1989 overall the scenario warming indicated that increasing water temperatures may not only lead to earlier timing of increased abundances of cyanobacteria and delayed timing of increased abundances of cladocerans but also stimulate phytoplankton grazing by cladocerans during the spring to summer period as observed in mesocosms experiments by velthuis et al 2017 the phenological p sensitivities indicated slightly delayed but higher spring peaks of both cyanobacteria and diatoms but earlier and 40 higher summer peaks of cyanobacteria whilst diatoms declined in summer as demonstrated by shatwell and köhler 2019 phytoplankton in lake müggelsee is limited by low dip concentrations between march and june during this study period that may explain higher modelled abundances of diatoms and cyanobacteria in response to p enrichment in spring as a consequence cyanobacteria start with a higher biomass in the summer season allowing them higher summer peaks the growth of diatoms is not favoured by summer temperatures and less limited by dip demott 1989 the phenological din sensitivities revealed in spring declining peaks of diatoms cladocerans and copepods but hardly noticeable responses by cyanobacteria however increased cyanobacteria abundances occurred slightly earlier in summer with further declining abundances of diatoms and cladocerans according to shatwell and köhler 2019 din concentrations in lake müggelsee were consistently low from june to august during this study period possibly explaining the positive response of cyanobacteria in summer to din enrichment these results agree with pearl et al 2016 that in the light of rising anthropogenic n and p loads instead of considering traditionally p as the only driver of cyanobacteria growth the combined n and p enrichment needs to be taken into account diatoms responded to the scenario warming and n p enrichment primarily by declined summer peaks however cyanobacteria showed higher but delayed peaks in spring and strongly increased summer peaks occurring 12 days earlier the increased abundance of cyanobacteria in spring corresponds with findings by shatwell et al 2008 that filamentous cyanobacteria are favoured by warming over diatoms cladocerans developed much higher peaks in spring and with a delay by 35 days in summer this significant gap of 47 days between summer peaks of cyanobacteria and cladocerans is another indication of likely phenological asynchrony within the plankton community of lake müggelsee under the influence of global warming and eutrophication results of the scenario warming and n p reduction suggested slightly delayed but strongly declined spring and summer peaks of diatoms but higher and delayed spring peaks and earlier but almost unchanged summer peaks for cyanobacteria these results demonstrate that gradually decreasing phosphorus and nitrogen concentrations may cause a trade off with the water temperature driven cyanobacteria growth these results correspond with findings by stich and brinker 2010 who concluded that oligotrophication effects are likely to outweigh climate effects on phytoplankton productivity in the foreseeable future 5 conclusions inferential modelling by means of evolutionary algorithms offers a vital alternative to process based modelling in limnology evolutionary algorithms induce multivariate condition action rules from complex data patterns following the cognitive principles of generative creation and choices over open ended possibilities holland et al 1986 the fittest models evolved by the hybrid evolutionary algorithm hea simulated seasonal and inter annual patterns in plankton dynamics concisely and quantitatively and are as empirical and simple as anticipated by peters 1986 as requirements for predictive limnology following conclusions can be drawn from this study 1 the ensemble of complementary inferential models proved to be predictive for plankton community dynamics of lake müggelsee across two decades 2 simulating global change scenarios as events fully imposed to all years at once affects results by cumulated artefacts simulating scenarios as gradual processes avoids such artefacts resulting in more realistic forecasts for consecutive years 3 the scenario analysis has revealed that warming of the lake by 2 per year decreased spring and summer maxima of diatoms advanced higher spring and summer maxima of cyanobacteria delayed higher summer maxima of cladocerans and delayed summer maxima of copepods corresponding well with findings from retrospective data analysis of lake müggelsee by adrian et al 2006 warming and n p enrichment suggested that the gap between increased summer peaks of cyanobacteria and cladocerans to be widened to 47 days causing severe phenological asynchrony within the plankton community warming and n p reduction increased spring maxima but kept summer peaks of cyanobacteria almost unchanged demonstrating that gradually decreasing phosphorus and nitrogen concentrations may trade off effects of water temperature driven cyanobacteria growth 4 a comparison of phenological sensitivities derived from the warming din and p enrichment scenarios suggested that cyanobacteria tend to have delayed but higher spring peaks and earlier and higher summer peaks whilst cladocerans are likely to have delayed summer peaks to be higher in response to warming and p enrichment but slightly lower to din enrichment these results gave further evidence of distinct phenological asynchrony in the plankton community of lake müggelsee under the influence of warming and eutrophication the fact that copepods displayed only minor phenological sensitivities in summer 0 2 days c and 0 25 days per μg tp l 1 confirms their resilience to habitat changes due to changing life stages and species dominance throughout the years e g adrian et al 2006 future work aims to extend the ensemble of inferential models by endogenous drivers reflecting competition and predation between the phyto and zooplankton groups another ongoing task is to verify outcomes of this research by applying the proposed modelling approach to a variety of lakes with different climates and trophic states declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful to steven c maberly for his valuable comments on the first draft of the manuscript we are also thankful to the leibniz institute of freshwater ecology and inland fishery berlin for providing 22 years of unique limnological time series of lake müggelsee we acknowledge helgard täuscher katrin preuβ phytoplankton and renate rusche and uschi newen zooplankton for taxonomic analysis this work was partially supported by jst sicosrp grant number jpmjsc1804 
25706,in this paper two emerging strategies for the reduction of the computational time of 2d large scale flood simulations are compared with the aim of evaluating their strengths and limitations and of suggesting guidelines for their effective application the analysis is based on two state of the art raster flood models with different governing equations and parallelization strategies parflood a gpu accelerated code that solves the fully dynamic shallow water equations and lisflood fp which combines a parallel implementation for cpu with simplified equations local inertial approximation the results of two case studies a river flood propagation and a lowland inundation suggest that at coarse grid resolutions the parallelized simplified model lisflood fp can represent a good alternative to fully dynamic models in terms of accuracy and runtime while the gpu parallel code parflood is more efficient in case of high resolution simulations with millions of cells despite the greater complexity of the numerical scheme keywords 2d inundation models shallow water equations local inertial approximation model benchmarking hydraulic simulations parallel computations 1 introduction floods have the potential to cause huge economic losses and casualties and future projections on climate change and on the settlement of people and assets in flood prone areas indicate that flood losses are expected to increase in the next decades winsemius et al 2016 dottori et al 2018b for this reason most countries are developing flood risk management strategies to prevent or mitigate the adverse impacts of floods on the communities klijn et al 2008 these plans must be based on flood hazard and flood risk maps de moel et al 2009 as demanded for example by the european legislation european commission 2007 in this context numerical models represent an essential tool for flood hazard assessment even though the large number of available approaches teng et al 2017 makes the selection of the most appropriate model for each application a non trivial task for large scale studies simplified conceptual models e g lhomme et al 2008 nobre et al 2016 also called low complexity methods are sometimes preferred over physically based approaches for flood mapping especially for data scarce regions samela et al 2017 tavares da costa et al 2020 even though these methods are only based on geomorphological indicators e g nardi et al 2006 manfreda et al 2014 zheng et al 2018 tavares da costa et al 2019 or represent the underlying physical process of flooding in a very simplified way e g dottori et al 2018a teng et al 2019 they are able to provide inundation extents that are reasonably comparable to those obtained from hydraulic studies falter et al 2013 afshari et al 2018 in a fraction of the computational time néelz and pender 2013 however only physically based numerical models can provide detailed flow data néelz and pender 2013 wing et al 2019a i e all flow variables depths and velocities but also derived physical quantity e g froude number hydraulic thrust etc at all locations and also their variation in time i e flow dynamics including the flood arrival time all these data are required not only for flood hazard assessment but also for other applications such as the evaluation of flood protection strategies e g luke et al 2015 schubert et al 2017 and emergency planning and civil protection activities e g arrighi et al 2019 amadio et al 2019 ferrari et al 2020 one dimensional 1d hydrodynamic models e g brunner 2016 dhi 2015 are still widely employed for river studies e g horritt and bates 2002 paz et al 2010 schumann et al 2010 ali et al 2015 their computational requirement is usually limited but these models may cause some inaccuracies in representing the actual flow field outside the main channel i e when floodplains are inundated or when the design return period of structural flood defense systems e g river levees is exceeded and lowland areas are flooded for these applications the 1d model for the river is often combined with a two dimensional 2d model for the floodplains tayefi et al 2007 bladé et al 2012 morales hernández et al 2013 ahmadian et al 2018 especially when levee breach inundations need to be modelled vorogushyn et al 2010 pinter et al 2016 d oria et al 2019 alternatively fully 2d models e g galland et al 1991 alcrudo and garcia navarro 1993 aureli et al 2008 sanders et al 2010 bates et al 2010 can be used for simulating the whole domain until recently the limiting factors for 2d models were the common unavailability of digital terrain models dtms for geographically large areas and most prominently the insufficient computational power to run fully 2d simulations on large domains nevertheless high resolution terrain data are now increasingly available thanks to remote sensing techniques in particular airborne light detection and ranging lidar surveys can provide dtms with spatial resolution up to 0 5 1 m and vertical accuracy between 0 05 and 0 15 m di baldassarre and uhlenbrook 2012 and now represent the main topographic source for hydraulic models marks and bates 2000 sanders 2007 in data sparse regions digital elevation models with lower resolution and accuracy can be obtained from open access global data sets e g sanders 2007 yan et al 2013 courty et al 2019 as regards the prohibitive runtimes of 2d simulations different strategies have been adopted for speeding up the computations for large scale applications first low resolution meshes obtained from the aggregation of the original lidar data are often used typical cell sizes are in the order of 25 100 m aureli and mignosa 2004 vorogushyn et al 2010 falter et al 2013 jarihani et al 2015 morsy et al 2018 though some studies underline the importance of including relevant terrain elements e g levees embankments channels etc to predict the flooding dynamics more accurately e g vacondio et al 2016 wing et al 2019b automatic tools for the extraction of terrain features e g sofia et al 2014 sangiretti et al 2016 or sub grid models e g yu and lane 2006 neal et al 2012a can be exploited to counterbalance the loss of topographic details in low resolution meshes another approach developed for reducing the computational burden of 2d models is the simplification of the governing equations free surface flows are described by depth averaged mass and momentum conservation laws the shallow water equations swes toro 2001 neglecting one or more terms in the momentum equation leads to models with decreasing levels of complexity in particular the diffusive formulation cunge et al 1980 cancels both inertial terms local and convective acceleration and has the advantage of allowing simple solution and implementation compared to a fully dynamic model prestininzi 2008 aricò et al 2011 however some limitations have been identified applicability only in cases with slowly varying flow néelz and pender 2013 difficulty in simulating processes where inertia plays a key role e g flow over a bump neal et al 2012b or where highly unsteady and transcritical flows are expected e g dam breaks and urban inundations hunter et al 2008 costabile et al 2017 2020a and requirement of a small time step for stability hunter et al 2008 bates et al 2010 for these reasons less simplified formulations were later proposed the local inertial approximation aronica et al 1998 bates et al 2010 martins et al 2015 only neglects the convective acceleration term in the momentum equation and allows a larger time step for stability compared to a diffusive model while maintaining its numerical simplicity bates et al 2010 this kind of model is widely used for large scale studies neal et al 2012a falter et al 2013 savage et al 2016a wing et al 2017 though some problems are reported for wet dry fronts cozzolino et al 2019 for low friction values and high froude numbers bates et al 2010 de almeida and bates 2013 and for processes involving hydraulic jumps rapidly varying flows etc neal et al 2012b in all these cases models based on the fully dynamic swes are necessary these latter models may be characterized by different numerical discretization methods finite difference finite volumes finite elements implicit or explicit and grid types structured unstructured flexible teng et al 2017 but the importance of using shock capturing schemes is widely acknowledged néelz and pender 2013 kvočka et al 2015 besides or in addition to simplifying the governing equations or using coarse meshes the best strategy for reducing the computational burden is undoubtedly the use of parallelized codes which can exploit multi core processors clusters for high performance computing hpc or graphic processing units gpu devices examples of different parallelization techniques applied to 2d hydraulic models can be found in neal et al 2009 2010 sanders et al 2010 2019 lacasta et al 2014 vacondio et al 2014 and morsy et al 2018 the continuous improvement in computer hardware and the increasing trend in accessibility to hpc facilities or cloud computing services make the use of large scale fully 2d models much more affordable nowadays in this context benchmarking studies are becoming crucial to provide insight into the strengths and limitations of the available models and to guide practitioners into the choice of the most appropriate approach previous works on the comparison of 2d models mainly focused on synthetic cases or small scale applications e g neal et al 2012b néelz and pender 2013 willis et al 2019 while more comprehensive analyses were carried out for urban flooding hunter et al 2008 costabile et al 2020a and rainfall runoff modelling e g cea et al 2010 costabile et al 2012 2020b caviedes voullième et al 2020 however studies on highly detailed large scale flood modelling using fully 2d codes are limited in this work we evaluate the efficiency and accuracy of 2d large scale simulations based on high resolution lidar terrain data in order to meet the modern tendencies of hydraulic modelling we compare the capabilities of two state of the art raster flood models namely lisflood fp bates et al 2010 and parflood vacondio et al 2014 which are characterized by different governing equations numerical schematizations and parallelization strategies in particular lisflood fp exploits the local inertial approximation for simplifying the numerical scheme and a shared memory parallel implementation for cpu while parflood is a gpu accelerated code that solves the fully dynamic swes with an accurate shock capturing scheme the comparison is based on two real case studies which were identified as suitable for both models i e we deliberately excluded applications outside the range of applicability of simplified equations the paper is aimed at outlining the main differences between the two models discussing factors influencing their accuracy and runtime e g grid resolution and providing potential users with guidelines on the most viable and fruitful implementation strategies and settings the paper is structured as follows we first briefly describe the two models and discuss their similarities and differences section 2 then the two test cases and the model setup are presented section 3 the main simulation results are reported and discussed in section 4 while conclusions are drawn in the last section 2 model description 2 1 lisflood fp lisflood fp is an inundation model that has been developed for research purposes at the university of bristol united kingdom it is specifically designed for large scale applications and has been tested on various scales including the continental and global scales savage et al 2016b schumann et al 2016 in recent years the code has been updated and the current version and the version used in this paper applies the local inertial formulation of the swes bates et al 2010 such simplification allows for numerically stable solutions for subcritical flow conditions the 2d model used in this work operates on a staggered cartesian grid and applies an explicit finite difference scheme the unit flow q between two cells is calculated by the following form of momentum equation 1 q t δ t q t g h t δ t δ h t z δ x 1 g h t δ t n 2 q t h t 10 3 where t and t δt indicate the current and next time step respectively g is the acceleration due to the gravity h is the depth n is manning s roughness coefficient δx is the cell resolution z is the cell elevation δt is the time step and h t is the difference between highest bed elevation and highest water surface elevation between two cells the equation is coupled in x and y direction four directional and then the continuity equation is used to update the water depth at each time step 2 δ h i j δ t q x i 1 j q x i j q y i j 1 q y i j δ x where i and j are the coordinates of a cell coulthard et al 2013 the full functionality of the code includes a sub grid version 1d representation of the channel flow not used herein and may include rainfall evaporation runoff grid as well as levee breach and dam break sub routines shustikova et al 2020 moreover it allows to count for certain hydraulic structures for more details the interested readers are referred to the lisflood fp user manual bates et al 2013 the code is parallelized and can be run on multiple cores on shared memory systems neal et al 2010 the stability of the model is secured by the adaptive time step which is derived from the courant friedrichs lewy cfl condition and described in bates et al 2010 the time step for solution updating is calculated in the code as 3 δ t α δ x g h m a x where α is a coefficient ranging from 0 2 to 0 7 which ensures the numerical stability for floodplain flows coulthard et al 2013 in this work the default value i e 0 7 was adopted 2 2 parflood parflood vacondio et al 2014 2017 a research code developed at the university of parma italy exploits an explicit finite volume fv scheme to solve the complete 2d swe written in conservative form toro 2001 4 u t f x g y s 0 s f where the vector of conserved variables u the fluxes in the x and y directions f and g and the bed and friction slope source terms s 0 and s f are expressed using the well balanced formulation of liang and marche 2009 5 u η u h v h t f u h u 2 h 1 2 g η 2 2 η z u v h t g v h u v h v 2 h 1 2 g η 2 2 η z t s 0 0 g η z x g η z y t s f 0 g h n 2 u u 2 v 2 h 4 3 g h n 2 v u 2 v 2 h 4 3 t in which η h z is the water surface elevation relative to the mean sea level the ground elevation z is fixed and u and v are the velocity components along x and y respectively the partial differential equations eq 4 are solved on a structured grid using a fv scheme that is explicit and second order accurate in space and time thanks to a depth positive muscl reconstruction toro 2001 and to the adoption of the second order runge kutta method to advance the solution in time 6 u i j t δ t u i j t 1 2 δ t d i u i j t d i u i j t δ t 2 where u i j t δt 2 is evaluated as follows 7 u i j t δ t 2 u i j t δ t d i u i j t while the operator d i u i j is defined as 8 d i u i j t f i 1 2 j f i 1 2 j δ x g i j 1 2 g i j 1 2 δ y s 0 s f where δx and δy are the grid sizes in the x and y directions respectively and the time step δt is calculated according to the cfl stability condition e g toro 2001 9 δ t 1 2 c r min δ x u g h δ y v g h where cr is the courant number 1 in this work cr was assumed equal to 0 8 in eq 8 fluxes are computed using the hllc approximate riemann solver toro 2001 the bed slope source term is discretized with a centered approximation while for the friction source term an implicit formulation is adopted moreover in order to avoid non physical flow velocities at the wet dry front the correction proposed by kurganov and petrova 2007 is introduced the domain can be discretized adopting either a cartesian or a non uniform structured grid named block uniform quadtree buq grid vacondio et al 2017 but in this work only uniform meshes are used for compatibility with lisflood fp a first order accurate version of the code is also available but not used herein in order to reduce the high computational cost of this accurate and robust numerical scheme the code is efficiently implemented in c and compute unified device architecture cuda languages basically the cuda framework introduced by nvidia allows offloading the intensive computations on the gpu and exploiting its intrinsic parallelization capabilities while the cpu only controls the execution flow and the time advancement of the simulation for more details the reader is referred to vacondio et al 2014 the model also features extensions to include levee dam breaches dazzi et al 2019 and hydraulic structures dazzi et al 2020 previous applications to real flood simulations showed the good computational performance of the code even for domains with several million cells dazzi et al 2018 ferrari et al 2020 2 3 comparison of model features table 1 summarizes the main features of the two models used in this work please notice that lisflood fp is a package including different solvers but here we only consider the acceleration version of the model which is widely used in large scale studies as explained above the main difference between the two models is represented by the governing equations and numerical scheme adopted on the other hand both codes rely on an explicit solution method and on an adaptive time step for updating the flow variables though the time step is restricted by a different version of the cfl stability condition eq 3 for lisflood fp eq 9 for parflood both models perform computations on a raster based grid even if parflood also supports non uniform structured grids vacondio et al 2017 the grid preparation and the visualization of results can be performed directly using any gis software which is convenient considering that these research codes are not provided with a graphical user interface gui in fact simulations can only be launched from the command line simulation results include the maps of water depths and velocities at predefined time intervals and final maps with maxima or other useful information e g arrival times however the two models inherently differ in the way velocity is computed parflood updates cell centered values for the unit discharges from which the velocity magnitude can be directly obtained for each cell lisflood fp adopts a staggered grid hence unit discharge values are evaluated at cell interfaces in the x and y directions the velocity magnitude v in each cell is then retrieved as in eq 10 10 v i j m a x v i 1 2 j v i 1 2 j 2 m a x v i j 1 2 v i j 1 2 2 0 5 another important difference between the two models lies in their parallel implementation the open multiprocessing openmp application programming interface is adopted to parallelize computations in lisflood fp neal et al 2009 taking advantage of shared memory multiprocessors if available on the host machine conversely the parflood model exploits acceleration on a gpu device thanks to its cuda implementation for this reason hardware requirements are more restrictive for the latter model since a machine equipped with an nvidia gpu is mandatory the fact that the two codes run on different types of hardware makes the direct comparison on the same machine impossible the runtimes reported in this work are obtained using the following hardware unless otherwise specified lisflood fp 2 cpus having 20 real cores and hyperthreading 40 cores parflood nvidia tesla p100 12 gb memory release year 2016 all simulations are run using double precision 3 case studies and model set up in this section the two case studies selected for the models comparison are described as already mentioned we focused on typical applications for which the choice of either fully dynamic or simplified models may be considered suitable i e gradually varied flows the first test case concerns the propagation of a severe flood wave in a large river po river italy where a fully 2d model is appropriate to predict the inundation of protected floodplains the second test case is an inundation event that followed the opening of a sudden levee breach on the secchia river italy the outflows propagated over an initially dry lowland area characterized by anthropogenic terrain features a complex network of irrigation canals and embankments local hydraulic shocks or critical flows which can occur in this complex test case are not expected to dominate the overall phenomenon thus this flood event can still be described well by the local inertial formulation of lisflood fp hunter et al 2008 neal et al 2012b 3 1 po river we first analyse a well documented flooding event which occurred on the po river italy in october 2000 this flood is considered as one of most severe in the recent decades uncommonly heavy rainfalls extended all over the upstream part of the basin where the flood caused several deaths and severe damage cassardo et al 2001 in the current study however we limit the analysis to the mid lower portion of the po river fig 1 where the flood was contained by the main embankments designed to protect against a 1 in 200 years flood during the event the floodplains behind minor embankments with a design return period of 50 years were also activated in the recent past this case study was used to setup a quasi 2d model of the po river for flood risk mitigation purposes castellarin et al 2011 domeneghetti et al 2015 the study area more than 800 km2 wide covers the 330 km long reach of the po river between ponte becca and serravalle fig 1 the domain includes the compartments i e defended floodplains inside the main levees and a 100 m wide buffer area outside the embankment fig 1 also shows a detail of the extensive system of main and minor embankments with different design return periods the 2 m resolution dtm of the area was obtained from lidar surveys performed in years 2004 2005 the bathymetric data were collected using multi beam sonars during approximately same period castellarin et al 2011 the dtm used in the study represents bare earth terrain cleared from trees and buildings with vertical accuracy of about 0 15 m we aggregated the 2 m terrain data into 30 50 and 100 m raster grids using the pixels mean value the resulting meshes included 9 105 3 105 and 9 104 cells respectively we also manually burnt in the actual height of main and minor embankments since the models adopt the same cell size as the input data different resolutions will have an impact on the storage capacity of the floodplain for example a dtm with 100 m resolution will result in 100 m wide levees which in turn reduce the potential volume between the main embankments the upstream boundary condition on the po river was set at ponte becca and the inflow from the main tributaries trebbia lambro taro and adda was also included these hydrographs were obtained from the river gauges measurements a free outflow boundary condition was set 40 km downstream from pontelagoscuro gauging station fig 1 just upstream of the po delta the simulation ends after 191 h of physical time model calibration was performed with reference to the maximum water levels along the reach altogether 171 watermark records taken from the official report delivered by the po river basin authority were available the vertical errors in such measurements are known to be up to 0 5 m dottori et al 2013 the calibration was performed separately for each numerical model considering the configuration with 30 m resolution the initial distribution of spatially variable roughness coefficients divided in two main classes channel and floodplain was taken from the work done by domeneghetti et al 2015 manning s coefficients were manually varied trying to minimize the root mean square error rmse of simulated and observed maximum water surface elevations on the 171 water marks because the flooding extent was outlined by the main embankments we additionally visually inspected the levees to verify the presence of overflows if some non negligible overflows occurred during the simulation we would disqualify the configuration we adopted the same set of roughness coefficients calibrated for the 30 m dtm also for 50 and 100 m configurations for these latter simulations we are mainly interested in a comparison between the two models runtimes hence a specific calibration was not carried out 3 2 secchia river the second case study chosen for the comparison is the levee breach induced inundation that occurred in 2014 on the secchia river a tributary of the po river the study area is shown in fig 2 a this well documented event d alpaos et al 2014 has been used in previous studies for the validation of numerical models vacondio et al 2016 for flood loss models carisi et al 2018 and for the comparison of existing hydraulic models shustikova et al 2019 during the flood event of january 19 2014 the right embankment of the secchia river collapsed post event investigations ascribed this failure to the dens of burrowing animals that most likely contributed to weakening the levee orlandini et al 2015 the breach final width was approximately 80 m and the overall flooded volume reached almost 40 mm3 in 48 h the inundation affected roughly 50 km2 of lowland area in the municipalities of modena bastiglia and bomporto and caused about 400 million euro of economic losses one casualty and the displacement of thousands of people vacondio et al 2016 the flood propagation was strongly influenced by terrain features like road embankments and minor channel levees the study area is roughly 170 km2 wide and is fully covered by a dtm of 1 m resolution obtained from a lidar survey fig 2a terrain data were then down sampled to create grids with 5 25 and 50 m resolution by taking the pixels mean value however the crest elevation of artificial embankments obtained from the original lidar dtm was enforced in low resolution cells crossed by these elements which can be thus correctly represented even adopting coarse meshes the resulting grids consist of roughly 6 8 106 3 105 and 8 104 cells for 5 m 25 m and 50 m resolutions respectively the land use of the area was analyzed on orthophotographs available from the geoportal of emilia romagna region in order to define spatially variable values for the roughness coefficient two main classes were identified urban areas including both residential and industrial land use and rural areas the remaining part of the domain which is mainly covered by agricultural fields based on the previous studies carried out by vacondio et al 2016 on the same area the value 0 143 m 1 3s for manning s coefficient was adopted for the urban areas outlined in fig 2a while three different values were investigated for the rural areas in particular 0 03 0 05 and 0 07 m 1 3s these values are in line with indications from the literature for floodplains chow et al 1988 please notice that a detailed representation of buildings in the urban areas would require a higher resolution 1 2 m i e at least three cells across each street according to gallegos et al 2009 while the strategy of imposing a high resistance in the urban area is commonly adopted for large scale modelling the discharge hydrograph flowing through the breach fig 2b was obtained from previous works vacondio et al 2016 and imposed as inflow boundary condition at the breach location the river flood was therefore not included in the simulation the simulations cover 48 h after the breach opening available data for this event include the flood arrival times at two selected locations in the study area namely the urban areas of bastiglia and bomporto and 46 high watermarks at certain points collected during post event surveys carisi et al 2018 shustikova et al 2019 both sets of data are affected by large uncertainties the accuracy of watermarks can be estimated to be around 0 5 m dottori et al 2013 while information on flood arrival times was obtained from newspaper reports hence the location is uncertain and errors can be estimated to be up to 0 5 1 h in spite of this these data can be useful to compare the overall performance of the two hydraulic models and their sensitivity to spatial resolution and roughness 4 results 4 1 po river table 2 reports the range of calibrated values for manning s coefficient for the two models and the corresponding rmse of simulated and observed high watermarks in 171 locations lisflood fp requires higher roughness values to obtain a correct prediction of the maximum water surface elevations compared to parflood in particular as regards the floodplains in general both models are able to reproduce the observed water elevation with a rmse well below the observation error the errors of individual points along the river fig 3 are in general below 50 cm except for isolated cases in a few locations and the mean error is equal to 0 03 m for parflood and to 0 04 m for lisflood fp in general there are regions in the domain where parflood overestimates and lisflood fp underestimates the maximum levels e g 0 30 km and other regions where the opposite is true e g 180 220 km the only significant trend towards over prediction for both models can be observed in the downstream part of the domain near pontelagoscuro this narrow section roughly 200 m wide represents a bottleneck which is not described well at this spatial resolution 30 m due to the fictitious enlargement of embankments wing et al 2019b the problem could have been mitigated by manually modifying the mesh in this stretch i e slightly shifting the levee locations to restore the correct floodway or even better by increasing the spatial resolution locally this possibility was not explored here because only parflood supports non uniform grids fig 4 compares the numerical results with the observed time series of water levels and discharges at three available river gauges see fig 1 for their position both models predict the water level hydrographs in the upstream gauges of cremona and borgoforte with acceptable accuracy fig 4a and b some discrepancies can be observed in the rising falling limbs for both models which can be ascribed to multiple reasons i there are uncertainties in the upstream inflow especially in the falling limb due to the conversion of recorded levels into discharges using a single valued rating curve instead of a looped one ii during the event overtopped minor embankments might have experienced breaching which could lead to a more rapid filling of the floodplain behind and this behavior was not considered in the simulations despite this the maximum level is well caught within 25 cm compared to observations and the timing of the peak is captured satisfactorily being only slightly anticipated in cremona 12 h for lisflood fp 4 h for parflood and slightly delayed in borgoforte 5 h for lisflood fp 1 h for parflood please notice that the peaks of these hydrographs are quite flat hence an uncertainty of a few hours is totally admissible downstream at pontelagoscuro gauging station fig 4c both models largely overestimate the maximum level 1 34 m for lisflood fp 0 78 m for parflood the peak is also delayed for parflood 9 h while timing is acceptable for lisflood fp 5 h as previously explained this location is a bottleneck where the mesh resolution 30 m does not provide a good representation of the area available to flow discharge time series at the same locations can be analyzed in fig 4d f to ensure a fair comparison between simulations and observations the figure also reports an uncertainty band that takes account of a 10 expected error associated to river discharges castellarin et al 2011 at cremona fig 4d both numerical discharge time series lie within this band around the peak although parflood slightly overestimates the maximum discharge 4 at borgoforte fig 4e both models predict a lower peak discharge compared to observations 8 for lisflood fp 4 for parflood finally at pontelagoscuro fig 4f despite the discussed limitations of the model at this site the peak discharge is captured satisfactorily with a tendency to overestimation for both models 9 for lisflood fp 10 for parflood overall both models replicate the attenuation of the peak discharge along the river fig 4d f fig 5 depicts the maps of maximum water depths obtained from the simulations which confirm that no overflows can be detected from the main embankments and that most dyke protected floodplains are inundated during the event and contribute to attenuating the peak flow downstream in general the differences between the two models are below 50 cm in the main channel fig 5c and no systematic bias can be identified see also fig 3 in the mid lower portion of the domain around borgoforte and downstream slightly different filling levels are predicted inside the leveed floodplains as a consequence of slightly different in channel depths but this behavior is mainly related to the severity of the event and to the uncertainty in model calibration rather than to the models characteristics compared to 1d models 2d simulations bring additional value to river flow analyses thanks to the detailed prediction of velocity fields which can be of use for the identification of critical spots e g high velocities near levees susceptible to erosion and instability however velocity measurements are not available for this event and in general for field case studies hence only an inter model comparison can be performed here fig 6 shows the maps of maximum velocities obtained from the simulations with 30 m resolution while the mean value of velocity maps is similar for the two models 0 75 m s for parflood 0 72 m s for lisflood fp the local distribution presents some differences as shown in fig 6c and d the lisflood fp simulation provides higher velocity values than parflood along the river main channel where the calibrated values of the roughness coefficient are similar for the two models see table 2 this difference is particularly noticeable up to 1 2 m s at the outside bend of meanders in locations where the largest velocities are obtained conversely in floodplains lisflood fp predicts lower velocities than parflood difference below 0 6 m s in this areas a smoother manning s coefficient was assumed in parflood simulations the possible reason for these differences is further analyzed in section 4 3 finally both models simulate very low velocities inside dyke protected floodplains however on the landside slope of overtopped minor levees see an example in fig 7 for two floodplains upstream of borgoforte parflood predicts high maximum velocities up to 3 3 5 m s and supercritical flow the froude number exceeds 1 in these spots see fig 7c as the occurrence of transitions from subcritical to supercritical flow is localized and limited in time lisflood fp provides stable results for this test case despite the local inertial approximation neal et al 2012b simulations performed with coarser mesh resolutions lead to less satisfactory results the maximum water surface elevations are overestimated compared to the observations for both models the rmse is equal to 0 68 m for parflood and to 0 39 m for lisflood fp when using the 50 m resolution mesh this suggests that the set of roughness coefficients previously calibrated for lisflood fp is still adequate to simulate the event even when the resolution is coarsened from 30 m to 50 m while for this case study parflood requires mesh dependent coefficients the largest deviations from high water marks are found downstream near pontelagoscuro here the peak level is significantly overestimated 1 53 m for lisflood fp 1 74 m for parflood for both models errors on the peak discharges at cremona borgoforte and pontelagoscuro remain within the 10 uncertainty band and the maximum velocities are similar to the configuration with 30 m differences in the order of 0 5 m s however overflows are observed in some upstream locations for both models as regards the simulations performed with the 100 m resolution mesh the capacity of the floodplain between the main embankments is reduced to the extent that the water spilled out from the floodplains in multiple locations for both resolutions the problem lies in the misrepresentation of embankments previous studies shustikova et al 2019 wing et al 2019b suggest that coarse models often misestimate flood parameters due to the difficulty of representing linear features finer than the mesh resolution overall the 100 m configuration is considered not fit for this case study and should be disqualified while the adherence to observations for the 50 m configuration is still adequate except near pontelagoscuro as regards lisflood fp and can probably be improved by calibrating a specific set of roughness coefficients for this spatial resolution as regards parflood this additional parameter tuning was not performed in this study since the aim of low resolution simulations was mainly to compare the models runtimes when coarser grids were adopted see section 4 4 4 2 secchia river overall 9 simulations were performed with each model 3 grids 3 roughness values in the following we will refer to each test configuration with a label including both the grid size 5 25 50 m and the type of roughness for the rural areas s smooth value i e 0 03 m 1 3s c central value i e 0 05 m 1 3s r rough value i e 0 07 m 1 3s for example the simulations performed with the 25 m resolution grid and manning s coefficient equal to 0 05 m 1 3s will be labelled 25 c test configurations are summarized in table 3 the capability of the parflood model of reproducing the flood evolution and inundation extent for this event was assessed in a previous study vacondio et al 2016 where the 5 m mesh and the calibrated value of 0 05 m 1 3s for the roughness coefficient in rural areas were adopted for this reason the flooded area obtained from the simulation performed with these parameters labelled 5 c is here assumed as reference i e observed for comparing the model performances of both models i e lisflood fp and parflood in terms of inundation extent by evaluating the parameter f e g bates and de roo 2000 defined as follows 11 f a s i m a r e f a s i m a r e f where a sim and a ref represent the simulated and reference flooded areas respectively fig 8 compares the maps of maximum water depths predicted by the two models for configuration 5 c the inundation extent is very similar also thanks to the partial confinement guaranteed by the panaro river levee and by other minor embankments only marginal differences can be observed especially downstream north overall the predicted flooded area is equal to 49 2 km2 according to parflood and 50 3 km2 according to lisflood fp the latter simulation gives a parameter f equal to 91 see table 3 which implies a very good correspondence with the reference parflood simulation the differences between predicted and observed water levels at 46 watermarks are also reported in fig 8 the rmse see table 3 is equal to 0 35 m for both models and indicates an acceptable agreement with observations lisflood fp performs slightly better in the focus area of bastiglia the rmse for the subset of 25 points surveyed in this area is equal to 0 26 m vs 0 31 m for parflood while the mean difference is 0 16 m for lisflood fp and 0 22 m for parflood no significant trend of under overestimation can be detected on the other hand in bomporto the other focus area both models overestimate the observed water depths the mean difference between simulated and observed levels limited to the 11 points surveyed here is 0 40 m for parflood and 0 46 m for lisflood fp in this area parflood is slightly more adherent to observations than lisflood fp since the local rmse is 0 45 m vs 0 51 m for lisflood fp it is worth noting that during the event water accumulated in bomporto and formed a lake at rest due to the local topography bounded by the levees of the panaro river and of the naviglio channel hence deviations from observations in this area can also be partly attributable to the uncertainties in the breach outflow discharge which may lead to a slight overestimation of the flooded volume table 3 reports the values of the f parameter for all the 9 simulations different grid size and roughness performed with each model at the highest resolution 5 m the overall flooded area only slightly differs from the reference simulation even when the roughness coefficient is varied as regards the parflood simulations the values of f are equal to 93 94 using either smoother or rougher values of manning s coefficient conversely for lisflood fp the best agreement is actually obtained from the simulation with the roughest coefficient 0 07 m 1 3s f 97 while the adoption of a smoother coefficient gives a less satisfactory value of 84 due to the slight overestimation of the flooded area to the north see fig 9 which compares the inundation extents obtained from different simulations with the reference one when a lower grid resolution is adopted both models slightly overestimate the flooded area fig 9 simulations with the 25 m mesh give a value for the f parameter around 82 84 for parflood and 81 82 for lisflood fp only marginally variable with the roughness coefficient similarly the f value computed for all simulations performed with the coarsest resolution 50 m is equal to 79 for parflood and to 78 for lisflood fp regardless of the assumptions on manning s coefficient the overestimation of the inundation extent at coarse resolutions is often observed in real applications savage et al 2016b and is related to the poorer terrain description moreover the inclusion of embankments which are artificially widened at low resolution may reduce the volume available for water storage wing et al 2019b similarly to the po river case study for example let us consider the area bounded by the levees of the panaro river and of the naviglio channel the western channel in fig 2 where water accumulates in a lake at rest fashion when the observed maximum water surface elevation approximately 26 m a s l is reached the volume that can be stored in this area reduces by 9 and by 20 for the 25 m and 50 m meshes respectively compared to the 5 m mesh this generates an increase in the amount of flood volume that flows downstream and consequently in the flooded area downstream however neglecting the presence of minor embankments would lead to a much less accurate prediction of the flooding dynamics and inundation extent see the results by shustikova et al 2019 the agreement with the high watermarks was also analyzed for all simulations see values of rmse in table 3 while the parflood model provides the same rmse for all the high resolution simulations 0 35 m slight differences can be observed for the lisflood fp simulations among these the best performance is obtained with the roughest coefficient 0 34 m when a coarser resolution is used the values of rmse are not particularly sensitive to roughness for both models however compared to the results of simulations with higher resolution parflood gives a slightly higher rmse 0 37 m for the 25 m mesh and 0 41 0 42 m for the 50 m mesh instead lisflood fp performs similarly rmse equal to 0 33 0 34 m when the 25 m mesh is used while the rmse increases to 0 39 0 40 m with the 50 m mesh in all cases the rmses for both models are within the data observation error 0 5 m the maps of flood arrival times were also obtained from all simulations even if fig 10 reports only 5 test configurations for each model moreover the extent of the flooded area at 12 24 36 and 48 h is plotted in fig 10c d for these tests as expected the roughness coefficient adopted to describe the rural areas remarkably affects the flood propagation and the flooded area at a fixed time increases when manning s coefficient is reduced i e the inundation propagates faster this is true for both models however as regards the high resolution mesh simulations performed with lisflood fp are slightly more influenced by roughness compared to those performed with parflood especially upstream of bomporto comparable maps of flood arrival times are obtained from simulation 5 c with parflood and 5 r with lisflood fp this means that the flood arrival time at a given point is larger for parflood than for lisflood fp when the same roughness coefficient is adopted faster arrival times in a lisflood fp simulation with the local inertial approximation than in a fully swe simulation are also reported by neal et al 2012b for a dam break case this fact was related to the adoption of decoupled equations in the x and y direction which leads to the creation of preferential flow pathways along the diagonal as discussed by neal et al 2012b and more recently by cozzolino et al 2021 to numerical diffusion and to a lesser extent to the wetting and drying treatment when the resolution is coarsened both models provide arrival times that are quite similar to those obtained with the high resolution mesh up to 12 h whereas larger differences can be observed between the results of high and low resolution simulations in the 24 48 h time interval finally simulations performed with the 25 m and 50 m meshes adopting the same model and same roughness coefficient provide comparable maps of flood arrival times for both lisflood fp and parflood besides the comparison between the two models and between different configurations simulated arrival times can also be compared with the observed arrival times at two locations the urban areas of bastiglia and bomporto see locations in fig 10a b the difference between simulated and observed arrival time for each model and test configuration is represented in fig 11 please notice that given the large uncertainty in the observed values up to 0 5 1 h simulated values were rounded to quarters for the comparison regardless of the resolution parflood predicts the flood arrival in bastiglia quite well using the central value of the roughness coefficient 0 05 m 1 3s although the error is acceptable below 1 h for all configurations conversely lisflood fp provides better results in bastiglia with the roughest value 0 07 m 1 3s in this case the flood arrival time is anticipated up to 1 5 h when the smoothest value of manning s coefficient is adopted the inundation of bastiglia occurred only 7 h after the breach opening without any significant obstacle to slow down the flooding in bomporto instead the inundation arrived after approximately 27 h from the breach opening thanks to the obstruction to flood propagation provided by the levees of minor channels which were eventually circumvented by water this complexity in the flood dynamics affects the numerical results as shown by their higher sensitivity to roughness and grid size as regards the arrival times in bomporto fig 11b at high resolution lisflood fp anticipates the flood arrival time for all roughness coefficients and the best performance is obtained for the roughest value 0 07 m 1 3s though the error is 1 5 h conversely parflood correctly predicts the inundation of bomporto using the central value 0 05 m 1 3s while errors are up to 1 5 h if different values are used at lower resolutions lisflood fp predicts the correct arrival time using the central value while parflood exhibits a slightly larger error up to 1 25 h however the range of flood arrival times using different roughness coefficients is much larger for lisflood fp about 5 h than for parflood about 3 3 5 h results in bomporto especially for lisflood fp are somehow in contrast with the general expectation that the use of fine resolution grids produces later arrival times compared to coarser grids as a result of a more accurate terrain description however in this case study the incorporation of the main embankments in the mesh guarantees the correct representation of the topography and of the inundation dynamics for all grid sizes so this tendency may not be detected overall with equal roughness coefficient parflood provides similar arrival times throughout different resolutions while lisflood fp is slightly more sensitive to grid size switching from high to low resolution however simulations performed with coarse meshes 25 m and 50 m produce almost identical flood arrival times as regards the sensitivity to the values of the roughness coefficient for rural areas similar trends are observed in bastiglia range of about 1 5 h regardless of the grid size for both models whereas simulated arrival times in bomporto are more influenced by this parameter for all resolutions especially for lisflood fp results presented so far suggest that the configurations that best fit the observations are 5 c for parflood and 5 r for lisflood fp however the value of manning s coefficient that guarantees the best prediction for parflood is the same 0 05 m 1 3s for all resolutions while lisflood fp switches from the roughest one 0 07 m 1 3s for the high resolution mesh to the central one for the coarser grids in flood hazard mapping another important variable to be considered is the flow velocity which has consequences on the safety of pedestrians and vehicles e g arrighi et al 2019 and is an important component of flood damage models merz et al 2013 only an inter model comparison is performed here since velocity measurements are obviously unavailable for this event the maps of maximum flow velocities for selected configurations are reported in fig 12 overall velocity values remain well below 1 m s in most of the domain except for local spots where values up to 1 5 2 m s are obtained especially in simulations with low roughness moreover both models predict high velocities up to 3 7 m s depending on the model and configuration at the breach location here inter model differences can also be ascribed to the implementation of boundary conditions in the two codes please notice that the color ramp in fig 12 is saturated at 1 5 m s in order to ease the visual comparison of maps the inter model comparison of these maps somehow confirms the trend that could be anticipated from the analysis of flood arrival times that is other things being equal lisflood fp predicts higher flow velocities than parflood for example in the base configuration 5 c the mean value for the maximum velocity in the flooded cells is equal to 0 29 m s for lisflood fp and to 0 19 m s for parflood while the median values are 0 23 and 0 15 m s respectively the velocity value of 0 4 m s is exceeded by only 10 of the flooded cells in the domain for parflood against 25 for lisflood fp similar trends can be observed for other high resolution simulations although differences in median values are slightly larger for configuration 5 s 0 28 m s for lisflood fp and 0 17 m s for parflood and slightly lower for configuration 5 r 0 2 m s for lisflood fp and 0 13 m s for parflood as expected if we compare the best fit simulations lisflood fp configuration 5 r still predicts higher maximum velocities than parflood configuration 5 c the mean values are equal to 0 25 m s for the former model and to 0 19 m s for the latter while the velocity of 0 48 m s is exceeded only by 5 of parflood flooded cells vs 10 for lisflood fp at low resolutions 25 c and 50 c maximum velocities are slightly lower than those of the high resolution simulation 5 c performed with the same model for example the mean and median values are 16 and 13 lower respectively for both models however local high values of maximum velocity are remarkably reduced especially for lisflood fp when the same model is used simulations with 25 m and 50 m provide very similar overall results at coarse resolution lisflood fp still predicts slightly higher maximum velocities than parflood with equal roughness coefficient the difference between the maximum velocities predicted by the two models can be better observed in fig 13 which zooms on the portion of the domain around bastiglia inside the urban area represented by means of a higher roughness coefficient than the one adopted for rural areas velocities are slightly lower than outside and this is particularly noticeable for lisflood fp results an interesting observation is the fact that at high resolution the maximum velocity predicted by lisflood fp seems influenced by micro topographic features in the terrain such as small ditches and furrows while parflood provides slightly more uniform results when the roughness is increased this effect can still be observed even if the velocities are lower see for example configuration 5 r when the resolution is reduced as for example for configuration 25 c this behavior can no longer be clearly noticed this particular velocity pattern which is only evident in lisflood fp results can partly be ascribed to the different way in which velocity is computed for the two models see section 2 3 but another possible reason is analyzed in section 4 3 4 3 convective acceleration analysis we performed an additional analysis concerning the relative importance of the different terms of the swes for the two previous case studies for this purpose the momentum equations were re written in 2d non conservative form as follows 12 1 g u t u g u x v g u y u g v y η x s f x 0 1 g v t v g u x u g v x v g v y η y s f y 0 i i i i i i i v in this non dimensional form each term represents a slope cunge et al 1980 i e i local acceleration ii convective acceleration iii free surface slope and iv friction slope the first two terms are the inertial terms or acceleration slopes please note that the second term is neglected in the lisflood fp approximation numerical results provided by parflood which solves the fully dynamic swes were elaborated to compute the derivatives and consequently the absolute values of the four terms in eq 12 fig 14 represents the order of magnitude of all terms both x and y directions along the po river reach near borgoforte gauging station approximately at the time of the peak the local acceleration is negligible compared to the other terms below 10 6 10 5 almost everywhere see fig 14a d the free surface slope fig 14c f is certainly the most significant term in the whole reach followed by the friction slope fig 14g h this means that the local inertial or even diffusive approximation can be considered suitable for modelling this event however fig 14b e shows that even at the relatively large resolution 30 m adopted the convective acceleration term is not completely negligible along the main channel especially downstream where the river width decreases and the path is more winding here the spatial variation of the velocity head is fostered by differences in flow direction bed elevation and roughness while the global effect induced by the convective acceleration term on flood propagation and maximum levels can be somehow compensated in the simplified model lisflood fp by adjusting the roughness coefficients neglecting its importance probably leads to the observed differences in the velocity distribution in the main channel compared to a fully dynamic model see fig 6 on the other hand this term does not influence the inundation of dyke protected floodplains dominated by the free surface slope which can be over 10 2 during the overtopping of minor embankments a similar analysis was repeated for the secchia case study configurations 5 c and 25 c fig 15 shows the maps of the convective acceleration free surface and friction slopes along the x and y directions obtained from the parflood simulations 8 h after the breach opening when flooding had just reached the urban area of bastiglia note that local acceleration is negligible compared to the other terms and is not reported at the highest resolution fig 15a the free surface and friction slopes terms dominate the phenomenon while convective acceleration is approximately one order of magnitude lower being relevant only in few cells characterized by large velocity variations due to micro topography when the mesh size increases fig 15b term ii of eq 12 becomes negligible compared to terms iii and iv in the whole domain this confirms the suitability of the local inertial approximation when a coarse grid is adopted while the effects of micro topography at high resolution might require the use of fully dynamic equations for obtaining an accurate estimation of the velocity distribution indeed the fact that lisflood fp neglects the convective acceleration probably determines the difference shown in fig 13 4 4 computational times time step size and mass conservation mass conservation represents a critical point for the numerical simulation of flood propagation over an initially dry bathymetry such as the secchia case we analyzed the mass balance guaranteed by the two models for this test case with only one inflow boundary condition for configurations 5 c 25 c and 50 c the mass errors for this subset of simulations are negligible compared to the total inflow the relative error is below 0 005 for all simulations since large mass conservation errors represent an indirect indicator of instabilities for lisflood fp neal et al 2012b these data also confirm that the model remains stable throughout the simulation despite critical flows can be expected somewhere as mentioned earlier the two models are characterized by different formulations of the cfl stability condition for determining the time step size during the simulations table 4 compares the average time step size for a subset of simulations as expected for both models the allowable time step increases with the grid size because of the linear dependency between these two parameters see eqs 3 and 9 however when the same grid resolution is used time steps of lisflood fp are 1 5 2 times larger than those of parflood the cfl condition of the former model states the inverse proportionality of the allowable time step with the celerity of gravity waves g h while the flow velocity also contributes to restrict the time step for the latter model for both models the po river test case requires a smaller time step for stability compared to the secchia test case when a similar grid size is adopted because of the higher water levels predicted during the simulations the computational times required by the two models for different simulations are summarized in table 5 together with the values of the ratio of physical to computational time r t the physical duration of the events here simulated is equal to 191 h and 48 h for the po river and secchia river test cases respectively as regards the secchia test case only runtimes for configurations 5 c 25 c and 50 c are reported since similar runtimes were obtained for simulations with different roughness coefficient and equal grid size for the simulations with the lowest resolution grid domains with less than 105 cells lisflood fp runs slightly faster than parflood runtimes vary between less than 1 min to a few minutes actually it is well known that the computational capabilities of gpu accelerated models like parflood are not fully exploited when a relatively low number of cells is processed due to the impossibility of masking the high memory latencies that characterize the gpus vacondio et al 2017 however as the number of cells in the domain increases parflood tends to outperform lisflood fp despite the smaller allowable time step table 4 and the more complex numerical scheme which requires more floating point operations for a given number of cells for the simulations with the highest spatial resolution the former model is 2 3 times faster than the latter with runtimes ranging from 1 5 1 8 h for parflood to almost 4 h for lisflood fp incidentally rather than by the total number of cells in the domain runtimes are influenced by the number of wet cells in the simulation in fact both models are designed to skip the updating of dry cells for the po river test case almost 90 of the domain is wet while flooded cells are only up to 30 for the secchia test case together with the larger allowable time step for stability as previously discussed this contributes to obtain larger values of r t for the latter test case than the former as regards simulations characterized by roughly the same total number of cells a direct comparison of the computational times of the two models using the same workstation cluster is impossible given the intrinsically different hardware requirements in this work simulations were run on high end computational resources however the reported runtimes can obviously change depending on the type of processor and number of cores for lisflood fp and on the gpu device for parflood just to give an idea previous works neal et al 2018 report that the computational times of lisflood fp simulations performed on 16 cores can be 4 8 times faster for 2d real test cases with 104 105 cells and up to 10 times faster for analytical test cases with 106 cells compared to the same runs on a single core here we are using 40 cores hence the runtimes would largely increase if a more limited number of cores were available for computation similarly parflood simulations can require a longer runtime when a more outdated gpu is used for example on a nvidia k40 gpu released in 2013 the po 50 m test runs in 83 min and test secchia 5 c runs in 422 min on the other hand when a more modern video card nvidia v100 gpu released in 2017 is used the runtimes of parflood can be reduced by roughly 25 30 e g test case po 50 m only takes 17 min to run while test secchia 5 c takes 78 min one final remark must be dedicated to the floating point precision assumed for the computations in this work all simulations were run in double precision mode in order to minimize the accumulation of numerical truncation errors which may generate non negligible mass errors this is nowadays the default choice for most cpu systems while gpu processors were originally developed and optimized for single precision computations which can be up to eight times faster than double precision depending on the device morales hernández et al 2020 in this work we verified that the runtime of single precision simulations with parflood is roughly halved compared to double precision on modern gpus for example the secchia 5 c simulation runs in 55 min on the nvidia p100 gpu and in only 36 min on the v100 gpu results are not deeply affected and the total mass error remains acceptable below 0 06 of the total inflow hence gpu accelerated models can be considered even more competitive for applications for which single precision can be adopted 5 discussion and conclusions this work aimed at comparing two modelling strategies that have recently emerged for performing 2d large scale flood simulations the first one implemented in the lisflood fp code combines a simplification of the swes i e local inertial approximation and a parallelization based on openmp the second approach adopted by the parflood code exploits the computational power of gpu devices to reduce runtimes while solving the fully dynamic swes two case studies in italy were selected for the comparison once calibrated the two models perform similarly as regards the prediction of the inundated area and maximum water levels this is in line with previous benchmarking studies e g néelz and pender 2013 the 2000 flood event on the po river is reasonably reproduced by both models the lowland inundation due to the secchia levee breach is also predicted quite well the two models however require quite different values of the roughness coefficients to obtain the best results for both test cases in particular the calibrated values for lisflood fp are higher than the ones of parflood especially for the po river floodplains roughness also influences the flood arrival times and maximum velocities which is evident from the results of the secchia test case fig 11 where again lisflood fp requires higher values of manning s coefficient to predict the correct flood arrival times compared to parflood in case of a high resolution mesh these results confirm the importance of the calibration phase if possible i e if observed data are available for both models and suggest that the direct adoption of any literature value based on the land use of the area is not recommended in general since the best values may be model mesh and case dependent indeed roughness coefficients somehow compensate for possible inaccuracies in the terrain description especially at coarse resolution and in the case of simplified models like lisflood fp also for the reduced complexity of the governing equations the analysis reported in section 4 3 shows that the convective acceleration term of the swes can be locally non negligible even for case studies involving gradually varied flows and this fact probably needs to be counterbalanced by a higher friction term for lisflood fp compared to parflood when the importance of inertial terms decreases e g for the secchia case study at coarse grid sizes lisflood fp guarantees a good adherence to observations even when lower roughness values are considered incidentally no preliminary evaluations concerning the applicability of simplified equations to the specific case study e g does inertia influence the hydraulic process need to be carried out if the fully dynamic equations are adopted the influence of the grid size was also analyzed for the secchia case study since the use of low resolution meshes is a common strategy to reduce computational times however coarse grids for raster based models may decrease the accuracy of the terrain representation unless suitable aggregation techniques that consider linear features are adopted in this work embankments were manually burnt into the aggregated data at all resolutions to avoid misestimating the inundated area thanks to this strategy the overall accuracy at low resolutions remains fairly good for both models though lower than the one obtained with the high resolution mesh at the same time the simulation times are largely reduced this trade off effect often allows practitioners to use a coarse grid size for practical applications in large scale domains however the possibility of using a low resolution can sometimes be hindered by particular topographic conditions in the study area an example is represented by the bottleneck of pontelagoscuro for the po river test case where water levels are largely overestimated by both models because of the artificial flow area shrinkage due to the coarse mesh finally an inter model comparison was performed based on the computational times required for each simulation runtimes obviously depend on the hardware typical hpc resources were used here but also on the total number of cells in the domain on the physical time of simulation on the percentage of wet cells and on the time step size this latter parameter is larger for lisflood fp than parflood due to the different stability condition this leads to a lower number of overall time steps for completing the simulation physical time being equal despite this for grids with a relatively low number of cells 104 105 both models perform similarly while parflood outperforms lisflood fp when the number of cells increases 106 thanks to the gpu parallelization moreover parflood s runtimes can be further reduced if first order accurate and or single precision simulations are performed additionally the upcoming new version of lisflood fp will also support gpu acceleration shaw et al 2021 and multi gpu codes with enhanced computational performance are underway e g turchetto et al 2019 morales hernández et al 2020 in the near future these extremely efficient codes are expected to become widely available and enable unprecedented hydraulic simulations in the wide context of 2d large scale hydraulic studies these considerations suggest that for simulations that can be performed at coarse resolution and with a limited number of cells parallelized simplified models like lisflood fp can provide similar results to fully dynamic models and comparable runtimes to gpu accelerated codes and represent a good alternative for applications that require a higher level of detail fine resolution meshes with millions of cells the adoption of reduced complexity models is no longer justified by their computational performance compared to gpu parallelized codes like parflood which additionally provides reliable results in all flow conditions including rapidly varying and supercritical flows thanks to its fully dynamic implementation 6 software availability the lisflood fp software is developed by the university of bristol website http www bristol ac uk geography research hydrology models lisflood the parflood code is developed by the university of parma website http www hylab unipr it it servizi numerico development of 2d parallel algorithms for flood propagation the main software specifications for both models are reported in table 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the po river basin authority for providing terrain data this research benefits from the hpc facility of the university of parma s d and r v gratefully acknowledge the support of cineca under project nemorino id hp10cr41j6 
25706,in this paper two emerging strategies for the reduction of the computational time of 2d large scale flood simulations are compared with the aim of evaluating their strengths and limitations and of suggesting guidelines for their effective application the analysis is based on two state of the art raster flood models with different governing equations and parallelization strategies parflood a gpu accelerated code that solves the fully dynamic shallow water equations and lisflood fp which combines a parallel implementation for cpu with simplified equations local inertial approximation the results of two case studies a river flood propagation and a lowland inundation suggest that at coarse grid resolutions the parallelized simplified model lisflood fp can represent a good alternative to fully dynamic models in terms of accuracy and runtime while the gpu parallel code parflood is more efficient in case of high resolution simulations with millions of cells despite the greater complexity of the numerical scheme keywords 2d inundation models shallow water equations local inertial approximation model benchmarking hydraulic simulations parallel computations 1 introduction floods have the potential to cause huge economic losses and casualties and future projections on climate change and on the settlement of people and assets in flood prone areas indicate that flood losses are expected to increase in the next decades winsemius et al 2016 dottori et al 2018b for this reason most countries are developing flood risk management strategies to prevent or mitigate the adverse impacts of floods on the communities klijn et al 2008 these plans must be based on flood hazard and flood risk maps de moel et al 2009 as demanded for example by the european legislation european commission 2007 in this context numerical models represent an essential tool for flood hazard assessment even though the large number of available approaches teng et al 2017 makes the selection of the most appropriate model for each application a non trivial task for large scale studies simplified conceptual models e g lhomme et al 2008 nobre et al 2016 also called low complexity methods are sometimes preferred over physically based approaches for flood mapping especially for data scarce regions samela et al 2017 tavares da costa et al 2020 even though these methods are only based on geomorphological indicators e g nardi et al 2006 manfreda et al 2014 zheng et al 2018 tavares da costa et al 2019 or represent the underlying physical process of flooding in a very simplified way e g dottori et al 2018a teng et al 2019 they are able to provide inundation extents that are reasonably comparable to those obtained from hydraulic studies falter et al 2013 afshari et al 2018 in a fraction of the computational time néelz and pender 2013 however only physically based numerical models can provide detailed flow data néelz and pender 2013 wing et al 2019a i e all flow variables depths and velocities but also derived physical quantity e g froude number hydraulic thrust etc at all locations and also their variation in time i e flow dynamics including the flood arrival time all these data are required not only for flood hazard assessment but also for other applications such as the evaluation of flood protection strategies e g luke et al 2015 schubert et al 2017 and emergency planning and civil protection activities e g arrighi et al 2019 amadio et al 2019 ferrari et al 2020 one dimensional 1d hydrodynamic models e g brunner 2016 dhi 2015 are still widely employed for river studies e g horritt and bates 2002 paz et al 2010 schumann et al 2010 ali et al 2015 their computational requirement is usually limited but these models may cause some inaccuracies in representing the actual flow field outside the main channel i e when floodplains are inundated or when the design return period of structural flood defense systems e g river levees is exceeded and lowland areas are flooded for these applications the 1d model for the river is often combined with a two dimensional 2d model for the floodplains tayefi et al 2007 bladé et al 2012 morales hernández et al 2013 ahmadian et al 2018 especially when levee breach inundations need to be modelled vorogushyn et al 2010 pinter et al 2016 d oria et al 2019 alternatively fully 2d models e g galland et al 1991 alcrudo and garcia navarro 1993 aureli et al 2008 sanders et al 2010 bates et al 2010 can be used for simulating the whole domain until recently the limiting factors for 2d models were the common unavailability of digital terrain models dtms for geographically large areas and most prominently the insufficient computational power to run fully 2d simulations on large domains nevertheless high resolution terrain data are now increasingly available thanks to remote sensing techniques in particular airborne light detection and ranging lidar surveys can provide dtms with spatial resolution up to 0 5 1 m and vertical accuracy between 0 05 and 0 15 m di baldassarre and uhlenbrook 2012 and now represent the main topographic source for hydraulic models marks and bates 2000 sanders 2007 in data sparse regions digital elevation models with lower resolution and accuracy can be obtained from open access global data sets e g sanders 2007 yan et al 2013 courty et al 2019 as regards the prohibitive runtimes of 2d simulations different strategies have been adopted for speeding up the computations for large scale applications first low resolution meshes obtained from the aggregation of the original lidar data are often used typical cell sizes are in the order of 25 100 m aureli and mignosa 2004 vorogushyn et al 2010 falter et al 2013 jarihani et al 2015 morsy et al 2018 though some studies underline the importance of including relevant terrain elements e g levees embankments channels etc to predict the flooding dynamics more accurately e g vacondio et al 2016 wing et al 2019b automatic tools for the extraction of terrain features e g sofia et al 2014 sangiretti et al 2016 or sub grid models e g yu and lane 2006 neal et al 2012a can be exploited to counterbalance the loss of topographic details in low resolution meshes another approach developed for reducing the computational burden of 2d models is the simplification of the governing equations free surface flows are described by depth averaged mass and momentum conservation laws the shallow water equations swes toro 2001 neglecting one or more terms in the momentum equation leads to models with decreasing levels of complexity in particular the diffusive formulation cunge et al 1980 cancels both inertial terms local and convective acceleration and has the advantage of allowing simple solution and implementation compared to a fully dynamic model prestininzi 2008 aricò et al 2011 however some limitations have been identified applicability only in cases with slowly varying flow néelz and pender 2013 difficulty in simulating processes where inertia plays a key role e g flow over a bump neal et al 2012b or where highly unsteady and transcritical flows are expected e g dam breaks and urban inundations hunter et al 2008 costabile et al 2017 2020a and requirement of a small time step for stability hunter et al 2008 bates et al 2010 for these reasons less simplified formulations were later proposed the local inertial approximation aronica et al 1998 bates et al 2010 martins et al 2015 only neglects the convective acceleration term in the momentum equation and allows a larger time step for stability compared to a diffusive model while maintaining its numerical simplicity bates et al 2010 this kind of model is widely used for large scale studies neal et al 2012a falter et al 2013 savage et al 2016a wing et al 2017 though some problems are reported for wet dry fronts cozzolino et al 2019 for low friction values and high froude numbers bates et al 2010 de almeida and bates 2013 and for processes involving hydraulic jumps rapidly varying flows etc neal et al 2012b in all these cases models based on the fully dynamic swes are necessary these latter models may be characterized by different numerical discretization methods finite difference finite volumes finite elements implicit or explicit and grid types structured unstructured flexible teng et al 2017 but the importance of using shock capturing schemes is widely acknowledged néelz and pender 2013 kvočka et al 2015 besides or in addition to simplifying the governing equations or using coarse meshes the best strategy for reducing the computational burden is undoubtedly the use of parallelized codes which can exploit multi core processors clusters for high performance computing hpc or graphic processing units gpu devices examples of different parallelization techniques applied to 2d hydraulic models can be found in neal et al 2009 2010 sanders et al 2010 2019 lacasta et al 2014 vacondio et al 2014 and morsy et al 2018 the continuous improvement in computer hardware and the increasing trend in accessibility to hpc facilities or cloud computing services make the use of large scale fully 2d models much more affordable nowadays in this context benchmarking studies are becoming crucial to provide insight into the strengths and limitations of the available models and to guide practitioners into the choice of the most appropriate approach previous works on the comparison of 2d models mainly focused on synthetic cases or small scale applications e g neal et al 2012b néelz and pender 2013 willis et al 2019 while more comprehensive analyses were carried out for urban flooding hunter et al 2008 costabile et al 2020a and rainfall runoff modelling e g cea et al 2010 costabile et al 2012 2020b caviedes voullième et al 2020 however studies on highly detailed large scale flood modelling using fully 2d codes are limited in this work we evaluate the efficiency and accuracy of 2d large scale simulations based on high resolution lidar terrain data in order to meet the modern tendencies of hydraulic modelling we compare the capabilities of two state of the art raster flood models namely lisflood fp bates et al 2010 and parflood vacondio et al 2014 which are characterized by different governing equations numerical schematizations and parallelization strategies in particular lisflood fp exploits the local inertial approximation for simplifying the numerical scheme and a shared memory parallel implementation for cpu while parflood is a gpu accelerated code that solves the fully dynamic swes with an accurate shock capturing scheme the comparison is based on two real case studies which were identified as suitable for both models i e we deliberately excluded applications outside the range of applicability of simplified equations the paper is aimed at outlining the main differences between the two models discussing factors influencing their accuracy and runtime e g grid resolution and providing potential users with guidelines on the most viable and fruitful implementation strategies and settings the paper is structured as follows we first briefly describe the two models and discuss their similarities and differences section 2 then the two test cases and the model setup are presented section 3 the main simulation results are reported and discussed in section 4 while conclusions are drawn in the last section 2 model description 2 1 lisflood fp lisflood fp is an inundation model that has been developed for research purposes at the university of bristol united kingdom it is specifically designed for large scale applications and has been tested on various scales including the continental and global scales savage et al 2016b schumann et al 2016 in recent years the code has been updated and the current version and the version used in this paper applies the local inertial formulation of the swes bates et al 2010 such simplification allows for numerically stable solutions for subcritical flow conditions the 2d model used in this work operates on a staggered cartesian grid and applies an explicit finite difference scheme the unit flow q between two cells is calculated by the following form of momentum equation 1 q t δ t q t g h t δ t δ h t z δ x 1 g h t δ t n 2 q t h t 10 3 where t and t δt indicate the current and next time step respectively g is the acceleration due to the gravity h is the depth n is manning s roughness coefficient δx is the cell resolution z is the cell elevation δt is the time step and h t is the difference between highest bed elevation and highest water surface elevation between two cells the equation is coupled in x and y direction four directional and then the continuity equation is used to update the water depth at each time step 2 δ h i j δ t q x i 1 j q x i j q y i j 1 q y i j δ x where i and j are the coordinates of a cell coulthard et al 2013 the full functionality of the code includes a sub grid version 1d representation of the channel flow not used herein and may include rainfall evaporation runoff grid as well as levee breach and dam break sub routines shustikova et al 2020 moreover it allows to count for certain hydraulic structures for more details the interested readers are referred to the lisflood fp user manual bates et al 2013 the code is parallelized and can be run on multiple cores on shared memory systems neal et al 2010 the stability of the model is secured by the adaptive time step which is derived from the courant friedrichs lewy cfl condition and described in bates et al 2010 the time step for solution updating is calculated in the code as 3 δ t α δ x g h m a x where α is a coefficient ranging from 0 2 to 0 7 which ensures the numerical stability for floodplain flows coulthard et al 2013 in this work the default value i e 0 7 was adopted 2 2 parflood parflood vacondio et al 2014 2017 a research code developed at the university of parma italy exploits an explicit finite volume fv scheme to solve the complete 2d swe written in conservative form toro 2001 4 u t f x g y s 0 s f where the vector of conserved variables u the fluxes in the x and y directions f and g and the bed and friction slope source terms s 0 and s f are expressed using the well balanced formulation of liang and marche 2009 5 u η u h v h t f u h u 2 h 1 2 g η 2 2 η z u v h t g v h u v h v 2 h 1 2 g η 2 2 η z t s 0 0 g η z x g η z y t s f 0 g h n 2 u u 2 v 2 h 4 3 g h n 2 v u 2 v 2 h 4 3 t in which η h z is the water surface elevation relative to the mean sea level the ground elevation z is fixed and u and v are the velocity components along x and y respectively the partial differential equations eq 4 are solved on a structured grid using a fv scheme that is explicit and second order accurate in space and time thanks to a depth positive muscl reconstruction toro 2001 and to the adoption of the second order runge kutta method to advance the solution in time 6 u i j t δ t u i j t 1 2 δ t d i u i j t d i u i j t δ t 2 where u i j t δt 2 is evaluated as follows 7 u i j t δ t 2 u i j t δ t d i u i j t while the operator d i u i j is defined as 8 d i u i j t f i 1 2 j f i 1 2 j δ x g i j 1 2 g i j 1 2 δ y s 0 s f where δx and δy are the grid sizes in the x and y directions respectively and the time step δt is calculated according to the cfl stability condition e g toro 2001 9 δ t 1 2 c r min δ x u g h δ y v g h where cr is the courant number 1 in this work cr was assumed equal to 0 8 in eq 8 fluxes are computed using the hllc approximate riemann solver toro 2001 the bed slope source term is discretized with a centered approximation while for the friction source term an implicit formulation is adopted moreover in order to avoid non physical flow velocities at the wet dry front the correction proposed by kurganov and petrova 2007 is introduced the domain can be discretized adopting either a cartesian or a non uniform structured grid named block uniform quadtree buq grid vacondio et al 2017 but in this work only uniform meshes are used for compatibility with lisflood fp a first order accurate version of the code is also available but not used herein in order to reduce the high computational cost of this accurate and robust numerical scheme the code is efficiently implemented in c and compute unified device architecture cuda languages basically the cuda framework introduced by nvidia allows offloading the intensive computations on the gpu and exploiting its intrinsic parallelization capabilities while the cpu only controls the execution flow and the time advancement of the simulation for more details the reader is referred to vacondio et al 2014 the model also features extensions to include levee dam breaches dazzi et al 2019 and hydraulic structures dazzi et al 2020 previous applications to real flood simulations showed the good computational performance of the code even for domains with several million cells dazzi et al 2018 ferrari et al 2020 2 3 comparison of model features table 1 summarizes the main features of the two models used in this work please notice that lisflood fp is a package including different solvers but here we only consider the acceleration version of the model which is widely used in large scale studies as explained above the main difference between the two models is represented by the governing equations and numerical scheme adopted on the other hand both codes rely on an explicit solution method and on an adaptive time step for updating the flow variables though the time step is restricted by a different version of the cfl stability condition eq 3 for lisflood fp eq 9 for parflood both models perform computations on a raster based grid even if parflood also supports non uniform structured grids vacondio et al 2017 the grid preparation and the visualization of results can be performed directly using any gis software which is convenient considering that these research codes are not provided with a graphical user interface gui in fact simulations can only be launched from the command line simulation results include the maps of water depths and velocities at predefined time intervals and final maps with maxima or other useful information e g arrival times however the two models inherently differ in the way velocity is computed parflood updates cell centered values for the unit discharges from which the velocity magnitude can be directly obtained for each cell lisflood fp adopts a staggered grid hence unit discharge values are evaluated at cell interfaces in the x and y directions the velocity magnitude v in each cell is then retrieved as in eq 10 10 v i j m a x v i 1 2 j v i 1 2 j 2 m a x v i j 1 2 v i j 1 2 2 0 5 another important difference between the two models lies in their parallel implementation the open multiprocessing openmp application programming interface is adopted to parallelize computations in lisflood fp neal et al 2009 taking advantage of shared memory multiprocessors if available on the host machine conversely the parflood model exploits acceleration on a gpu device thanks to its cuda implementation for this reason hardware requirements are more restrictive for the latter model since a machine equipped with an nvidia gpu is mandatory the fact that the two codes run on different types of hardware makes the direct comparison on the same machine impossible the runtimes reported in this work are obtained using the following hardware unless otherwise specified lisflood fp 2 cpus having 20 real cores and hyperthreading 40 cores parflood nvidia tesla p100 12 gb memory release year 2016 all simulations are run using double precision 3 case studies and model set up in this section the two case studies selected for the models comparison are described as already mentioned we focused on typical applications for which the choice of either fully dynamic or simplified models may be considered suitable i e gradually varied flows the first test case concerns the propagation of a severe flood wave in a large river po river italy where a fully 2d model is appropriate to predict the inundation of protected floodplains the second test case is an inundation event that followed the opening of a sudden levee breach on the secchia river italy the outflows propagated over an initially dry lowland area characterized by anthropogenic terrain features a complex network of irrigation canals and embankments local hydraulic shocks or critical flows which can occur in this complex test case are not expected to dominate the overall phenomenon thus this flood event can still be described well by the local inertial formulation of lisflood fp hunter et al 2008 neal et al 2012b 3 1 po river we first analyse a well documented flooding event which occurred on the po river italy in october 2000 this flood is considered as one of most severe in the recent decades uncommonly heavy rainfalls extended all over the upstream part of the basin where the flood caused several deaths and severe damage cassardo et al 2001 in the current study however we limit the analysis to the mid lower portion of the po river fig 1 where the flood was contained by the main embankments designed to protect against a 1 in 200 years flood during the event the floodplains behind minor embankments with a design return period of 50 years were also activated in the recent past this case study was used to setup a quasi 2d model of the po river for flood risk mitigation purposes castellarin et al 2011 domeneghetti et al 2015 the study area more than 800 km2 wide covers the 330 km long reach of the po river between ponte becca and serravalle fig 1 the domain includes the compartments i e defended floodplains inside the main levees and a 100 m wide buffer area outside the embankment fig 1 also shows a detail of the extensive system of main and minor embankments with different design return periods the 2 m resolution dtm of the area was obtained from lidar surveys performed in years 2004 2005 the bathymetric data were collected using multi beam sonars during approximately same period castellarin et al 2011 the dtm used in the study represents bare earth terrain cleared from trees and buildings with vertical accuracy of about 0 15 m we aggregated the 2 m terrain data into 30 50 and 100 m raster grids using the pixels mean value the resulting meshes included 9 105 3 105 and 9 104 cells respectively we also manually burnt in the actual height of main and minor embankments since the models adopt the same cell size as the input data different resolutions will have an impact on the storage capacity of the floodplain for example a dtm with 100 m resolution will result in 100 m wide levees which in turn reduce the potential volume between the main embankments the upstream boundary condition on the po river was set at ponte becca and the inflow from the main tributaries trebbia lambro taro and adda was also included these hydrographs were obtained from the river gauges measurements a free outflow boundary condition was set 40 km downstream from pontelagoscuro gauging station fig 1 just upstream of the po delta the simulation ends after 191 h of physical time model calibration was performed with reference to the maximum water levels along the reach altogether 171 watermark records taken from the official report delivered by the po river basin authority were available the vertical errors in such measurements are known to be up to 0 5 m dottori et al 2013 the calibration was performed separately for each numerical model considering the configuration with 30 m resolution the initial distribution of spatially variable roughness coefficients divided in two main classes channel and floodplain was taken from the work done by domeneghetti et al 2015 manning s coefficients were manually varied trying to minimize the root mean square error rmse of simulated and observed maximum water surface elevations on the 171 water marks because the flooding extent was outlined by the main embankments we additionally visually inspected the levees to verify the presence of overflows if some non negligible overflows occurred during the simulation we would disqualify the configuration we adopted the same set of roughness coefficients calibrated for the 30 m dtm also for 50 and 100 m configurations for these latter simulations we are mainly interested in a comparison between the two models runtimes hence a specific calibration was not carried out 3 2 secchia river the second case study chosen for the comparison is the levee breach induced inundation that occurred in 2014 on the secchia river a tributary of the po river the study area is shown in fig 2 a this well documented event d alpaos et al 2014 has been used in previous studies for the validation of numerical models vacondio et al 2016 for flood loss models carisi et al 2018 and for the comparison of existing hydraulic models shustikova et al 2019 during the flood event of january 19 2014 the right embankment of the secchia river collapsed post event investigations ascribed this failure to the dens of burrowing animals that most likely contributed to weakening the levee orlandini et al 2015 the breach final width was approximately 80 m and the overall flooded volume reached almost 40 mm3 in 48 h the inundation affected roughly 50 km2 of lowland area in the municipalities of modena bastiglia and bomporto and caused about 400 million euro of economic losses one casualty and the displacement of thousands of people vacondio et al 2016 the flood propagation was strongly influenced by terrain features like road embankments and minor channel levees the study area is roughly 170 km2 wide and is fully covered by a dtm of 1 m resolution obtained from a lidar survey fig 2a terrain data were then down sampled to create grids with 5 25 and 50 m resolution by taking the pixels mean value however the crest elevation of artificial embankments obtained from the original lidar dtm was enforced in low resolution cells crossed by these elements which can be thus correctly represented even adopting coarse meshes the resulting grids consist of roughly 6 8 106 3 105 and 8 104 cells for 5 m 25 m and 50 m resolutions respectively the land use of the area was analyzed on orthophotographs available from the geoportal of emilia romagna region in order to define spatially variable values for the roughness coefficient two main classes were identified urban areas including both residential and industrial land use and rural areas the remaining part of the domain which is mainly covered by agricultural fields based on the previous studies carried out by vacondio et al 2016 on the same area the value 0 143 m 1 3s for manning s coefficient was adopted for the urban areas outlined in fig 2a while three different values were investigated for the rural areas in particular 0 03 0 05 and 0 07 m 1 3s these values are in line with indications from the literature for floodplains chow et al 1988 please notice that a detailed representation of buildings in the urban areas would require a higher resolution 1 2 m i e at least three cells across each street according to gallegos et al 2009 while the strategy of imposing a high resistance in the urban area is commonly adopted for large scale modelling the discharge hydrograph flowing through the breach fig 2b was obtained from previous works vacondio et al 2016 and imposed as inflow boundary condition at the breach location the river flood was therefore not included in the simulation the simulations cover 48 h after the breach opening available data for this event include the flood arrival times at two selected locations in the study area namely the urban areas of bastiglia and bomporto and 46 high watermarks at certain points collected during post event surveys carisi et al 2018 shustikova et al 2019 both sets of data are affected by large uncertainties the accuracy of watermarks can be estimated to be around 0 5 m dottori et al 2013 while information on flood arrival times was obtained from newspaper reports hence the location is uncertain and errors can be estimated to be up to 0 5 1 h in spite of this these data can be useful to compare the overall performance of the two hydraulic models and their sensitivity to spatial resolution and roughness 4 results 4 1 po river table 2 reports the range of calibrated values for manning s coefficient for the two models and the corresponding rmse of simulated and observed high watermarks in 171 locations lisflood fp requires higher roughness values to obtain a correct prediction of the maximum water surface elevations compared to parflood in particular as regards the floodplains in general both models are able to reproduce the observed water elevation with a rmse well below the observation error the errors of individual points along the river fig 3 are in general below 50 cm except for isolated cases in a few locations and the mean error is equal to 0 03 m for parflood and to 0 04 m for lisflood fp in general there are regions in the domain where parflood overestimates and lisflood fp underestimates the maximum levels e g 0 30 km and other regions where the opposite is true e g 180 220 km the only significant trend towards over prediction for both models can be observed in the downstream part of the domain near pontelagoscuro this narrow section roughly 200 m wide represents a bottleneck which is not described well at this spatial resolution 30 m due to the fictitious enlargement of embankments wing et al 2019b the problem could have been mitigated by manually modifying the mesh in this stretch i e slightly shifting the levee locations to restore the correct floodway or even better by increasing the spatial resolution locally this possibility was not explored here because only parflood supports non uniform grids fig 4 compares the numerical results with the observed time series of water levels and discharges at three available river gauges see fig 1 for their position both models predict the water level hydrographs in the upstream gauges of cremona and borgoforte with acceptable accuracy fig 4a and b some discrepancies can be observed in the rising falling limbs for both models which can be ascribed to multiple reasons i there are uncertainties in the upstream inflow especially in the falling limb due to the conversion of recorded levels into discharges using a single valued rating curve instead of a looped one ii during the event overtopped minor embankments might have experienced breaching which could lead to a more rapid filling of the floodplain behind and this behavior was not considered in the simulations despite this the maximum level is well caught within 25 cm compared to observations and the timing of the peak is captured satisfactorily being only slightly anticipated in cremona 12 h for lisflood fp 4 h for parflood and slightly delayed in borgoforte 5 h for lisflood fp 1 h for parflood please notice that the peaks of these hydrographs are quite flat hence an uncertainty of a few hours is totally admissible downstream at pontelagoscuro gauging station fig 4c both models largely overestimate the maximum level 1 34 m for lisflood fp 0 78 m for parflood the peak is also delayed for parflood 9 h while timing is acceptable for lisflood fp 5 h as previously explained this location is a bottleneck where the mesh resolution 30 m does not provide a good representation of the area available to flow discharge time series at the same locations can be analyzed in fig 4d f to ensure a fair comparison between simulations and observations the figure also reports an uncertainty band that takes account of a 10 expected error associated to river discharges castellarin et al 2011 at cremona fig 4d both numerical discharge time series lie within this band around the peak although parflood slightly overestimates the maximum discharge 4 at borgoforte fig 4e both models predict a lower peak discharge compared to observations 8 for lisflood fp 4 for parflood finally at pontelagoscuro fig 4f despite the discussed limitations of the model at this site the peak discharge is captured satisfactorily with a tendency to overestimation for both models 9 for lisflood fp 10 for parflood overall both models replicate the attenuation of the peak discharge along the river fig 4d f fig 5 depicts the maps of maximum water depths obtained from the simulations which confirm that no overflows can be detected from the main embankments and that most dyke protected floodplains are inundated during the event and contribute to attenuating the peak flow downstream in general the differences between the two models are below 50 cm in the main channel fig 5c and no systematic bias can be identified see also fig 3 in the mid lower portion of the domain around borgoforte and downstream slightly different filling levels are predicted inside the leveed floodplains as a consequence of slightly different in channel depths but this behavior is mainly related to the severity of the event and to the uncertainty in model calibration rather than to the models characteristics compared to 1d models 2d simulations bring additional value to river flow analyses thanks to the detailed prediction of velocity fields which can be of use for the identification of critical spots e g high velocities near levees susceptible to erosion and instability however velocity measurements are not available for this event and in general for field case studies hence only an inter model comparison can be performed here fig 6 shows the maps of maximum velocities obtained from the simulations with 30 m resolution while the mean value of velocity maps is similar for the two models 0 75 m s for parflood 0 72 m s for lisflood fp the local distribution presents some differences as shown in fig 6c and d the lisflood fp simulation provides higher velocity values than parflood along the river main channel where the calibrated values of the roughness coefficient are similar for the two models see table 2 this difference is particularly noticeable up to 1 2 m s at the outside bend of meanders in locations where the largest velocities are obtained conversely in floodplains lisflood fp predicts lower velocities than parflood difference below 0 6 m s in this areas a smoother manning s coefficient was assumed in parflood simulations the possible reason for these differences is further analyzed in section 4 3 finally both models simulate very low velocities inside dyke protected floodplains however on the landside slope of overtopped minor levees see an example in fig 7 for two floodplains upstream of borgoforte parflood predicts high maximum velocities up to 3 3 5 m s and supercritical flow the froude number exceeds 1 in these spots see fig 7c as the occurrence of transitions from subcritical to supercritical flow is localized and limited in time lisflood fp provides stable results for this test case despite the local inertial approximation neal et al 2012b simulations performed with coarser mesh resolutions lead to less satisfactory results the maximum water surface elevations are overestimated compared to the observations for both models the rmse is equal to 0 68 m for parflood and to 0 39 m for lisflood fp when using the 50 m resolution mesh this suggests that the set of roughness coefficients previously calibrated for lisflood fp is still adequate to simulate the event even when the resolution is coarsened from 30 m to 50 m while for this case study parflood requires mesh dependent coefficients the largest deviations from high water marks are found downstream near pontelagoscuro here the peak level is significantly overestimated 1 53 m for lisflood fp 1 74 m for parflood for both models errors on the peak discharges at cremona borgoforte and pontelagoscuro remain within the 10 uncertainty band and the maximum velocities are similar to the configuration with 30 m differences in the order of 0 5 m s however overflows are observed in some upstream locations for both models as regards the simulations performed with the 100 m resolution mesh the capacity of the floodplain between the main embankments is reduced to the extent that the water spilled out from the floodplains in multiple locations for both resolutions the problem lies in the misrepresentation of embankments previous studies shustikova et al 2019 wing et al 2019b suggest that coarse models often misestimate flood parameters due to the difficulty of representing linear features finer than the mesh resolution overall the 100 m configuration is considered not fit for this case study and should be disqualified while the adherence to observations for the 50 m configuration is still adequate except near pontelagoscuro as regards lisflood fp and can probably be improved by calibrating a specific set of roughness coefficients for this spatial resolution as regards parflood this additional parameter tuning was not performed in this study since the aim of low resolution simulations was mainly to compare the models runtimes when coarser grids were adopted see section 4 4 4 2 secchia river overall 9 simulations were performed with each model 3 grids 3 roughness values in the following we will refer to each test configuration with a label including both the grid size 5 25 50 m and the type of roughness for the rural areas s smooth value i e 0 03 m 1 3s c central value i e 0 05 m 1 3s r rough value i e 0 07 m 1 3s for example the simulations performed with the 25 m resolution grid and manning s coefficient equal to 0 05 m 1 3s will be labelled 25 c test configurations are summarized in table 3 the capability of the parflood model of reproducing the flood evolution and inundation extent for this event was assessed in a previous study vacondio et al 2016 where the 5 m mesh and the calibrated value of 0 05 m 1 3s for the roughness coefficient in rural areas were adopted for this reason the flooded area obtained from the simulation performed with these parameters labelled 5 c is here assumed as reference i e observed for comparing the model performances of both models i e lisflood fp and parflood in terms of inundation extent by evaluating the parameter f e g bates and de roo 2000 defined as follows 11 f a s i m a r e f a s i m a r e f where a sim and a ref represent the simulated and reference flooded areas respectively fig 8 compares the maps of maximum water depths predicted by the two models for configuration 5 c the inundation extent is very similar also thanks to the partial confinement guaranteed by the panaro river levee and by other minor embankments only marginal differences can be observed especially downstream north overall the predicted flooded area is equal to 49 2 km2 according to parflood and 50 3 km2 according to lisflood fp the latter simulation gives a parameter f equal to 91 see table 3 which implies a very good correspondence with the reference parflood simulation the differences between predicted and observed water levels at 46 watermarks are also reported in fig 8 the rmse see table 3 is equal to 0 35 m for both models and indicates an acceptable agreement with observations lisflood fp performs slightly better in the focus area of bastiglia the rmse for the subset of 25 points surveyed in this area is equal to 0 26 m vs 0 31 m for parflood while the mean difference is 0 16 m for lisflood fp and 0 22 m for parflood no significant trend of under overestimation can be detected on the other hand in bomporto the other focus area both models overestimate the observed water depths the mean difference between simulated and observed levels limited to the 11 points surveyed here is 0 40 m for parflood and 0 46 m for lisflood fp in this area parflood is slightly more adherent to observations than lisflood fp since the local rmse is 0 45 m vs 0 51 m for lisflood fp it is worth noting that during the event water accumulated in bomporto and formed a lake at rest due to the local topography bounded by the levees of the panaro river and of the naviglio channel hence deviations from observations in this area can also be partly attributable to the uncertainties in the breach outflow discharge which may lead to a slight overestimation of the flooded volume table 3 reports the values of the f parameter for all the 9 simulations different grid size and roughness performed with each model at the highest resolution 5 m the overall flooded area only slightly differs from the reference simulation even when the roughness coefficient is varied as regards the parflood simulations the values of f are equal to 93 94 using either smoother or rougher values of manning s coefficient conversely for lisflood fp the best agreement is actually obtained from the simulation with the roughest coefficient 0 07 m 1 3s f 97 while the adoption of a smoother coefficient gives a less satisfactory value of 84 due to the slight overestimation of the flooded area to the north see fig 9 which compares the inundation extents obtained from different simulations with the reference one when a lower grid resolution is adopted both models slightly overestimate the flooded area fig 9 simulations with the 25 m mesh give a value for the f parameter around 82 84 for parflood and 81 82 for lisflood fp only marginally variable with the roughness coefficient similarly the f value computed for all simulations performed with the coarsest resolution 50 m is equal to 79 for parflood and to 78 for lisflood fp regardless of the assumptions on manning s coefficient the overestimation of the inundation extent at coarse resolutions is often observed in real applications savage et al 2016b and is related to the poorer terrain description moreover the inclusion of embankments which are artificially widened at low resolution may reduce the volume available for water storage wing et al 2019b similarly to the po river case study for example let us consider the area bounded by the levees of the panaro river and of the naviglio channel the western channel in fig 2 where water accumulates in a lake at rest fashion when the observed maximum water surface elevation approximately 26 m a s l is reached the volume that can be stored in this area reduces by 9 and by 20 for the 25 m and 50 m meshes respectively compared to the 5 m mesh this generates an increase in the amount of flood volume that flows downstream and consequently in the flooded area downstream however neglecting the presence of minor embankments would lead to a much less accurate prediction of the flooding dynamics and inundation extent see the results by shustikova et al 2019 the agreement with the high watermarks was also analyzed for all simulations see values of rmse in table 3 while the parflood model provides the same rmse for all the high resolution simulations 0 35 m slight differences can be observed for the lisflood fp simulations among these the best performance is obtained with the roughest coefficient 0 34 m when a coarser resolution is used the values of rmse are not particularly sensitive to roughness for both models however compared to the results of simulations with higher resolution parflood gives a slightly higher rmse 0 37 m for the 25 m mesh and 0 41 0 42 m for the 50 m mesh instead lisflood fp performs similarly rmse equal to 0 33 0 34 m when the 25 m mesh is used while the rmse increases to 0 39 0 40 m with the 50 m mesh in all cases the rmses for both models are within the data observation error 0 5 m the maps of flood arrival times were also obtained from all simulations even if fig 10 reports only 5 test configurations for each model moreover the extent of the flooded area at 12 24 36 and 48 h is plotted in fig 10c d for these tests as expected the roughness coefficient adopted to describe the rural areas remarkably affects the flood propagation and the flooded area at a fixed time increases when manning s coefficient is reduced i e the inundation propagates faster this is true for both models however as regards the high resolution mesh simulations performed with lisflood fp are slightly more influenced by roughness compared to those performed with parflood especially upstream of bomporto comparable maps of flood arrival times are obtained from simulation 5 c with parflood and 5 r with lisflood fp this means that the flood arrival time at a given point is larger for parflood than for lisflood fp when the same roughness coefficient is adopted faster arrival times in a lisflood fp simulation with the local inertial approximation than in a fully swe simulation are also reported by neal et al 2012b for a dam break case this fact was related to the adoption of decoupled equations in the x and y direction which leads to the creation of preferential flow pathways along the diagonal as discussed by neal et al 2012b and more recently by cozzolino et al 2021 to numerical diffusion and to a lesser extent to the wetting and drying treatment when the resolution is coarsened both models provide arrival times that are quite similar to those obtained with the high resolution mesh up to 12 h whereas larger differences can be observed between the results of high and low resolution simulations in the 24 48 h time interval finally simulations performed with the 25 m and 50 m meshes adopting the same model and same roughness coefficient provide comparable maps of flood arrival times for both lisflood fp and parflood besides the comparison between the two models and between different configurations simulated arrival times can also be compared with the observed arrival times at two locations the urban areas of bastiglia and bomporto see locations in fig 10a b the difference between simulated and observed arrival time for each model and test configuration is represented in fig 11 please notice that given the large uncertainty in the observed values up to 0 5 1 h simulated values were rounded to quarters for the comparison regardless of the resolution parflood predicts the flood arrival in bastiglia quite well using the central value of the roughness coefficient 0 05 m 1 3s although the error is acceptable below 1 h for all configurations conversely lisflood fp provides better results in bastiglia with the roughest value 0 07 m 1 3s in this case the flood arrival time is anticipated up to 1 5 h when the smoothest value of manning s coefficient is adopted the inundation of bastiglia occurred only 7 h after the breach opening without any significant obstacle to slow down the flooding in bomporto instead the inundation arrived after approximately 27 h from the breach opening thanks to the obstruction to flood propagation provided by the levees of minor channels which were eventually circumvented by water this complexity in the flood dynamics affects the numerical results as shown by their higher sensitivity to roughness and grid size as regards the arrival times in bomporto fig 11b at high resolution lisflood fp anticipates the flood arrival time for all roughness coefficients and the best performance is obtained for the roughest value 0 07 m 1 3s though the error is 1 5 h conversely parflood correctly predicts the inundation of bomporto using the central value 0 05 m 1 3s while errors are up to 1 5 h if different values are used at lower resolutions lisflood fp predicts the correct arrival time using the central value while parflood exhibits a slightly larger error up to 1 25 h however the range of flood arrival times using different roughness coefficients is much larger for lisflood fp about 5 h than for parflood about 3 3 5 h results in bomporto especially for lisflood fp are somehow in contrast with the general expectation that the use of fine resolution grids produces later arrival times compared to coarser grids as a result of a more accurate terrain description however in this case study the incorporation of the main embankments in the mesh guarantees the correct representation of the topography and of the inundation dynamics for all grid sizes so this tendency may not be detected overall with equal roughness coefficient parflood provides similar arrival times throughout different resolutions while lisflood fp is slightly more sensitive to grid size switching from high to low resolution however simulations performed with coarse meshes 25 m and 50 m produce almost identical flood arrival times as regards the sensitivity to the values of the roughness coefficient for rural areas similar trends are observed in bastiglia range of about 1 5 h regardless of the grid size for both models whereas simulated arrival times in bomporto are more influenced by this parameter for all resolutions especially for lisflood fp results presented so far suggest that the configurations that best fit the observations are 5 c for parflood and 5 r for lisflood fp however the value of manning s coefficient that guarantees the best prediction for parflood is the same 0 05 m 1 3s for all resolutions while lisflood fp switches from the roughest one 0 07 m 1 3s for the high resolution mesh to the central one for the coarser grids in flood hazard mapping another important variable to be considered is the flow velocity which has consequences on the safety of pedestrians and vehicles e g arrighi et al 2019 and is an important component of flood damage models merz et al 2013 only an inter model comparison is performed here since velocity measurements are obviously unavailable for this event the maps of maximum flow velocities for selected configurations are reported in fig 12 overall velocity values remain well below 1 m s in most of the domain except for local spots where values up to 1 5 2 m s are obtained especially in simulations with low roughness moreover both models predict high velocities up to 3 7 m s depending on the model and configuration at the breach location here inter model differences can also be ascribed to the implementation of boundary conditions in the two codes please notice that the color ramp in fig 12 is saturated at 1 5 m s in order to ease the visual comparison of maps the inter model comparison of these maps somehow confirms the trend that could be anticipated from the analysis of flood arrival times that is other things being equal lisflood fp predicts higher flow velocities than parflood for example in the base configuration 5 c the mean value for the maximum velocity in the flooded cells is equal to 0 29 m s for lisflood fp and to 0 19 m s for parflood while the median values are 0 23 and 0 15 m s respectively the velocity value of 0 4 m s is exceeded by only 10 of the flooded cells in the domain for parflood against 25 for lisflood fp similar trends can be observed for other high resolution simulations although differences in median values are slightly larger for configuration 5 s 0 28 m s for lisflood fp and 0 17 m s for parflood and slightly lower for configuration 5 r 0 2 m s for lisflood fp and 0 13 m s for parflood as expected if we compare the best fit simulations lisflood fp configuration 5 r still predicts higher maximum velocities than parflood configuration 5 c the mean values are equal to 0 25 m s for the former model and to 0 19 m s for the latter while the velocity of 0 48 m s is exceeded only by 5 of parflood flooded cells vs 10 for lisflood fp at low resolutions 25 c and 50 c maximum velocities are slightly lower than those of the high resolution simulation 5 c performed with the same model for example the mean and median values are 16 and 13 lower respectively for both models however local high values of maximum velocity are remarkably reduced especially for lisflood fp when the same model is used simulations with 25 m and 50 m provide very similar overall results at coarse resolution lisflood fp still predicts slightly higher maximum velocities than parflood with equal roughness coefficient the difference between the maximum velocities predicted by the two models can be better observed in fig 13 which zooms on the portion of the domain around bastiglia inside the urban area represented by means of a higher roughness coefficient than the one adopted for rural areas velocities are slightly lower than outside and this is particularly noticeable for lisflood fp results an interesting observation is the fact that at high resolution the maximum velocity predicted by lisflood fp seems influenced by micro topographic features in the terrain such as small ditches and furrows while parflood provides slightly more uniform results when the roughness is increased this effect can still be observed even if the velocities are lower see for example configuration 5 r when the resolution is reduced as for example for configuration 25 c this behavior can no longer be clearly noticed this particular velocity pattern which is only evident in lisflood fp results can partly be ascribed to the different way in which velocity is computed for the two models see section 2 3 but another possible reason is analyzed in section 4 3 4 3 convective acceleration analysis we performed an additional analysis concerning the relative importance of the different terms of the swes for the two previous case studies for this purpose the momentum equations were re written in 2d non conservative form as follows 12 1 g u t u g u x v g u y u g v y η x s f x 0 1 g v t v g u x u g v x v g v y η y s f y 0 i i i i i i i v in this non dimensional form each term represents a slope cunge et al 1980 i e i local acceleration ii convective acceleration iii free surface slope and iv friction slope the first two terms are the inertial terms or acceleration slopes please note that the second term is neglected in the lisflood fp approximation numerical results provided by parflood which solves the fully dynamic swes were elaborated to compute the derivatives and consequently the absolute values of the four terms in eq 12 fig 14 represents the order of magnitude of all terms both x and y directions along the po river reach near borgoforte gauging station approximately at the time of the peak the local acceleration is negligible compared to the other terms below 10 6 10 5 almost everywhere see fig 14a d the free surface slope fig 14c f is certainly the most significant term in the whole reach followed by the friction slope fig 14g h this means that the local inertial or even diffusive approximation can be considered suitable for modelling this event however fig 14b e shows that even at the relatively large resolution 30 m adopted the convective acceleration term is not completely negligible along the main channel especially downstream where the river width decreases and the path is more winding here the spatial variation of the velocity head is fostered by differences in flow direction bed elevation and roughness while the global effect induced by the convective acceleration term on flood propagation and maximum levels can be somehow compensated in the simplified model lisflood fp by adjusting the roughness coefficients neglecting its importance probably leads to the observed differences in the velocity distribution in the main channel compared to a fully dynamic model see fig 6 on the other hand this term does not influence the inundation of dyke protected floodplains dominated by the free surface slope which can be over 10 2 during the overtopping of minor embankments a similar analysis was repeated for the secchia case study configurations 5 c and 25 c fig 15 shows the maps of the convective acceleration free surface and friction slopes along the x and y directions obtained from the parflood simulations 8 h after the breach opening when flooding had just reached the urban area of bastiglia note that local acceleration is negligible compared to the other terms and is not reported at the highest resolution fig 15a the free surface and friction slopes terms dominate the phenomenon while convective acceleration is approximately one order of magnitude lower being relevant only in few cells characterized by large velocity variations due to micro topography when the mesh size increases fig 15b term ii of eq 12 becomes negligible compared to terms iii and iv in the whole domain this confirms the suitability of the local inertial approximation when a coarse grid is adopted while the effects of micro topography at high resolution might require the use of fully dynamic equations for obtaining an accurate estimation of the velocity distribution indeed the fact that lisflood fp neglects the convective acceleration probably determines the difference shown in fig 13 4 4 computational times time step size and mass conservation mass conservation represents a critical point for the numerical simulation of flood propagation over an initially dry bathymetry such as the secchia case we analyzed the mass balance guaranteed by the two models for this test case with only one inflow boundary condition for configurations 5 c 25 c and 50 c the mass errors for this subset of simulations are negligible compared to the total inflow the relative error is below 0 005 for all simulations since large mass conservation errors represent an indirect indicator of instabilities for lisflood fp neal et al 2012b these data also confirm that the model remains stable throughout the simulation despite critical flows can be expected somewhere as mentioned earlier the two models are characterized by different formulations of the cfl stability condition for determining the time step size during the simulations table 4 compares the average time step size for a subset of simulations as expected for both models the allowable time step increases with the grid size because of the linear dependency between these two parameters see eqs 3 and 9 however when the same grid resolution is used time steps of lisflood fp are 1 5 2 times larger than those of parflood the cfl condition of the former model states the inverse proportionality of the allowable time step with the celerity of gravity waves g h while the flow velocity also contributes to restrict the time step for the latter model for both models the po river test case requires a smaller time step for stability compared to the secchia test case when a similar grid size is adopted because of the higher water levels predicted during the simulations the computational times required by the two models for different simulations are summarized in table 5 together with the values of the ratio of physical to computational time r t the physical duration of the events here simulated is equal to 191 h and 48 h for the po river and secchia river test cases respectively as regards the secchia test case only runtimes for configurations 5 c 25 c and 50 c are reported since similar runtimes were obtained for simulations with different roughness coefficient and equal grid size for the simulations with the lowest resolution grid domains with less than 105 cells lisflood fp runs slightly faster than parflood runtimes vary between less than 1 min to a few minutes actually it is well known that the computational capabilities of gpu accelerated models like parflood are not fully exploited when a relatively low number of cells is processed due to the impossibility of masking the high memory latencies that characterize the gpus vacondio et al 2017 however as the number of cells in the domain increases parflood tends to outperform lisflood fp despite the smaller allowable time step table 4 and the more complex numerical scheme which requires more floating point operations for a given number of cells for the simulations with the highest spatial resolution the former model is 2 3 times faster than the latter with runtimes ranging from 1 5 1 8 h for parflood to almost 4 h for lisflood fp incidentally rather than by the total number of cells in the domain runtimes are influenced by the number of wet cells in the simulation in fact both models are designed to skip the updating of dry cells for the po river test case almost 90 of the domain is wet while flooded cells are only up to 30 for the secchia test case together with the larger allowable time step for stability as previously discussed this contributes to obtain larger values of r t for the latter test case than the former as regards simulations characterized by roughly the same total number of cells a direct comparison of the computational times of the two models using the same workstation cluster is impossible given the intrinsically different hardware requirements in this work simulations were run on high end computational resources however the reported runtimes can obviously change depending on the type of processor and number of cores for lisflood fp and on the gpu device for parflood just to give an idea previous works neal et al 2018 report that the computational times of lisflood fp simulations performed on 16 cores can be 4 8 times faster for 2d real test cases with 104 105 cells and up to 10 times faster for analytical test cases with 106 cells compared to the same runs on a single core here we are using 40 cores hence the runtimes would largely increase if a more limited number of cores were available for computation similarly parflood simulations can require a longer runtime when a more outdated gpu is used for example on a nvidia k40 gpu released in 2013 the po 50 m test runs in 83 min and test secchia 5 c runs in 422 min on the other hand when a more modern video card nvidia v100 gpu released in 2017 is used the runtimes of parflood can be reduced by roughly 25 30 e g test case po 50 m only takes 17 min to run while test secchia 5 c takes 78 min one final remark must be dedicated to the floating point precision assumed for the computations in this work all simulations were run in double precision mode in order to minimize the accumulation of numerical truncation errors which may generate non negligible mass errors this is nowadays the default choice for most cpu systems while gpu processors were originally developed and optimized for single precision computations which can be up to eight times faster than double precision depending on the device morales hernández et al 2020 in this work we verified that the runtime of single precision simulations with parflood is roughly halved compared to double precision on modern gpus for example the secchia 5 c simulation runs in 55 min on the nvidia p100 gpu and in only 36 min on the v100 gpu results are not deeply affected and the total mass error remains acceptable below 0 06 of the total inflow hence gpu accelerated models can be considered even more competitive for applications for which single precision can be adopted 5 discussion and conclusions this work aimed at comparing two modelling strategies that have recently emerged for performing 2d large scale flood simulations the first one implemented in the lisflood fp code combines a simplification of the swes i e local inertial approximation and a parallelization based on openmp the second approach adopted by the parflood code exploits the computational power of gpu devices to reduce runtimes while solving the fully dynamic swes two case studies in italy were selected for the comparison once calibrated the two models perform similarly as regards the prediction of the inundated area and maximum water levels this is in line with previous benchmarking studies e g néelz and pender 2013 the 2000 flood event on the po river is reasonably reproduced by both models the lowland inundation due to the secchia levee breach is also predicted quite well the two models however require quite different values of the roughness coefficients to obtain the best results for both test cases in particular the calibrated values for lisflood fp are higher than the ones of parflood especially for the po river floodplains roughness also influences the flood arrival times and maximum velocities which is evident from the results of the secchia test case fig 11 where again lisflood fp requires higher values of manning s coefficient to predict the correct flood arrival times compared to parflood in case of a high resolution mesh these results confirm the importance of the calibration phase if possible i e if observed data are available for both models and suggest that the direct adoption of any literature value based on the land use of the area is not recommended in general since the best values may be model mesh and case dependent indeed roughness coefficients somehow compensate for possible inaccuracies in the terrain description especially at coarse resolution and in the case of simplified models like lisflood fp also for the reduced complexity of the governing equations the analysis reported in section 4 3 shows that the convective acceleration term of the swes can be locally non negligible even for case studies involving gradually varied flows and this fact probably needs to be counterbalanced by a higher friction term for lisflood fp compared to parflood when the importance of inertial terms decreases e g for the secchia case study at coarse grid sizes lisflood fp guarantees a good adherence to observations even when lower roughness values are considered incidentally no preliminary evaluations concerning the applicability of simplified equations to the specific case study e g does inertia influence the hydraulic process need to be carried out if the fully dynamic equations are adopted the influence of the grid size was also analyzed for the secchia case study since the use of low resolution meshes is a common strategy to reduce computational times however coarse grids for raster based models may decrease the accuracy of the terrain representation unless suitable aggregation techniques that consider linear features are adopted in this work embankments were manually burnt into the aggregated data at all resolutions to avoid misestimating the inundated area thanks to this strategy the overall accuracy at low resolutions remains fairly good for both models though lower than the one obtained with the high resolution mesh at the same time the simulation times are largely reduced this trade off effect often allows practitioners to use a coarse grid size for practical applications in large scale domains however the possibility of using a low resolution can sometimes be hindered by particular topographic conditions in the study area an example is represented by the bottleneck of pontelagoscuro for the po river test case where water levels are largely overestimated by both models because of the artificial flow area shrinkage due to the coarse mesh finally an inter model comparison was performed based on the computational times required for each simulation runtimes obviously depend on the hardware typical hpc resources were used here but also on the total number of cells in the domain on the physical time of simulation on the percentage of wet cells and on the time step size this latter parameter is larger for lisflood fp than parflood due to the different stability condition this leads to a lower number of overall time steps for completing the simulation physical time being equal despite this for grids with a relatively low number of cells 104 105 both models perform similarly while parflood outperforms lisflood fp when the number of cells increases 106 thanks to the gpu parallelization moreover parflood s runtimes can be further reduced if first order accurate and or single precision simulations are performed additionally the upcoming new version of lisflood fp will also support gpu acceleration shaw et al 2021 and multi gpu codes with enhanced computational performance are underway e g turchetto et al 2019 morales hernández et al 2020 in the near future these extremely efficient codes are expected to become widely available and enable unprecedented hydraulic simulations in the wide context of 2d large scale hydraulic studies these considerations suggest that for simulations that can be performed at coarse resolution and with a limited number of cells parallelized simplified models like lisflood fp can provide similar results to fully dynamic models and comparable runtimes to gpu accelerated codes and represent a good alternative for applications that require a higher level of detail fine resolution meshes with millions of cells the adoption of reduced complexity models is no longer justified by their computational performance compared to gpu parallelized codes like parflood which additionally provides reliable results in all flow conditions including rapidly varying and supercritical flows thanks to its fully dynamic implementation 6 software availability the lisflood fp software is developed by the university of bristol website http www bristol ac uk geography research hydrology models lisflood the parflood code is developed by the university of parma website http www hylab unipr it it servizi numerico development of 2d parallel algorithms for flood propagation the main software specifications for both models are reported in table 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the po river basin authority for providing terrain data this research benefits from the hpc facility of the university of parma s d and r v gratefully acknowledge the support of cineca under project nemorino id hp10cr41j6 
25707,grid based spatially distributed hydrological modeling has become feasible with advances in watershed routing schemes remote sensing technology and computing resources however the need for long running times on a substantial set of computational resources prevents a spatially detailed modeling program from being widely used particularly in fine resolution large scale studies parallelizing computational tasks successfully mitigate this difficulty we propose a novel way to improve the simulation efficiency of direct runoff transport processes by grouping watershed areas based on a time area routing scheme the proposed parallelization method was applied to simulating the runoff routing processes of two watersheds in different sizes and landscapes the method substantially improved the computational efficiency of the time area routing simulation with common computing resources the efficiency of the parallelization was not limited by the hierarchical relationship between upstream and downstream catchments along the flow paths which could be possible with the lagrangian tracking of the time area routing method keywords time area routing distributed watershed model parallel computing shared memory parallelism stream network grid based simulation 1 introduction hydrological models have evolved from a set of organized or tabulated simple computational procedures for engineering design to thousands of computer code lines describing the interaction between surface water groundwater and the atmosphere as tools for scientific analysis clark et al 2017 singh 2018 studies now attempt to describe the continental scale and long term hydrological cycle clark et al 2015a coe 2000 trambauer et al 2013 wood et al 1997 yang and musiake 2003 ever improving remote sensing technology and computing resources have helped hydrological models to take much more details of the landscape into account chen and wang 2018 gleason et al 2018 kite and pietroniro 1996 qin et al 2008 xu et al 2014 several models describe internal transport processes rather than just predicting the overall responses of a watershed to a rainfall event fenicia et al 2016 freeze and harlan 1969 her and heatwole 2016 karssenberg 2002 yang et al 2000 process based approaches have gained popularity as they can provide a precise picture of hydrological processes and improve modeling quality by accommodating many different ways to calibrate parameters and evaluate performance clark et al 2015b fatichi et al 2016 montanari and koutsoyiannis 2012 shen and phanikumar 2010 however the computational burden of process based methods still encourages hydrologic modelers to simplify the representation of flow routing processes especially in large scale and long term simulations chaney et al 2016 clark et al 2015b 2017 fatichi et al 2016 freeze and harlan 1969 rouholahnejad et al 2012 overland flow routing is a hydrological process that controls many other processes including direct runoff generation soil moisture groundwater recharge and their spatial distributions engman and rogowski 1974 kim et al 2013 liu et al 2007 valeron and meixner 2010 thus overland flow simulation is a critical tool for process based hydrologic modeling and its accuracy there are several different approaches proposed to describe two dimensional overland flow routing processes based on hydraulics and hydrologic theories including solving a governing partial differential equation i e the st venant equation and its variants butts et al 2004 downer et al 2002 downer and ogden 2004 vieux et al 2004 assuming a steady state hillslope surface velocity ivanov et al 2004 wigmosta et al 1994 and using a dynamic time area method her and heatwole 2016 the numerical methods such as finite difference element methods fdm fem often require the use of a tiny time step e g seconds depending on the spatial resolution of a simulation to maintain numerical stability e g the courant condition such a limitation leads to inefficiency in the long term simulation such as for a climate change impact assessment borah and bera 2004 fread 1993 murillo et al 2006 moreover the constant flow velocity assumption is not realistic when considering the temporal variation of rainfall intensity and water transport processes along flow paths cantone and schmidt 2011 lee and yen 1997 saco and kumar 2002 the dynamic time area method provides an alternative way to describe the spatiotemporal changes of direct runoff velocity in the two dimensional simulation of overland flow routing her and heatwole 2016 2018 this method combines the classic time area watershed routing concept with the curve number cn methods modified for updating cns based on soil water content and considering the overlap and re infiltration of direct runoff routed from upstream areas parallel computing techniques have mitigated the computational burden of process based overland routing modeling and made spatially distributed simulation feasible maxwell et al 2015 studies have demonstrated a scalable parallel implementation of parflow surface and groundwater modeling for integrated large scale flow simulations kollet et al 2010 maxwell 2013 maxwell et al 2015 li et al 2011 and wang et al 2012 parallelized the subwatershed level simulation of hydrological models the digital yellow river model and xinanjiang model respectively by representing drainage networks with binary tree structures and passing upstream subwatershed modeling results to processors beginning simulations for downstream areas so that channel flow could be routed vivoni et al 2011 investigated the relationship between the number of processors catchment complexity and subwatershed partitioning methods in the parallel implementation of tribs modeling using the message passing interface mpi technique liu et al 2014 grouped subwatersheds by their stream orders or grid layers so that the subwatershed level simulations of a grid based distributed hydrological model was independent of each other in parallel computing liu et al 2018 also parallelized grid based hydrologic modeling at subwatershed and grid cell levels using mpi and shared memory programming techniques respectively hwang et al 2014 improved the computational efficiency of a three dimensional finite element model by parallelizing matrix manipulations zhang et al 2016 distributed parameter sets sampled by a genetic algorithm over the multiple nodes of a high performance computing hpc system and implemented subwatershed scale hydrologic simulations using multiple cpus of each node in a parallel way using an mpi technique le et al 2015 and vacondio et al 2017 employed a graphics processing unit gpu based parallelization algorithm to improve the computational efficiency of two dimensional shallow water modeling zhu et al 2019 proposed an openmp based computing technique that facilitates model level parallelization to improve the efficiency of sensitivity analysis and automatic calibration that require many model runs one of the common parallel computing strategies for surface water hydrologic modeling is to split an entire watershed into subwatersheds that do not interact or do not exchange surface water across ridgelines then a modeler can assign each available computing resource i e processor to a subwatershed and model internal processes happening within each subwatershed using the single processor once all the subwatershed level simulations are completed upstream modeling results can be fed to downstream simulation as a boundary condition in such routing processes upstream outputs cannot be simply added to downstream ones to determine the hydraulics of downstream flow because the responses of downstream hydraulics to different boundary conditions or flow routed from upstream channels and areas are nonlinear sivakumar and singh 2012 for instance manning s equation demonstrates that the velocity of flow is a nonlinear function of the hydraulic radius which is calculated based on the nonlinear relationship between discharge and channel dimension usace 1995 from a computational perspective due to the nonlinear relationship between velocity and discharge the velocity of the downstream flow cannot be determined until water transported from the upstream areas is added to the downstream flow in a unit time interval thus downstream routing simulations need to wait until all the associated upstream simulations are completed which can create a bottleneck in the computations and delay the overall simulation process a time area concept has emerged since clark 1945 introduced a way to synthesize unit hydrographs by translating an excess rainfall depth to a direct runoff hydrograph based on a time area histogram her 2011 maidment et al 1996 in the time area unit hydrograph theory the relationship between travel time and the corresponding drainage areas determines the shape of the direct runoff hydrographs such a unique and intuitive idea inspired additional studies that tried to relate the hydrological features of a watershed to its responses to rainfall events ajward and muzik 2000 du et al 2009 gyasi agyei et al 1996 maidment et al 1996 martínez et al 2002 melesse and graham 2004 muzik 1996 noto and loggia 2007 rodríguez iturbe et al 1979rodríguez iturbe et al 1979 saghafian et al 2002 her and heatwole 2016 employed a time area concept to develop direct runoff hydrographs by describing a two dimensional watershed runoff routing process using travel time calculated from flow hydraulics the time area routing method was successfully tested in an agricultural catchment virginia u s her and heatwole 2016 two mountainous watersheds south korea and brazil and a wetland woody area florida u s the efficiency of the time area routing method demonstrated its potential as an alternative to solving the saint venant equation using numerical methods for two dimensional watershed routing simulations the time area flow routing method proposed by her and heatwole 2016 and her 2011 distributes the amount of direct runoff generated on or routed onto the individual grid cells to their downstream isochrones on time area maps updated based on flow hydraulics in every time interval as the method traces the transport processes of direct runoff volume along its flow paths in the grid representation of a watershed it can be said that in the time area method direct runoff routing is simulated by a lagrangian approach flow tracking in an eulerian description of the flow hydraulics travel time grid maps in the lagrangian point of view direct runoff in a location is considered as an aggregation of fluid parcels generated on the location or transported from other areas and the marked volumes or depths of direct runoff generated on different locations individually move at the average velocities bear 1972 fischer et al 1979 thus the history of individual direct runoff can be tracked along its flow paths and the locations of moving direct runoff are represented as functions of time or travel time bear 1972 the lagrangian approach is commonly used to describe the advective transport processes of particles as it can avoid numerical dispersion in the application to advective dominated transport modeling zheng and bennett 2002 the lagrangian flow tracking provides a new way to investigate runoff routing processes for watershed management her and heatwole 2018 also this new routing approach allows the direct runoff of individual grid cells to be distributed to downstream areas independently in a simulation time interval this capability inspires us to explore parallel computing options for improved computational efficiency of two dimensional watershed simulations especially in large scale fine resolution flow modeling this study proposed a way to parallelize the time area routing method and tested its efficacy in two watersheds we investigated the relationship between the parallel efficiency measures stream network densities watershed areas cell sizes rainfall depths and the number of processors employed in this study the overland flow routing of a grid based watershed model hydrology simulation using time area method hystar was parallelized to demonstrate the implementation of the proposed parallel computing strategies we also examined bottlenecks that delay the overall simulation processes and discussed the potentials and limitations of the proposed parallel routing method in this article 2 overview of time area routing scheme the time area routing method represents a watershed as a group of isochrones varying over time the length of time it takes for direct runoff to pass an overland or channel cell is calculated based on local hydraulics characterized by runoff discharge ground surface slope and roughness of the cell manning s formula fig 1 then the calculated cell passing time is accumulated along the flow paths from the watershed outlet to each cell to determine the travel time required for the runoff to reach the outlet from the cell once travel time is determined for all the cells within a watershed travel time isochrones are updated and the amount of runoff passing the outlet in a time interval is calculated from the isochrones fig 2 in the time area method direct runoff routing simulation is implemented on the travel time maps or isochrones rather than on the velocity field of direct runoff and thus we can simply add up cell passing time from the outlet to individual cells to calculate the cumulative direct runoff travel time fig 3 in other words we can represent non linear flow routing processes with the linearly addable property i e passing or travel time of local flow on individual cells by projecting local hydraulics i e velocity onto the time domain in the grid representation of a watershed then the time area routing simulation can be parallelized regardless of the hierarchical relationship between upstream and downstream subwatersheds it is worth noting again that the travel time of a cell means the amount of time required for runoff generated on the cell to reach the outlet or the most downstream cell thus the travel time of a cell is calculated by accumulating the travel times of cells located downstream of the cell from the most downstream or the outlet of the watershed to its neighboring or immediate downstream cell along the flow path and the cell itself in other words the travel time is accumulated in the direction opposite to the flow direction for example water flows from a2 to a1 b2 to b1 a1 to c2 b1 to c2 and then c2 to c1 flow direction in fig 3 the travel time of 15 min from a2 to the subwatershed watershed 2 2 in fig 2 outlet i e a1 is calculated by adding 9 min required to pass a1 to 6 min required to pass a2 fig 3 a and b similarly the travel time of 113 min required to move from c2 to the outlet not shown in fig 3 but only in fig 2 of the subwatershed watershed 1 in fig 2 is determined by adding 4 min to 109 min required to reach the outlet from c1 fig 3 a and b finally the travel time of 122 min for a1 is calculated by adding 9 min to the travel time of c2 113 min the travel time of 128 min for a2 is the addition of 15 min a2 in fig 3 b and 122 min a1 in fig 3 c 2 1 hydrologic and hydraulic background in the grid based time area routing scheme the cell passing time of direct runoff is calculated based on the local flow hydraulics represented by manning s formula fig 1 which requires information about the depth of direct runoff created on each cell there are two sources of water coming into a cell the cell itself rainfall and its upstream cells routed water the calculation of direct runoff depth should consider both of these sources effectively in the hystar model the time area routing scheme is combined with the curve number methods which were modified from its original form to consider direct runoff routed to a cell as well as raindrops falling directly into the cell to determine the depth of direct runoff occurring in the cell in every simulation time interval fig 1 her and heatwole 2016 moglen 2000 the calculated depth of direct runoff is then fed into manning s equation and the mass balance equation to calculate the velocity of direct runoff passing through a cell and the amount of water infiltrating into the soil respectively fig 1 the soil water content of each cell is updated based on the water mass balance which is used to determine the curve numbers of the cells for the next simulation time step fig 1 the entire calculation procedures described above are repeated for every time interval so that the spatiotemporal variations of the direct runoff transport processes can be simulated 2 2 travel time isochrones and direct runoff hydrographs once the velocity of the direct runoff passing each overland or channel cell is determined the time required for the runoff to pass the cell can be calculated fig 1 here the length of the flow path is assumed to equal the length of a side or diagonal path of a cell then the passing time calculated is accumulated as moving from the outlet to each cell along the flow paths to determine the cumulative direct runoff travel time on which isochrones are mapped fig 2 the travel time isochrones can expand to upstream areas or shrink down to the outlet depending on the flow hydraulics calculated fig 2 demonstrates how travel time zones expand with the passage of time the direct runoff rate or the ordinate of the direct runoff hydrograph of a particular time interval i e 1 h is determined by adding up the amount of direct runoff that occurred in the 1 h travel time zone areas in blue in fig 2 the travel time maps are updated as the flow velocity changes in every time interval there may be some areas that do not contribute to the direct runoff at the outlet during a rainfall event due to their hydrologic characteristics such as permeable soils and shallow slopes as shown in fig 2 gray areas on the other hand the vicinity of the stream networks may have more opportunities to contribute to the downstream direct runoff hydrographs since channelized flow usually has a faster velocity than the shallow overland flow 3 parallel simulation strategy 3 1 travel time calculation there are two main computational procedures to implement for the time area routing scheme in two dimensional hydrologic modeling travel time calculation and direct runoff distribution in the time area routing scheme travel time can be calculated independently and simultaneously at the subwatershed scale which facilitates its parallelization fig 4 the travel times calculated for all the cells in each subwatershed can be accumulated with consideration of the hierarchical relationships between the subwatersheds along the stream networks fig 3 for instance if a watershed is split into seven subwatersheds and there are eight processors available for simulation a sequential routing scheme requires at least the number of iterations equal to the highest stream order i e 3 in the case of fig 4 a of the watershed to complete parallel simulation of the routing on the other hand the travel time calculation of the time area method can be completed in a single parallel computing iteration by evenly and randomly distributing the seven subwatersheds into seven of the eight processors cores or threads fig 4 b a set of pseudocodes is provided as supporting information to describe how the parallel simulation strategy was implemented with r at the end of this article when there are only two processors and each of the seven subwatersheds is assigned to one of them there are 21 different combinations of subwatersheds and processors 7 2 7 2 7 2 21 in the case of the parallel computing method fig 4 b if there are four processors available for the parallel computing one or two processors may have only one subwatershed assigned depending on how fast routing computations for the first four subwatersheds can be completed which will reduce the overall parallel computing efficiency thus the efficiency of parallel time area routing will depend on the number of subwatersheds and processors available as well as the size of a study watershed once the computation of direct runoff travel time is completed for individual subwatersheds the calculated travel time values need to be aggregated on a watershed scale in the time area routing scheme the velocity of direct runoff passing a cell is converted to the cell passing time by dividing the flow length of each cell by the calculated velocity then the cell passing time is accumulated from the outlet of each subwatershed to calculate the travel time required for the direct runoff to reach the outlet from individual cells figs 2 and 3 in the parallel travel time calculation the travel times required for runoff to pass individual cells are calculated first fig 3 a and the travel times are accumulated along the flow paths at the subwatershed scale fig 3 b these subwatershed level travel time calculations are implemented by multiple processors in parallel once the subwatershed scale travel times are obtained for all the cells they accumulate again from the watershed outlet to each of the subwatershed outlets along the stream network fig 3 for example the travel time of a direct runoff occurred in the most upstream cell of a downstream subwatershed is added to those of all the cells of its upstream subwatershed unlike the case of calculating cell passing time at a subwatershed scale such a travel time accumulation is sequentially implemented from the most downstream subwatershed to the most upstream one so that the travel time can indicate the total time taken for direct runoff to reach the main outlet from any point along its flow paths in a watershed 3 2 direct runoff distribution once a travel time map is updated the time area routing distributes direct runoff created in a cell to downstream areas in the current or next travel time zone depending on the spatial arrangement of the travel time zones or isochrones along the flow paths fig 5 and table 1 when the travel time of a zone in which a cell of interest is located is only 1 h i e simulation time interval longer than its neighboring downstream zone the two zones or isochrones are connected to each other so that direct runoff can move on to the downstream zone from the current or upstream one case 1 in fig 5 in this case the amount or depth of direct runoff occurred in a cell will pass the current travel time zone and then it will be evenly distributed to cells located in the downstream travel time isochrone along flow paths in the next simulation time step on the other hand when two adjacent zones are not connected to each other or the difference between the travel times of the two zones is greater than the simulation time interval of 1 h the direct runoff of the cell cannot go beyond the boundary of the travel time zone where it is located in the current time step in the next time step case 2 in fig 5 in such an instance the direct runoff of the cell is evenly allocated onto downstream cells in the current zone in the next time step as described earlier the calculation of direct runoff travel time can be parallelized on the subwatershed scale on the other hand the direct runoff distribution of multiple grid cells can be implemented simultaneously which can increase local data granularity e g the amount of computation to communication between processors and thus be more favorable to parallel computing compared to the case of travel time calculations in case 1 for instance 2 mm and 7 mm of direct runoff on the cells a and g are evenly distributed onto two cells d1 and e1 in its downstream isochrone in yellow fig 5 and table 1 in case 2 3 mm and 4 mm of direct runoff on the cells b and h are distributed onto their downstream cells c2 for b and f2 and g2 for h in the same isochrone in blue when seven processors are available thus the direct runoff depths on all seven cells a b c f g h and i in the current travel time zone or isochrone in blue are individually and concurrently routed onto their downstream cells d1 and e1 in case 1 b2 c2 f2 g2 and i2 in case 2 identified based on the flow direction and travel time similarly to the case of the travel time calculation there are 21 different combinations of cells rather than subwatersheds and processors when two processors are available the spatial scale i e cells of the parallelization for the direct runoff distribution is finer than that i e subwatersheds of the travel time calculation parallelization which is expected to bring differences in their efficiency 3 3 parallel computing environment the hystar model was written in r which provides several packages and options for parallel computing https cran r project org web views highperformancecomputing html hystar was designed as a tool to simulate hydrological processes in routine analysis and planning practices whose users may not have access to high performance computing facilities that usually have several hundreds of central processing units cpus connected in networks e g cores and nodes thus the mpi and open multi processing openmp approaches which are suitable for large computer networks with distributed or shared memory systems respectively were not considered in this study as a graphics processing unit gpu has received attention as an efficient computing resource for large scale calculations especially matrix manipulations parallel computing techniques that employ gpus are emerging however the use of gpu requires the development of computer codes for gpus that are different from cpu codes and hence hydrologic models written for gpu computing are not common yet also it is known that gpus are not efficient for the parallelization of irregular problems including trees and graphs e g drainage networks aurousseau et al 2009 burtscher et al 2012 in this study we explored several shared memory c f distributed memory parallelism strategies and techniques that allow multiple processors to share the same memory which is one of the simplest ways to parallelize hydrological routing using common computing resources such as a personal computer pc then we found the snow library of r satisfied our needs for parallel time area routing simulation 1 applicability of a shared memory approach that can be used with computing resources commonly used in hydrologic modeling 2 compatibility with the programming language i e r used to materialize the routing scheme and 3 ease of updating the codes later tierney et al 2009 4 experiment setup 4 1 numerical setup numerical experiments were designed to demonstrate the performance improvements that can be achieved by implementing the proposed parallel routing approach we investigated the relationship between the parallel computing efficiency and factors that are expected to influence the efficiency including watershed sizes spatial resolutions or cell sizes the density of stream networks storm sizes and the number of processors employed in the experiments the number of processors varied from 1 to 16 while the execution time was being monitored in addition a routing model was implemented in two watersheds that have different drainage areas and topography hp6 small steep and ordway swisher biological station osbs large shallow the routing modeling was also implemented at two different spatial resolutions 3 m and 10 m for hp6 and 10 m and 30 m for osbs in the experiment the stream networks were defined with the different sizes of threshold areas that are required to initiate stream networks in the watersheds 2 5 dense and 20 sparse her and heatwole 2016 the use of different stream network thresholds snts provided different numbers of subwatersheds and channel cells we also investigated how the depths of effective rainfall influence the model running time and efficiency fig 6 to make sure that the entire watershed can contribute to the direct runoff hydrography at the outlet in the experiment we simulated routing for a hypothetical storm event that produced 400 mm over 4 h the first 2 h of rainfall total 200 mm was used to saturate the soil layer and thus increase the amount of direct runoff to be routed in the experiment then the rainfall depths in the third and fourth hours were used to represent two rainfall events with different initial soil water conditions fig 6 a and b the numerical experiment was implemented on a computer that had two intel xeon processor e5 2680 v3 cpus and 256 gb of ram in the ubuntu ver 19 04 operating system each experiment was repeated ten times to get an average execution time and reduce the influence of random performance variation of the pcs on the results the speed up ratio and parallelization efficiency were measured to evaluate the parallel routing simulation strategy proposed in this study the speed up ratio s is defined as the ratio of the serial execution time t s to the parallel execution time t p as follows rauber and rünger 2013 s t s t p the parallel efficiency e is defined as the ratio of speed up to the number of processors p rauber and rünger 2013 e s p 4 2 example application areas and model preparation the study watersheds hp6 and osbs are located in south korea and florida usa respectively fig 7 the hp6 is a mountainous watershed with a drainage area of 3 79 km2 and an average slope of 17 9 the hp6 watershed mainly consists of forest 46 paddy 21 built up 23 and upland 6 and its predominant soil texture is loam the osbs watershed drains 133 59 km2 and it has gentle slopes from 0 to 5 average 2 8 on sandy soil the osbs watershed is covered by forest 32 shrubland 30 grass 10 and wetlands 10 the geospatial layers including digital elevation models dems land cover and soil maps were obtained from various sources the korean national geographic information institute ngii for the dem of hp6 the korean ministry of environment moe for the land cover of hp6 the korean rural development administration rda for the soil of hp6 the united states geological survey usgs for the dem of osbs and the united states department of agriculture usda for the land cover and soil of osbs the topographic characteristics of the watersheds such as slope and streamflow networks were derived from the dems table 2 the soil characteristics including texture hydrological soil groups depth field capacity and hydraulic conductivity size were derived from the detailed soil map hp6 and the soil survey geographic database ssurgo osbs the land cover information was extracted from the level ii land cover map hp6 and cropland data layer cdl osbs the input parameters such as curve number roughness coefficients and crop coefficients were obtained from the literature allen et al 1998 neitsch et al 2011 reybold and teselle 1989 the original dems that were prepared at the resolutions of 10 m hp6 and 30 m osbs were interpolated at 3 m hp6 and 10 m osbs respectively using the spline method her et al 2015 slope and stream networks were then derived from the interpolated dems and provided to the routing simulation the categorical data of the land use maps were resampled at finer resolutions using the nearest neighbor algorithm 5 application and results in the case of calculating the travel time in the osbs watershed the computational cost represented by clock time decreased with increases in the number of processors up to 4 to 8 dense and 4 sparse and then it started increasing when processors more than eight were employed fig 8 and table 3 a pattern similar to that of the osbd case was found in hp6 3m and the minimum computational cost was found when 4 to 8 processors were deployed fig 8 and table 3 the speed up ratios exhibited patterns similar to those of the computational cost or clock time the speed up ratio reached its maximum values with 4 to 8 dense and 4 sparse processors in osbs 10m and osbs 30m and 8 dense and 4 sparse processors in hp6 3m in the case of hp6 10m however the computational cost proportionally increased and the speed up ratio decreased when increasing the number of processors from 1 to 16 fig 8 and table 3 the inefficiency might be attributed to the fact that the sequential simulation was fast enough the sequential computing took only 0 4 0 5 s at most to complete the travel time calculation for hp6 10m due to the small number of cells and computational load such a finding implies that parallel computing may not be efficient for small scale simulation the result also suggests that the local workload should be larger than the communication cost e g high data granularity to see tangible efficiency gains from parallel computing the computational cost and efficiency were not substantially affected by the size of rainfall in the case of travel time calculation which is expected because the rainfall depths do not change the number of cells or calculation units but only the size of figures to be calculated i e travel time the execution time required to complete the simulation of direct runoff routing decreased overall with increases in the number of processors used in parallel computing fig 9 and table 3 for instance in the case of osbs 10m d the speed up ratio increased from 1 97 to 6 36 while increasing the number of processors from 2 to 16 under the large storm scenario the computational cost and efficiency were responsive to the rainfall depths which is different from the case of the travel time calculation deeper rainfall depth is likely to make two adjacent isochrones travel time zones connected to each other which creates longer direct runoff connectivity and thus longer routing paths across neighboring isochrones in fig 10 for instance the s2 storm that has an additional 100 mm rainfall in the fourth hour from its beginning compared to the s1 storm fig 6 a and b created larger areas or isochrones connected to the outlet of hp6 in the fourth hour to the end of the storm two isochrones are regarded as connected to each other when the difference between their travel times is equal to or less than an hour i e the simulation time interval in addition an isochrone is considered to be connected to the outlet if it is connected to a series of other downstream isochrones eventually connected to the outlet or the 1 h isochrone in the fourth hour of the s1 and s2 events the isochrones connected to the outlet of hp6 cover 24 s1 and 100 s2 of the drainage areas of hp6 the difference is 76 respectively in the fifth hour the coverage difference decreased to 8 15 for s1 and 23 for s2 as both s1 and s2 events do not have any additional rainfall such a comparison demonstrates the effect of additional rainfall on the direct runoff connectivity and routing path length therefore a large storm event will require a longer time to implement direct runoff routing overall the results show that the parallel computing methods for direct runoff routing and travel time calculation become more efficient for relatively large computational loads such as for smaller spatial resolutions denser stream networks and deeper rainfall depths in the case of hp6 10m we could not find any efficacy of the parallel computing methods and the speed up ratios were less than 1 0 regardless of the cell sizes and stream network densities fig 9 such a result was also found in the case of travel time calculation fig 8 the serial execution of the direct runoff routing takes a short time the computational costs or clock times were only 0 5 0 6 s at most to be completed which was not substantially large compared to the overhead time the computational costs were 0 1 0 2 s table 3 as the number of processors increased parallel computing efficiency gradually decreased in most cases such a finding implies that more computational resources were directed to scheduling multiple tasks i e the overhead rather than executing the routing simulation itself when the number of processors employed increased liu et al 2014 the results demonstrate how sensitive the parallel computing efficiency to the data granularity in the case of hp6 10m for instance the number of overland cells on which the calculation was implemented was smaller than those of the other cases only 2 8 9 0 and 25 of the overland cell numbers of osbs 10m hp6 3m and osbs 30m respectively while the numbers of subwatersheds remained similar to each other table 2 assuming the communication cost was kept relatively unchanged the granularity could decrease proportionally to the relative sizes of the overland cell numbers e g 2 8 compared to the case of osbs 10m generally the parallel computing methods provided relatively higher efficiency in the cases of obsb than hp6 when the same number of processors were employed in parallel figs 8 and 9 these results are mainly attributed to the differences in the computational loads or the number of overland cells where the computations are performed and the level of data granularity however the data granularity could not explain all and the hydrological and landscape characteristics of the watersheds might affect the efficiency the impact of cell sizes the number of overland cells rainfall depths and stream network densities on speed up or efficiency were compared when employing the number of processors that provided the overall best performance in the direct runoff routing 16 processors and travel time calculation 4 processors fig 11 in the case of direct runoff routing the speed up ratios of osbs were higher than those of hp6 even when the numbers of overland 420 900 and channel 2 447 cells of hp6 3m were larger than those 148 438 and 1 253 of osbs 30m table 2 fig 11 such a finding implies that parallelization of direct runoff routing might be more useful for a watershed that has a low relief than a mountainous topography the average slopes of osbs and hp6 are 2 8 and 17 9 respectively the travel time of direct runoff is longer on shallow slopes compared to that of steep ones when assuming the same soils and land covers then the longer travel time tends to create more isochrones or travel time zones and thus requires a longer time to complete the redistribution or routing of the direct runoff this result indicates that topography and flow hydraulics as well as the number of cells or spatial resolution influence the performance of the parallel direct runoff routing methods such a finding can be explained by the characteristics of the lagrangian flow description or the material coordinate system employed by the time area routing method which tracks the detailed route of direct runoff depth generated on a cell li et al 2018 salamon et al 2006 overall the speed up ratios increased as the cell size decreased the amount of time spent to complete the assigned calculation increased with increases in the sizes of the assignment which led to higher data granularity and relatively smaller communication overhead and thus better parallelization performance liu et al 2014 2016 in the cases of osbs with the relatively small rainfall depth s1 however osbs 10m showed higher speed up ratios than osbs 30m such a result is associated with the watershed slopes the original 30 m dem of the osbs watershed was interpolated into the 10 m dem using the spline method which decreased the average slopes from 2 80 osbs 30m to 2 08 osbs 10m as described previously shallow slopes create more isochrones and thus require longer computing time than do steep ones the result might also be related to the soil characteristics of the watersheds loamy hp6 and sandy osbs sandy soil has higher infiltration capacity than does a loamy one which may make the osbs watershed have a relatively large portion of unsaturated areas even after direct runoff routed from upstream areas are added compared to the hp6 watershed especially when the depth of rainfall is relatively small s1 fig 11 the unsaturated areas create more discontinuous isochrones or low direct runoff connectivity which require more time to complete direct runoff distribution than do the saturated areas in the case of travel time calculations the speed up ratios or efficiency were higher with the dense stream networks than the sparse ones fig 11 the denser networks are likely to create continuous isochrones as concentrated flow in the stream tends to have faster velocity than sheet flow on overland areas the efficiency of the parallel direct runoff routing was not responsive to the density of stream networks in the case of simulating direct runoff routing in hp6 where sequential computing can be done quickly as much as the overhead time of the parallel computing this study compared the computational costs required to complete the travel time and direct runoff distribution for a year using the proposed parallel computing methods with different numbers of threads to demonstrate the efficacy in the practice of hydrological modeling fig 6 c and d and table 4 similar to the cases of the hypothetical rainfall fig 6 the parallelization could substantially save computational time for the direct runoff distribution in long term hydrological modeling in the application to simulating the hydrological processes of hp6 during 1998 and osbs during 2018 the parallelization reduced the computational costs of the direct runoff distributions by 80 e g 7 42 h vs 1 48 h in hp6 3m d and 75 48 57 h vs 11 74 h in osbs 10m d using 16 threads compared to the case of the sequential calculation or using only one thread the overall model running time decreased as much as the amount of computational costs saved by parallelizing the direct runoff distribution for example the running time of 10 29 h was reduced to 4 85 h in the case of hp6 3m d when 16 threads were employed in the parallel direct runoff distribution and the difference of 5 44 h 10 29 h 4 85 h is similar to the difference of 5 94 h between the total running times of 7 42 h sequential and 1 48 h parallel with 16 threads similar observations were made in the cases of osbs regardless of the number of threads used simulations other than the direct runoff distribution and travel time required the relatively constant amount of time e g around 2 7 h 4 85 h 1 48 h 0 63 h in hp6 3m d and 33 3 h 53 70 h 11 74 h 8 68 h in osbs 10m d such findings indicate that the overall computational efficiency of the long term modeling can be efficiently improved by parallelizing the direct runoff distribution in the case of the travel time calculation the parallel computing did not help save time but even increased the computational costs especially in hp6 where sequential computing was fast enough to complete travel time accumulation in less than an hour table 4 this implies that the parallelization may not be efficient when the communication overhead of parallel computing is relatively large compared to the amount of time required to sequentially implement the computational loads the result also indicates that the overhead time may increase with increases in the number of threads used in the parallel computing the overall computational time for the entire modeling decreased until the number of threads increased to eight and then increased when 16 threads were employed which made the case of using eight threads to complete the entire modeling relatively fast in 51 69 h compared to the others in osbs 10m d such findings demonstrate the relationship between communication overhead and computational efficiency which may lead to decrease in the overall performance with increases in the size of a thread pool overall the comparison suggests the two dimensional runoff transport simulation can be more efficiently improved by focusing on the direct runoff distribution when the size of a watershed is small 6 discussion our parallel formulation of two dimensional time area watershed routing consists of three steps 1 partitioning the study area into multiple non overlapping areas 2 computing the time area routing for each non overlapping area independently and parallelly and 3 combining the results of these smaller areas to produce the optimal watershed routing simulation for the study area one of the critical ideas of our approach is to use a spatial data partitioning scheme to improve the efficiency of parallel processing in the experimental design we partitioned the study areas into smaller groups of subwatersheds and cells and evenly distributed the groups onto multiple processors since the individual tasks assigned to the processors are independent of each other the output of the parallel processing can guarantee the optimality therefore the proposed parallelization approach is scalable for sizable time area runoff routing the second key idea is to increase the local data granularity to improve the degree of concurrency of local tasks without losing solution quality in our analysis we saw that the performance of parallel computation represented by speed up and efficiency increased as the size of the watershed increased it is important to note that the proposed approach can adjust the cell size and balance the workload for the parallel computation we used the fine grained cells to partition the study watershed and evenly distribute the cells to local machines for improved load balance since the tasks are independent of each other the computational loads can be balanced to further improve the efficiency of parallel processing it should be worth exploring new spatial data partitioning techniques that can balance the workload and efficiently distribute subwatersheds across multiple machines the experimental results show that the proposed parallel formulation can efficiently partition both data and tasks and significantly speed up the computation of two dimensional time area watershed routings future work will evaluate the efficiency of a parallel formulation using distributed computing environments apache mapreduce and apache spark have received growing attention for large spatial data sets these big data processing platforms utilize large sized memory and disk storage to process sizable spatial datasets e g the entire basin of the mississippi river a parallel formulation for the big data processing platforms raises several interesting research questions big data processing platforms may not be scalable when the communication cost is high because the network i os or communication will be the main bottleneck to remedy this issue the communication cost should be minimized or data granularity should be increased for instance we can minimize the size of task results or outputs e g travel time and direct runoff depth maps and reduce the size of data to be delivered to the master processor thus the next study will explore data reduction techniques and evaluate their efficiency in representing the outputs using minimal description formats as some of the local data in tasks are invariant asynchronous communication techniques are expected to reduce the network i os without losing solution quality 7 conclusions this study proposed a new parallel computing strategy for the time area watershed routing to improve the efficiency of two dimensional direct runoff transport simulation and evaluated its efficacy in two watersheds having different sizes and landscapes the results suggested that the new method could substantially speed up the direct runoff distribution with common computing resources and environments the parallelization was much less efficient sometimes even counterproductive for the travel time calculation than the direct runoff distribution due to its relatively low local data granularity the speed up of the parallel computing increased with increases in the size of computational loads represented by the size of watersheds and subwatersheds or local data granularity storm events and grid cells the speed up did not always increase with an increase in the number of processors used depending on the size of computation relative to that of the communication overhead this study demonstrated that the efficiency of the parallel routing was a function of watershed landscape and resulting flow hydraulics as well as computing resources which was attributed to the feature of the lagrangian approach the time area method as the lagrangian tracking approach enabled direct runoff routing from individual grid cells to be independent of each other and thus increased the efficiency of parallel computing such a finding suggests the potential of parallel computing for improved efficiency of a lagrangian simulation approach software data availability name of software hystar hydrology simulation using time area method programming language r operating system windows linux and macos hardware requirements depend on the size of a watershed to be modeled and the spatial resolution or cell size of modeling a computer with multiple cores or threads and at least 8 gb ram is preferred for parallel computing software required rstudio https www rstudio com and the snow r package for parallel computing https cran r project org web packages snow index html version 0 0 2 as of april 2021 availability anyone can download use and share the model source code with data and files prepared for the study watersheds presented in this article her y g 2021 hystar hp6 hydroshare http www hydroshare org resource 6227021dde6d4c5bae7ec9630a9bb23d her y g 2021 hystar osbs hydroshare http www hydroshare org resource 9fa39b142f6c4505bc64888b724d29a1 license creative commons attribution nocommercial sharealike cc by nc sa https creativecommons org licenses by nc sa 4 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the usda national institute of food and agriculture hatch project fla trc 005551 and mcintire stennis project fla trc 005658 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105222 pseudocodes for the parallelization algorithms call the snow package if the parallel routing option is selected library snow specify the number of threads to be used e g 4 threads will be used in this example no cluster 4 create a sock cluster on the local machine cl makecluster no cluster type sock calculate the travel time required for direct runoff to reach the main outlet of the watershed from a single cell where the direct runoff is generated and then route or distribute direct runoff based on the travel time or isochrone calculated previously figs 3 5 and table 1 for rain time in 1 event length calculate the velocity of direct runoff from its depth on each of the individual cells fig 1 flow velocity cell flow velocity runoff depth cell size cell roughness cell slope convert the velocity of direct runoff to its travel time required to pass each cell fig 3 a travel time cell cell size flow velocity cell export the travel time and flow direction to the cluster or the multiple threads clusterexport cl c travel time cell flow direction split a watershed into multiple subwatersheds and assign the subwatersheds to the threads randomly then call and run a function travel time parallel to calculate the travel time of direct runoff on cells within each subwatershed assigned to each thread fig 3 b travel time subws parlapply cl clustersplit cl subws outlet travel time parallel export the subwatershed scale travel time and flow direction to the cluster clusterexport cl c travel time subws add up the travel time of direct runoff along the flow direction from the main outlet of the watershed to the individual cells at the watershed level figs 3 c and 4 travel time parlapply cl clustersplit cl subws outlet travel time subws acc export the travel time direct runoff depth and flow direction to the cluster to route or distribute direct runoff onto downstream areas clusterexport cl c travel time runoff depth flow direction split a watershed into multiple cells and assign the cells to the threads randomly then call and run a function runoff route parallel to distribute the direct runoff onto downstream cells identified based on the flow direction and travel time or isochrone fig 5 and table 1 runoff route parlapply cl clustersplit cl cell position runoff route parallel calculate the depth of direct runoff by adding the depth of routed direct runoff and the depth of rainfall runoff depth rain depth runoff route define the function travel time parallel travel time parallel function subws outlet split for subws outlet each in subws outlet split add up the travel time of direct runoff travel time cell to calculate the amount of time required to reach the subwatershed outlet subws outlet along the flow direction flow direction from individual cells fig 3 b define the function travel time subws acc travel time subws acc function subws outlet split for subws outlet each in subws outlet split add up the travel time of direct runoff travel time cell to calculate the amount of time required to reach the watershed outlet along the flow direction flow direction from individual cells figs 3 c and 4 define the function runoff route parallel runoff route parallel function cell position split for cell position each in cell position split distribute the amount of direct runoff runoff depth on a cell onto its neighboring downstream isochrone defined based on the travel time map travel time along the flow direction flow direction fig 5 and table 1 
25707,grid based spatially distributed hydrological modeling has become feasible with advances in watershed routing schemes remote sensing technology and computing resources however the need for long running times on a substantial set of computational resources prevents a spatially detailed modeling program from being widely used particularly in fine resolution large scale studies parallelizing computational tasks successfully mitigate this difficulty we propose a novel way to improve the simulation efficiency of direct runoff transport processes by grouping watershed areas based on a time area routing scheme the proposed parallelization method was applied to simulating the runoff routing processes of two watersheds in different sizes and landscapes the method substantially improved the computational efficiency of the time area routing simulation with common computing resources the efficiency of the parallelization was not limited by the hierarchical relationship between upstream and downstream catchments along the flow paths which could be possible with the lagrangian tracking of the time area routing method keywords time area routing distributed watershed model parallel computing shared memory parallelism stream network grid based simulation 1 introduction hydrological models have evolved from a set of organized or tabulated simple computational procedures for engineering design to thousands of computer code lines describing the interaction between surface water groundwater and the atmosphere as tools for scientific analysis clark et al 2017 singh 2018 studies now attempt to describe the continental scale and long term hydrological cycle clark et al 2015a coe 2000 trambauer et al 2013 wood et al 1997 yang and musiake 2003 ever improving remote sensing technology and computing resources have helped hydrological models to take much more details of the landscape into account chen and wang 2018 gleason et al 2018 kite and pietroniro 1996 qin et al 2008 xu et al 2014 several models describe internal transport processes rather than just predicting the overall responses of a watershed to a rainfall event fenicia et al 2016 freeze and harlan 1969 her and heatwole 2016 karssenberg 2002 yang et al 2000 process based approaches have gained popularity as they can provide a precise picture of hydrological processes and improve modeling quality by accommodating many different ways to calibrate parameters and evaluate performance clark et al 2015b fatichi et al 2016 montanari and koutsoyiannis 2012 shen and phanikumar 2010 however the computational burden of process based methods still encourages hydrologic modelers to simplify the representation of flow routing processes especially in large scale and long term simulations chaney et al 2016 clark et al 2015b 2017 fatichi et al 2016 freeze and harlan 1969 rouholahnejad et al 2012 overland flow routing is a hydrological process that controls many other processes including direct runoff generation soil moisture groundwater recharge and their spatial distributions engman and rogowski 1974 kim et al 2013 liu et al 2007 valeron and meixner 2010 thus overland flow simulation is a critical tool for process based hydrologic modeling and its accuracy there are several different approaches proposed to describe two dimensional overland flow routing processes based on hydraulics and hydrologic theories including solving a governing partial differential equation i e the st venant equation and its variants butts et al 2004 downer et al 2002 downer and ogden 2004 vieux et al 2004 assuming a steady state hillslope surface velocity ivanov et al 2004 wigmosta et al 1994 and using a dynamic time area method her and heatwole 2016 the numerical methods such as finite difference element methods fdm fem often require the use of a tiny time step e g seconds depending on the spatial resolution of a simulation to maintain numerical stability e g the courant condition such a limitation leads to inefficiency in the long term simulation such as for a climate change impact assessment borah and bera 2004 fread 1993 murillo et al 2006 moreover the constant flow velocity assumption is not realistic when considering the temporal variation of rainfall intensity and water transport processes along flow paths cantone and schmidt 2011 lee and yen 1997 saco and kumar 2002 the dynamic time area method provides an alternative way to describe the spatiotemporal changes of direct runoff velocity in the two dimensional simulation of overland flow routing her and heatwole 2016 2018 this method combines the classic time area watershed routing concept with the curve number cn methods modified for updating cns based on soil water content and considering the overlap and re infiltration of direct runoff routed from upstream areas parallel computing techniques have mitigated the computational burden of process based overland routing modeling and made spatially distributed simulation feasible maxwell et al 2015 studies have demonstrated a scalable parallel implementation of parflow surface and groundwater modeling for integrated large scale flow simulations kollet et al 2010 maxwell 2013 maxwell et al 2015 li et al 2011 and wang et al 2012 parallelized the subwatershed level simulation of hydrological models the digital yellow river model and xinanjiang model respectively by representing drainage networks with binary tree structures and passing upstream subwatershed modeling results to processors beginning simulations for downstream areas so that channel flow could be routed vivoni et al 2011 investigated the relationship between the number of processors catchment complexity and subwatershed partitioning methods in the parallel implementation of tribs modeling using the message passing interface mpi technique liu et al 2014 grouped subwatersheds by their stream orders or grid layers so that the subwatershed level simulations of a grid based distributed hydrological model was independent of each other in parallel computing liu et al 2018 also parallelized grid based hydrologic modeling at subwatershed and grid cell levels using mpi and shared memory programming techniques respectively hwang et al 2014 improved the computational efficiency of a three dimensional finite element model by parallelizing matrix manipulations zhang et al 2016 distributed parameter sets sampled by a genetic algorithm over the multiple nodes of a high performance computing hpc system and implemented subwatershed scale hydrologic simulations using multiple cpus of each node in a parallel way using an mpi technique le et al 2015 and vacondio et al 2017 employed a graphics processing unit gpu based parallelization algorithm to improve the computational efficiency of two dimensional shallow water modeling zhu et al 2019 proposed an openmp based computing technique that facilitates model level parallelization to improve the efficiency of sensitivity analysis and automatic calibration that require many model runs one of the common parallel computing strategies for surface water hydrologic modeling is to split an entire watershed into subwatersheds that do not interact or do not exchange surface water across ridgelines then a modeler can assign each available computing resource i e processor to a subwatershed and model internal processes happening within each subwatershed using the single processor once all the subwatershed level simulations are completed upstream modeling results can be fed to downstream simulation as a boundary condition in such routing processes upstream outputs cannot be simply added to downstream ones to determine the hydraulics of downstream flow because the responses of downstream hydraulics to different boundary conditions or flow routed from upstream channels and areas are nonlinear sivakumar and singh 2012 for instance manning s equation demonstrates that the velocity of flow is a nonlinear function of the hydraulic radius which is calculated based on the nonlinear relationship between discharge and channel dimension usace 1995 from a computational perspective due to the nonlinear relationship between velocity and discharge the velocity of the downstream flow cannot be determined until water transported from the upstream areas is added to the downstream flow in a unit time interval thus downstream routing simulations need to wait until all the associated upstream simulations are completed which can create a bottleneck in the computations and delay the overall simulation process a time area concept has emerged since clark 1945 introduced a way to synthesize unit hydrographs by translating an excess rainfall depth to a direct runoff hydrograph based on a time area histogram her 2011 maidment et al 1996 in the time area unit hydrograph theory the relationship between travel time and the corresponding drainage areas determines the shape of the direct runoff hydrographs such a unique and intuitive idea inspired additional studies that tried to relate the hydrological features of a watershed to its responses to rainfall events ajward and muzik 2000 du et al 2009 gyasi agyei et al 1996 maidment et al 1996 martínez et al 2002 melesse and graham 2004 muzik 1996 noto and loggia 2007 rodríguez iturbe et al 1979rodríguez iturbe et al 1979 saghafian et al 2002 her and heatwole 2016 employed a time area concept to develop direct runoff hydrographs by describing a two dimensional watershed runoff routing process using travel time calculated from flow hydraulics the time area routing method was successfully tested in an agricultural catchment virginia u s her and heatwole 2016 two mountainous watersheds south korea and brazil and a wetland woody area florida u s the efficiency of the time area routing method demonstrated its potential as an alternative to solving the saint venant equation using numerical methods for two dimensional watershed routing simulations the time area flow routing method proposed by her and heatwole 2016 and her 2011 distributes the amount of direct runoff generated on or routed onto the individual grid cells to their downstream isochrones on time area maps updated based on flow hydraulics in every time interval as the method traces the transport processes of direct runoff volume along its flow paths in the grid representation of a watershed it can be said that in the time area method direct runoff routing is simulated by a lagrangian approach flow tracking in an eulerian description of the flow hydraulics travel time grid maps in the lagrangian point of view direct runoff in a location is considered as an aggregation of fluid parcels generated on the location or transported from other areas and the marked volumes or depths of direct runoff generated on different locations individually move at the average velocities bear 1972 fischer et al 1979 thus the history of individual direct runoff can be tracked along its flow paths and the locations of moving direct runoff are represented as functions of time or travel time bear 1972 the lagrangian approach is commonly used to describe the advective transport processes of particles as it can avoid numerical dispersion in the application to advective dominated transport modeling zheng and bennett 2002 the lagrangian flow tracking provides a new way to investigate runoff routing processes for watershed management her and heatwole 2018 also this new routing approach allows the direct runoff of individual grid cells to be distributed to downstream areas independently in a simulation time interval this capability inspires us to explore parallel computing options for improved computational efficiency of two dimensional watershed simulations especially in large scale fine resolution flow modeling this study proposed a way to parallelize the time area routing method and tested its efficacy in two watersheds we investigated the relationship between the parallel efficiency measures stream network densities watershed areas cell sizes rainfall depths and the number of processors employed in this study the overland flow routing of a grid based watershed model hydrology simulation using time area method hystar was parallelized to demonstrate the implementation of the proposed parallel computing strategies we also examined bottlenecks that delay the overall simulation processes and discussed the potentials and limitations of the proposed parallel routing method in this article 2 overview of time area routing scheme the time area routing method represents a watershed as a group of isochrones varying over time the length of time it takes for direct runoff to pass an overland or channel cell is calculated based on local hydraulics characterized by runoff discharge ground surface slope and roughness of the cell manning s formula fig 1 then the calculated cell passing time is accumulated along the flow paths from the watershed outlet to each cell to determine the travel time required for the runoff to reach the outlet from the cell once travel time is determined for all the cells within a watershed travel time isochrones are updated and the amount of runoff passing the outlet in a time interval is calculated from the isochrones fig 2 in the time area method direct runoff routing simulation is implemented on the travel time maps or isochrones rather than on the velocity field of direct runoff and thus we can simply add up cell passing time from the outlet to individual cells to calculate the cumulative direct runoff travel time fig 3 in other words we can represent non linear flow routing processes with the linearly addable property i e passing or travel time of local flow on individual cells by projecting local hydraulics i e velocity onto the time domain in the grid representation of a watershed then the time area routing simulation can be parallelized regardless of the hierarchical relationship between upstream and downstream subwatersheds it is worth noting again that the travel time of a cell means the amount of time required for runoff generated on the cell to reach the outlet or the most downstream cell thus the travel time of a cell is calculated by accumulating the travel times of cells located downstream of the cell from the most downstream or the outlet of the watershed to its neighboring or immediate downstream cell along the flow path and the cell itself in other words the travel time is accumulated in the direction opposite to the flow direction for example water flows from a2 to a1 b2 to b1 a1 to c2 b1 to c2 and then c2 to c1 flow direction in fig 3 the travel time of 15 min from a2 to the subwatershed watershed 2 2 in fig 2 outlet i e a1 is calculated by adding 9 min required to pass a1 to 6 min required to pass a2 fig 3 a and b similarly the travel time of 113 min required to move from c2 to the outlet not shown in fig 3 but only in fig 2 of the subwatershed watershed 1 in fig 2 is determined by adding 4 min to 109 min required to reach the outlet from c1 fig 3 a and b finally the travel time of 122 min for a1 is calculated by adding 9 min to the travel time of c2 113 min the travel time of 128 min for a2 is the addition of 15 min a2 in fig 3 b and 122 min a1 in fig 3 c 2 1 hydrologic and hydraulic background in the grid based time area routing scheme the cell passing time of direct runoff is calculated based on the local flow hydraulics represented by manning s formula fig 1 which requires information about the depth of direct runoff created on each cell there are two sources of water coming into a cell the cell itself rainfall and its upstream cells routed water the calculation of direct runoff depth should consider both of these sources effectively in the hystar model the time area routing scheme is combined with the curve number methods which were modified from its original form to consider direct runoff routed to a cell as well as raindrops falling directly into the cell to determine the depth of direct runoff occurring in the cell in every simulation time interval fig 1 her and heatwole 2016 moglen 2000 the calculated depth of direct runoff is then fed into manning s equation and the mass balance equation to calculate the velocity of direct runoff passing through a cell and the amount of water infiltrating into the soil respectively fig 1 the soil water content of each cell is updated based on the water mass balance which is used to determine the curve numbers of the cells for the next simulation time step fig 1 the entire calculation procedures described above are repeated for every time interval so that the spatiotemporal variations of the direct runoff transport processes can be simulated 2 2 travel time isochrones and direct runoff hydrographs once the velocity of the direct runoff passing each overland or channel cell is determined the time required for the runoff to pass the cell can be calculated fig 1 here the length of the flow path is assumed to equal the length of a side or diagonal path of a cell then the passing time calculated is accumulated as moving from the outlet to each cell along the flow paths to determine the cumulative direct runoff travel time on which isochrones are mapped fig 2 the travel time isochrones can expand to upstream areas or shrink down to the outlet depending on the flow hydraulics calculated fig 2 demonstrates how travel time zones expand with the passage of time the direct runoff rate or the ordinate of the direct runoff hydrograph of a particular time interval i e 1 h is determined by adding up the amount of direct runoff that occurred in the 1 h travel time zone areas in blue in fig 2 the travel time maps are updated as the flow velocity changes in every time interval there may be some areas that do not contribute to the direct runoff at the outlet during a rainfall event due to their hydrologic characteristics such as permeable soils and shallow slopes as shown in fig 2 gray areas on the other hand the vicinity of the stream networks may have more opportunities to contribute to the downstream direct runoff hydrographs since channelized flow usually has a faster velocity than the shallow overland flow 3 parallel simulation strategy 3 1 travel time calculation there are two main computational procedures to implement for the time area routing scheme in two dimensional hydrologic modeling travel time calculation and direct runoff distribution in the time area routing scheme travel time can be calculated independently and simultaneously at the subwatershed scale which facilitates its parallelization fig 4 the travel times calculated for all the cells in each subwatershed can be accumulated with consideration of the hierarchical relationships between the subwatersheds along the stream networks fig 3 for instance if a watershed is split into seven subwatersheds and there are eight processors available for simulation a sequential routing scheme requires at least the number of iterations equal to the highest stream order i e 3 in the case of fig 4 a of the watershed to complete parallel simulation of the routing on the other hand the travel time calculation of the time area method can be completed in a single parallel computing iteration by evenly and randomly distributing the seven subwatersheds into seven of the eight processors cores or threads fig 4 b a set of pseudocodes is provided as supporting information to describe how the parallel simulation strategy was implemented with r at the end of this article when there are only two processors and each of the seven subwatersheds is assigned to one of them there are 21 different combinations of subwatersheds and processors 7 2 7 2 7 2 21 in the case of the parallel computing method fig 4 b if there are four processors available for the parallel computing one or two processors may have only one subwatershed assigned depending on how fast routing computations for the first four subwatersheds can be completed which will reduce the overall parallel computing efficiency thus the efficiency of parallel time area routing will depend on the number of subwatersheds and processors available as well as the size of a study watershed once the computation of direct runoff travel time is completed for individual subwatersheds the calculated travel time values need to be aggregated on a watershed scale in the time area routing scheme the velocity of direct runoff passing a cell is converted to the cell passing time by dividing the flow length of each cell by the calculated velocity then the cell passing time is accumulated from the outlet of each subwatershed to calculate the travel time required for the direct runoff to reach the outlet from individual cells figs 2 and 3 in the parallel travel time calculation the travel times required for runoff to pass individual cells are calculated first fig 3 a and the travel times are accumulated along the flow paths at the subwatershed scale fig 3 b these subwatershed level travel time calculations are implemented by multiple processors in parallel once the subwatershed scale travel times are obtained for all the cells they accumulate again from the watershed outlet to each of the subwatershed outlets along the stream network fig 3 for example the travel time of a direct runoff occurred in the most upstream cell of a downstream subwatershed is added to those of all the cells of its upstream subwatershed unlike the case of calculating cell passing time at a subwatershed scale such a travel time accumulation is sequentially implemented from the most downstream subwatershed to the most upstream one so that the travel time can indicate the total time taken for direct runoff to reach the main outlet from any point along its flow paths in a watershed 3 2 direct runoff distribution once a travel time map is updated the time area routing distributes direct runoff created in a cell to downstream areas in the current or next travel time zone depending on the spatial arrangement of the travel time zones or isochrones along the flow paths fig 5 and table 1 when the travel time of a zone in which a cell of interest is located is only 1 h i e simulation time interval longer than its neighboring downstream zone the two zones or isochrones are connected to each other so that direct runoff can move on to the downstream zone from the current or upstream one case 1 in fig 5 in this case the amount or depth of direct runoff occurred in a cell will pass the current travel time zone and then it will be evenly distributed to cells located in the downstream travel time isochrone along flow paths in the next simulation time step on the other hand when two adjacent zones are not connected to each other or the difference between the travel times of the two zones is greater than the simulation time interval of 1 h the direct runoff of the cell cannot go beyond the boundary of the travel time zone where it is located in the current time step in the next time step case 2 in fig 5 in such an instance the direct runoff of the cell is evenly allocated onto downstream cells in the current zone in the next time step as described earlier the calculation of direct runoff travel time can be parallelized on the subwatershed scale on the other hand the direct runoff distribution of multiple grid cells can be implemented simultaneously which can increase local data granularity e g the amount of computation to communication between processors and thus be more favorable to parallel computing compared to the case of travel time calculations in case 1 for instance 2 mm and 7 mm of direct runoff on the cells a and g are evenly distributed onto two cells d1 and e1 in its downstream isochrone in yellow fig 5 and table 1 in case 2 3 mm and 4 mm of direct runoff on the cells b and h are distributed onto their downstream cells c2 for b and f2 and g2 for h in the same isochrone in blue when seven processors are available thus the direct runoff depths on all seven cells a b c f g h and i in the current travel time zone or isochrone in blue are individually and concurrently routed onto their downstream cells d1 and e1 in case 1 b2 c2 f2 g2 and i2 in case 2 identified based on the flow direction and travel time similarly to the case of the travel time calculation there are 21 different combinations of cells rather than subwatersheds and processors when two processors are available the spatial scale i e cells of the parallelization for the direct runoff distribution is finer than that i e subwatersheds of the travel time calculation parallelization which is expected to bring differences in their efficiency 3 3 parallel computing environment the hystar model was written in r which provides several packages and options for parallel computing https cran r project org web views highperformancecomputing html hystar was designed as a tool to simulate hydrological processes in routine analysis and planning practices whose users may not have access to high performance computing facilities that usually have several hundreds of central processing units cpus connected in networks e g cores and nodes thus the mpi and open multi processing openmp approaches which are suitable for large computer networks with distributed or shared memory systems respectively were not considered in this study as a graphics processing unit gpu has received attention as an efficient computing resource for large scale calculations especially matrix manipulations parallel computing techniques that employ gpus are emerging however the use of gpu requires the development of computer codes for gpus that are different from cpu codes and hence hydrologic models written for gpu computing are not common yet also it is known that gpus are not efficient for the parallelization of irregular problems including trees and graphs e g drainage networks aurousseau et al 2009 burtscher et al 2012 in this study we explored several shared memory c f distributed memory parallelism strategies and techniques that allow multiple processors to share the same memory which is one of the simplest ways to parallelize hydrological routing using common computing resources such as a personal computer pc then we found the snow library of r satisfied our needs for parallel time area routing simulation 1 applicability of a shared memory approach that can be used with computing resources commonly used in hydrologic modeling 2 compatibility with the programming language i e r used to materialize the routing scheme and 3 ease of updating the codes later tierney et al 2009 4 experiment setup 4 1 numerical setup numerical experiments were designed to demonstrate the performance improvements that can be achieved by implementing the proposed parallel routing approach we investigated the relationship between the parallel computing efficiency and factors that are expected to influence the efficiency including watershed sizes spatial resolutions or cell sizes the density of stream networks storm sizes and the number of processors employed in the experiments the number of processors varied from 1 to 16 while the execution time was being monitored in addition a routing model was implemented in two watersheds that have different drainage areas and topography hp6 small steep and ordway swisher biological station osbs large shallow the routing modeling was also implemented at two different spatial resolutions 3 m and 10 m for hp6 and 10 m and 30 m for osbs in the experiment the stream networks were defined with the different sizes of threshold areas that are required to initiate stream networks in the watersheds 2 5 dense and 20 sparse her and heatwole 2016 the use of different stream network thresholds snts provided different numbers of subwatersheds and channel cells we also investigated how the depths of effective rainfall influence the model running time and efficiency fig 6 to make sure that the entire watershed can contribute to the direct runoff hydrography at the outlet in the experiment we simulated routing for a hypothetical storm event that produced 400 mm over 4 h the first 2 h of rainfall total 200 mm was used to saturate the soil layer and thus increase the amount of direct runoff to be routed in the experiment then the rainfall depths in the third and fourth hours were used to represent two rainfall events with different initial soil water conditions fig 6 a and b the numerical experiment was implemented on a computer that had two intel xeon processor e5 2680 v3 cpus and 256 gb of ram in the ubuntu ver 19 04 operating system each experiment was repeated ten times to get an average execution time and reduce the influence of random performance variation of the pcs on the results the speed up ratio and parallelization efficiency were measured to evaluate the parallel routing simulation strategy proposed in this study the speed up ratio s is defined as the ratio of the serial execution time t s to the parallel execution time t p as follows rauber and rünger 2013 s t s t p the parallel efficiency e is defined as the ratio of speed up to the number of processors p rauber and rünger 2013 e s p 4 2 example application areas and model preparation the study watersheds hp6 and osbs are located in south korea and florida usa respectively fig 7 the hp6 is a mountainous watershed with a drainage area of 3 79 km2 and an average slope of 17 9 the hp6 watershed mainly consists of forest 46 paddy 21 built up 23 and upland 6 and its predominant soil texture is loam the osbs watershed drains 133 59 km2 and it has gentle slopes from 0 to 5 average 2 8 on sandy soil the osbs watershed is covered by forest 32 shrubland 30 grass 10 and wetlands 10 the geospatial layers including digital elevation models dems land cover and soil maps were obtained from various sources the korean national geographic information institute ngii for the dem of hp6 the korean ministry of environment moe for the land cover of hp6 the korean rural development administration rda for the soil of hp6 the united states geological survey usgs for the dem of osbs and the united states department of agriculture usda for the land cover and soil of osbs the topographic characteristics of the watersheds such as slope and streamflow networks were derived from the dems table 2 the soil characteristics including texture hydrological soil groups depth field capacity and hydraulic conductivity size were derived from the detailed soil map hp6 and the soil survey geographic database ssurgo osbs the land cover information was extracted from the level ii land cover map hp6 and cropland data layer cdl osbs the input parameters such as curve number roughness coefficients and crop coefficients were obtained from the literature allen et al 1998 neitsch et al 2011 reybold and teselle 1989 the original dems that were prepared at the resolutions of 10 m hp6 and 30 m osbs were interpolated at 3 m hp6 and 10 m osbs respectively using the spline method her et al 2015 slope and stream networks were then derived from the interpolated dems and provided to the routing simulation the categorical data of the land use maps were resampled at finer resolutions using the nearest neighbor algorithm 5 application and results in the case of calculating the travel time in the osbs watershed the computational cost represented by clock time decreased with increases in the number of processors up to 4 to 8 dense and 4 sparse and then it started increasing when processors more than eight were employed fig 8 and table 3 a pattern similar to that of the osbd case was found in hp6 3m and the minimum computational cost was found when 4 to 8 processors were deployed fig 8 and table 3 the speed up ratios exhibited patterns similar to those of the computational cost or clock time the speed up ratio reached its maximum values with 4 to 8 dense and 4 sparse processors in osbs 10m and osbs 30m and 8 dense and 4 sparse processors in hp6 3m in the case of hp6 10m however the computational cost proportionally increased and the speed up ratio decreased when increasing the number of processors from 1 to 16 fig 8 and table 3 the inefficiency might be attributed to the fact that the sequential simulation was fast enough the sequential computing took only 0 4 0 5 s at most to complete the travel time calculation for hp6 10m due to the small number of cells and computational load such a finding implies that parallel computing may not be efficient for small scale simulation the result also suggests that the local workload should be larger than the communication cost e g high data granularity to see tangible efficiency gains from parallel computing the computational cost and efficiency were not substantially affected by the size of rainfall in the case of travel time calculation which is expected because the rainfall depths do not change the number of cells or calculation units but only the size of figures to be calculated i e travel time the execution time required to complete the simulation of direct runoff routing decreased overall with increases in the number of processors used in parallel computing fig 9 and table 3 for instance in the case of osbs 10m d the speed up ratio increased from 1 97 to 6 36 while increasing the number of processors from 2 to 16 under the large storm scenario the computational cost and efficiency were responsive to the rainfall depths which is different from the case of the travel time calculation deeper rainfall depth is likely to make two adjacent isochrones travel time zones connected to each other which creates longer direct runoff connectivity and thus longer routing paths across neighboring isochrones in fig 10 for instance the s2 storm that has an additional 100 mm rainfall in the fourth hour from its beginning compared to the s1 storm fig 6 a and b created larger areas or isochrones connected to the outlet of hp6 in the fourth hour to the end of the storm two isochrones are regarded as connected to each other when the difference between their travel times is equal to or less than an hour i e the simulation time interval in addition an isochrone is considered to be connected to the outlet if it is connected to a series of other downstream isochrones eventually connected to the outlet or the 1 h isochrone in the fourth hour of the s1 and s2 events the isochrones connected to the outlet of hp6 cover 24 s1 and 100 s2 of the drainage areas of hp6 the difference is 76 respectively in the fifth hour the coverage difference decreased to 8 15 for s1 and 23 for s2 as both s1 and s2 events do not have any additional rainfall such a comparison demonstrates the effect of additional rainfall on the direct runoff connectivity and routing path length therefore a large storm event will require a longer time to implement direct runoff routing overall the results show that the parallel computing methods for direct runoff routing and travel time calculation become more efficient for relatively large computational loads such as for smaller spatial resolutions denser stream networks and deeper rainfall depths in the case of hp6 10m we could not find any efficacy of the parallel computing methods and the speed up ratios were less than 1 0 regardless of the cell sizes and stream network densities fig 9 such a result was also found in the case of travel time calculation fig 8 the serial execution of the direct runoff routing takes a short time the computational costs or clock times were only 0 5 0 6 s at most to be completed which was not substantially large compared to the overhead time the computational costs were 0 1 0 2 s table 3 as the number of processors increased parallel computing efficiency gradually decreased in most cases such a finding implies that more computational resources were directed to scheduling multiple tasks i e the overhead rather than executing the routing simulation itself when the number of processors employed increased liu et al 2014 the results demonstrate how sensitive the parallel computing efficiency to the data granularity in the case of hp6 10m for instance the number of overland cells on which the calculation was implemented was smaller than those of the other cases only 2 8 9 0 and 25 of the overland cell numbers of osbs 10m hp6 3m and osbs 30m respectively while the numbers of subwatersheds remained similar to each other table 2 assuming the communication cost was kept relatively unchanged the granularity could decrease proportionally to the relative sizes of the overland cell numbers e g 2 8 compared to the case of osbs 10m generally the parallel computing methods provided relatively higher efficiency in the cases of obsb than hp6 when the same number of processors were employed in parallel figs 8 and 9 these results are mainly attributed to the differences in the computational loads or the number of overland cells where the computations are performed and the level of data granularity however the data granularity could not explain all and the hydrological and landscape characteristics of the watersheds might affect the efficiency the impact of cell sizes the number of overland cells rainfall depths and stream network densities on speed up or efficiency were compared when employing the number of processors that provided the overall best performance in the direct runoff routing 16 processors and travel time calculation 4 processors fig 11 in the case of direct runoff routing the speed up ratios of osbs were higher than those of hp6 even when the numbers of overland 420 900 and channel 2 447 cells of hp6 3m were larger than those 148 438 and 1 253 of osbs 30m table 2 fig 11 such a finding implies that parallelization of direct runoff routing might be more useful for a watershed that has a low relief than a mountainous topography the average slopes of osbs and hp6 are 2 8 and 17 9 respectively the travel time of direct runoff is longer on shallow slopes compared to that of steep ones when assuming the same soils and land covers then the longer travel time tends to create more isochrones or travel time zones and thus requires a longer time to complete the redistribution or routing of the direct runoff this result indicates that topography and flow hydraulics as well as the number of cells or spatial resolution influence the performance of the parallel direct runoff routing methods such a finding can be explained by the characteristics of the lagrangian flow description or the material coordinate system employed by the time area routing method which tracks the detailed route of direct runoff depth generated on a cell li et al 2018 salamon et al 2006 overall the speed up ratios increased as the cell size decreased the amount of time spent to complete the assigned calculation increased with increases in the sizes of the assignment which led to higher data granularity and relatively smaller communication overhead and thus better parallelization performance liu et al 2014 2016 in the cases of osbs with the relatively small rainfall depth s1 however osbs 10m showed higher speed up ratios than osbs 30m such a result is associated with the watershed slopes the original 30 m dem of the osbs watershed was interpolated into the 10 m dem using the spline method which decreased the average slopes from 2 80 osbs 30m to 2 08 osbs 10m as described previously shallow slopes create more isochrones and thus require longer computing time than do steep ones the result might also be related to the soil characteristics of the watersheds loamy hp6 and sandy osbs sandy soil has higher infiltration capacity than does a loamy one which may make the osbs watershed have a relatively large portion of unsaturated areas even after direct runoff routed from upstream areas are added compared to the hp6 watershed especially when the depth of rainfall is relatively small s1 fig 11 the unsaturated areas create more discontinuous isochrones or low direct runoff connectivity which require more time to complete direct runoff distribution than do the saturated areas in the case of travel time calculations the speed up ratios or efficiency were higher with the dense stream networks than the sparse ones fig 11 the denser networks are likely to create continuous isochrones as concentrated flow in the stream tends to have faster velocity than sheet flow on overland areas the efficiency of the parallel direct runoff routing was not responsive to the density of stream networks in the case of simulating direct runoff routing in hp6 where sequential computing can be done quickly as much as the overhead time of the parallel computing this study compared the computational costs required to complete the travel time and direct runoff distribution for a year using the proposed parallel computing methods with different numbers of threads to demonstrate the efficacy in the practice of hydrological modeling fig 6 c and d and table 4 similar to the cases of the hypothetical rainfall fig 6 the parallelization could substantially save computational time for the direct runoff distribution in long term hydrological modeling in the application to simulating the hydrological processes of hp6 during 1998 and osbs during 2018 the parallelization reduced the computational costs of the direct runoff distributions by 80 e g 7 42 h vs 1 48 h in hp6 3m d and 75 48 57 h vs 11 74 h in osbs 10m d using 16 threads compared to the case of the sequential calculation or using only one thread the overall model running time decreased as much as the amount of computational costs saved by parallelizing the direct runoff distribution for example the running time of 10 29 h was reduced to 4 85 h in the case of hp6 3m d when 16 threads were employed in the parallel direct runoff distribution and the difference of 5 44 h 10 29 h 4 85 h is similar to the difference of 5 94 h between the total running times of 7 42 h sequential and 1 48 h parallel with 16 threads similar observations were made in the cases of osbs regardless of the number of threads used simulations other than the direct runoff distribution and travel time required the relatively constant amount of time e g around 2 7 h 4 85 h 1 48 h 0 63 h in hp6 3m d and 33 3 h 53 70 h 11 74 h 8 68 h in osbs 10m d such findings indicate that the overall computational efficiency of the long term modeling can be efficiently improved by parallelizing the direct runoff distribution in the case of the travel time calculation the parallel computing did not help save time but even increased the computational costs especially in hp6 where sequential computing was fast enough to complete travel time accumulation in less than an hour table 4 this implies that the parallelization may not be efficient when the communication overhead of parallel computing is relatively large compared to the amount of time required to sequentially implement the computational loads the result also indicates that the overhead time may increase with increases in the number of threads used in the parallel computing the overall computational time for the entire modeling decreased until the number of threads increased to eight and then increased when 16 threads were employed which made the case of using eight threads to complete the entire modeling relatively fast in 51 69 h compared to the others in osbs 10m d such findings demonstrate the relationship between communication overhead and computational efficiency which may lead to decrease in the overall performance with increases in the size of a thread pool overall the comparison suggests the two dimensional runoff transport simulation can be more efficiently improved by focusing on the direct runoff distribution when the size of a watershed is small 6 discussion our parallel formulation of two dimensional time area watershed routing consists of three steps 1 partitioning the study area into multiple non overlapping areas 2 computing the time area routing for each non overlapping area independently and parallelly and 3 combining the results of these smaller areas to produce the optimal watershed routing simulation for the study area one of the critical ideas of our approach is to use a spatial data partitioning scheme to improve the efficiency of parallel processing in the experimental design we partitioned the study areas into smaller groups of subwatersheds and cells and evenly distributed the groups onto multiple processors since the individual tasks assigned to the processors are independent of each other the output of the parallel processing can guarantee the optimality therefore the proposed parallelization approach is scalable for sizable time area runoff routing the second key idea is to increase the local data granularity to improve the degree of concurrency of local tasks without losing solution quality in our analysis we saw that the performance of parallel computation represented by speed up and efficiency increased as the size of the watershed increased it is important to note that the proposed approach can adjust the cell size and balance the workload for the parallel computation we used the fine grained cells to partition the study watershed and evenly distribute the cells to local machines for improved load balance since the tasks are independent of each other the computational loads can be balanced to further improve the efficiency of parallel processing it should be worth exploring new spatial data partitioning techniques that can balance the workload and efficiently distribute subwatersheds across multiple machines the experimental results show that the proposed parallel formulation can efficiently partition both data and tasks and significantly speed up the computation of two dimensional time area watershed routings future work will evaluate the efficiency of a parallel formulation using distributed computing environments apache mapreduce and apache spark have received growing attention for large spatial data sets these big data processing platforms utilize large sized memory and disk storage to process sizable spatial datasets e g the entire basin of the mississippi river a parallel formulation for the big data processing platforms raises several interesting research questions big data processing platforms may not be scalable when the communication cost is high because the network i os or communication will be the main bottleneck to remedy this issue the communication cost should be minimized or data granularity should be increased for instance we can minimize the size of task results or outputs e g travel time and direct runoff depth maps and reduce the size of data to be delivered to the master processor thus the next study will explore data reduction techniques and evaluate their efficiency in representing the outputs using minimal description formats as some of the local data in tasks are invariant asynchronous communication techniques are expected to reduce the network i os without losing solution quality 7 conclusions this study proposed a new parallel computing strategy for the time area watershed routing to improve the efficiency of two dimensional direct runoff transport simulation and evaluated its efficacy in two watersheds having different sizes and landscapes the results suggested that the new method could substantially speed up the direct runoff distribution with common computing resources and environments the parallelization was much less efficient sometimes even counterproductive for the travel time calculation than the direct runoff distribution due to its relatively low local data granularity the speed up of the parallel computing increased with increases in the size of computational loads represented by the size of watersheds and subwatersheds or local data granularity storm events and grid cells the speed up did not always increase with an increase in the number of processors used depending on the size of computation relative to that of the communication overhead this study demonstrated that the efficiency of the parallel routing was a function of watershed landscape and resulting flow hydraulics as well as computing resources which was attributed to the feature of the lagrangian approach the time area method as the lagrangian tracking approach enabled direct runoff routing from individual grid cells to be independent of each other and thus increased the efficiency of parallel computing such a finding suggests the potential of parallel computing for improved efficiency of a lagrangian simulation approach software data availability name of software hystar hydrology simulation using time area method programming language r operating system windows linux and macos hardware requirements depend on the size of a watershed to be modeled and the spatial resolution or cell size of modeling a computer with multiple cores or threads and at least 8 gb ram is preferred for parallel computing software required rstudio https www rstudio com and the snow r package for parallel computing https cran r project org web packages snow index html version 0 0 2 as of april 2021 availability anyone can download use and share the model source code with data and files prepared for the study watersheds presented in this article her y g 2021 hystar hp6 hydroshare http www hydroshare org resource 6227021dde6d4c5bae7ec9630a9bb23d her y g 2021 hystar osbs hydroshare http www hydroshare org resource 9fa39b142f6c4505bc64888b724d29a1 license creative commons attribution nocommercial sharealike cc by nc sa https creativecommons org licenses by nc sa 4 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the usda national institute of food and agriculture hatch project fla trc 005551 and mcintire stennis project fla trc 005658 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105222 pseudocodes for the parallelization algorithms call the snow package if the parallel routing option is selected library snow specify the number of threads to be used e g 4 threads will be used in this example no cluster 4 create a sock cluster on the local machine cl makecluster no cluster type sock calculate the travel time required for direct runoff to reach the main outlet of the watershed from a single cell where the direct runoff is generated and then route or distribute direct runoff based on the travel time or isochrone calculated previously figs 3 5 and table 1 for rain time in 1 event length calculate the velocity of direct runoff from its depth on each of the individual cells fig 1 flow velocity cell flow velocity runoff depth cell size cell roughness cell slope convert the velocity of direct runoff to its travel time required to pass each cell fig 3 a travel time cell cell size flow velocity cell export the travel time and flow direction to the cluster or the multiple threads clusterexport cl c travel time cell flow direction split a watershed into multiple subwatersheds and assign the subwatersheds to the threads randomly then call and run a function travel time parallel to calculate the travel time of direct runoff on cells within each subwatershed assigned to each thread fig 3 b travel time subws parlapply cl clustersplit cl subws outlet travel time parallel export the subwatershed scale travel time and flow direction to the cluster clusterexport cl c travel time subws add up the travel time of direct runoff along the flow direction from the main outlet of the watershed to the individual cells at the watershed level figs 3 c and 4 travel time parlapply cl clustersplit cl subws outlet travel time subws acc export the travel time direct runoff depth and flow direction to the cluster to route or distribute direct runoff onto downstream areas clusterexport cl c travel time runoff depth flow direction split a watershed into multiple cells and assign the cells to the threads randomly then call and run a function runoff route parallel to distribute the direct runoff onto downstream cells identified based on the flow direction and travel time or isochrone fig 5 and table 1 runoff route parlapply cl clustersplit cl cell position runoff route parallel calculate the depth of direct runoff by adding the depth of routed direct runoff and the depth of rainfall runoff depth rain depth runoff route define the function travel time parallel travel time parallel function subws outlet split for subws outlet each in subws outlet split add up the travel time of direct runoff travel time cell to calculate the amount of time required to reach the subwatershed outlet subws outlet along the flow direction flow direction from individual cells fig 3 b define the function travel time subws acc travel time subws acc function subws outlet split for subws outlet each in subws outlet split add up the travel time of direct runoff travel time cell to calculate the amount of time required to reach the watershed outlet along the flow direction flow direction from individual cells figs 3 c and 4 define the function runoff route parallel runoff route parallel function cell position split for cell position each in cell position split distribute the amount of direct runoff runoff depth on a cell onto its neighboring downstream isochrone defined based on the travel time map travel time along the flow direction flow direction fig 5 and table 1 
25708,tough3 is a powerful numerical tool that simulates the heat and fluid flows in porous and fractured media however setting up the mesh file for a model with several rock types is tedious and error prone this study presents a matlab code for converting arcmap three dimensional conceptual models into tough3 input files this code builds a rectangular computational mesh using the meshmaker tool and a text file containing the arcmap geological model information the meshmaker tool performs the discretization of the model and generates the coordinates the code reads the coordinates and rock types file to rewrite a mesh file with the correct assignment of the various rock types required to build the model for tough3 the modular structure of this code provides a useful framework for rewriting mesh the versatility and capabilities of the matlab code developed herein are demonstrated on a complex conceptual model of the los humeros geothermal reservoir in puebla mexico keywords geothermal reservoir tough3 three dimensional model los humeros matlab software availability software availability name of software threedmesh to tough developer and contact information josé alonso aguilar ojeda contact address carr transpeninsular 3917 u a b c 22 860 ensenada baja california mexico telephone number 55 646 175 0700 e mail alonso aguilar uabc edu mx year first available 2021 hardware required windows pc software required matlab and tough3 availability and cost free program language matlab program size 5 kb the threedmesh to tough program is an open source code and can be obtained from the web site https gist github com alonso a 1ea701f386238cec38c7f5c1ef2d8da8 or from the corresponding author for use in many conceptual models 1 introduction the construction of a conceptual model is fundamental to simulating geothermal reservoir behavior bundschuh and suarez 2010 the closer the conceptualization of the model to reality the better are the model results in representing the behavior of the system this conceptual model must also be transformed into a numerical model for analysis in the numerical model space is discretized into cells and during the transformation process the number and size of cells chosen to be used in the model is an important consideration if small volume cells are used the model can represent the distribution of rock types more accurately but the number of cells will be huge and the time to solve the problem will also increase significantly while if the model cells have a moderate volume the simulation time will be short however these larger cells may not accurately represent the distribution of the rock types of the real geothermal field complex and fine scale models enhance prediction accuracy but demand more computational resources khait and voskov 2018 the construction of a three dimensional model requires finding a balance between the number of cells and their size tough3 is a nonisothermal multi phase and multi component model that simulates heat and fluid flow in porous and fractured media jung et al 2017 performing a simulation in tough3 requires an infile file with the following data blocks each identifed by a five character code a rocks defines the properties of the rocks namely material name rock type rock grain density kg m3 default porosity void fraction absolute permeabilities along the three principal axes formation heat conductivity under fully liquid saturated conditions and rock grain specific heat b multi specifies the number of fluid components and number of governing equations c gener defines information on sources and sinks d param specifies computational parameters such as time step and convergence parameters program options and predetermined initial conditions e solvr includes parameters for the linear equation solver f eleme provides a list of grid blocks volume elements g conne gives a list of flow connections between grid blocks h incon lists initial condition list for all grid blocks and i endcy is the last record to close the tough3 input file and start the simulation the data blocks that define rock properties and geometric configuration of the model are rocks eleme and conne tough3 uses its meshmaker tool for building the eleme and conne data blocks for a three dimensional model moridis 2016 however the model created with this tool assumes that all its cells are of the same rock type therefore it is necessary to manually modify the rock type for each cell which becomes a very tedious task when the number of cells and or rock types is large there is some commercial software that helps with the construction of three dimensional geological models that include several rock types among them leapfrog allows the rapid construction of a 3d conceptual model directly from dispersed boreholes data and geographic information system gis data aranz geo limited 2014 mview and re studio are numerical tools used in pre and post processing of tough2 models avis et al 2012 lawrence berkeley national laboratory 2021 while petrasim is a graphical interface for pre and post processing of tough2 models thunderhead engineering 2018 however the user licenses for these software are expensive and they are only compatible with tough2 in addition there exist public domain software such as addbound and assignrock that help to change the rock type in a mesh file a file that contains information blocks eleme and conne nevertheless they require that cells in which the rock type needs to be changed must be in a specific box or a polygonal prism there is also imattough tran et al 2016 but this also requires has the condition that cells in which the rock type needs to be changed must be in a specific box or a polygonal prism however for some study areas boxes or prisms cannot represent the complex geological formations in a real geothermal reservoir in addition there is a python interface developed at the university of auckland for converting leapfrog geological models into tough2 numerical models o sullivan et al 2019 however the model must be created in leapfrog a software that requires a user license on the other hand there is also pytough library croucher 2011 which can define various rock types for the aquifers and confining layers assigning the appropriate rock type to each element simply based on the elevation of each block however for a complex distribution of these materials some mathematical function would have to be found that defines the elevation of the materials and this becomes more complex than assigning the rock types manually croucher 2015 the open source gui graphical user interface called tim yeh et al 2013 uses the same geometry file as mulgraph o sullivan and bullivant 1995 for define the model grid which is a difficult task when it is trying to build the model grid to represent a complex distribution of the geology taking into account the above it is a difficult task to set up models representing the complex geological formations in a real geothermal reservoir using a software that is compatible with tough3 and does not have an expensive user license in addition software is not available that does not require that cells in which the rock type needs to be changed must be in a specific box therefore the aim of the present study was to develop a program capable of representing a comprehensive and morerealistic numerical model for a geothermal system as it was done by other researchers in their respective study areas verma et al 2008 li et al 2018 criollo et al 2019 r s gonçalves et al 2020 pham et al 2020 dehina et al 2020 carlini et al 2020 this study presents a code developed to adapt arcmap three dimensional conceptual models built of layers and convert them into input files for tough3 the code was developed in matlab a high performance language for technical calculations which is both an environment and a programming language one of matlab s strengths is that it allows the building of customised reusable tools casado fernández 2006 2 program overview it is common to have a hydrogeological conceptual model of the study area composed of surfaces each defining the separation between the geological strata for example the first top surface might be the digital elevation model topography which defines the separation between the atmosphere and the first geological strata the second surface will define the separation of the first and second geological strata and so on until the bottom of the model is reached the first step in building an arcmap model composed of layers numerically compatible with tough3 is to build a mesh uniformly distributed on x y axes on the arcmap model with the create fishnet tool when using this tool a window is displayedwhere the size and number of cells should be defined it is also essential to check the option to create label points which creates a point on each cell center so we will be able to input the elevation data for each layer of the hydrogeologic conceptual model in addition it is important to define the number of cells and their size taking into account that in the process to convert from curved and irregularly shaped surfaces arcmap model to sets of regular cells tough3 model resolution may be lost since some cells in the model may contain morethan one material however the program will assign to each cell the rock type id of the material found in the coordinates of each cell center independently of whether the layers on the arcmap model are horizontal vertical or inclined then it is necessary to export the elevation information of the model layers for each point in a txt file format this txt file summarizes the conceptual model and it will be used by the program developed herein to create a file compatible with tough3 the next step consists of running the matlab code which will ask the user for the meshmaker input the number of cells on the x axis and their size in meters then it requests the same data for the other two dimensions y and z axes although tough3 has the ability to work with radial and rectangular meshes so far our program is focused only on rectangular meshes then the code asks about the number of layers in the arcmap geologic model and the elevation at the model s highest point this elevation is used to reference the arcmap model coordinates with the coordinates created by meshmaker as the coordinates of the z axis go from zero downwards at this part of the code if you input the water table elevation of your model you will have a delimited model that discards the upper layer corresponding to the unsaturated zone it is important to mention that horizontal and vertical geological strata geological faults and intrusions or discontinuous strata can be part of the geological model in order to consider the physical setup of this type of geological strata and structures during the modelling it is necessary to define all layers and surfaces that comprise the model as continuous for example to consider a horizontal geological stratum there must be define two surfaces with different elevations each one at the edges i e the stratum is located between them if a discontinuous stratum is analyzed the base and upper surface corresponding to the stratum limits must have the same elevation to define the gap in this way the elevations difference between both surfaces limits is equal to zero thus the code will interpret the stratum where the elevations difference between the base and upper surfaces is different to zero all this information allows the code to create an input file infile with instructions to build a mesh and execute tough3 inside the active folder additionally a mesh file will be generated with the geometry entered by the user the matlab code imports and reads the tough3 mesh file and the txt file format exported from arcmap containing the conceptual model afterward the code reads all the cell center coordinatesand compares them against the topographic elevations and geological strata to assign the corresponding rock type id for the stratum in order to assign the rock type id the code starts by comparing the coordinates values of the selected cell center with the topographic elevation if the coordinates of the cell are above the ground surface the code assign the number 1 to the rock type column for that cell while if the coordinates values are below the topographic elevation and above the first stratum elevation a value of 2 is assigned to the rock type column for that cell and so on this procedure is repeated until the last cell of the model is reached after the code has finished the classification of all model cells it creates a new mesh file that contains the correct rock type column entry for each cell in the model the flowchart of the code developed is presented in fig 1 the topography is matched up with the upper part of the model by mean of the cells that are defined with the rock type corresponding to the atmosphere properties if the user prefer can delimit his model and discard the entire upper layer that corresponds to the unsaturated zone which include to the atmosphere and topography after executing the code and obtaining the mesh with the rock types distributed as is desired in the conceptual model it is necessary to copy the mesh file and use it in the tough model which require to have load the initial and border conditions as well as the number of fluid components sources and sinks time step and convergence parameters among others the matlab code described here only handles to adapt 3d conceptual models from arcmap to tough3 3 application to real data the los humeros geothermal field lhgf was selected to test the correct execution of the code developed herein the field is located in the eastern central part of mexico within the limits of the states of puebla veracruz and tlaxcala fig 2 at the eastern edge of the region known as the trans mexican volcanic belt that crosses the entire country in the we direction at the intersection with the sierra madre oriental the lhgf is the third largest geothermal field in mexico norini et al 2019 ithas an installed capacity of 119 8 mwe gutiérrez negrín et al 2020 and is estimated to be at an elevation of about 3000 masl barragán et al 2016 it is estimated that the geothermal field reservoir covers an area greater than 400 km2 and the maximum temperature of the reservoiris around 350 c diaz martos 2018 the exploitation of the deep thermal aquifer for commercial purposes began in 1990 arellano et al 2000 the defined study area has a square shape covering a grid of 20 km 20 km which comprises the whole of los humeros caldera and an area of 400 km2 4 methods six geologic cross sectionsof the study area reported by the comision federal de electricidad cfe 2014 were digitized and georeferenced in an arcmap project these cross sections were analyzed considering 136 points distributed over them located 1000 m away from each other fig 3 show three of the six geologic cross sections the depth of each geological strata was interpolated using kriging thus obtaining geological surfaces throughout the model and a three dimensional geological model a total of 1600 points on a regular x y grid were digitized in the georeferenced model using the create fishnet tool to build the mesh file created by meshmaker containing all the cells that comprise the conceptual model each point on the model surface was located at the center of the corresponding 1600 cells the topographic elevation and the elevations of each one of the geological surfaces were obtained using the extract multi values to points tool for all cell centres in the x y plan this information is compiled and exported as a table in a txt file format as shown in table 1 with 1600 points and n 8 rock types plus the topographic surface the table contains 1600 8 1 14 400 entries this file summarizes the conceptual model but it is not compatible with tough3 using a matlab code a file compatible with tough3 was built as input to the matlab code it is necessary to specify the number and dimensions of the cells along each axis of the tough grid to be created with the meshmaker tool for this study case the x axis contains 40 cells 500 m in length the y axis contains 40 cells 500 m in length and the z axis contains 40 cells 100 m in length this information was stored in the infile file as shown in fig 4 thus the model was built with a spatial discretization of 40 horizontal layers of constant thickness divided into a 40 40 mesh in the x and y axes the resulting mesh spacing is 0 5 km 0 5 km 0 1 km in the x y and z directions respectively the matlab code builds the infile file and executes tough3 when tough3 is started up it will look for the infile file and it will execute the commands contained in it and so the mesh file will be created fig 5 the mesh file contains 64 000 elements each one with its own name rock type volume and three coordinates x y and z of its cell center but in the first version of the mesh file all of these elements have the same rock type because the meshmaker tool only assigns the same rock type then the matlab code imports and reads the mesh file and the txt file format that was exported from arcmap containing the conceptual model it reads the center coordinates of all cells and compares them against the topographic elevations and geological strata to assign the rock type id corresponding to the stratum in the arcmap conceptual model after the code has finished evaluating all the cells in the model it creates a new mesh file that now contains a distinct rock type id in the column 20 with the correct values for each cell in the model fig 6 5 results and discussion the rock type distribution in the new mesh file built with the matlab code was plotted to compare it with the initially digitized geologic cross section data fig 7 a the plots show good correspondence between both information types those initially digitized and plotted with data of the new mesh file built with this matlab code fig 7b shows the geologic cross sectionobtained with the program presented herein as fig 7b shows strata of the hydrogeological conceptual model adapted to tough3 present a lower resolution when representing sloping strata clearly the complex hydrogeological structures commonly occurring in geothermal reservoirs can be better represented by reducing the cells size because the model resolution is inversely proportional to the cell size used for the discretization however based on the cell size consider for this model it can be seen that the most significant characteristics of the hydrogeological conceptual modelincluded in arcmap project are well represented in addition as was done this study the user can delimit the model and discard all the upper layer that represents to the unsaturated zone which can be very useful for case studies where the topography is very rugged and it is desired to input the surface temperature of the soil in the model as a condition of the upper limit 6 conclusions the matlab code developed and presented herein threedmesh to tough facilitates the construction of three dimensional models for tough3 this code was described here in addition as a case study its application to the hydrogeological conceptual model of the lhgf is discussed which is a three dimensional heterogeneous geothermal reservoir with a complex distribution of up to nine different rock types including the atmosphere composed of eight slanted faulted and deformed layers as is common in this type of environments and with a mountainous topography in spite of that complexity the results showed that the matlab code developed here built a geological model for tough3 with good correspondence to the hydrogeological conceptual model constructed in arcmap thus this tool allows not just the exporting of conceptual models of complex geothermal reservoirs constructed easily in arcmap into an input file for tough3 but also executes tough3 and using meshmaker build a rectangular mesh that reduces the time effort and error prone process of configuring and modifying the original mesh for tough3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25708,tough3 is a powerful numerical tool that simulates the heat and fluid flows in porous and fractured media however setting up the mesh file for a model with several rock types is tedious and error prone this study presents a matlab code for converting arcmap three dimensional conceptual models into tough3 input files this code builds a rectangular computational mesh using the meshmaker tool and a text file containing the arcmap geological model information the meshmaker tool performs the discretization of the model and generates the coordinates the code reads the coordinates and rock types file to rewrite a mesh file with the correct assignment of the various rock types required to build the model for tough3 the modular structure of this code provides a useful framework for rewriting mesh the versatility and capabilities of the matlab code developed herein are demonstrated on a complex conceptual model of the los humeros geothermal reservoir in puebla mexico keywords geothermal reservoir tough3 three dimensional model los humeros matlab software availability software availability name of software threedmesh to tough developer and contact information josé alonso aguilar ojeda contact address carr transpeninsular 3917 u a b c 22 860 ensenada baja california mexico telephone number 55 646 175 0700 e mail alonso aguilar uabc edu mx year first available 2021 hardware required windows pc software required matlab and tough3 availability and cost free program language matlab program size 5 kb the threedmesh to tough program is an open source code and can be obtained from the web site https gist github com alonso a 1ea701f386238cec38c7f5c1ef2d8da8 or from the corresponding author for use in many conceptual models 1 introduction the construction of a conceptual model is fundamental to simulating geothermal reservoir behavior bundschuh and suarez 2010 the closer the conceptualization of the model to reality the better are the model results in representing the behavior of the system this conceptual model must also be transformed into a numerical model for analysis in the numerical model space is discretized into cells and during the transformation process the number and size of cells chosen to be used in the model is an important consideration if small volume cells are used the model can represent the distribution of rock types more accurately but the number of cells will be huge and the time to solve the problem will also increase significantly while if the model cells have a moderate volume the simulation time will be short however these larger cells may not accurately represent the distribution of the rock types of the real geothermal field complex and fine scale models enhance prediction accuracy but demand more computational resources khait and voskov 2018 the construction of a three dimensional model requires finding a balance between the number of cells and their size tough3 is a nonisothermal multi phase and multi component model that simulates heat and fluid flow in porous and fractured media jung et al 2017 performing a simulation in tough3 requires an infile file with the following data blocks each identifed by a five character code a rocks defines the properties of the rocks namely material name rock type rock grain density kg m3 default porosity void fraction absolute permeabilities along the three principal axes formation heat conductivity under fully liquid saturated conditions and rock grain specific heat b multi specifies the number of fluid components and number of governing equations c gener defines information on sources and sinks d param specifies computational parameters such as time step and convergence parameters program options and predetermined initial conditions e solvr includes parameters for the linear equation solver f eleme provides a list of grid blocks volume elements g conne gives a list of flow connections between grid blocks h incon lists initial condition list for all grid blocks and i endcy is the last record to close the tough3 input file and start the simulation the data blocks that define rock properties and geometric configuration of the model are rocks eleme and conne tough3 uses its meshmaker tool for building the eleme and conne data blocks for a three dimensional model moridis 2016 however the model created with this tool assumes that all its cells are of the same rock type therefore it is necessary to manually modify the rock type for each cell which becomes a very tedious task when the number of cells and or rock types is large there is some commercial software that helps with the construction of three dimensional geological models that include several rock types among them leapfrog allows the rapid construction of a 3d conceptual model directly from dispersed boreholes data and geographic information system gis data aranz geo limited 2014 mview and re studio are numerical tools used in pre and post processing of tough2 models avis et al 2012 lawrence berkeley national laboratory 2021 while petrasim is a graphical interface for pre and post processing of tough2 models thunderhead engineering 2018 however the user licenses for these software are expensive and they are only compatible with tough2 in addition there exist public domain software such as addbound and assignrock that help to change the rock type in a mesh file a file that contains information blocks eleme and conne nevertheless they require that cells in which the rock type needs to be changed must be in a specific box or a polygonal prism there is also imattough tran et al 2016 but this also requires has the condition that cells in which the rock type needs to be changed must be in a specific box or a polygonal prism however for some study areas boxes or prisms cannot represent the complex geological formations in a real geothermal reservoir in addition there is a python interface developed at the university of auckland for converting leapfrog geological models into tough2 numerical models o sullivan et al 2019 however the model must be created in leapfrog a software that requires a user license on the other hand there is also pytough library croucher 2011 which can define various rock types for the aquifers and confining layers assigning the appropriate rock type to each element simply based on the elevation of each block however for a complex distribution of these materials some mathematical function would have to be found that defines the elevation of the materials and this becomes more complex than assigning the rock types manually croucher 2015 the open source gui graphical user interface called tim yeh et al 2013 uses the same geometry file as mulgraph o sullivan and bullivant 1995 for define the model grid which is a difficult task when it is trying to build the model grid to represent a complex distribution of the geology taking into account the above it is a difficult task to set up models representing the complex geological formations in a real geothermal reservoir using a software that is compatible with tough3 and does not have an expensive user license in addition software is not available that does not require that cells in which the rock type needs to be changed must be in a specific box therefore the aim of the present study was to develop a program capable of representing a comprehensive and morerealistic numerical model for a geothermal system as it was done by other researchers in their respective study areas verma et al 2008 li et al 2018 criollo et al 2019 r s gonçalves et al 2020 pham et al 2020 dehina et al 2020 carlini et al 2020 this study presents a code developed to adapt arcmap three dimensional conceptual models built of layers and convert them into input files for tough3 the code was developed in matlab a high performance language for technical calculations which is both an environment and a programming language one of matlab s strengths is that it allows the building of customised reusable tools casado fernández 2006 2 program overview it is common to have a hydrogeological conceptual model of the study area composed of surfaces each defining the separation between the geological strata for example the first top surface might be the digital elevation model topography which defines the separation between the atmosphere and the first geological strata the second surface will define the separation of the first and second geological strata and so on until the bottom of the model is reached the first step in building an arcmap model composed of layers numerically compatible with tough3 is to build a mesh uniformly distributed on x y axes on the arcmap model with the create fishnet tool when using this tool a window is displayedwhere the size and number of cells should be defined it is also essential to check the option to create label points which creates a point on each cell center so we will be able to input the elevation data for each layer of the hydrogeologic conceptual model in addition it is important to define the number of cells and their size taking into account that in the process to convert from curved and irregularly shaped surfaces arcmap model to sets of regular cells tough3 model resolution may be lost since some cells in the model may contain morethan one material however the program will assign to each cell the rock type id of the material found in the coordinates of each cell center independently of whether the layers on the arcmap model are horizontal vertical or inclined then it is necessary to export the elevation information of the model layers for each point in a txt file format this txt file summarizes the conceptual model and it will be used by the program developed herein to create a file compatible with tough3 the next step consists of running the matlab code which will ask the user for the meshmaker input the number of cells on the x axis and their size in meters then it requests the same data for the other two dimensions y and z axes although tough3 has the ability to work with radial and rectangular meshes so far our program is focused only on rectangular meshes then the code asks about the number of layers in the arcmap geologic model and the elevation at the model s highest point this elevation is used to reference the arcmap model coordinates with the coordinates created by meshmaker as the coordinates of the z axis go from zero downwards at this part of the code if you input the water table elevation of your model you will have a delimited model that discards the upper layer corresponding to the unsaturated zone it is important to mention that horizontal and vertical geological strata geological faults and intrusions or discontinuous strata can be part of the geological model in order to consider the physical setup of this type of geological strata and structures during the modelling it is necessary to define all layers and surfaces that comprise the model as continuous for example to consider a horizontal geological stratum there must be define two surfaces with different elevations each one at the edges i e the stratum is located between them if a discontinuous stratum is analyzed the base and upper surface corresponding to the stratum limits must have the same elevation to define the gap in this way the elevations difference between both surfaces limits is equal to zero thus the code will interpret the stratum where the elevations difference between the base and upper surfaces is different to zero all this information allows the code to create an input file infile with instructions to build a mesh and execute tough3 inside the active folder additionally a mesh file will be generated with the geometry entered by the user the matlab code imports and reads the tough3 mesh file and the txt file format exported from arcmap containing the conceptual model afterward the code reads all the cell center coordinatesand compares them against the topographic elevations and geological strata to assign the corresponding rock type id for the stratum in order to assign the rock type id the code starts by comparing the coordinates values of the selected cell center with the topographic elevation if the coordinates of the cell are above the ground surface the code assign the number 1 to the rock type column for that cell while if the coordinates values are below the topographic elevation and above the first stratum elevation a value of 2 is assigned to the rock type column for that cell and so on this procedure is repeated until the last cell of the model is reached after the code has finished the classification of all model cells it creates a new mesh file that contains the correct rock type column entry for each cell in the model the flowchart of the code developed is presented in fig 1 the topography is matched up with the upper part of the model by mean of the cells that are defined with the rock type corresponding to the atmosphere properties if the user prefer can delimit his model and discard the entire upper layer that corresponds to the unsaturated zone which include to the atmosphere and topography after executing the code and obtaining the mesh with the rock types distributed as is desired in the conceptual model it is necessary to copy the mesh file and use it in the tough model which require to have load the initial and border conditions as well as the number of fluid components sources and sinks time step and convergence parameters among others the matlab code described here only handles to adapt 3d conceptual models from arcmap to tough3 3 application to real data the los humeros geothermal field lhgf was selected to test the correct execution of the code developed herein the field is located in the eastern central part of mexico within the limits of the states of puebla veracruz and tlaxcala fig 2 at the eastern edge of the region known as the trans mexican volcanic belt that crosses the entire country in the we direction at the intersection with the sierra madre oriental the lhgf is the third largest geothermal field in mexico norini et al 2019 ithas an installed capacity of 119 8 mwe gutiérrez negrín et al 2020 and is estimated to be at an elevation of about 3000 masl barragán et al 2016 it is estimated that the geothermal field reservoir covers an area greater than 400 km2 and the maximum temperature of the reservoiris around 350 c diaz martos 2018 the exploitation of the deep thermal aquifer for commercial purposes began in 1990 arellano et al 2000 the defined study area has a square shape covering a grid of 20 km 20 km which comprises the whole of los humeros caldera and an area of 400 km2 4 methods six geologic cross sectionsof the study area reported by the comision federal de electricidad cfe 2014 were digitized and georeferenced in an arcmap project these cross sections were analyzed considering 136 points distributed over them located 1000 m away from each other fig 3 show three of the six geologic cross sections the depth of each geological strata was interpolated using kriging thus obtaining geological surfaces throughout the model and a three dimensional geological model a total of 1600 points on a regular x y grid were digitized in the georeferenced model using the create fishnet tool to build the mesh file created by meshmaker containing all the cells that comprise the conceptual model each point on the model surface was located at the center of the corresponding 1600 cells the topographic elevation and the elevations of each one of the geological surfaces were obtained using the extract multi values to points tool for all cell centres in the x y plan this information is compiled and exported as a table in a txt file format as shown in table 1 with 1600 points and n 8 rock types plus the topographic surface the table contains 1600 8 1 14 400 entries this file summarizes the conceptual model but it is not compatible with tough3 using a matlab code a file compatible with tough3 was built as input to the matlab code it is necessary to specify the number and dimensions of the cells along each axis of the tough grid to be created with the meshmaker tool for this study case the x axis contains 40 cells 500 m in length the y axis contains 40 cells 500 m in length and the z axis contains 40 cells 100 m in length this information was stored in the infile file as shown in fig 4 thus the model was built with a spatial discretization of 40 horizontal layers of constant thickness divided into a 40 40 mesh in the x and y axes the resulting mesh spacing is 0 5 km 0 5 km 0 1 km in the x y and z directions respectively the matlab code builds the infile file and executes tough3 when tough3 is started up it will look for the infile file and it will execute the commands contained in it and so the mesh file will be created fig 5 the mesh file contains 64 000 elements each one with its own name rock type volume and three coordinates x y and z of its cell center but in the first version of the mesh file all of these elements have the same rock type because the meshmaker tool only assigns the same rock type then the matlab code imports and reads the mesh file and the txt file format that was exported from arcmap containing the conceptual model it reads the center coordinates of all cells and compares them against the topographic elevations and geological strata to assign the rock type id corresponding to the stratum in the arcmap conceptual model after the code has finished evaluating all the cells in the model it creates a new mesh file that now contains a distinct rock type id in the column 20 with the correct values for each cell in the model fig 6 5 results and discussion the rock type distribution in the new mesh file built with the matlab code was plotted to compare it with the initially digitized geologic cross section data fig 7 a the plots show good correspondence between both information types those initially digitized and plotted with data of the new mesh file built with this matlab code fig 7b shows the geologic cross sectionobtained with the program presented herein as fig 7b shows strata of the hydrogeological conceptual model adapted to tough3 present a lower resolution when representing sloping strata clearly the complex hydrogeological structures commonly occurring in geothermal reservoirs can be better represented by reducing the cells size because the model resolution is inversely proportional to the cell size used for the discretization however based on the cell size consider for this model it can be seen that the most significant characteristics of the hydrogeological conceptual modelincluded in arcmap project are well represented in addition as was done this study the user can delimit the model and discard all the upper layer that represents to the unsaturated zone which can be very useful for case studies where the topography is very rugged and it is desired to input the surface temperature of the soil in the model as a condition of the upper limit 6 conclusions the matlab code developed and presented herein threedmesh to tough facilitates the construction of three dimensional models for tough3 this code was described here in addition as a case study its application to the hydrogeological conceptual model of the lhgf is discussed which is a three dimensional heterogeneous geothermal reservoir with a complex distribution of up to nine different rock types including the atmosphere composed of eight slanted faulted and deformed layers as is common in this type of environments and with a mountainous topography in spite of that complexity the results showed that the matlab code developed here built a geological model for tough3 with good correspondence to the hydrogeological conceptual model constructed in arcmap thus this tool allows not just the exporting of conceptual models of complex geothermal reservoirs constructed easily in arcmap into an input file for tough3 but also executes tough3 and using meshmaker build a rectangular mesh that reduces the time effort and error prone process of configuring and modifying the original mesh for tough3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25709,portability of web applications between web servers of different organizations can be challenging and can complicate sharing and collaborative use of such tools given the distributed nature of the web this lack of portability is usually not a concern because a user in one organization can link to and use a web application hosted by another organization however access control or differentiation may be needed by an organization in terms of area of interest input data analytical techniques access control presentation branding and language this is true for many government organizations and their associated web sites and servers in such cases there are compelling political branding security and privacy motivations that require each organization or agency to host and manage web applications on their own servers rather than using third party web sites over which they have little or no control we present the design development and testing of a system for discovering installing and configuring environmental analysis web applications on localized web servers the system works with applications developed using tethys platform which is an open source software stack for creating geospatially enabled web based applications the developed tethys app store includes a tethys application user interface that allows a server manager to retrieve applications from the central repository and install them on a local server with relative simplicity similar to the installation of a mobile application to a mobile device from a mobile application store the system was developed to support deployment of water and environmental analysis web apps for the international group on earth observations geo global water sustainability geoglows initiative of the national aeronautics and space administration nasa and several partner organizations keywords geoglows environmental analysis web applications conda tethys platform 1 introduction 1 1 problem statement and research motivation recent years have witnessed a significant increase in research and development in the area of web based applications for environmental data analysis and modelling swain et al 2015 rolph et al 2017 wong and kerkez 2016 zeng et al 2021 gan et al 2020 saah et al 2019 these web based applications can be simple data viewers or complex analytical tools to support analysis and decision making in various fields including hydrology land use environmental analysis etc dile et al 2016 evans et al 2020 such applications generally rely on many underlying technologies and database platforms kuria et al 2019 the selection of these technologies depends on the application goal the expected latency in obtaining post processing results and available computer hardware and architecture in our experience complex environmental analysis web applications cannot be easily shared at the source code level for use on other servers because of the heterogeneity of their deployment environments furthermore related difficulties in deploying environmental models significantly hinder collaboration amongst modelers in conducting integrated modelling research and hence raise the need for a common deployment strategy wen et al 2017 in a typical development and deployment scenario the components of a web based environmental analysis application e g database web server application server and geoprocessing workflow tools are usually located together on a single web server alcantara et al 2018 aburizaiza and ames 2009 crawley et al 2017 steiniger and hunter 2012 each component is built separately and integrated with code that is specific to and written for the underlying technologies and server architecture this can cause the application to be limited in terms of portability as the application architecture is tightly coupled with the server architecture to replicate the application on a different system the same server architecture must be duplicated and associated components installed the integration and management of these components is non trivial and can require extensive support from experts with highly specialized information technology training and technical skills installation and management of these components usually involves running a set of commands on the command line interface this process can be difficult for scientists and researchers who are not well versed in information technology and performing command line tasks shared usage of a tightly self integrated web application is possible by providing hyperlinks to web applications hosted by different organizations in the geographic information systems gis field a common solution has been to create a so called geoportal that simply provides links to web based applications and spatial data from different organizations karabegovic and ponjavic 2012 this approach allows for reuse of the same application by various organizations without having to set up another instance of the application on a local server however this type of reuse e g simple linking comes at the cost of customization various organizations have different access requirements areas of interest work on different types of input data sets and perform a wide array of analyses organizations may also want to brand the application and provide the interface in local languages linking to a web application that is developed and maintained by a third party organization provides by design a less customized more generic web application experience for users local or national government agencies may impose restrictions on data privacy sharing and security depending on the type of data being shared in the web based application access restrictions to protect sensitive data or the personal privacy of any individuals are needed tullis and kar 2020 onsrud et al 1994 this motivates organizations to host and manage web applications on their own servers as hosting them on third party services provides little or no control over the nuances of access functionality appearance datasets or other features of the application organizations can realize many benefits from the ability to host their own web applications instead of simply linking to other websites for example consider a web application for handling data on a watershed that crosses disputed international boundaries the indus river basin is bisected by the partition of the area between india and pakistan and while the indus water treaty calls for monthly sharing of data collected on the basin between the two countries there are conflicts between the two nations that affect sharing indus water data and related web applications miner et al 2009 hence a single web application hosted by one or the other nation would be less desirable than the ability to host localized instances of the application the inability to customize linked applications can force organizations to try to develop their own regionalized applications from scratch examples include the hawaii geoportal hawaii statewide gis program 2017 california geoportal california state geoportal 2020 and the european geoportal bernard et al 2005 each of these portals are custom built rather than reusing existing code and applications custom creation of web applications increases redundancy across web based environmental data applications and can be viewed as a major inefficiency and potential error source there are hundreds of such custom and purpose built web portals across the internet providing various levels of functionality dempsey 2015 esri s arcgis online arcgis online cloud based gis mapping software solution 2021 offers tools to develop and deploy simple web based mapping applications in a geoportal type environment though the closed nature of the arcgis online system prohibits extensive localization or transfer to third party web servers not operated by esri or running on esri software sharing of data and models across the internet and hence across web applications has been advanced through the development of standards to integrate models hosted on disparate servers and systems harpham et al 2019 chen et al 2020 zhang et al 2019 and by using common data elements during the application development process chang and park 2006 peckham et al 2013 this type of model and data sharing is critical to the success of any integrated web based data and modelling system and supports the idea of distributed customized web applications that can consume and display in a locally relevant way resulting data sharing of web applications at the source code level can be accomplished by using a github repository gharehyazie et al 2017 mergel 2015 a github repository allows developers to host their code online and provides version control using git torvalds and hamano 2010 developers create the application code and add it to a github repository end users can then download or clone an existing github repository to their local system using the repository s clone address which can be obtained from the github page for that repository as more developers publish their software and web applications under open source licenses on github or equivalent the opportunity arises to share web applications at the source code level by simply cloning a repository to another server and customizing the code on that server this approach can be considered as a generic web application distribution system in the same way that a developer could clone the source code for a mobile application and compile and install it on his her own device for use technically it can be done but it lacks the smooth integration of an app store and requires significantly more technical knowledge a source code repository in itself lacks a central catalog that can be searched specifically for web based environmental analysis applications compatible with the end user s system and does not install any supporting packages or services on the web server another issue is keeping track of new application versions while version control systems such as github have a mechanism known as releases which tags a repository at specific points in time with a version number using github directly without any overlay interface for versioning presents challenges for developers and users in addition these approaches are completely manual i e a developer would have to manually push all the changes to the version control system or email the end user the changed files to install the application on their servers the end user needs to have an extensive knowledge of the innerworkings of the code being shared to perform any regionalization as well as updates to the web application container technologies such as docker can also be used to share preinstalled applications using docker or other containerization systems require building a shareable package image which contains a base operating system the related components and tools required by the application as well as the application code another approach to using docker can be to include just the application code in a shareable package and a master application management tool in another package that can connect to each installed application s instance and present them to the user within a unified interface docker ensures that an application package will work on any system on which it is installed however since each application can have various dependencies and required components developing a standard package building recipe without having a common framework platform that hosts these applications is not possible and requires each application package to be built manually by the developer containerization technologies such as docker and kubernetes introduce complexity and a technical requirement of setting up the containerization platform on the hosting web server managing various containers and a deep understanding of various command line tools and network configurations needed to keep these containers connected to the internet and available to use 1 2 research objective as described previously there remains a need for a system and approach for distributing web based environmental data analysis applications for installation on localized servers operated by various government agencies we have found this to be particularly true for the group on earth observations geo global water sustainability geoglows member organizations described below such a system should enable developers to share their web applications by registering the application in a central repository allow the repository to be searchable by end users and allow users to install the application on their local web server optimally applications installed from the repository would require minimal local configuration but could be customized at will this could require a varying number of tasks involving a range of technical skills such as database integration web server modification or set up installing required geoprocessing tools and creating and configuring a host of other local web services or capabilities the system should also provide a means for updating and removing an installed application and would perform any required cleanup of the local computing environment we describe the design and development of the proposed system with a focus on discovering transferring installing and configuring web based environmental analysis applications from a central catalog into individual web portals operated by different organizations specifically we present the design and development of the tethys app store modeled after the mobile device app store concept we built the tethys app store to distribute web applications and to simplify installation of an application on third party servers the examples we present in this paper are tethys web applications mostly based on the django framework written in a combination of the python programming language with html and javascript this approach can be expanded to distribute web based applications written in other languages and web frameworks as well our tethys app store uses tethys platform and the conda packaging repository this system has similar goals to community model sharing and discovery tools such as csdms peckham et al 2013 epa smart search us epa 2019 and the earth system modelling framework esmf 2021 however the key difference is that these tools focus on easier discovery and sharing of modelling components while our system is designed for sharing web based applications which may include modelling components but also comprise a web interface to access and use those components integration with data storage services and depending on the needs of the application connections to web processing services task schedulers and high performance computing systems the system presented here was implemented and tested using a number of applications developed for member states of geoglows and the nasa servir programs hardin et al 2006 the geoglows initiative seeks to enable earth scientists to solve the challenges associated with achieving global water sustainability gutierrez et al 2018 several tethys applications have been developed and deployed through applied science research projects with nasa nsf and most recently as part of the servir applied science team ast nelson jones and ames 2016 as part of the nasa servir project as well as geoglows initiatives other principal investigators have expressed interest in creating new tethys applications and reusing existing applications to support visualization and decision making for their projects this establishes the need for improved capabilities to easily share and reuse web applications our solution allows for relatively rapid deployment of environmental analysis applications for managing and using essential water resources variables in support of geoglows this paper is organized as follows section 2 introduces the technologies we used presents the software design and architecture of the tethys app store and the design of a geoglows experimental test case section 3 1 presents the implementation of the tethys app store including steps for registering applications in the system retrieving them and installing them into a specific portal section 3 2 presents the results of the geoglows experimental test case including the deployment of three application portals using the tethys app store to populate each portal with specific applications as needed and section 4 includes a discussion of results and conclusions including potential for future maintenance and development of this system 2 methods 2 1 core technologies the tethys app store is built on two major technologies tethys platform and conda tethys platform is an open source software stack for creating web based applications swain et al 2016 and is comprised of three main elements tethys software suite tethys software development kit sdk and tethys portal tethys is built on django a popular python model view controller mvc web framework that facilitates developing web applications and deploying completed web applications in a production setting conda analytics continuum 2017 is a software distribution system which allows a server manager to pull applications from the central repository and install them with relative simplicity it is based on the anaconda software project an open source software platform that unifies several popular data science packages and standardizes the install and discovery process for these packages using the anaconda repository anaconda distribution 2020 it is a robust package management system that was designed and built to manage software packages written in any language but is most commonly used for python packages vanderplas 2016 2 2 tethys app store software design our approach to registering discovering transferring and installing web applications to a third party server is modeled somewhat after the concepts of the ios app store apple devices or the google play store android devices which enable a user to download and install applications to their mobile devices similarly the tethys app store facilitates a government agency or organization to transfer and install web based environmental analysis applications onto their own servers and hence attempts to address the installation and distribution challenge faced by developers and users historically the process for installing a completed tethys application requires the end user to follow a list of instructions that include transferring the application source code to the server running a series of installation and file management commands in the terminal shell and configuring the custom settings and services for the application using command line interfaces or the administrator interface of tethys portal this process has proved difficult for end users who are not well versed in information technology and performing command line tasks fig 1 shows the overall system architecture design for the tethys app store built as a tethys application using the conda registry and repository to store and retrieve tethys applications it also uses github actions as a build system to package the tethys application and websockets are used within the tethys app store to get user input and provide status updates from the server in section 3 we present several case studies that help to better understand the system architecture and use case 2 2 1 distribution service conda the tethys app store makes web applications portable by packaging tethys applications into a conda package and hosting them in conda repositories when a developer creates an installation package using conda the package ensures that the application components will be built compiled configured and packaged together in a way that they can be installed on another tethys portal with little additional effort the tethys app store hosts its packages in a conda repository which can be searched using a public restful web service application programming interface api the tethys app store integrates with the conda api and performs all the requests on behalf of the end user further lowering the barrier since searching and installing a package from the conda repository requires some understanding of how to search the repository the tethys app store packages are configured such that conda will find and install any other required dependencies that are listed within the tethys application each tethys application that is submitted to the tethys app store gets tagged with a version number that is updated with new versions this allows the store to keep track of any updates the tethys app store uses the conda versioning system to detect if any updates to installed applications are available and notifies the user this is another service that reduces the knowledge and time required for local users to keep their tethys server and associated applications up to date the tethys app store also helps developers if they publish their tethys applications to the tethys app store by making it easier for the end users to find install and use their applications we designed the tethys app store to facilitate access to tethys applications which use geospatial data geospatial applications require complicated server set ups making server and application set up complicated and error prone the tethys app store automates much of this work making it easy for a local user to maintain a tethys server and find and install available applications this is very similar to browsing the qgis plugin manager qgis development team 2021 for a suitable qgis plugin to install on one s local program or using the apple ios app store to find a new application to install on an iphone it helps tethys application developers make their applications available and since the developers create the conda installation packages using the tethys app store it also ensures that an installation of their application will work correctly the conda package provides a standard build recipe that each tethys application uses to guide the conda build process this standard recipe ensures that regardless of the operating system and underlying technologies on the developer s or user s machine the tethys application is always packaged in a way that it can be installed on any tethys portal that uses the app store to install applications section 2 2 4 provides more details on this process the conda repository groups its packages in various channels for ease of organization and discovery we created a new channel named tethysapp in the conda repository that is the central repository location where all tethys applications submitted to the tethys app store will be hosted we distribute the tethys app store as an open source project which provides a rich environment for developers to collaborate in making new applications modify existing ones and distributing these applications to end users these tools which make both distributing and installing applications easier supporting both developers and users should help widely in distributing cutting edge tools this same approach of standard distribution and application deployment can be applied to a variety of other scientific disciplines 2 2 2 install method the conda repository is queried for all available applications and their metadata the end user selects the application to be installed and the app store fetches the selected application s conda package it then allows for custom settings configuration and services configuration as described in the sections below we describe how using the above mentioned distribution system and underlying tools within tethys platform we orchestrate a seamless and intuitive workflow for installing new applications on to a tethys portal as shown in fig 2 the first step in the discovery process is to query the conda channel tethysapp for all available applications during the discovery step the app store extracts metadata from the query results and caches it this metadata is used to display a list of available applications with their corresponding descriptions author information versions etc to the user this list can be filtered and searched by the end user the in memory list of applications is updated whenever the tethys portal is restarted or it can be manually refreshed by the user as well the next step in the install process runs after the user selects which application they would like to install a list of available versions for that application is retrieved from the metadata cache and is presented to the user for selection upon selecting the desired version the tethys app store initiates a conda install command with the selected application s name and version during the conda install process conda performs a number of tasks including fetching the compressed archive that contains the application code installs any dependencies that the new application needs checks for compatibility between previously installed python packages and the requested packages and performs any required cleanup of the python environment this process can get extremely complex and hence this step is the most time consuming also if the conda process identifies potential conflicts between the ins talled packages and the application being installed the process cannot continue and the end user is provided debugging information to fix their local python environments this is a common issue while managing virtual environments and there is no automatic method to resolve these conflicts however if no conflicts are found the conda install process will extract the application archive into the site packages directory where it can be discovered by the tethys portal as an installed application an important aspect of the tethys app store is that each install workflow is run in a new python thread which means that it does not cause our existing thread to freeze and allows the user to continue browsing the tethys app store or performing other actions on their tethys portal without having to wait for the install to complete there are a few checkpoints within the conda install process that inform the tethys app store application of the progress of installation and these are communicated to the user as notifications the communication layer and protocol for these notifications is described in section 2 2 5 before proceeding to the next step the tethys portal running as a python process needs to be made aware of the new application this is done by clearing django caches as well as reloading of two specific python packages site and tethysapp the reloading of the site package refreshes the list of installed python packages without having to restart the python process reloading the tethysapp package refreshes the list of tethys applications that are found installed in the site packages directory of the user s python environment by default django caches various aspects of the tethys portal including a list of the installed applications the tethys app store clears the cache which forces django to re evaluate the list of installed applications for the tethys portal step 3 in the install process is to configure custom settings and tethys services which were typically configured using the portal administration interface the portal administration interface can be confusing for a user to navigate and was an extra manual step during application installation the tethys app store brings the configuration of these settings within the install workflow offering a smooth experience resulting in a fully configured application once the process is completed example custom settings include the ability to change the colors and layout of the application add custom logos and change header texts custom settings can also be used to configure access credentials api endpoints and rate limits for tethys applications that consume third party services after a successful installation of the application the tethys app store scans the application files for any custom settings this is done by loading the application configuration that is stored within the application files the tethys app store iterates over the application configuration file and retrieves all the custom settings that are required by the application this is then presented to the user over our communication layer as described in section 2 2 5 if default values are defined for those custom settings those are presented to the user as well along with a description of these settings the end user enters the values for these custom settings and the tethys app store will configure the values in the application being installed the final step in the application configuration is to set up and link any required tethys services with the application being installed the services are also loaded from the application configuration file mentioned above the tethys portal may have previously configured services that are compatible with the application being installed for each service a list of compatible services is displayed in a drop down which can be selected for use the service configuration window also contains options to create new services from the installation workflow itself without having to use the administration interface if a new service is created the tethys app store updates the lists of options for various services with any new compatible services once a compatible service definition is selected the tethys app store links that service with the tethys application being installed finally the tethys app store also allows the end user to skip any of the custom setting or service configuration if they choose to once all the above steps are completed it triggers a restart of the tethys portal after issuing commands to collect all static files and workspaces within the tethys portal since each application has its own set of static files i e javascript images cascading style sheets css etc these need to be collected by the tethys portal before they can be accessed by the end user this collection process is a standard part of tethys platform that allows it to set the right permissions on these files so that they can be accessed by the end user restarting the tethys portal is critical to reload the url schema that django uses to identify and route the user to the right applications when they visit the portal it is the only way to ensure that the newly installed application can be accessed using the tethys portal s web interface 2 2 3 updates the tethys app store presents a view to the end user which lists all the installed applications this list is obtained by querying the tethysapp python namespace which contains the paths to each installed tethys application when the tethys app store is accessed it compares each of the installed applications with their corresponding versions from the conda repository if any newer versions are found the application is highlighted on the end user s screen and the user may choose to run the update from the user interface itself the update process works very similar to the install process mentioned above but differs in three critical steps first it creates a backup of all the custom settings and tethys services that the current version of the application is using secondly it cleans and uninstalls the existing version of the tethys application being updated as leftover files and directories can cause conflicts with the newer version of the application finally after running the install process for the new version of the application the tethys app store tries to restore the custom settings and tethys services configuration to the application if the same settings are required in the new version any incompatible settings or service configurations are reported to the end user so they can be resolved 2 2 4 application submission the above mentioned approaches to application installation and updates depend on a unified method for tethys application developers to submit their applications to populate the app store developers contribute their applications by following a two step workflow within the user interface while the app store takes care of all the heavy lifting of correctly preparing the code and making it available as an easily installable conda package an important component of any application within the tethys app store is its metadata such as name description tags version license etc this metadata is used to uniquely identify the application and special application parameters such as api inputs outputs user interface instructions or similar are not stored in this metadata this information helps end users search for an application that suits their needs this metadata can be defined in the setup file that is a universal standard for python applications as well as is included when a tethys application is scaffolded filling out the fields in the setup file is the one critical step that the application developer needs to perform before submitting the application to the tethys app store the build process reads this setup file and determines the correct name and version of the application to be built as well as the following fields from setup py description keywords author name author email url and license there are two key files that are important for the build process to run conda build recipe and github workflow job definition the conda build recipe is a directory within the application folder that defines the various aspects of the application being built and instructs conda on how to run the build process this is important since various python packages have to be packaged in a way that ensures compatibility between the package and the underlying operating system that it is installed on however a tethys application doesn t contain dependencies in other languages such as c java etc and hence needs to be built as a pure python package the app store provides the conda build recipe as suggested by the official conda build documentation for pure python packages once the build recipe is generated the tethys app store needs to run the conda build process so that a compressed archive of the application code is generated which can be hosted on the conda repository one possibility is to run this on the developer s computer as a part of tethys app store however this approach will require the conda build utilities to be installed on the developer s system as well as use local system resources instead the tethys app store utilizes github actions github actions makes it easy to automate software workflows and allows for building testing and deploying applications from github daigle 2018 for open source public repositories github actions run the build and testing workflows free of charge for this purpose a github organization tethysapp was created and is used to host and run the tethys application deploy workflow this workflow is responsible for reading the input files testing that the files required for the build process are present generating the correct build and publishing the package to conda the flow diagram shown in fig 3 describes the various steps that the developer as well as the tethys app store performs to prepare and submit an application the preparation phase begins by creating a local copy of the application code once the developer has submitted the github url if the github repository contains multiple branches the developer is asked to select which branch contains the latest stable code that needs to be packaged else the master branch is used the selected branch is checked out and the conda build recipe is generated and added to the application code the github actions directory is created and the workflow action file as shown in the technical appendix section is copied into that directory finally prior to pushing this the application code to github for packaging the tethys app store cleans up the existing setup file and removes the dependency on tethys platform during the build process to speed up build and deploys the next step pushes the prepared application code to github this code is not pushed to the developer s original repository but a new repository for this application is created on the tethysapp github organization if a previous repository exists for this application it is removed prior to creating a new one the creation and removal of this repository is completely handled by the tethys app store this provides the build environment a clean slate to start on each time we need to publish a new version of an application the push triggers a github action that runs on github s servers which packages the application code into a shareable compressed archive and publishes it to the conda repository as soon as the publish happens that version of the specific tethys application will be available for the user community to download on their individual tethys portals all packages are pushed to a specific conda channel tethysapp which makes the discovery and filtering of tethys applications much easier 2 2 5 communication layer in each of the above mentioned steps there is a constant back and forth communication between the tethys app store and the end user the end user receives notifications or a prompt for an input from the tethys app store and the tethys app store in turn receives the end user s selections and response to any selections that they needed to make the tethys app store uses websockets to achieve this communication as well as detect when the tethys platform is offline due to any restarts that might happen as outlined in section 2 2 2 if the restart was left unhandled it would cause the entire install process to reset and the end user will have to start all over again just to get stuck again if a restart happens the tethys app store also saves the current state of whichever process the user was on and keeps trying to connect back to the tethys portal once the tethys app store receives a valid connection again indicating that the restart has completed successfully it restores the state and continues the step process that the user was performing before the disconnect happened 2 3 geoglows experimental test case design in partnership with geoglows efforts we tested our system by packaging the following tethys web applications and making them available in the tethys app store next we deployed 2 new tethys portals byu s tethys portal which hosts tethys applications created within the byu hydroinformatics lab for demonstration to our partners and the crrh portal the crrh sica regional committee for hydraulic resources is a technical body of the central american integration system sica specialized in meteorology climatology hydrology and hydric and hydraulic resources of the central american region a regional and customized portal for crrh was installed on their servers within each portal we used the tethys app store application to retrieve and install selected applications from this list gfs and gldas tethys applications the gldas data tool is a tool to facilitate downloading visualizing processing analyzing and sharing gldas v2 x data from nasa gldas global land data assimilation system is a historical earth observation dataset based on the lis land information system land surface model j nelson et al 2019 hydrostats tethys app this is an application based on the hydrostats python package which helps characterize predicted and observed hydrologic time series data and has three main capabilities data storage and retrieval visualization and plotting routines and a metrics library that currently contains routines to compute over 70 different error metrics and routines for ensemble forecast skill scores roberts et al 2018 geoglows ecmwf streamflow hydroviewer this tethys application presents a global streamflow viewer and animated streamflow forecast maps the data is generated by routing global runoff ensemble forecasts and global historical runoff generated by the european centre for medium range weather forecasts model using the routing application for parallel computation of discharge to produce high spatial resolution 15 day stream forecasts hydroviewer central america this is a localized version of a tethys application developed for the central america region and displays the model results as described above it also facilitates data consumption and integration at the local level this application has the potential to allow decision makers to focus on solving some of the most pressing water related issues we face as a society souffront alcantara et al 2019 snow inspector an open source tethys application for snow cover probability time series extraction from map images the application also contains a waterml compatible web api which gives access to third party applications for automation and embedding in modeling tools kadlec et al 2016 3 results 3 1 resulting software implementation the tethys app store was implemented as a tethys application and can currently be obtained from github or a conda install the tethys app store is only compatible with tethys portal versions 3 0 and above more information on how to obtain the app store is found in the software availability section below this paper is focused on the discovery download and installation of applications on local web servers specific application end user graphical interfaces and instructions are provided by individual developers who have contributed applications to the tethys app store the tethys app store is located on the end user s tethys portal application library page this installation is done by running a conda install command for the tethys app store the end user will only be able to access the tethys app store if they are logged in as an admin user of the portal since this tethys app store can make significant changes to the installed portal and its applications it was critical to enforce this login requirement the tethys app store is launched by clicking on the app store icon on the application library page upon opening the app store the end user is presented with the landing page as shown in fig 4 the tethys app store has 3 major components as highlighted above each component is responsible for an important feature of the tethys app store and their usage is demonstrated in the sections below the first section displays the list of available applications for install this is the most important component of the tethys app store as it searches through the conda repository and displays a list of available tethys applications that the end user may install on their system only applications that are developed for tethys version 3 0 and packaged with compatibility with the tethys app store are shown in this list the end user can also use the search interface on the table to search and filter for a specific application based on keywords once a suitable tethys application is found the end user can start the install process by clicking on the install button the first step for the end user is to select which version of the application they wish to install from the version drop down after selecting the version and clicking the go button the installation process begins with the conda install step as described above in the package installation methodology section 2 1 2 the user gets updates on the conda process as it is running in the background once that process completes the end user may choose to configure any custom settings the tethys application might need helpful description and default values if present in the tethys application are also displayed fig 5 shows an example of the custom setting configuration modal fig 6 shows the final step in the installation process which is the configuration of any services geoserver postgis etc that the tethys application might need upon completing skipping the custom settings configuration modal the end user is presented with a service configuration modal the modal dialog box lists all services that are described within the application and allows the end user to link an existing service to the application the tethys app store supports configuration of all types of services that are present in the tethys portal the end user may also choose to create a new service of the type that the application needs by clicking the create new service button as seen in fig 7 following the configuration of these services the installation process is completed and the app store restarts the tethys portal instance for changes to take effect the second section in fig 4 shows a list of applications that the user has installed using the app store and allows for management of these installed applications from this table the end user can choose to uninstall or reconfigure any settings for the corresponding tethys application if the installed version of the application is lower than the latest available version an update button is also shown which can be used to update the application tethys app store also offers an interface and helpful utilities that allows developers to prepare and submit their application to the app store s repository any tethys application that has been developed in compliance with the instructions found on tethys documentation will be compatible with this process the tethys application once ready to publish should exist in a github repository to get the process started the developer will click on the add application button located on the top right corner of the app store interface fig 8 shows the modal presented to the user where they will be required to enter the web address of the github repository for their application a list of available branches from the github repository is presented to the developer after selecting the correct branch the application is prepared and packaged as described in section 2 2 4 if any errors happen during this build process the developer is informed of those errors within the same modal upon successful completion of the build the developer receives an email informing them that a new version of their application has been published in the app store 3 2 case study results figs 9 and 10 show the two new tethys portals that were installed on ubuntu virtual machines crrh and byu s main tethys portal both of these portals were set up following the production installation steps as described in the official tethys platform documentation following portal install and configuration the tethys app store was installed on each portal by running the conda install command in the command line interface this was the only application that needed to be installed via the command line once the app store was installed on each portal the tethys app store was used to fetch install and configure the other applications required for the portal below is a list of contributions of this work based on our expertise with tethys platform since its inception and or experience training new web app developers searching for an application all registered applications can be easily searched from within the app store interface instead of performing a search through various github repositories and using global search engines such as google retrieving application code and installing the app store downloads the code and installs dependencies sets up services and configures custom settings via an interactive user interface without the app store the user would need to run commands on the command line and understand python virtual environments and github to fetch the code and install it also setting up services via the command line for tethys has proven to be a complicated process and can be avoided by using the app store installation cleanup for the newly installed application to load correctly the portal needs to collect all the static files from the application code and perform a restart the app store takes care of collecting the static files for the portal and performs a portal restart without requiring the user to manually enter the right commands in the command line prompt updating and installed application the app store informs the user of a new version that is available and does all the heavy lifting of updating an application including retrieving the new code installing it and restoring any custom setting or service configurations uninstalling an application the app store allows the user to remove an installed application completely by running the tethys uninstall command performing a cleanup of any leftover application files and restarting the portal for the changes to take effect no user interaction is required after confirming the installation in terms of scalability since the tethys app store actually does not host any applications itself and uses the conda packaging system this architecture is as scalable as conda the conda packaging system is highly scalable as conda forge which is the main channel for most packages saw 3751 new additions and over 100 million downloads a month for 2020 2020 in review the conda forge blog 2020 4 conclusions easier discovery installation and management of environmental analysis web applications is made possible by the introduction of an app store the requirements for such an app store and design methodology are discussed with tethys platform the design is implemented to create an interface to search for existing tethys applications install and manage them as well as submit new applications to the collection of available tethys applications for improved sharing across regional boundaries we were able to test the tethys app store by populating two new tethys portals with applications from the tethys app store all steps including installation and configuration of custom settings and services was done from within the interactive user interface of the tethys application store a decrease in the time and skill required for application installation and management is observed in the results of our test case future improvements to the tethys app store include a review process so that newly submitted applications can be reviewed by the tethys app store managers and verify if the application passes the standards of the tethys community the tethys app store also serves as a proof of concept that the sharing of web based environmnetal analysis applications can be simplified and standardized to implement a similar app store for other web based application frameworks the following aspects need to be considered a standard process for packaging the web application that works across various operating systems a centralized list of all packaged and available web applications and finally an intuitive user interface to perform the retrieval installation and management of these applications 5 software availability name of the software tethys app store developer rohit khattar developer email m year first available 2020 hardware required no specific requirements any system that can run and support python will be compatible with this software software required o python version 3 7 or higher https www python org downloads o miniconda conda for python version 3 7 or higher https docs conda io en latest miniconda html o tethys platform version 3 0 or higher http docs tethysplatform org en stable installation html o the software has been tested with conda environments setup on ubuntu 16 macos x and windows 10 cost this application is open source free to use and is distributed under the bsd 3 clause license software repository https github com byu hydroinformatics tethysapp tethys app store documentation details on how to obtain the software and install it locally can be found at https tethys app store readthedocs io en latest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by nasa grant number 80nscc18k0440 an amerigeoss cloud based platform for rapid deployment of geoglows water and food security nexus decision support apps author contributions to this article are as follows khattar conducted the bulk of the work and is the primary contributor hales helped with software development and technical support ames organized the funding coordinated the research and helped write the text nelson jones and williams assisted with the research and contributed to the text editorial conflict of interest statement given his role as environmental modelling software editor in chief daniel p ames was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor min chen technical appendix this section contains additional screenshots code snippets and technical information about the tethys app store the following code snippet show the setup py file definition for the store application this refers to the setup py file mentioned in section 2 2 4 code snippet 1 fields that can be configured in setup py code snippet 1 by default when a tethys application is scaffolded the setup includes a call to a function which is part of the tethys portal package leaving that dependency in would force the app store to require the tethys platform package during the build process which increases build times instead the cleanup code replaces the call to that method by adding the actual function definition of find resource files to the setup py file hence bypassing the need to include the tethys platform package during build code snippet 2 before setup py cleanup code snippet 2 code snippet 3 after setup py cleanup code snippet 3 code snippet 4 is an example of the build recipe that is generated by the app store the build recipe is derived from a standard build recipe and instructions found at https docs conda io projects conda build en latest resources define metadata html the build recipe uses a popular markup language yaml ben kikievans and brian ingerson 2009 which is commonly used by programmers to set up configuration files for their applications amongst its many uses conda also supports templating using jinja ronacher 2009 within its recipe files which allows the app store to get the metadata from the setup py file instead of forcing the developer to enter the same information in multiple files a description of each of the sections is provided below code snippet 4 conda build recipe package this configures the name and the version under which this application will be available in the app store about the about section contains some more standard metadata that is used to display and filter applications in the app store source specifies the path to the directory containing the application code since this recipe is within a subdirectory conda recipes the path tells the build command to look for application files one level above the recipe directory build these parameters instruct conda on how to build the application for tethys applications these parameters instruct conda to build a pure python package which is not tied with any specific operating system and hence can be distributed to any other tethys portal requirements tethys applications can have dependencies on other python libraries for various reasons including geoprocessing data analysis and scientific file parsing tethys platform design pattern requires the developers to list all the dependencies in an application s install yml file which is provided when a new tethys application is scaffolded this list is copied over to the conda build recipe during the file preparation stage avoiding the need for the developer to list the dependencies twice outputs and extra these sections provide fields for additional metadata code snippet 4 code snippet 5 describes the github action workflow that is run on github s servers when the application is uploaded after being prepared and packaged within the tethys app store code snippet 5 github action workflow for tethys applications code snippet 5 
25709,portability of web applications between web servers of different organizations can be challenging and can complicate sharing and collaborative use of such tools given the distributed nature of the web this lack of portability is usually not a concern because a user in one organization can link to and use a web application hosted by another organization however access control or differentiation may be needed by an organization in terms of area of interest input data analytical techniques access control presentation branding and language this is true for many government organizations and their associated web sites and servers in such cases there are compelling political branding security and privacy motivations that require each organization or agency to host and manage web applications on their own servers rather than using third party web sites over which they have little or no control we present the design development and testing of a system for discovering installing and configuring environmental analysis web applications on localized web servers the system works with applications developed using tethys platform which is an open source software stack for creating geospatially enabled web based applications the developed tethys app store includes a tethys application user interface that allows a server manager to retrieve applications from the central repository and install them on a local server with relative simplicity similar to the installation of a mobile application to a mobile device from a mobile application store the system was developed to support deployment of water and environmental analysis web apps for the international group on earth observations geo global water sustainability geoglows initiative of the national aeronautics and space administration nasa and several partner organizations keywords geoglows environmental analysis web applications conda tethys platform 1 introduction 1 1 problem statement and research motivation recent years have witnessed a significant increase in research and development in the area of web based applications for environmental data analysis and modelling swain et al 2015 rolph et al 2017 wong and kerkez 2016 zeng et al 2021 gan et al 2020 saah et al 2019 these web based applications can be simple data viewers or complex analytical tools to support analysis and decision making in various fields including hydrology land use environmental analysis etc dile et al 2016 evans et al 2020 such applications generally rely on many underlying technologies and database platforms kuria et al 2019 the selection of these technologies depends on the application goal the expected latency in obtaining post processing results and available computer hardware and architecture in our experience complex environmental analysis web applications cannot be easily shared at the source code level for use on other servers because of the heterogeneity of their deployment environments furthermore related difficulties in deploying environmental models significantly hinder collaboration amongst modelers in conducting integrated modelling research and hence raise the need for a common deployment strategy wen et al 2017 in a typical development and deployment scenario the components of a web based environmental analysis application e g database web server application server and geoprocessing workflow tools are usually located together on a single web server alcantara et al 2018 aburizaiza and ames 2009 crawley et al 2017 steiniger and hunter 2012 each component is built separately and integrated with code that is specific to and written for the underlying technologies and server architecture this can cause the application to be limited in terms of portability as the application architecture is tightly coupled with the server architecture to replicate the application on a different system the same server architecture must be duplicated and associated components installed the integration and management of these components is non trivial and can require extensive support from experts with highly specialized information technology training and technical skills installation and management of these components usually involves running a set of commands on the command line interface this process can be difficult for scientists and researchers who are not well versed in information technology and performing command line tasks shared usage of a tightly self integrated web application is possible by providing hyperlinks to web applications hosted by different organizations in the geographic information systems gis field a common solution has been to create a so called geoportal that simply provides links to web based applications and spatial data from different organizations karabegovic and ponjavic 2012 this approach allows for reuse of the same application by various organizations without having to set up another instance of the application on a local server however this type of reuse e g simple linking comes at the cost of customization various organizations have different access requirements areas of interest work on different types of input data sets and perform a wide array of analyses organizations may also want to brand the application and provide the interface in local languages linking to a web application that is developed and maintained by a third party organization provides by design a less customized more generic web application experience for users local or national government agencies may impose restrictions on data privacy sharing and security depending on the type of data being shared in the web based application access restrictions to protect sensitive data or the personal privacy of any individuals are needed tullis and kar 2020 onsrud et al 1994 this motivates organizations to host and manage web applications on their own servers as hosting them on third party services provides little or no control over the nuances of access functionality appearance datasets or other features of the application organizations can realize many benefits from the ability to host their own web applications instead of simply linking to other websites for example consider a web application for handling data on a watershed that crosses disputed international boundaries the indus river basin is bisected by the partition of the area between india and pakistan and while the indus water treaty calls for monthly sharing of data collected on the basin between the two countries there are conflicts between the two nations that affect sharing indus water data and related web applications miner et al 2009 hence a single web application hosted by one or the other nation would be less desirable than the ability to host localized instances of the application the inability to customize linked applications can force organizations to try to develop their own regionalized applications from scratch examples include the hawaii geoportal hawaii statewide gis program 2017 california geoportal california state geoportal 2020 and the european geoportal bernard et al 2005 each of these portals are custom built rather than reusing existing code and applications custom creation of web applications increases redundancy across web based environmental data applications and can be viewed as a major inefficiency and potential error source there are hundreds of such custom and purpose built web portals across the internet providing various levels of functionality dempsey 2015 esri s arcgis online arcgis online cloud based gis mapping software solution 2021 offers tools to develop and deploy simple web based mapping applications in a geoportal type environment though the closed nature of the arcgis online system prohibits extensive localization or transfer to third party web servers not operated by esri or running on esri software sharing of data and models across the internet and hence across web applications has been advanced through the development of standards to integrate models hosted on disparate servers and systems harpham et al 2019 chen et al 2020 zhang et al 2019 and by using common data elements during the application development process chang and park 2006 peckham et al 2013 this type of model and data sharing is critical to the success of any integrated web based data and modelling system and supports the idea of distributed customized web applications that can consume and display in a locally relevant way resulting data sharing of web applications at the source code level can be accomplished by using a github repository gharehyazie et al 2017 mergel 2015 a github repository allows developers to host their code online and provides version control using git torvalds and hamano 2010 developers create the application code and add it to a github repository end users can then download or clone an existing github repository to their local system using the repository s clone address which can be obtained from the github page for that repository as more developers publish their software and web applications under open source licenses on github or equivalent the opportunity arises to share web applications at the source code level by simply cloning a repository to another server and customizing the code on that server this approach can be considered as a generic web application distribution system in the same way that a developer could clone the source code for a mobile application and compile and install it on his her own device for use technically it can be done but it lacks the smooth integration of an app store and requires significantly more technical knowledge a source code repository in itself lacks a central catalog that can be searched specifically for web based environmental analysis applications compatible with the end user s system and does not install any supporting packages or services on the web server another issue is keeping track of new application versions while version control systems such as github have a mechanism known as releases which tags a repository at specific points in time with a version number using github directly without any overlay interface for versioning presents challenges for developers and users in addition these approaches are completely manual i e a developer would have to manually push all the changes to the version control system or email the end user the changed files to install the application on their servers the end user needs to have an extensive knowledge of the innerworkings of the code being shared to perform any regionalization as well as updates to the web application container technologies such as docker can also be used to share preinstalled applications using docker or other containerization systems require building a shareable package image which contains a base operating system the related components and tools required by the application as well as the application code another approach to using docker can be to include just the application code in a shareable package and a master application management tool in another package that can connect to each installed application s instance and present them to the user within a unified interface docker ensures that an application package will work on any system on which it is installed however since each application can have various dependencies and required components developing a standard package building recipe without having a common framework platform that hosts these applications is not possible and requires each application package to be built manually by the developer containerization technologies such as docker and kubernetes introduce complexity and a technical requirement of setting up the containerization platform on the hosting web server managing various containers and a deep understanding of various command line tools and network configurations needed to keep these containers connected to the internet and available to use 1 2 research objective as described previously there remains a need for a system and approach for distributing web based environmental data analysis applications for installation on localized servers operated by various government agencies we have found this to be particularly true for the group on earth observations geo global water sustainability geoglows member organizations described below such a system should enable developers to share their web applications by registering the application in a central repository allow the repository to be searchable by end users and allow users to install the application on their local web server optimally applications installed from the repository would require minimal local configuration but could be customized at will this could require a varying number of tasks involving a range of technical skills such as database integration web server modification or set up installing required geoprocessing tools and creating and configuring a host of other local web services or capabilities the system should also provide a means for updating and removing an installed application and would perform any required cleanup of the local computing environment we describe the design and development of the proposed system with a focus on discovering transferring installing and configuring web based environmental analysis applications from a central catalog into individual web portals operated by different organizations specifically we present the design and development of the tethys app store modeled after the mobile device app store concept we built the tethys app store to distribute web applications and to simplify installation of an application on third party servers the examples we present in this paper are tethys web applications mostly based on the django framework written in a combination of the python programming language with html and javascript this approach can be expanded to distribute web based applications written in other languages and web frameworks as well our tethys app store uses tethys platform and the conda packaging repository this system has similar goals to community model sharing and discovery tools such as csdms peckham et al 2013 epa smart search us epa 2019 and the earth system modelling framework esmf 2021 however the key difference is that these tools focus on easier discovery and sharing of modelling components while our system is designed for sharing web based applications which may include modelling components but also comprise a web interface to access and use those components integration with data storage services and depending on the needs of the application connections to web processing services task schedulers and high performance computing systems the system presented here was implemented and tested using a number of applications developed for member states of geoglows and the nasa servir programs hardin et al 2006 the geoglows initiative seeks to enable earth scientists to solve the challenges associated with achieving global water sustainability gutierrez et al 2018 several tethys applications have been developed and deployed through applied science research projects with nasa nsf and most recently as part of the servir applied science team ast nelson jones and ames 2016 as part of the nasa servir project as well as geoglows initiatives other principal investigators have expressed interest in creating new tethys applications and reusing existing applications to support visualization and decision making for their projects this establishes the need for improved capabilities to easily share and reuse web applications our solution allows for relatively rapid deployment of environmental analysis applications for managing and using essential water resources variables in support of geoglows this paper is organized as follows section 2 introduces the technologies we used presents the software design and architecture of the tethys app store and the design of a geoglows experimental test case section 3 1 presents the implementation of the tethys app store including steps for registering applications in the system retrieving them and installing them into a specific portal section 3 2 presents the results of the geoglows experimental test case including the deployment of three application portals using the tethys app store to populate each portal with specific applications as needed and section 4 includes a discussion of results and conclusions including potential for future maintenance and development of this system 2 methods 2 1 core technologies the tethys app store is built on two major technologies tethys platform and conda tethys platform is an open source software stack for creating web based applications swain et al 2016 and is comprised of three main elements tethys software suite tethys software development kit sdk and tethys portal tethys is built on django a popular python model view controller mvc web framework that facilitates developing web applications and deploying completed web applications in a production setting conda analytics continuum 2017 is a software distribution system which allows a server manager to pull applications from the central repository and install them with relative simplicity it is based on the anaconda software project an open source software platform that unifies several popular data science packages and standardizes the install and discovery process for these packages using the anaconda repository anaconda distribution 2020 it is a robust package management system that was designed and built to manage software packages written in any language but is most commonly used for python packages vanderplas 2016 2 2 tethys app store software design our approach to registering discovering transferring and installing web applications to a third party server is modeled somewhat after the concepts of the ios app store apple devices or the google play store android devices which enable a user to download and install applications to their mobile devices similarly the tethys app store facilitates a government agency or organization to transfer and install web based environmental analysis applications onto their own servers and hence attempts to address the installation and distribution challenge faced by developers and users historically the process for installing a completed tethys application requires the end user to follow a list of instructions that include transferring the application source code to the server running a series of installation and file management commands in the terminal shell and configuring the custom settings and services for the application using command line interfaces or the administrator interface of tethys portal this process has proved difficult for end users who are not well versed in information technology and performing command line tasks fig 1 shows the overall system architecture design for the tethys app store built as a tethys application using the conda registry and repository to store and retrieve tethys applications it also uses github actions as a build system to package the tethys application and websockets are used within the tethys app store to get user input and provide status updates from the server in section 3 we present several case studies that help to better understand the system architecture and use case 2 2 1 distribution service conda the tethys app store makes web applications portable by packaging tethys applications into a conda package and hosting them in conda repositories when a developer creates an installation package using conda the package ensures that the application components will be built compiled configured and packaged together in a way that they can be installed on another tethys portal with little additional effort the tethys app store hosts its packages in a conda repository which can be searched using a public restful web service application programming interface api the tethys app store integrates with the conda api and performs all the requests on behalf of the end user further lowering the barrier since searching and installing a package from the conda repository requires some understanding of how to search the repository the tethys app store packages are configured such that conda will find and install any other required dependencies that are listed within the tethys application each tethys application that is submitted to the tethys app store gets tagged with a version number that is updated with new versions this allows the store to keep track of any updates the tethys app store uses the conda versioning system to detect if any updates to installed applications are available and notifies the user this is another service that reduces the knowledge and time required for local users to keep their tethys server and associated applications up to date the tethys app store also helps developers if they publish their tethys applications to the tethys app store by making it easier for the end users to find install and use their applications we designed the tethys app store to facilitate access to tethys applications which use geospatial data geospatial applications require complicated server set ups making server and application set up complicated and error prone the tethys app store automates much of this work making it easy for a local user to maintain a tethys server and find and install available applications this is very similar to browsing the qgis plugin manager qgis development team 2021 for a suitable qgis plugin to install on one s local program or using the apple ios app store to find a new application to install on an iphone it helps tethys application developers make their applications available and since the developers create the conda installation packages using the tethys app store it also ensures that an installation of their application will work correctly the conda package provides a standard build recipe that each tethys application uses to guide the conda build process this standard recipe ensures that regardless of the operating system and underlying technologies on the developer s or user s machine the tethys application is always packaged in a way that it can be installed on any tethys portal that uses the app store to install applications section 2 2 4 provides more details on this process the conda repository groups its packages in various channels for ease of organization and discovery we created a new channel named tethysapp in the conda repository that is the central repository location where all tethys applications submitted to the tethys app store will be hosted we distribute the tethys app store as an open source project which provides a rich environment for developers to collaborate in making new applications modify existing ones and distributing these applications to end users these tools which make both distributing and installing applications easier supporting both developers and users should help widely in distributing cutting edge tools this same approach of standard distribution and application deployment can be applied to a variety of other scientific disciplines 2 2 2 install method the conda repository is queried for all available applications and their metadata the end user selects the application to be installed and the app store fetches the selected application s conda package it then allows for custom settings configuration and services configuration as described in the sections below we describe how using the above mentioned distribution system and underlying tools within tethys platform we orchestrate a seamless and intuitive workflow for installing new applications on to a tethys portal as shown in fig 2 the first step in the discovery process is to query the conda channel tethysapp for all available applications during the discovery step the app store extracts metadata from the query results and caches it this metadata is used to display a list of available applications with their corresponding descriptions author information versions etc to the user this list can be filtered and searched by the end user the in memory list of applications is updated whenever the tethys portal is restarted or it can be manually refreshed by the user as well the next step in the install process runs after the user selects which application they would like to install a list of available versions for that application is retrieved from the metadata cache and is presented to the user for selection upon selecting the desired version the tethys app store initiates a conda install command with the selected application s name and version during the conda install process conda performs a number of tasks including fetching the compressed archive that contains the application code installs any dependencies that the new application needs checks for compatibility between previously installed python packages and the requested packages and performs any required cleanup of the python environment this process can get extremely complex and hence this step is the most time consuming also if the conda process identifies potential conflicts between the ins talled packages and the application being installed the process cannot continue and the end user is provided debugging information to fix their local python environments this is a common issue while managing virtual environments and there is no automatic method to resolve these conflicts however if no conflicts are found the conda install process will extract the application archive into the site packages directory where it can be discovered by the tethys portal as an installed application an important aspect of the tethys app store is that each install workflow is run in a new python thread which means that it does not cause our existing thread to freeze and allows the user to continue browsing the tethys app store or performing other actions on their tethys portal without having to wait for the install to complete there are a few checkpoints within the conda install process that inform the tethys app store application of the progress of installation and these are communicated to the user as notifications the communication layer and protocol for these notifications is described in section 2 2 5 before proceeding to the next step the tethys portal running as a python process needs to be made aware of the new application this is done by clearing django caches as well as reloading of two specific python packages site and tethysapp the reloading of the site package refreshes the list of installed python packages without having to restart the python process reloading the tethysapp package refreshes the list of tethys applications that are found installed in the site packages directory of the user s python environment by default django caches various aspects of the tethys portal including a list of the installed applications the tethys app store clears the cache which forces django to re evaluate the list of installed applications for the tethys portal step 3 in the install process is to configure custom settings and tethys services which were typically configured using the portal administration interface the portal administration interface can be confusing for a user to navigate and was an extra manual step during application installation the tethys app store brings the configuration of these settings within the install workflow offering a smooth experience resulting in a fully configured application once the process is completed example custom settings include the ability to change the colors and layout of the application add custom logos and change header texts custom settings can also be used to configure access credentials api endpoints and rate limits for tethys applications that consume third party services after a successful installation of the application the tethys app store scans the application files for any custom settings this is done by loading the application configuration that is stored within the application files the tethys app store iterates over the application configuration file and retrieves all the custom settings that are required by the application this is then presented to the user over our communication layer as described in section 2 2 5 if default values are defined for those custom settings those are presented to the user as well along with a description of these settings the end user enters the values for these custom settings and the tethys app store will configure the values in the application being installed the final step in the application configuration is to set up and link any required tethys services with the application being installed the services are also loaded from the application configuration file mentioned above the tethys portal may have previously configured services that are compatible with the application being installed for each service a list of compatible services is displayed in a drop down which can be selected for use the service configuration window also contains options to create new services from the installation workflow itself without having to use the administration interface if a new service is created the tethys app store updates the lists of options for various services with any new compatible services once a compatible service definition is selected the tethys app store links that service with the tethys application being installed finally the tethys app store also allows the end user to skip any of the custom setting or service configuration if they choose to once all the above steps are completed it triggers a restart of the tethys portal after issuing commands to collect all static files and workspaces within the tethys portal since each application has its own set of static files i e javascript images cascading style sheets css etc these need to be collected by the tethys portal before they can be accessed by the end user this collection process is a standard part of tethys platform that allows it to set the right permissions on these files so that they can be accessed by the end user restarting the tethys portal is critical to reload the url schema that django uses to identify and route the user to the right applications when they visit the portal it is the only way to ensure that the newly installed application can be accessed using the tethys portal s web interface 2 2 3 updates the tethys app store presents a view to the end user which lists all the installed applications this list is obtained by querying the tethysapp python namespace which contains the paths to each installed tethys application when the tethys app store is accessed it compares each of the installed applications with their corresponding versions from the conda repository if any newer versions are found the application is highlighted on the end user s screen and the user may choose to run the update from the user interface itself the update process works very similar to the install process mentioned above but differs in three critical steps first it creates a backup of all the custom settings and tethys services that the current version of the application is using secondly it cleans and uninstalls the existing version of the tethys application being updated as leftover files and directories can cause conflicts with the newer version of the application finally after running the install process for the new version of the application the tethys app store tries to restore the custom settings and tethys services configuration to the application if the same settings are required in the new version any incompatible settings or service configurations are reported to the end user so they can be resolved 2 2 4 application submission the above mentioned approaches to application installation and updates depend on a unified method for tethys application developers to submit their applications to populate the app store developers contribute their applications by following a two step workflow within the user interface while the app store takes care of all the heavy lifting of correctly preparing the code and making it available as an easily installable conda package an important component of any application within the tethys app store is its metadata such as name description tags version license etc this metadata is used to uniquely identify the application and special application parameters such as api inputs outputs user interface instructions or similar are not stored in this metadata this information helps end users search for an application that suits their needs this metadata can be defined in the setup file that is a universal standard for python applications as well as is included when a tethys application is scaffolded filling out the fields in the setup file is the one critical step that the application developer needs to perform before submitting the application to the tethys app store the build process reads this setup file and determines the correct name and version of the application to be built as well as the following fields from setup py description keywords author name author email url and license there are two key files that are important for the build process to run conda build recipe and github workflow job definition the conda build recipe is a directory within the application folder that defines the various aspects of the application being built and instructs conda on how to run the build process this is important since various python packages have to be packaged in a way that ensures compatibility between the package and the underlying operating system that it is installed on however a tethys application doesn t contain dependencies in other languages such as c java etc and hence needs to be built as a pure python package the app store provides the conda build recipe as suggested by the official conda build documentation for pure python packages once the build recipe is generated the tethys app store needs to run the conda build process so that a compressed archive of the application code is generated which can be hosted on the conda repository one possibility is to run this on the developer s computer as a part of tethys app store however this approach will require the conda build utilities to be installed on the developer s system as well as use local system resources instead the tethys app store utilizes github actions github actions makes it easy to automate software workflows and allows for building testing and deploying applications from github daigle 2018 for open source public repositories github actions run the build and testing workflows free of charge for this purpose a github organization tethysapp was created and is used to host and run the tethys application deploy workflow this workflow is responsible for reading the input files testing that the files required for the build process are present generating the correct build and publishing the package to conda the flow diagram shown in fig 3 describes the various steps that the developer as well as the tethys app store performs to prepare and submit an application the preparation phase begins by creating a local copy of the application code once the developer has submitted the github url if the github repository contains multiple branches the developer is asked to select which branch contains the latest stable code that needs to be packaged else the master branch is used the selected branch is checked out and the conda build recipe is generated and added to the application code the github actions directory is created and the workflow action file as shown in the technical appendix section is copied into that directory finally prior to pushing this the application code to github for packaging the tethys app store cleans up the existing setup file and removes the dependency on tethys platform during the build process to speed up build and deploys the next step pushes the prepared application code to github this code is not pushed to the developer s original repository but a new repository for this application is created on the tethysapp github organization if a previous repository exists for this application it is removed prior to creating a new one the creation and removal of this repository is completely handled by the tethys app store this provides the build environment a clean slate to start on each time we need to publish a new version of an application the push triggers a github action that runs on github s servers which packages the application code into a shareable compressed archive and publishes it to the conda repository as soon as the publish happens that version of the specific tethys application will be available for the user community to download on their individual tethys portals all packages are pushed to a specific conda channel tethysapp which makes the discovery and filtering of tethys applications much easier 2 2 5 communication layer in each of the above mentioned steps there is a constant back and forth communication between the tethys app store and the end user the end user receives notifications or a prompt for an input from the tethys app store and the tethys app store in turn receives the end user s selections and response to any selections that they needed to make the tethys app store uses websockets to achieve this communication as well as detect when the tethys platform is offline due to any restarts that might happen as outlined in section 2 2 2 if the restart was left unhandled it would cause the entire install process to reset and the end user will have to start all over again just to get stuck again if a restart happens the tethys app store also saves the current state of whichever process the user was on and keeps trying to connect back to the tethys portal once the tethys app store receives a valid connection again indicating that the restart has completed successfully it restores the state and continues the step process that the user was performing before the disconnect happened 2 3 geoglows experimental test case design in partnership with geoglows efforts we tested our system by packaging the following tethys web applications and making them available in the tethys app store next we deployed 2 new tethys portals byu s tethys portal which hosts tethys applications created within the byu hydroinformatics lab for demonstration to our partners and the crrh portal the crrh sica regional committee for hydraulic resources is a technical body of the central american integration system sica specialized in meteorology climatology hydrology and hydric and hydraulic resources of the central american region a regional and customized portal for crrh was installed on their servers within each portal we used the tethys app store application to retrieve and install selected applications from this list gfs and gldas tethys applications the gldas data tool is a tool to facilitate downloading visualizing processing analyzing and sharing gldas v2 x data from nasa gldas global land data assimilation system is a historical earth observation dataset based on the lis land information system land surface model j nelson et al 2019 hydrostats tethys app this is an application based on the hydrostats python package which helps characterize predicted and observed hydrologic time series data and has three main capabilities data storage and retrieval visualization and plotting routines and a metrics library that currently contains routines to compute over 70 different error metrics and routines for ensemble forecast skill scores roberts et al 2018 geoglows ecmwf streamflow hydroviewer this tethys application presents a global streamflow viewer and animated streamflow forecast maps the data is generated by routing global runoff ensemble forecasts and global historical runoff generated by the european centre for medium range weather forecasts model using the routing application for parallel computation of discharge to produce high spatial resolution 15 day stream forecasts hydroviewer central america this is a localized version of a tethys application developed for the central america region and displays the model results as described above it also facilitates data consumption and integration at the local level this application has the potential to allow decision makers to focus on solving some of the most pressing water related issues we face as a society souffront alcantara et al 2019 snow inspector an open source tethys application for snow cover probability time series extraction from map images the application also contains a waterml compatible web api which gives access to third party applications for automation and embedding in modeling tools kadlec et al 2016 3 results 3 1 resulting software implementation the tethys app store was implemented as a tethys application and can currently be obtained from github or a conda install the tethys app store is only compatible with tethys portal versions 3 0 and above more information on how to obtain the app store is found in the software availability section below this paper is focused on the discovery download and installation of applications on local web servers specific application end user graphical interfaces and instructions are provided by individual developers who have contributed applications to the tethys app store the tethys app store is located on the end user s tethys portal application library page this installation is done by running a conda install command for the tethys app store the end user will only be able to access the tethys app store if they are logged in as an admin user of the portal since this tethys app store can make significant changes to the installed portal and its applications it was critical to enforce this login requirement the tethys app store is launched by clicking on the app store icon on the application library page upon opening the app store the end user is presented with the landing page as shown in fig 4 the tethys app store has 3 major components as highlighted above each component is responsible for an important feature of the tethys app store and their usage is demonstrated in the sections below the first section displays the list of available applications for install this is the most important component of the tethys app store as it searches through the conda repository and displays a list of available tethys applications that the end user may install on their system only applications that are developed for tethys version 3 0 and packaged with compatibility with the tethys app store are shown in this list the end user can also use the search interface on the table to search and filter for a specific application based on keywords once a suitable tethys application is found the end user can start the install process by clicking on the install button the first step for the end user is to select which version of the application they wish to install from the version drop down after selecting the version and clicking the go button the installation process begins with the conda install step as described above in the package installation methodology section 2 1 2 the user gets updates on the conda process as it is running in the background once that process completes the end user may choose to configure any custom settings the tethys application might need helpful description and default values if present in the tethys application are also displayed fig 5 shows an example of the custom setting configuration modal fig 6 shows the final step in the installation process which is the configuration of any services geoserver postgis etc that the tethys application might need upon completing skipping the custom settings configuration modal the end user is presented with a service configuration modal the modal dialog box lists all services that are described within the application and allows the end user to link an existing service to the application the tethys app store supports configuration of all types of services that are present in the tethys portal the end user may also choose to create a new service of the type that the application needs by clicking the create new service button as seen in fig 7 following the configuration of these services the installation process is completed and the app store restarts the tethys portal instance for changes to take effect the second section in fig 4 shows a list of applications that the user has installed using the app store and allows for management of these installed applications from this table the end user can choose to uninstall or reconfigure any settings for the corresponding tethys application if the installed version of the application is lower than the latest available version an update button is also shown which can be used to update the application tethys app store also offers an interface and helpful utilities that allows developers to prepare and submit their application to the app store s repository any tethys application that has been developed in compliance with the instructions found on tethys documentation will be compatible with this process the tethys application once ready to publish should exist in a github repository to get the process started the developer will click on the add application button located on the top right corner of the app store interface fig 8 shows the modal presented to the user where they will be required to enter the web address of the github repository for their application a list of available branches from the github repository is presented to the developer after selecting the correct branch the application is prepared and packaged as described in section 2 2 4 if any errors happen during this build process the developer is informed of those errors within the same modal upon successful completion of the build the developer receives an email informing them that a new version of their application has been published in the app store 3 2 case study results figs 9 and 10 show the two new tethys portals that were installed on ubuntu virtual machines crrh and byu s main tethys portal both of these portals were set up following the production installation steps as described in the official tethys platform documentation following portal install and configuration the tethys app store was installed on each portal by running the conda install command in the command line interface this was the only application that needed to be installed via the command line once the app store was installed on each portal the tethys app store was used to fetch install and configure the other applications required for the portal below is a list of contributions of this work based on our expertise with tethys platform since its inception and or experience training new web app developers searching for an application all registered applications can be easily searched from within the app store interface instead of performing a search through various github repositories and using global search engines such as google retrieving application code and installing the app store downloads the code and installs dependencies sets up services and configures custom settings via an interactive user interface without the app store the user would need to run commands on the command line and understand python virtual environments and github to fetch the code and install it also setting up services via the command line for tethys has proven to be a complicated process and can be avoided by using the app store installation cleanup for the newly installed application to load correctly the portal needs to collect all the static files from the application code and perform a restart the app store takes care of collecting the static files for the portal and performs a portal restart without requiring the user to manually enter the right commands in the command line prompt updating and installed application the app store informs the user of a new version that is available and does all the heavy lifting of updating an application including retrieving the new code installing it and restoring any custom setting or service configurations uninstalling an application the app store allows the user to remove an installed application completely by running the tethys uninstall command performing a cleanup of any leftover application files and restarting the portal for the changes to take effect no user interaction is required after confirming the installation in terms of scalability since the tethys app store actually does not host any applications itself and uses the conda packaging system this architecture is as scalable as conda the conda packaging system is highly scalable as conda forge which is the main channel for most packages saw 3751 new additions and over 100 million downloads a month for 2020 2020 in review the conda forge blog 2020 4 conclusions easier discovery installation and management of environmental analysis web applications is made possible by the introduction of an app store the requirements for such an app store and design methodology are discussed with tethys platform the design is implemented to create an interface to search for existing tethys applications install and manage them as well as submit new applications to the collection of available tethys applications for improved sharing across regional boundaries we were able to test the tethys app store by populating two new tethys portals with applications from the tethys app store all steps including installation and configuration of custom settings and services was done from within the interactive user interface of the tethys application store a decrease in the time and skill required for application installation and management is observed in the results of our test case future improvements to the tethys app store include a review process so that newly submitted applications can be reviewed by the tethys app store managers and verify if the application passes the standards of the tethys community the tethys app store also serves as a proof of concept that the sharing of web based environmnetal analysis applications can be simplified and standardized to implement a similar app store for other web based application frameworks the following aspects need to be considered a standard process for packaging the web application that works across various operating systems a centralized list of all packaged and available web applications and finally an intuitive user interface to perform the retrieval installation and management of these applications 5 software availability name of the software tethys app store developer rohit khattar developer email m year first available 2020 hardware required no specific requirements any system that can run and support python will be compatible with this software software required o python version 3 7 or higher https www python org downloads o miniconda conda for python version 3 7 or higher https docs conda io en latest miniconda html o tethys platform version 3 0 or higher http docs tethysplatform org en stable installation html o the software has been tested with conda environments setup on ubuntu 16 macos x and windows 10 cost this application is open source free to use and is distributed under the bsd 3 clause license software repository https github com byu hydroinformatics tethysapp tethys app store documentation details on how to obtain the software and install it locally can be found at https tethys app store readthedocs io en latest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by nasa grant number 80nscc18k0440 an amerigeoss cloud based platform for rapid deployment of geoglows water and food security nexus decision support apps author contributions to this article are as follows khattar conducted the bulk of the work and is the primary contributor hales helped with software development and technical support ames organized the funding coordinated the research and helped write the text nelson jones and williams assisted with the research and contributed to the text editorial conflict of interest statement given his role as environmental modelling software editor in chief daniel p ames was not involved in the peer review of this article and has no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to journal editor min chen technical appendix this section contains additional screenshots code snippets and technical information about the tethys app store the following code snippet show the setup py file definition for the store application this refers to the setup py file mentioned in section 2 2 4 code snippet 1 fields that can be configured in setup py code snippet 1 by default when a tethys application is scaffolded the setup includes a call to a function which is part of the tethys portal package leaving that dependency in would force the app store to require the tethys platform package during the build process which increases build times instead the cleanup code replaces the call to that method by adding the actual function definition of find resource files to the setup py file hence bypassing the need to include the tethys platform package during build code snippet 2 before setup py cleanup code snippet 2 code snippet 3 after setup py cleanup code snippet 3 code snippet 4 is an example of the build recipe that is generated by the app store the build recipe is derived from a standard build recipe and instructions found at https docs conda io projects conda build en latest resources define metadata html the build recipe uses a popular markup language yaml ben kikievans and brian ingerson 2009 which is commonly used by programmers to set up configuration files for their applications amongst its many uses conda also supports templating using jinja ronacher 2009 within its recipe files which allows the app store to get the metadata from the setup py file instead of forcing the developer to enter the same information in multiple files a description of each of the sections is provided below code snippet 4 conda build recipe package this configures the name and the version under which this application will be available in the app store about the about section contains some more standard metadata that is used to display and filter applications in the app store source specifies the path to the directory containing the application code since this recipe is within a subdirectory conda recipes the path tells the build command to look for application files one level above the recipe directory build these parameters instruct conda on how to build the application for tethys applications these parameters instruct conda to build a pure python package which is not tied with any specific operating system and hence can be distributed to any other tethys portal requirements tethys applications can have dependencies on other python libraries for various reasons including geoprocessing data analysis and scientific file parsing tethys platform design pattern requires the developers to list all the dependencies in an application s install yml file which is provided when a new tethys application is scaffolded this list is copied over to the conda build recipe during the file preparation stage avoiding the need for the developer to list the dependencies twice outputs and extra these sections provide fields for additional metadata code snippet 4 code snippet 5 describes the github action workflow that is run on github s servers when the application is uploaded after being prepared and packaged within the tethys app store code snippet 5 github action workflow for tethys applications code snippet 5 
