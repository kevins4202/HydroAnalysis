index,text
5660,urban flooding is a critical challenge in metropolitan cities around the world thus urban flood forecasting is required to support water related managers in mitigating damage nevertheless the accuracy of rainfall forecasting systems remains limited for example the predictions of radar based systems are often inaccurate for heavy rainfall events this study proposes a framework that couples a forecasting system and a developed 1d 2d urban hydrological model to predict water levels and inundation phenomena in an urban catchment in the framework a long short term memory lstm network uses the quantitative precipitation forecasts qpfs of the mcgill algorithm for precipitation nowcasting by lagrangian extrapolation maple system to reproduce three hour mean areal precipitation map forecasts a coupled 1d 2d urban hydrological model was also developed in this study the gangnam urban catchment located in seoul south korea was selected as a case study for the proposed framework to train and test the lstm model a database was established based on 24 heavy rainfall events 22 grid points from the maple system and the observed map values estimated from five ground rain gauges the corrected map forecasts were input into the developed coupled model to predict water levels and relevant inundation areas the results indicate the viability of the proposed framework for generating three hour map forecasts and urban flooding predictions this study demonstrates that despite slightly underestimating extreme values of rainfall and peak water levels for certain events the framework has high practicability and can be used to improve map forecasts and urban inundation forecasts keywords radar based forecasting system mean areal precipitation long short term memory coupled 1d 2d model urban flooding 1 introduction urban flooding is one of the most severe hazards in most cities worldwide urban inundation is becoming less predictable and more complicated than in the past with large uncertainties because of hydrological and hydrometeorological variations driven by overall environmental change aich et al 2016 hine and hall 2010 forecasts of urban flooding are required to support water resource management and to mitigate the damage caused by floods nevertheless the accuracy of forecasts remains limited due to the uncertainties associated with rainfall prediction models such as radar based systems foresti et al 2016 these limitations may lead to the ineffective alleviation of urban flood damage the coupling of meteorological and urban hydrological models is the most popular method for predicting streamflow and inundation phenomena in urban catchments this approach has received considerable attention from scientists in such studies the outputs of radar based forecasting systems were utilized to drive urban hydrological models lee et al 2013 ravazzani et al 2016 the accuracy of inundation prediction in this framework strongly depends on the performance of radar based forecasting systems however the forecasting performance of radar based forecasting algorithms that are commonly based on the lagrangian extrapolation of the rain fields observed by weather radar systems decreases with increasing lead time notably in the case of small scale rain fields and heavy rainfall events the quality of forecasts is appropriate only at the scale of a few tens of minutes sokol et al 2017 this uncertainty may lead to imprecise inundation forecasts in fact the operational radar based system of the mcgill algorithm for precipitation nowcasting by lagrangian extrapolation maple is widely used to make forecasts from composite radar images maple was first introduced by germann and zawadzki 2002 and includes the variational echo tracking vet laroche and zawadzki 1994 and semi lagrangian advection methods the maple system was implemented in korea with a minor change in the algorithm bellon et al 2010 lee et al 2010 the findings of the implementation illustrated that based on the contiguous rain area method with a threshold of 1 mm h the system could produce forecasts effectively up to 2 5 h in advance for both the rainfall location and amount of rainfall nevertheless due to the intrinsic drawbacks of the lagrangian extrapolation algorithm including those related to the growth and decay of precipitation and changes in the movement of rain fields germann et al 2006 the system issues poor forecasts for severe rainfall events additionally sources of uncertainty can exist in maple input data quantitative precipitation estimate qpe data such as mean field systematic errors e g the radar reflectivity rainfall intensity z r relationship range dependent systematic errors random errors and radar to radar calibration differences bellon et al 2010 yoon et al 2012 which can affect the accuracy of forecasts therefore it is necessary to improve the quality of rainfall forecasts and enhance the performance of urban flooding prediction for the heavy rainfall events mean areal precipitation map is one of the essential inputs for studying urban hydrological processes and urban water management especially for lumped models however map forecasts extracted from the maple system have low accuracy for heavy rainfall events resulting in poor predictions in hydrological applications to correct the forecasted map for such events artificial neural networks anns provide a potential postprocessing solution this approach may lead to quickly correcting map forecasts without requiring other sources of qpf data in this study we propose a long short term memory lstm network to improve the quality of map forecasts the lstm is a modern type of recurrent neural network rnn that involves feedback links in the architecture of the network in the rnn the output from the previous step is applied as the input of the current step and the input and output of anns are unconstrained however one drawback of the rnn is that it can struggle to learn long sequences hence training can be extremely problematic and lead to the vanishing exploding gradient problem hochreiter and urgen schmidhuber 1997 lstm can solve this problem in rnn training by considering the short term state and long term state lstm network can identify valuable inputs save them in the long term state and extract this information whenever it is needed lipton et al 2015 in recent years several studies have demonstrated the superiority of lstm compared to other ann models in hydrological forecasting e g zaytar and amrani 2016 asanjan and yang 2018 zhang et al 2018 hu et al 2018 modeling urban inundation is difficult and challenging due in part to the complex interactions between sewer drainage systems and the overland surface in urban areas leandro et al 2016 an effective way to reflect the flow interchange between sewer and overland systems is to couple a one dimensional 1d sewer model with a two dimensional 2d overland model some of the commercial coupled 1d 2d models allow rainwater to flow out of and return to the sewer system depending on the hydraulic conditions and such models include mike 21 carr and smith 2006 sobek urban bolle et al 2006 tuflow phillips et al 2005 and xp swmm xp software 2013 moreover kidmose et al 2015 coupled mike she and mouse to simulate the urban freshwater cycle in addition a large number of coupled models have been developed without relying on any commercial model for urban inundation simulation and prediction these models include sipson uim leandro et al 2009 sipson p dwave leandro and martins 2016 swmm lisflood fb wu et al 2017 and swmm brezo adeogun et al 2015 huong and pathirana 2013 additionally some researchers have developed components of or full coupled 1d 2d models including li et al 2009 seyoum et al 2012 zhang and pan 2014 and fan et al 2017 in this study we attempt to develop a coupled 1d 2d model with a simple model structure for a small urban catchment in the model the 1d component is based on two modules of the storm water management model swmm version 4 4 h the objective of this study is to propose a framework to correct the map forecasts of radar based forecasting systems and improve urban flood predictions to reproduce the map forecasts an lstm network was constructed and trained based on a database of forecasted grid data from the maple system and map estimated from observation gauges the developed 1d 2d urban inundation model was utilized to quantify the improvements in predictions of urban flooding by comparing the drivers corrected map uncorrected map from the maple and observed map a three hour forecasting time was selected based on the reliable lead time of the maple system and the short duration of heavy rainfall events to test this framework the gangnam catchment located in seoul south korea was used as a case study detailed information on the study area and data processing steps is introduced in section 2 a detailed description of the methods is presented in section 3 the performance of the methods is demonstrated and discussed in section 4 finally the conclusions are given in section 5 2 study area and data processing 2 1 study area the gangnam catchment which is located in the southern part of seoul south korea was selected as the study area the seoul metropolitan area is one of the largest urbanized cities in korea consequently heavy rainfall events usually result in urban flooding within a few hours in the summer season additionally in the city there are some residential and industrial areas located in flood plains therefore the damage caused by urban flooding can be extensive the gangnam catchment is situated in the gangnam district which is seoul s downtown area this district has a high density of residential and commercial areas the geomorphology of the catchment includes relatively low lying areas with complicated drainage networks compared to those in the surrounding areas therefore this catchment is susceptible to flooding during heavy rainfall events and flood damage such as that observed for major events in 2010 2011 and 2013 there are five drainage areas in gangnam including nonhyeon yeoksam seocho 1 seocho 2 and seocho 3 which jointly cover an area of 7 4 km2 the locations of the gangnam catchment drainage system rain gauges and forecasting data points are shown in fig 1 where five rain gauges and the points of forecasted data are denoted by triangles and circles fig 1 b respectively 2 2 rainfall observations and forecasting data from the maple system rainfall measurement data with a recording resolution of 1 min from 5 ground gauges named 400 401 415 421 and 889 were obtained from the automatic weather system aws of the korean meteorological administration kma a series of strong rainfall events was selected and processed during the period from 2009 to 2018 the 1 min data from the rain gauges were collected during the period which included twenty four events as shown in table 1 table 1 illustrates the events and the corresponding configuration used in this study additionally the forecasting data from the maple system over the same duration were collected and processed the domain of the maple system implemented in the korean radar network is 1024 1024 pixels with a 1 km horizontal resolution the composite radar network used as the input to maple is based on data from 11 korean radar systems six hour quantitative precipitation forecasts qpfs were generated every 10 min with a 10 min time step bellon et al 2010 lee et al 2010 in this study three hour qpfs were collected to construct the lstm model because of the high uncertainty of maple 6 hour forecasting data and the short duration of heavy rainfall events the gangnam catchment is covered by twenty two grid points in the maple system the qpfs of the grid points were used as inputs for the proposed model at each grid point the qpfs that were generated from the beginning of each storm event to one hour later were examined to obtain data for the training stage the number of time steps in the maple data was based on the duration of each event and experimental method it is noted that the temporal resolution of observed rainfall used in this research is one minute and the temporal resolution of the forecasted data utilized herein is 10 min hence the 1 min observed rainfall was processed to obtain 10 minute rainfall after missing data were corrected using the inverse distance weighting idw method kurtzman et al 2009 then the observed map was estimated with the 10 min observed rainfall data from ground gauges using the thiessen polygons method the observed map data were used as the true values output for the lstm model in the training stage 2 3 training datasets to provide the input and output to the neural network the forecasted data of twenty two grids and observed map should be established with the same time scale the time step length of maple qpfs from the beginning of each storm event was considered approximately 1 0 h the input data for the training and validation stages were collected from nineteen events as shown in table 1 the nineteen events were arranged as continuous time series data for the training set consequently an input matrix was generated with twenty two columns representing the grid data of qpfs covering the entire gangnam catchment the rows of this matrix depended on the total number of three hour qpfs taken from the test events simultaneously the vector output of the observed map was produced at the same time scale as the input matrix the remaining five events in table 1 were used in the testing stage to test the lstm model three qpfs for each test event were selected starting at the beginning of the event the reason for this selection is to avoid zero forecasts for the final qpfs of events due to the short duration of the selected heavy rainfall events 2 4 sewer network and topographical data the sewer system of the gangnam catchment includes 4170 manholes conduits with a total length of 200698 km and two drainage pump systems at the sapyeong and seocho stations however to obtain input data for runoff analysis with the coupled 1d 2d model sewer system data were collected from the seoul drainage network map and then simplified the conduit input data were simplified in the coupled model using a minimum diameter of 450 mm and an average length of 90 m finally after the simplification process 1056 manholes 1445 conduits and 1053 subcatchments were used digital elevation models dems of gangnam were used after quality control processing by the han river flood control office the dems used in this study were based on high resolution 6 6 m elevation datasets the gangnam catchment is mainly covered by buildings and paved streets that spread to the foot of umyeon mountain thus to illustrate the blockage effect of buildings on overland flows from the original dem the height of the buildings was set to a fixed value of 200 m shown in fig 1 d the slope of subcatchments was estimated using the original dem the calculated slopes range from 0 001 to 10 29 the initial infiltration parameters of the horton equation were determined based on the soil types in the subcatchments 3 methodology 3 1 lstm network at time t rnns determine the current state of the input vector x t and the output state at the previous time step h t 1 ishak et al 2003 lipton et al 2015 the equations of the neural state at t 1 and t are shown below 1 h t σ w h h t 1 w i x t b 2 h t 1 σ w h 1 h t w i 1 x t 1 b where h t 1 h t and h t 1 are states of the hidden neuron at time steps t 1 t and t 1 respectively w h 1 and w h are the weights between input values and hidden neurons w i 1 and w i are the weights between input hidden neurons b is the bias term and σ is the activation function in the training stage of the rnn the backpropagation through time bptt algorithm is used the difficulty in the training of a long term sequence associated with the vanishing exploding gradient problem can be overcome by the lstm architecture gers et al 2000 lipton et al 2015 the lstm network is a particular type of rnn and is mainly designed to learn long sequences in the hidden layer the hidden neurons are replaced with lstm cells with four layers including a main layer as the input modulation gate g and three other layers as gate controllers the forget gate f input gate i and output gate o fig 2 shows the structure of a simple rnn and an lstm cell used in the hidden neuron layer of the rnn model the notation c represents a memory cell the lstm cell algorithm is based on the following equations 3 i t σ s w hi h t 1 w i x t b i 4 f t σ s w hf h t 1 w f x t b f 5 o t σ s w ho h t 1 w o x t b o 6 g t t a n h w hg h t 1 w g x t b g cell state 7 c t f t c t 1 i t g t output vector 8 h t y t o t t a n h c t where x t is the input vector w w and b are weight and bias parameters is the scalar product of two vectors σ s z 1 1 e z is the sigmoid function and t a n h z e z e z e z e z is the hyperbolic tangent function for more detailed information refer to gers et al 2000 and lipton et al 2015 briefly the lstm can control information overflow with a sigmoid function σ s which outputs values between 0 and 1 a value of 0 indicates that information flow is completely restricted and a value of 1 reflects complete information flow with the tanh function the model can overcome the vanishing gradient problem the tanh function helps to hold the second derivative for a long range before decreasing to 0 moreover in the equations h t plays a role in considering the short term state and c t plays a role in considering the long term state with these types of algorithms the lstm model can make predictions in which previous significant values are maintained at a far distance therefore lstm can be a suitable method for training datasets sorted as a long time series qpfs of an event joined with the set of events as described in section 2 3 in this study the lstm network model was programmed using version 1 8 0 of the tensorflow library which is an open source program for machine learning that was published by google in 2015 in addition to tensorflow other sources of python libraries such as numpy sklearn and matplotlib were used the errors in the training datasets and validation datasets were used as the basis to calibrate the hyperparameters such as the number of hidden layers number of neutrons in each layer batch size and number of iterations 3 2 coupled 1d 2d urban inundation model the coupled model was developed with two main models a 1d conduit network model and a 2d overland flow model detailed information about the two models is presented in the following sections 3 2 1 1d conduit network model in this study the conduit network model was developed mainly based on two modules of swmm 4 4h the runoff and extran modules swmm was first developed in 1971 as an open source rainfall runoff model by the us environmental protection agency epa the runoff module can simulate the runoff quantity and quality in a drainage basin and runoff routing to the sewer system the extran module can dynamically simulate the hydraulic flow along open channels and closed conduit systems this module is widely recognized as one of the most typical and efficient components of swmm leandro and martins 2016 detailed depictions of the two modules of swmm can be found in the previous literature e g huber and dickinson 1992 3 2 2 the 2d overland flow model to simulate urban flooding it is necessary to couple a 1d conduit network model with a 2d overland flow model in this study we developed a 2d overland flow model to link the surcharging flows at the manholes of the 1d sewer network model and to route the overland flow on the surface using two dimensional diffusive forms of the saint venant equations of continuity and momentum to consider the surcharge flows from the output of the 1d model the 2d continuity equation in partial differential form is written as follows 9 h t q x x q y y q where h is surface water depth q x and q y are the flow rates m2 s in the x and y directions and q is the surcharge per unit area m s the partial differential momentum equations in the x and y directions are written below in simplified form based on the assumption that the acceleration terms are negligible 10 h x s 0 x s fx 11 h y s 0 y s fy where h is the surface water depth s 0 y and s fy are the bed and frictional slopes in the y direction respectively and s 0 x and s fx are the bed and frictional slopes in the x direction respectively in addition the 2d model can estimate the overland flow interactions with manholes as weir flow when no surcharge flow occurs at the manholes 12 q c w 2 g h 3 2 where q is the weir flow into a manhole m3 s c is the weir discharge coefficient w is the weir crest width m and h is the overland flow depth m 4 results and discussion 4 1 coupled 1d 2d model calibration and verification in this study the calibration and verification of the coupled 1d 2d model were conducted for the 1d module and 2d module the rainfall input of the coupled model was the observed map that was calculated from five ground gauges first to optimize the parameters of the 1d module water depth data were collected at two stations 22 0006 and 22 0002 as displayed in fig 1 c for four rainfall events 20130823 20130722 20130806 and 20160705 in this study the 20130823 and 20130722 events were used for calibration and the 20130806 and 20160705 events were used for the verification of the 1d module sewage from the seocho 3 seocho 2 and yeoksam drainage areas flows to station 22 0002 station 22 0006 is affected by the seocho 1 and nonhyeon drainage areas the initial depth of the combined sewer system was defined by the water depth using wastewater data for each area per capita per day during a period without rain the other parameters of the 1d module including the subcatchment width pipeline roughness coefficient and soil infiltration parameters were optimized based on the observed water depth data at the two stations and a trial and error method table 2 shows the calibration and verification results of the 1d module in terms of three performance parameters including the root mean square error rmse correlation coefficient r and relative error of peak depth repd the repd is an indicator of the quality of the simulated peak of the water level the values of rmse and r reflect the scatter degree and fitting status between simulations and observations these three error statistics are determined using the following equations 13 rmse i 1 n y i sim y i obs 2 n 14 r i 1 n y i sim y sim mean y i obs y obs mean i 1 n y i sim y sim mean 2 i 1 n y i obs y obs mean 2 15 repd max y i sim m a x y i obs max y i sim where y i sim is the ith simulated value of an event m y i obs is the ith observed value of an event m and y sim mean and y obs mean are the mean values of the simulations and observations of an event respectively m as shown in table 2 the rmse values for the two water level stations ranged from approximately 0 14 m to 0 22 m and the correlation coefficient values for the stations were generally greater than 0 90 the relative error of the peak depth ranged from approximately 16 to 17 at the stations it should be noted that streamflow predictions can be influenced by the errors associated with estimating map from point rainfall measurements however assessing the effects of this uncertainty is beyond the scope of this study therefore in general the 1d module displayed a satisfactory response for the four events the selected parameters were considered consistent and suitable for modeling second we used two historical inundation events namely the 20130722 event and the 20110727 event to calibrate and validate the 2d module to reduce the computational burden the 2d module was used to simulate inundation when surcharge flow occurred at the manholes the module ended the simulation at specific time based on the period of heavy rainfall in the storm events the routing time step of the 2d module was set to 0 1 s the key sensitive parameter to calibrate in this module is manning s value which is determined by the type of land use in the catchment the observed inundation depths in the gangnamdae road zone and other reported inundation zones were used for the calibration and validation of the 2d module table 3 contains the simulation results for the module the relative error for the maximum inundation depth was 6 7 and 13 3 in the calibration and validation processes respectively fig 3 shows the simulated maximum inundations for the 20130722 event fig 3 a and the 20110727 event fig 3 b the simulated inundation results are reasonable based on the reports of the seoul metropolitan government in terms of the locations and inundation zones the corresponding information was provided by lee et al 2018 and yoon and lee 2017 in general the selected parameters of the two modules were considered suitable for further modeling 4 2 correcting map forecasts 4 2 1 training of the lstm model to correct map forecasts from the radar forecasting system for heavy rainfall events in the gangnam catchment the lstm network model was developed the developed lstm model aims to reproduce three hour map forecasts based on the grid data from maple and observed map data obtained from ground rain gauges after setting maple grid points as the input matrix and map observations as the actual values output vector a trial and error procedure was implemented to adjust the hyperparameters based on the error of the training and validation datasets early stopping was applied as a regularization constraint to avoid the overfitting problem this technique was used to stop training when the validation error reached a minimum level the adam technique was chosen as the optimization algorithm kingma and ba 2015 the learning rate of the model was selected as 0 01 the rmse was selected to measure the model prediction error in this stage the optimal architecture of the lstm model has one layer and the number of hidden neurons was forty five the suitable batch size was eighteen 4 2 2 the performance of the lstm model the three qpfs that were generated from the beginning of five test events were considered to evaluate the performance of the lstm model the qpfs were separately constructed as input matrixes in the same manner as applied in the training stage these input matrixes were consequentially fed to the trained model to obtain the corrected maps the results of the corrected three hour map forecasts are provided in figs 4 and 5 which show a statistical comparison of the three hour forecasts of maple map and the lstm corrected map for 5 testing events fig 4 shows comparisons of the average rmse values and average forecasting bias fb with a 3 hour lead time for the events the fb is used to assess the ratio of total forecasted rainfall to total observed rainfall as shown below 16 fb i 1 n y i for i 1 n y i obs if fb is approximately 1 0 0 9 1 1 the forecast is considered unbiased values of fb ranging from 0 to 0 9 indicate underestimation and values larger than 1 1 reflect overestimation fig 4 shows that the lstm model provided significant improvements in comparison with the raw maple map values the model can reproduce map forecasts with lower rmse values in the studied lead time than those of map forecasts calculated from raw maple data for the five events in detail the rmse values of the raw map forecasts range from 2 0 8 7 1 8 3 3 4 1 6 9 2 2 4 3 and 3 2 5 1 and the values of the corrections range from 2 7 6 5 2 3 2 6 1 7 4 7 1 4 2 4 and 1 2 4 3 for the 20130722 event fig 4 a 20130823 event fig 4 b 20140821 event fig 4 c 20160701 event fig 4 d and 20160705 event fig 4 e respectively the rmse values of the correction show slight decreases after two thirds of the forecasting time compared to those of the raw forecasting data for the 20130823 and 20160705 events the fb values of the corrected maps for the five events are much closer to perfect value 1 than those of maple in the lead time fig 4 f k it is noted that the fb values for 20130722 improve over time however the correction still led to considerable underestimation fig 4 f the patterns of observed map the maple map forecasts and the corrected map forecasts are shown in figs 7 9 in the next section fig 5 presents the critical success index csi of the maple map and lstm corrected map values with different thresholds including 1 mm 10 min 3 mm 10 min and 5 mm 10 min the csi scores represent the success of forecasting based on the hit rate and is defined by the following equation 17 csi hr hr m f a where hr m and fa are the numbers of hits misses and false alarms respectively for an examined threshold these comparisons were made by aggregating 5 test events generally the lstm model made remarkable improvements compared to the predictions of maple in the cases of the thresholds the csi scores of the corrected maps varied in the ranges of 0 97 0 71 and 0 59 0 46 and the scores of raw data varied in the ranges of 0 56 0 31 and 0 12 0 06 in terms of thresholds of 1 mm 10 min fig 5 a and 3 mm 10 min fig 5 b respectively the improvements in the corrected maps for the threshold of 5 mm 10 min displayed acceptable scores 0 2 0 32 even though the scores of the raw maps were 0 fig 5 c the above results might be explained by the fact that the lstm model includes a feedback link additionally the lstm cells consider both long and short term states additionally the grid data from maple forecasts covering the whole catchment area were taken into account therefore lsmt may recognize the pattern of heavy rainfall events in the training stage and thus reduce the bias between raw map forecasts and observed map this approach might also help reduce the misinterpretation of the spatial pattern of storm events in addition to the improved results the model does not always effectively improve the accuracy of map forecasts from qpfs the 20130722 event is an obvious example of this drawback for such a machine learning method the dataset remains relatively small even though the number of collected events is quite large the limitation might be attributed to the volume of the dataset however the lstm model has shown improvements in the five test events and its strengths compared with those of other methods as shown below therefore the model can be calibrated sufficiently by using a large enough dataset to achieve acceptable performance additionally due to the uncertainties associated with the radar based forecasting system and its inputs the lstm model can improve the quality of the forecasts only to a certain extent but the errors of the forecasts were reduced considerably to demonstrate the comparative performance of the lstm model this study compared csi results with those obtained by two simpler models namely multiple linear regression lr and the multi layer perceptron mlp lr was chosen as it is the popular method used to conduct regression analysis in the postprocessing van schaeybroeck and vannitsem 2011 as one of the most popular anns the mlp is widely used for rainfall forecasts jabbari and bae 2018 lin and wu 2009 for a detailed description of the mlp refer to zhang et al 2018 the training and testing datasets of the two models are the same as those of the lstm model the parameters of the mlp were set in accordance with the best performance based on the training and validation errors as follows hidden layers 1 number of neurons in the hidden layer 45 and learning rate 0 01 fig 6 presents comparative csi results with thresholds of 3 mm 10 min and 5 mm 10 min the results show that the performances of the lstm network is significantly better than those of the mlp and lr models with higher csi values along a 3 hour lead time in the comparisons of high thresholds as shown in fig 6 with regard to the threshold of 3 mm 10 min fig 6 a the two models show noticeable improvements compared to the performance of maple especially the mlp model which provides slightly lower csi scores than the lstm model however concerning the threshold of 5 mm 10 min fig 6 b the csi scores of the two models are approximately 2 3 times lower than those of the lstm model notably the predictions of the simple benchmark models might be smoother than those of the lstm model these results prove that the lstm model can more effectively retain previous significant values the extreme values of rainfall than can the simpler models this advancement is critical for flood warning applications the main focus of this research in addition according to tyralis et al 2019 the results regarding the comparison of the methods may vary with different prediction times this problem should be addressed in further studies 4 3 urban flooding prediction with corrected maps increasing the accuracy of rainfall forecasts should directly lead to improvements in predictions of streamflow because rainfall is a fundamental input to hydrological models fabry 2004 in this study the raw maple map forecasts and corrected map forecasts were used to drive the coupled 1d 2d model and obtain predictions of water level at the two water level stations and urban inundation levels in the gangnam catchment to quantify the improvements these predictions of urban inundation were compared to the results of the coupled model and observed map values five testing events were used to assess the performance of the coupled model in terms of the water levels at the 22 0006 and 22 0002 stations the 20130722 event and 20160705 event were used to assess urban inundation predictions figs 7 9 illustrate the rainfall patterns from observed map original maple map and lstm corrected map values at the several time steps in the five events and the corresponding water level simulations at the two stations as shown in these figures the corrected map forecasts for the five events reasonably predict the peak water depth and water depth change over time the predicted raw maps underestimate the water depth peaks and water depth changes for the 20130722 event fig 7 the corrected map forecasts provide improved water levels in the first two hours of forecasting and then underestimate the remaining values table 4 illustrates the statistical comparisons of water level predictions obtained from the three types of map including the observed raw maple and corrected values at the three selected time steps in the five events the statistical errors based on r rmse and repd were used for comparison the lstm corrected maps predicted the water level with better accuracy than the original maple maps and the r values of the water levels simulated by the corrected maps were much better than those for water levels simulated by raw maps moreover the rmse values of the corrected values were lower than those of the original maple maps these improvements can be explained by the fact that the corrected model produced high accuracy map forecasts based on the repd values the performance obtained by using corrected maps in the model for 4 events namely 20130823 20140821 20160701 and 20160705 was acceptable with slight overestimation and slight underestimation for the remaining event 20130722 the simulated peak depth obtained by the corrected maps was slightly improved however the values were still significantly underestimated additionally for this event the water level predictions were significantly underestimated in the last hour of the lead time period these underestimations might be affected by the high uncertainty of the raw data leading to the low accuracy of rainfall correction the slight underestimations for the other events 20140821 may be explained by the artificial intelligence model generating smooth predictions and a lower amplitude of variation compared to actual rainfall patterns therefore the extreme rainfall values from the model might be underestimated in some cases to illustrate the prediction accuracy of the lstm corrected map forecasts in the two directional model table 5 and fig 10 show the predicted node flooding results and the predicted maximum inundations for the 20130722 event time step 5 30 and 20160705 event time step 8 40 notably the quantitative rainfall overflow for the corrected map was underestimated with fewer flooded manholes and a lower water depth than those based on the observed map for the 20130722 event however the locations and inundated areas predicted with the corrected map were relatively similar to the observed values fig 10 a the performance of the corrected map model for the 20160705 event was acceptable with almost the same number of flooded manholes and same flood time as those based on the observed map moreover the water depths were slightly lower than the observed values due to the slight underestimation of the maximum surcharge flow rate fig 10 b table 5 the underestimation of maple map generated nodes with no flooding in the two events generally these results demonstrate that the lstm model can be used to improve map forecasts and evaluate urban flood predictions however it may generate slight underestimations 5 conclusions to support urban water management in urban catchments it is essential to reproduce map forecasts from a radar forecasting system which is often associated with low accuracy in predictions for heavy rainfall events this paper proposed a framework in which an lstm model was constructed to improve maple map forecasts and the developed coupled 1d 2d model was used to assess the effect of corrected map forecasts on urban flood predictions the framework was established through a case study in the gangnam drainage area in seoul south korea the developed coupled 1d 2d model was tested and verified in the drainage area using four rainfall events for the 1d module and two historical flooding events for the 2d module the data from twenty four strong rainfall events including twenty two grid point values from the maple system and the observed map values extracted from five aws rain gauges were used to train and test the developed lstm model the water depth data from two observation stations and inundation reports were used to evaluate the developed urban inundation model for the relevant events the original maple map and lstm corrected map values were used as inputs to the coupled 1d 2d model to obtain urban flood predictions analyses of the rmse fb and csi for correcting map forecasts and r rmse repd and relevant urban flood terms for urban flooding predictions demonstrated the applicability of the proposed framework and ability to produce more reasonable three hour corrected map forecasts than the original maple forecasts the results show that the quality of corrected map forecasts for the catchment was improved considerably using the model the results of comparison of lstm model against two simpler models namely lr and mlp models demonstrated the superior correction capability with higher csi scores at high thresholds for the lstm model corresponding to the improvement in the map forecasts the flooding predictions also reflect remarkable increases in accuracy the results of the case study demonstrate the reliability and high practicability of the correction method used in this framework hence this method appears to be an optional approach for the postprocessing of radar based rainfall forecasting data however some limitations exist as described in sections 4 2 and 4 3 the approach to improving rainfall forecasts in this study depends strongly on training datasets and the quality of qpfs used as inputs for the trained model in addition the proposed lstm model is solely applied to heavy rainfall events thus it might not be applicable for different types of rainfall events moreover this model may generate slightly underestimated extreme values of rainfall therefore the peak water level and relevant inundation levels simulated were lower than the observed levels in some cases it should be noted that the coupling of radar based forecasting systems and coupled hydrodynamic models may help related stakeholders mitigate urban flood damage therefore future works should focus on investigating the performance of the framework in other cities further research is also needed to better understand how other types of anns can improve map forecasts and urban flood predictions within the proposed framework credit authorship contribution statement duc hai nguyen conceptualization methodology software formal analysis investigation data curation writing original draft visualization deg hyo bae methodology software investigation resources data curation writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the korea environment industry technology institute keiti through the water management research program funded by the ministry of environment moe of korea 130747 
5660,urban flooding is a critical challenge in metropolitan cities around the world thus urban flood forecasting is required to support water related managers in mitigating damage nevertheless the accuracy of rainfall forecasting systems remains limited for example the predictions of radar based systems are often inaccurate for heavy rainfall events this study proposes a framework that couples a forecasting system and a developed 1d 2d urban hydrological model to predict water levels and inundation phenomena in an urban catchment in the framework a long short term memory lstm network uses the quantitative precipitation forecasts qpfs of the mcgill algorithm for precipitation nowcasting by lagrangian extrapolation maple system to reproduce three hour mean areal precipitation map forecasts a coupled 1d 2d urban hydrological model was also developed in this study the gangnam urban catchment located in seoul south korea was selected as a case study for the proposed framework to train and test the lstm model a database was established based on 24 heavy rainfall events 22 grid points from the maple system and the observed map values estimated from five ground rain gauges the corrected map forecasts were input into the developed coupled model to predict water levels and relevant inundation areas the results indicate the viability of the proposed framework for generating three hour map forecasts and urban flooding predictions this study demonstrates that despite slightly underestimating extreme values of rainfall and peak water levels for certain events the framework has high practicability and can be used to improve map forecasts and urban inundation forecasts keywords radar based forecasting system mean areal precipitation long short term memory coupled 1d 2d model urban flooding 1 introduction urban flooding is one of the most severe hazards in most cities worldwide urban inundation is becoming less predictable and more complicated than in the past with large uncertainties because of hydrological and hydrometeorological variations driven by overall environmental change aich et al 2016 hine and hall 2010 forecasts of urban flooding are required to support water resource management and to mitigate the damage caused by floods nevertheless the accuracy of forecasts remains limited due to the uncertainties associated with rainfall prediction models such as radar based systems foresti et al 2016 these limitations may lead to the ineffective alleviation of urban flood damage the coupling of meteorological and urban hydrological models is the most popular method for predicting streamflow and inundation phenomena in urban catchments this approach has received considerable attention from scientists in such studies the outputs of radar based forecasting systems were utilized to drive urban hydrological models lee et al 2013 ravazzani et al 2016 the accuracy of inundation prediction in this framework strongly depends on the performance of radar based forecasting systems however the forecasting performance of radar based forecasting algorithms that are commonly based on the lagrangian extrapolation of the rain fields observed by weather radar systems decreases with increasing lead time notably in the case of small scale rain fields and heavy rainfall events the quality of forecasts is appropriate only at the scale of a few tens of minutes sokol et al 2017 this uncertainty may lead to imprecise inundation forecasts in fact the operational radar based system of the mcgill algorithm for precipitation nowcasting by lagrangian extrapolation maple is widely used to make forecasts from composite radar images maple was first introduced by germann and zawadzki 2002 and includes the variational echo tracking vet laroche and zawadzki 1994 and semi lagrangian advection methods the maple system was implemented in korea with a minor change in the algorithm bellon et al 2010 lee et al 2010 the findings of the implementation illustrated that based on the contiguous rain area method with a threshold of 1 mm h the system could produce forecasts effectively up to 2 5 h in advance for both the rainfall location and amount of rainfall nevertheless due to the intrinsic drawbacks of the lagrangian extrapolation algorithm including those related to the growth and decay of precipitation and changes in the movement of rain fields germann et al 2006 the system issues poor forecasts for severe rainfall events additionally sources of uncertainty can exist in maple input data quantitative precipitation estimate qpe data such as mean field systematic errors e g the radar reflectivity rainfall intensity z r relationship range dependent systematic errors random errors and radar to radar calibration differences bellon et al 2010 yoon et al 2012 which can affect the accuracy of forecasts therefore it is necessary to improve the quality of rainfall forecasts and enhance the performance of urban flooding prediction for the heavy rainfall events mean areal precipitation map is one of the essential inputs for studying urban hydrological processes and urban water management especially for lumped models however map forecasts extracted from the maple system have low accuracy for heavy rainfall events resulting in poor predictions in hydrological applications to correct the forecasted map for such events artificial neural networks anns provide a potential postprocessing solution this approach may lead to quickly correcting map forecasts without requiring other sources of qpf data in this study we propose a long short term memory lstm network to improve the quality of map forecasts the lstm is a modern type of recurrent neural network rnn that involves feedback links in the architecture of the network in the rnn the output from the previous step is applied as the input of the current step and the input and output of anns are unconstrained however one drawback of the rnn is that it can struggle to learn long sequences hence training can be extremely problematic and lead to the vanishing exploding gradient problem hochreiter and urgen schmidhuber 1997 lstm can solve this problem in rnn training by considering the short term state and long term state lstm network can identify valuable inputs save them in the long term state and extract this information whenever it is needed lipton et al 2015 in recent years several studies have demonstrated the superiority of lstm compared to other ann models in hydrological forecasting e g zaytar and amrani 2016 asanjan and yang 2018 zhang et al 2018 hu et al 2018 modeling urban inundation is difficult and challenging due in part to the complex interactions between sewer drainage systems and the overland surface in urban areas leandro et al 2016 an effective way to reflect the flow interchange between sewer and overland systems is to couple a one dimensional 1d sewer model with a two dimensional 2d overland model some of the commercial coupled 1d 2d models allow rainwater to flow out of and return to the sewer system depending on the hydraulic conditions and such models include mike 21 carr and smith 2006 sobek urban bolle et al 2006 tuflow phillips et al 2005 and xp swmm xp software 2013 moreover kidmose et al 2015 coupled mike she and mouse to simulate the urban freshwater cycle in addition a large number of coupled models have been developed without relying on any commercial model for urban inundation simulation and prediction these models include sipson uim leandro et al 2009 sipson p dwave leandro and martins 2016 swmm lisflood fb wu et al 2017 and swmm brezo adeogun et al 2015 huong and pathirana 2013 additionally some researchers have developed components of or full coupled 1d 2d models including li et al 2009 seyoum et al 2012 zhang and pan 2014 and fan et al 2017 in this study we attempt to develop a coupled 1d 2d model with a simple model structure for a small urban catchment in the model the 1d component is based on two modules of the storm water management model swmm version 4 4 h the objective of this study is to propose a framework to correct the map forecasts of radar based forecasting systems and improve urban flood predictions to reproduce the map forecasts an lstm network was constructed and trained based on a database of forecasted grid data from the maple system and map estimated from observation gauges the developed 1d 2d urban inundation model was utilized to quantify the improvements in predictions of urban flooding by comparing the drivers corrected map uncorrected map from the maple and observed map a three hour forecasting time was selected based on the reliable lead time of the maple system and the short duration of heavy rainfall events to test this framework the gangnam catchment located in seoul south korea was used as a case study detailed information on the study area and data processing steps is introduced in section 2 a detailed description of the methods is presented in section 3 the performance of the methods is demonstrated and discussed in section 4 finally the conclusions are given in section 5 2 study area and data processing 2 1 study area the gangnam catchment which is located in the southern part of seoul south korea was selected as the study area the seoul metropolitan area is one of the largest urbanized cities in korea consequently heavy rainfall events usually result in urban flooding within a few hours in the summer season additionally in the city there are some residential and industrial areas located in flood plains therefore the damage caused by urban flooding can be extensive the gangnam catchment is situated in the gangnam district which is seoul s downtown area this district has a high density of residential and commercial areas the geomorphology of the catchment includes relatively low lying areas with complicated drainage networks compared to those in the surrounding areas therefore this catchment is susceptible to flooding during heavy rainfall events and flood damage such as that observed for major events in 2010 2011 and 2013 there are five drainage areas in gangnam including nonhyeon yeoksam seocho 1 seocho 2 and seocho 3 which jointly cover an area of 7 4 km2 the locations of the gangnam catchment drainage system rain gauges and forecasting data points are shown in fig 1 where five rain gauges and the points of forecasted data are denoted by triangles and circles fig 1 b respectively 2 2 rainfall observations and forecasting data from the maple system rainfall measurement data with a recording resolution of 1 min from 5 ground gauges named 400 401 415 421 and 889 were obtained from the automatic weather system aws of the korean meteorological administration kma a series of strong rainfall events was selected and processed during the period from 2009 to 2018 the 1 min data from the rain gauges were collected during the period which included twenty four events as shown in table 1 table 1 illustrates the events and the corresponding configuration used in this study additionally the forecasting data from the maple system over the same duration were collected and processed the domain of the maple system implemented in the korean radar network is 1024 1024 pixels with a 1 km horizontal resolution the composite radar network used as the input to maple is based on data from 11 korean radar systems six hour quantitative precipitation forecasts qpfs were generated every 10 min with a 10 min time step bellon et al 2010 lee et al 2010 in this study three hour qpfs were collected to construct the lstm model because of the high uncertainty of maple 6 hour forecasting data and the short duration of heavy rainfall events the gangnam catchment is covered by twenty two grid points in the maple system the qpfs of the grid points were used as inputs for the proposed model at each grid point the qpfs that were generated from the beginning of each storm event to one hour later were examined to obtain data for the training stage the number of time steps in the maple data was based on the duration of each event and experimental method it is noted that the temporal resolution of observed rainfall used in this research is one minute and the temporal resolution of the forecasted data utilized herein is 10 min hence the 1 min observed rainfall was processed to obtain 10 minute rainfall after missing data were corrected using the inverse distance weighting idw method kurtzman et al 2009 then the observed map was estimated with the 10 min observed rainfall data from ground gauges using the thiessen polygons method the observed map data were used as the true values output for the lstm model in the training stage 2 3 training datasets to provide the input and output to the neural network the forecasted data of twenty two grids and observed map should be established with the same time scale the time step length of maple qpfs from the beginning of each storm event was considered approximately 1 0 h the input data for the training and validation stages were collected from nineteen events as shown in table 1 the nineteen events were arranged as continuous time series data for the training set consequently an input matrix was generated with twenty two columns representing the grid data of qpfs covering the entire gangnam catchment the rows of this matrix depended on the total number of three hour qpfs taken from the test events simultaneously the vector output of the observed map was produced at the same time scale as the input matrix the remaining five events in table 1 were used in the testing stage to test the lstm model three qpfs for each test event were selected starting at the beginning of the event the reason for this selection is to avoid zero forecasts for the final qpfs of events due to the short duration of the selected heavy rainfall events 2 4 sewer network and topographical data the sewer system of the gangnam catchment includes 4170 manholes conduits with a total length of 200698 km and two drainage pump systems at the sapyeong and seocho stations however to obtain input data for runoff analysis with the coupled 1d 2d model sewer system data were collected from the seoul drainage network map and then simplified the conduit input data were simplified in the coupled model using a minimum diameter of 450 mm and an average length of 90 m finally after the simplification process 1056 manholes 1445 conduits and 1053 subcatchments were used digital elevation models dems of gangnam were used after quality control processing by the han river flood control office the dems used in this study were based on high resolution 6 6 m elevation datasets the gangnam catchment is mainly covered by buildings and paved streets that spread to the foot of umyeon mountain thus to illustrate the blockage effect of buildings on overland flows from the original dem the height of the buildings was set to a fixed value of 200 m shown in fig 1 d the slope of subcatchments was estimated using the original dem the calculated slopes range from 0 001 to 10 29 the initial infiltration parameters of the horton equation were determined based on the soil types in the subcatchments 3 methodology 3 1 lstm network at time t rnns determine the current state of the input vector x t and the output state at the previous time step h t 1 ishak et al 2003 lipton et al 2015 the equations of the neural state at t 1 and t are shown below 1 h t σ w h h t 1 w i x t b 2 h t 1 σ w h 1 h t w i 1 x t 1 b where h t 1 h t and h t 1 are states of the hidden neuron at time steps t 1 t and t 1 respectively w h 1 and w h are the weights between input values and hidden neurons w i 1 and w i are the weights between input hidden neurons b is the bias term and σ is the activation function in the training stage of the rnn the backpropagation through time bptt algorithm is used the difficulty in the training of a long term sequence associated with the vanishing exploding gradient problem can be overcome by the lstm architecture gers et al 2000 lipton et al 2015 the lstm network is a particular type of rnn and is mainly designed to learn long sequences in the hidden layer the hidden neurons are replaced with lstm cells with four layers including a main layer as the input modulation gate g and three other layers as gate controllers the forget gate f input gate i and output gate o fig 2 shows the structure of a simple rnn and an lstm cell used in the hidden neuron layer of the rnn model the notation c represents a memory cell the lstm cell algorithm is based on the following equations 3 i t σ s w hi h t 1 w i x t b i 4 f t σ s w hf h t 1 w f x t b f 5 o t σ s w ho h t 1 w o x t b o 6 g t t a n h w hg h t 1 w g x t b g cell state 7 c t f t c t 1 i t g t output vector 8 h t y t o t t a n h c t where x t is the input vector w w and b are weight and bias parameters is the scalar product of two vectors σ s z 1 1 e z is the sigmoid function and t a n h z e z e z e z e z is the hyperbolic tangent function for more detailed information refer to gers et al 2000 and lipton et al 2015 briefly the lstm can control information overflow with a sigmoid function σ s which outputs values between 0 and 1 a value of 0 indicates that information flow is completely restricted and a value of 1 reflects complete information flow with the tanh function the model can overcome the vanishing gradient problem the tanh function helps to hold the second derivative for a long range before decreasing to 0 moreover in the equations h t plays a role in considering the short term state and c t plays a role in considering the long term state with these types of algorithms the lstm model can make predictions in which previous significant values are maintained at a far distance therefore lstm can be a suitable method for training datasets sorted as a long time series qpfs of an event joined with the set of events as described in section 2 3 in this study the lstm network model was programmed using version 1 8 0 of the tensorflow library which is an open source program for machine learning that was published by google in 2015 in addition to tensorflow other sources of python libraries such as numpy sklearn and matplotlib were used the errors in the training datasets and validation datasets were used as the basis to calibrate the hyperparameters such as the number of hidden layers number of neutrons in each layer batch size and number of iterations 3 2 coupled 1d 2d urban inundation model the coupled model was developed with two main models a 1d conduit network model and a 2d overland flow model detailed information about the two models is presented in the following sections 3 2 1 1d conduit network model in this study the conduit network model was developed mainly based on two modules of swmm 4 4h the runoff and extran modules swmm was first developed in 1971 as an open source rainfall runoff model by the us environmental protection agency epa the runoff module can simulate the runoff quantity and quality in a drainage basin and runoff routing to the sewer system the extran module can dynamically simulate the hydraulic flow along open channels and closed conduit systems this module is widely recognized as one of the most typical and efficient components of swmm leandro and martins 2016 detailed depictions of the two modules of swmm can be found in the previous literature e g huber and dickinson 1992 3 2 2 the 2d overland flow model to simulate urban flooding it is necessary to couple a 1d conduit network model with a 2d overland flow model in this study we developed a 2d overland flow model to link the surcharging flows at the manholes of the 1d sewer network model and to route the overland flow on the surface using two dimensional diffusive forms of the saint venant equations of continuity and momentum to consider the surcharge flows from the output of the 1d model the 2d continuity equation in partial differential form is written as follows 9 h t q x x q y y q where h is surface water depth q x and q y are the flow rates m2 s in the x and y directions and q is the surcharge per unit area m s the partial differential momentum equations in the x and y directions are written below in simplified form based on the assumption that the acceleration terms are negligible 10 h x s 0 x s fx 11 h y s 0 y s fy where h is the surface water depth s 0 y and s fy are the bed and frictional slopes in the y direction respectively and s 0 x and s fx are the bed and frictional slopes in the x direction respectively in addition the 2d model can estimate the overland flow interactions with manholes as weir flow when no surcharge flow occurs at the manholes 12 q c w 2 g h 3 2 where q is the weir flow into a manhole m3 s c is the weir discharge coefficient w is the weir crest width m and h is the overland flow depth m 4 results and discussion 4 1 coupled 1d 2d model calibration and verification in this study the calibration and verification of the coupled 1d 2d model were conducted for the 1d module and 2d module the rainfall input of the coupled model was the observed map that was calculated from five ground gauges first to optimize the parameters of the 1d module water depth data were collected at two stations 22 0006 and 22 0002 as displayed in fig 1 c for four rainfall events 20130823 20130722 20130806 and 20160705 in this study the 20130823 and 20130722 events were used for calibration and the 20130806 and 20160705 events were used for the verification of the 1d module sewage from the seocho 3 seocho 2 and yeoksam drainage areas flows to station 22 0002 station 22 0006 is affected by the seocho 1 and nonhyeon drainage areas the initial depth of the combined sewer system was defined by the water depth using wastewater data for each area per capita per day during a period without rain the other parameters of the 1d module including the subcatchment width pipeline roughness coefficient and soil infiltration parameters were optimized based on the observed water depth data at the two stations and a trial and error method table 2 shows the calibration and verification results of the 1d module in terms of three performance parameters including the root mean square error rmse correlation coefficient r and relative error of peak depth repd the repd is an indicator of the quality of the simulated peak of the water level the values of rmse and r reflect the scatter degree and fitting status between simulations and observations these three error statistics are determined using the following equations 13 rmse i 1 n y i sim y i obs 2 n 14 r i 1 n y i sim y sim mean y i obs y obs mean i 1 n y i sim y sim mean 2 i 1 n y i obs y obs mean 2 15 repd max y i sim m a x y i obs max y i sim where y i sim is the ith simulated value of an event m y i obs is the ith observed value of an event m and y sim mean and y obs mean are the mean values of the simulations and observations of an event respectively m as shown in table 2 the rmse values for the two water level stations ranged from approximately 0 14 m to 0 22 m and the correlation coefficient values for the stations were generally greater than 0 90 the relative error of the peak depth ranged from approximately 16 to 17 at the stations it should be noted that streamflow predictions can be influenced by the errors associated with estimating map from point rainfall measurements however assessing the effects of this uncertainty is beyond the scope of this study therefore in general the 1d module displayed a satisfactory response for the four events the selected parameters were considered consistent and suitable for modeling second we used two historical inundation events namely the 20130722 event and the 20110727 event to calibrate and validate the 2d module to reduce the computational burden the 2d module was used to simulate inundation when surcharge flow occurred at the manholes the module ended the simulation at specific time based on the period of heavy rainfall in the storm events the routing time step of the 2d module was set to 0 1 s the key sensitive parameter to calibrate in this module is manning s value which is determined by the type of land use in the catchment the observed inundation depths in the gangnamdae road zone and other reported inundation zones were used for the calibration and validation of the 2d module table 3 contains the simulation results for the module the relative error for the maximum inundation depth was 6 7 and 13 3 in the calibration and validation processes respectively fig 3 shows the simulated maximum inundations for the 20130722 event fig 3 a and the 20110727 event fig 3 b the simulated inundation results are reasonable based on the reports of the seoul metropolitan government in terms of the locations and inundation zones the corresponding information was provided by lee et al 2018 and yoon and lee 2017 in general the selected parameters of the two modules were considered suitable for further modeling 4 2 correcting map forecasts 4 2 1 training of the lstm model to correct map forecasts from the radar forecasting system for heavy rainfall events in the gangnam catchment the lstm network model was developed the developed lstm model aims to reproduce three hour map forecasts based on the grid data from maple and observed map data obtained from ground rain gauges after setting maple grid points as the input matrix and map observations as the actual values output vector a trial and error procedure was implemented to adjust the hyperparameters based on the error of the training and validation datasets early stopping was applied as a regularization constraint to avoid the overfitting problem this technique was used to stop training when the validation error reached a minimum level the adam technique was chosen as the optimization algorithm kingma and ba 2015 the learning rate of the model was selected as 0 01 the rmse was selected to measure the model prediction error in this stage the optimal architecture of the lstm model has one layer and the number of hidden neurons was forty five the suitable batch size was eighteen 4 2 2 the performance of the lstm model the three qpfs that were generated from the beginning of five test events were considered to evaluate the performance of the lstm model the qpfs were separately constructed as input matrixes in the same manner as applied in the training stage these input matrixes were consequentially fed to the trained model to obtain the corrected maps the results of the corrected three hour map forecasts are provided in figs 4 and 5 which show a statistical comparison of the three hour forecasts of maple map and the lstm corrected map for 5 testing events fig 4 shows comparisons of the average rmse values and average forecasting bias fb with a 3 hour lead time for the events the fb is used to assess the ratio of total forecasted rainfall to total observed rainfall as shown below 16 fb i 1 n y i for i 1 n y i obs if fb is approximately 1 0 0 9 1 1 the forecast is considered unbiased values of fb ranging from 0 to 0 9 indicate underestimation and values larger than 1 1 reflect overestimation fig 4 shows that the lstm model provided significant improvements in comparison with the raw maple map values the model can reproduce map forecasts with lower rmse values in the studied lead time than those of map forecasts calculated from raw maple data for the five events in detail the rmse values of the raw map forecasts range from 2 0 8 7 1 8 3 3 4 1 6 9 2 2 4 3 and 3 2 5 1 and the values of the corrections range from 2 7 6 5 2 3 2 6 1 7 4 7 1 4 2 4 and 1 2 4 3 for the 20130722 event fig 4 a 20130823 event fig 4 b 20140821 event fig 4 c 20160701 event fig 4 d and 20160705 event fig 4 e respectively the rmse values of the correction show slight decreases after two thirds of the forecasting time compared to those of the raw forecasting data for the 20130823 and 20160705 events the fb values of the corrected maps for the five events are much closer to perfect value 1 than those of maple in the lead time fig 4 f k it is noted that the fb values for 20130722 improve over time however the correction still led to considerable underestimation fig 4 f the patterns of observed map the maple map forecasts and the corrected map forecasts are shown in figs 7 9 in the next section fig 5 presents the critical success index csi of the maple map and lstm corrected map values with different thresholds including 1 mm 10 min 3 mm 10 min and 5 mm 10 min the csi scores represent the success of forecasting based on the hit rate and is defined by the following equation 17 csi hr hr m f a where hr m and fa are the numbers of hits misses and false alarms respectively for an examined threshold these comparisons were made by aggregating 5 test events generally the lstm model made remarkable improvements compared to the predictions of maple in the cases of the thresholds the csi scores of the corrected maps varied in the ranges of 0 97 0 71 and 0 59 0 46 and the scores of raw data varied in the ranges of 0 56 0 31 and 0 12 0 06 in terms of thresholds of 1 mm 10 min fig 5 a and 3 mm 10 min fig 5 b respectively the improvements in the corrected maps for the threshold of 5 mm 10 min displayed acceptable scores 0 2 0 32 even though the scores of the raw maps were 0 fig 5 c the above results might be explained by the fact that the lstm model includes a feedback link additionally the lstm cells consider both long and short term states additionally the grid data from maple forecasts covering the whole catchment area were taken into account therefore lsmt may recognize the pattern of heavy rainfall events in the training stage and thus reduce the bias between raw map forecasts and observed map this approach might also help reduce the misinterpretation of the spatial pattern of storm events in addition to the improved results the model does not always effectively improve the accuracy of map forecasts from qpfs the 20130722 event is an obvious example of this drawback for such a machine learning method the dataset remains relatively small even though the number of collected events is quite large the limitation might be attributed to the volume of the dataset however the lstm model has shown improvements in the five test events and its strengths compared with those of other methods as shown below therefore the model can be calibrated sufficiently by using a large enough dataset to achieve acceptable performance additionally due to the uncertainties associated with the radar based forecasting system and its inputs the lstm model can improve the quality of the forecasts only to a certain extent but the errors of the forecasts were reduced considerably to demonstrate the comparative performance of the lstm model this study compared csi results with those obtained by two simpler models namely multiple linear regression lr and the multi layer perceptron mlp lr was chosen as it is the popular method used to conduct regression analysis in the postprocessing van schaeybroeck and vannitsem 2011 as one of the most popular anns the mlp is widely used for rainfall forecasts jabbari and bae 2018 lin and wu 2009 for a detailed description of the mlp refer to zhang et al 2018 the training and testing datasets of the two models are the same as those of the lstm model the parameters of the mlp were set in accordance with the best performance based on the training and validation errors as follows hidden layers 1 number of neurons in the hidden layer 45 and learning rate 0 01 fig 6 presents comparative csi results with thresholds of 3 mm 10 min and 5 mm 10 min the results show that the performances of the lstm network is significantly better than those of the mlp and lr models with higher csi values along a 3 hour lead time in the comparisons of high thresholds as shown in fig 6 with regard to the threshold of 3 mm 10 min fig 6 a the two models show noticeable improvements compared to the performance of maple especially the mlp model which provides slightly lower csi scores than the lstm model however concerning the threshold of 5 mm 10 min fig 6 b the csi scores of the two models are approximately 2 3 times lower than those of the lstm model notably the predictions of the simple benchmark models might be smoother than those of the lstm model these results prove that the lstm model can more effectively retain previous significant values the extreme values of rainfall than can the simpler models this advancement is critical for flood warning applications the main focus of this research in addition according to tyralis et al 2019 the results regarding the comparison of the methods may vary with different prediction times this problem should be addressed in further studies 4 3 urban flooding prediction with corrected maps increasing the accuracy of rainfall forecasts should directly lead to improvements in predictions of streamflow because rainfall is a fundamental input to hydrological models fabry 2004 in this study the raw maple map forecasts and corrected map forecasts were used to drive the coupled 1d 2d model and obtain predictions of water level at the two water level stations and urban inundation levels in the gangnam catchment to quantify the improvements these predictions of urban inundation were compared to the results of the coupled model and observed map values five testing events were used to assess the performance of the coupled model in terms of the water levels at the 22 0006 and 22 0002 stations the 20130722 event and 20160705 event were used to assess urban inundation predictions figs 7 9 illustrate the rainfall patterns from observed map original maple map and lstm corrected map values at the several time steps in the five events and the corresponding water level simulations at the two stations as shown in these figures the corrected map forecasts for the five events reasonably predict the peak water depth and water depth change over time the predicted raw maps underestimate the water depth peaks and water depth changes for the 20130722 event fig 7 the corrected map forecasts provide improved water levels in the first two hours of forecasting and then underestimate the remaining values table 4 illustrates the statistical comparisons of water level predictions obtained from the three types of map including the observed raw maple and corrected values at the three selected time steps in the five events the statistical errors based on r rmse and repd were used for comparison the lstm corrected maps predicted the water level with better accuracy than the original maple maps and the r values of the water levels simulated by the corrected maps were much better than those for water levels simulated by raw maps moreover the rmse values of the corrected values were lower than those of the original maple maps these improvements can be explained by the fact that the corrected model produced high accuracy map forecasts based on the repd values the performance obtained by using corrected maps in the model for 4 events namely 20130823 20140821 20160701 and 20160705 was acceptable with slight overestimation and slight underestimation for the remaining event 20130722 the simulated peak depth obtained by the corrected maps was slightly improved however the values were still significantly underestimated additionally for this event the water level predictions were significantly underestimated in the last hour of the lead time period these underestimations might be affected by the high uncertainty of the raw data leading to the low accuracy of rainfall correction the slight underestimations for the other events 20140821 may be explained by the artificial intelligence model generating smooth predictions and a lower amplitude of variation compared to actual rainfall patterns therefore the extreme rainfall values from the model might be underestimated in some cases to illustrate the prediction accuracy of the lstm corrected map forecasts in the two directional model table 5 and fig 10 show the predicted node flooding results and the predicted maximum inundations for the 20130722 event time step 5 30 and 20160705 event time step 8 40 notably the quantitative rainfall overflow for the corrected map was underestimated with fewer flooded manholes and a lower water depth than those based on the observed map for the 20130722 event however the locations and inundated areas predicted with the corrected map were relatively similar to the observed values fig 10 a the performance of the corrected map model for the 20160705 event was acceptable with almost the same number of flooded manholes and same flood time as those based on the observed map moreover the water depths were slightly lower than the observed values due to the slight underestimation of the maximum surcharge flow rate fig 10 b table 5 the underestimation of maple map generated nodes with no flooding in the two events generally these results demonstrate that the lstm model can be used to improve map forecasts and evaluate urban flood predictions however it may generate slight underestimations 5 conclusions to support urban water management in urban catchments it is essential to reproduce map forecasts from a radar forecasting system which is often associated with low accuracy in predictions for heavy rainfall events this paper proposed a framework in which an lstm model was constructed to improve maple map forecasts and the developed coupled 1d 2d model was used to assess the effect of corrected map forecasts on urban flood predictions the framework was established through a case study in the gangnam drainage area in seoul south korea the developed coupled 1d 2d model was tested and verified in the drainage area using four rainfall events for the 1d module and two historical flooding events for the 2d module the data from twenty four strong rainfall events including twenty two grid point values from the maple system and the observed map values extracted from five aws rain gauges were used to train and test the developed lstm model the water depth data from two observation stations and inundation reports were used to evaluate the developed urban inundation model for the relevant events the original maple map and lstm corrected map values were used as inputs to the coupled 1d 2d model to obtain urban flood predictions analyses of the rmse fb and csi for correcting map forecasts and r rmse repd and relevant urban flood terms for urban flooding predictions demonstrated the applicability of the proposed framework and ability to produce more reasonable three hour corrected map forecasts than the original maple forecasts the results show that the quality of corrected map forecasts for the catchment was improved considerably using the model the results of comparison of lstm model against two simpler models namely lr and mlp models demonstrated the superior correction capability with higher csi scores at high thresholds for the lstm model corresponding to the improvement in the map forecasts the flooding predictions also reflect remarkable increases in accuracy the results of the case study demonstrate the reliability and high practicability of the correction method used in this framework hence this method appears to be an optional approach for the postprocessing of radar based rainfall forecasting data however some limitations exist as described in sections 4 2 and 4 3 the approach to improving rainfall forecasts in this study depends strongly on training datasets and the quality of qpfs used as inputs for the trained model in addition the proposed lstm model is solely applied to heavy rainfall events thus it might not be applicable for different types of rainfall events moreover this model may generate slightly underestimated extreme values of rainfall therefore the peak water level and relevant inundation levels simulated were lower than the observed levels in some cases it should be noted that the coupling of radar based forecasting systems and coupled hydrodynamic models may help related stakeholders mitigate urban flood damage therefore future works should focus on investigating the performance of the framework in other cities further research is also needed to better understand how other types of anns can improve map forecasts and urban flood predictions within the proposed framework credit authorship contribution statement duc hai nguyen conceptualization methodology software formal analysis investigation data curation writing original draft visualization deg hyo bae methodology software investigation resources data curation writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the korea environment industry technology institute keiti through the water management research program funded by the ministry of environment moe of korea 130747 
5661,the abundance of satellite remotely sensed data in the past few decades has provided a great opportunity to improve hydrological models simulations and their forecasting skills this however requires an advanced data integration strategy which is usually implemented using a data assimilation approach in this study a recently proposed assimilation method the unsupervised weak constrained ensemble kalman filter uwcenkf is extended to calibrate model parameters simultaneously with the state the derivation of the new method is based on a one step ahead osa smoothing formulation of the standard joint state parameter filtering problem which results in a dual type filtering scheme separately updating the state and parameters using two interactive ensemble kalman filters enkfs the new calibration and assimilation method comprises three main steps at each assimilation cycle 1 calibrate the parameters based on the observations 2 update the system state based on the calibrated parameters and observations and 3 enforce the water budget constraint numerical experiments based on assimilating multiple datasets simultaneously into a hydrological model are carried out to assess the performance of the proposed approach over different basins and over two testing periods calibration and forecasting assimilation results suggest that the new filtering algorithm successfully improves the simulated water components during both the calibration and forecasting periods these improvements are the result of the effective assimilation calibration procedure introduced by the proposed method keywords hydrological modelling data assimilation state parameter estimation model calibration osa smoothing based filtering uwcenkf 1 introduction monitoring water resources and the ability to predict their changes are crucial to set effective water management policies numerical physical hydrological models with high spatial and temporal resolutions are valuable tools to undertake these tasks at different scales and time frames these have been particularly developed during the past few decades following the abundance of observations from various resources e g dingman 2002 van dijk et al 2011 nevertheless a variety of factors such as complex hydrological processes uncertainties in observations inputs and models parameters and simplified modelling of physical properties introduce big challenges in hydrologic modelling traditionally models calibrations were carried out manually based on trial and error procedures to improve the parameters this approach is however largely empirical computationally demanding and time consuming more advanced data fitting approaches especially following the availability of satellite remotely sensed products have been developed to enhance the estimates of the state and or parameters of hydrological models data assimilation and automatic optimization methods are the most popular to perform these tasks and have been extensively used in various studies e g eicker et al 2014 girotto et al 2016 gupta et al 1998 khaki et al 2017 kumar et al 2014 madsen 2000 neal et al 2009 reichle et al 2002 schuurmans et al 2003 van dijk et al 2014 vrugt et al 2006 khaki et al 2017b data assimilation is an established approach for continuously constraining a model with available observations to improve the consistency between the model s simulations with the real counterparts edwards et al 2015 hoteit et al 2018 this is different from how optimization algorithms work optimization or batch calibration algorithms basically explore the model parameters space to find the best values based on numerical measures of the goodness of fit to the data by optimizing an objective function e g ibbitt 1970 gupta and sorooshian 1985 sorooshian et al 1993 wagener et al 2003 yu et al 2013 jie et al 2016 this may potentially lead to better forecasting skills nevertheless optimization methods are problem dependent and may suffer when the problem involves large numbers of variables and constraints e g patnaik et al 1995 venter 2010 that could prevent successful calibration due to computational efficiency and solution robustness e g jacomino and fields 1997 iskra and droste 2007 sahoo et al 2010 these methods are further limited to offline applications because they use historical data in batches for model evaluation and subsequently parameter calibration moradkhani et al 2005 shi et al 2014 alternatively data assimilation methods can be used for estimating both state and parameters and in sequentially in time i e whenever data become available this approach has been widely used for various hydroclimate applications e g smith et al 2013 yang et al 2016 gharamti et al 2015 gharamti et al 2016 mehta and linares 2018 in contrast with the optimization methods data assimilation techniques such as the popular ensemble kalman filter enkf have the advantage of being able to account for observational and model errors which could affect the recovery of the uncertain parameters and inputs hendricks franssen and kinzelbach 2008 moreover due to its sequential formulation one does not necessarily require to store historical values of the state and parameters offering important advantages in terms of computations and storage mclaughlin 2002 ait el fquih et al 2016 data assimilation approaches have been applied in a number of state parameters problems in hydrology for instance moradkhani et al 2005 used enkf to estimate rainfall runoff model parameters and obtained comparable results with those of an optimisation technique see also lu et al 2013 moradkhani et al 2005 also successfully applied particle filter for state parameter estimation in two case studies using a parsimonious conceptual hydrologic model in another effort enkf was applied by xie and zhang 2010 to estimate parameters with different hydrologic response units hrus in a spatially distributed model shi et al 2014 and shi et al 2015 also applied the enkf for multi variate assimilation to estimate the parameters of a physically based land surface hydrologic model in recent studies a weak constrained ensemble kalman filter wcenkf followed by its unsupervised formulation uwcenkf were proposed for effective data assimilation into water balance models to mitigate for water budget imbalances caused by the assimilation of additional observations khaki et al 2017c khaki et al 2018a these approaches also take uncertainties associated with the water balance observations into an account for a more efficient filtering scheme which involves two update steps at each assimilation cycle 1 incorporating the current observations to improve the water storage simulations and 2 using the water flux observations including water storage change water discharge evaporation and precipitation to constrain the water budget closure considering the uncertainty in the data here we further extend the method to the online calibration of the model parameters simultaneously with the estimation of the state this is achieved by adapting the ensemble dual filtering approach recently proposed by ait el fquih et al 2016 to our specific hydrological data assimilation problem state parameter estimation is generally performed either jointly or following a dual approach in the joint scheme an augmented vector gathering the state and parameters is formed leading to an augmented state space model on which the classical enkf is applied this method has been widely used in various surface and sub surface hydrological studies e g wan et al 1999 chen and zhang 2006 liu et al 2008 franssen et al 2011 li et al 2012 gharamti et al 2014 for example the joint enkf based state parameter estimation was implemented by lu et al 2013 to a lumped rainfall runoff model schumacher et al 2016 applied this approach to calibrate a land surface model by assimilating terrestrial water storage tws products from the gravity recovery and climate experiment grace multiple studies have however reported some inconsistencies between the estimated state and parameters that may degrade the filter performance especially for nonlinear and or large dimensional systems e g moradkhani et al 2005 wen et al 2007 gharamti et al 2014 the dual approach has been proposed to alleviate these issues by separately updating the state and parameters using two interactive enkfs todini et al 1976 todini 1978 this dual enkf implementation is heuristic but was shown to provide improved state parameter estimates compared to the joint approach e g tian et al 2008 gharamti et al 2013 gharamti et al 2014 in an attempt to formulate a bayesian consistent framework for the dual filtering scheme ait el fquih et al 2016 see also gharamti et al 2015 proposed a new dual algorithm following the one step ahead osa smoothing formulation of the filtering problem the smoothing step in this new dual algorithm allows for an additional state update with the future observation and can potentially mitigate for standard enkf issues that often arise when dealing with poorly known noise models and limited ensembles e g ait el fquih et al 2016 the proposed approach suggests a general framework for efficient data assimilation and calibration of hydrological models it is founded on the osa smoothing formulation of the filtering problem which led to a dual type scheme separately updating the state and parameters using two interactive enkfs it further imposes a water budget constraint to account for the potential imbalance caused by the assimilated observations this is the first application of this original approach in any hydrological context the performance of the new method is assessed by assimilating multivariate satellite data into a model the satellite observations are used here to update the water storage components such as soil moisture of a water balance model as well as its parameters in addition various observations of precipitation and evaporation from satellites and also water discharge from in situ stations are exploited to enforce the water balance in the filter estimates the remainder of this paper is organized as follows the proposed approach is presented in section 2 the datasets used for modelling data assimilation and results evaluation are described in section 3 assimilation results are analyzed and discussed in section 4 the study concludes with a summary of the main findings in section 5 2 methodology 2 1 problem formulation the aim is to estimate the system state and parameters of a hydrological model using available observations considering an additional constraint i e the water budget closure at a given time t denote system state x t r n x comprises various water storage compartments including soil moisture at different layers of top shallow and deep zone as well as water stored as snow vegetation surface and groundwater observations e g from grace and satellite soil moisture are represented by y t r n y and θ r n θ represents the parameters vector see details in section 3 the state observations and parameters are related through a discrete time state parameter dynamical system of the form 1 x t m t 1 x t 1 θ ν t y t h t x t w t where m t 1 indicates a nonlinear model operator and h t is assumed a linear observational operator for simplicity the model and observation process noises assumed to be independent are shown by ν t n 0 q t and w t n 0 r t respectively and are independent of x 0 and θ q t and r t are the model and observation error covariance matrices respectively imposing water balance requires additional steps as explained by khaki et al 2017c data assimilation may degrade the balance between water fluxes described by the equation δ s p e q where δ s water storage changes p precipitation e evaporation and q water discharge considering the uncertainties in the different water flux data khaki et al 2017c this can be formulated as 2 d t x t x t 1 p t e t q t ρ t where ρ t n 0 σ is a gaussian noise process of covariance σ assumed independent of ρ t t ν t w t and x 0 defining z t as z t def d t p t e t q t which can be considered as a pseudo observation an equality constraint can be included in the state space formulation as 3 z t gx t lx t 1 μ t where l is an n z n x identity matrix and g l the problem then reduces to the estimation of the unknowns of interest from the augmented set of observations r t y t t z t t t khaki et al 2017c reported that this approach is sensitive to the choose of the covariance σ this led khaki et al 2018a to propose an unsupervised framework to estimate σ along with the state the state parameter estimation problem consists of deriving the state x t and the parameters θ at time t from the available observations up to the estimation time r 0 t r 0 r t a standard estimate is the a posteriori mean am 4 e p x t r 0 t x t x t p x t θ r 0 t d x t d θ 5 e p θ r 0 t θ t θ p x t θ r 0 t d x t d θ which translates into minimizing the a posteriori mean square error ait el fquih and hoteit 2016 hereafter p x t r 0 t p θ r 0 t and p x t θ r 0 t respectively refer to the posterior probability density function pdf of x t θ and x t θ given r 0 t these describe all the information about the state and parameters given the observations in particular these form the basis of computation of any posterior statistic such as for instance a posteriori mean as in eqs 4 5 and corresponding covariances analytical computation of eqs 4 and 5 is however often not feasible hoteit et al 2008 numerical approximation methods such as the particle filters and enkfs have been suggested for standard state space models not involving 3 see e g hoteit et al 2018 using an enkf the state parameter assimilation can be achieved following either a joint or a dual approach the joint enkf formulates the state parameter estimation problem as a standard state space model by concatenating the state and parameters as an augmented state vector for applying the enkf the dual enkf on the other hand updates the parameters and state separately through a succession of two interactive enkfs gharamti et al 2015 ait el fquih et al 2016 we follow the same approach to introduce a dual osa smoothing variant of the uwcenkf for state parameter estimation of the constrained system 1 3 this extension also includes the unsupervised σ estimation as an important component of the uwcenkf for an optimized water budget constraint 2 2 uwcenkf we first recall the algorithm of uwcenkf khaki et al 2018a for estimating the state x t and covariance σ not involving parameters θ in eq 1 the algorithm involves three successive steps at each assimilation cycle a forecast step and two update steps the process begins at time t 1 and uses the analysis ensemble x t 1 a i i 1 m m is ensemble size to compute the analysis ensemble x t a i i 1 m at the current time t as follows forecast step integrate x t 1 a i i 1 m with the hydrological model to obtain the state forecast ensemble x t f i i 1 m and observation forecast ensemble y t f i i 1 m 6 x t f i m t 1 x t 1 a i ν t i y t f i h t x t f i w t i with ν t i and w t i sampled from n 0 q t and n 0 r t respectively first analysis step the state and observation forecast ensembles are used to update the forecast ensemble once observation y t becomes available using a standard enkf analysis step this provides an unconstrained analysis ensemble x t a i i 1 m and smoothing ensemble x t 1 s i i 1 m as 7 x t a i x t f i p x t f h t hp x t f h t r t 1 y t y t f i μ t i 8 x t 1 s i x t 1 a i p x t 1 a x t f h t μ t i the cross covariance matrices p x t f and p x t 1 a x t f are given by 9 p x t f n 1 1 s x t f s x t f t 10 p x t 1 a x t f n 1 1 s x t 1 a s x t f t where s x t 1 a and s x t f are the perturbation matrices composed of the corresponding centered ensemble members or ensemble anomalies second analysis step to constrain the water budget an another enkf analysis step based on the pseudo observation z t is applied this however requires the corresponding noise covariance σ which is usually poorly known two cases have been considered in khaki et al 2018a homogeneous σ λ i n z and inhomogeneous σ diag λ 1 λ n z here for simplicity we only present the homogeneous case this second update step is then performed iteratively at each iteration ℓ ℓ 0 l with l the iteration number ensemble x t a i ℓ i 1 m and estimate σ t ℓ are estimated as 11 z t f i ℓ g x t a i l x t 1 s i ρ t i ℓ ρ t i ℓ n 0 σ t ℓ 12 x t a i ℓ x t a i p x t a z t f ℓ mp η t m t σ t ℓ 1 z t z t f i ℓ ν t i ℓ 13 x t 1 s i ℓ x t 1 s i p x t 1 s z t f ℓ ν t i ℓ where m def g l p x t a z t f ℓ and p x t 1 s z t f ℓ are the sample cross covariances computed from x t a i i 1 m x t 1 s i i 1 m and z t f i ℓ i 1 m and p η t is the sample covariance of the ensemble η t i i 1 m with η t i def x t a i t x t 1 s i t t the noise covariance matrix for the next iteration is then estimated as 14 β t ℓ 1 β t 1 1 2 z t g x t a ℓ l x t 1 s ℓ 2 trace mp γ t ℓ m t 15 λ t ℓ 1 β t ℓ 1 α t 16 σ t ℓ 1 λ t ℓ 1 i n z where x t a ℓ and x t 1 s ℓ are the ensemble means of x t a i ℓ i 1 m and x t 1 s i ℓ i 1 m respectively p γ t ℓ is the sample covariance of γ t i ℓ i 1 m with γ t i ℓ def x t a i ℓ t x t 1 s i ℓ t t and α t α t 1 n z 2 at the initial iteration λ t 0 β t 1 α t and σ t 0 λ t 0 i n z the analysis covariance σ t l as well as the analysis state estimates x t a i l i 1 m are then used in the next data assimilation cycle 2 3 dual state parameter estimation with the uwcenkf we first present the parameter estimation steps in section 2 3 1 then the state estimation steps in section 2 3 2 2 3 1 parameter estimation the goal is to update the parameters based on the current observations y t the parameters members given the past observations θ t 1 i i 1 m and the previous analysis ensemble x t 1 a i i 1 m this begins with a forecast step to compute the state forecast ensemble x t f i i 1 m and associated observation forecast ensemble y t f i i 1 m as 17 x t f i m t 1 x t 1 a i θ t 1 i ν t i y t f i h t x t f i w t i the current parameter ensemble θ t i i 1 m based on y 0 t can then be obtained using 18 θ t i θ t 1 i p θ t 1 x t f h t hp x t f h t r t 1 y t y t f i τ t i with the sample forecast error covariance p x t f and the sample cross covariance between the analysis parameter at t 1 and the state forecast errors at t p θ t 1 x t f given by 19 p x t f n 1 1 s x t f s x t f t 20 p θ t 1 x t f n 1 1 s θ t 1 s x t f t 2 3 2 state estimation the state estimation with uwcenkf involves an osa smoothing step a forecast step and two successive analysis steps these should provide better state estimates based on the updated parameters and most recent observations and also to enforce the water balance smoothing step the smoothed ensemble is first computed as 21 x t 1 s i x t 1 a i p x t 1 a x t f h t τ t i where p x t 1 a x t f is the sample cross covariance between the analysis state at t 1 and the forecast state at t 22 p x t 1 a x t f n 1 1 s x t 1 a s x t f t forecast step the state forecast ensemble and associated observation forecast ensemble at time t are then obtained by integrating x t 1 s i θ t i i 1 m with the model 23 ξ t f i m t 1 x t 1 s i θ t i ν t 1 i y t f i h t ξ t f i w t i first analysis step the resulting state ensemble ξ t f i i 1 m is then updated based on the available observation y t using the uwcenkf first analysis step eq 7 replacing x t f i with ξ t f i to update the unconstrained state analysis ensemble x t a i i 1 m second analysis step the water balance constraint is finally imposed based on z t using an enkf update σ t is also estimated as it is needed for the computation of the analysis ensemble x t a i i 1 m these are performed using eqs 11 16 2 4 summary of the dual uwcenkf algorithm the process begins by integrating the updated parameters and analysis state at time t 1 x t 1 a i θ t 1 i i 1 m with the model to obtain x t f i i 1 m e g eq 17 these are then used along with the observations y t and z t to update the parameters and the state with the following steps parameter update parameters are first updated with y t using eqs 18 20 to obtain θ t i i 1 m satellite soil moisture and grace tws are used as observations in this step see second dataset in table 2 section 3 2 state update osa smoothing the observation forecast y t f i i 1 m computed from the previous state and parameters cf eq 17 is used to calculate the smoothed state ensemble based on the current observation eqs 21 22 forecast step once x t 1 s i θ t i i 1 m are calculated these are integrated with the model to forecast the state and observation i e to compute ξ t f i i 1 m and y t f i i 1 m respectively eq 23 first analysis step the forecast state ξ t f i i 1 m is then updated based on the observation following eq 7 again y t includes satellite soil moisture and grace tws this is done to obtain the unconstrained analysis ensemble x t a i i 1 m second analysis step finally x t a i i 1 m is updated with the pseudo observation z t along with the associated error covariance σ t using eqs 11 16 to obtain the final state analysis ensemble x t a i i 1 m z t contains water fluxes observations of p e q and δ s from satellite and in situ measurements see third dataset in table 2 section 3 2 2 5 computational cost important for an efficient state parameter estimation is a reasonable computational complexity this is particularly of interest when dealing with high dimensional models and or operational systems therefore while achieving accurate estimates is crucial computational efficiency is also an important factor for developing an efficient data assimilation system this is investigated in this section in the following the state parameter filter using osa smoothing is referred to as uwcenkf assimilation calibration uwcenkf ac to distinguish it from uwcenkf assimilation only uwcenkf ao including only state estimation using the classical enkf khaki et al 2018a because of their similar number of forecast and update steps and iterative character both uwcenkf algorithms uwcenkf ac and uwcenkf ao have larger computational costs than the standard enkf while both enkf and uwcenkf ao apply one forecast step i e one model run they have a different number of kalman like update steps the former involves one update of the state whereas the latter includes 1 2 l additional updates one for smoothing step based on y t following 8 l for iterative analysis based on r t following 12 and l for iterative smoothing step based on r t following 13 these additional updates along with those used for estimating the covariance σ 14 16 result in a larger computational burden for the uwcenkf ao uwcenkf ac in turn applies two more additional steps compared to uwcenkf ao which makes it computationally more demanding one for updating the parameters based on y t as in 18 and one for forecasting the pseudo smoothed state and updated parameters as in 23 3 numerical experiment 3 1 hydrological model the numerical test is done using the world wide water resources assessment w3ra model w3ra is a biophysical model which was developed for representing water storage initially over australia its simulations are based on the water balance between different components such as soil moisture surface water storage and groundwater independently over each grid point khaki et al 2018b more details about this model and its underlying processes can be found in van dijk 2010 and van dijk et al 2013 in summary at each grid cell the following vertical interactions are applied the top layer soil column is fed by surface water as net precipitation and deep root soil layer is supplied through capillary rise from groundwater water then leaves the column via soil evaporation drainage into the groundwater and river runoff or extraction by shallow and deep rooted vegetation the model also includes multiple processes such as partitioning of precipitation between interception evaporation and net precipitation which is divided between infiltration infiltration excess surface runoff and saturation excess runoff soil water storage is represented by water balance of three unsaturated soil layers topsoil shallow and deep soil layer including infiltration and drainage other surface and sub surface components are modelled as groundwater dynamics including recharge capillary rise and discharge and surface water body dynamics including inflows from runoff and discharge open water evaporation and catchment water yield in addition w3ra includes vegetation processes i e transpiration and vegetation cover adjustment the daily meteorological forcing datasets include precipitation maximum and minimum temperature and downwelling short wave radiation extracted from the era interim reanalysis data the modelling in w3ra is carried out for two hydrological response units hrus these include water and energy fluxes simulation for vegetation with deeper roots hru1 and for vegetation with short and shallow roots hru2 each occupying a fraction of the grid cell accordingly parameterizations are implemented based on sub grids and vary spatially to correspondingly simulate the area fractions with water compartments previous studies e g poovakka et al 2013 have shown that w3ra calibration is essential the model applies various parameters to simulate different water storage this includes soil evaporation and its capacity of water holding parameters to represent the connectivity of catchment characteristics e g catchment geomorphology its climate terrain and land cover and saturated area as well as groundwater recession and greenness more details can be seen in van dijk et al 2013 the list of parameters used here for calibration is presented in table 1 3 2 dataset various data sets are used here for the different tasks of the experiment these are mostly derived from satellite remotely sensed products and acquired over different basins including mississippi amazon yangtze danube indus murray darling st lawrence and the orange these are mainly selected due to the availability of water discharge data from in situ measurements to compute the state and parameters estimates and to examine the capability of the proposed assimilation method over different basins three different types of datasets are considered their corresponding spatial and temporal resolutions are detailed in table 2 the first dataset is used to force the model cf section 3 1 the second dataset is used for data assimilation and includes two sets of observations for 1 state parameter estimation and 2 water budget enforcement for 1 satellite soil moisture and grace tws are used grace data level 2 are derived from the itsg grace2014 gravity field model developed by mayer gurr et al 2014 the dataset includes stokes coefficients and their errors up to degree and order 90 post processing steps are implemented following khaki et al 2018d and khaki et al 2019 to calculate tws changes to update the model summation of water stored in different soil layers groundwater and surface storage soil moisture observations are used during the calibration period to adjust the model surface soil moisture content regarding soil moisture uncertainties 0 04 m 3 m 3 and 0 05 m 3 m 3 error standard deviations are selected for smos and amsr e respectively as suggested by literature see e g de jeu et al 2008 leroux et al 2016 for 2 water fluxes observations of p e q and δ s are used to constrain the water budget closure note that multiple observations of the water components are merged following sahoo et al 2011 to achieve more accurate p and e over the basins see details in khaki et al 2018a the water flux observations are then used for data assimilation and specifically to enforce water budget constraint during the filtering process using uwcenkf the third set of data refers to in situ measurements which are used as independent information to validate the results during the calibration and forecasting periods which will be described separately hereafter this includes groundwater and soil moisture measurements note that we use specific yield values extracted from the various studies e g gutentag et al 1984 strassberg et al 2007 seoane et al 2013 khaki et al 2017c to derive groundwater storage from bore measurements 3 3 experimental setup the study period is divided into three parts 2000 2002 to generate the initial ensemble 2003 2012 to assimilate observations and calibrate the model and 2013 2016 to monitor the model forecastings as mentioned the state vector for every model grid point is composed of soil moisture at different layers of the top shallow and deep zone as well as water stored as snow vegetation surface and groundwater the observational operator h t converts these state variables into the observation space by taking into account discrepancies between the model s and observations spatial resolutions in the case of grace tws for example h t accumulates the state variables the individual water storages at grid points to determine the simulated tws in 1 1 spatial resolution in order to update them with the grace tws during assimilation further to layout the experiment an initial state ensemble of m 30 members is generated by perturbing the meteorological forcing fields this in particular is done for precipitation by considering a gaussian multiplicative error with a standard deviation of 30 for the shortwave radiation by assuming an additive gaussian error with a standard deviation of 50 wm 2 and for temperature by considering a gaussian additive error with a standard deviation of 2 c following jones et al 2007 renzullo et al 2014 sampling of zero mean gaussian distributions with these standard deviations are then performed to produce the initial ensemble a parameter ensemble is produced by independently drawing 30 random samples from each parameter s defined range cf table 1 the resulting state and parameter ensemble members are then integrated with the model from 2000 to 2002 to generate the filter initial ensemble at the beginning of 2003 as an essential step for an enkf implementation trial and errors experiments to set the value of ensemble inflation 1 1 1 3 for the parameters and state updates and localization 2 5 radii are conducted to tune the filters performances anderson et al 2007 khaki et al 2018c 4 results various aspects of the proposed filtering method are analyzed here below we first focus on the parameter calibration results section 4 1 in order to investigate the effectiveness of the calibration methods for improving water storage estimates both during the calibration and forecast periods the performance of the proposed filter with and without calibration i e uwcenkf ac and uwcenkf ao respectively as well as its comparison with the standard enkf are compared in section 4 2 section 4 3 evaluates the filter s capability in reflecting hydroclimate variabilities into the estimates by studying the impact of extreme climatic events on the water compartments and also the balance between the water fluxes 4 1 parameters calibration the proposed assimilation calibration scheme updates the parameters while assimilating new incoming observations into the system the method performance for calibrating the model parameters is discussed here a successful calibration method should provide better water storage estimates especially during the forecast period in which the model highly relies on its parameters where no observations are assimilated to constrain the state to investigate this the parameters update during the calibration period are analyzed through the patterns of their time evolution the parameters variations during the calibration period are displayed in fig 1 a convergence pattern can be clearly seen for all parameters this suggests that the parameters reach a stable and potentially optimum value given the available observations it can also be seen that generally larger variations occur in the first four years during which the method explores the parameters space this however decreases over time with much less variations after 2009 for all parameters final estimated values are relatively different from their initial values cf fig 1 the adjusted parameters and their standard deviation std by the different filters are reported in table 3 the values in this table displays average estimated parameters over the study regions spatially varying parameters can realistically lead to better estimates over different areas with different atmospheric and environmental characteristics as can be seen from table 3 some parameters indicate larger stds e g β i 0 λ ref p ref c sla which generally suggests more spatial variability moreover it is found that the estimated parameters are significantly different from the initial values which can potentially lead to different state estimate too this is particularly true when comparing the results of uwcenkf ac with uwcenkf ao which only differs in the parameters update step this is further investigated in the following sections 4 2 state estimation results 4 2 1 impact of data assimilation the performances of the filtering algorithms for estimating water storage are discussed separately for the calibration 2003 2013 and forecasting 2013 2016 periods it is worth mentioning that the better performance of uwcenkf compared to enkf and the osa based ensemble filtering over the standard joint and dual enkfs were demonstrated and discussed in details in khaki et al 2018a and ait el fquih et al 2016 respectively here the performance of the newly proposed method is compared with uwcenkf without calibration uwcenkf ao to analyse its relevance for state parameter estimation which is of particular interest during the forecast period to begin with average tws changes time series over the calibration period from model simulations with assimilation only uwcenkf ao with both assimilation and calibration uwcenkf ac without assimilation calibration open loop as well as grace for all basins are shown in fig 2 the average absolute misfits between uwcenkf ao and uwcenkf ac tws results and also for the open loop run with respect to the grace tws data are also reported in each subfigure while improvements i e smaller discrepancy from observations are noticeable from both uwcenkf ao and uwcenkf ac a slightly better matching is achieved by uwcenkf ac over most of the basins this is expected since both methods incorporate observations soil moisture and grace tws into the system state which pulls the estimates toward the observations based on the reported average errors both uwcenkf ao and uwcenkf ac improve the model outputs with respect to the observations by about 31 and 34 respectively this indicates slightly better performance of uwcenkf ac which is mainly pronounced during periods of large anomalies e g mississippi 2011 2012 and danube between 2004 and 2006 nevertheless both assimilation methods can effectively reflect extreme events in their outputs fig 3 depicts the same results as fig 2 but over the forecasting period the positive effect of calibration is clear for all basins the average error reduction i e the difference between grace tws and the filters results using uwcenkf ao and uwcenkf ac with respect to the difference between grace tws and the open loop results is respectively 7 and 26 on average these results suggest that both filters provide better estimates compared to the open loop results even though no assimilation is performed during this period this improvement is clearly more pronounced for uwcenkf ac the considerably smaller error reduction during the forecast period with uwcenkf ao is due to the lack of parameter calibration it can then be concluded from the results that the error reduction by uwcenkf ac comes from the calibrated parameters as well as the use of the osa formulation in uwcenkf ao the positive impact of uwcenkf ao at the beginning of the forecasting period is solely due to the improved initial state at the end of the assimilation window which is quickly lost after the first few forecasting steps similar to fig 2 uwcenkf ac results better reflect larger tws variabilities e g over amazon strong anomalies e g indus 2013 2014 and weaker tws changes e g mississippi during 2014 to further investigate the performance of uwcenkf ac its soil moisture estimates are compared against satellite measurements over four basins in fig 4 this is done to particularly show the impact of the assimilation calibration approach on the ensemble spread especially over the forecast period more comprehensive soil moisture evaluation for all applied filters will be discussed in the following section in fig 4 in addition to the time evolution of the soil moisture ensemble mean from uwcenkf ac the ensemble members are also plotted to compare their variations before and after 2013 which corresponds to the calibration and forecast periods similar to the previous results and as expected the assimilation results better agree with the observations during the calibration period this is also true during the forecast period especially over the indus and mississippi basins the ensemble evolution in fig 4 suggests that the variation of the ensemble spread is relatively stable during the calibration period but increases gradually over the forecasting period due to the absence of assimilation indicating larger uncertainties in the estimates the results demonstrate that the impact of uwcenkf ac may decline over time often due to the impacts of sources of uncertainties such as forcing errors nonetheless the calibration along with assimilation allows for an extended forecasting skill through the adjustment of the model parameters 4 2 2 validation with in situ measurements to further evaluate the results of data assimilation from different schemes independent in situ measurements of groundwater soil moisture and water discharge are used the estimates of different water components including soil moisture groundwater and water discharge as they result from the open loop uwcenkf ao and uwcenkf ac are interpolated spatially to the in situ measurements locations for comparisons fig 5 depicts the groundwater results based on the average estimated std and root mean squared error rmse as well as correlation for each run over two basins murray darling top row and mississippi bottom row the evaluation analysis is undertaken separately for the calibration left panel and forecasting right panel periods over the calibration period the best performance is achieved by uwcenkf ac providing the smallest rmse and std and the highest correlation with the in situ groundwater measurements followed closely by uwcenkf ao this is true for both basins with both filters improving groundwater simulations from fig 5 the smallest rmse and std as well as the highest correlations are also obtained with uwcenkf ac over the forecast period uwcenkf ac achieves considerably larger improvements with respect to the open loop run compared to uwcenkf ao which agrees with the previous results where parameters calibration effectively improve the assimilation performance over the forecast period the correlation values between the soil moisture in situ measurements and their filters estimates as they result from uwcenkf ao uwcenkf ac and the open loop run at different depths are assumed here to this end soil moisture estimates at different depths i e top shallow and deep layers are compared with in situ measurements at the corresponding depths for example while top layer soil moisture estimates are compared with the first 10 cm in situ measurements the summation of the estimated soil moisture at all layers is compared with 0 100 cm in situ soil measurements the averaged results for each method are reported in table 4 again both uwcenkf ao and uwcenkf ac achieve comparable results over the calibration period the estimated soil moisture correlation to the in situ measurements is higher for both compared to the open loop estimates over the forecasting period while uwcenkf ao improves the correlation values approximately 3 with respect to the open loop higher correlation 20 to the in situ measurements are achieved by uwcenkf ac this reflects the capability of uwcenkf ac for not only improving the state estimates during the calibration period but more importantly during the forecasting period which largely comes from the calibration of the parameters the comparison between the simulated water discharge from the three different runs open loop uwcenkf ao and uwcenkf ac with the measurements from in situ stations are displayed in fig 6 note that these in situ measurements are independent from the assimilation experiments the assessment is again undertaken separately for the calibration fig 6 top row and forecast fig 6 bottom row periods over st lawrence mississippi danube and amazon basins it can be seen that the best results are achieved by uwcenkf ac over both time periods as reflected by closer scatter points to the reference line black dashed line which represents a perfect match to the in situ measurements this becomes clearer when comparing the interpolated lines to scatter points from each method with the results of uwcenkf ac being the closest to the in situ measurements over all basins uwcenkf ao results to a lesser degree match well the in situ measurements compared to the open loop outputs especially over the calibration period over the forecast period consistent with the previous results the best performance is again achieved by uwcenkf ac suggesting the smallest discrepancies between its results and the in situ measurements these results confirm the superiority of uwcenkf ac particularly over the forecast period as shown in fig 5 and table 4 4 3 hydroclimate variability 4 3 1 extreme events in addition to providing reliable water storage estimates it is important for a hydrologic system estimates to agree with independent hydroclimatic variables this is particularly of interest for forecasting where outcomes rely largely on the model and its parameters to analyse this we focus here on the results of uwcenkf ac over the forecast period one of the main objectives of model calibration is to improve its skill for predicting the impacts of climate variability such as climate extreme events on water storage estimates to investigate this two events are selected over the indus and orange basins to monitor their corresponding effect on model simulations with and without data assimilation according to pakistan national disaster management authority ndma 2015 heavy monsoon rains the rapid melting of snow and outbursts from glacial lakes in 2015 caused flash floods over different areas within the indus basin over the orange basin monyela 2017 reported a two year drought from 2014 to 2015 mostly due to el niño impact these two strong anomalies can be clearly observed in fig 7 which shows monthly basin averaged precipitation time series as well as tws changes from uwcenkf ac and the open loop run rainfall pattern shows a noticeable increase over the indus basin during 2015 on the other hand a decline rainfall pattern is evident after 2014 over the orange basin the corresponding tws changes for the same periods from the open loop run as well as uwcenkf ac are also shown in fig 7 the results indicate a disagreement between the open loop and uwcenkf ac predictions especially over the indus basin the missing positive trend in the open loop time series is successfully retrieved after the application of uwcenkf ac over the orange basin the negative trend is predicted by both the open loop and uwcenkf ac results this however is more pronounced in the uwcenkf ac tws time series which better capture the drought event the results clearly suggest the effectiveness of the calibration through uwcenkf ac for improving the model simulations over the forecast period 4 3 2 water balance results as discussed in section 2 one of the main features of uwcenkf is its ability to enforce the water budget enforcement while assimilating incoming observations as demonstrated by khaki et al 2017c khaki et al 2018a here we investigate the balance between the estimated water storage changes as they result from uwcenkf ac and independent precipitation evaporation and water discharge data which are not used by the model this is done for the forecast period to explore how effective is uwcenkf ac for reducing the open loop imbalance through the estimation of the parameters the average imbalance over all basins are calculated and the average and std values over the forecast period are presented in fig 8 arrows in the figure start from the open loop results and end on the uwcenkf ac results it is clear that uwcenkf ac successfully decreases the imbalances for all basins this is particularly true for the larger basins such as the amazon and mississippi basins which are generally more difficult to model because of the contribution of various hydrologic processes e g the existence of the large surface body of water fig 8 also suggests that water storage estimates from uwcenkf ac better preserve the balance between water fluxes 5 conclusions a new filtering scheme was proposed for efficient data assimilation and forecasting of hydrological applications the scheme is designed to incorporate multiple observations types into a hydrological model while estimating both the system state and parameters the scheme was developed based on the recently proposed unsupervised weak constrained ensemble kalman filter uwcenkf to update the system state based on the incoming observations simultaneously estimate the parameters and also enforce the water budget constraint numerical experiments conducted over different basins showcased the ability of the proposed scheme to effectively improve various water compartments during the assimilation calibration and forecasting periods the evaluation of the resulting system assimilating the gravity recovery and climate experiment grace derived terrestrial water storage tws and satellite soil moisture observations and validating its outputs against independent in situ measurements indicated that much better model performance can be achieved using the new approach considerable improvements were also obtained for water storage components over the forecasting period 2013 2016 compared to state only estimates the proposed hybrid assimilation calibration approach also led to better predictions of extreme events such as flood and drought events all these results stress the capability of the proposed assimilation and forecasting method for enhancing the model s performance the new scheme will be further investigated for assimilation with different hydrological models and for assimilation of different observation types in future efforts credit authorship contribution statement m khaki b ait el fquih and i hoteit conceived the presented study m khaki proposed the problem under study and b ait el fquih derived the constrained state parameter filtering solution m khaki designed and implemented the numerical analysis b ait el fquih and i hoteit provided critical insights on methodology and result interpretation m khaki prepared the initial draft of the manuscript which was improved due to substantial contributions of b ait el fquih and i hoteit declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5661,the abundance of satellite remotely sensed data in the past few decades has provided a great opportunity to improve hydrological models simulations and their forecasting skills this however requires an advanced data integration strategy which is usually implemented using a data assimilation approach in this study a recently proposed assimilation method the unsupervised weak constrained ensemble kalman filter uwcenkf is extended to calibrate model parameters simultaneously with the state the derivation of the new method is based on a one step ahead osa smoothing formulation of the standard joint state parameter filtering problem which results in a dual type filtering scheme separately updating the state and parameters using two interactive ensemble kalman filters enkfs the new calibration and assimilation method comprises three main steps at each assimilation cycle 1 calibrate the parameters based on the observations 2 update the system state based on the calibrated parameters and observations and 3 enforce the water budget constraint numerical experiments based on assimilating multiple datasets simultaneously into a hydrological model are carried out to assess the performance of the proposed approach over different basins and over two testing periods calibration and forecasting assimilation results suggest that the new filtering algorithm successfully improves the simulated water components during both the calibration and forecasting periods these improvements are the result of the effective assimilation calibration procedure introduced by the proposed method keywords hydrological modelling data assimilation state parameter estimation model calibration osa smoothing based filtering uwcenkf 1 introduction monitoring water resources and the ability to predict their changes are crucial to set effective water management policies numerical physical hydrological models with high spatial and temporal resolutions are valuable tools to undertake these tasks at different scales and time frames these have been particularly developed during the past few decades following the abundance of observations from various resources e g dingman 2002 van dijk et al 2011 nevertheless a variety of factors such as complex hydrological processes uncertainties in observations inputs and models parameters and simplified modelling of physical properties introduce big challenges in hydrologic modelling traditionally models calibrations were carried out manually based on trial and error procedures to improve the parameters this approach is however largely empirical computationally demanding and time consuming more advanced data fitting approaches especially following the availability of satellite remotely sensed products have been developed to enhance the estimates of the state and or parameters of hydrological models data assimilation and automatic optimization methods are the most popular to perform these tasks and have been extensively used in various studies e g eicker et al 2014 girotto et al 2016 gupta et al 1998 khaki et al 2017 kumar et al 2014 madsen 2000 neal et al 2009 reichle et al 2002 schuurmans et al 2003 van dijk et al 2014 vrugt et al 2006 khaki et al 2017b data assimilation is an established approach for continuously constraining a model with available observations to improve the consistency between the model s simulations with the real counterparts edwards et al 2015 hoteit et al 2018 this is different from how optimization algorithms work optimization or batch calibration algorithms basically explore the model parameters space to find the best values based on numerical measures of the goodness of fit to the data by optimizing an objective function e g ibbitt 1970 gupta and sorooshian 1985 sorooshian et al 1993 wagener et al 2003 yu et al 2013 jie et al 2016 this may potentially lead to better forecasting skills nevertheless optimization methods are problem dependent and may suffer when the problem involves large numbers of variables and constraints e g patnaik et al 1995 venter 2010 that could prevent successful calibration due to computational efficiency and solution robustness e g jacomino and fields 1997 iskra and droste 2007 sahoo et al 2010 these methods are further limited to offline applications because they use historical data in batches for model evaluation and subsequently parameter calibration moradkhani et al 2005 shi et al 2014 alternatively data assimilation methods can be used for estimating both state and parameters and in sequentially in time i e whenever data become available this approach has been widely used for various hydroclimate applications e g smith et al 2013 yang et al 2016 gharamti et al 2015 gharamti et al 2016 mehta and linares 2018 in contrast with the optimization methods data assimilation techniques such as the popular ensemble kalman filter enkf have the advantage of being able to account for observational and model errors which could affect the recovery of the uncertain parameters and inputs hendricks franssen and kinzelbach 2008 moreover due to its sequential formulation one does not necessarily require to store historical values of the state and parameters offering important advantages in terms of computations and storage mclaughlin 2002 ait el fquih et al 2016 data assimilation approaches have been applied in a number of state parameters problems in hydrology for instance moradkhani et al 2005 used enkf to estimate rainfall runoff model parameters and obtained comparable results with those of an optimisation technique see also lu et al 2013 moradkhani et al 2005 also successfully applied particle filter for state parameter estimation in two case studies using a parsimonious conceptual hydrologic model in another effort enkf was applied by xie and zhang 2010 to estimate parameters with different hydrologic response units hrus in a spatially distributed model shi et al 2014 and shi et al 2015 also applied the enkf for multi variate assimilation to estimate the parameters of a physically based land surface hydrologic model in recent studies a weak constrained ensemble kalman filter wcenkf followed by its unsupervised formulation uwcenkf were proposed for effective data assimilation into water balance models to mitigate for water budget imbalances caused by the assimilation of additional observations khaki et al 2017c khaki et al 2018a these approaches also take uncertainties associated with the water balance observations into an account for a more efficient filtering scheme which involves two update steps at each assimilation cycle 1 incorporating the current observations to improve the water storage simulations and 2 using the water flux observations including water storage change water discharge evaporation and precipitation to constrain the water budget closure considering the uncertainty in the data here we further extend the method to the online calibration of the model parameters simultaneously with the estimation of the state this is achieved by adapting the ensemble dual filtering approach recently proposed by ait el fquih et al 2016 to our specific hydrological data assimilation problem state parameter estimation is generally performed either jointly or following a dual approach in the joint scheme an augmented vector gathering the state and parameters is formed leading to an augmented state space model on which the classical enkf is applied this method has been widely used in various surface and sub surface hydrological studies e g wan et al 1999 chen and zhang 2006 liu et al 2008 franssen et al 2011 li et al 2012 gharamti et al 2014 for example the joint enkf based state parameter estimation was implemented by lu et al 2013 to a lumped rainfall runoff model schumacher et al 2016 applied this approach to calibrate a land surface model by assimilating terrestrial water storage tws products from the gravity recovery and climate experiment grace multiple studies have however reported some inconsistencies between the estimated state and parameters that may degrade the filter performance especially for nonlinear and or large dimensional systems e g moradkhani et al 2005 wen et al 2007 gharamti et al 2014 the dual approach has been proposed to alleviate these issues by separately updating the state and parameters using two interactive enkfs todini et al 1976 todini 1978 this dual enkf implementation is heuristic but was shown to provide improved state parameter estimates compared to the joint approach e g tian et al 2008 gharamti et al 2013 gharamti et al 2014 in an attempt to formulate a bayesian consistent framework for the dual filtering scheme ait el fquih et al 2016 see also gharamti et al 2015 proposed a new dual algorithm following the one step ahead osa smoothing formulation of the filtering problem the smoothing step in this new dual algorithm allows for an additional state update with the future observation and can potentially mitigate for standard enkf issues that often arise when dealing with poorly known noise models and limited ensembles e g ait el fquih et al 2016 the proposed approach suggests a general framework for efficient data assimilation and calibration of hydrological models it is founded on the osa smoothing formulation of the filtering problem which led to a dual type scheme separately updating the state and parameters using two interactive enkfs it further imposes a water budget constraint to account for the potential imbalance caused by the assimilated observations this is the first application of this original approach in any hydrological context the performance of the new method is assessed by assimilating multivariate satellite data into a model the satellite observations are used here to update the water storage components such as soil moisture of a water balance model as well as its parameters in addition various observations of precipitation and evaporation from satellites and also water discharge from in situ stations are exploited to enforce the water balance in the filter estimates the remainder of this paper is organized as follows the proposed approach is presented in section 2 the datasets used for modelling data assimilation and results evaluation are described in section 3 assimilation results are analyzed and discussed in section 4 the study concludes with a summary of the main findings in section 5 2 methodology 2 1 problem formulation the aim is to estimate the system state and parameters of a hydrological model using available observations considering an additional constraint i e the water budget closure at a given time t denote system state x t r n x comprises various water storage compartments including soil moisture at different layers of top shallow and deep zone as well as water stored as snow vegetation surface and groundwater observations e g from grace and satellite soil moisture are represented by y t r n y and θ r n θ represents the parameters vector see details in section 3 the state observations and parameters are related through a discrete time state parameter dynamical system of the form 1 x t m t 1 x t 1 θ ν t y t h t x t w t where m t 1 indicates a nonlinear model operator and h t is assumed a linear observational operator for simplicity the model and observation process noises assumed to be independent are shown by ν t n 0 q t and w t n 0 r t respectively and are independent of x 0 and θ q t and r t are the model and observation error covariance matrices respectively imposing water balance requires additional steps as explained by khaki et al 2017c data assimilation may degrade the balance between water fluxes described by the equation δ s p e q where δ s water storage changes p precipitation e evaporation and q water discharge considering the uncertainties in the different water flux data khaki et al 2017c this can be formulated as 2 d t x t x t 1 p t e t q t ρ t where ρ t n 0 σ is a gaussian noise process of covariance σ assumed independent of ρ t t ν t w t and x 0 defining z t as z t def d t p t e t q t which can be considered as a pseudo observation an equality constraint can be included in the state space formulation as 3 z t gx t lx t 1 μ t where l is an n z n x identity matrix and g l the problem then reduces to the estimation of the unknowns of interest from the augmented set of observations r t y t t z t t t khaki et al 2017c reported that this approach is sensitive to the choose of the covariance σ this led khaki et al 2018a to propose an unsupervised framework to estimate σ along with the state the state parameter estimation problem consists of deriving the state x t and the parameters θ at time t from the available observations up to the estimation time r 0 t r 0 r t a standard estimate is the a posteriori mean am 4 e p x t r 0 t x t x t p x t θ r 0 t d x t d θ 5 e p θ r 0 t θ t θ p x t θ r 0 t d x t d θ which translates into minimizing the a posteriori mean square error ait el fquih and hoteit 2016 hereafter p x t r 0 t p θ r 0 t and p x t θ r 0 t respectively refer to the posterior probability density function pdf of x t θ and x t θ given r 0 t these describe all the information about the state and parameters given the observations in particular these form the basis of computation of any posterior statistic such as for instance a posteriori mean as in eqs 4 5 and corresponding covariances analytical computation of eqs 4 and 5 is however often not feasible hoteit et al 2008 numerical approximation methods such as the particle filters and enkfs have been suggested for standard state space models not involving 3 see e g hoteit et al 2018 using an enkf the state parameter assimilation can be achieved following either a joint or a dual approach the joint enkf formulates the state parameter estimation problem as a standard state space model by concatenating the state and parameters as an augmented state vector for applying the enkf the dual enkf on the other hand updates the parameters and state separately through a succession of two interactive enkfs gharamti et al 2015 ait el fquih et al 2016 we follow the same approach to introduce a dual osa smoothing variant of the uwcenkf for state parameter estimation of the constrained system 1 3 this extension also includes the unsupervised σ estimation as an important component of the uwcenkf for an optimized water budget constraint 2 2 uwcenkf we first recall the algorithm of uwcenkf khaki et al 2018a for estimating the state x t and covariance σ not involving parameters θ in eq 1 the algorithm involves three successive steps at each assimilation cycle a forecast step and two update steps the process begins at time t 1 and uses the analysis ensemble x t 1 a i i 1 m m is ensemble size to compute the analysis ensemble x t a i i 1 m at the current time t as follows forecast step integrate x t 1 a i i 1 m with the hydrological model to obtain the state forecast ensemble x t f i i 1 m and observation forecast ensemble y t f i i 1 m 6 x t f i m t 1 x t 1 a i ν t i y t f i h t x t f i w t i with ν t i and w t i sampled from n 0 q t and n 0 r t respectively first analysis step the state and observation forecast ensembles are used to update the forecast ensemble once observation y t becomes available using a standard enkf analysis step this provides an unconstrained analysis ensemble x t a i i 1 m and smoothing ensemble x t 1 s i i 1 m as 7 x t a i x t f i p x t f h t hp x t f h t r t 1 y t y t f i μ t i 8 x t 1 s i x t 1 a i p x t 1 a x t f h t μ t i the cross covariance matrices p x t f and p x t 1 a x t f are given by 9 p x t f n 1 1 s x t f s x t f t 10 p x t 1 a x t f n 1 1 s x t 1 a s x t f t where s x t 1 a and s x t f are the perturbation matrices composed of the corresponding centered ensemble members or ensemble anomalies second analysis step to constrain the water budget an another enkf analysis step based on the pseudo observation z t is applied this however requires the corresponding noise covariance σ which is usually poorly known two cases have been considered in khaki et al 2018a homogeneous σ λ i n z and inhomogeneous σ diag λ 1 λ n z here for simplicity we only present the homogeneous case this second update step is then performed iteratively at each iteration ℓ ℓ 0 l with l the iteration number ensemble x t a i ℓ i 1 m and estimate σ t ℓ are estimated as 11 z t f i ℓ g x t a i l x t 1 s i ρ t i ℓ ρ t i ℓ n 0 σ t ℓ 12 x t a i ℓ x t a i p x t a z t f ℓ mp η t m t σ t ℓ 1 z t z t f i ℓ ν t i ℓ 13 x t 1 s i ℓ x t 1 s i p x t 1 s z t f ℓ ν t i ℓ where m def g l p x t a z t f ℓ and p x t 1 s z t f ℓ are the sample cross covariances computed from x t a i i 1 m x t 1 s i i 1 m and z t f i ℓ i 1 m and p η t is the sample covariance of the ensemble η t i i 1 m with η t i def x t a i t x t 1 s i t t the noise covariance matrix for the next iteration is then estimated as 14 β t ℓ 1 β t 1 1 2 z t g x t a ℓ l x t 1 s ℓ 2 trace mp γ t ℓ m t 15 λ t ℓ 1 β t ℓ 1 α t 16 σ t ℓ 1 λ t ℓ 1 i n z where x t a ℓ and x t 1 s ℓ are the ensemble means of x t a i ℓ i 1 m and x t 1 s i ℓ i 1 m respectively p γ t ℓ is the sample covariance of γ t i ℓ i 1 m with γ t i ℓ def x t a i ℓ t x t 1 s i ℓ t t and α t α t 1 n z 2 at the initial iteration λ t 0 β t 1 α t and σ t 0 λ t 0 i n z the analysis covariance σ t l as well as the analysis state estimates x t a i l i 1 m are then used in the next data assimilation cycle 2 3 dual state parameter estimation with the uwcenkf we first present the parameter estimation steps in section 2 3 1 then the state estimation steps in section 2 3 2 2 3 1 parameter estimation the goal is to update the parameters based on the current observations y t the parameters members given the past observations θ t 1 i i 1 m and the previous analysis ensemble x t 1 a i i 1 m this begins with a forecast step to compute the state forecast ensemble x t f i i 1 m and associated observation forecast ensemble y t f i i 1 m as 17 x t f i m t 1 x t 1 a i θ t 1 i ν t i y t f i h t x t f i w t i the current parameter ensemble θ t i i 1 m based on y 0 t can then be obtained using 18 θ t i θ t 1 i p θ t 1 x t f h t hp x t f h t r t 1 y t y t f i τ t i with the sample forecast error covariance p x t f and the sample cross covariance between the analysis parameter at t 1 and the state forecast errors at t p θ t 1 x t f given by 19 p x t f n 1 1 s x t f s x t f t 20 p θ t 1 x t f n 1 1 s θ t 1 s x t f t 2 3 2 state estimation the state estimation with uwcenkf involves an osa smoothing step a forecast step and two successive analysis steps these should provide better state estimates based on the updated parameters and most recent observations and also to enforce the water balance smoothing step the smoothed ensemble is first computed as 21 x t 1 s i x t 1 a i p x t 1 a x t f h t τ t i where p x t 1 a x t f is the sample cross covariance between the analysis state at t 1 and the forecast state at t 22 p x t 1 a x t f n 1 1 s x t 1 a s x t f t forecast step the state forecast ensemble and associated observation forecast ensemble at time t are then obtained by integrating x t 1 s i θ t i i 1 m with the model 23 ξ t f i m t 1 x t 1 s i θ t i ν t 1 i y t f i h t ξ t f i w t i first analysis step the resulting state ensemble ξ t f i i 1 m is then updated based on the available observation y t using the uwcenkf first analysis step eq 7 replacing x t f i with ξ t f i to update the unconstrained state analysis ensemble x t a i i 1 m second analysis step the water balance constraint is finally imposed based on z t using an enkf update σ t is also estimated as it is needed for the computation of the analysis ensemble x t a i i 1 m these are performed using eqs 11 16 2 4 summary of the dual uwcenkf algorithm the process begins by integrating the updated parameters and analysis state at time t 1 x t 1 a i θ t 1 i i 1 m with the model to obtain x t f i i 1 m e g eq 17 these are then used along with the observations y t and z t to update the parameters and the state with the following steps parameter update parameters are first updated with y t using eqs 18 20 to obtain θ t i i 1 m satellite soil moisture and grace tws are used as observations in this step see second dataset in table 2 section 3 2 state update osa smoothing the observation forecast y t f i i 1 m computed from the previous state and parameters cf eq 17 is used to calculate the smoothed state ensemble based on the current observation eqs 21 22 forecast step once x t 1 s i θ t i i 1 m are calculated these are integrated with the model to forecast the state and observation i e to compute ξ t f i i 1 m and y t f i i 1 m respectively eq 23 first analysis step the forecast state ξ t f i i 1 m is then updated based on the observation following eq 7 again y t includes satellite soil moisture and grace tws this is done to obtain the unconstrained analysis ensemble x t a i i 1 m second analysis step finally x t a i i 1 m is updated with the pseudo observation z t along with the associated error covariance σ t using eqs 11 16 to obtain the final state analysis ensemble x t a i i 1 m z t contains water fluxes observations of p e q and δ s from satellite and in situ measurements see third dataset in table 2 section 3 2 2 5 computational cost important for an efficient state parameter estimation is a reasonable computational complexity this is particularly of interest when dealing with high dimensional models and or operational systems therefore while achieving accurate estimates is crucial computational efficiency is also an important factor for developing an efficient data assimilation system this is investigated in this section in the following the state parameter filter using osa smoothing is referred to as uwcenkf assimilation calibration uwcenkf ac to distinguish it from uwcenkf assimilation only uwcenkf ao including only state estimation using the classical enkf khaki et al 2018a because of their similar number of forecast and update steps and iterative character both uwcenkf algorithms uwcenkf ac and uwcenkf ao have larger computational costs than the standard enkf while both enkf and uwcenkf ao apply one forecast step i e one model run they have a different number of kalman like update steps the former involves one update of the state whereas the latter includes 1 2 l additional updates one for smoothing step based on y t following 8 l for iterative analysis based on r t following 12 and l for iterative smoothing step based on r t following 13 these additional updates along with those used for estimating the covariance σ 14 16 result in a larger computational burden for the uwcenkf ao uwcenkf ac in turn applies two more additional steps compared to uwcenkf ao which makes it computationally more demanding one for updating the parameters based on y t as in 18 and one for forecasting the pseudo smoothed state and updated parameters as in 23 3 numerical experiment 3 1 hydrological model the numerical test is done using the world wide water resources assessment w3ra model w3ra is a biophysical model which was developed for representing water storage initially over australia its simulations are based on the water balance between different components such as soil moisture surface water storage and groundwater independently over each grid point khaki et al 2018b more details about this model and its underlying processes can be found in van dijk 2010 and van dijk et al 2013 in summary at each grid cell the following vertical interactions are applied the top layer soil column is fed by surface water as net precipitation and deep root soil layer is supplied through capillary rise from groundwater water then leaves the column via soil evaporation drainage into the groundwater and river runoff or extraction by shallow and deep rooted vegetation the model also includes multiple processes such as partitioning of precipitation between interception evaporation and net precipitation which is divided between infiltration infiltration excess surface runoff and saturation excess runoff soil water storage is represented by water balance of three unsaturated soil layers topsoil shallow and deep soil layer including infiltration and drainage other surface and sub surface components are modelled as groundwater dynamics including recharge capillary rise and discharge and surface water body dynamics including inflows from runoff and discharge open water evaporation and catchment water yield in addition w3ra includes vegetation processes i e transpiration and vegetation cover adjustment the daily meteorological forcing datasets include precipitation maximum and minimum temperature and downwelling short wave radiation extracted from the era interim reanalysis data the modelling in w3ra is carried out for two hydrological response units hrus these include water and energy fluxes simulation for vegetation with deeper roots hru1 and for vegetation with short and shallow roots hru2 each occupying a fraction of the grid cell accordingly parameterizations are implemented based on sub grids and vary spatially to correspondingly simulate the area fractions with water compartments previous studies e g poovakka et al 2013 have shown that w3ra calibration is essential the model applies various parameters to simulate different water storage this includes soil evaporation and its capacity of water holding parameters to represent the connectivity of catchment characteristics e g catchment geomorphology its climate terrain and land cover and saturated area as well as groundwater recession and greenness more details can be seen in van dijk et al 2013 the list of parameters used here for calibration is presented in table 1 3 2 dataset various data sets are used here for the different tasks of the experiment these are mostly derived from satellite remotely sensed products and acquired over different basins including mississippi amazon yangtze danube indus murray darling st lawrence and the orange these are mainly selected due to the availability of water discharge data from in situ measurements to compute the state and parameters estimates and to examine the capability of the proposed assimilation method over different basins three different types of datasets are considered their corresponding spatial and temporal resolutions are detailed in table 2 the first dataset is used to force the model cf section 3 1 the second dataset is used for data assimilation and includes two sets of observations for 1 state parameter estimation and 2 water budget enforcement for 1 satellite soil moisture and grace tws are used grace data level 2 are derived from the itsg grace2014 gravity field model developed by mayer gurr et al 2014 the dataset includes stokes coefficients and their errors up to degree and order 90 post processing steps are implemented following khaki et al 2018d and khaki et al 2019 to calculate tws changes to update the model summation of water stored in different soil layers groundwater and surface storage soil moisture observations are used during the calibration period to adjust the model surface soil moisture content regarding soil moisture uncertainties 0 04 m 3 m 3 and 0 05 m 3 m 3 error standard deviations are selected for smos and amsr e respectively as suggested by literature see e g de jeu et al 2008 leroux et al 2016 for 2 water fluxes observations of p e q and δ s are used to constrain the water budget closure note that multiple observations of the water components are merged following sahoo et al 2011 to achieve more accurate p and e over the basins see details in khaki et al 2018a the water flux observations are then used for data assimilation and specifically to enforce water budget constraint during the filtering process using uwcenkf the third set of data refers to in situ measurements which are used as independent information to validate the results during the calibration and forecasting periods which will be described separately hereafter this includes groundwater and soil moisture measurements note that we use specific yield values extracted from the various studies e g gutentag et al 1984 strassberg et al 2007 seoane et al 2013 khaki et al 2017c to derive groundwater storage from bore measurements 3 3 experimental setup the study period is divided into three parts 2000 2002 to generate the initial ensemble 2003 2012 to assimilate observations and calibrate the model and 2013 2016 to monitor the model forecastings as mentioned the state vector for every model grid point is composed of soil moisture at different layers of the top shallow and deep zone as well as water stored as snow vegetation surface and groundwater the observational operator h t converts these state variables into the observation space by taking into account discrepancies between the model s and observations spatial resolutions in the case of grace tws for example h t accumulates the state variables the individual water storages at grid points to determine the simulated tws in 1 1 spatial resolution in order to update them with the grace tws during assimilation further to layout the experiment an initial state ensemble of m 30 members is generated by perturbing the meteorological forcing fields this in particular is done for precipitation by considering a gaussian multiplicative error with a standard deviation of 30 for the shortwave radiation by assuming an additive gaussian error with a standard deviation of 50 wm 2 and for temperature by considering a gaussian additive error with a standard deviation of 2 c following jones et al 2007 renzullo et al 2014 sampling of zero mean gaussian distributions with these standard deviations are then performed to produce the initial ensemble a parameter ensemble is produced by independently drawing 30 random samples from each parameter s defined range cf table 1 the resulting state and parameter ensemble members are then integrated with the model from 2000 to 2002 to generate the filter initial ensemble at the beginning of 2003 as an essential step for an enkf implementation trial and errors experiments to set the value of ensemble inflation 1 1 1 3 for the parameters and state updates and localization 2 5 radii are conducted to tune the filters performances anderson et al 2007 khaki et al 2018c 4 results various aspects of the proposed filtering method are analyzed here below we first focus on the parameter calibration results section 4 1 in order to investigate the effectiveness of the calibration methods for improving water storage estimates both during the calibration and forecast periods the performance of the proposed filter with and without calibration i e uwcenkf ac and uwcenkf ao respectively as well as its comparison with the standard enkf are compared in section 4 2 section 4 3 evaluates the filter s capability in reflecting hydroclimate variabilities into the estimates by studying the impact of extreme climatic events on the water compartments and also the balance between the water fluxes 4 1 parameters calibration the proposed assimilation calibration scheme updates the parameters while assimilating new incoming observations into the system the method performance for calibrating the model parameters is discussed here a successful calibration method should provide better water storage estimates especially during the forecast period in which the model highly relies on its parameters where no observations are assimilated to constrain the state to investigate this the parameters update during the calibration period are analyzed through the patterns of their time evolution the parameters variations during the calibration period are displayed in fig 1 a convergence pattern can be clearly seen for all parameters this suggests that the parameters reach a stable and potentially optimum value given the available observations it can also be seen that generally larger variations occur in the first four years during which the method explores the parameters space this however decreases over time with much less variations after 2009 for all parameters final estimated values are relatively different from their initial values cf fig 1 the adjusted parameters and their standard deviation std by the different filters are reported in table 3 the values in this table displays average estimated parameters over the study regions spatially varying parameters can realistically lead to better estimates over different areas with different atmospheric and environmental characteristics as can be seen from table 3 some parameters indicate larger stds e g β i 0 λ ref p ref c sla which generally suggests more spatial variability moreover it is found that the estimated parameters are significantly different from the initial values which can potentially lead to different state estimate too this is particularly true when comparing the results of uwcenkf ac with uwcenkf ao which only differs in the parameters update step this is further investigated in the following sections 4 2 state estimation results 4 2 1 impact of data assimilation the performances of the filtering algorithms for estimating water storage are discussed separately for the calibration 2003 2013 and forecasting 2013 2016 periods it is worth mentioning that the better performance of uwcenkf compared to enkf and the osa based ensemble filtering over the standard joint and dual enkfs were demonstrated and discussed in details in khaki et al 2018a and ait el fquih et al 2016 respectively here the performance of the newly proposed method is compared with uwcenkf without calibration uwcenkf ao to analyse its relevance for state parameter estimation which is of particular interest during the forecast period to begin with average tws changes time series over the calibration period from model simulations with assimilation only uwcenkf ao with both assimilation and calibration uwcenkf ac without assimilation calibration open loop as well as grace for all basins are shown in fig 2 the average absolute misfits between uwcenkf ao and uwcenkf ac tws results and also for the open loop run with respect to the grace tws data are also reported in each subfigure while improvements i e smaller discrepancy from observations are noticeable from both uwcenkf ao and uwcenkf ac a slightly better matching is achieved by uwcenkf ac over most of the basins this is expected since both methods incorporate observations soil moisture and grace tws into the system state which pulls the estimates toward the observations based on the reported average errors both uwcenkf ao and uwcenkf ac improve the model outputs with respect to the observations by about 31 and 34 respectively this indicates slightly better performance of uwcenkf ac which is mainly pronounced during periods of large anomalies e g mississippi 2011 2012 and danube between 2004 and 2006 nevertheless both assimilation methods can effectively reflect extreme events in their outputs fig 3 depicts the same results as fig 2 but over the forecasting period the positive effect of calibration is clear for all basins the average error reduction i e the difference between grace tws and the filters results using uwcenkf ao and uwcenkf ac with respect to the difference between grace tws and the open loop results is respectively 7 and 26 on average these results suggest that both filters provide better estimates compared to the open loop results even though no assimilation is performed during this period this improvement is clearly more pronounced for uwcenkf ac the considerably smaller error reduction during the forecast period with uwcenkf ao is due to the lack of parameter calibration it can then be concluded from the results that the error reduction by uwcenkf ac comes from the calibrated parameters as well as the use of the osa formulation in uwcenkf ao the positive impact of uwcenkf ao at the beginning of the forecasting period is solely due to the improved initial state at the end of the assimilation window which is quickly lost after the first few forecasting steps similar to fig 2 uwcenkf ac results better reflect larger tws variabilities e g over amazon strong anomalies e g indus 2013 2014 and weaker tws changes e g mississippi during 2014 to further investigate the performance of uwcenkf ac its soil moisture estimates are compared against satellite measurements over four basins in fig 4 this is done to particularly show the impact of the assimilation calibration approach on the ensemble spread especially over the forecast period more comprehensive soil moisture evaluation for all applied filters will be discussed in the following section in fig 4 in addition to the time evolution of the soil moisture ensemble mean from uwcenkf ac the ensemble members are also plotted to compare their variations before and after 2013 which corresponds to the calibration and forecast periods similar to the previous results and as expected the assimilation results better agree with the observations during the calibration period this is also true during the forecast period especially over the indus and mississippi basins the ensemble evolution in fig 4 suggests that the variation of the ensemble spread is relatively stable during the calibration period but increases gradually over the forecasting period due to the absence of assimilation indicating larger uncertainties in the estimates the results demonstrate that the impact of uwcenkf ac may decline over time often due to the impacts of sources of uncertainties such as forcing errors nonetheless the calibration along with assimilation allows for an extended forecasting skill through the adjustment of the model parameters 4 2 2 validation with in situ measurements to further evaluate the results of data assimilation from different schemes independent in situ measurements of groundwater soil moisture and water discharge are used the estimates of different water components including soil moisture groundwater and water discharge as they result from the open loop uwcenkf ao and uwcenkf ac are interpolated spatially to the in situ measurements locations for comparisons fig 5 depicts the groundwater results based on the average estimated std and root mean squared error rmse as well as correlation for each run over two basins murray darling top row and mississippi bottom row the evaluation analysis is undertaken separately for the calibration left panel and forecasting right panel periods over the calibration period the best performance is achieved by uwcenkf ac providing the smallest rmse and std and the highest correlation with the in situ groundwater measurements followed closely by uwcenkf ao this is true for both basins with both filters improving groundwater simulations from fig 5 the smallest rmse and std as well as the highest correlations are also obtained with uwcenkf ac over the forecast period uwcenkf ac achieves considerably larger improvements with respect to the open loop run compared to uwcenkf ao which agrees with the previous results where parameters calibration effectively improve the assimilation performance over the forecast period the correlation values between the soil moisture in situ measurements and their filters estimates as they result from uwcenkf ao uwcenkf ac and the open loop run at different depths are assumed here to this end soil moisture estimates at different depths i e top shallow and deep layers are compared with in situ measurements at the corresponding depths for example while top layer soil moisture estimates are compared with the first 10 cm in situ measurements the summation of the estimated soil moisture at all layers is compared with 0 100 cm in situ soil measurements the averaged results for each method are reported in table 4 again both uwcenkf ao and uwcenkf ac achieve comparable results over the calibration period the estimated soil moisture correlation to the in situ measurements is higher for both compared to the open loop estimates over the forecasting period while uwcenkf ao improves the correlation values approximately 3 with respect to the open loop higher correlation 20 to the in situ measurements are achieved by uwcenkf ac this reflects the capability of uwcenkf ac for not only improving the state estimates during the calibration period but more importantly during the forecasting period which largely comes from the calibration of the parameters the comparison between the simulated water discharge from the three different runs open loop uwcenkf ao and uwcenkf ac with the measurements from in situ stations are displayed in fig 6 note that these in situ measurements are independent from the assimilation experiments the assessment is again undertaken separately for the calibration fig 6 top row and forecast fig 6 bottom row periods over st lawrence mississippi danube and amazon basins it can be seen that the best results are achieved by uwcenkf ac over both time periods as reflected by closer scatter points to the reference line black dashed line which represents a perfect match to the in situ measurements this becomes clearer when comparing the interpolated lines to scatter points from each method with the results of uwcenkf ac being the closest to the in situ measurements over all basins uwcenkf ao results to a lesser degree match well the in situ measurements compared to the open loop outputs especially over the calibration period over the forecast period consistent with the previous results the best performance is again achieved by uwcenkf ac suggesting the smallest discrepancies between its results and the in situ measurements these results confirm the superiority of uwcenkf ac particularly over the forecast period as shown in fig 5 and table 4 4 3 hydroclimate variability 4 3 1 extreme events in addition to providing reliable water storage estimates it is important for a hydrologic system estimates to agree with independent hydroclimatic variables this is particularly of interest for forecasting where outcomes rely largely on the model and its parameters to analyse this we focus here on the results of uwcenkf ac over the forecast period one of the main objectives of model calibration is to improve its skill for predicting the impacts of climate variability such as climate extreme events on water storage estimates to investigate this two events are selected over the indus and orange basins to monitor their corresponding effect on model simulations with and without data assimilation according to pakistan national disaster management authority ndma 2015 heavy monsoon rains the rapid melting of snow and outbursts from glacial lakes in 2015 caused flash floods over different areas within the indus basin over the orange basin monyela 2017 reported a two year drought from 2014 to 2015 mostly due to el niño impact these two strong anomalies can be clearly observed in fig 7 which shows monthly basin averaged precipitation time series as well as tws changes from uwcenkf ac and the open loop run rainfall pattern shows a noticeable increase over the indus basin during 2015 on the other hand a decline rainfall pattern is evident after 2014 over the orange basin the corresponding tws changes for the same periods from the open loop run as well as uwcenkf ac are also shown in fig 7 the results indicate a disagreement between the open loop and uwcenkf ac predictions especially over the indus basin the missing positive trend in the open loop time series is successfully retrieved after the application of uwcenkf ac over the orange basin the negative trend is predicted by both the open loop and uwcenkf ac results this however is more pronounced in the uwcenkf ac tws time series which better capture the drought event the results clearly suggest the effectiveness of the calibration through uwcenkf ac for improving the model simulations over the forecast period 4 3 2 water balance results as discussed in section 2 one of the main features of uwcenkf is its ability to enforce the water budget enforcement while assimilating incoming observations as demonstrated by khaki et al 2017c khaki et al 2018a here we investigate the balance between the estimated water storage changes as they result from uwcenkf ac and independent precipitation evaporation and water discharge data which are not used by the model this is done for the forecast period to explore how effective is uwcenkf ac for reducing the open loop imbalance through the estimation of the parameters the average imbalance over all basins are calculated and the average and std values over the forecast period are presented in fig 8 arrows in the figure start from the open loop results and end on the uwcenkf ac results it is clear that uwcenkf ac successfully decreases the imbalances for all basins this is particularly true for the larger basins such as the amazon and mississippi basins which are generally more difficult to model because of the contribution of various hydrologic processes e g the existence of the large surface body of water fig 8 also suggests that water storage estimates from uwcenkf ac better preserve the balance between water fluxes 5 conclusions a new filtering scheme was proposed for efficient data assimilation and forecasting of hydrological applications the scheme is designed to incorporate multiple observations types into a hydrological model while estimating both the system state and parameters the scheme was developed based on the recently proposed unsupervised weak constrained ensemble kalman filter uwcenkf to update the system state based on the incoming observations simultaneously estimate the parameters and also enforce the water budget constraint numerical experiments conducted over different basins showcased the ability of the proposed scheme to effectively improve various water compartments during the assimilation calibration and forecasting periods the evaluation of the resulting system assimilating the gravity recovery and climate experiment grace derived terrestrial water storage tws and satellite soil moisture observations and validating its outputs against independent in situ measurements indicated that much better model performance can be achieved using the new approach considerable improvements were also obtained for water storage components over the forecasting period 2013 2016 compared to state only estimates the proposed hybrid assimilation calibration approach also led to better predictions of extreme events such as flood and drought events all these results stress the capability of the proposed assimilation and forecasting method for enhancing the model s performance the new scheme will be further investigated for assimilation with different hydrological models and for assimilation of different observation types in future efforts credit authorship contribution statement m khaki b ait el fquih and i hoteit conceived the presented study m khaki proposed the problem under study and b ait el fquih derived the constrained state parameter filtering solution m khaki designed and implemented the numerical analysis b ait el fquih and i hoteit provided critical insights on methodology and result interpretation m khaki prepared the initial draft of the manuscript which was improved due to substantial contributions of b ait el fquih and i hoteit declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5662,the primary focus in this paper is the estimation of precipitation from msg images meteosat second generation using a machine learning based multi classifier model learning and validation of the multiclass model is performed using the correspondences between msg satellite data and radar data to do this six classifiers were first combined in order to exploit the full potential of each of these classifiers these are random forest rf1 artificial neural network ann support vector machine svm naive bayesian nb weighted k nearest neighbors wknn and the kmeans algorithm kmeans the application of these classifiers makes it possible to carry out a classification at level 1 a pixel can therefore be assigned to more than one class by the different classifiers we calculated six certainty coefficients from these classification results to refine these results in a second step a classification at level 2 was performed using the random forest classifier rf2 taking the certainty coefficients as input parameters six classes of precipitation intensities are thus obtained very high precipitation intensities moderate to high precipitation intensities moderate precipitation intensities light to moderate precipitation intensities light precipitation intensities and no rain comparisons between the results of the multi classifiers model and those obtained by the classifiers used separately show a clear improvement in the quality of classification using multiple linear regressions precipitation rates for the different classes were determined using data from the rain gauges to validate the model precipitation amounts were estimated and compared to actual rain gauge data and then compared to the results of a few precipitation estimation methods the results are very interesting and show superior performance for the elaborated model indeed the coefficient of correlation has a value of 0 93 for developed scheme against 0 46 to 90 for the different techniques presented here for comparison also a better bias 2 2 mm and a better rmsd 12 mm were obtained for developed scheme while the other methods indicate bias between 11 mm to 16 mm and rmsd higher than 14 mm keywords precipitation estimation msg image multi classifier model learning machine classification 1 introduction today it is more than essential to develop a plan for the rational management of water resources in order to promote the development of the agricultural industrial sectors to achieve this it is necessary to know the cartography of the amounts of rainfall in time and space precipitation measurements can be carried out by rain gauges in situ or by remote sensing devices such as meteorological radar and satellites michaelides et al 2009 rain gauges installed on the ground thus make it possible to measure rainfall locally with a great precision however to cover large areas the establishment of a network of rain gauges is difficult and impossible for regions difficult to access the use of meteorological radars for the direct measurement of rainfall intensities has become widespread in order to remedy to the punctual measures of rain gauges ochoa rodriguez et al 2019 despite this success radar coverage is still limited to cover large areas geostationary satellites are therefore essential for the collection of precipitation information at much larger spatial scales however these satellites do not provide direct measurements of precipitation intensities they provide implicit information about precipitation thies et al 2008 kühnlein et al 2014 for this purpose methods that attempt to relate satellite information to rainfall amounts are developed eg kühnlein et al 2014 bensafi et al 2019 ouallouche et al 2018 lazri and ameur 2018 in most cases of precipitation quantification the principle consists of classifying the pixels of the satellite images and then assigning a precipitation rate to each identified and classified pixel e g lazri and ameur 2016a 2013 2014a precipitation is generated by vertically developing convective systems or by horizontal development advective stratiform systems in the case of convective systems the spatial extent of these systems is more localized than those of large scale systems with almost zero spatial displacement if not stationary and with intense precipitation the origin of this precipitation is the combination of several smaller mesoscale convective systems such as single cell storms multicellular storms and super cellular storms on the other hand precipitation of large scale stratiform systems is characterized by continuous persistence with low to moderate intensity these systems are mainly represented by the dynamic mid latitude frontal systems that are the result of the meeting of two hot and cold air masses classification is therefore essential in the quantification of precipitation indeed the precipitation quantification results will strongly depend on the quality of the classification currently we are witnessing the emergence of a variety of machine learning classifiers applied to the classification of satellite images among others the random forest ouallouche et al 2018 lazri and ameur 2018 kühnlein et al 2014 vector support machine sehad et al 2017 tebbi and haddad 2016 artificial neural network lazri et al 2014a b lazri et al 2012 naive bayes hameg et al 2016 weighted k nearest neighbors bensafi et al 2019 the results vary from one classifier to another all showing good performances however some models are more suited to types of precipitation than others for example the support vector machine svm and artificial neural network ann methods work best for clouds from convective systems e g sehad et al 2017 lazri et al 2014a b the results obtained in the work conducted by sehad et al 2017 and lazri et al 2014a b all showed this superiority in the case of convective clouds the large number of parameters corresponding to the infrared parameters have influenced the svm and ann in convective cloud detection in contrast the naive bayes nb and weighted k nearest neighbours methods perform well for cloud stratiform type e g hameg et al 2016 bensafi et al 2019 according to the work done by bensafi et al 2019 this remarkable performance for stratiform precipitation is due to distance calculations when assigning pixels indeed these distances are spaced in the case of stratiform precipitations because of the large fluctuations in the spectral parameters used this allows to better separate between the different classes on the other hand in the case of convective precipitation the values of these spectral parameters are more concentrated the calculated distances are close together and overlap which makes classification sometimes incorrect therefore it is important to combine several classifiers in order to take advantage of each classifier in the case of mixed precipitation this combination makes it possible to better identify and classify both convective and stratiform precipitation the mediterranean region is a region characterized by subtropical and middle latitude climates with convective and stratiform precipitation rich in spectral information the meteosat second generation msg satellite provides a comprehensive view of the mediterranean region in the 12 observation channels between the visible and infrared with a spatial resolution of 3x3 km2 at the sub satellite point at a frequency of 15 min acquisition rainfall estimation in this region requires the use of visible and infrared channels because of the complexity of its climate indeed as has been mentioned many times in the literature estimation methods using infrared are adapted to convective systems eg arkin and meisner 1987 amorati et al 2000 the latter was easily identifiable in the infrared stratiform systems on the other hand are characterized by a large and homogeneous horizontal structure they are relatively hot at the top and cannot be identified in the infrared channel therefore in the case of mixed situations techniques using the optical and microphysical properties of clouds seem to be the most appropriate eg thies et al 2010 2008 lazri and ameur 2016b levizzani 2003 thus in the context of optimizing precipitation estimates in the mediterranean region we have developed a multi classfiers model using the optical and microphysical properties of clouds extracted from msg satellite observation channel combinations this multi classifiers model mmultic takes into account the advantages of each of the classifiers exploiting the information resulting from the combinations of the observation channels as part of this work the aim is to first classify msg images into six classes according to precipitation intensities very high precipitation intensities moderate to high precipitation intensities moderate precipitation intensities light to moderate precipitation intensities light precipitation intensities and no rain in a second time precipitation estimates for the northern region of algeria were made for this purpose in the context of classification the methodology consists firstly of performing a classification at level 1 of msg images using six classifiers namely random forest rf1 artificial neural network ann support vector machine svm naive bayesian nb weighted k nearest neighbors wknn and the kmeans algorithm kmeans then using rf2 classifier a classification at level 2 is performed to refine the results as part of the estimation precipitation rates are determined for the different classes by comparison with rain gauge measurements thus precipitation totals are quantified and precipitation mapping is elaborated the rest of the article is organized as follows in section 2 data source and preprocessing are presented the classification methodology and the results obtained are described in section 3 the precipitation estimate and the results are discussed in section 4 conclusion is given in section 5 2 data source and preliminary treatment to estimate precipitation from data from the msg satellite a multi classifier model was developed and applied in north east algeria see fig 1 for learning and validation of the model reference data from the meteorological weather radar and data from rain gauges are used the methodology consists of two parts classification of msg images precipitation estimation indeed the elaborate multi classifier model was learned then tested using the pairs of msg radar data in the case of classification in the case of the estimation the pairs of rain gauges satellite data are used 2 1 data used as mentioned earlier precipitation estimation requires three types of data these are msg data used for classification and estimation corresponding radar data used for classification and corresponding rain gauge data used for estimation so two pairs of data are built msg data radar data for classification msg data rain gauge data for estimation 2 1 1 msg data the satellite data used in this article are generated by the seviri spinning enhanced visible and infrared imager imagery radiometer embedded in the msg geostationary satellite that is positioned in an orbit at an altitude of approximately 36 000 km second generation meteosat msg is part of meteosat meteorological satellites data from msg consists of multispectral images collected at a frequency of 15 min over twelve observation channels ranging from visible to infrared four channels in the visible vis0 6 vis0 8 nir1 6 and the high resolution hr vis 0 5 0 9 and 8 channels in the infrared and water vapor ir3 9 wv 6 2 wv7 3 ir8 7 ir9 3 ir10 8 ir12 0 ir13 the spatial resolution at the closest point to the satellite equator point of these data is 1x1km2 for the hr vis channel and 3x3km2 for the other 11 channels the resolution varies depending on the latitude and longitude of the regions observed for the case of our study area north of algeria the resolution reaches 4 5 km2 the value of a pixel also called count can be converted into radiance then into reflectance for visible channels and brightness temperature for infrared channels eumetsat 2004 2 1 2 radar data in the case of classification msg multispectral data is compared to radar data images during learning and validation of the multi classifier model these are instantaneous data and are reliable for classification the radar data are collected at a frequency of 15 min by the meteorological radar of sétif with a resolution of 1 1 km2 this radar is installed in the region of sétif at geographical coordinates of 36 11 n 5 25 e and at an altitude of 1700 m above sea level the inclination movement is between 20 and 90 and the azimuth displacement is between 0 and 360 continuously the polarization of wave propagation is linear and horizontal this is awsr 81c c band radar and its operating frequency is 5 6 ghz radar images recorded in the plane position indicator ppi are 512 512 pixels in size the pixel is coded on four bits which gives 16 levels of radar reflectivity 04 12 18 22 26 30 34 38 42 46 50 54 58 62 66 70 given in dbz rainfall intensities are classified according to the level of reflectivity the weather radar was calibrated using a dense rain gauge network spread over the study area this work is carried out within the framework of a collaboration between the national office of meteorology onm of algiers and the laboratory of analysis and modeling of random phenomena lampa of tizi ouzou lazri et al 2013 the relationship between the precipitation intensity r mm h and the radar reflectivity factor z dbz is given in table 1 with 1 z d b z 10 log z a control of quality of this data is performed before being integrated into the database at the acquisition and digitizing and preprocessing station note that here the processed pixels are located near the radar center to avoid errors due to the attenuation of the signal for the distant pixels 2 1 3 rain gauges data in the case of precipitation estimation the pairs of rain gauges msg data are used rain gauges measurements provide reliable information on precipitation totals hence their integration in this estimation part rain gauge data used here are collected on a daily scale by a network of 219 rain gauges spread over the study area fig 1 the data were quality controlled before being integrated into the database 2 1 4 matching between data msg and radar data msg and rain gauge for a better comparison between the three types of data in the learning and validation stages the preliminary treatment for spatial and temporal correspondences is realized to adjust the radar satellite spatial match during the classification the spatial resolution of the radar which is 1x1km2 has been changed according to the resolution of the satellite which is 4x5km2 and in order to decrease the parallax shift due to the observation by the seviri radiometer of the earth under an oblique angle and which is aggravated by the presence of high clouds the pixels of reprojected radar images were also spatially aggregated within 5 5 pixel box in the case of high level clouds and 3 3 pixel box for the other heights in order to find the maximum correspondence with the seviri images lazri et al 2013 on the other hand since both instruments provide data at 3 min offset temporal matching was not necessary for an optimal comparison between satellite data and rain gauge data during the estimation the pixels that are vertically above the stations are collocated with these rain gauges in order to better adjust this correspondence the average value of a 5 5 pixel array for high level clouds and the average value of a 3 3 pixel array for low level clouds are compared to the rain gauge data in addition to guarantee the quality of the vis and nir channels in the presence of the sun the scenes having a solar zenith angle lower than 70 are part of the data set of daytime otherwise the scenes are considered taken during the nighttime 2 1 5 choice of input parameters as mentioned earlier the objective of this work is the estimation of precipitation from satellite data however satellite observations are indirect measurements of precipitation but they provide information on the optical and microphysical properties of clouds indeed the combination of the different observation channels gives information on the optical and micro physical properties of the clouds which are decisive in the quantification of precipitation totals 2 1 5 1 cloud optical and microphysical properties cloud optical and micro physical properties can be determined from satellite observations in different wavelengths from visible to infrared these observations provide twelve images in the different channels in the case of msg satellite visible observations allow the detection and monitoring of cloud masses the nir is used in the distinction cloud of water and ice distinction snow clouds laden with water the combination of visible and near infrared that can be used during the daytime is sensitive to the size of the water droplets and the optical thickness e g kühnlein et al 2014 the wavelength around 3 9 μm as for the visible and near infrared is sensitive to the size of the water droplets and the optical thickness during the nighttime e g thies et al 2008 in these lengths low clouds and night fogs are better detected water vapor canals measure water vapor in the middle and upper troposphere levizzani 2003 with these channels it is possible to identify cirrus by combining them with infrared channels satellite observations between 8 30um and 13 40um detect clouds with cold summits such as cirrus and cumulonimbus as well as atmospheric and terrestrial surface temperatures levizzani 2003 to properly identify and classify precipitation intensities the cloud optical and micro physical properties are exploited they are not direct measurements of precipitation but they are decisive in the classification of precipitation intensities these properties can be summarized as follows 2 1 5 1 1 cloud top temperature ctt the cloud top temperature is widely used in precipitation estimation for convective systems feidas and giannakos 2011 thies et al 2008 lazri et al 2014a b indeed the probability that a cold cloud is precipitating in the case of convective systems is important that a cloud not cold 2 1 5 1 2 cloud top high cth the cloud height depends on the level of development of the cloud feidas and giannakos 2011 thies et al 2008 lazri et al 2014a b a cloud with significant vertical development is known to contain significant rainfall 2 1 5 1 3 cloud water path cwp this parameter informs both the size of the water or ice droplet and the optical thickness of the clouds thies et al 2008 thies et al 2010 a cloud with large droplet and a large optical thickness is likely to generate more precipitation 2 1 5 1 4 cloud phase cp the cloud thermodynamic phase provides information on the presence of ice drops in the clouds strabala et al 1994 ackerman et al 1998 the presence of ice in the clouds except for the cirrus is an indicator of precipitation 2 1 5 1 5 cloud type ct the spatial features can give precious information about the cloud type this can be implicitly related to cloud precipitation rates lazri et al 2014b this information is indicated by the integration of the neighborhood of the pixel to be processed 2 1 5 1 6 cloud evolution ce the knowledge of cloud evolution between the development phase and the decay phase makes it possible to better characterize precipitation intensities in the clouds lazri et al 2014b 2 1 5 2 combination of observation channels observation channels are combined to extract information on the optical and microphysical properties of clouds thus we used twelve multi spectal parameters from msg data it is a question of the parameter tir10 8 used to gain information about ctt and cth especially for convective clouds the cloud top height is usually related to the brightness temperature in this channel e g thies et al 2008 lazri et al 2014a b the parameter δtir10 8 ir12 1 used to gain information about the cloud thermodynamic phase cp indeed the absorption of ice and water is different in the two channels ir10 8 and ir12 1 baum and platnick 2006 in the case of water particle absorption is lower in the ir10 8 channel than in the ir12 1 channel on the other hand in the case of ice particles the two channels have opposite similarities the parameter δτir8 7 ir10 8 presents the same characteristics than δτir10 8 ir12 1 it is used to separate between ice and water this is directly related to the cloud thermodynamic phase cp strabala et al 1994 thies et al 2008 the presence of ice particles gives positive values for the δτir8 7 ir10 8 in contrast the δτir8 7 ir10 8 has a tendency to negative values for water particles baum and platnick 2006 the parameter δtir3 9 wv7 3 can be used to extract information about cloud effective radius and cloud optical thickness cwp that is critical in precipitation characterization thies et al 2008 during the daytime the ir3 9 channel is disturbed by the presence of both reflected solar radiance and thermal emitted radiance and therefore gives erroneous information to prevent misinterpretation the ir3 9 channel is used only during nighttime the parameter δtir3 9 ir10 8 has the same properties as the parameter δtir3 9 wv7 3 it is retained for the extraction of information on the cwp during the nighttime the parameter δtwv7 3 ir12 1 is used to get information about ctt and cth its use is essential to separate convective clouds from non precipitating cirrus clouds indeed the emission in the water vapor channel wv7 3 is low for cirrus with a summit very cold the parameter δtwv6 2 ir10 8 has the same properties as the parameter δtwv7 3 ir12 1 it is therefore used to eliminate non precipitating cirrus and also to reinforce information on the presence of convective systems it is a parameter used daytime and nighttime the parameter rvis0 6 which is only available on the daytime provides information on the cwp the higher the reflectance in this channel the larger the cwp the parameter rnir1 6 of which its use is recommended during the daytime is also incorporated to gain information about the cwp it has characteristics similar to the parameter rvis0 6 the variance and mean in the ir10 8 channel are also integrated to take account of information about the cloud type ct the temporal features δt10 8 t 10 8 t 1 permits to give the information about cloud evolution ce in particular for the convective cloud which has a different evolution compared to a stratiform cloud lazri et al 2014b table 2 gives the different input parameters used in the classification for daytime prototype and nightime prototype the possible values range that each parameter can take are also given 2 1 6 rainfall intensity differentiation in the mediterranean region because of its geographical and climatic complexity the mediterranean region is characterized by very varied precipitation in terms of the frequency and intensity of its weather events its climate is influenced by both the sub saharan climate generating convective systems and the mid latitude climate often producing stratiform systems consequently the precipitation of the mediterranean region is located between low advective stratiform areas and convectively dominated areas with intermediary precipitations such as enhanced and moderate advective stratiform convective precipitation is unstable and comes from the elevation of moisture laden air masses by buoyancy this convection of moist air is responsible for the formation of cumuliform type clouds with a large vertical extension the precipitation associated with the convective system is characterized by a strong intensity to very strong a spatial heterogeneity a short duration on the other hand the advective stratiform precipitation area with a stable character is generated by the frontal system which is the result of widespread rising processes houze 1993 the latter is responsible for the formation of nimbostratus clouds stratocumulus stratus the presence of the enhanced advective stratiform in the envelope of the frontal precipitation systems produces relatively large precipitation due to a local confined strengthening of the buoyancy in the upper cloud levels houze 1993 also the associated seeder feeder effect leads to the moderate at low advective stratiform precipitation area the synoptic precipitation caused by mid latitude depressions are perfect examples advective stratiform precipitation is characterized by a low to moderate intensity a continuous aspect a relative spatial homogeneity a long duration taking into account all these situations precipitation in the mediterranean region can therefore be divided into the six subareas class 1 class 2 class 3 class 4 class 5 and class 6 representing the different precipitation processes see table 3 the correspondences of these six subareas in radar reflectivity are also given in table 3 3 classification methodology in this section the principle of the methodology used to identify and classify precipitation zones into six classes class 1 class 2 class 3 class 4 class 5 and class 6 according to precipitation intensities was explained to do this two successive classifications level 1 and then level 2 have been carried out in fact six classifiers based on machine learning were used in the context of this study for the classification of msg images at level 1 in this level of classification ten input parameters table 2 from msg are incorporated the classifiers employed are naive bayes nb support vector machine svm artificial neural network ann weighted k nearest neighbor wknn kmeans random forest rf1 the methodology consists in applying all the classifiers for classification at level 1 thus a pixel can be assigned to several classes by the different classifiers in other words classifiers can point to different classes for the same pixel during classification to make the decision of belonging of each pixel according to the results obtained certainty coefficients are calculated the random forest rf2 classifier is then applied for classification at level 2 using certainty coefficients as input parameters each pixel is therefore assigned to a single class by the classification at level 2 in the following the details of our approach by giving a short review of different classifiers used in this article are presented 3 1 l artificial neural network as part of that work the artificial neural network ann is used in the classification at level 1 the ann is one of the machine learning techniques best suited to image classification the structure of neuron networks consists of several layers and each layer contains a set of neurons mathematically a neuron network can be formulated by eq 2 2 h j x σ w j i 1 n w ij x i where σ represents a non linear activation function to form a multilayer percepton neuron network mlp these units are given as successive layers the outputs of one layer being connected to the inputs of the next layer by weighted connections called synapse coefficients the first layer input layer contains input values x x1 x2 xp that are connected to the second layer hidden layer the second layer consists of activation units hj for converting weighted values of the input layer to nonlinear output the third layer output layer takes the weighted outputs of the second layer as inputs into a single activation unit and outputs the predicted value it should be noted that an mlp may consist of more than three layers the principle that connects inputs to outputs remains the same in order to learn mlp it is necessary to estimate weights wij by minimizing the loss function using the back propagation algorithm which is a best known optimization methods the activation functions for back propagation networks is the sigmoid function fa ir 0 1 given by the eq 3 3 f a x 1 1 e a x where a is a constant which can be selected arbitrarily the reciprocal 1 a is called the temperature parameter in neural networks neural networks have been widely used in classification as with all classifiers used in this work this classifier is part of supervised learning the performance of a neural network is of the same order as the svms but depends on the data the best neural network structure to use is usually identified by performing several tests it also depends on the number of input attributes and the output classes several studies have been published using neural networks in the classification and estimation of precipitation lazri et al 2014a b lazri and ameur 2018 the mlp is used here constituted of 3 layers an input layer consisting of ten predictive variables ten input parameters 13 neurons are used for the hidden layer and six neuron for output layer six predicted variables for the activation function sigmoid function was chosen thus two mlps of the same structure are constructed one for daytime data and another for nighttime data 3 2 svm multiclasses to create a multiclass svm the approach adopted here is to simultaneously optimize k binary classifiers crammer and singer 2002 the principle is to create a classifier fk for each class to separate the points of this class from all other points if the classifiers are assumed logistic regressions which gives fk x p y k x it is therefore logical to predict that x belongs to the class whose prediction is the highest the fk x indicates the distance between x and the hyperplane that separates the class k from the others the higher this value the greater the chance that x belongs to the class k therefore it is necessary to simply predict the class whose decision function makes it possible to return the highest value eq 4 4 f x arg max k f k x to design the multi class svm k hyperplane equation separators fk x wk x bk are built one after the other each of these hyperplanes must check the following expression 5 arg min w k r p b k r ξ k r 1 2 w k 2 2 c i 1 n ξ ik with ξ ik 0 et δ k y i w k x b k 1 ξ ik i 1 n où δ k y i worth 1 if y i k and 1 if not this gives k optimization problems with the number of 2n constraints to solve the constraints are posed as follows each point x i must be at a maximum distance ξi who is in the wrong side of the border of the zone of indecision that separates his class and the union of other classes however it is possible to change the constraint to identify the k hyperplanes simultaneously allowing each point of the training game to be in the right side of the hyperplane separator of its class of equation f y i x and also to be further from this hyperplan compared to all others given by f k k y i in principle it is therefore preferable that for each point x i 6 w y i x i b y i w k x i b k 1 k y i however this is not always possible so it is possible to reintroduce the adjustment variables with the condition given by the following expression 7 w y i x i b y i w k x i b k 1 ξ ik k y i the reformulation of the multi class svm can be posed by 8 arg min w r pxk b r k ξ r 1 2 w k 2 2 c i 1 n ξ i with ξ i 0 and w y i x i b y i w k x i b k 1 ξ i k y i the prediction is done according to the maximum decision function 9 f x a r g m a x k 0 k 1 w k x b k svms are a family of machine learning algorithms unlike other classifiers in the case of multi classes the approach used is the separation of a class from other classes this process is repeated until the entire separation of all classes in other words several binary classifications are made sehad et al 2017 and lazri and ameur 2018 applied svms for the estimation and classification of precipitation respectively here the multi class support vector classifier svm is also used for the classification at level 1 two respective svms with the same architecture are built for daytime data and nighttime data 3 3 weighted k nearest neighbor wknn the wknn classifier is a method inspired by the standard knn method of supervised learning cover and hart 1967 the wknn classification method employed here is the improvement of the knn classifier by reducing the error rate indeed the definition of the proximity between the samples for the prediction of the class of membership of a new sample is insufficient because of the equal influence of the samples of all the neighbors the wknn method uses several weightings whose role is to assign a significant weight to close neighbors and weak for the furthest neighbors macleod et al 1987 to describe the wknn method let s consider s k the set of k nearest neighbors is formed from the metric distance of an observed sample x with the learning samples and are ordered in a growing way d 1 d k 10 s k x 1 y 2 x i y i x k y k the number of samples from the set of k nearest neighbors belonging to each class is counted the expression is the decision function of belonging of the observed sample to a class is given by eq 11 11 f j x i y ij 1 i 1 k j 1 l with l is the number of classes 12 y ij 1 s i x i j i e m e c l a s s e 1 s i n o n by assigning each nearest neighbor to a non negative weight w i 0 the decision function that represents the sum of the weights of the samples belonging to each class is given by eq 13 13 f j x i 1 k w i y ij 1 a majority vote is used to assign x to one of the only classes as follows 14 y ij 1 i f f j x f j x j 1 l j j 1 i f n o t where y i y i 1 y ij y il t represents the binary vector of the sample labels x i according to the kernel function k the passage of distances to weights is performed hechenbichler and schliep 2004 indeed the distances d i possessing a minimum in d k 0 are the values that get bigger with the decreasing absolute value of d i the following properties are thus preserved k d i 0 d i r k d i m a x i f d i 0 k d i a m o n o t o n o u s d e c r e a s i n g i f d i several core functions are used they consist of weighting as a function of distance the k closest learning samples that separate them from the observed sample indeed a larger weight is allocated for the closest samples with the exception of the rectangular function that adds a parameter to the wknn method hechenbichler and schliep 2004 the choice of the kernel function does not reveal a crucial difference in the results the rectangular kernel function is given by the following equation 15 k d i 1 2 i d i 1 with i d i 1 1 i f d i 1 0 i f d i 1 several main distances are used to calculate the distance matrix between the different neighbors in the case of this study the standardized distance is calculated in relation to k 1 i e m e closest neighbor according to the following equation 16 d x x i d x x i d x x k 1 i 1 k thus the class y of belonging of our observed sample is determined by using the following expression 17 y max r i 1 k k d x x i i y i r the wknn method differs from other classifiers in terms of learning this classifier does not imply a training phase as such the only preliminary operation is the storage of the training examples this is to avoid the aberrant classes unlike the other classifiers optimization is based on the sample size this method has been applied to the estimation of precipitation and has shown interesting results especially for stratiform precipitation bensafi et al 2019 it should be noted that the wknn method has been used in the classification at level 1 the application of this classifier on our database requires two wknns one for the nighttime data and the other for the daytime data 3 4 naive bayes nb the classification used here is a supervised classification based on bayes theorem with strong independence so called naive of hypotheses bayes naive classifier the naive hypothesis indicates that the explanatory variables are assumed to be independent conditionally to the variable to be predicted the naive bayes classifier simply requires in entry the estimation of conditional probabilities to highlight this classifier we will present a mathematic description indeed let s consider n x1 xi the set of descriptors and y the variable to be predicted target with the class attribute comprising k modalities in supervised learning for an individual w to classify the optimal assignment by the bayesian rule consists in maximizing the posterior probability of belonging to the different classes eq 18 18 y w a r g max k p y k n w so the decision is based on a viable estimate of the conditional probability p y x given by eq 19 19 p y y k n w p y y k p n w y y k p n w note that the maximization of this quantity according to yk does not depend on the denominator the assignment rule can be rewritten as follows 20 y w a r g max k p y y k p n w y y k assuming the conditional independence of the descriptors the quantity p y yk can be easily estimated from a sample of observations this consists in calculating the proportions of each modality of the variable to be predicted target the m probability estimate is used to smooth estimates on small numbers the relation used in a sample of n observations is given by eq 21 21 p y y k p k n k m n m k where nk is the number of individuals of the modality yk for a naïve bayesian classifier the descriptors are conditionally two to two independent to the values of the variable to be predicted this gives the following expression 22 p n w y y k i 1 i p x i w y y k usually the m 1 is fixed to produce an optimal value of the constant m it must be greater than 0 in order to avoid having the estimated zero probabilities of p xi y which makes the calculation of the conditional probability obsolete p n w y yk the implementation of the method involves logarithms especially when the number of descriptors is high the product of many values less than 1 the estimated conditional probabilities can lead to capacity overflows for this purpose the allocation rule used here is given by equation 23 y w arg max k ln p y y k i 1 i ln p x i w y y k as for the other classifiers this model uses supervised learning the classifier is based on bayes theorem to calculate conditional probabilities in class assignment at the end of the operation the nb uses probability maximization while the rf uses majority voting the nb method has been applied to the estimation of convective precipitation hameg et al 2016 note that this classifier is used in the classification at level 1 also two nbs are used for daytime data and nighttime data respectively 3 5 k means clustering algorithme the k means method arthur and vassilvitskii 2007 is the improved version of the standard k means method the latter can be considered an unsupervised method because it only requires the number of classes the k means method is introduced by hartigan and wong 1979 its operating principle consists of minimizing the average squared distance between points in the same cluster because of its speed and simplicity in classification the k means method is widely used the algorithm of standard k means in the case of image classification is composed of following steps arbitrary initialization of the centers of classes m1 m2 mn for n classes assignment of each pixel p in the image to one of the classes whose center of class is the closest according to the distance previously chosen calculation of new class centers m1 m2 mn for n classes using 24 m i 1 card c i p c i p repeat steps 2 and 3 until class centers do not change however the k means method depends on the choice of class centers indeed the result is often not accurate due to different initial cluster centers in a certain data distribution several treatments have shown that the k means method generally arbitrarily bad results pei et al 2015 to remedy this in the case of the k mean method the initial cluster centers are chosen more efficiently and robustly therefore k means differs from k means method by the select suitable cluster centers usually this algorithm unlike other classifiers does not use supervised learning pixels are assigned to the nearest key the mobile class centers are initially chosen arbitrarily in this work the class centers are determined based on the learning data therefore learning is considered supervised as part of this work a supervised classification algorithm based on k means has been developed the class centers are determined by learning by comparing the msg data with the corresponding radar data because of the number of input parameters used the majority voting strategy is used to assign x to one of the classes it should be noted that this algorithm is used in the classification at level 1 also two kmeans are used for the daytime data and for the nighttime data respectively 3 6 random forest rf the rf classifier was used in both classifications classification at level 1 and classification at level 2 this classifier is one of the recent machine learning techniques that uses the random forest algorithm the latter is particularly effective in linking a variable to explain with explanatory variables it based on use of decision trees such as the algorithm forest ri presented by breiman 2001 random forests are a well known ensemble learning algorithm combining not pruned classification and regression trees cart each tree through bagging method is built using the bootstrapping from the input sample subsets which are independent and identically distributed breiman 1996 random forest represents an ensemble of p trees t1 x tp x where x1 xm is vector with a m dimensional of variables of a classified object the ensemble generates p outputs y1 t1 x yp tp x where yk k 1 2 p is the prediction found by the kth tree for a classified object outputs of all trees are aggregated in order to produce one final prediction in case of the classification the predicted class is obtained by the majority of trees on the other hand for the regressions it takes the average of the predictions of each tree the algorithm is composed of the following steps draw randomly a bootstrap sample with replacement from the data set for each bootstrap sample grow a classification and regression tree choosing the best split among a randomly selected subset of mtry rather than all variables at each node the mtry represents here one of the parameters that should be adjusted in the algorithm the split process is repeated recursively on each derived subset and the tree is grown to the maximum size i e until the node products similar samples and not pruned back repeat the above steps until ntree bootstrap samples which a sufficiently large number such a tree is grown to the largest extent possible output data prediction is obtained by aggregating the predictions of ntree outputs majority vote for classification or average for regression the rf algorithm becomes the same as bagging when mtry m the best split at each node is chosen among all variables the advantage of rf is that it operates with only two parameters the number of variables in the random subset at each node mtry and the number of trees in the forest ntree liaw and wiener 2002 as part of this present work the tuning is used to determinate the optimal values of ntree and mtry is carried out before the learning of rf model the oob out of bag error which is an important feature of rf model is used in the tuning step the learning data is therefore divided into two parts a part consisting of three quarter is used in building of tree on a bootstrap sample the second remaining part consists of one quarter oob is used for the test by modifying the values of ntree and mtry we applied the rf model to the oob data and calculated each time the errors between predicted and updated classes the oob error is calculated in the case of the classification using eq 25 25 ooberror 1 n i 1 n 1 p c l a s s a c l a s s it is the number of times the predicted class p class is different from the actual class a class the results indicate that the accuracy were optimized with 400 trees ntree and six features mtry randomly sampled as candidates at each split as for the ann svm and nb rf is part of the machine learning techniques this algorithm combines the concepts of random subspaces and bagging this decision tree forest algorithm performs training on multiple decision trees trained on slightly different subsets of data unlike different classifiers the optimization of these parameters such as ntree and mtry is based on the oob out of bag error calculation in the classification and estimation of precipitation the rf classifier was used ouallouche et al 2018 lazri and ameur 2018 kühnlein et al 2014 as for the different classifiers two rfs in each level of classification level 1 and level 2 are used one for daytime data and other for nighttime data 3 7 strategy of learning and application of classifiers all classifiers were learned and tested using the correspondences between satellite data inputs and radar data output to do this three data seasons are used see table 4 the scenes recorded during these periods are a mixture of convective precipitation and stratiform precipitation in the presence of different intensities of precipitation in all 2109 precipitation scenes of which 1072 are daytime scenes and 1037 are nighttime scenes for the season 2006 2007 and 1936 precipitation events are observed during the season 2010 2011 with 985 observed during the daytime and 951 during nighttime for the season 2011 2012 2188 precipitation events are registered of which 1120 are daytime scenes and 1068 are nighttime scenes it should be noted that the processed scenes are those that have at least one raining pixel the strategy adopted here consists firstly in learning the six classifiers svm ann nb wknn kmeans and rf1 for use in the classification at level 1 using the data pairs msg radar collected during the season 2006 2007 see section 3 1 1 thus information from the learning of each classifier has been determined in order to learn the rf2 that will be used in the classification at level 2 the six classifiers are applied to the msg data of the season 2010 2011 and from the classification results the certainty coefficients are calculated see section 3 1 2 thus the rf2 classifier is learned using the data pairs of certainty coefficient radar data for the season 2010 2011 see section 3 1 3 after having learned on the one hand the classifiers svm ann nb wknn kmeans and rf1 and the classifier rf2 on the other hand the developed model was tested to classify the scenes observed during the rainy season 2011 2012 see section 4 the strategy of learning classifiers and their application is summarized in flowchart given in fig 2 3 7 1 learning of the six classifiers two prototypes for each of the classifiers were thus constructed and apprenticed one for the daytime data and the other for the nighttime data the msg data ten daytime parameters and ten nighttime parameters are confronted with the radar data see fig 3 as mentioned in the previous sections the data pairs msg radar of season 2006 2007 was used to learn the six classifiers thus the information from the learning was determined according to the theory of each of the classifiers 3 7 2 calculation of certainty coefficients the precipitation scenes from the msg satellite collected during the season 2010 2011 were classified using the six classifiers learned previously the performances of the different classifiers were evaluated to calculate the certainty coefficients this will also help to identify the best performing classifier to do this a comparison between the classification results and the corresponding radar data is carried out indeed the correct rate cr also called percentage of correct classification or on the contrary by error rate er also called percentage of wrong classification are calculated the cr and er is an overall assessment parameter of the performance classifier which are given by eqs 25 and 2 6 26 cr n c n c n nc 100 27 er n nc n c n nc 100 100 c where nc is the number of correct classification and nnc is the number of wrong classification the results of correct rate cr and error rate er for the different classifiers are presented in table 5 the results show that the classifier rf1 gives the best classifications however each of the classifiers shows a certain superiority in the identification of certain classes indeed the svm has been able to better identify class 3 and class 5 whereas class 6 is better detected by rf1 and nb the classifiers rf1 and ann better identify classes 1 and 4 we also noted that the kmeans operates better for class 2 as for the wknn shows a good classification rate for class 3 the detection of the class 6 is more or less identical because it represents the non precipitating situation that is easily identifiable consequently the integration of all this information from the different classifiers makes it possible to improve the decision of belonging of a pixel in the case where the classifiers do not point to the same class thus six coefficients of certainty cc c l a s s i corresponding to the six classes are calculated for each classified pixel using the following equation 28 cc c l a s s i cr c l a s s i svm a 1 cr c l a s s i ann a 2 cr c l a s s i wknn a 3 cr c l a s s i nb a 4 cr class i kmeans a 5 cr class i r f 1 a 6 with aj 1 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 0 8 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 1 o r c l a s s i 1 0 6 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 2 o r c l a s s i 2 0 4 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 3 o r c l a s s i 3 0 e l s e cr c l a s s i classifier represents le correct rate of classifier svm ann wknn nb kmeans or rf1 of the class i 3 7 3 learning of the classifiers rf2 the learning of the rf2 is carried out in order to use this classifier in the classification at level 2 to do this this learning is done by comparing the certainty coefficients calculated in section 3 1 2 used as inputs of the rf2 to the radar data used as output of the rf2 see fig 3 it should be noted that the rf classifier used for the classification at level 2 was chosen because it presents the best results found in the classification presented in section 3 1 2 the database used to learn the classifiers is collected during the season 2010 2011 information from this learning is determined the purpose of using this classifier of level 2 is to optimize the classification obtained by the different classifiers at level 1 3 8 test and validation in this part the scenes collected during the rainy season 2011 2012 are classified at level 1 by the six classifiers using the information from the learning of section 3 1 1 then to optimize the results a classification at level 2 was performed by the rf2 classifier using the information from the training in section 3 1 3 the classification procedure performed by the model developed is shown in fig 3 using the comparison between the classified data and radar data at the pixel scale we counted the coefficients a b c and d of the contingency table table 6 so to evaluate the classification results we were able to calculate the probability of detection pod the probability of false detection pofd the false alarm ratio far the critical success index csi and the percentage of corrects pc and the frequency bias index bias the pod indicates the fraction of pixels observed by radar that were correctly classified by the elaborated model 29 p o d a a c the pofd shows the part of pixels which is incorrectly identified by the mmultic 30 p o f d b b d the far indicates the part of pixels identified by cnn dmlp but ne correspond pas aux observation du radar 31 f a r b a b the bias shows the underestimation or the overestimation of elaborated model 32 b i a s a b a c with a bias below 1 correspond to underestimation while a bias above 1 correspond to overestimation the csi shows the fraction of observed and or identified pixels that were correctly diagnosed 33 c s i a a b c the pc measures the percentage of correct identification 34 p c a d n thus classification results of the developed model are obtained in order to evaluate the performance of mmultic these results are compared with those obtained by the six classifiers used separately see table 7 the developed model mmultic shows superior performance in classifying images relative to different classifiers employed separately in fact the statistical evaluation parameters obtained are better in the case of the model developed unlike classifiers that have more or less important overestimations and underestimates the classifications are almost balanced according to the value of the bias obtained for the model developed however a very slight over estimation is found especially for class 1 and class 2 of high intensity and which are generally derived from convective precipitation for the average intensities the case of stratiform precipitation represented by classes 3 4 and 5 we noted a very slight underestimation in terms of the probability of pod detection the mmultic shows a clear improvement in the identification of all classes for both convective and stratiform classes the far parameter values are also better compared to those obtained for classifiers used individually the developed model has thus considerably reduced the number of false alarm ratio by analyzing the values obtained from csi and the pc it is clearly remarkable that the performances of the mmultic model are far better indeed critical success index and percentage are much better also we have shown the efficiency of classifiers for each class in fact the misclassified and properly classified pixel rates are shown in fig 4 for illustrative purposes only the rates of the two best classifiers of each class were compared with that of the mmultic model for a visual impression a scene with convective and stratiform precipitation recorded on 16 01 2012 at 15 h was classified by mmultic model the scene was also classified using the different classifiers used separately the classification results are shown in fig 5 misclassified pixels are almost non existent for the scene classified by the developed mmultic model however they are clearly remarkable in the case of the results obtained by the different classifiers used individually this indicates the superiority of the module in analyzing all these results it is clear that the contribution of the classification at level 2 has greatly contributed to the improvement of the classification results the potential of each of the classifiers has been exploited for a better assignment of the pixels obtained for the classification at level 2 because in the case of the classification at level 1 the classifiers do not converge towards the same result indeed everyone operates better for certain classes of precipitation intensity 4 precipitation estimates after classifying the scenes a precipitation rate is determined and assigned to each class using the multiple linear regressions the latter was performed by comparing rain gauges and classified data rain gauge data were used because of their reliability in measuring precipitation totals data from some 219 rain gauges are used for the period 2011 2012 which contains 2188 precipitation events a part 1300 of precipitation events taken at random was used for the calculation of precipitation rates while the other part will be used for testing and validating estimates thus using multiple linear regressions we have linked the occurrences of different classes occ to the pixel located above the rain gauge to the r mm the relation used is given by eq 35 35 r m m i 1 5 r r i m m h o c c i h c m m with c mm is a constant of adjustment in the case where occi h 0 c mm 0 the occi is the number of times the class i is appeared during the period of regression it should be noted that the number of appearances of the class is converted to hours 4 appearances of the same class correspond to one hour car la temporal resolution of msg is 15 min the rri mm h is rain rate for the class i that should be calculated the rain rates obtained for each class is given in table 8 using these precipitation rates we were able to estimate the precipitation for the precipitation events of the second part of database of the season 2011 2012 to evaluate the quality of the estimates the latter are compared to the precipitation totals recorded by the rain gauges thus we calculated the root mean square difference rmsd the bias mean absolute difference mad and the percent difference pd between the rain gauge measurements and estimations they are given by 36 b i a s 1 n i 1 n e i v i 37 r m s d 1 n i 1 n e i v i 2 where ei is the estimated value and vi is the actual values for an inter comparison we have implemented the technique developed by ouallouche et al 2018 based on the classification and regression by random forest carf and the technique convective stratiform rain area technical delineation cs radt for rainfall estimation in mediterranean region developed by lazri et al 2013 this inter comparison is more plausible because the techniques cs radt carf and mmultic are all calibrated and applied in the same region using the same database we also compared the results of the mmultic with some known precipitation estimation methods in the literature jobard et al 2011 indeed the epsat sg method developed by satellites second generation berges et al 2009 the tamsat tropical applications of meteorology using satellite method grimes et al 1999 and the rfe 2 0 method rain fall estimate xie and arkin 1996 which are regional products of precipitation estimates designed for applications over africa or west africa only were used in the comparison also two methods of global products that are the gsmap mvk method ushio et al 2009 and the tropical rainfall measuring mission trmm 3b42 huffman et al 2001 are selected the near real time methods namely the method precipitation estimation from remotely sensed information using artificial neural networks the 3b42 rt method huffman et al 2007 the cmorph method cpc morphing joyce et al 2004 and gpi method arkin and meisner 1987 were selected rfe 2 0 is a method used to estimate rainfall it is developed by the national oceanic and atmospheric administration s noaa s climate prediction center cpc in this case input data from four different sources is merged these sources are gpi goes precipitation index daily gts rain gauge advanced microwave sounding unit amsu and special sensor microwave imager ssm i xie and arkin 1996 the tamsat is a method used to identify mainly the storm clouds in the ir imagery grimes et al 1999 for clouds colder than a prescribed temperature threshold a rain rate is attributed the parameter ccd cold cloud duration is computed it represents the duration where the pixel is colder for rainfall estimation a linear relationship is determined linking the cumulated rainfall to ccd in the case of the global satellite mapping of precipitation technique gsmap mvk passive microwave data with geostationary ir images are combined ushio et al 2009 the microwave data are generated from six satellite radiometers namely advanced microwave scanning radiometer amsr e on board aqua trmm microwave imager tmi on board trmm ssmi on board defense meteorological satellites program dmsp f13 f14 and f15 and amsr on board advanced earth observing satellite adeos ii the gsmap differs from trmm and gpcp products by the microwave retrieval method the trmm multi satellite precipitation analysis tmpa scheme uses the 3b42 product the precipitation related passive microwave data are collected from several satellites such as leo satellites instruments amsu b ssm i amsr e tmi the details of trmm 3b42 product is described in huffman et al 2007 the 3b42 rt precipitation product is produced by the trmm multi satellite precipitation analysis tmpa scheme as for the 3b42 product the 3b42 rt combines precipitation estimates from several satellites huffman et al 2007 the goes precipitation index gpi technique estimates tropical rainfall using cloud top temperature as the sole predictor arkin and meisner 1987 the multiplying the fractional coverage fc of cold clouds by period p and rainfall rate rr give the estimation the persiann is precipitation estimation from remotely sensed information using artificial neural networks it is an algorithm based on neural network it uses the ir brightness temperature image collected from five geo satellites namely goes 8 goes 10 gms 5 meteosat 6 and meteosat 7 satellites the procedure computes an estimate of rainfall rate at fixed area of pixel the cmorph cpc morphing technique uses data derived from the passive microwave data of all available leo satellites radiometers amsu b noaa 15 16 17 and 18 ssm i dmsp 13 14 and 15 tmi trmm and amsr e aqua the full explanation of this technique is available in joyce et al 2004 the monthly satellite estimates obtained by applying the mmutlic model are compared to the reference data from the rain gauges see figs 6 and 7 validation parameters such as rmse and bias are calculated for all methods see table 9 precipitation rates recorded by rain gauges vary between 45 mm and 370 mm during the estimation period for monthly estimates precipitation rates recorded vary between 10 mm and 160 mm in estimates using the mmultic method slight underestimates and overestimates are observed the trend is the same for all estimation periods the comparison was carried out between cumulative monthly precipitation estimated from november 2011 to march 2012 and the corresponding rainfall measurements from rain gauges the analysis of the fig 6 shows that the fluctuations are slightly important in december january and february than in november and march this is corroborated by the obtained correlation coefficients the correlation coefficients vary between 0 91 and 0 94 the months of november and march have the highest correlation coefficients r 0 94 and r 0 93 respectively due to the large presence of convective precipitation for the months of december january and february the prevailing rainfall is of stratiform type from which the correlation coefficients obtained 0 92 0 92 and 0 91 respectively in the case of the total period the correlation coefficient also has a significant level r 0 93 from the cartography fig 7 we found that significant amounts of precipitation were concentrated in the north east of the study area indeed a difference between east and west is notable in agreement with the results we have noted the coincidence of precipitation fields estimated by the developed model with those of the rain gauge network however a significant overestimate of rainfall is evident particularly in the south west of the study area we also noted underestimated precipitation in some places in the study region the spatial differences may be due to poor interpolation of rain gauge data according to the results obtained the statistics shows superior performances in the case of the model developed the coefficient of correlation has a value of 0 93 which is better comparably to those of other methods we also noticed a better bias and a better rmsd for the mmultic model such as the north is characterized by a significant orographic the observed difference can be related to potential orographic effects indeed the morphology of the orography plays an important role in maintaining of rainfall therefore the presented method is distinguished by the quality of its results found in the previous sections this performance in precipitation estimation is a consequence of the best classification obtained by the model developed on the whole the comparisons show an interesting performance for the developed method and its results are in the globally accepted range however this performance is valid only in the region where the method is applied therefore to apply it to other regions or to adapt it to global scales the presented technique requires geo climatic rehabilitation by introducing the corresponding local parameters to obtain a more reliable comparison the algorithms must be calibrated and applied to the same region with the same database algorithms may work better for some regions than others or vice versa this makes it difficult to develop a general method that can be applied to all regions and at any time the application of the mmulti technique to seviri data has improved the results of precipitation estimation by taking advantage of different classifiers in other words first classification has revealed certainty coefficients that have been exploited to refine the results in the second classification this property for classification is of interest for precipitation estimation it reduces overestimation and underestimation of rainfall in addition the mmultic technique exploits global information on vertical extension cth and cloud top temperature ctt using ir on the one hand and water ice capacity in cloud cwp using ir and or vis on the other hand contributed to better precipitation estimates 5 conclusion the objective of this paper was to develop a model for precipitation estimation from satellite data the robustness and the power of the developed model reside in the combined use of different classifiers namely svm ann nb wknn kmeans and rf1 it has benefited of the potential of each classifier in separate use each classifier identifies some classes better than others the idea is to exploit all the classifiers for an optimized classification after a suitable learning for each of the classifiers a classification at level 1 was first made using the classifiers separately a pixel can therefore be assigned to more than one class by the different classifiers the choice of the final class will depend on certainty coefficients calculated according to the classification results at level 1 to optimize the results the certainty coefficients are used as inputs for a new rf2 classifier thus a classification at level 2 was carried out by this classifier this allowed each pixel to be assigned to a single class a comparison between the classification results of the model and those obtained by the classifiers used separately is carried out we noted a marked improvement in the quality of classification indicated by all statistical evaluation parameters indeed better values are obtained the classification at level 2 allowed improving the classification rate up to 4 according to the results this rate represents the misclassified pixels at the classification at level 1 which were correctly reclassified in the classification at level 2 rainfall estimates were also made the results obtained are very interesting compared to other precipitation estimation methods the performance of the developed model is much higher these performances are due mainly to the quality of the classification of the precipitation scenes performed by mmultic furthermore in the case of classification the results can be improved by integrating data from other geostationary instruments also the implementation of principle boosting for some classifiers can optimize the performance of the model in the case of precipitation estimation the application of random forest regression can also lead to better estimates the interest of such a work is to be able to estimate the precipitation using only the satellite data thus benefiting from the good coverage credit authorship contribution statement mourad lazri conceptualization methodology software formal analysis writing original draft investigation karim labadi methodology investigation resources writing original draft writing review editing jean michel brucker conceptualization methodology validation formal analysis data curation soltane ameur project administration supervision funding acquisition visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5662,the primary focus in this paper is the estimation of precipitation from msg images meteosat second generation using a machine learning based multi classifier model learning and validation of the multiclass model is performed using the correspondences between msg satellite data and radar data to do this six classifiers were first combined in order to exploit the full potential of each of these classifiers these are random forest rf1 artificial neural network ann support vector machine svm naive bayesian nb weighted k nearest neighbors wknn and the kmeans algorithm kmeans the application of these classifiers makes it possible to carry out a classification at level 1 a pixel can therefore be assigned to more than one class by the different classifiers we calculated six certainty coefficients from these classification results to refine these results in a second step a classification at level 2 was performed using the random forest classifier rf2 taking the certainty coefficients as input parameters six classes of precipitation intensities are thus obtained very high precipitation intensities moderate to high precipitation intensities moderate precipitation intensities light to moderate precipitation intensities light precipitation intensities and no rain comparisons between the results of the multi classifiers model and those obtained by the classifiers used separately show a clear improvement in the quality of classification using multiple linear regressions precipitation rates for the different classes were determined using data from the rain gauges to validate the model precipitation amounts were estimated and compared to actual rain gauge data and then compared to the results of a few precipitation estimation methods the results are very interesting and show superior performance for the elaborated model indeed the coefficient of correlation has a value of 0 93 for developed scheme against 0 46 to 90 for the different techniques presented here for comparison also a better bias 2 2 mm and a better rmsd 12 mm were obtained for developed scheme while the other methods indicate bias between 11 mm to 16 mm and rmsd higher than 14 mm keywords precipitation estimation msg image multi classifier model learning machine classification 1 introduction today it is more than essential to develop a plan for the rational management of water resources in order to promote the development of the agricultural industrial sectors to achieve this it is necessary to know the cartography of the amounts of rainfall in time and space precipitation measurements can be carried out by rain gauges in situ or by remote sensing devices such as meteorological radar and satellites michaelides et al 2009 rain gauges installed on the ground thus make it possible to measure rainfall locally with a great precision however to cover large areas the establishment of a network of rain gauges is difficult and impossible for regions difficult to access the use of meteorological radars for the direct measurement of rainfall intensities has become widespread in order to remedy to the punctual measures of rain gauges ochoa rodriguez et al 2019 despite this success radar coverage is still limited to cover large areas geostationary satellites are therefore essential for the collection of precipitation information at much larger spatial scales however these satellites do not provide direct measurements of precipitation intensities they provide implicit information about precipitation thies et al 2008 kühnlein et al 2014 for this purpose methods that attempt to relate satellite information to rainfall amounts are developed eg kühnlein et al 2014 bensafi et al 2019 ouallouche et al 2018 lazri and ameur 2018 in most cases of precipitation quantification the principle consists of classifying the pixels of the satellite images and then assigning a precipitation rate to each identified and classified pixel e g lazri and ameur 2016a 2013 2014a precipitation is generated by vertically developing convective systems or by horizontal development advective stratiform systems in the case of convective systems the spatial extent of these systems is more localized than those of large scale systems with almost zero spatial displacement if not stationary and with intense precipitation the origin of this precipitation is the combination of several smaller mesoscale convective systems such as single cell storms multicellular storms and super cellular storms on the other hand precipitation of large scale stratiform systems is characterized by continuous persistence with low to moderate intensity these systems are mainly represented by the dynamic mid latitude frontal systems that are the result of the meeting of two hot and cold air masses classification is therefore essential in the quantification of precipitation indeed the precipitation quantification results will strongly depend on the quality of the classification currently we are witnessing the emergence of a variety of machine learning classifiers applied to the classification of satellite images among others the random forest ouallouche et al 2018 lazri and ameur 2018 kühnlein et al 2014 vector support machine sehad et al 2017 tebbi and haddad 2016 artificial neural network lazri et al 2014a b lazri et al 2012 naive bayes hameg et al 2016 weighted k nearest neighbors bensafi et al 2019 the results vary from one classifier to another all showing good performances however some models are more suited to types of precipitation than others for example the support vector machine svm and artificial neural network ann methods work best for clouds from convective systems e g sehad et al 2017 lazri et al 2014a b the results obtained in the work conducted by sehad et al 2017 and lazri et al 2014a b all showed this superiority in the case of convective clouds the large number of parameters corresponding to the infrared parameters have influenced the svm and ann in convective cloud detection in contrast the naive bayes nb and weighted k nearest neighbours methods perform well for cloud stratiform type e g hameg et al 2016 bensafi et al 2019 according to the work done by bensafi et al 2019 this remarkable performance for stratiform precipitation is due to distance calculations when assigning pixels indeed these distances are spaced in the case of stratiform precipitations because of the large fluctuations in the spectral parameters used this allows to better separate between the different classes on the other hand in the case of convective precipitation the values of these spectral parameters are more concentrated the calculated distances are close together and overlap which makes classification sometimes incorrect therefore it is important to combine several classifiers in order to take advantage of each classifier in the case of mixed precipitation this combination makes it possible to better identify and classify both convective and stratiform precipitation the mediterranean region is a region characterized by subtropical and middle latitude climates with convective and stratiform precipitation rich in spectral information the meteosat second generation msg satellite provides a comprehensive view of the mediterranean region in the 12 observation channels between the visible and infrared with a spatial resolution of 3x3 km2 at the sub satellite point at a frequency of 15 min acquisition rainfall estimation in this region requires the use of visible and infrared channels because of the complexity of its climate indeed as has been mentioned many times in the literature estimation methods using infrared are adapted to convective systems eg arkin and meisner 1987 amorati et al 2000 the latter was easily identifiable in the infrared stratiform systems on the other hand are characterized by a large and homogeneous horizontal structure they are relatively hot at the top and cannot be identified in the infrared channel therefore in the case of mixed situations techniques using the optical and microphysical properties of clouds seem to be the most appropriate eg thies et al 2010 2008 lazri and ameur 2016b levizzani 2003 thus in the context of optimizing precipitation estimates in the mediterranean region we have developed a multi classfiers model using the optical and microphysical properties of clouds extracted from msg satellite observation channel combinations this multi classifiers model mmultic takes into account the advantages of each of the classifiers exploiting the information resulting from the combinations of the observation channels as part of this work the aim is to first classify msg images into six classes according to precipitation intensities very high precipitation intensities moderate to high precipitation intensities moderate precipitation intensities light to moderate precipitation intensities light precipitation intensities and no rain in a second time precipitation estimates for the northern region of algeria were made for this purpose in the context of classification the methodology consists firstly of performing a classification at level 1 of msg images using six classifiers namely random forest rf1 artificial neural network ann support vector machine svm naive bayesian nb weighted k nearest neighbors wknn and the kmeans algorithm kmeans then using rf2 classifier a classification at level 2 is performed to refine the results as part of the estimation precipitation rates are determined for the different classes by comparison with rain gauge measurements thus precipitation totals are quantified and precipitation mapping is elaborated the rest of the article is organized as follows in section 2 data source and preprocessing are presented the classification methodology and the results obtained are described in section 3 the precipitation estimate and the results are discussed in section 4 conclusion is given in section 5 2 data source and preliminary treatment to estimate precipitation from data from the msg satellite a multi classifier model was developed and applied in north east algeria see fig 1 for learning and validation of the model reference data from the meteorological weather radar and data from rain gauges are used the methodology consists of two parts classification of msg images precipitation estimation indeed the elaborate multi classifier model was learned then tested using the pairs of msg radar data in the case of classification in the case of the estimation the pairs of rain gauges satellite data are used 2 1 data used as mentioned earlier precipitation estimation requires three types of data these are msg data used for classification and estimation corresponding radar data used for classification and corresponding rain gauge data used for estimation so two pairs of data are built msg data radar data for classification msg data rain gauge data for estimation 2 1 1 msg data the satellite data used in this article are generated by the seviri spinning enhanced visible and infrared imager imagery radiometer embedded in the msg geostationary satellite that is positioned in an orbit at an altitude of approximately 36 000 km second generation meteosat msg is part of meteosat meteorological satellites data from msg consists of multispectral images collected at a frequency of 15 min over twelve observation channels ranging from visible to infrared four channels in the visible vis0 6 vis0 8 nir1 6 and the high resolution hr vis 0 5 0 9 and 8 channels in the infrared and water vapor ir3 9 wv 6 2 wv7 3 ir8 7 ir9 3 ir10 8 ir12 0 ir13 the spatial resolution at the closest point to the satellite equator point of these data is 1x1km2 for the hr vis channel and 3x3km2 for the other 11 channels the resolution varies depending on the latitude and longitude of the regions observed for the case of our study area north of algeria the resolution reaches 4 5 km2 the value of a pixel also called count can be converted into radiance then into reflectance for visible channels and brightness temperature for infrared channels eumetsat 2004 2 1 2 radar data in the case of classification msg multispectral data is compared to radar data images during learning and validation of the multi classifier model these are instantaneous data and are reliable for classification the radar data are collected at a frequency of 15 min by the meteorological radar of sétif with a resolution of 1 1 km2 this radar is installed in the region of sétif at geographical coordinates of 36 11 n 5 25 e and at an altitude of 1700 m above sea level the inclination movement is between 20 and 90 and the azimuth displacement is between 0 and 360 continuously the polarization of wave propagation is linear and horizontal this is awsr 81c c band radar and its operating frequency is 5 6 ghz radar images recorded in the plane position indicator ppi are 512 512 pixels in size the pixel is coded on four bits which gives 16 levels of radar reflectivity 04 12 18 22 26 30 34 38 42 46 50 54 58 62 66 70 given in dbz rainfall intensities are classified according to the level of reflectivity the weather radar was calibrated using a dense rain gauge network spread over the study area this work is carried out within the framework of a collaboration between the national office of meteorology onm of algiers and the laboratory of analysis and modeling of random phenomena lampa of tizi ouzou lazri et al 2013 the relationship between the precipitation intensity r mm h and the radar reflectivity factor z dbz is given in table 1 with 1 z d b z 10 log z a control of quality of this data is performed before being integrated into the database at the acquisition and digitizing and preprocessing station note that here the processed pixels are located near the radar center to avoid errors due to the attenuation of the signal for the distant pixels 2 1 3 rain gauges data in the case of precipitation estimation the pairs of rain gauges msg data are used rain gauges measurements provide reliable information on precipitation totals hence their integration in this estimation part rain gauge data used here are collected on a daily scale by a network of 219 rain gauges spread over the study area fig 1 the data were quality controlled before being integrated into the database 2 1 4 matching between data msg and radar data msg and rain gauge for a better comparison between the three types of data in the learning and validation stages the preliminary treatment for spatial and temporal correspondences is realized to adjust the radar satellite spatial match during the classification the spatial resolution of the radar which is 1x1km2 has been changed according to the resolution of the satellite which is 4x5km2 and in order to decrease the parallax shift due to the observation by the seviri radiometer of the earth under an oblique angle and which is aggravated by the presence of high clouds the pixels of reprojected radar images were also spatially aggregated within 5 5 pixel box in the case of high level clouds and 3 3 pixel box for the other heights in order to find the maximum correspondence with the seviri images lazri et al 2013 on the other hand since both instruments provide data at 3 min offset temporal matching was not necessary for an optimal comparison between satellite data and rain gauge data during the estimation the pixels that are vertically above the stations are collocated with these rain gauges in order to better adjust this correspondence the average value of a 5 5 pixel array for high level clouds and the average value of a 3 3 pixel array for low level clouds are compared to the rain gauge data in addition to guarantee the quality of the vis and nir channels in the presence of the sun the scenes having a solar zenith angle lower than 70 are part of the data set of daytime otherwise the scenes are considered taken during the nighttime 2 1 5 choice of input parameters as mentioned earlier the objective of this work is the estimation of precipitation from satellite data however satellite observations are indirect measurements of precipitation but they provide information on the optical and microphysical properties of clouds indeed the combination of the different observation channels gives information on the optical and micro physical properties of the clouds which are decisive in the quantification of precipitation totals 2 1 5 1 cloud optical and microphysical properties cloud optical and micro physical properties can be determined from satellite observations in different wavelengths from visible to infrared these observations provide twelve images in the different channels in the case of msg satellite visible observations allow the detection and monitoring of cloud masses the nir is used in the distinction cloud of water and ice distinction snow clouds laden with water the combination of visible and near infrared that can be used during the daytime is sensitive to the size of the water droplets and the optical thickness e g kühnlein et al 2014 the wavelength around 3 9 μm as for the visible and near infrared is sensitive to the size of the water droplets and the optical thickness during the nighttime e g thies et al 2008 in these lengths low clouds and night fogs are better detected water vapor canals measure water vapor in the middle and upper troposphere levizzani 2003 with these channels it is possible to identify cirrus by combining them with infrared channels satellite observations between 8 30um and 13 40um detect clouds with cold summits such as cirrus and cumulonimbus as well as atmospheric and terrestrial surface temperatures levizzani 2003 to properly identify and classify precipitation intensities the cloud optical and micro physical properties are exploited they are not direct measurements of precipitation but they are decisive in the classification of precipitation intensities these properties can be summarized as follows 2 1 5 1 1 cloud top temperature ctt the cloud top temperature is widely used in precipitation estimation for convective systems feidas and giannakos 2011 thies et al 2008 lazri et al 2014a b indeed the probability that a cold cloud is precipitating in the case of convective systems is important that a cloud not cold 2 1 5 1 2 cloud top high cth the cloud height depends on the level of development of the cloud feidas and giannakos 2011 thies et al 2008 lazri et al 2014a b a cloud with significant vertical development is known to contain significant rainfall 2 1 5 1 3 cloud water path cwp this parameter informs both the size of the water or ice droplet and the optical thickness of the clouds thies et al 2008 thies et al 2010 a cloud with large droplet and a large optical thickness is likely to generate more precipitation 2 1 5 1 4 cloud phase cp the cloud thermodynamic phase provides information on the presence of ice drops in the clouds strabala et al 1994 ackerman et al 1998 the presence of ice in the clouds except for the cirrus is an indicator of precipitation 2 1 5 1 5 cloud type ct the spatial features can give precious information about the cloud type this can be implicitly related to cloud precipitation rates lazri et al 2014b this information is indicated by the integration of the neighborhood of the pixel to be processed 2 1 5 1 6 cloud evolution ce the knowledge of cloud evolution between the development phase and the decay phase makes it possible to better characterize precipitation intensities in the clouds lazri et al 2014b 2 1 5 2 combination of observation channels observation channels are combined to extract information on the optical and microphysical properties of clouds thus we used twelve multi spectal parameters from msg data it is a question of the parameter tir10 8 used to gain information about ctt and cth especially for convective clouds the cloud top height is usually related to the brightness temperature in this channel e g thies et al 2008 lazri et al 2014a b the parameter δtir10 8 ir12 1 used to gain information about the cloud thermodynamic phase cp indeed the absorption of ice and water is different in the two channels ir10 8 and ir12 1 baum and platnick 2006 in the case of water particle absorption is lower in the ir10 8 channel than in the ir12 1 channel on the other hand in the case of ice particles the two channels have opposite similarities the parameter δτir8 7 ir10 8 presents the same characteristics than δτir10 8 ir12 1 it is used to separate between ice and water this is directly related to the cloud thermodynamic phase cp strabala et al 1994 thies et al 2008 the presence of ice particles gives positive values for the δτir8 7 ir10 8 in contrast the δτir8 7 ir10 8 has a tendency to negative values for water particles baum and platnick 2006 the parameter δtir3 9 wv7 3 can be used to extract information about cloud effective radius and cloud optical thickness cwp that is critical in precipitation characterization thies et al 2008 during the daytime the ir3 9 channel is disturbed by the presence of both reflected solar radiance and thermal emitted radiance and therefore gives erroneous information to prevent misinterpretation the ir3 9 channel is used only during nighttime the parameter δtir3 9 ir10 8 has the same properties as the parameter δtir3 9 wv7 3 it is retained for the extraction of information on the cwp during the nighttime the parameter δtwv7 3 ir12 1 is used to get information about ctt and cth its use is essential to separate convective clouds from non precipitating cirrus clouds indeed the emission in the water vapor channel wv7 3 is low for cirrus with a summit very cold the parameter δtwv6 2 ir10 8 has the same properties as the parameter δtwv7 3 ir12 1 it is therefore used to eliminate non precipitating cirrus and also to reinforce information on the presence of convective systems it is a parameter used daytime and nighttime the parameter rvis0 6 which is only available on the daytime provides information on the cwp the higher the reflectance in this channel the larger the cwp the parameter rnir1 6 of which its use is recommended during the daytime is also incorporated to gain information about the cwp it has characteristics similar to the parameter rvis0 6 the variance and mean in the ir10 8 channel are also integrated to take account of information about the cloud type ct the temporal features δt10 8 t 10 8 t 1 permits to give the information about cloud evolution ce in particular for the convective cloud which has a different evolution compared to a stratiform cloud lazri et al 2014b table 2 gives the different input parameters used in the classification for daytime prototype and nightime prototype the possible values range that each parameter can take are also given 2 1 6 rainfall intensity differentiation in the mediterranean region because of its geographical and climatic complexity the mediterranean region is characterized by very varied precipitation in terms of the frequency and intensity of its weather events its climate is influenced by both the sub saharan climate generating convective systems and the mid latitude climate often producing stratiform systems consequently the precipitation of the mediterranean region is located between low advective stratiform areas and convectively dominated areas with intermediary precipitations such as enhanced and moderate advective stratiform convective precipitation is unstable and comes from the elevation of moisture laden air masses by buoyancy this convection of moist air is responsible for the formation of cumuliform type clouds with a large vertical extension the precipitation associated with the convective system is characterized by a strong intensity to very strong a spatial heterogeneity a short duration on the other hand the advective stratiform precipitation area with a stable character is generated by the frontal system which is the result of widespread rising processes houze 1993 the latter is responsible for the formation of nimbostratus clouds stratocumulus stratus the presence of the enhanced advective stratiform in the envelope of the frontal precipitation systems produces relatively large precipitation due to a local confined strengthening of the buoyancy in the upper cloud levels houze 1993 also the associated seeder feeder effect leads to the moderate at low advective stratiform precipitation area the synoptic precipitation caused by mid latitude depressions are perfect examples advective stratiform precipitation is characterized by a low to moderate intensity a continuous aspect a relative spatial homogeneity a long duration taking into account all these situations precipitation in the mediterranean region can therefore be divided into the six subareas class 1 class 2 class 3 class 4 class 5 and class 6 representing the different precipitation processes see table 3 the correspondences of these six subareas in radar reflectivity are also given in table 3 3 classification methodology in this section the principle of the methodology used to identify and classify precipitation zones into six classes class 1 class 2 class 3 class 4 class 5 and class 6 according to precipitation intensities was explained to do this two successive classifications level 1 and then level 2 have been carried out in fact six classifiers based on machine learning were used in the context of this study for the classification of msg images at level 1 in this level of classification ten input parameters table 2 from msg are incorporated the classifiers employed are naive bayes nb support vector machine svm artificial neural network ann weighted k nearest neighbor wknn kmeans random forest rf1 the methodology consists in applying all the classifiers for classification at level 1 thus a pixel can be assigned to several classes by the different classifiers in other words classifiers can point to different classes for the same pixel during classification to make the decision of belonging of each pixel according to the results obtained certainty coefficients are calculated the random forest rf2 classifier is then applied for classification at level 2 using certainty coefficients as input parameters each pixel is therefore assigned to a single class by the classification at level 2 in the following the details of our approach by giving a short review of different classifiers used in this article are presented 3 1 l artificial neural network as part of that work the artificial neural network ann is used in the classification at level 1 the ann is one of the machine learning techniques best suited to image classification the structure of neuron networks consists of several layers and each layer contains a set of neurons mathematically a neuron network can be formulated by eq 2 2 h j x σ w j i 1 n w ij x i where σ represents a non linear activation function to form a multilayer percepton neuron network mlp these units are given as successive layers the outputs of one layer being connected to the inputs of the next layer by weighted connections called synapse coefficients the first layer input layer contains input values x x1 x2 xp that are connected to the second layer hidden layer the second layer consists of activation units hj for converting weighted values of the input layer to nonlinear output the third layer output layer takes the weighted outputs of the second layer as inputs into a single activation unit and outputs the predicted value it should be noted that an mlp may consist of more than three layers the principle that connects inputs to outputs remains the same in order to learn mlp it is necessary to estimate weights wij by minimizing the loss function using the back propagation algorithm which is a best known optimization methods the activation functions for back propagation networks is the sigmoid function fa ir 0 1 given by the eq 3 3 f a x 1 1 e a x where a is a constant which can be selected arbitrarily the reciprocal 1 a is called the temperature parameter in neural networks neural networks have been widely used in classification as with all classifiers used in this work this classifier is part of supervised learning the performance of a neural network is of the same order as the svms but depends on the data the best neural network structure to use is usually identified by performing several tests it also depends on the number of input attributes and the output classes several studies have been published using neural networks in the classification and estimation of precipitation lazri et al 2014a b lazri and ameur 2018 the mlp is used here constituted of 3 layers an input layer consisting of ten predictive variables ten input parameters 13 neurons are used for the hidden layer and six neuron for output layer six predicted variables for the activation function sigmoid function was chosen thus two mlps of the same structure are constructed one for daytime data and another for nighttime data 3 2 svm multiclasses to create a multiclass svm the approach adopted here is to simultaneously optimize k binary classifiers crammer and singer 2002 the principle is to create a classifier fk for each class to separate the points of this class from all other points if the classifiers are assumed logistic regressions which gives fk x p y k x it is therefore logical to predict that x belongs to the class whose prediction is the highest the fk x indicates the distance between x and the hyperplane that separates the class k from the others the higher this value the greater the chance that x belongs to the class k therefore it is necessary to simply predict the class whose decision function makes it possible to return the highest value eq 4 4 f x arg max k f k x to design the multi class svm k hyperplane equation separators fk x wk x bk are built one after the other each of these hyperplanes must check the following expression 5 arg min w k r p b k r ξ k r 1 2 w k 2 2 c i 1 n ξ ik with ξ ik 0 et δ k y i w k x b k 1 ξ ik i 1 n où δ k y i worth 1 if y i k and 1 if not this gives k optimization problems with the number of 2n constraints to solve the constraints are posed as follows each point x i must be at a maximum distance ξi who is in the wrong side of the border of the zone of indecision that separates his class and the union of other classes however it is possible to change the constraint to identify the k hyperplanes simultaneously allowing each point of the training game to be in the right side of the hyperplane separator of its class of equation f y i x and also to be further from this hyperplan compared to all others given by f k k y i in principle it is therefore preferable that for each point x i 6 w y i x i b y i w k x i b k 1 k y i however this is not always possible so it is possible to reintroduce the adjustment variables with the condition given by the following expression 7 w y i x i b y i w k x i b k 1 ξ ik k y i the reformulation of the multi class svm can be posed by 8 arg min w r pxk b r k ξ r 1 2 w k 2 2 c i 1 n ξ i with ξ i 0 and w y i x i b y i w k x i b k 1 ξ i k y i the prediction is done according to the maximum decision function 9 f x a r g m a x k 0 k 1 w k x b k svms are a family of machine learning algorithms unlike other classifiers in the case of multi classes the approach used is the separation of a class from other classes this process is repeated until the entire separation of all classes in other words several binary classifications are made sehad et al 2017 and lazri and ameur 2018 applied svms for the estimation and classification of precipitation respectively here the multi class support vector classifier svm is also used for the classification at level 1 two respective svms with the same architecture are built for daytime data and nighttime data 3 3 weighted k nearest neighbor wknn the wknn classifier is a method inspired by the standard knn method of supervised learning cover and hart 1967 the wknn classification method employed here is the improvement of the knn classifier by reducing the error rate indeed the definition of the proximity between the samples for the prediction of the class of membership of a new sample is insufficient because of the equal influence of the samples of all the neighbors the wknn method uses several weightings whose role is to assign a significant weight to close neighbors and weak for the furthest neighbors macleod et al 1987 to describe the wknn method let s consider s k the set of k nearest neighbors is formed from the metric distance of an observed sample x with the learning samples and are ordered in a growing way d 1 d k 10 s k x 1 y 2 x i y i x k y k the number of samples from the set of k nearest neighbors belonging to each class is counted the expression is the decision function of belonging of the observed sample to a class is given by eq 11 11 f j x i y ij 1 i 1 k j 1 l with l is the number of classes 12 y ij 1 s i x i j i e m e c l a s s e 1 s i n o n by assigning each nearest neighbor to a non negative weight w i 0 the decision function that represents the sum of the weights of the samples belonging to each class is given by eq 13 13 f j x i 1 k w i y ij 1 a majority vote is used to assign x to one of the only classes as follows 14 y ij 1 i f f j x f j x j 1 l j j 1 i f n o t where y i y i 1 y ij y il t represents the binary vector of the sample labels x i according to the kernel function k the passage of distances to weights is performed hechenbichler and schliep 2004 indeed the distances d i possessing a minimum in d k 0 are the values that get bigger with the decreasing absolute value of d i the following properties are thus preserved k d i 0 d i r k d i m a x i f d i 0 k d i a m o n o t o n o u s d e c r e a s i n g i f d i several core functions are used they consist of weighting as a function of distance the k closest learning samples that separate them from the observed sample indeed a larger weight is allocated for the closest samples with the exception of the rectangular function that adds a parameter to the wknn method hechenbichler and schliep 2004 the choice of the kernel function does not reveal a crucial difference in the results the rectangular kernel function is given by the following equation 15 k d i 1 2 i d i 1 with i d i 1 1 i f d i 1 0 i f d i 1 several main distances are used to calculate the distance matrix between the different neighbors in the case of this study the standardized distance is calculated in relation to k 1 i e m e closest neighbor according to the following equation 16 d x x i d x x i d x x k 1 i 1 k thus the class y of belonging of our observed sample is determined by using the following expression 17 y max r i 1 k k d x x i i y i r the wknn method differs from other classifiers in terms of learning this classifier does not imply a training phase as such the only preliminary operation is the storage of the training examples this is to avoid the aberrant classes unlike the other classifiers optimization is based on the sample size this method has been applied to the estimation of precipitation and has shown interesting results especially for stratiform precipitation bensafi et al 2019 it should be noted that the wknn method has been used in the classification at level 1 the application of this classifier on our database requires two wknns one for the nighttime data and the other for the daytime data 3 4 naive bayes nb the classification used here is a supervised classification based on bayes theorem with strong independence so called naive of hypotheses bayes naive classifier the naive hypothesis indicates that the explanatory variables are assumed to be independent conditionally to the variable to be predicted the naive bayes classifier simply requires in entry the estimation of conditional probabilities to highlight this classifier we will present a mathematic description indeed let s consider n x1 xi the set of descriptors and y the variable to be predicted target with the class attribute comprising k modalities in supervised learning for an individual w to classify the optimal assignment by the bayesian rule consists in maximizing the posterior probability of belonging to the different classes eq 18 18 y w a r g max k p y k n w so the decision is based on a viable estimate of the conditional probability p y x given by eq 19 19 p y y k n w p y y k p n w y y k p n w note that the maximization of this quantity according to yk does not depend on the denominator the assignment rule can be rewritten as follows 20 y w a r g max k p y y k p n w y y k assuming the conditional independence of the descriptors the quantity p y yk can be easily estimated from a sample of observations this consists in calculating the proportions of each modality of the variable to be predicted target the m probability estimate is used to smooth estimates on small numbers the relation used in a sample of n observations is given by eq 21 21 p y y k p k n k m n m k where nk is the number of individuals of the modality yk for a naïve bayesian classifier the descriptors are conditionally two to two independent to the values of the variable to be predicted this gives the following expression 22 p n w y y k i 1 i p x i w y y k usually the m 1 is fixed to produce an optimal value of the constant m it must be greater than 0 in order to avoid having the estimated zero probabilities of p xi y which makes the calculation of the conditional probability obsolete p n w y yk the implementation of the method involves logarithms especially when the number of descriptors is high the product of many values less than 1 the estimated conditional probabilities can lead to capacity overflows for this purpose the allocation rule used here is given by equation 23 y w arg max k ln p y y k i 1 i ln p x i w y y k as for the other classifiers this model uses supervised learning the classifier is based on bayes theorem to calculate conditional probabilities in class assignment at the end of the operation the nb uses probability maximization while the rf uses majority voting the nb method has been applied to the estimation of convective precipitation hameg et al 2016 note that this classifier is used in the classification at level 1 also two nbs are used for daytime data and nighttime data respectively 3 5 k means clustering algorithme the k means method arthur and vassilvitskii 2007 is the improved version of the standard k means method the latter can be considered an unsupervised method because it only requires the number of classes the k means method is introduced by hartigan and wong 1979 its operating principle consists of minimizing the average squared distance between points in the same cluster because of its speed and simplicity in classification the k means method is widely used the algorithm of standard k means in the case of image classification is composed of following steps arbitrary initialization of the centers of classes m1 m2 mn for n classes assignment of each pixel p in the image to one of the classes whose center of class is the closest according to the distance previously chosen calculation of new class centers m1 m2 mn for n classes using 24 m i 1 card c i p c i p repeat steps 2 and 3 until class centers do not change however the k means method depends on the choice of class centers indeed the result is often not accurate due to different initial cluster centers in a certain data distribution several treatments have shown that the k means method generally arbitrarily bad results pei et al 2015 to remedy this in the case of the k mean method the initial cluster centers are chosen more efficiently and robustly therefore k means differs from k means method by the select suitable cluster centers usually this algorithm unlike other classifiers does not use supervised learning pixels are assigned to the nearest key the mobile class centers are initially chosen arbitrarily in this work the class centers are determined based on the learning data therefore learning is considered supervised as part of this work a supervised classification algorithm based on k means has been developed the class centers are determined by learning by comparing the msg data with the corresponding radar data because of the number of input parameters used the majority voting strategy is used to assign x to one of the classes it should be noted that this algorithm is used in the classification at level 1 also two kmeans are used for the daytime data and for the nighttime data respectively 3 6 random forest rf the rf classifier was used in both classifications classification at level 1 and classification at level 2 this classifier is one of the recent machine learning techniques that uses the random forest algorithm the latter is particularly effective in linking a variable to explain with explanatory variables it based on use of decision trees such as the algorithm forest ri presented by breiman 2001 random forests are a well known ensemble learning algorithm combining not pruned classification and regression trees cart each tree through bagging method is built using the bootstrapping from the input sample subsets which are independent and identically distributed breiman 1996 random forest represents an ensemble of p trees t1 x tp x where x1 xm is vector with a m dimensional of variables of a classified object the ensemble generates p outputs y1 t1 x yp tp x where yk k 1 2 p is the prediction found by the kth tree for a classified object outputs of all trees are aggregated in order to produce one final prediction in case of the classification the predicted class is obtained by the majority of trees on the other hand for the regressions it takes the average of the predictions of each tree the algorithm is composed of the following steps draw randomly a bootstrap sample with replacement from the data set for each bootstrap sample grow a classification and regression tree choosing the best split among a randomly selected subset of mtry rather than all variables at each node the mtry represents here one of the parameters that should be adjusted in the algorithm the split process is repeated recursively on each derived subset and the tree is grown to the maximum size i e until the node products similar samples and not pruned back repeat the above steps until ntree bootstrap samples which a sufficiently large number such a tree is grown to the largest extent possible output data prediction is obtained by aggregating the predictions of ntree outputs majority vote for classification or average for regression the rf algorithm becomes the same as bagging when mtry m the best split at each node is chosen among all variables the advantage of rf is that it operates with only two parameters the number of variables in the random subset at each node mtry and the number of trees in the forest ntree liaw and wiener 2002 as part of this present work the tuning is used to determinate the optimal values of ntree and mtry is carried out before the learning of rf model the oob out of bag error which is an important feature of rf model is used in the tuning step the learning data is therefore divided into two parts a part consisting of three quarter is used in building of tree on a bootstrap sample the second remaining part consists of one quarter oob is used for the test by modifying the values of ntree and mtry we applied the rf model to the oob data and calculated each time the errors between predicted and updated classes the oob error is calculated in the case of the classification using eq 25 25 ooberror 1 n i 1 n 1 p c l a s s a c l a s s it is the number of times the predicted class p class is different from the actual class a class the results indicate that the accuracy were optimized with 400 trees ntree and six features mtry randomly sampled as candidates at each split as for the ann svm and nb rf is part of the machine learning techniques this algorithm combines the concepts of random subspaces and bagging this decision tree forest algorithm performs training on multiple decision trees trained on slightly different subsets of data unlike different classifiers the optimization of these parameters such as ntree and mtry is based on the oob out of bag error calculation in the classification and estimation of precipitation the rf classifier was used ouallouche et al 2018 lazri and ameur 2018 kühnlein et al 2014 as for the different classifiers two rfs in each level of classification level 1 and level 2 are used one for daytime data and other for nighttime data 3 7 strategy of learning and application of classifiers all classifiers were learned and tested using the correspondences between satellite data inputs and radar data output to do this three data seasons are used see table 4 the scenes recorded during these periods are a mixture of convective precipitation and stratiform precipitation in the presence of different intensities of precipitation in all 2109 precipitation scenes of which 1072 are daytime scenes and 1037 are nighttime scenes for the season 2006 2007 and 1936 precipitation events are observed during the season 2010 2011 with 985 observed during the daytime and 951 during nighttime for the season 2011 2012 2188 precipitation events are registered of which 1120 are daytime scenes and 1068 are nighttime scenes it should be noted that the processed scenes are those that have at least one raining pixel the strategy adopted here consists firstly in learning the six classifiers svm ann nb wknn kmeans and rf1 for use in the classification at level 1 using the data pairs msg radar collected during the season 2006 2007 see section 3 1 1 thus information from the learning of each classifier has been determined in order to learn the rf2 that will be used in the classification at level 2 the six classifiers are applied to the msg data of the season 2010 2011 and from the classification results the certainty coefficients are calculated see section 3 1 2 thus the rf2 classifier is learned using the data pairs of certainty coefficient radar data for the season 2010 2011 see section 3 1 3 after having learned on the one hand the classifiers svm ann nb wknn kmeans and rf1 and the classifier rf2 on the other hand the developed model was tested to classify the scenes observed during the rainy season 2011 2012 see section 4 the strategy of learning classifiers and their application is summarized in flowchart given in fig 2 3 7 1 learning of the six classifiers two prototypes for each of the classifiers were thus constructed and apprenticed one for the daytime data and the other for the nighttime data the msg data ten daytime parameters and ten nighttime parameters are confronted with the radar data see fig 3 as mentioned in the previous sections the data pairs msg radar of season 2006 2007 was used to learn the six classifiers thus the information from the learning was determined according to the theory of each of the classifiers 3 7 2 calculation of certainty coefficients the precipitation scenes from the msg satellite collected during the season 2010 2011 were classified using the six classifiers learned previously the performances of the different classifiers were evaluated to calculate the certainty coefficients this will also help to identify the best performing classifier to do this a comparison between the classification results and the corresponding radar data is carried out indeed the correct rate cr also called percentage of correct classification or on the contrary by error rate er also called percentage of wrong classification are calculated the cr and er is an overall assessment parameter of the performance classifier which are given by eqs 25 and 2 6 26 cr n c n c n nc 100 27 er n nc n c n nc 100 100 c where nc is the number of correct classification and nnc is the number of wrong classification the results of correct rate cr and error rate er for the different classifiers are presented in table 5 the results show that the classifier rf1 gives the best classifications however each of the classifiers shows a certain superiority in the identification of certain classes indeed the svm has been able to better identify class 3 and class 5 whereas class 6 is better detected by rf1 and nb the classifiers rf1 and ann better identify classes 1 and 4 we also noted that the kmeans operates better for class 2 as for the wknn shows a good classification rate for class 3 the detection of the class 6 is more or less identical because it represents the non precipitating situation that is easily identifiable consequently the integration of all this information from the different classifiers makes it possible to improve the decision of belonging of a pixel in the case where the classifiers do not point to the same class thus six coefficients of certainty cc c l a s s i corresponding to the six classes are calculated for each classified pixel using the following equation 28 cc c l a s s i cr c l a s s i svm a 1 cr c l a s s i ann a 2 cr c l a s s i wknn a 3 cr c l a s s i nb a 4 cr class i kmeans a 5 cr class i r f 1 a 6 with aj 1 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 0 8 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 1 o r c l a s s i 1 0 6 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 2 o r c l a s s i 2 0 4 i f t h e c o r r e s p o n d i n g c l a s s i f i e r i n d i c a t e s t h e c l a s s i 3 o r c l a s s i 3 0 e l s e cr c l a s s i classifier represents le correct rate of classifier svm ann wknn nb kmeans or rf1 of the class i 3 7 3 learning of the classifiers rf2 the learning of the rf2 is carried out in order to use this classifier in the classification at level 2 to do this this learning is done by comparing the certainty coefficients calculated in section 3 1 2 used as inputs of the rf2 to the radar data used as output of the rf2 see fig 3 it should be noted that the rf classifier used for the classification at level 2 was chosen because it presents the best results found in the classification presented in section 3 1 2 the database used to learn the classifiers is collected during the season 2010 2011 information from this learning is determined the purpose of using this classifier of level 2 is to optimize the classification obtained by the different classifiers at level 1 3 8 test and validation in this part the scenes collected during the rainy season 2011 2012 are classified at level 1 by the six classifiers using the information from the learning of section 3 1 1 then to optimize the results a classification at level 2 was performed by the rf2 classifier using the information from the training in section 3 1 3 the classification procedure performed by the model developed is shown in fig 3 using the comparison between the classified data and radar data at the pixel scale we counted the coefficients a b c and d of the contingency table table 6 so to evaluate the classification results we were able to calculate the probability of detection pod the probability of false detection pofd the false alarm ratio far the critical success index csi and the percentage of corrects pc and the frequency bias index bias the pod indicates the fraction of pixels observed by radar that were correctly classified by the elaborated model 29 p o d a a c the pofd shows the part of pixels which is incorrectly identified by the mmultic 30 p o f d b b d the far indicates the part of pixels identified by cnn dmlp but ne correspond pas aux observation du radar 31 f a r b a b the bias shows the underestimation or the overestimation of elaborated model 32 b i a s a b a c with a bias below 1 correspond to underestimation while a bias above 1 correspond to overestimation the csi shows the fraction of observed and or identified pixels that were correctly diagnosed 33 c s i a a b c the pc measures the percentage of correct identification 34 p c a d n thus classification results of the developed model are obtained in order to evaluate the performance of mmultic these results are compared with those obtained by the six classifiers used separately see table 7 the developed model mmultic shows superior performance in classifying images relative to different classifiers employed separately in fact the statistical evaluation parameters obtained are better in the case of the model developed unlike classifiers that have more or less important overestimations and underestimates the classifications are almost balanced according to the value of the bias obtained for the model developed however a very slight over estimation is found especially for class 1 and class 2 of high intensity and which are generally derived from convective precipitation for the average intensities the case of stratiform precipitation represented by classes 3 4 and 5 we noted a very slight underestimation in terms of the probability of pod detection the mmultic shows a clear improvement in the identification of all classes for both convective and stratiform classes the far parameter values are also better compared to those obtained for classifiers used individually the developed model has thus considerably reduced the number of false alarm ratio by analyzing the values obtained from csi and the pc it is clearly remarkable that the performances of the mmultic model are far better indeed critical success index and percentage are much better also we have shown the efficiency of classifiers for each class in fact the misclassified and properly classified pixel rates are shown in fig 4 for illustrative purposes only the rates of the two best classifiers of each class were compared with that of the mmultic model for a visual impression a scene with convective and stratiform precipitation recorded on 16 01 2012 at 15 h was classified by mmultic model the scene was also classified using the different classifiers used separately the classification results are shown in fig 5 misclassified pixels are almost non existent for the scene classified by the developed mmultic model however they are clearly remarkable in the case of the results obtained by the different classifiers used individually this indicates the superiority of the module in analyzing all these results it is clear that the contribution of the classification at level 2 has greatly contributed to the improvement of the classification results the potential of each of the classifiers has been exploited for a better assignment of the pixels obtained for the classification at level 2 because in the case of the classification at level 1 the classifiers do not converge towards the same result indeed everyone operates better for certain classes of precipitation intensity 4 precipitation estimates after classifying the scenes a precipitation rate is determined and assigned to each class using the multiple linear regressions the latter was performed by comparing rain gauges and classified data rain gauge data were used because of their reliability in measuring precipitation totals data from some 219 rain gauges are used for the period 2011 2012 which contains 2188 precipitation events a part 1300 of precipitation events taken at random was used for the calculation of precipitation rates while the other part will be used for testing and validating estimates thus using multiple linear regressions we have linked the occurrences of different classes occ to the pixel located above the rain gauge to the r mm the relation used is given by eq 35 35 r m m i 1 5 r r i m m h o c c i h c m m with c mm is a constant of adjustment in the case where occi h 0 c mm 0 the occi is the number of times the class i is appeared during the period of regression it should be noted that the number of appearances of the class is converted to hours 4 appearances of the same class correspond to one hour car la temporal resolution of msg is 15 min the rri mm h is rain rate for the class i that should be calculated the rain rates obtained for each class is given in table 8 using these precipitation rates we were able to estimate the precipitation for the precipitation events of the second part of database of the season 2011 2012 to evaluate the quality of the estimates the latter are compared to the precipitation totals recorded by the rain gauges thus we calculated the root mean square difference rmsd the bias mean absolute difference mad and the percent difference pd between the rain gauge measurements and estimations they are given by 36 b i a s 1 n i 1 n e i v i 37 r m s d 1 n i 1 n e i v i 2 where ei is the estimated value and vi is the actual values for an inter comparison we have implemented the technique developed by ouallouche et al 2018 based on the classification and regression by random forest carf and the technique convective stratiform rain area technical delineation cs radt for rainfall estimation in mediterranean region developed by lazri et al 2013 this inter comparison is more plausible because the techniques cs radt carf and mmultic are all calibrated and applied in the same region using the same database we also compared the results of the mmultic with some known precipitation estimation methods in the literature jobard et al 2011 indeed the epsat sg method developed by satellites second generation berges et al 2009 the tamsat tropical applications of meteorology using satellite method grimes et al 1999 and the rfe 2 0 method rain fall estimate xie and arkin 1996 which are regional products of precipitation estimates designed for applications over africa or west africa only were used in the comparison also two methods of global products that are the gsmap mvk method ushio et al 2009 and the tropical rainfall measuring mission trmm 3b42 huffman et al 2001 are selected the near real time methods namely the method precipitation estimation from remotely sensed information using artificial neural networks the 3b42 rt method huffman et al 2007 the cmorph method cpc morphing joyce et al 2004 and gpi method arkin and meisner 1987 were selected rfe 2 0 is a method used to estimate rainfall it is developed by the national oceanic and atmospheric administration s noaa s climate prediction center cpc in this case input data from four different sources is merged these sources are gpi goes precipitation index daily gts rain gauge advanced microwave sounding unit amsu and special sensor microwave imager ssm i xie and arkin 1996 the tamsat is a method used to identify mainly the storm clouds in the ir imagery grimes et al 1999 for clouds colder than a prescribed temperature threshold a rain rate is attributed the parameter ccd cold cloud duration is computed it represents the duration where the pixel is colder for rainfall estimation a linear relationship is determined linking the cumulated rainfall to ccd in the case of the global satellite mapping of precipitation technique gsmap mvk passive microwave data with geostationary ir images are combined ushio et al 2009 the microwave data are generated from six satellite radiometers namely advanced microwave scanning radiometer amsr e on board aqua trmm microwave imager tmi on board trmm ssmi on board defense meteorological satellites program dmsp f13 f14 and f15 and amsr on board advanced earth observing satellite adeos ii the gsmap differs from trmm and gpcp products by the microwave retrieval method the trmm multi satellite precipitation analysis tmpa scheme uses the 3b42 product the precipitation related passive microwave data are collected from several satellites such as leo satellites instruments amsu b ssm i amsr e tmi the details of trmm 3b42 product is described in huffman et al 2007 the 3b42 rt precipitation product is produced by the trmm multi satellite precipitation analysis tmpa scheme as for the 3b42 product the 3b42 rt combines precipitation estimates from several satellites huffman et al 2007 the goes precipitation index gpi technique estimates tropical rainfall using cloud top temperature as the sole predictor arkin and meisner 1987 the multiplying the fractional coverage fc of cold clouds by period p and rainfall rate rr give the estimation the persiann is precipitation estimation from remotely sensed information using artificial neural networks it is an algorithm based on neural network it uses the ir brightness temperature image collected from five geo satellites namely goes 8 goes 10 gms 5 meteosat 6 and meteosat 7 satellites the procedure computes an estimate of rainfall rate at fixed area of pixel the cmorph cpc morphing technique uses data derived from the passive microwave data of all available leo satellites radiometers amsu b noaa 15 16 17 and 18 ssm i dmsp 13 14 and 15 tmi trmm and amsr e aqua the full explanation of this technique is available in joyce et al 2004 the monthly satellite estimates obtained by applying the mmutlic model are compared to the reference data from the rain gauges see figs 6 and 7 validation parameters such as rmse and bias are calculated for all methods see table 9 precipitation rates recorded by rain gauges vary between 45 mm and 370 mm during the estimation period for monthly estimates precipitation rates recorded vary between 10 mm and 160 mm in estimates using the mmultic method slight underestimates and overestimates are observed the trend is the same for all estimation periods the comparison was carried out between cumulative monthly precipitation estimated from november 2011 to march 2012 and the corresponding rainfall measurements from rain gauges the analysis of the fig 6 shows that the fluctuations are slightly important in december january and february than in november and march this is corroborated by the obtained correlation coefficients the correlation coefficients vary between 0 91 and 0 94 the months of november and march have the highest correlation coefficients r 0 94 and r 0 93 respectively due to the large presence of convective precipitation for the months of december january and february the prevailing rainfall is of stratiform type from which the correlation coefficients obtained 0 92 0 92 and 0 91 respectively in the case of the total period the correlation coefficient also has a significant level r 0 93 from the cartography fig 7 we found that significant amounts of precipitation were concentrated in the north east of the study area indeed a difference between east and west is notable in agreement with the results we have noted the coincidence of precipitation fields estimated by the developed model with those of the rain gauge network however a significant overestimate of rainfall is evident particularly in the south west of the study area we also noted underestimated precipitation in some places in the study region the spatial differences may be due to poor interpolation of rain gauge data according to the results obtained the statistics shows superior performances in the case of the model developed the coefficient of correlation has a value of 0 93 which is better comparably to those of other methods we also noticed a better bias and a better rmsd for the mmultic model such as the north is characterized by a significant orographic the observed difference can be related to potential orographic effects indeed the morphology of the orography plays an important role in maintaining of rainfall therefore the presented method is distinguished by the quality of its results found in the previous sections this performance in precipitation estimation is a consequence of the best classification obtained by the model developed on the whole the comparisons show an interesting performance for the developed method and its results are in the globally accepted range however this performance is valid only in the region where the method is applied therefore to apply it to other regions or to adapt it to global scales the presented technique requires geo climatic rehabilitation by introducing the corresponding local parameters to obtain a more reliable comparison the algorithms must be calibrated and applied to the same region with the same database algorithms may work better for some regions than others or vice versa this makes it difficult to develop a general method that can be applied to all regions and at any time the application of the mmulti technique to seviri data has improved the results of precipitation estimation by taking advantage of different classifiers in other words first classification has revealed certainty coefficients that have been exploited to refine the results in the second classification this property for classification is of interest for precipitation estimation it reduces overestimation and underestimation of rainfall in addition the mmultic technique exploits global information on vertical extension cth and cloud top temperature ctt using ir on the one hand and water ice capacity in cloud cwp using ir and or vis on the other hand contributed to better precipitation estimates 5 conclusion the objective of this paper was to develop a model for precipitation estimation from satellite data the robustness and the power of the developed model reside in the combined use of different classifiers namely svm ann nb wknn kmeans and rf1 it has benefited of the potential of each classifier in separate use each classifier identifies some classes better than others the idea is to exploit all the classifiers for an optimized classification after a suitable learning for each of the classifiers a classification at level 1 was first made using the classifiers separately a pixel can therefore be assigned to more than one class by the different classifiers the choice of the final class will depend on certainty coefficients calculated according to the classification results at level 1 to optimize the results the certainty coefficients are used as inputs for a new rf2 classifier thus a classification at level 2 was carried out by this classifier this allowed each pixel to be assigned to a single class a comparison between the classification results of the model and those obtained by the classifiers used separately is carried out we noted a marked improvement in the quality of classification indicated by all statistical evaluation parameters indeed better values are obtained the classification at level 2 allowed improving the classification rate up to 4 according to the results this rate represents the misclassified pixels at the classification at level 1 which were correctly reclassified in the classification at level 2 rainfall estimates were also made the results obtained are very interesting compared to other precipitation estimation methods the performance of the developed model is much higher these performances are due mainly to the quality of the classification of the precipitation scenes performed by mmultic furthermore in the case of classification the results can be improved by integrating data from other geostationary instruments also the implementation of principle boosting for some classifiers can optimize the performance of the model in the case of precipitation estimation the application of random forest regression can also lead to better estimates the interest of such a work is to be able to estimate the precipitation using only the satellite data thus benefiting from the good coverage credit authorship contribution statement mourad lazri conceptualization methodology software formal analysis writing original draft investigation karim labadi methodology investigation resources writing original draft writing review editing jean michel brucker conceptualization methodology validation formal analysis data curation soltane ameur project administration supervision funding acquisition visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
5663,the surface form of tension fracture in rock mass influences the seepage of fracture water greatly how to observe and quantify the fracture surface without destroying the original fracture form without separating the two surfaces of the fracture is of great significance to research the water seepage in fracture in this study tension fractures were made and developed by filling static expansion agent into drilling holes in granite blocks under the condition of keeping the original state of fracture the sample profiles including the fracture profiles were photographed and measured and the fracture aperture opening angle and closing direction along the fracture were measured accordingly the photos showed that the fracture aperture opening angle and roughness decreased gradually from the borehole wall where fractures occur to the end of the fracture the measurement results showed that the upper and lower surfaces of the fractures were very similar just like one surface fracture developing pattern was similar to branch extension pattern in structure the macroscopical trend of the fracture was closely related to the distribution of tensile stress in the specimen before the fracture occurred keywords tension fracture fracture surface form fracture aperture distribution fracture opening angle fractal dimension 1 introduction rock mass is rock which was divided into several parts by joints fractures and other structural planes the occurrence and propagation of fractures affect and change the mechanical properties of rock cook 1992 fractures are meanwhile main space for the storage and movement of groundwater in rock mass which are related to many research fields of groundwater including flow regime chen et al 2015a qian et al 2015 wu et al 2011 permeability pan et al 2010 ren et al 2015 wang and cardenas 2016 reynolds number qian et al 2016 roughness of fracture surface brown 1987 dou et al 2018 solute transport zhao et al 2016 zhao et al 2018 etc fractures are also closely related to many engineering practices such as water supply wen et al 2013 mining berkowitz 2002 geothermal development zhang et al 2020 geotechnical engineering chen et al 2008 molinero et al 2002 etc the influences of fractures on the movement of groundwater are mainly related to the characteristics of fractures themselves the fracture characteristics usually can be described by shape size and surface form including the fracture aperture the fracture opening angle the direction of fracture development and extension fracture tortuosity and the roughness of fracture surface all of them have influences on the movement of fracture water such as flow quantity flow velocity and flow pattern because it is formed by tensile force the surface of tension fracture is rougher than that of shear fracture tension fracture is widely distributed in nature which is of great significance for groundwater storage water supply and water movement the roughness of fracture surface is an important factor affecting the fracture water movement but a quantitative description of the rough surface form has always been a chronic difficulty in the study of fracture water barton and choubey 1977 lee et al 1990 lomize 1951 zhou and xie 1999 traditionally tension fracture was usually simplified as a pair of smooth parallel plates however the surface of real fracture wall is rough and uneven which is one of the most basic parameters of fracture and has a significant influence on the flow pattern of fracture water chen et al 2017 zhang et al 2019 many studies focused on the shape and size of fracture itself i e precise measurement of the surface morphology of fracture including concave and convexity points and roughness and have made considerable progress in order to describe the roughness of fracture surface some scholars applied fractal theory brandt and prokopski 1993 schmittbuhl et al 1993 sugimoto et al 1991 xie 1995 fracture aperture is another basic parameter describing the geometric characteristics of fractures which has a decisive influence on the water movement capacity of fractures meheust and schmittbuhl 2001 qian et al 2007 renshaw 1995 withpoon et al 1980 besides the flow pattern flow velocity and flow quantity are also important parameters of fracture water which involves the utility of formulas to characterize flow movement of fracture water and numerical simulation brown et al 1995 brush and thomson 2003 chaudhary et al 2011 chen et al 2015b ge 1997 konzuk and kueper 2004 mallikamas and harihar rajaram 2010 nicholl 2001 qian et al 2015 wang et al 2015 however there are some obvious shortcomings in previous studies the above topographic mapping techniques for measuring fracture surface depended on not only the precision of measuring instrument but also the separation of fracture in other words surfaces of unseparated fracture could not be measured and studied lee et al 1990 zhou and xie 1999 therefore many researches on fracture roughness were based on numerical simulation but it was still unable to reappear the true fracture roughness chen et al 2017 zhang et al 2019 once the two surfaces were separated to measure the original fracture state had been disturbed and the distance between the two fracture surfaces could not be measured which meant that the fracture aperture data became meaningless moreover a whole fracture can be divided into starting part and ending part and the fracture surface appears different morphology and geometric characteristics in different positions the occurrence and development of fractures are often related to initial fissures and initial internal forces in rocks so far few researches put sight on the relationship between the developing and propagating trend of the fracture and the initial fissures and initial internal forces the objective of this study is to describe and quantify the surface form of fractures kept the original state condition the artificial tension fractures in granite samples are studied the main work includes i the fracture surfaces are measured and studied without separating the upper and lower surfaces of the fracture throughout the whole measurement process the two wall surfaces of fractures are not separated and their original state is maintained including the wall distance fracture aperture and the original opening angle this method seems primitive but it is indeed an effective method to obtain the actual form of the fracture surface many cross sectional photographs are taken from the position where the fracture occurs to the end of the fracture then the position coordinates of the upper and lower fracture surfaces are measured on the basis of photos and the form of the fracture surfaces is evaluated distribution of fracture aperture opening angle development direction and fractal characteristics of fracture surface are also studied on this basis ii the developing and propagating trend of fractures are studied from the internal surface form of fractures and iii the relationship between the formation and form of tension fractures and the original internal stress in the rock is studied 2 material and methodology 2 1 samples preparation the samples used in this experiment were two different sizes of inada granite from japan as shown on the left side of fig 1 the size of granite block a was about 122 116 mm in cross section and the thickness was about 67 mm a breakthrough hole with a diameter of 10 mm was drilled in the center of the cross section and as shown on the left side of fig 2 the cross section of granite block b was 105 60 mm and its thickness was 40 mm a hole with diameter of 10 mm and depth of 20 mm was drilled in the center of the cross section and the hole penetrated only half of the block b instead of penetrating the rock the static expansion agent was loaded into the two drilling holes to create tensile fractures controlling the development of fractures making them occur but not penetrate the edge of the rock blocks and keeping the rock blocks in state of fractures but not completely separated that was the tips tails of the fractures were enclosed within the block bodies and did not reach the boundaries of the rock blocks thus the whole fractures could be measured from the site where they commenced to the site where they ended on the left side of figs 1 and 2 fracture profiles made of granite blocks a and b are shown the artificial fractures were roughly parallel to the bottom surfaces of the rock blocks and extended from the borehole wall to the deep some resin viscosity 5 10cp mixed with red dye was filled into the fractures the resin was made up of liquid epoxy resin and liquid curing agent when it was just mixed it had strong fluidity weak viscosity and could completely fill the fractures after chemical reaction the viscosity of the resin increased and the fluidity decreased gradually finally the resin solidified into a solid usually 24 h or longer and no heat released and firmly adhered to the fracture surface in this experiment the early fluidity and late viscosity of the resin were used the filling resin has three purposes 1 fix the two walls of fracture with the viscosity of resin to prevent the rock from expanding or contracting and prevent the rock from deforming under the force during the cutting and grinding process so as to change the fracture shape and affect the measurement accuracy 2 prevent the rock powder from entering the fracture during the grinding process and affect the photographic clarity 3 the resin is red and there is clear boundary between the resin and the rock after filling which can make it easy to distinguish the boundary when measuring and mapping after photographing after the resin had completely solidified and had strength the granite blocks were cut into slightly smaller size samples with fractures as shown on the right side of figs 1 and 2 for further grinding and measurement as shown in the figures sample 1 was cut from block a and sample 2 was cut from block b in addition it was well known that there were potential rifts in granite rock the fracture surface in sample 1 was parallel to the intrinsic rift surface however due to the missed opportunity to observe the rift the original situation of the rift in sample 2 was unknown 2 2 measurement methods of fracture surface form in samples 1 and 2 the x y z coordinate systems shown in the right side of figs 1 and 2 were established respectively fig 3 is a schematic diagram of experimental apparatus and observation procedure firstly the cross section x y plane containing the fracture was observed with a digital microscope at a magnification of 30 times and some photos of the cross section were taken then these photos were imported into the computer and smd 200 graphic analysis software was employed to measure and record the position coordinates of fracture fig 4 shows the grinding process after the x y plane had been photographed measured and recorded grind the x y plane of the sample along the z direction to the x y plane with a grinding thickness of about 2 mm the purple part in fig 4 b and repeat the photographing measuring and recording process shown in fig 3 for the new x y section continue to repeat the above grinding process grind the section along z direction to the x y plane purple part in fig 4 c is the part that continues to be ground off take photos measure and so on until the position coordinate data from the fracture generation position to the fracture pinch out position were obtained in sample 1 a total of 24 cross sections were measured in sample 2 21 cross sections were measured the measurement interval in each cross section in x direction was set about 0 4 mm according to the benchmark that measurement interval should be less than 1 30 full length of the measuring object regardless of the size and shape of the particles composing granite meanwhile the grinding thickness in z direction was set about 2 mm considering the manual transmission property of the surface grinder and the amount of data the measurement accuracy was controlled within the range of micron because a micrometer ruler was used to measure the grinding thickness after taking photos and using graphic analysis software to determine the position coordinates of measured points in the fracture profile line on each grinding plane in fact every fracture has two surfaces so each grinding plane has two fracture contour lines as shown in fig 5 here for the convenience of expression take one of them as an example these coordinates were input into mapping software to connect to form a 2d fracture outline and then these outlines from different sections grinding planes were connected into a surface by mapping software thus a complete 3d fracture surface was finally built figs 8 and 9 fig 5 shows an example of a fracture photograph taken with a digital microscope showing that the red resin was completely filled into the fracture and that the fracture surface boundary could be clearly distinguished the fracture had two surfaces the surface with larger value of y coordinate was named the upper fracture surface and that with lower value was named the lower fracture surface 2 3 analysis methods of fracture surface form the upper and lower fracture surfaces had complex surface forms based on the position coordinate data we studied several dominating characteristic quantities of the fracture surfaces as shown in fig 6 it was assumed that the upper and lower fracture surfaces were composed of paired small planar triangles bottom length 0 8 mm and height 2 mm therefore the normal vector surface area fracture aperture and fracture closing direction of the upper and lower fracture surfaces could be defined as shown in the projection plane in fig 6 the coordinates value of two vertexes were the same because two adjacent triangles shared one edge then the fracture aperture fracture opening angle and fracture closing direction were studied particularly firstly as shown in fig 7 a the bisection plane was obtained from two planes including the upper and lower fracture surfaces and the sum of vertical lengths h1 h2 from the centers of gravity of the upper and lower fracture surface planes to the bisection plane was defined as the fracture aperture secondly the included angle θ formed by the normal vectors of the upper and lower fracture surfaces shown in fig 7 b was defined as the fracture opening angle finally as shown in fig 7 c the intersection line of the two planes of the upper and lower fracture surface planes was obtained and the unit vectors from the gravity center of the upper and lower planes to the vertical direction of the intersection line were defined as the closing direction vectors of the upper and lower fracture planes respectively the closing direction of the fracture was expressed by the closing direction vectors 3 measurement results and analysis 3 1 measurement of position coordinate using the method shown in fig 3 to measure the position coordinates of fracture the visualization of the spatial morphology of the whole fracture could be realized figs 8 and 9 are three dimensional schematic diagram of the lower fracture surfaces of samples 1 and 2 respectively and the shape of the upper fracture surface are almost the same the fracture surfaces in figs 8 and 9 totally appeared complex roughness the height difference of sample 2 is slightly larger than that of sample 1 but the maximum difference between them is no more than 5 mm in addition although the differences between high and low areas can be clearly distinguished there is little evidence of the relationship between the differences and the original borehole location in order to reveal the correlation between the upper and lower fracture surfaces more attention should be paid to the measurement data of each cross section figs 10 and 11 are the coordinate measurements of the upper and lower fracture surfaces of sample 1 fig 10 shows the x y cross section at z 8 3 mm where the difference between the y coordinates of the upper and the lower fracture surfaces is the fracture aperture in fig 10 fracture aperture does not vary much but in fig 11 the y z section at x 20 3 mm shows that fracture aperture decreases along the z axis i e from the borehole wall to the sample boundary figs 12 and 13 are the measurement results of sample 2 fig 12 shows the x y section at z 18 1 mm and fig 13 shows the y z section at x 15 5 mm in fig 12 with the value increased along the x axis the fracture develops close toward the free surface of sample 2 and fracture aperture tends to increase towards the free surface furthermore fig 13 is similar to fig 11 fracture aperture decreases toward the terminal of fracture all these similar characteristics can also be observed in other cross sections compared with figs 10 and 12 the data in figs 11 and 13 fluctuate less due to fewer measurement points 3 2 fracture form analysis 3 2 1 fracture aperture as shown in fig 6 the triangles constituting the fracture surface were projected in x z plane and the macroscopic distribution of fracture aperture defined in fig 7 a was studied the fracture aperture of sample 1 is divided into four grades and its distribution is shown in fig 14 when the fracture is gradually away from the borehole the aperture decreases in a banded manner although there is a slight inhomogeneity the grade boundary tends to be almost parallel to the borehole axis x axis sample 2 was studied by the same way and the result is shown in fig 15 the fracture aperture of sample 2 is larger than that of sample 1 as a whole so the aperture classification grade ranges of sample 2 are bigger however the grade boundary of the aperture variation forms an angle of about 40 with the borehole axis the positive direction of x axis and the aperture decreases in the direction of about 130 with respect to the borehole axis in a banded manner this difference should be due to the influence of the borehole bottom the borehole did not penetrate the whole sample the fracture aperture distributions of two kinds of fracture with different boundary conditions were measured and the results were similar macroscopically fracture aperture was larger when the it was near the borehole the commencing position of fracture and tended smaller when developed forward however there was a difference in the distributions form of fracture aperture between the two mentioned above which was related to the boundary conditions of the moment that fractures came to be it will be discussed in section 4 3 2 2 fracture opening angle the fracture opening angle defined in fig 7 b represents the included angle formed by the upper and lower surfaces of fracture only the value of angle was studied and its positive and negative values were not considered figs 16 and 17 show the fracture opening angles which were divided into four grades and projected in x z plane corresponding to samples 1 and 2 respectively it is obviously that the opening angles of most fractures were less than 5 accounting for about 95 of the total number there were also fracture opening angles of 5 or larger especially those with 10 or larger angles were relatively concentrated near the borehole wall i e the location where fractures commenced with the fractures were farther away from the borehole the small opening angles of fractures became more and more dominant and near the end of the fracture more opening angles less than 1 could be observed here only the z direction positive direction was taken as the end of fracture the position near the borehole wall was where fracture suddenly occurred so many fractures opening angles is 10 or more more than 13 in sample 1 12 of 94 measured data and more than 57 in sample 2 47 of 84 measured data the quantity of smaller fracture opening angles increased gradually with the location away from the borehole wall in the middle part of fracture most fractures opening angles were less than 5 or smaller more than 97 in sample 1 92 of 94 measured data more than 88 in sample 2 74 of 84 measured data close to the end of fracture almost all fractures opening angles were less than 1 100 in sample 1 40 of 40 measured data more than 93 in sample 2 31 of 33 measured data when the opening angle was small the upper and lower fracture surfaces spread out just like one surface so it could be considered that the forms of the upper and lower fracture surfaces were roughly the same so when the fracture developed to a certain stage it was possible to evaluate the surface form by any one surface the upper one or the lower one of fracture as for the large opening angle of fracture scattered far away from the borehole wall it should be caused by the local intensity developing of fracture and the heterogeneity of rock 3 2 3 fracture closing direction according to the definition of fracture closing direction in fig 7 c the closing direction vectors were obtained in the corresponding upper and lower surfaces of fractures in each triangle constituting the fracture surface the closed direction vectors of upper and lower fracture surfaces were studied the results showed that the upper and lower fracture surfaces were almost similar in the small closing angles part of fracture this was also consistent with the findings above that when the fracture opening angle was small the form of the upper and lower fracture surfaces were roughly the same therefore only the closing direction vectors on the upper fracture surfaces were analyzed the closing direction vectors of fracture were projected in x z plane figs 18 and 19 are the projection results of samples 1 and 2 respectively the unit vectors obtained were showed within triangles by the arrow symbol although two dimensional diagrams they were the visualization of closing directions was still realized in figs 18 and 19 from the beginning to the end of the vector arrow the corresponding fracture aperture decreased gradually i e the fracture was closing in addition the circles symbol in figs 18 and 19 indicates that the closing direction vector could not be defined because the upper and lower fracture planes were parallel as shown in figs 18 and 19 the vectors repelled and collided repeatedly in x direction although most vectors were oriented towards the depth of borehole in somewhere a few of them were oriented towards the free surface however the distribution of closing direction vectors was very complex and its characteristics could not be clearly defined just from figs 18 and 19 for this reason the symbols positive or negative of x y and z components were investigated and the directions in which they were dominant as a whole were discussed fig 20 shows the proportion of each vector component of sample 1 and 2 in the positive and negative directions respectively by pie charts it shows clearly that the directions of the vector components in both samples were fairly consistent in y direction the positive and negative proportions of the vector directions were approximately the same which could be verified by the height difference shown in figs 8 and 9 although majority vectors in z direction were directed from the borehole wall towards the depth about one third of all vectors were oriented towards the borehole wall in x direction it was not limited to any side at all its proportion was almost 50 each which showed the complexity of closing direction in local area the fracture 4 discussion in this experiment two tensile fractures with different boundary conditions were prepared and the aperture opening angle and closing direction of fractures were measured based on the measurement results the fracture closing direction and aperture were analyzed based on the analysis results of the closing direction vectors shown in figs 18 and 19 the developing direction of fracture was discussed firstly as described in section 3 1 the distance between the upper and lower fracture surfaces decreased gradually from the borehole wall where fracture commenced to its tail this indicated that the fracture aperture decreased with the developing of fracture in other words the direction in which the fracture aperture decreased was consistent with the direction in which the fracture developed the closing direction vector shown in fig 7 c represented the direction towards which the upper and lower fracture surfaces were closing as described earlier in this direction the aperture between the upper and lower fracture surfaces became smaller therefore it was assumed that the relationship between the fracture aperture and the direction of fracture developing was locally valid and the developing direction of fracture could be inferred from the closing direction vector according to these preconditions the developing direction of fracture was studied also it was assumed that the closing direction vectors shown in figs 18 and 19 had the same direction everywhere in each triangle furthermore assuming that the beginning of vector was starting point and the conflicting position of arrows was finishing point the trajectory tracked in the direction shown by arrows was taken as the developing direction of fracture the results are shown in figs 21 and 22 they correspond to sample 1 and sample 2 respectively from figs 21 and 22 it can be seen that the fractures were generated near the borehole and extended in a straight line or radial shape similar to the extended dendritic structure and developed into the deep eventually forming a planar fracture surface on the other hand the developing direction of fracture in sample 1 was shown in fig 21 showing a tendency to be basically vertical to the borehole axis and directed towards the deep the developing direction of fracture in sample 2 is shown in fig 22 and it was approximately 120 to the borehole axis and stabbed into the inclined deep part the fracture developing pattern near the borehole wall was mainly rectilinear with small interval as the fracture went deeper the interval became bigger and the pattern turned into radioactive mainly the arrow representing the developing direction of vectors shown in figs 21 and 22 could be divided into two categories in the rectilinear pattern the repeated repelling and intersecting periods of the arrows were short in the radioactive pattern the periods became longer this phenomenon indicated that the increasing and decreasing of fracture opening angle was directly related to the concave and convexity of fracture surface the fracture fractal dimension of sample 1 whose fracture developing direction was almost perpendicular to the borehole axis was calculated according to the variogram method huang et al 1992 based on the position coordinates measured on the upper and lower fracture surface contour lines fig 23 shows the results that the fracture fractal dimension is from 1 17 to 1 68 although the value of fractal dimension fluctuated from the borehole wall to the end of the fracture it still showed a general trend of decreasing gradually this showed that the complexity of the surface roughness of fracture was gradually decreasing which was confirmed by the change from rectilinear pattern to radioactive pattern the dendritic structure observed in figs 21 and 22 was called a typical fractal graph which was inferred to be closely related to the fractal properties of concave and convexity fracture surfaces fig 14 shows that the fracture aperture distribution of sample 1 was parallel to the borehole axis and decreased gradually towards the fracture tail fig 15 shows the fracture aperture distribution of sample 2 the angle between the distribution and the borehole axis was about 130 that is the fracture aperture decreases towards the inclined deep part the above two results were consistent with the overall fracture developing direction shown in figs 21 and 22 respectively the differences of fracture aperture distribution and developing direction were related to the difference of boundary conditions between the two samples therefore the relationship between the fracture and the tensile stress around the borehole was studied the stress distributions of sample 1 and sample 2 were simulated by the finite element method the young s modulus of rock was set to 1 gpa and poisson s ratio was 0 2 the model was axisymmetric and the upper and lower boundaries of the model were free boundaries the left side of the model was constrained by horizontal displacement and the left side of the model was constrained by uniform compression stress generated by simulated static expansion agent the value of which was rock tensile strength 15 mpa and the direction of force was horizontal right the simulation results are shown in fig 24 the stress distribution showed the magnitude of the tensile stress σθ in the tangential direction of the borehole wall and the abscissa was expressed by the ratio of tensile stress σθ to the tensile strength p of the rock the distributions of tensile stress were different because of the different boundary conditions although these simulations of stress distribution were only qualitative analysis they were in good agreement with the fracture aperture distributions characteristics in figs 14 and 15 this showed that by investigating the distribution of tension stress before the fracture occurred the direction of fracture development could be predicted 5 conclusion and prospect in this research granite rock samples were studied after creating artificial tensile fractures without separating the two fracture surfaces and changing their original structure the fractures were dissected photographed and measured and its surface form was studied meanwhile the relationship between the form of fractures and the trend of fracture development and propagation and the relationship between the occurrence and form of tension fractures and the original internal stress in the rock were also studied 1 the method proposed in current research realized the visualization of the form of fracture surface without disturbing the initial state of fracture the form of the fracture surface could be clearly observed and measured and then the internal shape of the fracture could be observed in detail 2 the measurement results showed that the upper and lower surfaces of fracture were very similar and expanded like one surface the fracture aperture and fracture opening angle gradually decreased from the borehole wall where fractures occurred to the tail of fracture and their decreasing trend were banded the overall development of fracture was similar to the dendritic structure and the structure was related to the fractal nature of rough fracture surface the roughness of fracture surface also decreased gradually from the place where fracture occurred to the end of fracture 3 the form of artificial fracture was influenced by the depth of borehole and the free surface of original granite rock there was a good correspondence between the direction of fracture development and the distribution of tension stress in the sample before the occurrence of fracture the above observation and research results of the form of rock fracture surface would be helpful to the study of water seepage flow in fracture after the permeability test of the fracture the study of its seepage characteristics and fracture surface form could be combined credit authorship contribution statement qing zhang writing original draft validation visualization writing review editing shaohe luo conceptualization methodology software writing original draft haichun ma writing review editing formal analysis lei ma investigation writing review editing jiazhong qian funding acquisition resources supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by national natural science foundation of china grant no 41831289 grant no 41772250 grant no 41877191 grant no 51778215 and authors would thank professors tatsuhiko goto and jun ichi kodama and the laboratory of muroran institute of technology the author would also thank editor assistant editor and anonymous reviewers for their comments and constructive suggestion 
5663,the surface form of tension fracture in rock mass influences the seepage of fracture water greatly how to observe and quantify the fracture surface without destroying the original fracture form without separating the two surfaces of the fracture is of great significance to research the water seepage in fracture in this study tension fractures were made and developed by filling static expansion agent into drilling holes in granite blocks under the condition of keeping the original state of fracture the sample profiles including the fracture profiles were photographed and measured and the fracture aperture opening angle and closing direction along the fracture were measured accordingly the photos showed that the fracture aperture opening angle and roughness decreased gradually from the borehole wall where fractures occur to the end of the fracture the measurement results showed that the upper and lower surfaces of the fractures were very similar just like one surface fracture developing pattern was similar to branch extension pattern in structure the macroscopical trend of the fracture was closely related to the distribution of tensile stress in the specimen before the fracture occurred keywords tension fracture fracture surface form fracture aperture distribution fracture opening angle fractal dimension 1 introduction rock mass is rock which was divided into several parts by joints fractures and other structural planes the occurrence and propagation of fractures affect and change the mechanical properties of rock cook 1992 fractures are meanwhile main space for the storage and movement of groundwater in rock mass which are related to many research fields of groundwater including flow regime chen et al 2015a qian et al 2015 wu et al 2011 permeability pan et al 2010 ren et al 2015 wang and cardenas 2016 reynolds number qian et al 2016 roughness of fracture surface brown 1987 dou et al 2018 solute transport zhao et al 2016 zhao et al 2018 etc fractures are also closely related to many engineering practices such as water supply wen et al 2013 mining berkowitz 2002 geothermal development zhang et al 2020 geotechnical engineering chen et al 2008 molinero et al 2002 etc the influences of fractures on the movement of groundwater are mainly related to the characteristics of fractures themselves the fracture characteristics usually can be described by shape size and surface form including the fracture aperture the fracture opening angle the direction of fracture development and extension fracture tortuosity and the roughness of fracture surface all of them have influences on the movement of fracture water such as flow quantity flow velocity and flow pattern because it is formed by tensile force the surface of tension fracture is rougher than that of shear fracture tension fracture is widely distributed in nature which is of great significance for groundwater storage water supply and water movement the roughness of fracture surface is an important factor affecting the fracture water movement but a quantitative description of the rough surface form has always been a chronic difficulty in the study of fracture water barton and choubey 1977 lee et al 1990 lomize 1951 zhou and xie 1999 traditionally tension fracture was usually simplified as a pair of smooth parallel plates however the surface of real fracture wall is rough and uneven which is one of the most basic parameters of fracture and has a significant influence on the flow pattern of fracture water chen et al 2017 zhang et al 2019 many studies focused on the shape and size of fracture itself i e precise measurement of the surface morphology of fracture including concave and convexity points and roughness and have made considerable progress in order to describe the roughness of fracture surface some scholars applied fractal theory brandt and prokopski 1993 schmittbuhl et al 1993 sugimoto et al 1991 xie 1995 fracture aperture is another basic parameter describing the geometric characteristics of fractures which has a decisive influence on the water movement capacity of fractures meheust and schmittbuhl 2001 qian et al 2007 renshaw 1995 withpoon et al 1980 besides the flow pattern flow velocity and flow quantity are also important parameters of fracture water which involves the utility of formulas to characterize flow movement of fracture water and numerical simulation brown et al 1995 brush and thomson 2003 chaudhary et al 2011 chen et al 2015b ge 1997 konzuk and kueper 2004 mallikamas and harihar rajaram 2010 nicholl 2001 qian et al 2015 wang et al 2015 however there are some obvious shortcomings in previous studies the above topographic mapping techniques for measuring fracture surface depended on not only the precision of measuring instrument but also the separation of fracture in other words surfaces of unseparated fracture could not be measured and studied lee et al 1990 zhou and xie 1999 therefore many researches on fracture roughness were based on numerical simulation but it was still unable to reappear the true fracture roughness chen et al 2017 zhang et al 2019 once the two surfaces were separated to measure the original fracture state had been disturbed and the distance between the two fracture surfaces could not be measured which meant that the fracture aperture data became meaningless moreover a whole fracture can be divided into starting part and ending part and the fracture surface appears different morphology and geometric characteristics in different positions the occurrence and development of fractures are often related to initial fissures and initial internal forces in rocks so far few researches put sight on the relationship between the developing and propagating trend of the fracture and the initial fissures and initial internal forces the objective of this study is to describe and quantify the surface form of fractures kept the original state condition the artificial tension fractures in granite samples are studied the main work includes i the fracture surfaces are measured and studied without separating the upper and lower surfaces of the fracture throughout the whole measurement process the two wall surfaces of fractures are not separated and their original state is maintained including the wall distance fracture aperture and the original opening angle this method seems primitive but it is indeed an effective method to obtain the actual form of the fracture surface many cross sectional photographs are taken from the position where the fracture occurs to the end of the fracture then the position coordinates of the upper and lower fracture surfaces are measured on the basis of photos and the form of the fracture surfaces is evaluated distribution of fracture aperture opening angle development direction and fractal characteristics of fracture surface are also studied on this basis ii the developing and propagating trend of fractures are studied from the internal surface form of fractures and iii the relationship between the formation and form of tension fractures and the original internal stress in the rock is studied 2 material and methodology 2 1 samples preparation the samples used in this experiment were two different sizes of inada granite from japan as shown on the left side of fig 1 the size of granite block a was about 122 116 mm in cross section and the thickness was about 67 mm a breakthrough hole with a diameter of 10 mm was drilled in the center of the cross section and as shown on the left side of fig 2 the cross section of granite block b was 105 60 mm and its thickness was 40 mm a hole with diameter of 10 mm and depth of 20 mm was drilled in the center of the cross section and the hole penetrated only half of the block b instead of penetrating the rock the static expansion agent was loaded into the two drilling holes to create tensile fractures controlling the development of fractures making them occur but not penetrate the edge of the rock blocks and keeping the rock blocks in state of fractures but not completely separated that was the tips tails of the fractures were enclosed within the block bodies and did not reach the boundaries of the rock blocks thus the whole fractures could be measured from the site where they commenced to the site where they ended on the left side of figs 1 and 2 fracture profiles made of granite blocks a and b are shown the artificial fractures were roughly parallel to the bottom surfaces of the rock blocks and extended from the borehole wall to the deep some resin viscosity 5 10cp mixed with red dye was filled into the fractures the resin was made up of liquid epoxy resin and liquid curing agent when it was just mixed it had strong fluidity weak viscosity and could completely fill the fractures after chemical reaction the viscosity of the resin increased and the fluidity decreased gradually finally the resin solidified into a solid usually 24 h or longer and no heat released and firmly adhered to the fracture surface in this experiment the early fluidity and late viscosity of the resin were used the filling resin has three purposes 1 fix the two walls of fracture with the viscosity of resin to prevent the rock from expanding or contracting and prevent the rock from deforming under the force during the cutting and grinding process so as to change the fracture shape and affect the measurement accuracy 2 prevent the rock powder from entering the fracture during the grinding process and affect the photographic clarity 3 the resin is red and there is clear boundary between the resin and the rock after filling which can make it easy to distinguish the boundary when measuring and mapping after photographing after the resin had completely solidified and had strength the granite blocks were cut into slightly smaller size samples with fractures as shown on the right side of figs 1 and 2 for further grinding and measurement as shown in the figures sample 1 was cut from block a and sample 2 was cut from block b in addition it was well known that there were potential rifts in granite rock the fracture surface in sample 1 was parallel to the intrinsic rift surface however due to the missed opportunity to observe the rift the original situation of the rift in sample 2 was unknown 2 2 measurement methods of fracture surface form in samples 1 and 2 the x y z coordinate systems shown in the right side of figs 1 and 2 were established respectively fig 3 is a schematic diagram of experimental apparatus and observation procedure firstly the cross section x y plane containing the fracture was observed with a digital microscope at a magnification of 30 times and some photos of the cross section were taken then these photos were imported into the computer and smd 200 graphic analysis software was employed to measure and record the position coordinates of fracture fig 4 shows the grinding process after the x y plane had been photographed measured and recorded grind the x y plane of the sample along the z direction to the x y plane with a grinding thickness of about 2 mm the purple part in fig 4 b and repeat the photographing measuring and recording process shown in fig 3 for the new x y section continue to repeat the above grinding process grind the section along z direction to the x y plane purple part in fig 4 c is the part that continues to be ground off take photos measure and so on until the position coordinate data from the fracture generation position to the fracture pinch out position were obtained in sample 1 a total of 24 cross sections were measured in sample 2 21 cross sections were measured the measurement interval in each cross section in x direction was set about 0 4 mm according to the benchmark that measurement interval should be less than 1 30 full length of the measuring object regardless of the size and shape of the particles composing granite meanwhile the grinding thickness in z direction was set about 2 mm considering the manual transmission property of the surface grinder and the amount of data the measurement accuracy was controlled within the range of micron because a micrometer ruler was used to measure the grinding thickness after taking photos and using graphic analysis software to determine the position coordinates of measured points in the fracture profile line on each grinding plane in fact every fracture has two surfaces so each grinding plane has two fracture contour lines as shown in fig 5 here for the convenience of expression take one of them as an example these coordinates were input into mapping software to connect to form a 2d fracture outline and then these outlines from different sections grinding planes were connected into a surface by mapping software thus a complete 3d fracture surface was finally built figs 8 and 9 fig 5 shows an example of a fracture photograph taken with a digital microscope showing that the red resin was completely filled into the fracture and that the fracture surface boundary could be clearly distinguished the fracture had two surfaces the surface with larger value of y coordinate was named the upper fracture surface and that with lower value was named the lower fracture surface 2 3 analysis methods of fracture surface form the upper and lower fracture surfaces had complex surface forms based on the position coordinate data we studied several dominating characteristic quantities of the fracture surfaces as shown in fig 6 it was assumed that the upper and lower fracture surfaces were composed of paired small planar triangles bottom length 0 8 mm and height 2 mm therefore the normal vector surface area fracture aperture and fracture closing direction of the upper and lower fracture surfaces could be defined as shown in the projection plane in fig 6 the coordinates value of two vertexes were the same because two adjacent triangles shared one edge then the fracture aperture fracture opening angle and fracture closing direction were studied particularly firstly as shown in fig 7 a the bisection plane was obtained from two planes including the upper and lower fracture surfaces and the sum of vertical lengths h1 h2 from the centers of gravity of the upper and lower fracture surface planes to the bisection plane was defined as the fracture aperture secondly the included angle θ formed by the normal vectors of the upper and lower fracture surfaces shown in fig 7 b was defined as the fracture opening angle finally as shown in fig 7 c the intersection line of the two planes of the upper and lower fracture surface planes was obtained and the unit vectors from the gravity center of the upper and lower planes to the vertical direction of the intersection line were defined as the closing direction vectors of the upper and lower fracture planes respectively the closing direction of the fracture was expressed by the closing direction vectors 3 measurement results and analysis 3 1 measurement of position coordinate using the method shown in fig 3 to measure the position coordinates of fracture the visualization of the spatial morphology of the whole fracture could be realized figs 8 and 9 are three dimensional schematic diagram of the lower fracture surfaces of samples 1 and 2 respectively and the shape of the upper fracture surface are almost the same the fracture surfaces in figs 8 and 9 totally appeared complex roughness the height difference of sample 2 is slightly larger than that of sample 1 but the maximum difference between them is no more than 5 mm in addition although the differences between high and low areas can be clearly distinguished there is little evidence of the relationship between the differences and the original borehole location in order to reveal the correlation between the upper and lower fracture surfaces more attention should be paid to the measurement data of each cross section figs 10 and 11 are the coordinate measurements of the upper and lower fracture surfaces of sample 1 fig 10 shows the x y cross section at z 8 3 mm where the difference between the y coordinates of the upper and the lower fracture surfaces is the fracture aperture in fig 10 fracture aperture does not vary much but in fig 11 the y z section at x 20 3 mm shows that fracture aperture decreases along the z axis i e from the borehole wall to the sample boundary figs 12 and 13 are the measurement results of sample 2 fig 12 shows the x y section at z 18 1 mm and fig 13 shows the y z section at x 15 5 mm in fig 12 with the value increased along the x axis the fracture develops close toward the free surface of sample 2 and fracture aperture tends to increase towards the free surface furthermore fig 13 is similar to fig 11 fracture aperture decreases toward the terminal of fracture all these similar characteristics can also be observed in other cross sections compared with figs 10 and 12 the data in figs 11 and 13 fluctuate less due to fewer measurement points 3 2 fracture form analysis 3 2 1 fracture aperture as shown in fig 6 the triangles constituting the fracture surface were projected in x z plane and the macroscopic distribution of fracture aperture defined in fig 7 a was studied the fracture aperture of sample 1 is divided into four grades and its distribution is shown in fig 14 when the fracture is gradually away from the borehole the aperture decreases in a banded manner although there is a slight inhomogeneity the grade boundary tends to be almost parallel to the borehole axis x axis sample 2 was studied by the same way and the result is shown in fig 15 the fracture aperture of sample 2 is larger than that of sample 1 as a whole so the aperture classification grade ranges of sample 2 are bigger however the grade boundary of the aperture variation forms an angle of about 40 with the borehole axis the positive direction of x axis and the aperture decreases in the direction of about 130 with respect to the borehole axis in a banded manner this difference should be due to the influence of the borehole bottom the borehole did not penetrate the whole sample the fracture aperture distributions of two kinds of fracture with different boundary conditions were measured and the results were similar macroscopically fracture aperture was larger when the it was near the borehole the commencing position of fracture and tended smaller when developed forward however there was a difference in the distributions form of fracture aperture between the two mentioned above which was related to the boundary conditions of the moment that fractures came to be it will be discussed in section 4 3 2 2 fracture opening angle the fracture opening angle defined in fig 7 b represents the included angle formed by the upper and lower surfaces of fracture only the value of angle was studied and its positive and negative values were not considered figs 16 and 17 show the fracture opening angles which were divided into four grades and projected in x z plane corresponding to samples 1 and 2 respectively it is obviously that the opening angles of most fractures were less than 5 accounting for about 95 of the total number there were also fracture opening angles of 5 or larger especially those with 10 or larger angles were relatively concentrated near the borehole wall i e the location where fractures commenced with the fractures were farther away from the borehole the small opening angles of fractures became more and more dominant and near the end of the fracture more opening angles less than 1 could be observed here only the z direction positive direction was taken as the end of fracture the position near the borehole wall was where fracture suddenly occurred so many fractures opening angles is 10 or more more than 13 in sample 1 12 of 94 measured data and more than 57 in sample 2 47 of 84 measured data the quantity of smaller fracture opening angles increased gradually with the location away from the borehole wall in the middle part of fracture most fractures opening angles were less than 5 or smaller more than 97 in sample 1 92 of 94 measured data more than 88 in sample 2 74 of 84 measured data close to the end of fracture almost all fractures opening angles were less than 1 100 in sample 1 40 of 40 measured data more than 93 in sample 2 31 of 33 measured data when the opening angle was small the upper and lower fracture surfaces spread out just like one surface so it could be considered that the forms of the upper and lower fracture surfaces were roughly the same so when the fracture developed to a certain stage it was possible to evaluate the surface form by any one surface the upper one or the lower one of fracture as for the large opening angle of fracture scattered far away from the borehole wall it should be caused by the local intensity developing of fracture and the heterogeneity of rock 3 2 3 fracture closing direction according to the definition of fracture closing direction in fig 7 c the closing direction vectors were obtained in the corresponding upper and lower surfaces of fractures in each triangle constituting the fracture surface the closed direction vectors of upper and lower fracture surfaces were studied the results showed that the upper and lower fracture surfaces were almost similar in the small closing angles part of fracture this was also consistent with the findings above that when the fracture opening angle was small the form of the upper and lower fracture surfaces were roughly the same therefore only the closing direction vectors on the upper fracture surfaces were analyzed the closing direction vectors of fracture were projected in x z plane figs 18 and 19 are the projection results of samples 1 and 2 respectively the unit vectors obtained were showed within triangles by the arrow symbol although two dimensional diagrams they were the visualization of closing directions was still realized in figs 18 and 19 from the beginning to the end of the vector arrow the corresponding fracture aperture decreased gradually i e the fracture was closing in addition the circles symbol in figs 18 and 19 indicates that the closing direction vector could not be defined because the upper and lower fracture planes were parallel as shown in figs 18 and 19 the vectors repelled and collided repeatedly in x direction although most vectors were oriented towards the depth of borehole in somewhere a few of them were oriented towards the free surface however the distribution of closing direction vectors was very complex and its characteristics could not be clearly defined just from figs 18 and 19 for this reason the symbols positive or negative of x y and z components were investigated and the directions in which they were dominant as a whole were discussed fig 20 shows the proportion of each vector component of sample 1 and 2 in the positive and negative directions respectively by pie charts it shows clearly that the directions of the vector components in both samples were fairly consistent in y direction the positive and negative proportions of the vector directions were approximately the same which could be verified by the height difference shown in figs 8 and 9 although majority vectors in z direction were directed from the borehole wall towards the depth about one third of all vectors were oriented towards the borehole wall in x direction it was not limited to any side at all its proportion was almost 50 each which showed the complexity of closing direction in local area the fracture 4 discussion in this experiment two tensile fractures with different boundary conditions were prepared and the aperture opening angle and closing direction of fractures were measured based on the measurement results the fracture closing direction and aperture were analyzed based on the analysis results of the closing direction vectors shown in figs 18 and 19 the developing direction of fracture was discussed firstly as described in section 3 1 the distance between the upper and lower fracture surfaces decreased gradually from the borehole wall where fracture commenced to its tail this indicated that the fracture aperture decreased with the developing of fracture in other words the direction in which the fracture aperture decreased was consistent with the direction in which the fracture developed the closing direction vector shown in fig 7 c represented the direction towards which the upper and lower fracture surfaces were closing as described earlier in this direction the aperture between the upper and lower fracture surfaces became smaller therefore it was assumed that the relationship between the fracture aperture and the direction of fracture developing was locally valid and the developing direction of fracture could be inferred from the closing direction vector according to these preconditions the developing direction of fracture was studied also it was assumed that the closing direction vectors shown in figs 18 and 19 had the same direction everywhere in each triangle furthermore assuming that the beginning of vector was starting point and the conflicting position of arrows was finishing point the trajectory tracked in the direction shown by arrows was taken as the developing direction of fracture the results are shown in figs 21 and 22 they correspond to sample 1 and sample 2 respectively from figs 21 and 22 it can be seen that the fractures were generated near the borehole and extended in a straight line or radial shape similar to the extended dendritic structure and developed into the deep eventually forming a planar fracture surface on the other hand the developing direction of fracture in sample 1 was shown in fig 21 showing a tendency to be basically vertical to the borehole axis and directed towards the deep the developing direction of fracture in sample 2 is shown in fig 22 and it was approximately 120 to the borehole axis and stabbed into the inclined deep part the fracture developing pattern near the borehole wall was mainly rectilinear with small interval as the fracture went deeper the interval became bigger and the pattern turned into radioactive mainly the arrow representing the developing direction of vectors shown in figs 21 and 22 could be divided into two categories in the rectilinear pattern the repeated repelling and intersecting periods of the arrows were short in the radioactive pattern the periods became longer this phenomenon indicated that the increasing and decreasing of fracture opening angle was directly related to the concave and convexity of fracture surface the fracture fractal dimension of sample 1 whose fracture developing direction was almost perpendicular to the borehole axis was calculated according to the variogram method huang et al 1992 based on the position coordinates measured on the upper and lower fracture surface contour lines fig 23 shows the results that the fracture fractal dimension is from 1 17 to 1 68 although the value of fractal dimension fluctuated from the borehole wall to the end of the fracture it still showed a general trend of decreasing gradually this showed that the complexity of the surface roughness of fracture was gradually decreasing which was confirmed by the change from rectilinear pattern to radioactive pattern the dendritic structure observed in figs 21 and 22 was called a typical fractal graph which was inferred to be closely related to the fractal properties of concave and convexity fracture surfaces fig 14 shows that the fracture aperture distribution of sample 1 was parallel to the borehole axis and decreased gradually towards the fracture tail fig 15 shows the fracture aperture distribution of sample 2 the angle between the distribution and the borehole axis was about 130 that is the fracture aperture decreases towards the inclined deep part the above two results were consistent with the overall fracture developing direction shown in figs 21 and 22 respectively the differences of fracture aperture distribution and developing direction were related to the difference of boundary conditions between the two samples therefore the relationship between the fracture and the tensile stress around the borehole was studied the stress distributions of sample 1 and sample 2 were simulated by the finite element method the young s modulus of rock was set to 1 gpa and poisson s ratio was 0 2 the model was axisymmetric and the upper and lower boundaries of the model were free boundaries the left side of the model was constrained by horizontal displacement and the left side of the model was constrained by uniform compression stress generated by simulated static expansion agent the value of which was rock tensile strength 15 mpa and the direction of force was horizontal right the simulation results are shown in fig 24 the stress distribution showed the magnitude of the tensile stress σθ in the tangential direction of the borehole wall and the abscissa was expressed by the ratio of tensile stress σθ to the tensile strength p of the rock the distributions of tensile stress were different because of the different boundary conditions although these simulations of stress distribution were only qualitative analysis they were in good agreement with the fracture aperture distributions characteristics in figs 14 and 15 this showed that by investigating the distribution of tension stress before the fracture occurred the direction of fracture development could be predicted 5 conclusion and prospect in this research granite rock samples were studied after creating artificial tensile fractures without separating the two fracture surfaces and changing their original structure the fractures were dissected photographed and measured and its surface form was studied meanwhile the relationship between the form of fractures and the trend of fracture development and propagation and the relationship between the occurrence and form of tension fractures and the original internal stress in the rock were also studied 1 the method proposed in current research realized the visualization of the form of fracture surface without disturbing the initial state of fracture the form of the fracture surface could be clearly observed and measured and then the internal shape of the fracture could be observed in detail 2 the measurement results showed that the upper and lower surfaces of fracture were very similar and expanded like one surface the fracture aperture and fracture opening angle gradually decreased from the borehole wall where fractures occurred to the tail of fracture and their decreasing trend were banded the overall development of fracture was similar to the dendritic structure and the structure was related to the fractal nature of rough fracture surface the roughness of fracture surface also decreased gradually from the place where fracture occurred to the end of fracture 3 the form of artificial fracture was influenced by the depth of borehole and the free surface of original granite rock there was a good correspondence between the direction of fracture development and the distribution of tension stress in the sample before the occurrence of fracture the above observation and research results of the form of rock fracture surface would be helpful to the study of water seepage flow in fracture after the permeability test of the fracture the study of its seepage characteristics and fracture surface form could be combined credit authorship contribution statement qing zhang writing original draft validation visualization writing review editing shaohe luo conceptualization methodology software writing original draft haichun ma writing review editing formal analysis lei ma investigation writing review editing jiazhong qian funding acquisition resources supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by national natural science foundation of china grant no 41831289 grant no 41772250 grant no 41877191 grant no 51778215 and authors would thank professors tatsuhiko goto and jun ichi kodama and the laboratory of muroran institute of technology the author would also thank editor assistant editor and anonymous reviewers for their comments and constructive suggestion 
5664,passive microwave radiometry from space through missions such as the soil moisture and ocean salinity smos and soil moisture active passive smap satellites is the most reliable means for mapping global surface soil moisture ssm nonetheless microwave ssm retrievals are uncertain over densely vegetated surfaces or areas with high radio frequency interference this paper presents a new observationally driven approach to remote sensing of global ssm based on the terrestrial water storage anomaly twsa data acquired from the gravity recovery and climate experiment grace satellite this approach rests on a physically based yet parsimonious model based on the richards equation and the assumption that the twsa temporal rate of change ds dt approximates the land surface net water flux nwf as the surface boundary condition the grace based ssm is found to be in a reasonable agreement with in situ data and highly correlated with the smap and smos retrievals especially over wet regions where the assumption of nwf ds dt holds valid the grace retrievals contain new ssm information relative to the microwave satellite data and provide a potential solution to improve the microwave data over densely vegetated surfaces or areas with high radio frequency interference keywords grace smap smos terrestrial water storage soil moisture richards equation 1 introduction soil moisture makes up a very small fraction of the global freshwater budget however its importance far outweighs its physical amount in that it links water energy and carbon cycles that govern the climate and metabolism of the earth system across the soil vegetation atmosphere continuum entekhabi et al 2010 from an ecological point of view the pools of soil moisture are fundamental ecosystem resources providing water for plant transpiration and hence soil moisture is one of the major controls of ecosystem structure functioning and diversity especially in drylands robinson et al 2008 soil moisture also regulates the vegetation water content which is one of the most important elements in wildfire risk assessment maki et al 2004 a comprehensive review on the role of soil moisture in numerous earth and environmental science applications and its impacts on the global environment and human society can be found for example in robinson et al 2008 and babaeian et al 2019 today remote sensing provides an exceedingly powerful means for large scale mapping of surface 0 5 cm soil moisture ssm using a variety of electromagnetic sensors as ssm variation alters the land surface optical reflectance lobell and asner 2002 sadeghi et al 2015 temperature hulley et al 2010 aminzadeh and or 2013 and microwave emission njoku and kong 1977 tsang et al 1985 optical and thermal remote sensing have shown great potential for mapping ssm at high spatial resolutions e g carlson et al 1994 moran et al 1994 fensholt and sandholt 2003 sadeghi et al 2017a however owing to the microwave penetration through clouds and vegetation canopy microwave remote sensing is the most reliable approach to global monitoring of ssm this has motivated several satellite missions in the past two decades such as the soil moisture and ocean salinity smos kerr et al 2001 the advanced microwave scanning radiometer amsr e njoku et al 2003 the advanced scatterometer ascat wagner et al 2013 and the soil moisture active passive smap entekhabi et al 2010 the upwelling land surface microwave emission varies significantly as a function of ssm especially when vegetation water content is below 5 kg m 2 accordingly ssm can be retrieved via inversion of a forward radiative transfer model that links the soil microwave reflectivity to the observed brightness temperature njoku and kong 1977 tsang et al 1985 salient advances have been made in the microwave soil moisture retrieval theories and algorithms e g paloscia et al 2006 mattia et al 2006 wigneron et al 2007 2017 naeimi et al 2009 liu et al 2011 kurum et al 2010 2012 konings et al 2016 tabatabaeenejad et al 2014 2017 sadeghi et al 2017b schwank et al 2018 ebtehaj and bras 2019 gao et al 2020 yet ssm retrievals remain uncertain over densely vegetated surfaces areas with high radio frequency interference rfi high latitude areas when soil is frozen and deserts vittucci et al 2016 wigneron et al 2017 al yaari et al 2019 recent research has demonstrated great potential for using ssm data to estimate components of the surface water balance equation such as precipitation brocca et al 2013 2014 koster et al 2016 evapotranspiration mccoll et al 2017 purdy et al 2018 akbar et al 2018 2019 runoff koster et al 2018 and irrigation lawston et al 2017 brocca et al 2018 jalilvand et al 2019 zaussinger et al 2019 it is also indicated that ssm data by itself contain adequate information to close the subsurface water budget provided that the input ssm is accurate and robust koster et al 2018 sadeghi et al 2019a b accordingly small interannual trends in ssm are often equivalent to large shifts in the subsurface water resources which may occur due to the climate variability and or anthropogenic impacts rodell et al 2018 these recent findings have motivated this study to explore a new approach to remote sensing of global ssm to potentially add new information to those of optical thermal and microwave remote sensing the main objective of this study is to retrieve global ssm based on the observations by the gravity recovery and climate experiment grace tapley et al 2004 the grace satellite observes the earth s mass change on a near monthly timescale over land this mass change is mainly governed by the land atmosphere water exchange and hence yields the terrestrial water storage anomaly twsa the twsa data has been shown to be highly correlated with ssm data swenson et al 2008 abelen and seitz 2013 crow et al 2017 chiefly due to the physical linkage between ssm and the land surface net water flux nwf which largely determines the twsa seasonal dynamics the grace twsa has a lower spatial resolution than the microwave ssm data 3 degrees vs 40 km and less frequent temporal sampling monthly vs daily however it is not significantly affected by vegetation density rfi and the freeze thaw state of the soil water hence we hypothesize that twsa contains ssm information beyond what can be extracted from microwave data in this paper we introduce an observationally driven yet physically based approach to translate twsa to ssm this approach relies on two main assumptions i the nwf can be approximated by the grace twsa rate of change ds dt crow et al 2017 yin et al 2019 and ii the linearized richards 1931 equation can adequately represent the ssm nwf relationship at coarse spatiotemporal resolutions sadeghi et al 2019b in the next section we list the data sets used in this study in section 3 we explain our methodology to retrieve ssm from grace twsa data which relies on warrick s 1975 analytical solution to the linearized richards equation in section 4 we present and discuss the results of this study section 5 concludes and discusses some future directions of this research 2 data sets in this study we use in situ ssm 0 5 cm data from the international soil moisture network ismn dorigo et al 2011 and remote sensing observations from grace smap and smos satellites including 1 the jpl grace mascon terrestrial water storage anomaly wiese et al 2018 from may 2002 to may 2017 at 0 5 degree grid resolution but representing the 3 degree equal area caps with a near monthly temporal resolution 2 the smap l3 enhanced passive soil moisture o neill et al 2018 from april 2015 to may 2017 at 9 km grid resolution with a 2 3 day revisiting time and 3 the smos ic passive soil moisture fernandez moran et al 2017 product from may 2010 to may 2017 at 25 km grid resolution with a 2 3 day temporal resolution for consistency all products are projected onto the regular gird with 0 25 degree resolution via nearest neighbor interpolation this interpolation is used for redundant mapping of the low resolution data onto the target grid with a higher resolution to keep the information content of higher resolution data all products are also resampled to monthly temporal resolution by linear interpolation for grace and arithmetic averaging for smap and smos data 3 methodology 3 1 background richards 1931 equation captures soil moisture dynamics in space and time in response to the diffusive and gravitational water flow in unsaturated soil the soil moisture based richards equation is widely applied in land surface models decker and zeng 2009 this equation is 1 θ t d 2 θ z 2 k θ z where θ is the volumetric soil moisture content t is the time z is the soil depth positive downward d is the effective soil water diffusivity i e an average value over the whole saturation range and k is the average slope of the soil hydraulic conductivity function the relationship between unsaturated hydraulic conductivity k and volumetric soil moisture θ warrick 1975 analytically solved eq 1 for a semi infinite soil profile subject to the following boundary and initial conditions 2a d θ z k z 0 f t 2b limi t z θ z t θ 2c θ z 0 θ i z θ 1 g z where f is an arbitrary net water flux nwf at time t g is an arbitrary value determining the initial soil moisture profile θi z and θ is the limiting soil moisture content at large depths that is equivalent to the long term average of soil moisture along the soil profile the nwf can be positive infiltration gain or negative evaporative loss and is defined as any exchange of water at the land atmosphere interface z 0 which can be estimated as follows 3 f p i r e r where p is precipitation ir is irrigation e is evaporation and r is the surface runoff warrick s 1975 solution to eq 1 subject to the boundary and initial conditions in eq 2 is given as the sum of two solutions including i the solution due to the boundary condition θ bc and ii the solution due to the initial condition θ ic 4a θ z t θ bc z t θ ic z t 4b θ bc z t 0 t e 0 25 τ f τ e 0 25 z 2 t τ π t τ 0 5 e 0 25 t τ 0 5 z erfc 0 5 z t τ t τ d τ 4c θ ic z t 0 e 0 5 ζ g ζ e 0 25 z ζ 2 t e 0 25 z ζ 2 t 2 π t 0 5 e 0 25 t 0 5 z ζ erfc 0 5 z ζ t t d ζ where τ and ζ are the dummy variables of integration erfc denotes the complementary error function and t z θ and f are dimensionless representations of time t soil depth z volumetric soil moisture θ and net water flux f 5 t k 2 t d z kz d θ θ θ 1 e 0 5 z 0 25 t f f k θ k θ a closed form solution is obtained when assuming uniform initial condition θi θ and stepwise surface flux input of 6 f t f 1 0 t δ t f 2 δ t t 2 δ t f n n 1 δ t t n δ t the closed form solution for calculating soil moisture at any arbitrary depth and at the n th time step with δt time intervals is 7a θ n z e 0 5 z 0 25 t f 1 u z t n 1 e 0 5 z 0 25 t f 1 u z t i 2 n f i f i 1 u z n i 1 δ t n 1 7b u z t 0 5 e z z t 1 erfc 0 5 z t t t π e 0 5 z t t 2 0 5 erfc 0 5 z t t sadeghi et al 2019a validated the performance of an inverted form of eq 7 yielding nwf as a function of sm with ground based observations from four flux tower sites with different climatic conditions and land cover types in the u s they have shown that while warrick s solution was derived for bare soil it also works well for densely vegetated sites such as the tonzi ranch in california with 30 60 forest canopy cover baldocchi 2016 largely because of i the strong correlation between soil evaporation and plant transpiration on an annual basis and ii calibration of the model parameters based on actual data which capture the impacts of plant transpiration sadeghi et al 2019b further evaluated the inverted eq 7 on a global scale and confirmed its applicability over a large portion of the world however it should be noted that the model does not account for transpiration by deeply rooted plants when the soil surface dries out and soil evaporation and plant transpiration decouple the analytical nature of eq 7 facilitates calculation of soil moisture profile from the grace twsa in monthly time steps without any truncation error which is of a concern in numerical solutions of richards equation zeng and decker 2009 note that the nwf is equivalent to the total change of the soil moisture profile during time step δt hence running eq 7 with a coarse time step yields the net change of soil moisture due to the cumulative impacts of all positive and negative surface fluxes occurring during δt e g sub monthly infiltration and evaporation events for monthly time steps 3 2 model parameter estimation to map the grace twsa to ssm the model parameters including d k and θ need to be estimated globally this may be performed through computationally intensive backpropagation of the error by comparing the model outputs with some reference ssm observations however to keep the ssm estimates chiefly independent from ancillary data we introduce an analytical and computationally efficient approach to estimate the model parameters sadeghi et al 2019 showed that eq 7 can be closely approximated with the following simple equation for a constant net water flux f if z 0 near surface and t 0 1 equivalent to t 274 years for d 3000 cm2 month 1 and k 0 3 cm month 1 8 θ θ f k θ t d eq 8 implies that the higher the diffusivity parameter d the lower the soil moisture amplitude θ θ per a given pulse of surface flux f due to the linearization of the hydraulic conductivity function k θ the effective values of k in various soils are significantly smaller than the saturated hydraulic conductivity sadeghi et al 2019a therefore the average deep percolation kθ in the warrick model is mostly negligible compared to the nwf fluctuations on a monthly timescale with kθ negligible and f independent of the model parameters eq 8 yields the following relationship between the outputs of eq 7 for two different parameter sets θ vs θ 9 d d θ θ θ θ 2 considering θ to be the solution for an arbitrary initial estimate for the model parameters k d and θ the optimum values of d and θ can be approximated from eq 9 if having ssm observations at any two moments of the time series let us consider the wettest moment θ max and the driest moment θ min of the target ssm time series i e the strongest ssm amplitudes in response to the strongest nwf signals yielding 10 d d θ max θ θ max θ 2 θ min θ θ min θ 2 for the sake of generality we define a relative soil moisture s as follows 11 s θ θ min θ max θ min from eqs 10 and 11 we can derive 12 s s s min s max s min 13 d d s max s 1 s 2 where s max and s min are the relative ssm values at the wet and dry moments from eq 7 having θ replaced by s using an initial estimate of the model parameters s d and k and s and d denote estimates of the optimum values for these parameters finally replacing θ in eq 8 by s and assuming that the long term temporal mean of s equals to s the parameter k can be estimated from eq 8 as 14 k f mean s where f mean is the long term temporal mean of nwf from a given nwf time series e g for a given grace pixel and the parameters estimated via eqs 12 to 14 eq 7 results in a relative ssm time series bounded between 0 θ θ min and 1 θ θ max clearly this relative ssm can be converted to the absolute ssm via a linear scaling whenever θ max and θ min are known note that θ max and θ min do not have to be necessarily equal to the fully dry and saturated moisture contents respectively as s and θ are linearly related the solutions for s will be valid for any values of θ max and θ min regardless of whether they are representative of true long term extrema 3 3 model implementation we use eq 7 to map monthly scale ssm from grace twsa data following previous research crow et al 2017 yin et al 2019 we assume that the derivative of the grace twsa with respect to time approximates the land surface net water flux i e nwf ds dt this assumption and its validity rely on the fact that grace ds dt variations due to other factors e g groundwater abstraction surface water storage variations lateral groundwater flow are generally much smaller than its seasonal variations due to nwf accordingly we feed grace ds dt data into eq 7 to globally map ssm note that ds dt is considered to be equal to net water flux f in eq 5 that yields the dimensionless flux f in eq 7a to make the results compatible with the microwave retrievals the sensing depth z in eq 5 is set to 2 5 cm i e the middle of the surface layer assuming that the microwave ssm retrievals represent the moisture content in the top 0 5 cm of the soil column it should be noted that because d k 2 5 cm the normalized depth z kz d and subsequently our results are not overly sensitive to small changes in z for model calibration we pursue a two step approach in the first step we calibrate the model parameters based on eqs 12 to 14 yielding the relative ssm between 0 and 1 in the second step we rescale the relative ssm into the absolute ssm using smap data in the parameter calibration step we first run the model eq 7 with initial parameter estimates k 0 3 cm month 1 s 0 5 and d 3000 cm2 month 1 for all pixels sadeghi et al 2019a b then we update the parameters using eqs 12 to 14 using the extrema of the initial outputs of eq 7 s max and s min over the entire grace period 2002 2017 finally we run the model one more time with the updated parameters yielding the time series of s for all pixels the global retrievals of the relative ssm are finally calibrated to the absolute ssm values based on the soil moisture extrema defined by smap retrievals acquired between april 2015 and may 2017 using the following scaling relationship 15 θ grace 02 17 θ min smap 15 17 θ max smap 15 17 θ min smap 15 17 s grace 02 17 s min grace 15 17 s max grace 15 17 s min grace 15 17 where θ grace 02 17 is the grace based absolute ssm retrievals 2002 2017 s grace 02 17 is the grace based relative ssm retrievals 2002 2017 θ max smap 15 17 and θ min smap 15 17 are the maximum and minimum of the smap ssm retrievals 2015 2017 and s max grace 15 17 and s min grace 15 17 are the maximum and minimum of the grace relative ssm retrievals 2015 2017 note that we use smap for this calibration purpose rather than smos due to its improved spatial coverage and rfi mitigation algorithm piepmeier et al 2016 it is also important to note that the smap based calibration only scales the relative ssm estimates and does not change the temporal structure of the new ssm information derived from grace to determine uncertainties associated with the grace ssm estimates we conduct 100 monte carlo simulations by perturbing all model inputs via uniform zero mean random errors specifically we perturb the grace twsa by a random error varying between plus and minus one sigma uncertainty reported for the jpl grace mascon product the smap data are also perturbed by a random error of 10 the perturbed inputs are fed into the model to quantify the uncertainty of the retrieved ssms as 0 95 and 0 05 quantiles of all outputs 3 4 model validation we validate the grace ssm retrievals through comparisons with i microwave global retrievals from smos 2010 2017 and smap 2015 2017 which have been extensively validated with ground data jackson et al 2011 al bitar et al 2017 colliander et al 2017 2019a b al yaari et al 2017 2019 montzka et al 2017a el hajj et al 2018 chen et al 2018 and ii ground based gauge data from the ismn dorigo et al 2011 over a handful of regions with sufficiently dense ssm observations the coefficient of determination r2 and root mean squared deviation rmsd are employed as quality metrics in addition to comparisons between the global maps the time series of the spatial mean values are also compared over six global zones covering most of the smap and smos domains including 1 north america 50 125 w 0 42 n 2 south america 35 85 w 0 58 s 3 north africa 20 w 55 e 0 15 n 4 south africa 5 52 e 0 35 s 5 southeastern asia 65 122 e 5 32 n and 6 australia 113 157 e 10 40 s the model performance in capturing the ssm monthly and interannual variability is evaluated with r2 on the original time series and after removing the sub annual variability interannual r2 respectively the sub annual variability is removed by taking a moving average of the original time series over a 12 month time window for ground validation the ismn in situ data 2002 2017 of surface soil moisture 0 5 cm are used in areas containing at least 20 stations with an average temporal coverage longer than half of the study period 2002 2017 after removing the ssm data flagged as low quality and outliers i e values more than three standard deviations away from the median accordingly four areas are selected 1 the western contiguous united states conus with 800 stations 99 125 w 29 49 n 2 the eastern conus with 207 stations 67 99 w 24 49 n 3 the central spain with 24 stations 5 6 w 41 42 n and 4 the southeastern australia with 40 stations 143 151 e 31 37 s note that the eastern and western conus are considered separately due to differences in their land cover and ssm climatology the spatial averages of the ssm data are compared rather than the individual gage pixel pairs in order to reduce the scaling gap between the spatial resolution of in situ data and satellite retrievals i e 3 degrees for grace 4 results and discussion in section 4 1 we first discuss land regions where the proposed model assumptions e g nwf ds dt are expected to be valid and then explore the relationships between the model inputs ds dt and outputs relative ssm before calibration with smap in section 4 2 the grace ssm retrievals are compared with the microwave based retrievals section 4 3 presents the ground validation results 4 1 model behavior and validity domain as mentioned earlier in section 3 3 a central assumption in the proposed model is that the land surface net water flux can be approximated by the grace ds dt data and such an assumption is more valid over wet regions with strong seasonality in ds dt therefore the strength of ds dt seasonality fig 1 left may be considered as a useful proxy for establishing the domain for the proposed approach our results also indicate that the diffusivity parameter d is generally higher for wetter regions with stronger ds dt seasonality fig 1 right the parameters k not shown for brevity is more uniformly distributed than d as it is close to 0 globally the similarity between soil diffusivity and the ds dt seasonality pattern is expected from eq 8 and is consistent with the known connections between the climate regimes and soil hydraulic properties montzka et al 2017b for example higher d values correspond to densely vegetated tropical climate regions where the soil bulk density is extremely low due to the high organic matter content hengl et al 2014 figs 2 and 3 present sample results for the global spatial distribution and temporal dynamics of ds dt and relative ssm based on eq 7 as observed there are similarities over wet regions and dissimilarities over dry regions between the spatiotemporal variations of ds dt and relative ssm reflecting the fact that the relationship between ds dt and ssm is not necessarily linear in addition it is found that the ssm phase is lagged behind ds dt by about one month fig 3 this phase lag is documented in previous research and increases with the soil depth due to the decreased degree of coupling between soil moisture and land surface water flux e g hirabayashi et al 2003 swenson et al 2008 sadeghi et al 2019a results in figs 2 and 3 point to an important aspect of the proposed model that provides a physical basis to address a central question studied empirically in previous research swenson et al 2008 abelen and seitz 2013 crow et al 2017 short gianotti et al 2019 to what extent can a linear scaling relationship connect surface soil moisture dynamics to the subsurface water storage variations to answer this question we investigate the linear correlation between ds dt and relative ssm shown in fig 4 it is found that this correlation is stronger over wetter regions e g amazon and congo basins consequently a linear relationship may properly scale ds dt to ssm with a time lag in areas where the seasonal fluctuations of ds dt are sufficiently strong fig 1 left such linear mapping however may not be effective in areas with extremely dry climate e g sahara and gobi deserts in general it is observed that r2 between ds dt and ssm is higher than 0 5 fig 4 when the standard deviation of ds dt is larger than about 2 cm month 1 fig 1 4 2 inter comparison of satellite retrievals fig 5 shows the monthly global soil moisture maps from grace observations in comparison with smap and smos retrievals for january and july 2016 here for the sake of consistency we adjust the smos ssm mean values to those of smap in each pixel based on the retrievals from 2015 to 2017 to that end we subtract the smos mean from and add the smap mean to the original smos data as observed the spatial pattern of grace ssm is generally consistent with that of the microwave retrievals this consistency is largely due to the incorporation of the smap information in eq 15 however the consistency of the ssm variations across different seasons primarily shows the capability of the proposed approach in monitoring global ssm temporal dynamics we admit that incorporation of the smap information is the only reason that the final grace ssm spatial resolution appears comparable to that of the microwave satellites in fig 5 in fact the true spatiotemporal resolution of the grace ssm information is governed by the grace original resolutions 3 degrees monthly which is much coarser than those of smap and smos 40 km 2 3 days fig 6 shows pixel by pixel comparisons of the grace and microwave ssm monthly products shown in fig 5 the rmsd between the grace ssm and microwave retrievals is generally around 0 05 cm3 cm 3 which is only about 0 02 cm3 cm 3 larger than the rmsd between the microwave retrievals from smap and smos satellites the difference between grace and smap smos results is due to for example the simplifying assumptions underlying eq 7 the coarser spatial resolution of grace gravity observations and the uncertainty of microwave retrievals over tropical forests the latter uncertainty is mainly due to the strong attenuation of the soil signal by the vegetation canopy even at lower microwave frequencies such as l band the attenuated soil emission represents only about 5 of the emitted energy measured by the radiometer over dense tropical forests parrens et al 2017 reasonably good global agreement between the different ssm products are also found in terms of monthly ssm temporal dynamics figs 7 and 8 as fig 7 indicates both the monthly and interannual variations of the smap and smos ssm retrievals are adequately captured by grace ssm as expected the correlation is weaker in regions of weak seasonality e g north america and australia for the same reason the signal to noise ratio of the grace ssm retrievals is generally smaller for drier regions see the uncertainty bands in fig 7 for example the average signal to noise ratio i e ssm divided by thickness of the uncertainty band is 4 40 for south america and 2 02 for australia since grace ssm scaling bounds are based on smap retrievals a question may arise what actual added value is provided by the grace ssm retrievals one of the added values of the grace ssm can be attributed to its ability to resolve interannual ssm variability beyond the current record of microwave satellite data to elaborate on this advantage in fig 8 right column and table 1 we report r2 after removing the sub annual variability of the three ssm retrievals using a 12 month moving average as observed the correlation between these annual scale ssm time series is still relatively high except for grace versus smap in some regions table 1 which can be largely related to the short smap data period in australia and north america correlations of the annual scale ssm are even higher than the original ssms this observation indicates that in these regions ssm interannual variability is relatively more significant than the seasonal variability and is well captured by the grace ssm data for example australia has experienced a drastic ssm decline starting from 2011 detected consistently by all the three satellites since only 2 years of smap data 2015 2017 are used to calibrate the new ssm retrievals for the entire grace era 2002 2017 another important aspect of the presented approach is its acceptable performance outside the smap calibration period 2002 2015 as shown in fig 8 r2 and rmsd maps for grace versus smap and grace versus smos are comparable even though smap shares a much shorter temporal overlap with grace than smos the high correlations between the grace and smos annual scale ssm outside the calibration period table 1 reflect the real skill in the grace ssm retrievals beyond the incorporated smap skill via calibration this is critical since no interannual smap information is utilized for the calibration of grace ssm retrievals therefore such information can only be extracted from the proper interpretation of long term grace retrievals 4 3 ground validation the satellite ssm retrievals are compared with the ismn in situ ssm data in fig 9 in general all satellite retrievals are found to be consistent with the in situ data in terms of monthly dynamics rmsd r2 and interannual variability interannual r2 in western conus the new grace based retrievals are relatively better than smap and smos retrievals in capturing both the monthly and interannual ssm dynamics this might be attributed to the complex mountainous surfaces in this region and frequent freeze and thaw which could increase uncertainty of microwave retrievals al yaari et al 2017 this observation underlines the potential of grace derived ssm to reduce the uncertainty of microwave data products in other areas mostly covered with cropland and grassland smap and smos provide more accurate ssm estimates than the grace while the grace estimates are still in reasonable agreement with the in situ data as shown the grace retrievals can successfully capture the long term 2005 2014 decline of ssm in western conus observed by the in situ sensors interannual r2 0 74 in eastern conus smos is the most accurate in terms of rmsd 0 019 cm3 cm 3 while smap best captures the interannual variability interannual r2 0 84 in this region grace retrievals overestimate the in situ observations which could be largely due to the contamination effects of the abundant surface water bodies on the upper ssm bound θ max from the smap retrievals gao et al 2020 in spain the grace retrievals are less accurate than those of smap and smos in capturing both monthly and interannual variability in australia the new approach shows higher rmsd than smap and smos while still well capturing the interannual ssm variability interannual r2 0 65 5 conclusions and future perspective a parsimonious analytical model based on richards equation is introduced to retrieve surface soil moisture ssm from the grace satellite retrievals of the terrestrial water storage anomaly without relying on ancillary data this model can translate grace data into a relative ssm ranging from 0 to 1 which represents a measure of monthly soil wetness relative to the extrema of the absolute volumetric ssm the relative ssm can be calibrated to the absolute ssm using a linear scaling thus providing a new opportunity to infer ssm monthly and interannual variability using the grace data the presented approach provides an independent method of mapping global monthly ssm at 3 degree resolution complementary to other existing satellite modeled products monthly grace ssm estimates are shown to be well correlated with the microwave ssm retrievals especially over wet regions reasonably well correlations are found even after removing ssm sub annual variability showing the capability of the new ssm estimates in capturing interannual ssm variability which is important for climate studies although the grace ssm product is limited by its low spatiotemporal resolution compared to the microwave satellites it provides a new means for global retrieval of ssm that could complement the microwave data perhaps over problematic areas such those with high vegetation optical depth vod or large amounts of radio frequency interference rfi most of existing long term microwave ssm datasets such as the climate change initiative cci are based on c and x band observations which are commonly masked over moderately and densely vegetated areas due to the strong attenuation of the soil signal by the vegetation canopy liu et al 2011 2012 dorigo et al 2017 other ssm time series based on lower frequency observations at l band such as smos and smap have a potential to monitor ssm over denser vegetation but the time series are relatively short since 2010 for smos and 2015 for smap in contrast the grace ssm product provides a homogeneous ssm data set i e based on a single observation system which benefits from both a relatively large temporal coverage 2002 2017 and the potential to map ssm over densely vegetated surfaces besides soil moisture the grace ssm product may be used to improve microwave retrievals of vod which is considered as a key vegetation index for carbon studies tian et al 2016 brandt et al 2018 rodríguez fernández et al 2018 fan et al 2019 for instance the microwave vod estimates are uncertain over dense forests when retrieved simultaneously with ssm a potential solution is to constrain monthly microwave soil moisture retrievals with the grace ssm and thereby retrieve vod with less uncertainty since the new approach is based on the linearized richards equation which simulates the isothermal liquid soil water flow it is expected to be less accurate in hot and dry environments where non isothermal liquid and vapor flows are significant in addition the presented model does not account for snow dynamics and hence it is likely less accurate in high latitude areas future research is required to characterize relationships that can physically or empirically link the grace observed mass change due to snow accumulation or melt to soil moisture variations any success along this line will open new windows to use the grace data to fill the gaps in the global soil moisture records over high latitude areas where the microwave satellites are ineffective the proposed approach may also be used to retrieve the root zone soil moisture by adjusting the model parameters and the soil depth this may be considered as a potential way not only to expand the shallow vertical support of the microwave satellites but also to isolate the groundwater storage variations from the grace signal yeh et al 2006 frappart and ramillien 2018 the latter has been investigated through assimilating grace observations into computationally intensive land surface models zaitchik et al 2008 houborg et al 2012 or developing parsimonious empirical relationships swenson et al 2008 the proposed approach however benefits from the physical basis and parsimony at the same time credit authorship contribution statement morteza sadeghi conceptualization methodology data curation formal analysis writing original draft writing review editing lun gao data curation writing review editing ardeshir ebtehaj funding acquisition methodology writing review editing jean pierre wigneron data curation writing review editing wade t crow methodology writing review editing john t reager data curation writing review editing arthur w warrick methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we gratefully acknowledge support from the nasa terrestrial hydrology program thp 80nssc18k152 and the new early career investigator program nip 80nssc18k0742 
5664,passive microwave radiometry from space through missions such as the soil moisture and ocean salinity smos and soil moisture active passive smap satellites is the most reliable means for mapping global surface soil moisture ssm nonetheless microwave ssm retrievals are uncertain over densely vegetated surfaces or areas with high radio frequency interference this paper presents a new observationally driven approach to remote sensing of global ssm based on the terrestrial water storage anomaly twsa data acquired from the gravity recovery and climate experiment grace satellite this approach rests on a physically based yet parsimonious model based on the richards equation and the assumption that the twsa temporal rate of change ds dt approximates the land surface net water flux nwf as the surface boundary condition the grace based ssm is found to be in a reasonable agreement with in situ data and highly correlated with the smap and smos retrievals especially over wet regions where the assumption of nwf ds dt holds valid the grace retrievals contain new ssm information relative to the microwave satellite data and provide a potential solution to improve the microwave data over densely vegetated surfaces or areas with high radio frequency interference keywords grace smap smos terrestrial water storage soil moisture richards equation 1 introduction soil moisture makes up a very small fraction of the global freshwater budget however its importance far outweighs its physical amount in that it links water energy and carbon cycles that govern the climate and metabolism of the earth system across the soil vegetation atmosphere continuum entekhabi et al 2010 from an ecological point of view the pools of soil moisture are fundamental ecosystem resources providing water for plant transpiration and hence soil moisture is one of the major controls of ecosystem structure functioning and diversity especially in drylands robinson et al 2008 soil moisture also regulates the vegetation water content which is one of the most important elements in wildfire risk assessment maki et al 2004 a comprehensive review on the role of soil moisture in numerous earth and environmental science applications and its impacts on the global environment and human society can be found for example in robinson et al 2008 and babaeian et al 2019 today remote sensing provides an exceedingly powerful means for large scale mapping of surface 0 5 cm soil moisture ssm using a variety of electromagnetic sensors as ssm variation alters the land surface optical reflectance lobell and asner 2002 sadeghi et al 2015 temperature hulley et al 2010 aminzadeh and or 2013 and microwave emission njoku and kong 1977 tsang et al 1985 optical and thermal remote sensing have shown great potential for mapping ssm at high spatial resolutions e g carlson et al 1994 moran et al 1994 fensholt and sandholt 2003 sadeghi et al 2017a however owing to the microwave penetration through clouds and vegetation canopy microwave remote sensing is the most reliable approach to global monitoring of ssm this has motivated several satellite missions in the past two decades such as the soil moisture and ocean salinity smos kerr et al 2001 the advanced microwave scanning radiometer amsr e njoku et al 2003 the advanced scatterometer ascat wagner et al 2013 and the soil moisture active passive smap entekhabi et al 2010 the upwelling land surface microwave emission varies significantly as a function of ssm especially when vegetation water content is below 5 kg m 2 accordingly ssm can be retrieved via inversion of a forward radiative transfer model that links the soil microwave reflectivity to the observed brightness temperature njoku and kong 1977 tsang et al 1985 salient advances have been made in the microwave soil moisture retrieval theories and algorithms e g paloscia et al 2006 mattia et al 2006 wigneron et al 2007 2017 naeimi et al 2009 liu et al 2011 kurum et al 2010 2012 konings et al 2016 tabatabaeenejad et al 2014 2017 sadeghi et al 2017b schwank et al 2018 ebtehaj and bras 2019 gao et al 2020 yet ssm retrievals remain uncertain over densely vegetated surfaces areas with high radio frequency interference rfi high latitude areas when soil is frozen and deserts vittucci et al 2016 wigneron et al 2017 al yaari et al 2019 recent research has demonstrated great potential for using ssm data to estimate components of the surface water balance equation such as precipitation brocca et al 2013 2014 koster et al 2016 evapotranspiration mccoll et al 2017 purdy et al 2018 akbar et al 2018 2019 runoff koster et al 2018 and irrigation lawston et al 2017 brocca et al 2018 jalilvand et al 2019 zaussinger et al 2019 it is also indicated that ssm data by itself contain adequate information to close the subsurface water budget provided that the input ssm is accurate and robust koster et al 2018 sadeghi et al 2019a b accordingly small interannual trends in ssm are often equivalent to large shifts in the subsurface water resources which may occur due to the climate variability and or anthropogenic impacts rodell et al 2018 these recent findings have motivated this study to explore a new approach to remote sensing of global ssm to potentially add new information to those of optical thermal and microwave remote sensing the main objective of this study is to retrieve global ssm based on the observations by the gravity recovery and climate experiment grace tapley et al 2004 the grace satellite observes the earth s mass change on a near monthly timescale over land this mass change is mainly governed by the land atmosphere water exchange and hence yields the terrestrial water storage anomaly twsa the twsa data has been shown to be highly correlated with ssm data swenson et al 2008 abelen and seitz 2013 crow et al 2017 chiefly due to the physical linkage between ssm and the land surface net water flux nwf which largely determines the twsa seasonal dynamics the grace twsa has a lower spatial resolution than the microwave ssm data 3 degrees vs 40 km and less frequent temporal sampling monthly vs daily however it is not significantly affected by vegetation density rfi and the freeze thaw state of the soil water hence we hypothesize that twsa contains ssm information beyond what can be extracted from microwave data in this paper we introduce an observationally driven yet physically based approach to translate twsa to ssm this approach relies on two main assumptions i the nwf can be approximated by the grace twsa rate of change ds dt crow et al 2017 yin et al 2019 and ii the linearized richards 1931 equation can adequately represent the ssm nwf relationship at coarse spatiotemporal resolutions sadeghi et al 2019b in the next section we list the data sets used in this study in section 3 we explain our methodology to retrieve ssm from grace twsa data which relies on warrick s 1975 analytical solution to the linearized richards equation in section 4 we present and discuss the results of this study section 5 concludes and discusses some future directions of this research 2 data sets in this study we use in situ ssm 0 5 cm data from the international soil moisture network ismn dorigo et al 2011 and remote sensing observations from grace smap and smos satellites including 1 the jpl grace mascon terrestrial water storage anomaly wiese et al 2018 from may 2002 to may 2017 at 0 5 degree grid resolution but representing the 3 degree equal area caps with a near monthly temporal resolution 2 the smap l3 enhanced passive soil moisture o neill et al 2018 from april 2015 to may 2017 at 9 km grid resolution with a 2 3 day revisiting time and 3 the smos ic passive soil moisture fernandez moran et al 2017 product from may 2010 to may 2017 at 25 km grid resolution with a 2 3 day temporal resolution for consistency all products are projected onto the regular gird with 0 25 degree resolution via nearest neighbor interpolation this interpolation is used for redundant mapping of the low resolution data onto the target grid with a higher resolution to keep the information content of higher resolution data all products are also resampled to monthly temporal resolution by linear interpolation for grace and arithmetic averaging for smap and smos data 3 methodology 3 1 background richards 1931 equation captures soil moisture dynamics in space and time in response to the diffusive and gravitational water flow in unsaturated soil the soil moisture based richards equation is widely applied in land surface models decker and zeng 2009 this equation is 1 θ t d 2 θ z 2 k θ z where θ is the volumetric soil moisture content t is the time z is the soil depth positive downward d is the effective soil water diffusivity i e an average value over the whole saturation range and k is the average slope of the soil hydraulic conductivity function the relationship between unsaturated hydraulic conductivity k and volumetric soil moisture θ warrick 1975 analytically solved eq 1 for a semi infinite soil profile subject to the following boundary and initial conditions 2a d θ z k z 0 f t 2b limi t z θ z t θ 2c θ z 0 θ i z θ 1 g z where f is an arbitrary net water flux nwf at time t g is an arbitrary value determining the initial soil moisture profile θi z and θ is the limiting soil moisture content at large depths that is equivalent to the long term average of soil moisture along the soil profile the nwf can be positive infiltration gain or negative evaporative loss and is defined as any exchange of water at the land atmosphere interface z 0 which can be estimated as follows 3 f p i r e r where p is precipitation ir is irrigation e is evaporation and r is the surface runoff warrick s 1975 solution to eq 1 subject to the boundary and initial conditions in eq 2 is given as the sum of two solutions including i the solution due to the boundary condition θ bc and ii the solution due to the initial condition θ ic 4a θ z t θ bc z t θ ic z t 4b θ bc z t 0 t e 0 25 τ f τ e 0 25 z 2 t τ π t τ 0 5 e 0 25 t τ 0 5 z erfc 0 5 z t τ t τ d τ 4c θ ic z t 0 e 0 5 ζ g ζ e 0 25 z ζ 2 t e 0 25 z ζ 2 t 2 π t 0 5 e 0 25 t 0 5 z ζ erfc 0 5 z ζ t t d ζ where τ and ζ are the dummy variables of integration erfc denotes the complementary error function and t z θ and f are dimensionless representations of time t soil depth z volumetric soil moisture θ and net water flux f 5 t k 2 t d z kz d θ θ θ 1 e 0 5 z 0 25 t f f k θ k θ a closed form solution is obtained when assuming uniform initial condition θi θ and stepwise surface flux input of 6 f t f 1 0 t δ t f 2 δ t t 2 δ t f n n 1 δ t t n δ t the closed form solution for calculating soil moisture at any arbitrary depth and at the n th time step with δt time intervals is 7a θ n z e 0 5 z 0 25 t f 1 u z t n 1 e 0 5 z 0 25 t f 1 u z t i 2 n f i f i 1 u z n i 1 δ t n 1 7b u z t 0 5 e z z t 1 erfc 0 5 z t t t π e 0 5 z t t 2 0 5 erfc 0 5 z t t sadeghi et al 2019a validated the performance of an inverted form of eq 7 yielding nwf as a function of sm with ground based observations from four flux tower sites with different climatic conditions and land cover types in the u s they have shown that while warrick s solution was derived for bare soil it also works well for densely vegetated sites such as the tonzi ranch in california with 30 60 forest canopy cover baldocchi 2016 largely because of i the strong correlation between soil evaporation and plant transpiration on an annual basis and ii calibration of the model parameters based on actual data which capture the impacts of plant transpiration sadeghi et al 2019b further evaluated the inverted eq 7 on a global scale and confirmed its applicability over a large portion of the world however it should be noted that the model does not account for transpiration by deeply rooted plants when the soil surface dries out and soil evaporation and plant transpiration decouple the analytical nature of eq 7 facilitates calculation of soil moisture profile from the grace twsa in monthly time steps without any truncation error which is of a concern in numerical solutions of richards equation zeng and decker 2009 note that the nwf is equivalent to the total change of the soil moisture profile during time step δt hence running eq 7 with a coarse time step yields the net change of soil moisture due to the cumulative impacts of all positive and negative surface fluxes occurring during δt e g sub monthly infiltration and evaporation events for monthly time steps 3 2 model parameter estimation to map the grace twsa to ssm the model parameters including d k and θ need to be estimated globally this may be performed through computationally intensive backpropagation of the error by comparing the model outputs with some reference ssm observations however to keep the ssm estimates chiefly independent from ancillary data we introduce an analytical and computationally efficient approach to estimate the model parameters sadeghi et al 2019 showed that eq 7 can be closely approximated with the following simple equation for a constant net water flux f if z 0 near surface and t 0 1 equivalent to t 274 years for d 3000 cm2 month 1 and k 0 3 cm month 1 8 θ θ f k θ t d eq 8 implies that the higher the diffusivity parameter d the lower the soil moisture amplitude θ θ per a given pulse of surface flux f due to the linearization of the hydraulic conductivity function k θ the effective values of k in various soils are significantly smaller than the saturated hydraulic conductivity sadeghi et al 2019a therefore the average deep percolation kθ in the warrick model is mostly negligible compared to the nwf fluctuations on a monthly timescale with kθ negligible and f independent of the model parameters eq 8 yields the following relationship between the outputs of eq 7 for two different parameter sets θ vs θ 9 d d θ θ θ θ 2 considering θ to be the solution for an arbitrary initial estimate for the model parameters k d and θ the optimum values of d and θ can be approximated from eq 9 if having ssm observations at any two moments of the time series let us consider the wettest moment θ max and the driest moment θ min of the target ssm time series i e the strongest ssm amplitudes in response to the strongest nwf signals yielding 10 d d θ max θ θ max θ 2 θ min θ θ min θ 2 for the sake of generality we define a relative soil moisture s as follows 11 s θ θ min θ max θ min from eqs 10 and 11 we can derive 12 s s s min s max s min 13 d d s max s 1 s 2 where s max and s min are the relative ssm values at the wet and dry moments from eq 7 having θ replaced by s using an initial estimate of the model parameters s d and k and s and d denote estimates of the optimum values for these parameters finally replacing θ in eq 8 by s and assuming that the long term temporal mean of s equals to s the parameter k can be estimated from eq 8 as 14 k f mean s where f mean is the long term temporal mean of nwf from a given nwf time series e g for a given grace pixel and the parameters estimated via eqs 12 to 14 eq 7 results in a relative ssm time series bounded between 0 θ θ min and 1 θ θ max clearly this relative ssm can be converted to the absolute ssm via a linear scaling whenever θ max and θ min are known note that θ max and θ min do not have to be necessarily equal to the fully dry and saturated moisture contents respectively as s and θ are linearly related the solutions for s will be valid for any values of θ max and θ min regardless of whether they are representative of true long term extrema 3 3 model implementation we use eq 7 to map monthly scale ssm from grace twsa data following previous research crow et al 2017 yin et al 2019 we assume that the derivative of the grace twsa with respect to time approximates the land surface net water flux i e nwf ds dt this assumption and its validity rely on the fact that grace ds dt variations due to other factors e g groundwater abstraction surface water storage variations lateral groundwater flow are generally much smaller than its seasonal variations due to nwf accordingly we feed grace ds dt data into eq 7 to globally map ssm note that ds dt is considered to be equal to net water flux f in eq 5 that yields the dimensionless flux f in eq 7a to make the results compatible with the microwave retrievals the sensing depth z in eq 5 is set to 2 5 cm i e the middle of the surface layer assuming that the microwave ssm retrievals represent the moisture content in the top 0 5 cm of the soil column it should be noted that because d k 2 5 cm the normalized depth z kz d and subsequently our results are not overly sensitive to small changes in z for model calibration we pursue a two step approach in the first step we calibrate the model parameters based on eqs 12 to 14 yielding the relative ssm between 0 and 1 in the second step we rescale the relative ssm into the absolute ssm using smap data in the parameter calibration step we first run the model eq 7 with initial parameter estimates k 0 3 cm month 1 s 0 5 and d 3000 cm2 month 1 for all pixels sadeghi et al 2019a b then we update the parameters using eqs 12 to 14 using the extrema of the initial outputs of eq 7 s max and s min over the entire grace period 2002 2017 finally we run the model one more time with the updated parameters yielding the time series of s for all pixels the global retrievals of the relative ssm are finally calibrated to the absolute ssm values based on the soil moisture extrema defined by smap retrievals acquired between april 2015 and may 2017 using the following scaling relationship 15 θ grace 02 17 θ min smap 15 17 θ max smap 15 17 θ min smap 15 17 s grace 02 17 s min grace 15 17 s max grace 15 17 s min grace 15 17 where θ grace 02 17 is the grace based absolute ssm retrievals 2002 2017 s grace 02 17 is the grace based relative ssm retrievals 2002 2017 θ max smap 15 17 and θ min smap 15 17 are the maximum and minimum of the smap ssm retrievals 2015 2017 and s max grace 15 17 and s min grace 15 17 are the maximum and minimum of the grace relative ssm retrievals 2015 2017 note that we use smap for this calibration purpose rather than smos due to its improved spatial coverage and rfi mitigation algorithm piepmeier et al 2016 it is also important to note that the smap based calibration only scales the relative ssm estimates and does not change the temporal structure of the new ssm information derived from grace to determine uncertainties associated with the grace ssm estimates we conduct 100 monte carlo simulations by perturbing all model inputs via uniform zero mean random errors specifically we perturb the grace twsa by a random error varying between plus and minus one sigma uncertainty reported for the jpl grace mascon product the smap data are also perturbed by a random error of 10 the perturbed inputs are fed into the model to quantify the uncertainty of the retrieved ssms as 0 95 and 0 05 quantiles of all outputs 3 4 model validation we validate the grace ssm retrievals through comparisons with i microwave global retrievals from smos 2010 2017 and smap 2015 2017 which have been extensively validated with ground data jackson et al 2011 al bitar et al 2017 colliander et al 2017 2019a b al yaari et al 2017 2019 montzka et al 2017a el hajj et al 2018 chen et al 2018 and ii ground based gauge data from the ismn dorigo et al 2011 over a handful of regions with sufficiently dense ssm observations the coefficient of determination r2 and root mean squared deviation rmsd are employed as quality metrics in addition to comparisons between the global maps the time series of the spatial mean values are also compared over six global zones covering most of the smap and smos domains including 1 north america 50 125 w 0 42 n 2 south america 35 85 w 0 58 s 3 north africa 20 w 55 e 0 15 n 4 south africa 5 52 e 0 35 s 5 southeastern asia 65 122 e 5 32 n and 6 australia 113 157 e 10 40 s the model performance in capturing the ssm monthly and interannual variability is evaluated with r2 on the original time series and after removing the sub annual variability interannual r2 respectively the sub annual variability is removed by taking a moving average of the original time series over a 12 month time window for ground validation the ismn in situ data 2002 2017 of surface soil moisture 0 5 cm are used in areas containing at least 20 stations with an average temporal coverage longer than half of the study period 2002 2017 after removing the ssm data flagged as low quality and outliers i e values more than three standard deviations away from the median accordingly four areas are selected 1 the western contiguous united states conus with 800 stations 99 125 w 29 49 n 2 the eastern conus with 207 stations 67 99 w 24 49 n 3 the central spain with 24 stations 5 6 w 41 42 n and 4 the southeastern australia with 40 stations 143 151 e 31 37 s note that the eastern and western conus are considered separately due to differences in their land cover and ssm climatology the spatial averages of the ssm data are compared rather than the individual gage pixel pairs in order to reduce the scaling gap between the spatial resolution of in situ data and satellite retrievals i e 3 degrees for grace 4 results and discussion in section 4 1 we first discuss land regions where the proposed model assumptions e g nwf ds dt are expected to be valid and then explore the relationships between the model inputs ds dt and outputs relative ssm before calibration with smap in section 4 2 the grace ssm retrievals are compared with the microwave based retrievals section 4 3 presents the ground validation results 4 1 model behavior and validity domain as mentioned earlier in section 3 3 a central assumption in the proposed model is that the land surface net water flux can be approximated by the grace ds dt data and such an assumption is more valid over wet regions with strong seasonality in ds dt therefore the strength of ds dt seasonality fig 1 left may be considered as a useful proxy for establishing the domain for the proposed approach our results also indicate that the diffusivity parameter d is generally higher for wetter regions with stronger ds dt seasonality fig 1 right the parameters k not shown for brevity is more uniformly distributed than d as it is close to 0 globally the similarity between soil diffusivity and the ds dt seasonality pattern is expected from eq 8 and is consistent with the known connections between the climate regimes and soil hydraulic properties montzka et al 2017b for example higher d values correspond to densely vegetated tropical climate regions where the soil bulk density is extremely low due to the high organic matter content hengl et al 2014 figs 2 and 3 present sample results for the global spatial distribution and temporal dynamics of ds dt and relative ssm based on eq 7 as observed there are similarities over wet regions and dissimilarities over dry regions between the spatiotemporal variations of ds dt and relative ssm reflecting the fact that the relationship between ds dt and ssm is not necessarily linear in addition it is found that the ssm phase is lagged behind ds dt by about one month fig 3 this phase lag is documented in previous research and increases with the soil depth due to the decreased degree of coupling between soil moisture and land surface water flux e g hirabayashi et al 2003 swenson et al 2008 sadeghi et al 2019a results in figs 2 and 3 point to an important aspect of the proposed model that provides a physical basis to address a central question studied empirically in previous research swenson et al 2008 abelen and seitz 2013 crow et al 2017 short gianotti et al 2019 to what extent can a linear scaling relationship connect surface soil moisture dynamics to the subsurface water storage variations to answer this question we investigate the linear correlation between ds dt and relative ssm shown in fig 4 it is found that this correlation is stronger over wetter regions e g amazon and congo basins consequently a linear relationship may properly scale ds dt to ssm with a time lag in areas where the seasonal fluctuations of ds dt are sufficiently strong fig 1 left such linear mapping however may not be effective in areas with extremely dry climate e g sahara and gobi deserts in general it is observed that r2 between ds dt and ssm is higher than 0 5 fig 4 when the standard deviation of ds dt is larger than about 2 cm month 1 fig 1 4 2 inter comparison of satellite retrievals fig 5 shows the monthly global soil moisture maps from grace observations in comparison with smap and smos retrievals for january and july 2016 here for the sake of consistency we adjust the smos ssm mean values to those of smap in each pixel based on the retrievals from 2015 to 2017 to that end we subtract the smos mean from and add the smap mean to the original smos data as observed the spatial pattern of grace ssm is generally consistent with that of the microwave retrievals this consistency is largely due to the incorporation of the smap information in eq 15 however the consistency of the ssm variations across different seasons primarily shows the capability of the proposed approach in monitoring global ssm temporal dynamics we admit that incorporation of the smap information is the only reason that the final grace ssm spatial resolution appears comparable to that of the microwave satellites in fig 5 in fact the true spatiotemporal resolution of the grace ssm information is governed by the grace original resolutions 3 degrees monthly which is much coarser than those of smap and smos 40 km 2 3 days fig 6 shows pixel by pixel comparisons of the grace and microwave ssm monthly products shown in fig 5 the rmsd between the grace ssm and microwave retrievals is generally around 0 05 cm3 cm 3 which is only about 0 02 cm3 cm 3 larger than the rmsd between the microwave retrievals from smap and smos satellites the difference between grace and smap smos results is due to for example the simplifying assumptions underlying eq 7 the coarser spatial resolution of grace gravity observations and the uncertainty of microwave retrievals over tropical forests the latter uncertainty is mainly due to the strong attenuation of the soil signal by the vegetation canopy even at lower microwave frequencies such as l band the attenuated soil emission represents only about 5 of the emitted energy measured by the radiometer over dense tropical forests parrens et al 2017 reasonably good global agreement between the different ssm products are also found in terms of monthly ssm temporal dynamics figs 7 and 8 as fig 7 indicates both the monthly and interannual variations of the smap and smos ssm retrievals are adequately captured by grace ssm as expected the correlation is weaker in regions of weak seasonality e g north america and australia for the same reason the signal to noise ratio of the grace ssm retrievals is generally smaller for drier regions see the uncertainty bands in fig 7 for example the average signal to noise ratio i e ssm divided by thickness of the uncertainty band is 4 40 for south america and 2 02 for australia since grace ssm scaling bounds are based on smap retrievals a question may arise what actual added value is provided by the grace ssm retrievals one of the added values of the grace ssm can be attributed to its ability to resolve interannual ssm variability beyond the current record of microwave satellite data to elaborate on this advantage in fig 8 right column and table 1 we report r2 after removing the sub annual variability of the three ssm retrievals using a 12 month moving average as observed the correlation between these annual scale ssm time series is still relatively high except for grace versus smap in some regions table 1 which can be largely related to the short smap data period in australia and north america correlations of the annual scale ssm are even higher than the original ssms this observation indicates that in these regions ssm interannual variability is relatively more significant than the seasonal variability and is well captured by the grace ssm data for example australia has experienced a drastic ssm decline starting from 2011 detected consistently by all the three satellites since only 2 years of smap data 2015 2017 are used to calibrate the new ssm retrievals for the entire grace era 2002 2017 another important aspect of the presented approach is its acceptable performance outside the smap calibration period 2002 2015 as shown in fig 8 r2 and rmsd maps for grace versus smap and grace versus smos are comparable even though smap shares a much shorter temporal overlap with grace than smos the high correlations between the grace and smos annual scale ssm outside the calibration period table 1 reflect the real skill in the grace ssm retrievals beyond the incorporated smap skill via calibration this is critical since no interannual smap information is utilized for the calibration of grace ssm retrievals therefore such information can only be extracted from the proper interpretation of long term grace retrievals 4 3 ground validation the satellite ssm retrievals are compared with the ismn in situ ssm data in fig 9 in general all satellite retrievals are found to be consistent with the in situ data in terms of monthly dynamics rmsd r2 and interannual variability interannual r2 in western conus the new grace based retrievals are relatively better than smap and smos retrievals in capturing both the monthly and interannual ssm dynamics this might be attributed to the complex mountainous surfaces in this region and frequent freeze and thaw which could increase uncertainty of microwave retrievals al yaari et al 2017 this observation underlines the potential of grace derived ssm to reduce the uncertainty of microwave data products in other areas mostly covered with cropland and grassland smap and smos provide more accurate ssm estimates than the grace while the grace estimates are still in reasonable agreement with the in situ data as shown the grace retrievals can successfully capture the long term 2005 2014 decline of ssm in western conus observed by the in situ sensors interannual r2 0 74 in eastern conus smos is the most accurate in terms of rmsd 0 019 cm3 cm 3 while smap best captures the interannual variability interannual r2 0 84 in this region grace retrievals overestimate the in situ observations which could be largely due to the contamination effects of the abundant surface water bodies on the upper ssm bound θ max from the smap retrievals gao et al 2020 in spain the grace retrievals are less accurate than those of smap and smos in capturing both monthly and interannual variability in australia the new approach shows higher rmsd than smap and smos while still well capturing the interannual ssm variability interannual r2 0 65 5 conclusions and future perspective a parsimonious analytical model based on richards equation is introduced to retrieve surface soil moisture ssm from the grace satellite retrievals of the terrestrial water storage anomaly without relying on ancillary data this model can translate grace data into a relative ssm ranging from 0 to 1 which represents a measure of monthly soil wetness relative to the extrema of the absolute volumetric ssm the relative ssm can be calibrated to the absolute ssm using a linear scaling thus providing a new opportunity to infer ssm monthly and interannual variability using the grace data the presented approach provides an independent method of mapping global monthly ssm at 3 degree resolution complementary to other existing satellite modeled products monthly grace ssm estimates are shown to be well correlated with the microwave ssm retrievals especially over wet regions reasonably well correlations are found even after removing ssm sub annual variability showing the capability of the new ssm estimates in capturing interannual ssm variability which is important for climate studies although the grace ssm product is limited by its low spatiotemporal resolution compared to the microwave satellites it provides a new means for global retrieval of ssm that could complement the microwave data perhaps over problematic areas such those with high vegetation optical depth vod or large amounts of radio frequency interference rfi most of existing long term microwave ssm datasets such as the climate change initiative cci are based on c and x band observations which are commonly masked over moderately and densely vegetated areas due to the strong attenuation of the soil signal by the vegetation canopy liu et al 2011 2012 dorigo et al 2017 other ssm time series based on lower frequency observations at l band such as smos and smap have a potential to monitor ssm over denser vegetation but the time series are relatively short since 2010 for smos and 2015 for smap in contrast the grace ssm product provides a homogeneous ssm data set i e based on a single observation system which benefits from both a relatively large temporal coverage 2002 2017 and the potential to map ssm over densely vegetated surfaces besides soil moisture the grace ssm product may be used to improve microwave retrievals of vod which is considered as a key vegetation index for carbon studies tian et al 2016 brandt et al 2018 rodríguez fernández et al 2018 fan et al 2019 for instance the microwave vod estimates are uncertain over dense forests when retrieved simultaneously with ssm a potential solution is to constrain monthly microwave soil moisture retrievals with the grace ssm and thereby retrieve vod with less uncertainty since the new approach is based on the linearized richards equation which simulates the isothermal liquid soil water flow it is expected to be less accurate in hot and dry environments where non isothermal liquid and vapor flows are significant in addition the presented model does not account for snow dynamics and hence it is likely less accurate in high latitude areas future research is required to characterize relationships that can physically or empirically link the grace observed mass change due to snow accumulation or melt to soil moisture variations any success along this line will open new windows to use the grace data to fill the gaps in the global soil moisture records over high latitude areas where the microwave satellites are ineffective the proposed approach may also be used to retrieve the root zone soil moisture by adjusting the model parameters and the soil depth this may be considered as a potential way not only to expand the shallow vertical support of the microwave satellites but also to isolate the groundwater storage variations from the grace signal yeh et al 2006 frappart and ramillien 2018 the latter has been investigated through assimilating grace observations into computationally intensive land surface models zaitchik et al 2008 houborg et al 2012 or developing parsimonious empirical relationships swenson et al 2008 the proposed approach however benefits from the physical basis and parsimony at the same time credit authorship contribution statement morteza sadeghi conceptualization methodology data curation formal analysis writing original draft writing review editing lun gao data curation writing review editing ardeshir ebtehaj funding acquisition methodology writing review editing jean pierre wigneron data curation writing review editing wade t crow methodology writing review editing john t reager data curation writing review editing arthur w warrick methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we gratefully acknowledge support from the nasa terrestrial hydrology program thp 80nssc18k152 and the new early career investigator program nip 80nssc18k0742 
