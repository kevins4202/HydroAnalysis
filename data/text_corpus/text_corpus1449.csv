index,text
7245,quantitative analysis of recession curves of karst spring hydrographs is a vital tool for understanding karst hydrology and inferring hydraulic properties of karst aquifers this paper presents a new model for simulating karst spring recession curves the new model has the following characteristics 1 the model considers two separate but hydraulically connected reservoirs matrix reservoir and conduit reservoir 2 the model separates karst spring hydrograph recession into three stages conduit drainage stage mixed drainage stage with both conduit drainage and matrix drainage and matrix drainage stage and 3 in the mixed drainage stage the model uses multiple conduit layers to present different levels of conduit development the new model outperforms the classical mangin model and the recently developed fiorillo model for simulating observed discharge at the madison blue spring located in northern florida this is attributed to the latter two characteristics of the new model based on the new model a method is developed for estimating effective porosity of the matrix and conduit reservoirs for the three drainage stages the estimated porosity values are consistent with measured matrix porosity at the study site and with estimated conduit porosity reported in literature the new model for simulating karst spring hydrograph recession is mathematically general and can be applied to a wide range of karst spring hydrographs to understand groundwater flow in karst aquifers the limitations of the model are discussed at the end of this paper keywords karst aquifer karst spring hydrograph recession curve hydrograph separation effective porosity conduit flow 1 introduction quantitative analysis of the recession curves of karst spring hydrographs is a vital tool for various activities of water resources management in karst areas such as calculating water budgets estimating base flow rates protecting aquatic ecosystems and developing ecotourism resources under climate change fiorillo 2009 ghasemizadeh et al 2012 stevanovic 2015 a recession curve which is a falling limb after a peak on a spring hydrograph can be viewed as an indicator of overall aquifer behaviors during a period without precipitation kiraly 2003 chang et al 2015 fu et al 2016 analyzing the recession curves helps characterize a karst aquifer because hydrodynamic characteristics and hydraulic properties of the karst aquifer determine the shapes of recession curves bonacci 1993 dewandel et al 2003 fiorillo 2011 for example analysis of the recession curves is the basis for estimating effective aquifer porosity and many methods have been developed in literature the methods for estimating effective porosity can be categorized into direct methods e g borehole drilling speleological survey cave diving camera recording and logging and remote controlled vehicle and indirect methods e g tracing tests geophysical survey time series analysis isotopic analysis and spring hydrograph analysis boussinesq 1877 1904 maillet 1905 bonacci 1993 dewandel et al 2003 kovács 2003 kovács et al 2005 stevanovic et al 2010 fiorillo 2011 2014 among these methods the recession curves analysis has several advantages such as being cost effective and providing large scale aquifer properties and needing fewer parameters mangin 1975 bonacci 1993 dewandel et al 2003 kiraly 2003 kovács et al 2005 bailly comte et al 2010 ford and williams 2013 chang et al 2015 goldscheider 2015 stevanovic 2015 the choice of appropriate karst hydrogeology methods depends on the practical and or scientific research questions to be answered the level of understanding of the system to be studied and the amount of resources available goldscheider and drew 2007 ford and williams 2013 goldscheider 2015 this paper presents a new model for simulating the recession curves of karst spring hydrograph the new model uses matrix and conduit reservoirs to represent a karst aquifer which is a common feature of many models developed for karst spring hydrograph analysis martin and dean 2001 martin and screaton 2001 kiraly 2003 geyer et al 2008 shoemaker et al 2008 bailly comte et al 2010 ford and williams 2013 chang et al 2015 the new model is conceptually similar to the widely used mangin model mangin 1975 and the fiorillo model fiorillo 2011 but has its own features the mangin model uses two parallel reservoirs a quick flow reservoir for simulating discharge from unsaturated zones e g conduits and a slow flow reservoir for simulating discharge from saturated zones e g matrix the fiorillo model conceptualizes a karst aquifer as a series of tank reservoirs i e the torricelli reservoir for conduits the darcy reservoir for matrix and the poiseuille reservoir for fractures similar to the mangin model the new model of this study uses two reservoirs i e a conduit reservoir and a matrix reservoir the conduit reservoir is different from that of the mangin model but similar to the conduit reservoir of fiorillo model the matrix reservoir also differs from that of the mangin model and includes the matrix and fracture reservoirs of the fiorillo model because discharge from the matrix and fracture reservoirs can be simulated by two equations of the same form but with different physical meanings fiorillo 2011 similar to the conceptualization of taylor and greene 2008 the new model of this study separates a karst spring hydrograph into three stages the conduit drainage stage spring discharge from drainage conduit flow of conduit reservoir the mixed drainage stage spring discharge from both drainage conduit flow of conduit reservoir and drainage matrix flow of matrix reservoir first to conduit reservoir and then to spring and the matrix drainage stage spring discharge from drainage matrix flow of matrix reservoir first to conduit reservoir and then to spring using the three stages for hydrograph separation distinguishes the new model from the mangin model because the mangin model considers that matrix flow contributes to spring discharge for the entire recession period whereas the new model does not consider matrix flow in the conduit drainage stage of the recession period the reason of not considering the matrix flow is that groundwater flows from conduits to matrix during the conduit drainage stage given that hydraulic head in the conduits is higher than the hydraulic head in the matrix while the new model is similar to the fiorillo model to separates a hydrograph into three stages the new model differs from the fiorillo model in that the new model has the mixed drainage stage the fiorillo model does not use the concept of mixed drainage for the period between the conduit drainage stage and the matrix drainage stage for this between period the fiorillo model considers that the spring discharge is from well connected fissures and uses the poiseuille reservoir to represent the fissures the proposed mixed drainage stage is consistent with the observation that water level in conduits continues decreasing in the period between the conduit drainage stage and the matrix drainage stage which indicates that conduit flow exists in the period shevenell 1996 taylor and greene 2008 if conduit flow is negligible during the mixed drainage stage the mixed flow becomes the matrix flow only or the flow from the poiseuille reservoir in the fiorillo model as shown in section 4 below due to using the mixed drainage concept and enabling the flexibility of multiple conduit layers the results of the new model are better than those of the current implementation of the fiorillo model for simulating the observed discharge at the madison blue spring located in northern florida the new model of karst spring hydrograph analysis provides a basis for estimating groundwater flow from matrix and conduit reservoirs separately and for estimating effective porosity of matrix and conduit reservoirs separately estimating matrix and conduit drainage separately helps understand the relative significance of conduit drainage and matrix drainage in a recession period since effective porosity is an important aquifer property for managing water resources of karst areas fu et al 2016 many methods have been developed to estimate effective porosity of karst aquifers boussinesq 1904 bonacci 1993 shevenell 1996 szilagyi 1999 dewandel et al 2003 kovács et al 2005 fiorillo 2011 2014 however these methods consider a karst aquifer as a whole and do not separate matrix flow and conduit flow therefore the existing methods cannot estimate effective porosity of conduit and matrix reservoir separately which may limit our understanding of karst aquifers this problem is resolved in this study by separating matrix flow and conduit flow in addition multiple conduit layers are used to represent different levels of conduit development and the effective porosity of each conduit layer is estimated estimating effective matrix and conduit porosity for the three drainage stages enables us to better characterize the karst aquifer for the entire recession period the rest of the paper is organized as follows the new model for simulating recession curves of karst spring hydrograph the method of estimating conduit and matrix flows and the method of estimating effective porosity of matrix and conduit reservoirs are discussed in section 2 the new model was used for simulating observed spring discharge at the madison blue spring and the field site and observations of hydraulic head and discharge are described in section 3 two recession periods with different hydrological and groundwater conditions are chosen for evaluating the simulations given by the three models i e the new model of this study the mangin model and the fiorillo model the simulation results and the evaluation of the three models are given in section 4 this section also discusses the results of estimating matrix and conduit drainage as well as effective porosity of matrix and conduit reservoirs the major conclusions of this research are given in section 5 2 methodology this section starts with a detailed description of the new model of karst spring hydrograph analysis in section 2 1 section 2 2 includes a brief description of the mangin model and the fiorillo model that are compared with the new model in this study the method of estimating matrix and conduit flows and the method of estimating effective porosity of matrix and conduit reservoirs are described in section 2 3 2 1 new model of simulating recession curves of karst spring hydrograph the new model of simulating the recession curves of karst spring hydrograph has the following characteristics 1 the matrix and conduits of a karst aquifer are considered as two separate reservoirs this conceptualization of karst aquifer has been widely used for simulating karst spring hydrograph 2 a recession period on a karst spring hydrograph is separated into three stages conduit drainage stage stage i mixed drainage stage stage ii and matrix drainage stage stage iii as shown in fig 1 during the mixed drainage stage the conduit flow in conduits and matrix flow from matrix to conduits are separated explicitly in the new model 3 in the mixed drainage stage multiple conduit layers are used to represent different levels of conduit development fig 1 note that the conduit layers are different from the tanks of the fiorillo model that includes both conduits and matrix explicitly separating matrix and conduit flows and using multiple conduit layers for representing different levels of karst development are two unique features of the new model for simulating spring discharge of the recession periods of karst spring hydrograph fig 2 illustrates the three characteristics discussed above and the conceptual model of the dynamics of matrix and conduit flows for understanding karst spring hydrograph recession during the conduit drainage stage stage i fig 2a conduit flow is the only source of spring discharge and hydraulic head in the conduit reservoir decreases from h 0 c to h 1 c from t 0 to t 1 the spring discharge q t i l3t 1 of the conduit drainage stage for time t 0 t t 1 can be evaluated using the equations below derived by following fiorillo 2011 based on the conceptualization of torricelli reservoir of fiorillo 2011 the spring discharge q t i is expressed as 1 q t i a 2 c 2 gh t c where a 2 c is the area of spring outlet l2 g is the gravity acceleration lt 2 and ht c is hydraulic head of the conduit reservoir l based on the principle of mass balance that the variation rate of water storage in the conduit reservoir equals to the spring discharge during the hydrograph recession we have the differential equation of ht c as bailly comte et al 2010 fiorillo 2011 2 a 1 c dh t c dt q t i a 2 c 2 gh t c where a 1 c is the horizontal area of the conduit reservoir l2 integrating eq 2 for time t 0 to t 1 and for the hydraulic head from h 0 c to h 1 c gives fiorillo 2011 3 2 h t c 2 h 0 c a 2 c a 1 c 2 g t substituting eq 1 into eq 3 gives kullman 1990 fiorillo 2011 4 q t i q 0 i γ t for t 0 t t 1 where q 0 i a 2 c 2 gh 0 c is the initial spring discharge at time t 0 l3t 1 and γ a 2 c 2 a 1 c g is the recession coefficient of the conduit reservoir l3t 2 to use eq 4 for simulating karst spring discharge requires estimating q 0 and γ based on field measurements of spring discharge when estimating γ from discharge measurements the two areas a 1 c and a 2 c are not needed for using eq 4 in the conduit drainage stage stage i hydraulic head in the matrix increases from h 0 m to h 1 m fig 2a due to infiltration of rainfall into the matrix the discharge of conduit flow to the matrix is considered to be negligible according to peterson and wicks 2005 who found that the volume of fluid penetrating from flooded conduits into the matrix is less than 1 of the volume of fluid flowing in the conduits while the finding of peterson and wicks 2005 may not be the case for the floridan aquifer with relatively large secondary porosity the ignorance of the discharge of conduit flow into matrix appears to be valid in the field application of the new model as described below exploring the influence of the discharge volume i e bank storage on the recession mechanism and the recession curve is warranted in a future study when the hydraulic head in the conduit reservoir decreases from h 0 c to h 1 c from t 0 to t 1 and when the hydraulic head in matrix reservoir increases from h 0 m to h 1 m from t 0 to t 1 that equals to h 1 c the mixed drainage stage stage ii starts fig 2b during this stage the hydraulic head in the conduit reservoir continues decreasing from h 1 c to h 2 c from t 1 to t 2 and the hydraulic head in the matrix reservoir starts decreasing from h 1 m to h 2 m from t 1 to t 2 since the head decreases in the conduit reservoir is faster than that in the matrix reservoir groundwater discharges from the matrix to conduits and the spring discharge is composed of both conduit flow and matrix flow although the exchange mechanism between the conduit reservoir and the matrix reservoir is complicated it is reasonable to assume that matrix flow and conduit flow are independent i e the matrix flow do not affect the conduit flow and vice versa peterson and wicks 2005 malík and vojtková 2012 li and field 2013 li et al 2016 following the assumption the matrix flow and conduit flow are simulated separately the matrix flow for time period t 1 t t 2 can be evaluated via maillet 1905 kovács et al 2005 5 q t m ii q 1 m e α 1 t t 1 where q t m ii is the discharge rate of the matrix reservoir l3t 1 q 1 m is the starting matrix discharge at time t t 1 when h 1 m h 1 c l3t 1 and α 1 t 1 is the recession coefficient of the matrix reservoir during the mixed drainage stage the parameters q 1 m and α 1 are estimated from field measurements of spring discharge as described below fiorillo 2011 derived eq 5 for darcy reservoir of porous media and for poiseuille reservoir of fractured media since eq 5 can be used for evaluating both matrix flow and fracture flow the new model of this study does not distinguish matrix and fracture reservoirs but calls both of them as matrix reservoir note that eq 5 is not used for simulating the recession curve in the conduit drainage stage for the conduit flow in the mixed drainage stage stage ii the new model uses multiple conduit layers to represent different levels of karstification in depth fig 2b for the i th conduit layer denoted as li similar to the derivation of eq 4 the discharge rate for tlis c t tlie c tlis c and tlie c being the starting and ending times of groundwater drainage from conduit layer li respectively is derived as 6 q t li c ii q lis c β i t t lis c where q t li c ii is the discharge rate of the i th conduit layer l3t 1 q lis c is the starting discharge from the i th conduit layer l3t 1 β i is the recession coefficient for the i th conduit layer l3t 2 tlis c is the starting time of the discharge from the i th conduit layer t in this study coefficients qlis c βi and tli c of eq 6 are estimated from field measurements of spring discharge as described below combining eqs 5 and 6 gives the spring discharge from the i th conduit layer and the matrix reservoir as 7 q t li ii q t li c ii q t m ii q lis c β i t t lis c q 1 m e α 1 t t 1 during the mixed drainage stage stage ii the total spring discharge from the matrix reservoir and all the conduit layers is 8 q t li ii i q t li c ii q t m ii i q lis c β i t t lis c q 1 m e α 1 t t 1 for an aquifer with a high level of karstification the matrix flow may be negligible when the conduit flow is negligible our model is similar to the mangin model and the fiorillo model in that the spring discharge is controlled mainly by the matrix flow when the conduit flow is not negligible and both matrix flow and conduit are important our model is expected to provide better simulation to measured spring discharge than the mangin model and the fiorillo model do which is demonstrated in the real world application in section 4 the relative importance of matrix flow and conduit flow depends on the level of karstification as different levels of karstification result in the vertical variation of hydraulic conductivity and porosity milanovic 1981 kullman 1990 fiorillo 2011 based on hydrograph recession analyses of nine gauged springs located in a slovak aquifer malík and vojtková 2012 provided the link between recessional equations and a total of ten karstification degrees defined by the authors the recessional equations only include matrix flow for karstification degree less than four the recessional equations include both matrix flow and conduit flow for karstification degree between four and eight and the recessional equations include only conduit flow for karstification degree larger than eight it however should be noted that the definitions of the karstification levels are site specific and they should be used for other karst aquifers with cautions as shown in fig 2c the matrix drainage stage stage iii starts when the hydraulic head in the conduit reservoir decreases from h 1 c to h 2 c at time t t 2 and stabilizes at h 2 c meanwhile the hydraulic head in the matrix reservoir continues decreasing from h 2 m until the next rainfall event in this stage the recession curve of karst spring hydrograph is mainly controlled by the baseflow from the matrix reservoir according to kovács et al 2005 the hydraulic head h 2 c acts as the fixed head boundary when spring discharge is mainly controlled by the matrix reservoir because the conduit network has no influence on the spring discharge and negligible storage in the conduit reservoir contributes to spring discharge following the literature maillet 1905 kovács et al 2005 bailly comte et al 2010 malík and vojtková 2012 goldscheider 2015 the matrix flow q t iii l3t 1 is evaluated via 9 q t iii q 2 m e α 2 t t 2 where q 2 m is the discharge rate of the matrix reservoir at time t t 2 l3t 1 and α 2 is the recession coefficient of the matrix reservoir t 1 using eq 9 for simulating the spring hydrograph requires estimating q2 m and α 2 based on field measurements of spring discharge the procedure of using the equations above to simulate karst spring hydrograph is as follows 1 separate a karst spring hydrograph into the three stages conduit drainage mixed drainage and matrix drainage in two steps the first step is to determine time t 1 the end of the conduit drainage stage stage i since the flow rate of this stage is a linear function of time eq 4 t 1 is determined as the time when the linear hydrograph ends the second step is to determine time t 2 the beginning of the matrix drainage stage stage iii by first plotting the logarithm of the hydrograph with time time t 2 is selected as the beginning of linear semi log plot with respect to time because the semi log plot is a straight line with time eq 9 after t 1 and t 2 are determined the karst spring hydrograph is separated into the conduit drainage stage mixed drainage stage and matrix drainage stage 2 for the conduit drainage stage stage i by fitting eq 4 to discharge measurements to estimate q 0 i and γ 3 simulation the matrix drainage stage stage iii by fitting eq 9 to discharge measurements to estimate q2 m and α 2 4 simulate the matrix flow of the mixed drainage stage stage ii by using eq 5 which requires estimating parameters α 1 and q 1 m of the equation assuming that the matrix reservoir behaves in the same way for producing matrix flow in the matrix flow and mixed drainage stages assign the value of α 2 estimated above to α 1 which is also used in literature mangin 1975 kovács et al 2005 kovács and perrochet 2008 to estimate q 1 m because the matrix flow of the mixed drainage stage equals to the matrix flow of the matrix drainage stage at t t 2 we set q t m ii q 1 m e α 2 t 2 t 1 q 2 m based on eqs 5 and 9 this leads to q 1 m directly 5 simulate the conduit flow of the mixed drainage stage stage ii by subtracting the simulated matrix flow in step 4 above from the discharge measurements this results in the conduit flow from all conduit layers according to eq 8 since the conduit flow of each conduit layer is a linear function of time eq 6 the hydrograph of the conduit flow can be separated into multiple linear segments to determine tlis c the start time of the discharge from the i th conduit layer denoted as li afterward qlis c and βi of eq 6 can be estimated by fitting the equation to corresponding measurements of spring discharge the number of conduit layers is determined empirically for achieving satisfactory fit to measured spring discharge the real world application of the new model below shows that using two conduit layers achieves significantly better fit than using one conduit layer the curve fitting operations above is carried out manually and automatic curve fitting can be done using regression techniques draper and smith 1998 2 2 comparison with two other models the new model of this study is compared with two widely used models the mangin model and the fiorillo model as shown in fig 3 a the mangin model does not separate a karst aquifer into matrix and conduit reservoirs instead the model consists of a reservoir of saturated zone e g conduits and a reservoir of unsaturated zone e g matrix the spring discharge from the two reservoirs is calculated via mangin 1975 10 q t ψ t φ t q q 1 η t 1 ε t q b e α b t where ψ t is the discharge from unsaturated zone l3t 1 φ t is the discharge from saturated zone l3t 1 qq is the maximal infiltration flowrate l3t 1 η t 1 is the inverse of the period of infiltration ε is the concavity of the recession curve resulting from infiltration heterogeneity qb is the initial saturated flow rate l3t 1 and αb is a recession coefficient t 1 all the coefficients qq η ε qb and αb are estimated based on measurements of spring discharge more details of the mangin model and its application are referred to literature mangin 1975 dewandel et al 2003 ford and williams 2013 fu et al 2016 as shown in fig 3b the fiorillo model considers a series of tank reservoirs the first reservoir denoted as 01 in fig 3b is the torricelli tank reservoir that represents drainage from shafts and upper conduits and its flow rate is estimated as fiorillo 2011 11 q t q 0 1 α t t where q 0 1 is the initial discharge of the torricelli tank reservoir and αt is the recession coefficients of the reservoir t 1 other reservoirs are the darcy and poiseuille reservoirs and their discharges are estimated as fiorillo 2011 12 q t i q 0 i e α i t where q0 i l3t 1 is the initial discharge of the i th reservoir and αi t 1 is the recession coefficient of the i th reservoir all the coefficients q 0 1 αt q0 i and αi are estimated based on measurements of spring discharge more details of the fiorillo model and its application are referred to fiorillo 2011 to compare the new model of this study with the two models above the three models are used to simulate real world measurements of the discharge of the madison blue spring located in northern florida and the misfit between simulated and measured discharge is used as the criterion for evaluating the model performance the misfit is calculated as 13 misfit i 1 n r i where r i is the residual between simulated and measured discharge 2 3 estimation of discharge volume and effective porosity based on the hydrograph modeling above we can estimate the discharge volumes from the matrix and conduit reservoirs which in turn can be used to estimate effective porosity of the matrix and conduit reservoirs this section starts with the estimations for the mixed drainage stage stage ii and then discusses the estimations for the other two drainage stages fig 1 illustrates the separation of matrix discharge and conduit discharge for the mixed drainage stage the conduit discharge is further separated to the discharge from each conduit layer these make it possible to explicitly estimate the effective porosity of the matrix reservoir and the effective porosity of individual conduit layers for the mixed drainage stage the effective porosity of the matrix reservoir n m ii is defined as the volume of water discharged from the matrix reservoir when hydraulic head of the matrix reservoir decreases from h 1 m to h 2 m i e from time t 1 to time t 2 shown in fig 2b and estimated in step 1 of section 2 1 above i e 14 n m ii v m ii h 1 m h 2 m a c where v m ii is the groundwater discharge from the matrix reservoir during the mixed drainage stage stage ii and ac is the drainage area of the karst aquifer while ac is always available for a groundwater basin h 1 m h 2 m and v m ii need to be estimated since h 1 m and h 2 m are the average hydraulic head of the matrix reservoir the isoline method described in gupta 2016 is used for estimating the average heads in the isoline method contours of the hydraulic head are first generated using measurements of hydraulic head and the weighted average of hydraulic head h is evaluated via 15 h i 1 n a i h i 1 h i 2 i 1 n a i where h i 1 and h i are the values of the hydraulic head for a pair of isoclines and a i is the area between the pair of isolines the areas are used as the weights for estimating the average hydraulic head of the matrix reservoir after a time series of the average head is obtained based on head measurements h 1 m for time t 1 and h 2 m for time t 2 can be approximated the volume of groundwater discharge v m ii from the matrix reservoir is estimated by integrating the matrix flow eq 5 from t 1 to t 2 as 16 v m ii t 1 t 2 q t m ii dt t 1 t 2 q 1 m e α 1 t t 1 dt q 1 m q 2 m α 1 where q 1 m is estimated in step 4 and q2 m and α 1 are estimated in step 3 of section 2 1 above for the mixed drainage stage the effective porosity of the conduit reservoir n c ii is defined as the volume of water discharged from the conduit reservoir when hydraulic head of the conduit reservoir decreases from h 1 c and h 2 c i e from time t 1 to time t 2 shown in fig 2b and estimated in step 1 of section 2 1 above the effective porosity n li c ii of the i th conduit layer denoted as li is defined as 17 n li c ii v li c ii h lis c h lie c a c where v li c ii is the volume of groundwater discharged from the i th conduit layer and hlis c and hlie c are the starting and ending hydraulic heads when groundwater discharges from the i th conduit layer hlis c and hlie c correspond to tlis c and tlie c used in eq 6 respectively and the starting and ending discharge times tlis c and tlie c are obtained in step 5 of section 2 1 above the discharge volume v li c ii is estimated by integrating the conduit flow eq 6 from tlis to tlie the starting and ending times respectively when groundwater discharges from the i th conduit layer as 18 v li c ii t lis c t lie c q t li c ii dt t lis c t lie c q lis c β i t t lis c dt q lis c t t lis c 1 2 β i t t lis c 2 t lis c t lie c q lis c 2 2 q lis c β i t lie c t lis c β i t lie c t lis c 2 2 β i q lis c 2 2 q lis c β i t lis c t lis c β i t lis c t lis c 2 2 β i q lis c β i t lie c t lis c 2 2 β i q lis c β i t lis c t lis c 2 2 β i q lis c 2 q lie c 2 2 β i where q lis c q lie c and β i are estimated in step 5 of section 2 1 above since there is no measurement of conduit head the isoline method above cannot be used for estimating average conduit head e g hlis c and hlie c which is a theoretical weakness of the new model as conduit heads are rarely available in practice to address this problem this study estimates hydraulic head of each conduit layer by linking conduit head with conduit discharge the estimation starts from hl1s c and hl1e c the starting and ending hydraulic heads when groundwater discharges from the first conduit layer as shown in fig 2b for the starting hydraulic head we have hl1s c h 1 c h 1 m for estimating the ending hydraulic head hl1e c we relate the hydraulic head with the conduit discharge at the beginning and the ending time of the discharge period which are denoted as ql1s c and ql1e c respectively by virtue of eq 1 we have 19 q l 1 s c 2 g h l 1 s c h 2 c q l 1 e c 2 g h l 1 e c h 2 c taking the ratio between ql1e c and ql1s c leads to 20 q l 1 e c q l 1 s c h l 1 e c h 2 c h l 1 s c h 2 c rearranging the equation gives 21 h l 1 e c h l 1 s c h 2 c q l 1 e c q l 1 s c 2 h 2 c considering hl1s c h1 m q l 1 s c q t l 1 c ii and q l 1 e c q t l 2 c ii eq 21 becomes 22 h l 1 e c h 1 m h 2 c q t l 1 c ii q t l 2 c ii 2 h 2 c this equation can be readily evaluated because h 1 m is estimated using eq 15 and in addition q t l 1 c ii and q t l 2 c ii are estimated in step 5 of section 2 1 above for the i th conduit layer beneath the first layer eq 21 becomes 23 h lie c h lis c h 2 c q t li c ii q t li 1 c ii 2 h 2 c where h lis c h li 1 e c i e the starting head of the i th conduit layer is the ending head of the i 1 th conduit layer for the bottom conduit layer h lie c h 2 c the average hydraulic head in the conduit reservoir at t 2 fig 2c estimating h 2 c is more difficult than estimating h 2 m because measurements of hydraulic head in conduit reservoir are always lacking an assumed value may be assigned to h 2 c based on measurements of river stage and or matrix hydraulic head the impacts of assumed h 2 c on the estimation of effective porosity of conduit layers are discussed in section 4 of the real world application of the new method of hydrograph modeling the above estimation of effective porosity for the mixed drainage stage can be applied directly to the conduit flow and matrix drainage stages for the conduit drainage stage similar to eq 17 the effective porosity n c i of conduit reservoir is defined as 24 n c i v c i h 0 c h 1 c a c where v c i is the volume of groundwater discharged from the conduit reservoir when conduit head decreases from h 0 c to h 1 c from time t 0 to time t 1 i e the duration of stage i shown in fig 2a the discharge volume v c i can be estimated by integrating conduit flow q t i eq 4 from t 0 to t 1 which similar to eq 18 leads to 25 v i q 0 i 2 q 1 i 2 2 γ in this equation q 0 i and γ are estimated in step 2 of section 2 1 above and q 1 i the discharge at t 1 can be calculated using eq 4 the conduit hydraulic head h 1 c in eq 4 takes the value of h 1 m as discussed above and h 0 c in eq 24 needs to be estimated indirectly due to the lacking of conduit head data following the derivation of eqs 19 23 the expression of h 0 c is derived as 26 h 0 c h 1 c q 0 i q 1 i 2 the effective porosity n m iii of the matrix reservoir for the matrix drainage stage is defined as 27 n m iii v m iii h 2 m h 3 m where v m iii is the amount of groundwater discharge from the matrix reservoir it can be estimated by integrating the matrix flow q t iii eq 9 from t 2 to t 3 which similar to eq 16 gives 28 v iii t 2 t 3 q t m iii dt q 2 m q 3 m α 1 in this equation q 2 m and α 1 are estimated in step 3 of section 2 1 above and q 3 m can be estimated by using eq 9 the matrix heads h 2 m and h 3 m can be obtained from the average matrix head eq 15 discussed above 3 study area and field data fig 4 shows the location of the study area in the madison county florida the madison blue spring shown in the insertion is a first magnitude spring and the spring vent latitude 30 28 49 and longitude 83 14 40 is located on the eastern border of madison county adjacent to the withlacoochee river the spring vent is connected to an unconfined aquifer by a network of phreatic conduits in the suwannee limestone and the underlying ocala limestone the land surface altitude of the spring is 14 63 m above ngvd29 usgs 2017 the spring discharge ranges from 2 to 4 m3 s and the average discharge over the period of 2002 2017 is 2 8 m3 s usgs 2017 the annual average precipitation is 1335 mm noaa 2017 the average annual recharge from precipitation to the karst aquifer is approximately 254 mm copeland 2003 the wet season is june september and the dry season is october may the area of the springshed is estimated to be 259 km2 greenhalgh 2003 srwmd 2004a b and it is used in eqs 14 and 17 for estimating the effective porosity of matrix and conduit layers the aquifer system in the study area consists of from top to bottom a surficial aquifer an intermediate aquifer a confining unit and the floridan aquifer system scott 1988 despite of the existence of the confining unit the entire aquifer system is considered to be poorly confined for two reasons first since the siliciclastic and carbonate horizons in the intermediate aquifer and the confining unit are permeable the intermediate aquifer and the confining unit act as a semi confining unit that separates the surficial aquifer and the floridan aquifer in addition because of sinkholes and eroded zones in the intermediate aquifer and confining unit the floridan aquifer is poorly confined and recharge to the florida aquifer is relatively effective throughout the springshed bush and johnston 1988 grubbs 1998 arthur et al 2005 therefore it is suitable to apply the new model to analyze the karst spring hydrograph of the madison blue spring fig 5 plots the time series of daily precipitation spring discharge and hydraulic head for the period from october 6 2012 to june 6 2017 the spring discharge is monitored at the madison blue spring station by the u s geological and survey usgs usgs 2017 daily precipitation is monitored at the madison blue springs weather station and hopewell tower station fig 4 by the suwannee river water management district srwmd 2017a the average precipitation of the two stations is plotted in fig 5 a groundwater monitoring network has been established in this area by usgs and the water management district the daily hydraulic head is recorded at five monitoring wells blue spring nestle fsc 1 gibson tower westwood west and lovette tower srwmd 2017b all the wells were drilled into the floridan aquifer and fractures and small karst conduits were encountered during drilling for the convenience of hydrograph analysis a reference head h 0 m is set at h 12 4 m above ngvd29 which is the base flow spring water level usgs 2017 srwmd 2017b the hydraulic head at the blue spring well and nestle fsc 1 well are almost identical and overlap together in fig 5 fig 5 shows that the spring discharge and hydraulic head respond quickly to rainfall events in that the discharge and hydraulic head increase immediately after rainfalls it is noted that after hydraulic heads reach the peak values the spring discharge becomes smaller and sometimes becomes negative this is due to backflow of stream water into the spring vent because the water level of the withlacoochee river increases at a rate faster than that of groundwater in the spring vent gulley et al 2011 brown et al 2014 due to the influence of rainfall events and the river water backflow into spring vent not all the spring discharge data are suitable for analyzing the recession curve of karst spring hydrograph therefore this study selects two recession periods of spring discharge for which the influence of rainfall and river water backflow is small the two periods are marked in fig 5 and more details of the two recession periods are given in table 1 the two periods have different characteristics the first period has a shorter period and smaller spring discharge in comparison with the second period in addition the amount of cumulative precipitation 30 days before the first period is smaller than that before the second period the characteristic data of the two periods suggests that the conduit flow in the first period is smaller than that in the second period 4 results and discussion section 4 1 presents the results of simulating the observed data of spring discharge using the new model the mangin model and the fiorillo model followed by a comparison of the simulation results of the three models in section 4 2 the results of estimating groundwater discharge from the matrix and conduit reservoir and of estimating effective porosity of the matrix and conduit reservoirs of the three stages are given in section 4 3 in section 4 4 the estimated effective porosity of the matrix reservoir is compared with measured matrix porosity along two wells at the study site 4 1 simulated spring discharge fig 6 plots the measured and calculated spring discharge by using the new model with one conduit layer the new model with two conduit layers the mangin model and the fiorillo model the fitted equations of the three models are listed in table 2 the reason of considering the two options of using one conduit layer and two conduit layers for the new model during the mixed drainage stage is to investigate to what extent the use of multiple conduit layers to represent different levels of karstification in the conduit reservoir can improve the simulation of measured spring discharge in fig 6a and b for the new model with one conduit layer the two vertical dashed lines in red separate the karst spring hydrograph into the conduit drainage stage the mixed drainage stage and the matrix drainage stage in fig 6c and d for the new model with two conduit layers the vertical dashed lines in blue further separate the mixed drainage stage into two sub stages corresponding to the discharge from the two conduit layers comparing fig 6a b with fig 6c d shows that using two conduit layers improves the simulation of the measured spring hydrograph especially for the second recession period plotted in fig 6b and d and the improvement is quantified below using the misfit defined in eq 13 in fig 6a d for the mixed drainage stage the simulated matrix flow and conduit flow are plotted separately while the matrix flow varies slightly over time the conduit flow has a large variation especially for the second recession period plotted in fig 6b and d while the conduit flow is substantially smaller than the matrix flow in fig 6a and c the conduit flow is comparable with the matrix flow especially in the early time of the mixed drainage stage in fig 6b and d therefore it is necessary to separate conduit flow and matrix flow for the second recession period shown in fig 6b and d fig 6e and f plot the measured and calculated spring discharge by using the mangin model for the two recession periods in each figure the vertical dashed line separates the fast flow ψ t from the unsaturated zone and the slow flow φ t from the saturated zone fig 3a ψ t becomes zero in the period when spring discharge is only from slow flow i e in the periods after the vertical lines while the fitting between the measured and calculated spring discharge is satisfactory in fig 6e the fitting is less satisfactory in fig 6f especially in the early time when the quick flow is significant the reason is that the simulated slow flow in the early time is relatively large this problem cannot be resolved in the mangin model because it conceptualizes that slow flow from saturated zone always contributes to spring discharge during the entire recession period fig 6g and h plot the measured and calculated spring discharge by using the fiorillo model for the two recession periods in each figure the vertical dashed lines separate the hydrograph into three periods representing spring discharge from three tanks fig 3b the flow in the first period is the spring discharge from the tank that represents conduits and the flows in the other two periods are the spring discharge from the tanks that represent fracture and matrix while the fitting between the measured and calculated spring discharge is satisfactory in fig 6g the fitting is less satisfactory in fig 6h in the second period when both matrix flow and conduit flow contribute to the spring discharge to demonstrate the linear relation between the logarithm of discharge lnq and time t in the latter two flow periods fig 6g and h plot the relation of lnq t for the two periods and the linear relation is observed 4 2 comparison between the new model and the other two models fig 7 plots the residuals differences between observed and simulated spring discharge of the four model simulations for the two recession periods the misfit i e the sum of absolute residuals as defined in eq 13 is the largest for the new model with only one conduit layer denoted as new model 1 in fig 7 and the misfit is significantly larger than those of the other three simulations these indicate that it is necessary to separate the conduit reservoir into two conduit layers in the mixed drainage stage for the first recession period fig 7a the misfit of the new model with two conduit layers denoted as new model 2 in fig 7 is 0 433 which only slightly smaller than the misfit of 0 436 m3 s for the mangin model and the misfit of 0 447 m3 s for the fiorillo model this is not surprising because the simulations of the three models fit the observations almost equally well as shown in fig 6 this is attributed to the large matrix flow during the recession period which can be simulated by all the three models in other words the mixed drainage stage of the new model is essentially a matrix drainage stage and the separation of conduit flow and matrix flow in the mixed flow period does not improve the goodness of fit of the new model for the second recession period fig 7b when the conduit flow is substantial during the mixed flow period the new model with two conduits layers and the fiorillo model outperform the mangin model and the new model with two conduit layers outperforms the fiorillo model this is evidenced by the calculated misfit values which are 1 895 4 286 and 2 747 m3 s for the new model with two conduit layers the mangin model and the fiorillo model respectively the reason that the new model with two conduit layers and the fiorillo model outperform the mangin model is that the mangin model assumes that both conduit flow unsaturated flow and matrix flow saturated flow contribute to the fast flow unsaturated flow in other words the mangin model overestimates the early spring hydrograph because the model assumes that matrix flow occurs during the entire recession period as shown in fig 6f there are two reasons that the results of the new model with two conduit layers are better than those of this study s implementation of the fiorillo model one reason is that the new model separates matrix flow and conduit flow during the mixed drainage stage considering that the two models use the same equations for the conduit drainage stage and the matrix drainage stage table 2 the other reason is that the new model uses two conduit layers to represent different levels of karstification the latter reason is more important than the former reason which is self evident because the new model with only one conduit layer cannot satisfactorily simulate the measured spring discharge the new model has the following two flexibilities 1 since the new model separates the mixed flow into conduit flow and matrix flow it can use the linear equation and the exponential model for simulating conduit flow and matrix flow respectively 2 since the new model considers multiple conduit layers it can uses multiple linear equations to simulate the variation of conduit flow which may be caused by hydraulic head differences between the matrix and conduit reservoirs as observed at the aumelas thau karst system in france and the santa fe karst system in florida bailly comte et al 2010 these flexibilities are not available in this study s implementation of the fiorillo model it is possible that using multiple exponential equations during the mixed flow period may improve the results of the fiorillo model 4 3 estimated discharge volume and effective porosity table 3 lists the estimated effective porosity for the two recession periods the table also lists the estimated discharge volumes and head ranges needed for the porosity estimation the head ranges are listed in the format of heads at the beginning and ending time of each drainage stage for example the range of 6 06 m 5 93 m corresponds to h 1 m h 2 m the matrix heads of the beginning and ending time of the mixed drainage stage for recession period 1 the drainage area ac needed for the porosity estimation takes the value of 259 km2 srwmd 2004b the average hydraulic head h 2 c takes the value of 0 16 m the lowest hydraulic head at the monitored wells the impacts of the assumed h 2 c value are discussed below the table suggests that for the two recession periods while hydraulic head in the matrix decreases slightly from 6 06 m to 5 89 m in the first recession period and from 5 72 m to 5 28 m in the second recession period hydraulic head in the conduit reservoir decreases substantially from 8 24 m to 0 16 m in the first recession period and from 9 46 m to 0 16 m in the second recession period the head variations indicate that the conduit flow is more dynamics than the matrix flow as a result the estimation of matrix porosity is more stable than the estimate of conduit porosity as discussed below for the two recession periods groundwater discharge from the matrix reservoir is substantially larger than that from the conduit reservoir for the first recession period the groundwater discharges from the matrix and conduit reservoirs are 5 191 925 m3 and 1 220 991 m3 respectively for the second recession period the groundwater discharges from the matrix and conduit reservoirs increase to 12 737 966 m3 and 5 319 136 m3 respectively it is expected that the groundwater discharge from the matrix reservoir is larger than that from the conduit reservoir considering that the estimated matrix flow is substantially larger than the estimated conduit flow in this study area fig 6c and d the increase of groundwater discharge in the second recession period is reasonable considering that the precipitation before the second recession period is about twice as large as the precipitation before the first recession period table 1 therefore the estimated values of matrix and conduit porosity of the second recession period should better reflect the karst system than those of the first recession period table 3 indicates that while the estimated matrix porosity is consistent for the two recession periods the estimated conduit porosity varies substantially between the two recession periods for example the estimated conduit porosity of the mixed drainage stage in the first recession period is about one order of magnitude smaller than those in the second recession period the estimated conduit porosity of the second recession period should be more reasonable than that of the first recession period because the conduit flow of the second recession period is substantially larger than that of the first recession period fig 6d in other words the conduit reservoir is filled with more water in the second recession period than in the first recession period since the value of conduit head h 2 c is assumed to be 0 16 m it is necessary to evaluate the impact of the assumed value on the estimation of effective conduit porosity fig 8 plots the variation of estimated conduit porosity of the two conduit layers for a number of h 2 c values between 0 16 m and 1 m which was used by li et al 2013 2016 for studying the hydrograph of the st marks spring that is close to the madison blue spring the figure shows that the impact is small for conduit layer 1 in terms of the absolute difference for example when the conduit head h 2 c increases from 0 16 m to 1 m the estimated effective porosity increases from 0 010 to 0 012 for the first recession period and from 0 17 to 0 20 for the second recession period the absolute increase is larger for conduit layer 2 which is from 0 041 to 0 048 for the first recession period and from 0 37 to 0 44 for the second recession period the larger impact on conduit layer 2 than on conduit layer 1 is reasonable because conduit layer 2 is closer to the conduit vent than conduit layer 1 is when the relative increase of effective conduit porosity is calculated for the two conduit layers and the two recession periods the relative increase is about 20 which is significant therefore it is necessary to have a reasonable estimation of conduit head h 2 c 4 4 evaluation of estimated effective porosity the estimated effective porosity of the matrix reservoir is evaluated by comparing the estimated porosity with the measured from rock samples matrix porosity at two wells w 15515 and w 15537 completed in 1984 in the study area hoenstine 1990 upchurch 2004 fig 9 shows the vertical profiles of measured matrix porosity along the two wells while the measured matrix porosity varies substantially with depth the average matrix porosity is about 12 for the two wells close to the estimated values listed in table 3 it suggests that the estimated effective porosity of the matrix reservoir is reasonable since the estimated matrix porosity is not for the entire aquifer but for the portion of the aquifer where matrix head varies during the recession periods we calculate the average matrix porosity for the intervals where matrix head varies for the two wells the intervals were marked by the grey boxes based on the measured head of upchurch 2004 and choenstine 1990 the average matrix porosity is 13 5 for well 15 515 and 10 5 for well 15537 these average values bracket the estimated matrix porosity listed in table 3 suggesting that the estimated matrix porosity is reasonable it should be noted that the matrix porosity of the floridan aquifer can be as high as 30 budd and vacher 2004 peterson and wicks 2005 since on site measurements of conduit porosity are not available the estimated effective porosity values of conduit reservoirs listed in table 3 are compared with literature data the estimated values are comparable with the estimate channel porosity values listed in worthington 1999 and worthington et al 2000 which are 0 003 for smithville ontario canada 0 06 for mammoth cave kentucky usa 0 02 for the chalk england and 0 5 for nohoch nah chich yucatan mexico except the low value of 0 003 the literature values are of the same order of magnitude with those listed in table 3 at different drainage stages the matrix porosity and channel porosity specific to the cenozoic limestone at yucatan are of particular interest due to similar geology between yucatan and florida the estimated matrix porosity of the yucatan limestone is 17 about 42 larger than the estimate of 12 in this study the estimated channel porosity of the yucatan limestone is 0 5 about 35 larger than the largest estimate of 0 37 in this study this different is not unreasonable because the site area of this study is supposed to be smaller than that of the yucatan limestone worthington 1999 and worthington et al 2000 also listed the proportion of aquifer storage in matrix fracture and channel which are 99 7 for smithville 96 4 for mammoth cave 99 9 for the chalk and 96 6 for nohoch nah chich the matrix storage is conceptually similar to the matrix flow in this study although not the same and table 3 indicates that the matrix storage is about 97 for recession period 1 and 83 92 for recession period 2 this suggests that the estimated amount of flow and the estimated reservoir porosity are comparable to those reported in literature however it should be noted that the comparison is qualitative only and caution should be taken for example in worthington et al 2000 channel is referred to as all interconnected disolutional enlargements along joints faults and bedding planes this definition is certainly broader than the concept of conduit reservoir used in this study 5 conclusions this paper presents a new model for simulating karst spring recession curves the new model has the following three characteristics 1 the model considers two separate but hydraulically connected reservoirs matrix reservoir and conduit reservoir 2 the model separates a recession period of a karst spring hydrograph into three drainage stages conduit drainage stage with only conduit flow in conduits mixed drainage stage with both conduit flow in conduits and matrix flow from matrix to conduits and matrix drainage stage with only matrix flow from matrix to conduits and 3 in the mixed drainage stage the model uses multiple conduit layers to present different levels of conduit development the new model is conceptually similar to the fiorillo model but has two unique features for handling the mixed drainage stage i e explicitly separating conduit flow and matrix flow and using multiple conduit layers to represent different levels of karst development when using the new model the mangin model and the fiorillo model for simulating the observed spring discharge at the madison blue spring the new model and the fiorillo model outperform the mangin model because the mangin model assumes continuous matrix flow from the beginning of the recession and thus overestimates the early spring discharge due to the flexibilities of using linear equation and exponential equation to simulate conduit flow and matrix flow respectively and of using multiple conduit layers for the mixed drainage stage the results of the new model are better than those of this study s implementation of fiorillo model the results of the fiorillo model may be improved by using multiple exponential functions and this exploration is warranted in future studies based on the new model of simulating the recession curves of karst spring hydrograph the groundwater discharge volumes from the matrix and conduit reservoirs are estimated the results indicate that the discharge from the matrix reservoir is substantially larger than the discharge from the conduit reservoir especially in the first recession period when conduit flow is smaller than the second recession period as a result the estimated effective porosity of the conduit reservoir for the second recession period better characterize the conduit reservoir than that for the first recession period while there is lacking field measurements to evaluate the estimated effective porosity of the conduit reservoir the estimated effective porosity of the matrix reservoir agrees with the porosity values measured from borehole samples at the study site the new model of simulating the recession curves of karst spring hydrograph has four limitations the first limitation is that the new model requires a relatively long recession period until the matrix drainage stage appears on the karst spring hydrograph the requirement for a relatively long recession period is resulted from the procedure of hydrograph separation i e steps 3 and 4 described in section 2 1 specifically speaking the recession coefficient α 2 is first estimated for the matrix drainage stage and its value is assumed for the recession coefficient α 1 for simulating the matrix flow of the mixed drainage stage the long recession periods may not occur often for areas e g florida with frequent rainfall events this limits the applicability of the new model for simulating karst spring hydrograph and for characterizing karst aquifers the second limitation is that the estimation of effective porosity of matrix and conduit reservoirs requires knowing the springshed area ac and the conduit hydraulic head h 2 c at the end of the mixed drainage stage while the area ac can be estimated based on available hydrologic information the area is assumed to be a constant over time which may not be realistic especially when hydrologic conditions change dramatically over time in florida in addition the estimated springshed does not consider groundwater flow from adjacent springsheds and karst spring hydrograph may only represent a fraction of the springshed for conduit head h 2 c it cannot be directly estimated because measurements of conduit head are always lacking the third limitation is that the new model is only applicable to unconfined karst aquifers for which the karst spring recession curve can reflect the internal structure and properties of the karst aquifers for example the effective porosities defined in this study are only applicable to unconfined karst aquifers because head variation of confined aquifers does not reflect the volumes of matrix and conduit reservoirs the last limitation of this research is that it cannot provide a systematic way of evaluating the estimated conduit porosity since there is always lacking of measurements of conduit porosity it is necessary to evaluate the estimated conduit porosity using other approaches one of the approaches is the hydrochemical model of de rooij and graham 2017 that can explicitly characterize karst conduit networks and estimate conduit porosity worthington 2015 provided more insights on karst conduit formation and its characterization from the hydrogeological perspective future research is warranted for linking this research which is statistical in nature with other theoretical researches for better characterizing conduits of karst aquifers acknowledgements this research was supported in part by the national key r d program of china grant 2017yfc0804102 the first author was supported by the china scholarship council for his research in the department of scientific computing at the florida state university the second author was supported by national science foundation grantear 1828827 
7245,quantitative analysis of recession curves of karst spring hydrographs is a vital tool for understanding karst hydrology and inferring hydraulic properties of karst aquifers this paper presents a new model for simulating karst spring recession curves the new model has the following characteristics 1 the model considers two separate but hydraulically connected reservoirs matrix reservoir and conduit reservoir 2 the model separates karst spring hydrograph recession into three stages conduit drainage stage mixed drainage stage with both conduit drainage and matrix drainage and matrix drainage stage and 3 in the mixed drainage stage the model uses multiple conduit layers to present different levels of conduit development the new model outperforms the classical mangin model and the recently developed fiorillo model for simulating observed discharge at the madison blue spring located in northern florida this is attributed to the latter two characteristics of the new model based on the new model a method is developed for estimating effective porosity of the matrix and conduit reservoirs for the three drainage stages the estimated porosity values are consistent with measured matrix porosity at the study site and with estimated conduit porosity reported in literature the new model for simulating karst spring hydrograph recession is mathematically general and can be applied to a wide range of karst spring hydrographs to understand groundwater flow in karst aquifers the limitations of the model are discussed at the end of this paper keywords karst aquifer karst spring hydrograph recession curve hydrograph separation effective porosity conduit flow 1 introduction quantitative analysis of the recession curves of karst spring hydrographs is a vital tool for various activities of water resources management in karst areas such as calculating water budgets estimating base flow rates protecting aquatic ecosystems and developing ecotourism resources under climate change fiorillo 2009 ghasemizadeh et al 2012 stevanovic 2015 a recession curve which is a falling limb after a peak on a spring hydrograph can be viewed as an indicator of overall aquifer behaviors during a period without precipitation kiraly 2003 chang et al 2015 fu et al 2016 analyzing the recession curves helps characterize a karst aquifer because hydrodynamic characteristics and hydraulic properties of the karst aquifer determine the shapes of recession curves bonacci 1993 dewandel et al 2003 fiorillo 2011 for example analysis of the recession curves is the basis for estimating effective aquifer porosity and many methods have been developed in literature the methods for estimating effective porosity can be categorized into direct methods e g borehole drilling speleological survey cave diving camera recording and logging and remote controlled vehicle and indirect methods e g tracing tests geophysical survey time series analysis isotopic analysis and spring hydrograph analysis boussinesq 1877 1904 maillet 1905 bonacci 1993 dewandel et al 2003 kovács 2003 kovács et al 2005 stevanovic et al 2010 fiorillo 2011 2014 among these methods the recession curves analysis has several advantages such as being cost effective and providing large scale aquifer properties and needing fewer parameters mangin 1975 bonacci 1993 dewandel et al 2003 kiraly 2003 kovács et al 2005 bailly comte et al 2010 ford and williams 2013 chang et al 2015 goldscheider 2015 stevanovic 2015 the choice of appropriate karst hydrogeology methods depends on the practical and or scientific research questions to be answered the level of understanding of the system to be studied and the amount of resources available goldscheider and drew 2007 ford and williams 2013 goldscheider 2015 this paper presents a new model for simulating the recession curves of karst spring hydrograph the new model uses matrix and conduit reservoirs to represent a karst aquifer which is a common feature of many models developed for karst spring hydrograph analysis martin and dean 2001 martin and screaton 2001 kiraly 2003 geyer et al 2008 shoemaker et al 2008 bailly comte et al 2010 ford and williams 2013 chang et al 2015 the new model is conceptually similar to the widely used mangin model mangin 1975 and the fiorillo model fiorillo 2011 but has its own features the mangin model uses two parallel reservoirs a quick flow reservoir for simulating discharge from unsaturated zones e g conduits and a slow flow reservoir for simulating discharge from saturated zones e g matrix the fiorillo model conceptualizes a karst aquifer as a series of tank reservoirs i e the torricelli reservoir for conduits the darcy reservoir for matrix and the poiseuille reservoir for fractures similar to the mangin model the new model of this study uses two reservoirs i e a conduit reservoir and a matrix reservoir the conduit reservoir is different from that of the mangin model but similar to the conduit reservoir of fiorillo model the matrix reservoir also differs from that of the mangin model and includes the matrix and fracture reservoirs of the fiorillo model because discharge from the matrix and fracture reservoirs can be simulated by two equations of the same form but with different physical meanings fiorillo 2011 similar to the conceptualization of taylor and greene 2008 the new model of this study separates a karst spring hydrograph into three stages the conduit drainage stage spring discharge from drainage conduit flow of conduit reservoir the mixed drainage stage spring discharge from both drainage conduit flow of conduit reservoir and drainage matrix flow of matrix reservoir first to conduit reservoir and then to spring and the matrix drainage stage spring discharge from drainage matrix flow of matrix reservoir first to conduit reservoir and then to spring using the three stages for hydrograph separation distinguishes the new model from the mangin model because the mangin model considers that matrix flow contributes to spring discharge for the entire recession period whereas the new model does not consider matrix flow in the conduit drainage stage of the recession period the reason of not considering the matrix flow is that groundwater flows from conduits to matrix during the conduit drainage stage given that hydraulic head in the conduits is higher than the hydraulic head in the matrix while the new model is similar to the fiorillo model to separates a hydrograph into three stages the new model differs from the fiorillo model in that the new model has the mixed drainage stage the fiorillo model does not use the concept of mixed drainage for the period between the conduit drainage stage and the matrix drainage stage for this between period the fiorillo model considers that the spring discharge is from well connected fissures and uses the poiseuille reservoir to represent the fissures the proposed mixed drainage stage is consistent with the observation that water level in conduits continues decreasing in the period between the conduit drainage stage and the matrix drainage stage which indicates that conduit flow exists in the period shevenell 1996 taylor and greene 2008 if conduit flow is negligible during the mixed drainage stage the mixed flow becomes the matrix flow only or the flow from the poiseuille reservoir in the fiorillo model as shown in section 4 below due to using the mixed drainage concept and enabling the flexibility of multiple conduit layers the results of the new model are better than those of the current implementation of the fiorillo model for simulating the observed discharge at the madison blue spring located in northern florida the new model of karst spring hydrograph analysis provides a basis for estimating groundwater flow from matrix and conduit reservoirs separately and for estimating effective porosity of matrix and conduit reservoirs separately estimating matrix and conduit drainage separately helps understand the relative significance of conduit drainage and matrix drainage in a recession period since effective porosity is an important aquifer property for managing water resources of karst areas fu et al 2016 many methods have been developed to estimate effective porosity of karst aquifers boussinesq 1904 bonacci 1993 shevenell 1996 szilagyi 1999 dewandel et al 2003 kovács et al 2005 fiorillo 2011 2014 however these methods consider a karst aquifer as a whole and do not separate matrix flow and conduit flow therefore the existing methods cannot estimate effective porosity of conduit and matrix reservoir separately which may limit our understanding of karst aquifers this problem is resolved in this study by separating matrix flow and conduit flow in addition multiple conduit layers are used to represent different levels of conduit development and the effective porosity of each conduit layer is estimated estimating effective matrix and conduit porosity for the three drainage stages enables us to better characterize the karst aquifer for the entire recession period the rest of the paper is organized as follows the new model for simulating recession curves of karst spring hydrograph the method of estimating conduit and matrix flows and the method of estimating effective porosity of matrix and conduit reservoirs are discussed in section 2 the new model was used for simulating observed spring discharge at the madison blue spring and the field site and observations of hydraulic head and discharge are described in section 3 two recession periods with different hydrological and groundwater conditions are chosen for evaluating the simulations given by the three models i e the new model of this study the mangin model and the fiorillo model the simulation results and the evaluation of the three models are given in section 4 this section also discusses the results of estimating matrix and conduit drainage as well as effective porosity of matrix and conduit reservoirs the major conclusions of this research are given in section 5 2 methodology this section starts with a detailed description of the new model of karst spring hydrograph analysis in section 2 1 section 2 2 includes a brief description of the mangin model and the fiorillo model that are compared with the new model in this study the method of estimating matrix and conduit flows and the method of estimating effective porosity of matrix and conduit reservoirs are described in section 2 3 2 1 new model of simulating recession curves of karst spring hydrograph the new model of simulating the recession curves of karst spring hydrograph has the following characteristics 1 the matrix and conduits of a karst aquifer are considered as two separate reservoirs this conceptualization of karst aquifer has been widely used for simulating karst spring hydrograph 2 a recession period on a karst spring hydrograph is separated into three stages conduit drainage stage stage i mixed drainage stage stage ii and matrix drainage stage stage iii as shown in fig 1 during the mixed drainage stage the conduit flow in conduits and matrix flow from matrix to conduits are separated explicitly in the new model 3 in the mixed drainage stage multiple conduit layers are used to represent different levels of conduit development fig 1 note that the conduit layers are different from the tanks of the fiorillo model that includes both conduits and matrix explicitly separating matrix and conduit flows and using multiple conduit layers for representing different levels of karst development are two unique features of the new model for simulating spring discharge of the recession periods of karst spring hydrograph fig 2 illustrates the three characteristics discussed above and the conceptual model of the dynamics of matrix and conduit flows for understanding karst spring hydrograph recession during the conduit drainage stage stage i fig 2a conduit flow is the only source of spring discharge and hydraulic head in the conduit reservoir decreases from h 0 c to h 1 c from t 0 to t 1 the spring discharge q t i l3t 1 of the conduit drainage stage for time t 0 t t 1 can be evaluated using the equations below derived by following fiorillo 2011 based on the conceptualization of torricelli reservoir of fiorillo 2011 the spring discharge q t i is expressed as 1 q t i a 2 c 2 gh t c where a 2 c is the area of spring outlet l2 g is the gravity acceleration lt 2 and ht c is hydraulic head of the conduit reservoir l based on the principle of mass balance that the variation rate of water storage in the conduit reservoir equals to the spring discharge during the hydrograph recession we have the differential equation of ht c as bailly comte et al 2010 fiorillo 2011 2 a 1 c dh t c dt q t i a 2 c 2 gh t c where a 1 c is the horizontal area of the conduit reservoir l2 integrating eq 2 for time t 0 to t 1 and for the hydraulic head from h 0 c to h 1 c gives fiorillo 2011 3 2 h t c 2 h 0 c a 2 c a 1 c 2 g t substituting eq 1 into eq 3 gives kullman 1990 fiorillo 2011 4 q t i q 0 i γ t for t 0 t t 1 where q 0 i a 2 c 2 gh 0 c is the initial spring discharge at time t 0 l3t 1 and γ a 2 c 2 a 1 c g is the recession coefficient of the conduit reservoir l3t 2 to use eq 4 for simulating karst spring discharge requires estimating q 0 and γ based on field measurements of spring discharge when estimating γ from discharge measurements the two areas a 1 c and a 2 c are not needed for using eq 4 in the conduit drainage stage stage i hydraulic head in the matrix increases from h 0 m to h 1 m fig 2a due to infiltration of rainfall into the matrix the discharge of conduit flow to the matrix is considered to be negligible according to peterson and wicks 2005 who found that the volume of fluid penetrating from flooded conduits into the matrix is less than 1 of the volume of fluid flowing in the conduits while the finding of peterson and wicks 2005 may not be the case for the floridan aquifer with relatively large secondary porosity the ignorance of the discharge of conduit flow into matrix appears to be valid in the field application of the new model as described below exploring the influence of the discharge volume i e bank storage on the recession mechanism and the recession curve is warranted in a future study when the hydraulic head in the conduit reservoir decreases from h 0 c to h 1 c from t 0 to t 1 and when the hydraulic head in matrix reservoir increases from h 0 m to h 1 m from t 0 to t 1 that equals to h 1 c the mixed drainage stage stage ii starts fig 2b during this stage the hydraulic head in the conduit reservoir continues decreasing from h 1 c to h 2 c from t 1 to t 2 and the hydraulic head in the matrix reservoir starts decreasing from h 1 m to h 2 m from t 1 to t 2 since the head decreases in the conduit reservoir is faster than that in the matrix reservoir groundwater discharges from the matrix to conduits and the spring discharge is composed of both conduit flow and matrix flow although the exchange mechanism between the conduit reservoir and the matrix reservoir is complicated it is reasonable to assume that matrix flow and conduit flow are independent i e the matrix flow do not affect the conduit flow and vice versa peterson and wicks 2005 malík and vojtková 2012 li and field 2013 li et al 2016 following the assumption the matrix flow and conduit flow are simulated separately the matrix flow for time period t 1 t t 2 can be evaluated via maillet 1905 kovács et al 2005 5 q t m ii q 1 m e α 1 t t 1 where q t m ii is the discharge rate of the matrix reservoir l3t 1 q 1 m is the starting matrix discharge at time t t 1 when h 1 m h 1 c l3t 1 and α 1 t 1 is the recession coefficient of the matrix reservoir during the mixed drainage stage the parameters q 1 m and α 1 are estimated from field measurements of spring discharge as described below fiorillo 2011 derived eq 5 for darcy reservoir of porous media and for poiseuille reservoir of fractured media since eq 5 can be used for evaluating both matrix flow and fracture flow the new model of this study does not distinguish matrix and fracture reservoirs but calls both of them as matrix reservoir note that eq 5 is not used for simulating the recession curve in the conduit drainage stage for the conduit flow in the mixed drainage stage stage ii the new model uses multiple conduit layers to represent different levels of karstification in depth fig 2b for the i th conduit layer denoted as li similar to the derivation of eq 4 the discharge rate for tlis c t tlie c tlis c and tlie c being the starting and ending times of groundwater drainage from conduit layer li respectively is derived as 6 q t li c ii q lis c β i t t lis c where q t li c ii is the discharge rate of the i th conduit layer l3t 1 q lis c is the starting discharge from the i th conduit layer l3t 1 β i is the recession coefficient for the i th conduit layer l3t 2 tlis c is the starting time of the discharge from the i th conduit layer t in this study coefficients qlis c βi and tli c of eq 6 are estimated from field measurements of spring discharge as described below combining eqs 5 and 6 gives the spring discharge from the i th conduit layer and the matrix reservoir as 7 q t li ii q t li c ii q t m ii q lis c β i t t lis c q 1 m e α 1 t t 1 during the mixed drainage stage stage ii the total spring discharge from the matrix reservoir and all the conduit layers is 8 q t li ii i q t li c ii q t m ii i q lis c β i t t lis c q 1 m e α 1 t t 1 for an aquifer with a high level of karstification the matrix flow may be negligible when the conduit flow is negligible our model is similar to the mangin model and the fiorillo model in that the spring discharge is controlled mainly by the matrix flow when the conduit flow is not negligible and both matrix flow and conduit are important our model is expected to provide better simulation to measured spring discharge than the mangin model and the fiorillo model do which is demonstrated in the real world application in section 4 the relative importance of matrix flow and conduit flow depends on the level of karstification as different levels of karstification result in the vertical variation of hydraulic conductivity and porosity milanovic 1981 kullman 1990 fiorillo 2011 based on hydrograph recession analyses of nine gauged springs located in a slovak aquifer malík and vojtková 2012 provided the link between recessional equations and a total of ten karstification degrees defined by the authors the recessional equations only include matrix flow for karstification degree less than four the recessional equations include both matrix flow and conduit flow for karstification degree between four and eight and the recessional equations include only conduit flow for karstification degree larger than eight it however should be noted that the definitions of the karstification levels are site specific and they should be used for other karst aquifers with cautions as shown in fig 2c the matrix drainage stage stage iii starts when the hydraulic head in the conduit reservoir decreases from h 1 c to h 2 c at time t t 2 and stabilizes at h 2 c meanwhile the hydraulic head in the matrix reservoir continues decreasing from h 2 m until the next rainfall event in this stage the recession curve of karst spring hydrograph is mainly controlled by the baseflow from the matrix reservoir according to kovács et al 2005 the hydraulic head h 2 c acts as the fixed head boundary when spring discharge is mainly controlled by the matrix reservoir because the conduit network has no influence on the spring discharge and negligible storage in the conduit reservoir contributes to spring discharge following the literature maillet 1905 kovács et al 2005 bailly comte et al 2010 malík and vojtková 2012 goldscheider 2015 the matrix flow q t iii l3t 1 is evaluated via 9 q t iii q 2 m e α 2 t t 2 where q 2 m is the discharge rate of the matrix reservoir at time t t 2 l3t 1 and α 2 is the recession coefficient of the matrix reservoir t 1 using eq 9 for simulating the spring hydrograph requires estimating q2 m and α 2 based on field measurements of spring discharge the procedure of using the equations above to simulate karst spring hydrograph is as follows 1 separate a karst spring hydrograph into the three stages conduit drainage mixed drainage and matrix drainage in two steps the first step is to determine time t 1 the end of the conduit drainage stage stage i since the flow rate of this stage is a linear function of time eq 4 t 1 is determined as the time when the linear hydrograph ends the second step is to determine time t 2 the beginning of the matrix drainage stage stage iii by first plotting the logarithm of the hydrograph with time time t 2 is selected as the beginning of linear semi log plot with respect to time because the semi log plot is a straight line with time eq 9 after t 1 and t 2 are determined the karst spring hydrograph is separated into the conduit drainage stage mixed drainage stage and matrix drainage stage 2 for the conduit drainage stage stage i by fitting eq 4 to discharge measurements to estimate q 0 i and γ 3 simulation the matrix drainage stage stage iii by fitting eq 9 to discharge measurements to estimate q2 m and α 2 4 simulate the matrix flow of the mixed drainage stage stage ii by using eq 5 which requires estimating parameters α 1 and q 1 m of the equation assuming that the matrix reservoir behaves in the same way for producing matrix flow in the matrix flow and mixed drainage stages assign the value of α 2 estimated above to α 1 which is also used in literature mangin 1975 kovács et al 2005 kovács and perrochet 2008 to estimate q 1 m because the matrix flow of the mixed drainage stage equals to the matrix flow of the matrix drainage stage at t t 2 we set q t m ii q 1 m e α 2 t 2 t 1 q 2 m based on eqs 5 and 9 this leads to q 1 m directly 5 simulate the conduit flow of the mixed drainage stage stage ii by subtracting the simulated matrix flow in step 4 above from the discharge measurements this results in the conduit flow from all conduit layers according to eq 8 since the conduit flow of each conduit layer is a linear function of time eq 6 the hydrograph of the conduit flow can be separated into multiple linear segments to determine tlis c the start time of the discharge from the i th conduit layer denoted as li afterward qlis c and βi of eq 6 can be estimated by fitting the equation to corresponding measurements of spring discharge the number of conduit layers is determined empirically for achieving satisfactory fit to measured spring discharge the real world application of the new model below shows that using two conduit layers achieves significantly better fit than using one conduit layer the curve fitting operations above is carried out manually and automatic curve fitting can be done using regression techniques draper and smith 1998 2 2 comparison with two other models the new model of this study is compared with two widely used models the mangin model and the fiorillo model as shown in fig 3 a the mangin model does not separate a karst aquifer into matrix and conduit reservoirs instead the model consists of a reservoir of saturated zone e g conduits and a reservoir of unsaturated zone e g matrix the spring discharge from the two reservoirs is calculated via mangin 1975 10 q t ψ t φ t q q 1 η t 1 ε t q b e α b t where ψ t is the discharge from unsaturated zone l3t 1 φ t is the discharge from saturated zone l3t 1 qq is the maximal infiltration flowrate l3t 1 η t 1 is the inverse of the period of infiltration ε is the concavity of the recession curve resulting from infiltration heterogeneity qb is the initial saturated flow rate l3t 1 and αb is a recession coefficient t 1 all the coefficients qq η ε qb and αb are estimated based on measurements of spring discharge more details of the mangin model and its application are referred to literature mangin 1975 dewandel et al 2003 ford and williams 2013 fu et al 2016 as shown in fig 3b the fiorillo model considers a series of tank reservoirs the first reservoir denoted as 01 in fig 3b is the torricelli tank reservoir that represents drainage from shafts and upper conduits and its flow rate is estimated as fiorillo 2011 11 q t q 0 1 α t t where q 0 1 is the initial discharge of the torricelli tank reservoir and αt is the recession coefficients of the reservoir t 1 other reservoirs are the darcy and poiseuille reservoirs and their discharges are estimated as fiorillo 2011 12 q t i q 0 i e α i t where q0 i l3t 1 is the initial discharge of the i th reservoir and αi t 1 is the recession coefficient of the i th reservoir all the coefficients q 0 1 αt q0 i and αi are estimated based on measurements of spring discharge more details of the fiorillo model and its application are referred to fiorillo 2011 to compare the new model of this study with the two models above the three models are used to simulate real world measurements of the discharge of the madison blue spring located in northern florida and the misfit between simulated and measured discharge is used as the criterion for evaluating the model performance the misfit is calculated as 13 misfit i 1 n r i where r i is the residual between simulated and measured discharge 2 3 estimation of discharge volume and effective porosity based on the hydrograph modeling above we can estimate the discharge volumes from the matrix and conduit reservoirs which in turn can be used to estimate effective porosity of the matrix and conduit reservoirs this section starts with the estimations for the mixed drainage stage stage ii and then discusses the estimations for the other two drainage stages fig 1 illustrates the separation of matrix discharge and conduit discharge for the mixed drainage stage the conduit discharge is further separated to the discharge from each conduit layer these make it possible to explicitly estimate the effective porosity of the matrix reservoir and the effective porosity of individual conduit layers for the mixed drainage stage the effective porosity of the matrix reservoir n m ii is defined as the volume of water discharged from the matrix reservoir when hydraulic head of the matrix reservoir decreases from h 1 m to h 2 m i e from time t 1 to time t 2 shown in fig 2b and estimated in step 1 of section 2 1 above i e 14 n m ii v m ii h 1 m h 2 m a c where v m ii is the groundwater discharge from the matrix reservoir during the mixed drainage stage stage ii and ac is the drainage area of the karst aquifer while ac is always available for a groundwater basin h 1 m h 2 m and v m ii need to be estimated since h 1 m and h 2 m are the average hydraulic head of the matrix reservoir the isoline method described in gupta 2016 is used for estimating the average heads in the isoline method contours of the hydraulic head are first generated using measurements of hydraulic head and the weighted average of hydraulic head h is evaluated via 15 h i 1 n a i h i 1 h i 2 i 1 n a i where h i 1 and h i are the values of the hydraulic head for a pair of isoclines and a i is the area between the pair of isolines the areas are used as the weights for estimating the average hydraulic head of the matrix reservoir after a time series of the average head is obtained based on head measurements h 1 m for time t 1 and h 2 m for time t 2 can be approximated the volume of groundwater discharge v m ii from the matrix reservoir is estimated by integrating the matrix flow eq 5 from t 1 to t 2 as 16 v m ii t 1 t 2 q t m ii dt t 1 t 2 q 1 m e α 1 t t 1 dt q 1 m q 2 m α 1 where q 1 m is estimated in step 4 and q2 m and α 1 are estimated in step 3 of section 2 1 above for the mixed drainage stage the effective porosity of the conduit reservoir n c ii is defined as the volume of water discharged from the conduit reservoir when hydraulic head of the conduit reservoir decreases from h 1 c and h 2 c i e from time t 1 to time t 2 shown in fig 2b and estimated in step 1 of section 2 1 above the effective porosity n li c ii of the i th conduit layer denoted as li is defined as 17 n li c ii v li c ii h lis c h lie c a c where v li c ii is the volume of groundwater discharged from the i th conduit layer and hlis c and hlie c are the starting and ending hydraulic heads when groundwater discharges from the i th conduit layer hlis c and hlie c correspond to tlis c and tlie c used in eq 6 respectively and the starting and ending discharge times tlis c and tlie c are obtained in step 5 of section 2 1 above the discharge volume v li c ii is estimated by integrating the conduit flow eq 6 from tlis to tlie the starting and ending times respectively when groundwater discharges from the i th conduit layer as 18 v li c ii t lis c t lie c q t li c ii dt t lis c t lie c q lis c β i t t lis c dt q lis c t t lis c 1 2 β i t t lis c 2 t lis c t lie c q lis c 2 2 q lis c β i t lie c t lis c β i t lie c t lis c 2 2 β i q lis c 2 2 q lis c β i t lis c t lis c β i t lis c t lis c 2 2 β i q lis c β i t lie c t lis c 2 2 β i q lis c β i t lis c t lis c 2 2 β i q lis c 2 q lie c 2 2 β i where q lis c q lie c and β i are estimated in step 5 of section 2 1 above since there is no measurement of conduit head the isoline method above cannot be used for estimating average conduit head e g hlis c and hlie c which is a theoretical weakness of the new model as conduit heads are rarely available in practice to address this problem this study estimates hydraulic head of each conduit layer by linking conduit head with conduit discharge the estimation starts from hl1s c and hl1e c the starting and ending hydraulic heads when groundwater discharges from the first conduit layer as shown in fig 2b for the starting hydraulic head we have hl1s c h 1 c h 1 m for estimating the ending hydraulic head hl1e c we relate the hydraulic head with the conduit discharge at the beginning and the ending time of the discharge period which are denoted as ql1s c and ql1e c respectively by virtue of eq 1 we have 19 q l 1 s c 2 g h l 1 s c h 2 c q l 1 e c 2 g h l 1 e c h 2 c taking the ratio between ql1e c and ql1s c leads to 20 q l 1 e c q l 1 s c h l 1 e c h 2 c h l 1 s c h 2 c rearranging the equation gives 21 h l 1 e c h l 1 s c h 2 c q l 1 e c q l 1 s c 2 h 2 c considering hl1s c h1 m q l 1 s c q t l 1 c ii and q l 1 e c q t l 2 c ii eq 21 becomes 22 h l 1 e c h 1 m h 2 c q t l 1 c ii q t l 2 c ii 2 h 2 c this equation can be readily evaluated because h 1 m is estimated using eq 15 and in addition q t l 1 c ii and q t l 2 c ii are estimated in step 5 of section 2 1 above for the i th conduit layer beneath the first layer eq 21 becomes 23 h lie c h lis c h 2 c q t li c ii q t li 1 c ii 2 h 2 c where h lis c h li 1 e c i e the starting head of the i th conduit layer is the ending head of the i 1 th conduit layer for the bottom conduit layer h lie c h 2 c the average hydraulic head in the conduit reservoir at t 2 fig 2c estimating h 2 c is more difficult than estimating h 2 m because measurements of hydraulic head in conduit reservoir are always lacking an assumed value may be assigned to h 2 c based on measurements of river stage and or matrix hydraulic head the impacts of assumed h 2 c on the estimation of effective porosity of conduit layers are discussed in section 4 of the real world application of the new method of hydrograph modeling the above estimation of effective porosity for the mixed drainage stage can be applied directly to the conduit flow and matrix drainage stages for the conduit drainage stage similar to eq 17 the effective porosity n c i of conduit reservoir is defined as 24 n c i v c i h 0 c h 1 c a c where v c i is the volume of groundwater discharged from the conduit reservoir when conduit head decreases from h 0 c to h 1 c from time t 0 to time t 1 i e the duration of stage i shown in fig 2a the discharge volume v c i can be estimated by integrating conduit flow q t i eq 4 from t 0 to t 1 which similar to eq 18 leads to 25 v i q 0 i 2 q 1 i 2 2 γ in this equation q 0 i and γ are estimated in step 2 of section 2 1 above and q 1 i the discharge at t 1 can be calculated using eq 4 the conduit hydraulic head h 1 c in eq 4 takes the value of h 1 m as discussed above and h 0 c in eq 24 needs to be estimated indirectly due to the lacking of conduit head data following the derivation of eqs 19 23 the expression of h 0 c is derived as 26 h 0 c h 1 c q 0 i q 1 i 2 the effective porosity n m iii of the matrix reservoir for the matrix drainage stage is defined as 27 n m iii v m iii h 2 m h 3 m where v m iii is the amount of groundwater discharge from the matrix reservoir it can be estimated by integrating the matrix flow q t iii eq 9 from t 2 to t 3 which similar to eq 16 gives 28 v iii t 2 t 3 q t m iii dt q 2 m q 3 m α 1 in this equation q 2 m and α 1 are estimated in step 3 of section 2 1 above and q 3 m can be estimated by using eq 9 the matrix heads h 2 m and h 3 m can be obtained from the average matrix head eq 15 discussed above 3 study area and field data fig 4 shows the location of the study area in the madison county florida the madison blue spring shown in the insertion is a first magnitude spring and the spring vent latitude 30 28 49 and longitude 83 14 40 is located on the eastern border of madison county adjacent to the withlacoochee river the spring vent is connected to an unconfined aquifer by a network of phreatic conduits in the suwannee limestone and the underlying ocala limestone the land surface altitude of the spring is 14 63 m above ngvd29 usgs 2017 the spring discharge ranges from 2 to 4 m3 s and the average discharge over the period of 2002 2017 is 2 8 m3 s usgs 2017 the annual average precipitation is 1335 mm noaa 2017 the average annual recharge from precipitation to the karst aquifer is approximately 254 mm copeland 2003 the wet season is june september and the dry season is october may the area of the springshed is estimated to be 259 km2 greenhalgh 2003 srwmd 2004a b and it is used in eqs 14 and 17 for estimating the effective porosity of matrix and conduit layers the aquifer system in the study area consists of from top to bottom a surficial aquifer an intermediate aquifer a confining unit and the floridan aquifer system scott 1988 despite of the existence of the confining unit the entire aquifer system is considered to be poorly confined for two reasons first since the siliciclastic and carbonate horizons in the intermediate aquifer and the confining unit are permeable the intermediate aquifer and the confining unit act as a semi confining unit that separates the surficial aquifer and the floridan aquifer in addition because of sinkholes and eroded zones in the intermediate aquifer and confining unit the floridan aquifer is poorly confined and recharge to the florida aquifer is relatively effective throughout the springshed bush and johnston 1988 grubbs 1998 arthur et al 2005 therefore it is suitable to apply the new model to analyze the karst spring hydrograph of the madison blue spring fig 5 plots the time series of daily precipitation spring discharge and hydraulic head for the period from october 6 2012 to june 6 2017 the spring discharge is monitored at the madison blue spring station by the u s geological and survey usgs usgs 2017 daily precipitation is monitored at the madison blue springs weather station and hopewell tower station fig 4 by the suwannee river water management district srwmd 2017a the average precipitation of the two stations is plotted in fig 5 a groundwater monitoring network has been established in this area by usgs and the water management district the daily hydraulic head is recorded at five monitoring wells blue spring nestle fsc 1 gibson tower westwood west and lovette tower srwmd 2017b all the wells were drilled into the floridan aquifer and fractures and small karst conduits were encountered during drilling for the convenience of hydrograph analysis a reference head h 0 m is set at h 12 4 m above ngvd29 which is the base flow spring water level usgs 2017 srwmd 2017b the hydraulic head at the blue spring well and nestle fsc 1 well are almost identical and overlap together in fig 5 fig 5 shows that the spring discharge and hydraulic head respond quickly to rainfall events in that the discharge and hydraulic head increase immediately after rainfalls it is noted that after hydraulic heads reach the peak values the spring discharge becomes smaller and sometimes becomes negative this is due to backflow of stream water into the spring vent because the water level of the withlacoochee river increases at a rate faster than that of groundwater in the spring vent gulley et al 2011 brown et al 2014 due to the influence of rainfall events and the river water backflow into spring vent not all the spring discharge data are suitable for analyzing the recession curve of karst spring hydrograph therefore this study selects two recession periods of spring discharge for which the influence of rainfall and river water backflow is small the two periods are marked in fig 5 and more details of the two recession periods are given in table 1 the two periods have different characteristics the first period has a shorter period and smaller spring discharge in comparison with the second period in addition the amount of cumulative precipitation 30 days before the first period is smaller than that before the second period the characteristic data of the two periods suggests that the conduit flow in the first period is smaller than that in the second period 4 results and discussion section 4 1 presents the results of simulating the observed data of spring discharge using the new model the mangin model and the fiorillo model followed by a comparison of the simulation results of the three models in section 4 2 the results of estimating groundwater discharge from the matrix and conduit reservoir and of estimating effective porosity of the matrix and conduit reservoirs of the three stages are given in section 4 3 in section 4 4 the estimated effective porosity of the matrix reservoir is compared with measured matrix porosity along two wells at the study site 4 1 simulated spring discharge fig 6 plots the measured and calculated spring discharge by using the new model with one conduit layer the new model with two conduit layers the mangin model and the fiorillo model the fitted equations of the three models are listed in table 2 the reason of considering the two options of using one conduit layer and two conduit layers for the new model during the mixed drainage stage is to investigate to what extent the use of multiple conduit layers to represent different levels of karstification in the conduit reservoir can improve the simulation of measured spring discharge in fig 6a and b for the new model with one conduit layer the two vertical dashed lines in red separate the karst spring hydrograph into the conduit drainage stage the mixed drainage stage and the matrix drainage stage in fig 6c and d for the new model with two conduit layers the vertical dashed lines in blue further separate the mixed drainage stage into two sub stages corresponding to the discharge from the two conduit layers comparing fig 6a b with fig 6c d shows that using two conduit layers improves the simulation of the measured spring hydrograph especially for the second recession period plotted in fig 6b and d and the improvement is quantified below using the misfit defined in eq 13 in fig 6a d for the mixed drainage stage the simulated matrix flow and conduit flow are plotted separately while the matrix flow varies slightly over time the conduit flow has a large variation especially for the second recession period plotted in fig 6b and d while the conduit flow is substantially smaller than the matrix flow in fig 6a and c the conduit flow is comparable with the matrix flow especially in the early time of the mixed drainage stage in fig 6b and d therefore it is necessary to separate conduit flow and matrix flow for the second recession period shown in fig 6b and d fig 6e and f plot the measured and calculated spring discharge by using the mangin model for the two recession periods in each figure the vertical dashed line separates the fast flow ψ t from the unsaturated zone and the slow flow φ t from the saturated zone fig 3a ψ t becomes zero in the period when spring discharge is only from slow flow i e in the periods after the vertical lines while the fitting between the measured and calculated spring discharge is satisfactory in fig 6e the fitting is less satisfactory in fig 6f especially in the early time when the quick flow is significant the reason is that the simulated slow flow in the early time is relatively large this problem cannot be resolved in the mangin model because it conceptualizes that slow flow from saturated zone always contributes to spring discharge during the entire recession period fig 6g and h plot the measured and calculated spring discharge by using the fiorillo model for the two recession periods in each figure the vertical dashed lines separate the hydrograph into three periods representing spring discharge from three tanks fig 3b the flow in the first period is the spring discharge from the tank that represents conduits and the flows in the other two periods are the spring discharge from the tanks that represent fracture and matrix while the fitting between the measured and calculated spring discharge is satisfactory in fig 6g the fitting is less satisfactory in fig 6h in the second period when both matrix flow and conduit flow contribute to the spring discharge to demonstrate the linear relation between the logarithm of discharge lnq and time t in the latter two flow periods fig 6g and h plot the relation of lnq t for the two periods and the linear relation is observed 4 2 comparison between the new model and the other two models fig 7 plots the residuals differences between observed and simulated spring discharge of the four model simulations for the two recession periods the misfit i e the sum of absolute residuals as defined in eq 13 is the largest for the new model with only one conduit layer denoted as new model 1 in fig 7 and the misfit is significantly larger than those of the other three simulations these indicate that it is necessary to separate the conduit reservoir into two conduit layers in the mixed drainage stage for the first recession period fig 7a the misfit of the new model with two conduit layers denoted as new model 2 in fig 7 is 0 433 which only slightly smaller than the misfit of 0 436 m3 s for the mangin model and the misfit of 0 447 m3 s for the fiorillo model this is not surprising because the simulations of the three models fit the observations almost equally well as shown in fig 6 this is attributed to the large matrix flow during the recession period which can be simulated by all the three models in other words the mixed drainage stage of the new model is essentially a matrix drainage stage and the separation of conduit flow and matrix flow in the mixed flow period does not improve the goodness of fit of the new model for the second recession period fig 7b when the conduit flow is substantial during the mixed flow period the new model with two conduits layers and the fiorillo model outperform the mangin model and the new model with two conduit layers outperforms the fiorillo model this is evidenced by the calculated misfit values which are 1 895 4 286 and 2 747 m3 s for the new model with two conduit layers the mangin model and the fiorillo model respectively the reason that the new model with two conduit layers and the fiorillo model outperform the mangin model is that the mangin model assumes that both conduit flow unsaturated flow and matrix flow saturated flow contribute to the fast flow unsaturated flow in other words the mangin model overestimates the early spring hydrograph because the model assumes that matrix flow occurs during the entire recession period as shown in fig 6f there are two reasons that the results of the new model with two conduit layers are better than those of this study s implementation of the fiorillo model one reason is that the new model separates matrix flow and conduit flow during the mixed drainage stage considering that the two models use the same equations for the conduit drainage stage and the matrix drainage stage table 2 the other reason is that the new model uses two conduit layers to represent different levels of karstification the latter reason is more important than the former reason which is self evident because the new model with only one conduit layer cannot satisfactorily simulate the measured spring discharge the new model has the following two flexibilities 1 since the new model separates the mixed flow into conduit flow and matrix flow it can use the linear equation and the exponential model for simulating conduit flow and matrix flow respectively 2 since the new model considers multiple conduit layers it can uses multiple linear equations to simulate the variation of conduit flow which may be caused by hydraulic head differences between the matrix and conduit reservoirs as observed at the aumelas thau karst system in france and the santa fe karst system in florida bailly comte et al 2010 these flexibilities are not available in this study s implementation of the fiorillo model it is possible that using multiple exponential equations during the mixed flow period may improve the results of the fiorillo model 4 3 estimated discharge volume and effective porosity table 3 lists the estimated effective porosity for the two recession periods the table also lists the estimated discharge volumes and head ranges needed for the porosity estimation the head ranges are listed in the format of heads at the beginning and ending time of each drainage stage for example the range of 6 06 m 5 93 m corresponds to h 1 m h 2 m the matrix heads of the beginning and ending time of the mixed drainage stage for recession period 1 the drainage area ac needed for the porosity estimation takes the value of 259 km2 srwmd 2004b the average hydraulic head h 2 c takes the value of 0 16 m the lowest hydraulic head at the monitored wells the impacts of the assumed h 2 c value are discussed below the table suggests that for the two recession periods while hydraulic head in the matrix decreases slightly from 6 06 m to 5 89 m in the first recession period and from 5 72 m to 5 28 m in the second recession period hydraulic head in the conduit reservoir decreases substantially from 8 24 m to 0 16 m in the first recession period and from 9 46 m to 0 16 m in the second recession period the head variations indicate that the conduit flow is more dynamics than the matrix flow as a result the estimation of matrix porosity is more stable than the estimate of conduit porosity as discussed below for the two recession periods groundwater discharge from the matrix reservoir is substantially larger than that from the conduit reservoir for the first recession period the groundwater discharges from the matrix and conduit reservoirs are 5 191 925 m3 and 1 220 991 m3 respectively for the second recession period the groundwater discharges from the matrix and conduit reservoirs increase to 12 737 966 m3 and 5 319 136 m3 respectively it is expected that the groundwater discharge from the matrix reservoir is larger than that from the conduit reservoir considering that the estimated matrix flow is substantially larger than the estimated conduit flow in this study area fig 6c and d the increase of groundwater discharge in the second recession period is reasonable considering that the precipitation before the second recession period is about twice as large as the precipitation before the first recession period table 1 therefore the estimated values of matrix and conduit porosity of the second recession period should better reflect the karst system than those of the first recession period table 3 indicates that while the estimated matrix porosity is consistent for the two recession periods the estimated conduit porosity varies substantially between the two recession periods for example the estimated conduit porosity of the mixed drainage stage in the first recession period is about one order of magnitude smaller than those in the second recession period the estimated conduit porosity of the second recession period should be more reasonable than that of the first recession period because the conduit flow of the second recession period is substantially larger than that of the first recession period fig 6d in other words the conduit reservoir is filled with more water in the second recession period than in the first recession period since the value of conduit head h 2 c is assumed to be 0 16 m it is necessary to evaluate the impact of the assumed value on the estimation of effective conduit porosity fig 8 plots the variation of estimated conduit porosity of the two conduit layers for a number of h 2 c values between 0 16 m and 1 m which was used by li et al 2013 2016 for studying the hydrograph of the st marks spring that is close to the madison blue spring the figure shows that the impact is small for conduit layer 1 in terms of the absolute difference for example when the conduit head h 2 c increases from 0 16 m to 1 m the estimated effective porosity increases from 0 010 to 0 012 for the first recession period and from 0 17 to 0 20 for the second recession period the absolute increase is larger for conduit layer 2 which is from 0 041 to 0 048 for the first recession period and from 0 37 to 0 44 for the second recession period the larger impact on conduit layer 2 than on conduit layer 1 is reasonable because conduit layer 2 is closer to the conduit vent than conduit layer 1 is when the relative increase of effective conduit porosity is calculated for the two conduit layers and the two recession periods the relative increase is about 20 which is significant therefore it is necessary to have a reasonable estimation of conduit head h 2 c 4 4 evaluation of estimated effective porosity the estimated effective porosity of the matrix reservoir is evaluated by comparing the estimated porosity with the measured from rock samples matrix porosity at two wells w 15515 and w 15537 completed in 1984 in the study area hoenstine 1990 upchurch 2004 fig 9 shows the vertical profiles of measured matrix porosity along the two wells while the measured matrix porosity varies substantially with depth the average matrix porosity is about 12 for the two wells close to the estimated values listed in table 3 it suggests that the estimated effective porosity of the matrix reservoir is reasonable since the estimated matrix porosity is not for the entire aquifer but for the portion of the aquifer where matrix head varies during the recession periods we calculate the average matrix porosity for the intervals where matrix head varies for the two wells the intervals were marked by the grey boxes based on the measured head of upchurch 2004 and choenstine 1990 the average matrix porosity is 13 5 for well 15 515 and 10 5 for well 15537 these average values bracket the estimated matrix porosity listed in table 3 suggesting that the estimated matrix porosity is reasonable it should be noted that the matrix porosity of the floridan aquifer can be as high as 30 budd and vacher 2004 peterson and wicks 2005 since on site measurements of conduit porosity are not available the estimated effective porosity values of conduit reservoirs listed in table 3 are compared with literature data the estimated values are comparable with the estimate channel porosity values listed in worthington 1999 and worthington et al 2000 which are 0 003 for smithville ontario canada 0 06 for mammoth cave kentucky usa 0 02 for the chalk england and 0 5 for nohoch nah chich yucatan mexico except the low value of 0 003 the literature values are of the same order of magnitude with those listed in table 3 at different drainage stages the matrix porosity and channel porosity specific to the cenozoic limestone at yucatan are of particular interest due to similar geology between yucatan and florida the estimated matrix porosity of the yucatan limestone is 17 about 42 larger than the estimate of 12 in this study the estimated channel porosity of the yucatan limestone is 0 5 about 35 larger than the largest estimate of 0 37 in this study this different is not unreasonable because the site area of this study is supposed to be smaller than that of the yucatan limestone worthington 1999 and worthington et al 2000 also listed the proportion of aquifer storage in matrix fracture and channel which are 99 7 for smithville 96 4 for mammoth cave 99 9 for the chalk and 96 6 for nohoch nah chich the matrix storage is conceptually similar to the matrix flow in this study although not the same and table 3 indicates that the matrix storage is about 97 for recession period 1 and 83 92 for recession period 2 this suggests that the estimated amount of flow and the estimated reservoir porosity are comparable to those reported in literature however it should be noted that the comparison is qualitative only and caution should be taken for example in worthington et al 2000 channel is referred to as all interconnected disolutional enlargements along joints faults and bedding planes this definition is certainly broader than the concept of conduit reservoir used in this study 5 conclusions this paper presents a new model for simulating karst spring recession curves the new model has the following three characteristics 1 the model considers two separate but hydraulically connected reservoirs matrix reservoir and conduit reservoir 2 the model separates a recession period of a karst spring hydrograph into three drainage stages conduit drainage stage with only conduit flow in conduits mixed drainage stage with both conduit flow in conduits and matrix flow from matrix to conduits and matrix drainage stage with only matrix flow from matrix to conduits and 3 in the mixed drainage stage the model uses multiple conduit layers to present different levels of conduit development the new model is conceptually similar to the fiorillo model but has two unique features for handling the mixed drainage stage i e explicitly separating conduit flow and matrix flow and using multiple conduit layers to represent different levels of karst development when using the new model the mangin model and the fiorillo model for simulating the observed spring discharge at the madison blue spring the new model and the fiorillo model outperform the mangin model because the mangin model assumes continuous matrix flow from the beginning of the recession and thus overestimates the early spring discharge due to the flexibilities of using linear equation and exponential equation to simulate conduit flow and matrix flow respectively and of using multiple conduit layers for the mixed drainage stage the results of the new model are better than those of this study s implementation of fiorillo model the results of the fiorillo model may be improved by using multiple exponential functions and this exploration is warranted in future studies based on the new model of simulating the recession curves of karst spring hydrograph the groundwater discharge volumes from the matrix and conduit reservoirs are estimated the results indicate that the discharge from the matrix reservoir is substantially larger than the discharge from the conduit reservoir especially in the first recession period when conduit flow is smaller than the second recession period as a result the estimated effective porosity of the conduit reservoir for the second recession period better characterize the conduit reservoir than that for the first recession period while there is lacking field measurements to evaluate the estimated effective porosity of the conduit reservoir the estimated effective porosity of the matrix reservoir agrees with the porosity values measured from borehole samples at the study site the new model of simulating the recession curves of karst spring hydrograph has four limitations the first limitation is that the new model requires a relatively long recession period until the matrix drainage stage appears on the karst spring hydrograph the requirement for a relatively long recession period is resulted from the procedure of hydrograph separation i e steps 3 and 4 described in section 2 1 specifically speaking the recession coefficient α 2 is first estimated for the matrix drainage stage and its value is assumed for the recession coefficient α 1 for simulating the matrix flow of the mixed drainage stage the long recession periods may not occur often for areas e g florida with frequent rainfall events this limits the applicability of the new model for simulating karst spring hydrograph and for characterizing karst aquifers the second limitation is that the estimation of effective porosity of matrix and conduit reservoirs requires knowing the springshed area ac and the conduit hydraulic head h 2 c at the end of the mixed drainage stage while the area ac can be estimated based on available hydrologic information the area is assumed to be a constant over time which may not be realistic especially when hydrologic conditions change dramatically over time in florida in addition the estimated springshed does not consider groundwater flow from adjacent springsheds and karst spring hydrograph may only represent a fraction of the springshed for conduit head h 2 c it cannot be directly estimated because measurements of conduit head are always lacking the third limitation is that the new model is only applicable to unconfined karst aquifers for which the karst spring recession curve can reflect the internal structure and properties of the karst aquifers for example the effective porosities defined in this study are only applicable to unconfined karst aquifers because head variation of confined aquifers does not reflect the volumes of matrix and conduit reservoirs the last limitation of this research is that it cannot provide a systematic way of evaluating the estimated conduit porosity since there is always lacking of measurements of conduit porosity it is necessary to evaluate the estimated conduit porosity using other approaches one of the approaches is the hydrochemical model of de rooij and graham 2017 that can explicitly characterize karst conduit networks and estimate conduit porosity worthington 2015 provided more insights on karst conduit formation and its characterization from the hydrogeological perspective future research is warranted for linking this research which is statistical in nature with other theoretical researches for better characterizing conduits of karst aquifers acknowledgements this research was supported in part by the national key r d program of china grant 2017yfc0804102 the first author was supported by the china scholarship council for his research in the department of scientific computing at the florida state university the second author was supported by national science foundation grantear 1828827 
7246,in the context of hydrodynamic modeling the use of 2d models is adapted in areas where the flow is not mono dimensional confluence zones flood plains nonetheless the lack of field data and the computational cost constraints limit the extensive use of 2d models for operational flood forecasting multi dimensional coupling offers a solution with 1d models where the flow is mono dimensional and with local 2d models where needed this solution allows for the representation of complex processes in 2d models while the simulated hydraulic state is significantly better than that of the full 1d model in this study coupling is implemented between three 1d sub models and a local 2d model for a confluence on the adour river france a schwarz algorithm is implemented to guarantee the continuity of the variables at the 1d 2d interfaces while in situ observations are assimilated in the 1d sub models to improve results and forecasts in operational mode as carried out by the french flood forecasting services an implementation of the coupling and data assimilation da solution with domain decomposition and task data parallelism is proposed so that it is compatible with operational constraints the coupling with the 2d model improves the simulated hydraulic state compared to a global 1d model and da improves results in 1d and 2d areas keyword hydraulic modeling multi dimensional coupling data assimilation domain decomposition real time flood forecasting 1 introduction the equations of fluid mechanics are solved in hydrodynamics studies with a large variety of numerical models based on simplifying assumptions the shallow water equations swe which are derived from the three dimensional 3d navier stokes equations by integrating along the vertical provide the two dimensional 2d simplified model of the 3d fluid flow subsequently by integrating the 2d swe across the dominant flow direction one gets the one dimensional 1d swe also called the saint venant equations while the 1d assumption may be relevant for some rivers or sections of rivers 2d modeling may be necessary for more complex areas such as flood plains or confluence zones as of today advanced hydraulic multi dimensional modeling requires hpc high performance computers it stands in implementing robust and efficient numerical schemes over high resolution grids together with data driven methods such as data assimilation da and using rich and various geophysical data in situ and remote sensing nonetheless the lack of bathymetry topography data and the computational cost limit the extensive use of 2d models for operational flood forecasting multi dimensional coupling overcomes this limit as only complex flow areas are solved with the 2d swe elsewhere where the flow is one dimensional this solution makes the most of long time expertise in 1d model setting and calibration the present study focuses on multi dimensional coupling in the context of real time flood forecasting the 1d swe based network model is widely used in hydraulics due to its relatively low computational cost for such model the data assimilation da capability has been already developed and used for flood forecasting see ricci et al 2011 we demonstrate how the performance of the network model is improved by using the 2d swe based model locally i e around a confluence point or across the flood plain area multi dimensional coupling strategies in the field of hydrodynamics were proposed recently with heterogeneity in the complexity of the physics and the dimensions an overlapping coupling strategy between 1d and 2d swe models was proposed by gejadze and monnier 2007 they implemented a coupling strategy based on the conservation of the characteristic variables entering the 2d model and the injection of a lateral source term in the 1d model that is computed from the 2d model in a flooding phase this strategy was combined with variational da and led to improved simulated results for academic test cases another strategy consists of coupling models at the lateral or longitudinal interfaces without overlapping goutal et al 2014 introduces a lateral coupling method between the 1d and 2d swe models describing the river flow in the main channel and over the flood plain respectively by considering variables between the 1d and 2d models as boundary fluxes for the 2d model and source terms for the 1d model this strategy is based on successive resolutions of the 2d riemann problem at the coupling interfaces and requires the estimation of the transverse velocity at the interfaces the precision and efficiency of the method were illustrated with academic test cases based on steinebach et al 2004 miglio et al 2005 proposed longitudinal coupling between 1d and 2d swe models where the continuity of riemann variables across 1d 2d boundaries is preserved using the iterative schwarz algorithm this coupling is denoted by longitudinal as the interface is located at the upstream and downstream interfaces of 1d and 2d models along the flow of the river in another example of longitudinal coupling chen et al 2012 proposed a new method based on the theory of characteristics to couple numerical models the water stage prediction correction method the approach of miglio was re visited and implemented with the coupling platform openpalm by malleron et al 2011 it was tested for a flood event on the rhine river and the water level difference between 1d 2d coupling and a full 2d model remain below 1 following this work tayachi 2013 and blayo et al 2017 developed a schwarz algorithm for coupling the 1d swe with the navier stokes equations the convergence properties of the global time schwarz waveform relation method have been studied for different partial differential equations see e g gander and stuart 1998 gander et al 2003 the influence of the interface position between 1d and 2d models was highlighted the ideas of this work were further developed by daou et al 2014 for 1d 3d coupling for monophasic monophasic flow and monophasic diphasic flow it was implemented on a real applicative case for inflow and outflow at a hydroelectric plant following malleron et al 2011 the present study aims to develop an operational model of the adour catchment in collaboration with spc gad service de prévision des crues garonne adour dordogne and schapi service central d hydrométéorologie et d appui la prévision des inondations a 1d 2d longitudinal coupling strategy is proposed in the adour catchment at the confluence between the nive and the adour rivers in bayonne the river network under consideration is divided into 3 parts the upstream nive nu the upstream adour au and the downstream adour ad for each part the separate 1d sub model is used whereas the 2d model is created for the confluence area at bayonne as in malleron et al 2011 and miglio et al 2005 the iterative schwarz algorithm is applied to preserve the continuity of hydraulic variables at the 1d 2d interfaces where the boundary conditions for each model are defined an innovative feature of this work is that a da filtering algorithm is applied over the 1d sub models that are coupled to the 2d model the structure of the paper is as follows section 2 presents the 1d and 2d swe formulations and the model set up for the adour maritime and bayonne area the coupling strategy is described in section 3 along with the da algorithm for the 1d model the details of the implementation of the coupling da strategy are also described results concerning the convergence and the computational cost of the coupling algorithm are presented in section 4 1 and section 4 2 respectively the merits of the da coupling strategy for flood forecasting are evaluated for a set of flood events and illustrated in section 4 3 conclusions of the study and future work are described in section 5 2 modeling the hydrodynamics of the adour river 2 1 the hydraulic solvers the hydraulics numerical solvers mascaret goutal and maurel 2002 and telemac hervouet 2007 are used in this study these software were developed in the framework of the telemac mascaret consortium http www opentelemac org they are commonly used for simulations of dam break wave reservoir flushing and flooding 2 1 1 the 1d hydraulic model mascaret the one dimensional swe are written in terms of discharge q m 3 s 1 and wet cross section area a m 2 that relates to the water level h m in a 1d model the stream channel is described by a hydraulic axis corresponding to the main direction of the flow the curvilinear abscissa denoted by s in m the non conservative form of the one dimensional swe goutal 2014 thual 2010 1 t a h s q 0 t q s q h 2 a h ga h s h ga h s 0 s f 0 with g m s 2 the gravitational constant s 0 the channel slope and s f the friction slope these equations are usually combined with an equation for the friction slope s f here defined by the manning strickler formula 2 s f q 2 k s 2 a h 2 r h 4 3 where r h a h p h is the hydraulic radius in m written as the ratio of the wet cross section area a and the wet perimeter p h m k s is the strickler friction coefficient in m 1 3 s 1 the hydraulic model requires the following input data bathymetry upstream and downstream boundary conditions lateral inflows roughness coefficients specified over homogeneous zones and initial conditions for the hydraulic state 2 1 2 the 2d hydraulic model telemac the 2d swe are written in terms of the water depth h and the horizontal components of velocity u and v m s 1 3 h t hu x hv y 0 hu t u hu x v hu y gh z s x 1 ks 2 gu u 2 v 2 h 1 3 4 hv t u hv x v hv y gh z s y 1 ks 2 gv u 2 v 2 h 1 3 where k s m 1 3 s 1 is the river bed and flood plain strickler friction coefficient z s m is the water free surface elevation h z s z f where z f m is the bottom level described by the bathymetry and topography and ν e m 2 s 1 is the water diffusion coefficient to solve eqs 3 and 4 initial conditions h x y t 0 h 0 x y u x y t 0 u 0 x y v x y t 0 v 0 x y are provided as well as with boundary conditions at the surface at the bottom and at the upstream and downstream frontiers h x bc y bc t h bc t a two dimensional area is described by a set of elements triangular with telemac 2d whose nodes are assigned a bathymetric value in a 2d model the geometrical representation of the river differs from the 1d modeling there is no distinction between the main channel and the floodplain the friction coefficient is defined over homogeneous zones on this mesh 2 2 the hydraulic models for the adour maritime and bayonne areas the adour maritime 1d hydraulic model am displayed in fig 1 covers about 160 km it was set up for operational flood forecasting at schapi and spc gad it is composed of 7 reaches with 3 confluences between reaches 2 and 3 4 and 5 and 6 and 7 there are 5 water level observing stations along the network at peyrehorade urt lesseps pont blanc and villefranque the upstream forcings at dax orthez escos and cambo are given by discharges computed from observed water levels translated into discharge with local rating curves in forecast mode these forcing are defined constant using the last observed value the downstream forcing at convergent is given by water level observations or forecasts the entire network is under tidal influence except upstream of the dams located on reaches 3 6 and 7 the full 1d model is composed of 2247 grid points the adour network is under maritime tidal influence combined with wet meteorological conditions due to water mass and heat flux exchanges with the ocean it thus displays highly non linear dynamics the adour maritime area is thus one of the most sensitive zones to flooding risks with a largest number of medium and maximum alerts raised by the flood forecasting services flood events on the adour maritime network are characterized by sudden flood peaks along reaches 3 6 and 7 that can occur simultaneously and last from 2 to 4 days flood events in reach 4 are characterized by long water level rising periods up to 9 days and a slow water level decreases 4 to 9 days the am hydraulic model was created from 548 bathymetric surveys throughout the network the friction coefficient is defined in 20 areas and was calibrated to fit high tide water levels during past events the am model was decomposed in three 1d sub models that are coupled to a local 2d model colored area over the bayonne area as presented in fig 1 the adour upstream au sub model which includes the adour river upstream of the bayonne area it is composed of 5 reaches with 2 confluences and 2034 nodes the adour downstream ad sub model from lesseps downstream of the bayonne area to convergent it is composed of one reach with 95 nodes the nive upstream nu sub model which includes the nive river upstream of the garonne area it is composed of 1 reach with 404 nodes the city of bayonne is located on the banks of the adour river at the confluence between the adour and the nive confluence of reaches 1 2 and 3 in fig 1 this highly urbanized area is threatened by flood events on the nive river in this area the flow is tidally influenced and during flood events the nive river flows into the plaine d ansot and the barthe de quartier bas the flow adopts a non linear 2d dynamic recirculation for that reason a 2d model for the bayonne area where the niver and the adour rivers meet fig 3 a has been developed jointly by the schapi and the spc gad for operational purposes upstream of bayonne for the adour river the 2d model starts at the highway a63 where the flow is mono dimensionnal and the dikes stem the flow for the nive river the 2d area includes the plaine d ansot and the barthe de quartier bas flood plains that stock and drain water during flood events on the nive river the 2d area includes saint esprit petit bayonne and grand bayonne at the nive adour confluence it ends at the lesseps bridge downstream of bayonne in the present study the observations at lesseps are not assimilated and the dynamics are driven by the maritime forcing at convergent about 5 km farther downstream the 2d model with telemac is composed of 33748 nodes and 66982 element 3 numerical methods 3 1 coupling algorithm between the 1d and 2d hydraulic models the coupling algorithm used in this study is the multiplicative form of the global in time schwarz algorithm adapted to the hydraulic case miglio et al 2005 malleron et al 2011 in the present work the upstream boundary conditions of the 1d models are prescribed by the discharge and the downstream boundary conditions are prescribed by a wet surface the upstream boundary conditions of the 2d model are prescribed by velocity vectors while the 2d downstream boundary condition is prescribed by the water level the multiplicative form of the global in time schwarz algorithm applied to the am bayonne area consists in solving the sub problems in eqs 5 and 6 until the stopping criteria is verified 5 l 1 a k q k 0 on ω 1 d i t t t i 1 2 3 a i k r 21 z 2 d i k 1 on γ i t t t i 1 2 q 3 k s 21 u k 1 v k 1 2 d 3 on γ 3 t t t 6 l 2 h k u k v k 0 on ω 2 d t t t u k v k i s 12 q 1 d i k on γ i t t t i 1 2 h 2 d 3 k r 12 a 3 k on γ 3 t t t where l 1 and l 2 stand for the shallow water operators for the 1d and the 2d models respectively and ω 2 d and ω 1 d i with i 1 2 3 correspond to the 2d domain and the three 1d sub domains respectively represented in fig 2 γ i with i 1 2 3 is the three 2d liquid boundaries z 2 d i k 1 is the mean 2d water level at the liquid boundary i with i 1 2 the boundary condition operators r 21 r 12 s 21 and s 12 are applied following the x y local axis along and perpendicular to the flow and they are described as follows r 21 translates the 2d water level along the cross sectional interface γ into a wet cross sectional area for the 1d boundary condition the 2d river width w z is integrated along the vertical axis z from the bottom of the river z f to the surface level z s for elements in the 1d 2d interface cross section following y r 21 z f z s w z dz r 12 translates the 1d wet cross sectional area into the 2d water level along the wet cross section following y if s and z s denote the wet cross sectional area and the water free surface elevation then z s verify s z f z s w z dz hence the free surface elevation and the bathymetry at the coupling interface enable to compute the 2d water level to impose at the coupling interface s 21 translates the velocity field at the 2d boundary γ into a discharge that is imposed at the 1d boundary the 2d water depth h 2 d is multiplied by the velocity at each point along the cross sectional area described by y then projected onto the local vector normal to the flow x to compute the 1d discharge q γ h 2 d y u v n dy where n is the local normal vector to the 2d flow along x and stands for the cross product s 12 translates the 1d discharge into 2d velocity components along y the 1d water level is mapped onto the 2d interface to compute the 2d water depth h 2 d velocity vectors are derive from a parabolic profile proportional to h 2 d u x α h x n x and v x α h x n y the proportionality coefficient α is set to ensure the discharge continuity at the 1d 2d interface γ h y u y v y n dy q at coupling iteration k the boundary conditions applied at the interface of the 1d model for integration over t t t are prescribed from the results of the 2d model at iteration k 1 over t t t once the 1d integration is completed the 2d boundary conditions for iteration k are prescribed from the results of the 1d model for integration over t t t the iterative coupling strategy is applied sequentially in time over the length of the entire flood event setting the stopping criteria requires a compromise between precision and computational time here it is based on the continuity of the discharge and the water level at the interface which implies the continuity of the riemann invariant at the interface this condition is necessary with respect to the hyperbolicity of each sub problem for sub critical flow this formulation differs from that of miglio et al 2005 which ensures the continuity of wet cross sectional area discharge and characteristic entering at the 1d 2d interface following the conclusions of tayachi 2013 it was verified that the flow remains mono dimensional at the coupling interfaces even for high flow rates velocity vectors at the 1d 2d interface γ 1 ω 1 d 1 from fig 2 from the 2d model are shown in fig 3 b bottom left of the plot these vectors follow the 1d river center line for all simulation time steps only one time is shown ensuring that the coupling algorithm converges within a limited number of iterations 3 2 data assimilation algorithms in the 1d model several sources of uncertainty are identified in the hydraulic modeling hydrological forcing data which describe the boundary conditions usually result from the transformation of uncertain observed water levels into discharges through an uncertain rating curve or from discharges that are forecast by uncertain hydrological models additionally the description of the river channel and flood plain geometry relies on a limited number of in situ measurements of topographic and bathymetric profiles that are then spatially interpolated the simplification of the flow to a 1d representation is also a significant limitation finally the calibration of friction coefficients is a pragmatic way to account for a variety of sources of uncertainty globally speaking uncertainties in the input data and in the hydraulic parameters translate into uncertainties in the simulated hydraulic state these uncertainties can be reduced with a da algorithm that consists in combining water level or discharge in situ observations in the numerical model to correct the model forcing parameters and or state ricci et al 2011 habert et al 2016 madsen and skotner 2005 in the present case of 1d 2d coupling improving the hydraulic state in the 1d sub models results in improving the boundary conditions for the 2d model while ensemble data assimilation algorithms showed promising results for hydraulics and flood forecasting e g barthelemy et al 2017 such algorithms were not considered here because of the computational cost instead the invariant kalman filter ikf from ricci et al 2011 was implemented in this formulation a simplified version of the kalman filter kalman 1960 is implemented with the background error covariance matrix in eq 7 which is not propagated between assimilation cycles to avoid either propagating an ensemble or using the model tangent linear matrix and carrying our computationally expensive matrix multiplication the analysis and propagation steps of the analysis are 7 x i a x i b bh t hbh t r 1 y i o h x i b x i 1 b m i i 1 x i a where the control vector x a q corresponds to the discretized hydraulic state excluding boundary values x i b and x i a are respectively the background and analysis vector at time i b is the background error covariance matrix r is the observation error covariance matrix y i o is the observation vector at time i and h is the observation operator in this study since the flow is sub critical the background error correlation function is assumed to be an asymmetric gaussian function with shorter correlation length scale downstream than upstream as illustrated in fig 4 further details about the da algorithm and the results for the am network are given in ricci et al 2011 in the 1d 2d coupling method the da analysis is applied at each observation time in each 1d sub model the analyzed state is then propagated in time by the 1d sub models and exchanged with the 2d model interfaces 3 3 cycled implementation of the coupling with openpalm the da algorithm is implemented with openpalm http www cerfacs fr globc palm web piacentini et al 2011 it is an open source flexible and powerful dynamic code coupler that has been developed jointly by cerfacs centre européen de recherche et formation avancée en calcul scientifique and onera office national d etudes et de recherches aérospatiales since 1998 openpalm was originally designed for da algorithms in operational oceanography forecasting it has now reached a high degree of maturity and stability with applications ranging from operational da oceanography atmospheric chemistry hydrology to industrially oriented multi physics modeling fluid structure interactions combustion acoustics interactions openpalm provides a straightforward parallel environment based on high performance implementation of the message passing interface mpi standard i e mpich openmpi lam mpi this interface is able to perform both data parallelism i e simultaneous execution on multiple cores of the same code for a unique data set with domain decomposition and task parallelism i e simultaneous execution on multiples cores of multiple tasks for the same or different data sets the openpalm coupling strategy relies both on task parallelism and data parallelism the 2d model is executed on several processors data parallelism while in parallel task parallelism the 1d sub models are executed each one on different processors the number of 1d sub models and the number of processors to run the 2d model are user defined and are handled by the openpalm coupling implementation so that this si generic for the user and can be applied to any hydraulic network the schwarz algorithm is implemented in 3 steps initialization iterative over time loops and finalization the initialization step consists in reading the user defined parameters model parameters and geometries these variables are allocated and stored in the openpalm block structure shared memory these input variables as well as the simulated wet area discharge and flow velocities at the 1d 2d interfaces are available for telemac and mascaret within the block structure the data exchanges are achieved with openpalm communications the finalization step consists in deallocating the variables it should be noted that the stationary hypothesis for the kf algorithm implies that the cost of da is limited to the matrix vector product in eq 7 and thus represents a non significant additional cost for the coupling algorithm still when an observation is assimilated in the 1d model a discontinuity with the 2d model may occur and several iterations of the schwarz algorithm may then be necessary to reconcile 1d and 2d models the coupling da strategy is applied over a sliding time window that covers the coupled propagation of the dynamics and analysis steps for da and a forecast period over which the 1d and 2d models are also coupled 4 results 4 1 schwarz algorithm convergence to the authors knowledge there are no theoretical results showing the schwarz algorithm convergence in the case of multi dimensional swe yet a convergence study was completed by tayachi 2013 for a linearized 3d navier stokes equations system coupled with a linearized 1d swe system it was shown that when absorbing boundary conditions are defined at the coupling interfaces the schwarz algorithm converges in 2 iterations the convergence was studied in detail by tayachi et al 2014 for a toy model and the importance of the interface position between the 1d and 2d models was confirmed the coupling interface must be located in an area where there are no 2d effects meaning that the flow is mono dimensional in the present work the convergence study was carried out over a 2 day simulation period for a flood event in 2011 the stopping criteria was defined as a 5 10 3 m difference between the 1d and 2d water levels and a 0 01 m s 1 difference in the velocities at the interface the coupling time step is 8 s which is also the 1d model time step and the 2d model time step is 4 s fig 5 a displays the convergence of the velocity difference at the coupling interface as a function of the schwartz iteration this was calculated at the interface downstream of the 2d model it was numerically observed fig 5 b that for the first coupling time steps convergence usually occurs within a small number of iterations for further coupling time steps convergence is reached after the first iteration in all cases the maximum number of iterations is set to 5 to limit the computational cost when assimilation is performed a discontinuity between the 1d and 2d models is introduced and the convergence of the schwarz algorithm may require more than one iteration as illustrated in fig 5 c it should be noted that depending on the location of the 1d 2d interface as well as the choice of the continuity variable the convergence may be harder to reach yet the computational cost for 8 s of the 2d model simulation is not significant and the cost of additional iterations for a small number of time steps has a negligible impact on the overall cost of the coupling over a flood event 4 2 optimal computational resources for coupled model for the bayonne 1d 2d coupled model numerical coupling experiments with increasing numbers of processors were carried out on a high performance computing platform at cerfacs with 53 tflop s and 158 nodes with 2 intel 8 cores processors intel sandybridge 5 6 ghz and 32go of memory ddr3 fig 6 shows the accelerating factor which is the cost of the model running on several processors compared to the cost of the model running on only one processor for the local 2d model red curve and the coupled model blue curve the accelerating factor of the 2d model is smaller than the ideal accelerating factor the black line that linearly depends on the number of processors used and reaches a maximum of 15 8 for 32 processors vertical green dashed line this is consistent with the general behavior of telemac 2d which is most efficient when approximately 1000 nodes are computed on each processor the accelerating factor of the coupled model is also smaller than the ideal accelerating factor and than that of telemac alone reaching a maximum of 6 4 for 32 processors this result is very important for operational flood forecasting services to calibrate their computational resources in order to run the 1d 2d multi dimensional coupled model it should be noted that in this comparison the 2d model is local while the coupled model covers the entire network with the 1d model the loss of scalability is due to the multiplicative schwarz algorithm in which the 2d model waits for the 1d model results at the current iteration k 4 3 coupling used in flood forecasting the 1d 2d coupling da strategy was applied to model a set of 7 real flood events with and without da in the am catchment with hydrological data provided by the spc gad the water level at the observing stations is displayed for the 2014 flood event in fig 7 except at peyrehorade where the 2009 event is used this event presents the most visible results in each panel the blue line represents the observations the black line represents the full 1d am model output the green line represents the coupled model output without da and the red line represents the coupled model with da for each flood event the statistical results including the bias and root mean square error rmse computed with respect to the observations are presented in fig 8 and fig 9 respectively for the analysis at 0 h lead time solid lines and for the forecast at the maximum lead time of each observing station dashed lines the 0 h lead time and the maximum lead time are respectively the initial time of forecast and the transfer time of the upstream boundary condition beyond which a forecast can not be performed because upstream boundary conditions are set constant during the forecast period the 1d model results are plotted in black the coupled model without da results are plotted in green and the coupled model with da results are plotted in red the mean bias and rmse computed over the 7 flood events are shown in table 1 4 3 1 coupled model without data assimilation the merits of the 1d 2d coupling are described for the different observing stations pont blanc 2d area the 2d model around bayonne improves the simulated water level at pont blanc compared to the 1d model especially for high flow rates as illustrated in fig 7 a for the 2014 event the water level increases when using the coupling solution at low tide and the shape of the limnigraph is improved for high flow events even though the water level remains slightly underestimated the bias is reduced for all 7 flood events as shown in fig 8 a equivalently for the 0 h and maximum lead times with a mean negative bias of 19 cm for the 1d model reducing to 13 cm for the coupled model table 1 the rmse is also reduced for all events fig 9 a with a mean rmse of 0 25 cm for the 1d model reducing to 0 17 cm for the coupled model with smaller improvements as the lead time increases table 1 villefranque nu the improvements observed in the 2d area especially over the plaine d ansot and the barthe de quartier bas also improves the boundary condition at the 1d 2d interface and consequently in the 1d section upstream of the interface on the nive as illustrated in fig 7 b for the 2014 event the full 1d model strongly under estimates the water level especially at high flow rates and the coupling solution brings more water into the system the bias is significantly reduced for all 7 flood events as shown in fig 8 b b equivalently for the 0 h and maximum lead time with a mean negative bias of 77 cm reducing to 40 cm table 1 the rmse is also significantly reduced for all events fig 9 b with a rmse of 0 90 cm for the 1d model reducing to 0 46 cm for the coupled model with similar improvements as the lead time increases table 1 peyrehorade au the au hydrodynamics is mono dimensional thus the impact of the coupling with respect to the 1d model is limited at peyrehorade as shown in fig 7 c at peyrehorade which is far from the 2d area the improvement of the 1d 2d coupling is small for the 2009 event the bias is slightly reduced for all 7 flood events as shown in fig 8 c equivalently for the 0 h and maximum lead times with a mean negative bias of 30 cm reducing to 26 cm table 1 the rmse is reduced for all events fig 9 c with a mean rmse of 0 40 cm for the am 1d model reducing to 0 35 cm for the coupled model with similar improvements as the lead time increases table 1 urt au at urt the water level is improved for medium flow rates especially at low tide but not for high flow rates as shown in fig 7 d at this observing station the 1d hypothesis is not valid and the 2d flood plain modeling should be taken into account for high flow rates the 1d model tends to over estimate the water level since the coupling tends to add water into the system it has a negative impact at high flow rates at urt the time averaged bias decreased for some events and increased for others fig 8 d the rmse results are not impacted by the coupling fig 9 d and table 1 lesseps ad the impact of coupling at lesseps is not visible neither in the bias nor on the rmse independent of the lead time figs 8 9 e table 1 as the dynamics are driven by the maritime boundary conditions 4 3 2 coupled model with data assimilation the merits of da in the 1d model coupled to the 2d model are described here for the different observing stations pont blanc 2d area observations at pont blanc are not assimilated as this observing station is located in the 2d area the assimilation of observations at villefranque upstream of pont blanc tends to cause an over estimation of the water level at high flow rates for the 2014 event as illustrated in fig 7 a for the 7 flood events it has a positive impact on the bias fig 8 a with a decrease of the mean bias from 13 cm for the coupled model to less than 1 cm at 0 h and maximum lead times table 1 da strongly reduces the bias with respect to the 1d model and the coupled simulation yet as expected when correcting the model state the merits of da decrease as the lead time increases the rmse fig 9 a is slightly reduced by da but most of the rmse improvements were already achieved with the coupling table 1 villefranque nu the da analysis has a significant effect at villefranque where the 1d model does not represent well the observations even taking into account the improvements gained by the coupling during the 2014 event and especially at high flow rates as illustrated in fig 7 b the assimilation reduces the negative bias from 77 cm for the 1d model to 21 cm at the 0 h lead time and 38 cm at the maximum lead time table 1 the bias reduction is clearly visible in fig 8 b b with a reduced impact for a larger lead time the rmse is also significantly improved by da for all flood events fig 9 b the mean rmse is 25 cm at the 0 h lead time and 49 cm at the maximum lead time compared to 90 cm for the 1d model and 46 cm for the coupled model as shown in table 1 peyrehorade au at peyrehorade the model without da provides poor results especially during flood peaks da leads to a significant improvement in the simulated water level as illustrated in fig 7 c for the 2009 flood event the water level is increased for the 7 flood events the rmse and bias are greatly improved with stronger impacts for shorter lead times as expected figs 8 c and 9 c the bias and rmse are reduced on average to less than 10 cm and 12 cm table 1 urt au at urt da has little impact on the results and the simulated water level remains far from the observations especially for high flow rates as presented in fig 7 d the 1d model is not able to represent the flood plain dynamics and over estimates water levels in spite of the assimilation of observations at peyrehorade and urt in fact the assimilation of observations at peyrehorade may cause a degradation at urt when the water is over estimated at urt and under estimated at peyrehorade or conversely the 1d model bias was already small but was further reduced by coupling increasing a little due to da and the rmse of 20 cm did not change table 1 lesseps ad observations at lesseps are not assimilated because this station is located at the coupling interface between the 2d model and the 1d model adour downstream as previously noted for the coupling da has a negligible impact at lesseps 5 conclusion this study describes the application of a multi dimensional coupling strategy between 1d and 2d models on the adour maritime catchment where the confluence between the nive and the adour rivers is simulated with a 2d local model and the upstream and downstream parts of the rivers are simulated with 1d sub models the models are coupled at their longitudinal boundaries with an iterative schwarz algorithm applied at each interface a kalman filter data assimilation algorithm is also applied in the 1d models so that in situ water level observations are assimilated to correct the simulated and forecasted water level and discharge the data assimilation and coupling strategy is implemented with the openpalm dynamical coupling software that allows for an efficient management of task and data parallelism the developed strategy is compatible with operational computational cost constraints the strategy was applied to simulate a set of 7 flood events in the adour catchment it was shown that the coupling algorithm converges with at most 5 iterations and the water level and velocity continuity is guaranteed at the model interfaces numerical experiments were performed on 32 processors to achieve the scalability skills of the 2d model of the local area further work is needed to determine more precise conclusions since a full 2d model is not yet available on the adour area the results highlighted that the coupling with the local 2d solution significantly improves the simulation in the 2d and 1d areas data assimilation in 1d sub models also leads to significant improvements for simulations and for short term forecasts since only the model state is corrected yet the improvements are minor in 1d areas where the 1d model results are not satisfying for instance at urt where flood plain modeling should be activated future work includes the implementation of the additive scheme for the schwarz algorithm which should decrease the computational cost of the coupled strategy since the 1d and 2d models would then run in parallel for an iteration with no waiting delay preliminary work on this formulation presented convergence issues that require further investigation alternative coupling strategies are also being investigated lateral coupling between the 1d model of the river bed and the 2d model of the flood plains allows activating the 2d model only for high flow rates when the 1d model simulates overflows in the flood plains this strategy remains compatible with data assimilation in the 1d model finally efforts are being made to develop data assimilation for the 2d models for state and parameter corrections with an ensemble based approach this choice enables a flow dependent estimation of the background error covariances keeping the cost of such ensemble based algorithms compatible with operational computational constraints remains a key challenge for that reason the use of a surrogate model in place of the forward model should be envisaged acknowledgements the authors acknowledge the spc gad for providing the observed and hydrological data funding for this study was provided by schapi and the eocoe energy oriented center of excellence h2020 european project wp4 
7246,in the context of hydrodynamic modeling the use of 2d models is adapted in areas where the flow is not mono dimensional confluence zones flood plains nonetheless the lack of field data and the computational cost constraints limit the extensive use of 2d models for operational flood forecasting multi dimensional coupling offers a solution with 1d models where the flow is mono dimensional and with local 2d models where needed this solution allows for the representation of complex processes in 2d models while the simulated hydraulic state is significantly better than that of the full 1d model in this study coupling is implemented between three 1d sub models and a local 2d model for a confluence on the adour river france a schwarz algorithm is implemented to guarantee the continuity of the variables at the 1d 2d interfaces while in situ observations are assimilated in the 1d sub models to improve results and forecasts in operational mode as carried out by the french flood forecasting services an implementation of the coupling and data assimilation da solution with domain decomposition and task data parallelism is proposed so that it is compatible with operational constraints the coupling with the 2d model improves the simulated hydraulic state compared to a global 1d model and da improves results in 1d and 2d areas keyword hydraulic modeling multi dimensional coupling data assimilation domain decomposition real time flood forecasting 1 introduction the equations of fluid mechanics are solved in hydrodynamics studies with a large variety of numerical models based on simplifying assumptions the shallow water equations swe which are derived from the three dimensional 3d navier stokes equations by integrating along the vertical provide the two dimensional 2d simplified model of the 3d fluid flow subsequently by integrating the 2d swe across the dominant flow direction one gets the one dimensional 1d swe also called the saint venant equations while the 1d assumption may be relevant for some rivers or sections of rivers 2d modeling may be necessary for more complex areas such as flood plains or confluence zones as of today advanced hydraulic multi dimensional modeling requires hpc high performance computers it stands in implementing robust and efficient numerical schemes over high resolution grids together with data driven methods such as data assimilation da and using rich and various geophysical data in situ and remote sensing nonetheless the lack of bathymetry topography data and the computational cost limit the extensive use of 2d models for operational flood forecasting multi dimensional coupling overcomes this limit as only complex flow areas are solved with the 2d swe elsewhere where the flow is one dimensional this solution makes the most of long time expertise in 1d model setting and calibration the present study focuses on multi dimensional coupling in the context of real time flood forecasting the 1d swe based network model is widely used in hydraulics due to its relatively low computational cost for such model the data assimilation da capability has been already developed and used for flood forecasting see ricci et al 2011 we demonstrate how the performance of the network model is improved by using the 2d swe based model locally i e around a confluence point or across the flood plain area multi dimensional coupling strategies in the field of hydrodynamics were proposed recently with heterogeneity in the complexity of the physics and the dimensions an overlapping coupling strategy between 1d and 2d swe models was proposed by gejadze and monnier 2007 they implemented a coupling strategy based on the conservation of the characteristic variables entering the 2d model and the injection of a lateral source term in the 1d model that is computed from the 2d model in a flooding phase this strategy was combined with variational da and led to improved simulated results for academic test cases another strategy consists of coupling models at the lateral or longitudinal interfaces without overlapping goutal et al 2014 introduces a lateral coupling method between the 1d and 2d swe models describing the river flow in the main channel and over the flood plain respectively by considering variables between the 1d and 2d models as boundary fluxes for the 2d model and source terms for the 1d model this strategy is based on successive resolutions of the 2d riemann problem at the coupling interfaces and requires the estimation of the transverse velocity at the interfaces the precision and efficiency of the method were illustrated with academic test cases based on steinebach et al 2004 miglio et al 2005 proposed longitudinal coupling between 1d and 2d swe models where the continuity of riemann variables across 1d 2d boundaries is preserved using the iterative schwarz algorithm this coupling is denoted by longitudinal as the interface is located at the upstream and downstream interfaces of 1d and 2d models along the flow of the river in another example of longitudinal coupling chen et al 2012 proposed a new method based on the theory of characteristics to couple numerical models the water stage prediction correction method the approach of miglio was re visited and implemented with the coupling platform openpalm by malleron et al 2011 it was tested for a flood event on the rhine river and the water level difference between 1d 2d coupling and a full 2d model remain below 1 following this work tayachi 2013 and blayo et al 2017 developed a schwarz algorithm for coupling the 1d swe with the navier stokes equations the convergence properties of the global time schwarz waveform relation method have been studied for different partial differential equations see e g gander and stuart 1998 gander et al 2003 the influence of the interface position between 1d and 2d models was highlighted the ideas of this work were further developed by daou et al 2014 for 1d 3d coupling for monophasic monophasic flow and monophasic diphasic flow it was implemented on a real applicative case for inflow and outflow at a hydroelectric plant following malleron et al 2011 the present study aims to develop an operational model of the adour catchment in collaboration with spc gad service de prévision des crues garonne adour dordogne and schapi service central d hydrométéorologie et d appui la prévision des inondations a 1d 2d longitudinal coupling strategy is proposed in the adour catchment at the confluence between the nive and the adour rivers in bayonne the river network under consideration is divided into 3 parts the upstream nive nu the upstream adour au and the downstream adour ad for each part the separate 1d sub model is used whereas the 2d model is created for the confluence area at bayonne as in malleron et al 2011 and miglio et al 2005 the iterative schwarz algorithm is applied to preserve the continuity of hydraulic variables at the 1d 2d interfaces where the boundary conditions for each model are defined an innovative feature of this work is that a da filtering algorithm is applied over the 1d sub models that are coupled to the 2d model the structure of the paper is as follows section 2 presents the 1d and 2d swe formulations and the model set up for the adour maritime and bayonne area the coupling strategy is described in section 3 along with the da algorithm for the 1d model the details of the implementation of the coupling da strategy are also described results concerning the convergence and the computational cost of the coupling algorithm are presented in section 4 1 and section 4 2 respectively the merits of the da coupling strategy for flood forecasting are evaluated for a set of flood events and illustrated in section 4 3 conclusions of the study and future work are described in section 5 2 modeling the hydrodynamics of the adour river 2 1 the hydraulic solvers the hydraulics numerical solvers mascaret goutal and maurel 2002 and telemac hervouet 2007 are used in this study these software were developed in the framework of the telemac mascaret consortium http www opentelemac org they are commonly used for simulations of dam break wave reservoir flushing and flooding 2 1 1 the 1d hydraulic model mascaret the one dimensional swe are written in terms of discharge q m 3 s 1 and wet cross section area a m 2 that relates to the water level h m in a 1d model the stream channel is described by a hydraulic axis corresponding to the main direction of the flow the curvilinear abscissa denoted by s in m the non conservative form of the one dimensional swe goutal 2014 thual 2010 1 t a h s q 0 t q s q h 2 a h ga h s h ga h s 0 s f 0 with g m s 2 the gravitational constant s 0 the channel slope and s f the friction slope these equations are usually combined with an equation for the friction slope s f here defined by the manning strickler formula 2 s f q 2 k s 2 a h 2 r h 4 3 where r h a h p h is the hydraulic radius in m written as the ratio of the wet cross section area a and the wet perimeter p h m k s is the strickler friction coefficient in m 1 3 s 1 the hydraulic model requires the following input data bathymetry upstream and downstream boundary conditions lateral inflows roughness coefficients specified over homogeneous zones and initial conditions for the hydraulic state 2 1 2 the 2d hydraulic model telemac the 2d swe are written in terms of the water depth h and the horizontal components of velocity u and v m s 1 3 h t hu x hv y 0 hu t u hu x v hu y gh z s x 1 ks 2 gu u 2 v 2 h 1 3 4 hv t u hv x v hv y gh z s y 1 ks 2 gv u 2 v 2 h 1 3 where k s m 1 3 s 1 is the river bed and flood plain strickler friction coefficient z s m is the water free surface elevation h z s z f where z f m is the bottom level described by the bathymetry and topography and ν e m 2 s 1 is the water diffusion coefficient to solve eqs 3 and 4 initial conditions h x y t 0 h 0 x y u x y t 0 u 0 x y v x y t 0 v 0 x y are provided as well as with boundary conditions at the surface at the bottom and at the upstream and downstream frontiers h x bc y bc t h bc t a two dimensional area is described by a set of elements triangular with telemac 2d whose nodes are assigned a bathymetric value in a 2d model the geometrical representation of the river differs from the 1d modeling there is no distinction between the main channel and the floodplain the friction coefficient is defined over homogeneous zones on this mesh 2 2 the hydraulic models for the adour maritime and bayonne areas the adour maritime 1d hydraulic model am displayed in fig 1 covers about 160 km it was set up for operational flood forecasting at schapi and spc gad it is composed of 7 reaches with 3 confluences between reaches 2 and 3 4 and 5 and 6 and 7 there are 5 water level observing stations along the network at peyrehorade urt lesseps pont blanc and villefranque the upstream forcings at dax orthez escos and cambo are given by discharges computed from observed water levels translated into discharge with local rating curves in forecast mode these forcing are defined constant using the last observed value the downstream forcing at convergent is given by water level observations or forecasts the entire network is under tidal influence except upstream of the dams located on reaches 3 6 and 7 the full 1d model is composed of 2247 grid points the adour network is under maritime tidal influence combined with wet meteorological conditions due to water mass and heat flux exchanges with the ocean it thus displays highly non linear dynamics the adour maritime area is thus one of the most sensitive zones to flooding risks with a largest number of medium and maximum alerts raised by the flood forecasting services flood events on the adour maritime network are characterized by sudden flood peaks along reaches 3 6 and 7 that can occur simultaneously and last from 2 to 4 days flood events in reach 4 are characterized by long water level rising periods up to 9 days and a slow water level decreases 4 to 9 days the am hydraulic model was created from 548 bathymetric surveys throughout the network the friction coefficient is defined in 20 areas and was calibrated to fit high tide water levels during past events the am model was decomposed in three 1d sub models that are coupled to a local 2d model colored area over the bayonne area as presented in fig 1 the adour upstream au sub model which includes the adour river upstream of the bayonne area it is composed of 5 reaches with 2 confluences and 2034 nodes the adour downstream ad sub model from lesseps downstream of the bayonne area to convergent it is composed of one reach with 95 nodes the nive upstream nu sub model which includes the nive river upstream of the garonne area it is composed of 1 reach with 404 nodes the city of bayonne is located on the banks of the adour river at the confluence between the adour and the nive confluence of reaches 1 2 and 3 in fig 1 this highly urbanized area is threatened by flood events on the nive river in this area the flow is tidally influenced and during flood events the nive river flows into the plaine d ansot and the barthe de quartier bas the flow adopts a non linear 2d dynamic recirculation for that reason a 2d model for the bayonne area where the niver and the adour rivers meet fig 3 a has been developed jointly by the schapi and the spc gad for operational purposes upstream of bayonne for the adour river the 2d model starts at the highway a63 where the flow is mono dimensionnal and the dikes stem the flow for the nive river the 2d area includes the plaine d ansot and the barthe de quartier bas flood plains that stock and drain water during flood events on the nive river the 2d area includes saint esprit petit bayonne and grand bayonne at the nive adour confluence it ends at the lesseps bridge downstream of bayonne in the present study the observations at lesseps are not assimilated and the dynamics are driven by the maritime forcing at convergent about 5 km farther downstream the 2d model with telemac is composed of 33748 nodes and 66982 element 3 numerical methods 3 1 coupling algorithm between the 1d and 2d hydraulic models the coupling algorithm used in this study is the multiplicative form of the global in time schwarz algorithm adapted to the hydraulic case miglio et al 2005 malleron et al 2011 in the present work the upstream boundary conditions of the 1d models are prescribed by the discharge and the downstream boundary conditions are prescribed by a wet surface the upstream boundary conditions of the 2d model are prescribed by velocity vectors while the 2d downstream boundary condition is prescribed by the water level the multiplicative form of the global in time schwarz algorithm applied to the am bayonne area consists in solving the sub problems in eqs 5 and 6 until the stopping criteria is verified 5 l 1 a k q k 0 on ω 1 d i t t t i 1 2 3 a i k r 21 z 2 d i k 1 on γ i t t t i 1 2 q 3 k s 21 u k 1 v k 1 2 d 3 on γ 3 t t t 6 l 2 h k u k v k 0 on ω 2 d t t t u k v k i s 12 q 1 d i k on γ i t t t i 1 2 h 2 d 3 k r 12 a 3 k on γ 3 t t t where l 1 and l 2 stand for the shallow water operators for the 1d and the 2d models respectively and ω 2 d and ω 1 d i with i 1 2 3 correspond to the 2d domain and the three 1d sub domains respectively represented in fig 2 γ i with i 1 2 3 is the three 2d liquid boundaries z 2 d i k 1 is the mean 2d water level at the liquid boundary i with i 1 2 the boundary condition operators r 21 r 12 s 21 and s 12 are applied following the x y local axis along and perpendicular to the flow and they are described as follows r 21 translates the 2d water level along the cross sectional interface γ into a wet cross sectional area for the 1d boundary condition the 2d river width w z is integrated along the vertical axis z from the bottom of the river z f to the surface level z s for elements in the 1d 2d interface cross section following y r 21 z f z s w z dz r 12 translates the 1d wet cross sectional area into the 2d water level along the wet cross section following y if s and z s denote the wet cross sectional area and the water free surface elevation then z s verify s z f z s w z dz hence the free surface elevation and the bathymetry at the coupling interface enable to compute the 2d water level to impose at the coupling interface s 21 translates the velocity field at the 2d boundary γ into a discharge that is imposed at the 1d boundary the 2d water depth h 2 d is multiplied by the velocity at each point along the cross sectional area described by y then projected onto the local vector normal to the flow x to compute the 1d discharge q γ h 2 d y u v n dy where n is the local normal vector to the 2d flow along x and stands for the cross product s 12 translates the 1d discharge into 2d velocity components along y the 1d water level is mapped onto the 2d interface to compute the 2d water depth h 2 d velocity vectors are derive from a parabolic profile proportional to h 2 d u x α h x n x and v x α h x n y the proportionality coefficient α is set to ensure the discharge continuity at the 1d 2d interface γ h y u y v y n dy q at coupling iteration k the boundary conditions applied at the interface of the 1d model for integration over t t t are prescribed from the results of the 2d model at iteration k 1 over t t t once the 1d integration is completed the 2d boundary conditions for iteration k are prescribed from the results of the 1d model for integration over t t t the iterative coupling strategy is applied sequentially in time over the length of the entire flood event setting the stopping criteria requires a compromise between precision and computational time here it is based on the continuity of the discharge and the water level at the interface which implies the continuity of the riemann invariant at the interface this condition is necessary with respect to the hyperbolicity of each sub problem for sub critical flow this formulation differs from that of miglio et al 2005 which ensures the continuity of wet cross sectional area discharge and characteristic entering at the 1d 2d interface following the conclusions of tayachi 2013 it was verified that the flow remains mono dimensional at the coupling interfaces even for high flow rates velocity vectors at the 1d 2d interface γ 1 ω 1 d 1 from fig 2 from the 2d model are shown in fig 3 b bottom left of the plot these vectors follow the 1d river center line for all simulation time steps only one time is shown ensuring that the coupling algorithm converges within a limited number of iterations 3 2 data assimilation algorithms in the 1d model several sources of uncertainty are identified in the hydraulic modeling hydrological forcing data which describe the boundary conditions usually result from the transformation of uncertain observed water levels into discharges through an uncertain rating curve or from discharges that are forecast by uncertain hydrological models additionally the description of the river channel and flood plain geometry relies on a limited number of in situ measurements of topographic and bathymetric profiles that are then spatially interpolated the simplification of the flow to a 1d representation is also a significant limitation finally the calibration of friction coefficients is a pragmatic way to account for a variety of sources of uncertainty globally speaking uncertainties in the input data and in the hydraulic parameters translate into uncertainties in the simulated hydraulic state these uncertainties can be reduced with a da algorithm that consists in combining water level or discharge in situ observations in the numerical model to correct the model forcing parameters and or state ricci et al 2011 habert et al 2016 madsen and skotner 2005 in the present case of 1d 2d coupling improving the hydraulic state in the 1d sub models results in improving the boundary conditions for the 2d model while ensemble data assimilation algorithms showed promising results for hydraulics and flood forecasting e g barthelemy et al 2017 such algorithms were not considered here because of the computational cost instead the invariant kalman filter ikf from ricci et al 2011 was implemented in this formulation a simplified version of the kalman filter kalman 1960 is implemented with the background error covariance matrix in eq 7 which is not propagated between assimilation cycles to avoid either propagating an ensemble or using the model tangent linear matrix and carrying our computationally expensive matrix multiplication the analysis and propagation steps of the analysis are 7 x i a x i b bh t hbh t r 1 y i o h x i b x i 1 b m i i 1 x i a where the control vector x a q corresponds to the discretized hydraulic state excluding boundary values x i b and x i a are respectively the background and analysis vector at time i b is the background error covariance matrix r is the observation error covariance matrix y i o is the observation vector at time i and h is the observation operator in this study since the flow is sub critical the background error correlation function is assumed to be an asymmetric gaussian function with shorter correlation length scale downstream than upstream as illustrated in fig 4 further details about the da algorithm and the results for the am network are given in ricci et al 2011 in the 1d 2d coupling method the da analysis is applied at each observation time in each 1d sub model the analyzed state is then propagated in time by the 1d sub models and exchanged with the 2d model interfaces 3 3 cycled implementation of the coupling with openpalm the da algorithm is implemented with openpalm http www cerfacs fr globc palm web piacentini et al 2011 it is an open source flexible and powerful dynamic code coupler that has been developed jointly by cerfacs centre européen de recherche et formation avancée en calcul scientifique and onera office national d etudes et de recherches aérospatiales since 1998 openpalm was originally designed for da algorithms in operational oceanography forecasting it has now reached a high degree of maturity and stability with applications ranging from operational da oceanography atmospheric chemistry hydrology to industrially oriented multi physics modeling fluid structure interactions combustion acoustics interactions openpalm provides a straightforward parallel environment based on high performance implementation of the message passing interface mpi standard i e mpich openmpi lam mpi this interface is able to perform both data parallelism i e simultaneous execution on multiple cores of the same code for a unique data set with domain decomposition and task parallelism i e simultaneous execution on multiples cores of multiple tasks for the same or different data sets the openpalm coupling strategy relies both on task parallelism and data parallelism the 2d model is executed on several processors data parallelism while in parallel task parallelism the 1d sub models are executed each one on different processors the number of 1d sub models and the number of processors to run the 2d model are user defined and are handled by the openpalm coupling implementation so that this si generic for the user and can be applied to any hydraulic network the schwarz algorithm is implemented in 3 steps initialization iterative over time loops and finalization the initialization step consists in reading the user defined parameters model parameters and geometries these variables are allocated and stored in the openpalm block structure shared memory these input variables as well as the simulated wet area discharge and flow velocities at the 1d 2d interfaces are available for telemac and mascaret within the block structure the data exchanges are achieved with openpalm communications the finalization step consists in deallocating the variables it should be noted that the stationary hypothesis for the kf algorithm implies that the cost of da is limited to the matrix vector product in eq 7 and thus represents a non significant additional cost for the coupling algorithm still when an observation is assimilated in the 1d model a discontinuity with the 2d model may occur and several iterations of the schwarz algorithm may then be necessary to reconcile 1d and 2d models the coupling da strategy is applied over a sliding time window that covers the coupled propagation of the dynamics and analysis steps for da and a forecast period over which the 1d and 2d models are also coupled 4 results 4 1 schwarz algorithm convergence to the authors knowledge there are no theoretical results showing the schwarz algorithm convergence in the case of multi dimensional swe yet a convergence study was completed by tayachi 2013 for a linearized 3d navier stokes equations system coupled with a linearized 1d swe system it was shown that when absorbing boundary conditions are defined at the coupling interfaces the schwarz algorithm converges in 2 iterations the convergence was studied in detail by tayachi et al 2014 for a toy model and the importance of the interface position between the 1d and 2d models was confirmed the coupling interface must be located in an area where there are no 2d effects meaning that the flow is mono dimensional in the present work the convergence study was carried out over a 2 day simulation period for a flood event in 2011 the stopping criteria was defined as a 5 10 3 m difference between the 1d and 2d water levels and a 0 01 m s 1 difference in the velocities at the interface the coupling time step is 8 s which is also the 1d model time step and the 2d model time step is 4 s fig 5 a displays the convergence of the velocity difference at the coupling interface as a function of the schwartz iteration this was calculated at the interface downstream of the 2d model it was numerically observed fig 5 b that for the first coupling time steps convergence usually occurs within a small number of iterations for further coupling time steps convergence is reached after the first iteration in all cases the maximum number of iterations is set to 5 to limit the computational cost when assimilation is performed a discontinuity between the 1d and 2d models is introduced and the convergence of the schwarz algorithm may require more than one iteration as illustrated in fig 5 c it should be noted that depending on the location of the 1d 2d interface as well as the choice of the continuity variable the convergence may be harder to reach yet the computational cost for 8 s of the 2d model simulation is not significant and the cost of additional iterations for a small number of time steps has a negligible impact on the overall cost of the coupling over a flood event 4 2 optimal computational resources for coupled model for the bayonne 1d 2d coupled model numerical coupling experiments with increasing numbers of processors were carried out on a high performance computing platform at cerfacs with 53 tflop s and 158 nodes with 2 intel 8 cores processors intel sandybridge 5 6 ghz and 32go of memory ddr3 fig 6 shows the accelerating factor which is the cost of the model running on several processors compared to the cost of the model running on only one processor for the local 2d model red curve and the coupled model blue curve the accelerating factor of the 2d model is smaller than the ideal accelerating factor the black line that linearly depends on the number of processors used and reaches a maximum of 15 8 for 32 processors vertical green dashed line this is consistent with the general behavior of telemac 2d which is most efficient when approximately 1000 nodes are computed on each processor the accelerating factor of the coupled model is also smaller than the ideal accelerating factor and than that of telemac alone reaching a maximum of 6 4 for 32 processors this result is very important for operational flood forecasting services to calibrate their computational resources in order to run the 1d 2d multi dimensional coupled model it should be noted that in this comparison the 2d model is local while the coupled model covers the entire network with the 1d model the loss of scalability is due to the multiplicative schwarz algorithm in which the 2d model waits for the 1d model results at the current iteration k 4 3 coupling used in flood forecasting the 1d 2d coupling da strategy was applied to model a set of 7 real flood events with and without da in the am catchment with hydrological data provided by the spc gad the water level at the observing stations is displayed for the 2014 flood event in fig 7 except at peyrehorade where the 2009 event is used this event presents the most visible results in each panel the blue line represents the observations the black line represents the full 1d am model output the green line represents the coupled model output without da and the red line represents the coupled model with da for each flood event the statistical results including the bias and root mean square error rmse computed with respect to the observations are presented in fig 8 and fig 9 respectively for the analysis at 0 h lead time solid lines and for the forecast at the maximum lead time of each observing station dashed lines the 0 h lead time and the maximum lead time are respectively the initial time of forecast and the transfer time of the upstream boundary condition beyond which a forecast can not be performed because upstream boundary conditions are set constant during the forecast period the 1d model results are plotted in black the coupled model without da results are plotted in green and the coupled model with da results are plotted in red the mean bias and rmse computed over the 7 flood events are shown in table 1 4 3 1 coupled model without data assimilation the merits of the 1d 2d coupling are described for the different observing stations pont blanc 2d area the 2d model around bayonne improves the simulated water level at pont blanc compared to the 1d model especially for high flow rates as illustrated in fig 7 a for the 2014 event the water level increases when using the coupling solution at low tide and the shape of the limnigraph is improved for high flow events even though the water level remains slightly underestimated the bias is reduced for all 7 flood events as shown in fig 8 a equivalently for the 0 h and maximum lead times with a mean negative bias of 19 cm for the 1d model reducing to 13 cm for the coupled model table 1 the rmse is also reduced for all events fig 9 a with a mean rmse of 0 25 cm for the 1d model reducing to 0 17 cm for the coupled model with smaller improvements as the lead time increases table 1 villefranque nu the improvements observed in the 2d area especially over the plaine d ansot and the barthe de quartier bas also improves the boundary condition at the 1d 2d interface and consequently in the 1d section upstream of the interface on the nive as illustrated in fig 7 b for the 2014 event the full 1d model strongly under estimates the water level especially at high flow rates and the coupling solution brings more water into the system the bias is significantly reduced for all 7 flood events as shown in fig 8 b b equivalently for the 0 h and maximum lead time with a mean negative bias of 77 cm reducing to 40 cm table 1 the rmse is also significantly reduced for all events fig 9 b with a rmse of 0 90 cm for the 1d model reducing to 0 46 cm for the coupled model with similar improvements as the lead time increases table 1 peyrehorade au the au hydrodynamics is mono dimensional thus the impact of the coupling with respect to the 1d model is limited at peyrehorade as shown in fig 7 c at peyrehorade which is far from the 2d area the improvement of the 1d 2d coupling is small for the 2009 event the bias is slightly reduced for all 7 flood events as shown in fig 8 c equivalently for the 0 h and maximum lead times with a mean negative bias of 30 cm reducing to 26 cm table 1 the rmse is reduced for all events fig 9 c with a mean rmse of 0 40 cm for the am 1d model reducing to 0 35 cm for the coupled model with similar improvements as the lead time increases table 1 urt au at urt the water level is improved for medium flow rates especially at low tide but not for high flow rates as shown in fig 7 d at this observing station the 1d hypothesis is not valid and the 2d flood plain modeling should be taken into account for high flow rates the 1d model tends to over estimate the water level since the coupling tends to add water into the system it has a negative impact at high flow rates at urt the time averaged bias decreased for some events and increased for others fig 8 d the rmse results are not impacted by the coupling fig 9 d and table 1 lesseps ad the impact of coupling at lesseps is not visible neither in the bias nor on the rmse independent of the lead time figs 8 9 e table 1 as the dynamics are driven by the maritime boundary conditions 4 3 2 coupled model with data assimilation the merits of da in the 1d model coupled to the 2d model are described here for the different observing stations pont blanc 2d area observations at pont blanc are not assimilated as this observing station is located in the 2d area the assimilation of observations at villefranque upstream of pont blanc tends to cause an over estimation of the water level at high flow rates for the 2014 event as illustrated in fig 7 a for the 7 flood events it has a positive impact on the bias fig 8 a with a decrease of the mean bias from 13 cm for the coupled model to less than 1 cm at 0 h and maximum lead times table 1 da strongly reduces the bias with respect to the 1d model and the coupled simulation yet as expected when correcting the model state the merits of da decrease as the lead time increases the rmse fig 9 a is slightly reduced by da but most of the rmse improvements were already achieved with the coupling table 1 villefranque nu the da analysis has a significant effect at villefranque where the 1d model does not represent well the observations even taking into account the improvements gained by the coupling during the 2014 event and especially at high flow rates as illustrated in fig 7 b the assimilation reduces the negative bias from 77 cm for the 1d model to 21 cm at the 0 h lead time and 38 cm at the maximum lead time table 1 the bias reduction is clearly visible in fig 8 b b with a reduced impact for a larger lead time the rmse is also significantly improved by da for all flood events fig 9 b the mean rmse is 25 cm at the 0 h lead time and 49 cm at the maximum lead time compared to 90 cm for the 1d model and 46 cm for the coupled model as shown in table 1 peyrehorade au at peyrehorade the model without da provides poor results especially during flood peaks da leads to a significant improvement in the simulated water level as illustrated in fig 7 c for the 2009 flood event the water level is increased for the 7 flood events the rmse and bias are greatly improved with stronger impacts for shorter lead times as expected figs 8 c and 9 c the bias and rmse are reduced on average to less than 10 cm and 12 cm table 1 urt au at urt da has little impact on the results and the simulated water level remains far from the observations especially for high flow rates as presented in fig 7 d the 1d model is not able to represent the flood plain dynamics and over estimates water levels in spite of the assimilation of observations at peyrehorade and urt in fact the assimilation of observations at peyrehorade may cause a degradation at urt when the water is over estimated at urt and under estimated at peyrehorade or conversely the 1d model bias was already small but was further reduced by coupling increasing a little due to da and the rmse of 20 cm did not change table 1 lesseps ad observations at lesseps are not assimilated because this station is located at the coupling interface between the 2d model and the 1d model adour downstream as previously noted for the coupling da has a negligible impact at lesseps 5 conclusion this study describes the application of a multi dimensional coupling strategy between 1d and 2d models on the adour maritime catchment where the confluence between the nive and the adour rivers is simulated with a 2d local model and the upstream and downstream parts of the rivers are simulated with 1d sub models the models are coupled at their longitudinal boundaries with an iterative schwarz algorithm applied at each interface a kalman filter data assimilation algorithm is also applied in the 1d models so that in situ water level observations are assimilated to correct the simulated and forecasted water level and discharge the data assimilation and coupling strategy is implemented with the openpalm dynamical coupling software that allows for an efficient management of task and data parallelism the developed strategy is compatible with operational computational cost constraints the strategy was applied to simulate a set of 7 flood events in the adour catchment it was shown that the coupling algorithm converges with at most 5 iterations and the water level and velocity continuity is guaranteed at the model interfaces numerical experiments were performed on 32 processors to achieve the scalability skills of the 2d model of the local area further work is needed to determine more precise conclusions since a full 2d model is not yet available on the adour area the results highlighted that the coupling with the local 2d solution significantly improves the simulation in the 2d and 1d areas data assimilation in 1d sub models also leads to significant improvements for simulations and for short term forecasts since only the model state is corrected yet the improvements are minor in 1d areas where the 1d model results are not satisfying for instance at urt where flood plain modeling should be activated future work includes the implementation of the additive scheme for the schwarz algorithm which should decrease the computational cost of the coupled strategy since the 1d and 2d models would then run in parallel for an iteration with no waiting delay preliminary work on this formulation presented convergence issues that require further investigation alternative coupling strategies are also being investigated lateral coupling between the 1d model of the river bed and the 2d model of the flood plains allows activating the 2d model only for high flow rates when the 1d model simulates overflows in the flood plains this strategy remains compatible with data assimilation in the 1d model finally efforts are being made to develop data assimilation for the 2d models for state and parameter corrections with an ensemble based approach this choice enables a flow dependent estimation of the background error covariances keeping the cost of such ensemble based algorithms compatible with operational computational constraints remains a key challenge for that reason the use of a surrogate model in place of the forward model should be envisaged acknowledgements the authors acknowledge the spc gad for providing the observed and hydrological data funding for this study was provided by schapi and the eocoe energy oriented center of excellence h2020 european project wp4 
7247,soil moisture dynamics plays an active role in ecological and hydrological processes and it depends on a large number of environmental factors such as topographic attributes soil properties land use types and precipitation however studies must still clarify the relative significance of these environmental factors at different soil depths and at different spatial scales this study aimed 1 to characterize temporal and spatial variations in soil moisture content smc at four soil layers 0 40 40 100 100 200 and 200 500 cm and three spatial scales plot hillslope and region and 2 to determine their dominant controls in diverse soil layers at different spatial scales over semiarid and semi humid areas of the loess plateau china given the high co dependence of environmental factors partial least squares regression plsr was used to detect relative significance among 15 selected environmental factors that affect smc temporal variation in smc decreased with increasing soil depth and vertical changes in the 0 500 cm soil profile were divided into a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm plsr models simulated smc accurately in diverse soil layers at different scales almost all values for variation in response r2 and goodness of prediction q2 were 0 5 and 0 0975 respectively upper and lower layer smcs were the two most important factors that influenced diverse soil layers at three scales and these smc variables exhibited the highest importance in projection vip values the 7 day antecedent precipitation and 7 day antecedent potential evapotranspiration contributed significantly to smc only at the 0 40 cm soil layer vip of soil properties especially sand and silt content which influenced smc strongly increased significantly after increasing the measured scale mean annual precipitation and potential evapotranspiration also influenced smc at the regional scale significantly overall this study indicated that dominant controls of smc varied among three spatial scales on the loess plateau and vip was a function of spatial scale and soil depth keywords environmental factors scale dependency soil depth partial least squares regression loess plateau 1 introduction soil moisture dynamics is a significant variable that is related to a series of hydrological and soil biochemistry processes and vegetation growth istanbulluoglu and bras 2006 turcu et al 2005 schwinning et al 2004 characterizing soil moisture variations is important for both theoretical and practical applications in arid and semiarid ecosystems rodriguez iturbe and porporato 2005 however soil moisture exhibits high spatial and temporal variability hence knowledge of soil moisture and its spatiotemporal dynamics contributes significantly to rational management of water resources and land practices such as vegetation restoration projects on the loess plateau in china gao et al 2013 a large number of environmental factors control soil moisture which results in variability of spatial and temporal distributions these factors include climate henninger et al 1976 famiglietti et al 1998 zhu et al 2014 danelichen et al 2016 soil properties hawley et al 1983 wendroth et al 1999 cantón et al 2004 poltoradnev et al 2016 topography nyberg 1996 zhu and lin 2011 and land use jawson and niemann 2007 zhao et al 2011 huang et al 2016 table 1 however it is difficult to identify the relative significance of these factors due to their mutual and multiple influences on soil moisture some influential factors exhibited interdependence at multiple spatial scales and at different soil depths zhao et al 2011 zhu and lin 2011 soil moisture and its controlling factors vary at spatial scales from aggregate to global scales famiglietti et al 1998 wagenet 1998 entin et al 2000 qiu et al 2001 seneviratne et al 2010 zhu and lin 2011 zhu et al 2014 for example variability in soil moisture results mainly from the influence of pore size and organic films at the aggregate scale 0 025 0 1 m in diameter wagenet 1998 pallud et al 2010 from soil texture and organic matter at the plot scale 1 100 m2 wagenet 1998 entin et al 2000 qiu et al 2001 from soil properties topography and plant community at the hillslope scale 100 1 104 m2 famiglietti et al 1998 wagenet 1998 zhu et al 2014 from precipitation evapotranspiration geomorphology and land use at the regional scale 10 2 5 105 km2 wagenet 1998 entin et al 2000 seneviratne et al 2010 wang et al 2016 and from biome type and climate at the global scale groffman 1991 wagenet 1998 however most studies have focused primarily on one spatial scale zhu and lin 2011 therefore future research should center on analysis of various environmental factors at different scales to compare their relative significance to determine variations in soil moisture gao and shao 2012 noted that temporal stability of soil moisture content smc increased with increasing soil depth at three hillslope scales shen et al 2016 indicated dramatic changes in spatial pattern in smc among different soil layers along three 90 m transects bogena et al 2010 reported a decrease in spatial variability in soil moisture with increasing soil depth in a catchment however subsoil moisture received less attention in comparison with topsoil moisture in the region table 1 the main reason is that the topsoil moisture measurements at regional scale mainly rely on remote sensing techniques while it is not suitable for subsoil moisture estimations 15 cm depth seneviratne et al 2010 jawson and niemann 2007 poltoradnev et al 2016 implementation of the grain for green project on the loess plateau in 1999 aimed to reduce soil loss by planting trees and converting land use on steeper hillslopes from cropland to forest shrub and or grassland fu et al 2006 large scale engineering made significant changes in the natural environment and this influenced soil moisture significantly in this region limited rainfall and a deep groundwater table about 10 80 m below the soil surface make soil water a primary limiting factor that affects restoration of vegetation gao et al 2013 hu et al 2009 for example overuse of soil water by vegetation resulted in soil desiccation and formation of dry soil layers chen et al 2008b fu et al 2012 replenishment of soil water by rainwater failed to replenish soil water completely jian et al 2015 duan et al 2016a b consequently restored vegetation is subject to frequent periods of water stress therefore soil moisture dynamics and its dominant controls should be considered when restoring vegetation mcvicar et al 2010 the dominant controls of soil moisture dynamics must be identified for sustainable vegetation management three hillslopes with different climates soil textures and vegetation types were selected from northeast to southwest on the loess plateau and a series of soil moisture profiles were measured for two years along the transect of each hillslope climate variables soil properties topography and vegetation factors were collected and three spatial scales of plot hillslope and region were considered this study aimed 1 to characterize temporal and spatial variations in soil moisture at different depths and at different scales and 2 to detect dominant controls of soil moisture at different depths and at different scales using plsr 2 materials and methods 2 1 study site the study was conducted on the loess plateau of china which covers a total area of 620 000 km2 at an elevation of 200 3000 m above mean sea level amsl loess paleosol deposits are 30 200 m in thickness he et al 2003 shi and shao 2000 fig 1 soils are sandier in the northwest and more clayish in the southeast vegetation type from northwest to southeast changes from steppe desert through desert steppe to typical steppe forest steppe and then to forest zones the loess plateau belongs to a continental monsoon region where annual precipitation ranges from 150 mm in the northwest to 800 mm in the southeast and with 55 78 falling during june september 1951 2001 annual average temperature ranges from 3 6 c in the northwest to 14 3 c in the southeast he et al 2003 wang et al 2011 2 2 experimental design to study the effects of climate vegetation type soil texture and their combinations on soil moisture dynamics three hillslopes named shenmu ansai and changwu were selected from northeast to southwest in the central part of the loess plateau fig 1 the shenmu hillslope represented a semiarid steppe with mean annual precipitation of 437 mm and featured sandy loam as its primary soil texture korshinsk peashrub caragana korshinskii kom was the main plant species in shenmu the ansai hillslope was a semiarid forest steppe region with a mean annual precipitation of 505 mm and that featured silty loam as the main soil texture plant species in ansai were mainly comprised of a mixture of sea buckthorn hippophae rhamnoides l and black locust robinia pseudoacacia l the changwu hillslope was a semi humid forest zone with mean annual precipitation of 582 mm that included silty clay loam as the main soil texture changwu was populated primarily by black locust the groundwater table was approximately 18 110 and 50 m below ground surface at shenmu ansai and changwu hillslopes respectively the detailed information for each hillslope is shown in table 2 the number of sampling locations along transects at shenmu ansai and changwu hillslopes totaled 40 35 and 35 respectively where sampling locations were at uniform intervals of 6 5 6 and 5 4 m respectively an aluminum neutron probe access tube with a length of 5 2 m was installed at each sampling location for soil moisture measurements in this study soil moisture dynamics and dominant controls were analyzed at the three spatial scales plot hillslope and region sampling location in the middle of each transect represented the plot scale which was 6 6 m at the shenmu hillslope 5 6 5 6 m at the ansai hillslope and 5 4 5 4 m at the changwu hillslope each transect represented the hillslope scale shenmu 243 m 20 m ansai 191 m 18 m and changwu 189 m 20 m the combination of three transects depicted the regional scale of 458 km 100 km in the central part of the loess plateau which included the soil types of sandy loam silty loam and silty clay loam climatic conditions with mean annual precipitation of 350 600 mm and vegetative types from steppe to forest zones the plsr model was used to analyze variations in soil moisture dynamics and dominant variables at different scales 2 3 field collection of data soil volumetric moisture content was measured to a depth of 5 m using a neutron probe cnc 503b dr chaoneng china that was calibrated using standard methods huang and gallichand 2006 fu et al 2012 during april november volumetric smc was measured four times per month in the rainy season july september and two times per month in the dry season during the months of april june and october november measurements were made at depth increments of 10 and 20 cm in 0 1 and 1 5 m soil layers respectively mean smc in the 0 5 m profile was determined using depth weighting a total of 800 1050 and 700 data points were collected in two years at the shenmu ansai and changwu hillslopes respectively at each sampling location 25 disturbed soil samples were collected at depth increments of 0 2 m during installation of the access tube and these were used for soil particle analysis soil samples were air dried and passed through a 2 mm sieve soil particle sizes were measured using a mastersizer 2000 malvern instruments malvern england undisturbed soil samples were also collected at a depth of 30 cm and 0 5 m away from the access tube using a cutting ring 5 cm diameter 100 cm3 volume to measure soil bulk density bd g cm 3 bd was measured using a gravimetric method astmc29 c29m 09 2003 leaf area index lai was measured with a plant canopy analyzer lai 2000 li cor usa and synchronized with smc determination elevation at each sampling location slope gradient sg and slope aspect sa were measured using a differential kinematic gps a laser slope meter and an inclinometer respectively long term daily meteorological data were collected from the nearest meteorological station and daily potential evapotranspiration was calculated using the penman equation shuttleworth 1993 mcvicar et al 2012 2 4 soil profile division the soil profile division in smc for three locations of shenmu ansai and changwu was determined using the mean of coefficient of variation cv at each smc measured point the mean of cv at each measured point was calculated as follows 1 cv cv 1 j cv 2 j cv 3 j 3 2 cv i j sd i j θ i j 3 sd i j 1 n 1 k 1 n θ i j k θ i j 2 where i is the location i 1 2 3 j is the measured point in soil profile j 1 2 30 k is each smc measurement k 1 2 n n is the total number of smc measurements sdi j and θ i j are the standard deviation and the mean of all smc measurements at the j th measured point for the location i during the observation period cm3 cm 3 respectively and θi j k is the individual smc measurement during the observation period cm3 cm 3 on the loess plateau the standards of soil profile division were defined by chen et al 2008a as follows cv 30 is a fast changing layer cv 20 30 is an active layer cv 10 20 is a sub active layer and cv 10 is a stable layer 2 5 potential soil moisture controls we selected 15 potential environmental factors that affected soil moisture and they included smc of neighboring layers climatic variables soil properties vegetation factors and topographic attributes table 3 with the exception of 7 day antecedent precipitation a7p 7 day antecedent potential evapotranspiration a7e and the smc of neighboring layers the remaining potential environmental factors have been investigated frequently in previous studies gómez plaza et al 2001 qiu et al 2001 brocca et al 2011 smc of neighboring layers was selected in this study because the dried soil layer which often appears in the soil profile of 100 cm 500 cm has become a serious environmental problem for sustainable vegetation management on the loess plateau wang et al 2011 this dried soil layer occurs when water consumption by vegetation is greater than water replenishment by precipitation chen et al 2008a b fu et al 2012 using the factor of smc of neighboring layers the effect of different vegetation conditions on smc at different soil layers could be simulated vegetation types plant biomass vegetative cover plant density lai and fine root distribution in the soil profile are very important variables that affect smc however lai is one of the most important vegetation factors and it can integrate the effects of other vegetation factors on smc such as vegetation biomass coverage and density zhao et al 2011 gómez plaza et al 2001 cantón et al 2004 cheng et al 2009 han et al 2007 duan et al 2016a b and zhang 2018 measured fine root distributions in soil profiles at shenmu ansai and changwu hillslopes respectively and they found that the maximum root depth varied 3 5 4 5 m for three vegetation types their results also indicated that approximately 86 of the fine root system was in the 0 100 cm layer the variable of fine root distribution is difficult to obtain at the regional scale whereas the spatial variability in this variable was a function of amsl bd and soil texture at the hillslope scale duan et al 2017 duan 2017 zhang 2018 because amsl bd and soil texture were environmental factors of smc lai was used only to represent the effect of vegetation factors on smc in this study climatic controls included mean annual precipitation map mean annual potential evapotranspiration mae a7p and a7e soil controls consisted of bd soil median grain size d 0 5 sand content sac silt content sic and clay content clc vegetation control included lai and topographical attributes included amsl sg and sa at the plot scale five potential factors were selected from all environmental factors table 3 these factors included upper layer smc usmc lower layer smc lsmc a7p a7e and lai at the hillslope scale in addition to the five environmental controls at the plot scale terrain attributes and soil properties affected spatial variation in soil moisture although sac sic clc d 0 5 bd and amsl potentially controlled soil moisture dynamics at the regional scale map mae sg and sa were considered because of their corresponding variation with increasing space scale then 15 environmental factors were analyzed to predict smc at the regional scale table 3 2 6 partial least squares regression plsr the plsr technique was used to determine the relationship between two sets of variables the matrix xm n that consisted of m variables columns and n objects rows and a response vector yn 1 plsr uses a few important linear combinations components or factors of the original x values that describe most of the inherent variable information in y the most important linear combinations are identified mathematically by maximizing the covariance between y and all possible linear functions of x shi et al 2013 hence plsr is an alternative to ordinary regression for problems with partly or highly co linear predictor variables wold et al 2001 carrascal et al 2009 and it has been used widely in the fields of hydrology environment and ecology ai et al 2015 huang et al 2016 plsr can be used to identify significant variables from environmental factors of smc that have remarkable correlation details on theory principles and applications of plsr can be found in carrascal et al 2009 and abdi 2010 to overcome the problem of over fitting and to achieve an optimal balance between the explained variation in responses r2 and the predictive ability of the model goodness of prediction q2 an appropriate number of significant components in the plsr model are determined by the cross validation abdi 2010 r2 and q2 are computed using the following equations 4 r 2 i 1 n y i y 2 i 1 n y i y 2 5 q 2 1 press ss a a 1 2 3 m where y i and y i are the predicted and measured values respectively y is the mean the press is the prediction error sum of squares the ss is the residual sum of squares and m is the number of plsr components when r2 and q2 are 0 5 and 0 0975 respectively then the model has significantly predictive ability trap et al 2013 two important parameters of variable importance in project vip and regression coefficients rc explain the relative influence of each independent variable the vip scores summarize the influence of individual x variable on the plsr model and provide a useful method to select variables that contribute the most to the y variance explanation the vip score for the j th variable is given as 6 vip j f 1 f w jf 2 ssy f j ssy total f where wjf is the weight value for the j variable and the f component ssyf is the sum of squares explained by the variance for the f th component and j number of x variables ssytotal is the total sum of squares explained by the dependent variable and f is the total number of components the equations for calculating ssyf and ssytotal are given by farrés et al 2015 the w2 jf gives the importance of the j th variable in each f th component and wjf is calculated using the plsr algorithm wold et al 2001 farrés et al 2015 in general an independent variable with a vip value 1 is the most relevant in explaining the dependent variables of smc whereas a value 0 5 indicates that the variable does not explain dependent variables significantly the interval between 0 5 and 1 is a gray zone where the significance level depends on the vip value trap et al 2013 jia et al 2009 the rc value represents the change in the response when the scaled variable varies from 0 to 1 in coded units while other variables are kept at their averages abdi 2010 the rc is significant above the noise when the confidence interval does not cross zero carrascal et al 2009 farrés et al 2015 the rc reveals the impact direction and strength of each variable in the plsr model all environmental characteristics were not necessarily included in this model because redundant variables can lead to plsr models with low statistical significance therefore plsr analysis was used to obtain an optimal model first simulation was conducted using the plsr model with all predictors next a series of simulations used new plsr models where each new plsr analysis eliminated a variable the new plsr model with the highest q2 was selected this procedure was repeated until two predictor variables remained finally the model with the highest q2 was selected as the optimal smc model at plot scale all measured smcs at four soil layers of 0 40 40 100 100 200 and 200 500 cm during the observation period were used to implement plsr analysis while all measured smcs at four soil layers and at all sampling locations along each transect were used for plsr analysis at hillslope scale at regional scale plsr analysis included all individual measurements at all three hillslopes 3 results 3 1 soil water dynamics and soil profile division fig 2 shows smc variations in the 500 cm soil profile at each plot during the growing season april october 2015 at the three plots smc at the changwu plot had the highest mean of 0 15 cm3 cm 3 smc at the ansai plot had a lower mean of 0 11 cm3 cm 3 and both were higher than that at the shenmu plot 0 08 cm3 cm 3 smc exhibited variation over the entire profile but the degree of variation declined with increasing soil depth in the surface soil layer smc showed more significant variation in a range of 0 1 0 3 cm3 cm 3 at ansai and changwu plots with increasing depth degree of changes in smc decreased and it stabilized below the depth of 200 cm therefore soil moisture variation showed a remarkable division in the profile according to the mean cv along the soil profile at three locations vertical changes in the soil profile at 0 500 cm were divided into four layers a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm with cv values of 32 4 49 4 19 8 28 4 10 3 17 8 and 3 4 9 5 respectively 3 2 soil water dynamics in different layers at three hillslopes fig 3 shows the time series of hillslope mean smc for four soil layers at the three hillslopes at the shenmu hillslope the soil layer at 100 200 cm depth had the highest smc and showed minimal fluctuation over time however the surface layer exhibited higher smc than the underlying layers at ansai and changwu hillslopes and displayed more significant irregularity with time as annual precipitation and clay soil texture increased table 2 sds on each measured date also exhibited some differences among the three hillslopes in general sd values at the shenmu hillslope were less than those at ansai and changwu hillslopes with ranges of 0 005 0 051 0 002 0 101 and 0 017 0 044 cm3 cm 3 respectively during the rainy season from july to september the temporal variation in smc in the 0 40 cm and 40 100 cm soil layers were large at all three hillslopes due to the effects of large rainfall events and evapotranspiration in contrast the temporal changes in smc at the deeper soil layers of 100 200 cm and 200 500 cm were relatively stable due to lower rates of both rainfall infiltration and root water uptake therefore large rainfall events and evapotranspiration strongly influenced soil moisture dynamics in the 0 40 cm and 40 100 cm soil layers 3 3 dominant controls of soil moisture dynamics at different scales instead of the climatic controls that we selected in previous studies a7p and a7e were used to replace the 3 day antecedent precipitation a3p and 3 day potential evapotranspiration a3e which proved to influence soil moisture dynamics significantly in humid regions huang et al 2016 in this study a7p and a7e had a more significant influence on soil moisture dynamics in semiarid and semi humid regions of the loess plateau p 0 05 in comparison with a3p and a3e after comparing pearson correlation coefficients between smc and its antecedent precipitation and potential evapotranspiration during the 1 30 days therefore we used 15 potential environmental factors to investigate dominant controls of smc at different scales table 3 3 3 1 plot scale table 4 summarizes individual plsr models for soil moisture at each measured layer of the three plots including r2 and q2 in optimal models for depths of 40 100 cm and 100 200 cm q2 and r2 reached at least 0 5 and most reached 0 8 table 4 in other layers q2 was a little lower than 0 5 except the depth of 0 40 cm at shenmu plot and the depth of 200 500 cm at changwu plot plsr models predicted smc accurately at the plot scale fig 4 illustrates vip values for smc at the plot scale with rcs plotted against predictors in separate models for 0 40 cm depths lsmc had the highest vip values followed by three other factors that exhibited minimal difference in models for other layers usmc and lsmc were also the highest but a7p was not included in plsr models at 40 100 cm and 200 500 cm and a7p had small values in the 100 200 cm model vip of a7e and lai showed minimal variation and some differences among the three locations rcs of usmc lsmc and a7p were correlated positively to smc at rc values 0 but a7e and lai were correlated negatively to smc at rc 0 smc measurements at the depth of 100 200 cm of ansai plot were used as an example to conduct the sensitivity analysis of optimal plsr model to rc values the plsr model indicated that usmc lsmc a7p a7e and lai were dominant factors of smc at the depth of 100 200 cm of ansai plot fig 4 and the optimal equation was as follows 7 smcs 100 200 0 364 x usmc 0 527 x lsmc 0 086 x a 7 p 0 206 x a 7 e 0 210 x lai q 2 0 782 r 2 0 855 rmse 0 385 where smcs is the scaled smc coded by the mean of smc and xusmc xlsmc xa7p xa7e and xlai are the scaled variables of usmc lsmc a7p a7e and lai coded by the mean of each variable respectively the performance of optimal plsr model toward the changes of each rc value whose r2 and rmse varied with the rc of each dominant factor being either increased or decreased 10 from the optimized one in eq 7 is shown in table 5 among five rc values the rc of variable lsmc was the most sensitive whereas the rc of variable lai appeared to be the least sensitive 3 3 2 hillslope scale table 6 summarizes individual plsr models constructed for soil moisture at each layer for the three hillslopes similar to the plot scale q2 and r2 values at depths of 40 100 cm and 100 200 cm were higher than those in other layers but the value was relatively low except for that of the changwu hillslope r2 and q2 were minimal at the shenmu hillslope at a depth of 200 500 cm followed by the 0 40 cm depth only two models yielded r2 0 5 fig 5 depicts vip values separately calculated for smc at the hillslope scale with rcs plotted against predictors lsmc and usmc appeared as the most important controls for smc at all measurement depths but other controls varied at different layers a7p and a7e were the second dominant controls at a depth of 0 40 cm with vip values 1 however the influence of a7p and a7e weakened with increasing depth other factors which included bd lai and amsl showed a little variability among the three hillslopes with good performance at one or two sites vip values for soil texture were the lowest among all factors and most of them were 0 5 for all layers rcs of usmc and lsmc were correlated positively with smc at values 0 5 values of a7p and a7e varied at different layers at the 0 40 cm layer smc increased with high a7p indicated by a positive rc but a7e indicated by a negative rc contributed to low smcs no clear connection existed between smc and a7p or a7p in the other layers where their values approached 0 few rc values of other factors such as bd at the shenmu hillslope at a depth of 40 100 cm and amsl at the changwu hillslope at a depth of 200 500 cm showed a significant correlation with smc 3 3 3 regional scale table 7 summarizes individual plsr models constructed for soil moisture at each measurement layer at the regional scale models with r2 and q2 0 75 at all layers displayed good performances and models for all layers identified four components that reached maximum q2 table 7 the first component explained 59 1 of the variance in changes in smc at a depth of 0 40 cm and approximately 70 variance at the three other layers the addition of second components explained approximately 10 of the variance the first four components explained 75 5 86 1 90 2 and 84 1 of total variance at measurement depths of 0 40 40 100 100 200 200 500 cm respectively further addition of components to plsr models did not substantially increase the percentage of variance that was explained as linear combinations of original variables that define scores plsr weights can describe the quantitative relationship between predictors and results abdi 2010 usmc and lsmc showed the highest vip values followed by map mae sac and sic fig 6 six factors and a7e at the 0 40 cm depth were the most important factors with values 1 rcs indicated that smc was lower for higher mae sac and lai values and higher for increased usmc lsmc map and sic values vip values of other predictors were 1 therefore these predictors barely influenced the smc prediction cumulative q2 values of plsr models were 0 75 which indicated their good predictive ability and robustness validation of the best plsr prediction model was illustrated using a scatter diagram that compared the measured and predicted smc values fig 7 at a depth of 0 40 cm points of smc were the most scattered as depth increased points of smc became more aggregated and appeared closer to one line as reflected by the low root mean square error rmse rmse for 100 200 and 200 500 cm at 0 014 and 0 015 cm3 cm 3 were considerably lower than that for 0 40 and 40 100 cm at 0 035 and 0 022 cm3 cm 3 respectively slopes of linear equations for all soil layers showed high values within a range of 0 756 0 904 these results further showed the accuracy of simulation by the plsr model 4 discussion 4 1 soil water dynamics and soil profile division according to profile variations in smc and its cv values vertical changes in the 0 500 cm soil profile were divided into four layers fig 2 this pattern was attributed primarily to seasonal distribution of annual precipitation gómez plaza et al 2001 in the loess plateau 55 78 of precipitation occurred during july september shi and shao 2000 yang and shao 2000 which led to variations in smc in our study areas similarly li 2001 also observed that soil moisture depth which was affected by rainfall infiltration usually occurred at 100 300 cm with an average of 200 cm depth of soil moisture 500 cm only appeared during the wettest years in the loess plateau where the frequency of such wet years was about 10 which means a recurrence of this event occurred only once every 10 years fu et al 2012 4 2 soil moisture dynamics of different layers at three hillslopes at the shenmu hillslope smc was low and showed no significant difference between surface and underlying layers these results were attributed mainly to less precipitation and high potential evapotranspiration and sandy loam soil in this area reduced the storage of moisture table 1 with increasing amounts of rainfall and decreasing potential evapotranspiration surface layers yielded higher smc than the underlying layers at the ansai and changwu hillslopes soil moisture dynamics also displayed a high and low change during the growing season may november at the three hillslopes especially for deep soil layers which were less influenced by antecedent precipitation and potential evapotranspiration at the beginning of the growing season may july soil moisture dynamics showed a decreased trend and then reached the minimum near july which resulted from high water consumption by growing vegetation the significant increase in lai from 0 22 to 2 89 from 0 35 to 3 5 and from 0 41 to 2 9 at the shenmu ansai and changwu hillslopes respectively supported this trend with the arrival of monsoon rains during july september the supply of soil moisture was higher than consumption and soil moisture dynamics increased from august to november which indicated that soil moisture dynamics depended directly on uptake of moisture by vegetation and interactions between rainfall and potential evapotranspiration on the loess plateau 4 3 dominant controls of soil smc at different scales 4 3 1 modeling results of smc at different scales at the plot scale smc changed with time and was related to five potential temporal factors the model results at depths of 40 100 cm and 100 200 cm were better than at depths of 0 40 cm and 200 500 cm at the hillslope scale smc exhibited spatial variation and factors soil texture and amsl that were related to smc spatial variation were considered in plsr models although the number of environmental factors increased modeling results were poorer than at the plot scale as indicated by lower r2 and q2 this condition possibly resulted from soil water redistribution along the hillslope and spatial variation in soil textures and water uptake by roots at the regional scale all spatiotemporal factors were included in the plsr model table 1 all r2 and q2 values were 0 75 in all layers which indicated good simulation results two important factors of map and mae were considered in the model at the regional scale with vips 1 fig 6 4 3 2 dominant controls of smc at different scales usmc and lsmc had the highest vip values and they were the most important factors across the three scales which indicated a close relationship of soil moisture between neighboring soil layers figs 4 6 in contrast to the stability of smc between neighboring layers a7p and a7e showed significant differences among various soil layers at the three scales a7p and a7e influenced smc significantly with most vip values 1 at depths of 0 40 cm but they exhibited little influence in the other layers figs 4 6 hence most rainfall penetrated the soil profile at 0 40 cm and a7e exhibited a significant influence on smc in that layer these results were attributed mainly to supply and consumption of soil moisture rainfall events infiltration processes soil evaporation vegetation transpiration and lateral flows contributed to soil moisture in a highly variable pattern zhao et al 2011 zhu et al 2014 however the climate in our study area is semiarid and sub humid with annual precipitation of 437 582 mm and potential evapotranspiration of 1233 1398 mm precipitation and potential evapotranspiration showed limited influence in deep soils thus smc in the top layer only had the closest relationship with a7p and a7e though most factors that we examined were stable at all three scales the vip of soil texture differed remarkably between the hillslope and regional scales especially for sac and sic which were the most important factors with values 1 fig 6 zhu and lin 2011 obtained similar values and observed that soil properties became less significant to smc in the hillslope transects that had comparatively homogenous texture the main reason was that topographic attributes dominated over soil properties in controlling smc variation however soil properties had an obvious influence on smc when soil properties exhibited significant variation at the regional scale soil properties mainly included texture organic matter content porosity structure and macroporosity the impact of soil properties on spatial variation in soil moisture has been well documented with large scale data sets that were acquired from soil moisture field campaigns crow et al 2012 this study detected the dominant controls of smc in diverse soil layers at different spatial scales and identified their relative significance to impact soil moisture dynamics these results are useful to develop eco hydrological models and optimize soil water and vegetation management over semiarid and semi humid areas of the loess plateau china this region is well known for its serious soil erosion largely caused by cultivation of marginal lands and destruction of natural vegetation huang and gallichand 2006 chen et al 2008a b wang et al 2011 in order to control serious soil erosion a great effort has been made to plant trees and grasses on slope land since the end of 1950 s indeed annual sediment loss has reduced from 1760 mt in 1950 s to 360 mt in 2000 s and vegetation coverage has increased from 6 5 in 1970 s to 38 in 2006 guo et al 2014 however large scale vegetation restoration also aggravated water scarcity and gradually led to the formation of a dry soil layer in the soil profile of 100 600 cm in which smc was less than stable field water capacity chen et al 2008a b wang et al 2011 the dry soil layer prevented water interchanges between upper soil layer and groundwater conversely reduced the anti drought capability of deep rooted plants and heavily influenced the growth and natural succession of vegetation wang et al 2011 studied the thickness forming depth and smc of dry soil layer in the loess plateau their results indicated that the thickness of dry soil layer varied from 126 cm to 500 cm with an average of 289 cm the forming depth from 100 cm to 198 cm with an average of 139 cm and the smc from 1 7 to 11 7 with an average of 7 5 the distribution pattern of dry soil layer was mainly related to climate region soil properties vegetation type and growth age shangguan 2007 wang et al 2011 chen et al 2008b different from previous investigations this study used plsr to detect dominant controls of smc among 15 selected environmental factors the results indicated that dominant controls of smc varied in diverse soil layers at different spatial scales understanding the dominant factors of smc at different scales enables scientifically based policies to be made that would mediate the formation of dry soil layer and sustain development of the economy and restoration of the natural environment in the semiarid and semi humid regions of the loess plateau and in similar regions elsewhere 5 conclusion this study identified the main controlling factors of smc in various soil layers at different scales on the loess plateau using plsr models and the conclusions of this study are as follows 1 temporal variation in soil water decreased with increasing soil depth according to sd and cv of smc and vertical changes in the 0 500 cm soil profile were divided into a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm 2 plsr models simulated smc accurately in various soil layers at different scales with almost all r2 and q2 values 0 5 and 0 0975 respectively 3 usmc and lsmc which had the highest vip values were the most important factors in various soil layers at three scales a7p and a7e contributed significantly to smc at a depth of 0 40 cm but they showed little relationship with the other layers vip of soil properties showed a notable increase as the scale of measurement increased specifically sac and sic showed strong effects on smc and map and mae also had a significant influence on smc at the regional scale acknowledgements financial support for this work was provided by nsfc rcuk nerc no 41571130082 and the national natural science foundation of china nos 41390463 and 41571213 the authors thank the members of the changwu ansai and shenmu ecology stations chinese academy of sciences and ministry of water resources for their assistance special thanks are given to the editor in chief prof mcvicar and two anonymous reviewers for their comments and suggestions 
7247,soil moisture dynamics plays an active role in ecological and hydrological processes and it depends on a large number of environmental factors such as topographic attributes soil properties land use types and precipitation however studies must still clarify the relative significance of these environmental factors at different soil depths and at different spatial scales this study aimed 1 to characterize temporal and spatial variations in soil moisture content smc at four soil layers 0 40 40 100 100 200 and 200 500 cm and three spatial scales plot hillslope and region and 2 to determine their dominant controls in diverse soil layers at different spatial scales over semiarid and semi humid areas of the loess plateau china given the high co dependence of environmental factors partial least squares regression plsr was used to detect relative significance among 15 selected environmental factors that affect smc temporal variation in smc decreased with increasing soil depth and vertical changes in the 0 500 cm soil profile were divided into a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm plsr models simulated smc accurately in diverse soil layers at different scales almost all values for variation in response r2 and goodness of prediction q2 were 0 5 and 0 0975 respectively upper and lower layer smcs were the two most important factors that influenced diverse soil layers at three scales and these smc variables exhibited the highest importance in projection vip values the 7 day antecedent precipitation and 7 day antecedent potential evapotranspiration contributed significantly to smc only at the 0 40 cm soil layer vip of soil properties especially sand and silt content which influenced smc strongly increased significantly after increasing the measured scale mean annual precipitation and potential evapotranspiration also influenced smc at the regional scale significantly overall this study indicated that dominant controls of smc varied among three spatial scales on the loess plateau and vip was a function of spatial scale and soil depth keywords environmental factors scale dependency soil depth partial least squares regression loess plateau 1 introduction soil moisture dynamics is a significant variable that is related to a series of hydrological and soil biochemistry processes and vegetation growth istanbulluoglu and bras 2006 turcu et al 2005 schwinning et al 2004 characterizing soil moisture variations is important for both theoretical and practical applications in arid and semiarid ecosystems rodriguez iturbe and porporato 2005 however soil moisture exhibits high spatial and temporal variability hence knowledge of soil moisture and its spatiotemporal dynamics contributes significantly to rational management of water resources and land practices such as vegetation restoration projects on the loess plateau in china gao et al 2013 a large number of environmental factors control soil moisture which results in variability of spatial and temporal distributions these factors include climate henninger et al 1976 famiglietti et al 1998 zhu et al 2014 danelichen et al 2016 soil properties hawley et al 1983 wendroth et al 1999 cantón et al 2004 poltoradnev et al 2016 topography nyberg 1996 zhu and lin 2011 and land use jawson and niemann 2007 zhao et al 2011 huang et al 2016 table 1 however it is difficult to identify the relative significance of these factors due to their mutual and multiple influences on soil moisture some influential factors exhibited interdependence at multiple spatial scales and at different soil depths zhao et al 2011 zhu and lin 2011 soil moisture and its controlling factors vary at spatial scales from aggregate to global scales famiglietti et al 1998 wagenet 1998 entin et al 2000 qiu et al 2001 seneviratne et al 2010 zhu and lin 2011 zhu et al 2014 for example variability in soil moisture results mainly from the influence of pore size and organic films at the aggregate scale 0 025 0 1 m in diameter wagenet 1998 pallud et al 2010 from soil texture and organic matter at the plot scale 1 100 m2 wagenet 1998 entin et al 2000 qiu et al 2001 from soil properties topography and plant community at the hillslope scale 100 1 104 m2 famiglietti et al 1998 wagenet 1998 zhu et al 2014 from precipitation evapotranspiration geomorphology and land use at the regional scale 10 2 5 105 km2 wagenet 1998 entin et al 2000 seneviratne et al 2010 wang et al 2016 and from biome type and climate at the global scale groffman 1991 wagenet 1998 however most studies have focused primarily on one spatial scale zhu and lin 2011 therefore future research should center on analysis of various environmental factors at different scales to compare their relative significance to determine variations in soil moisture gao and shao 2012 noted that temporal stability of soil moisture content smc increased with increasing soil depth at three hillslope scales shen et al 2016 indicated dramatic changes in spatial pattern in smc among different soil layers along three 90 m transects bogena et al 2010 reported a decrease in spatial variability in soil moisture with increasing soil depth in a catchment however subsoil moisture received less attention in comparison with topsoil moisture in the region table 1 the main reason is that the topsoil moisture measurements at regional scale mainly rely on remote sensing techniques while it is not suitable for subsoil moisture estimations 15 cm depth seneviratne et al 2010 jawson and niemann 2007 poltoradnev et al 2016 implementation of the grain for green project on the loess plateau in 1999 aimed to reduce soil loss by planting trees and converting land use on steeper hillslopes from cropland to forest shrub and or grassland fu et al 2006 large scale engineering made significant changes in the natural environment and this influenced soil moisture significantly in this region limited rainfall and a deep groundwater table about 10 80 m below the soil surface make soil water a primary limiting factor that affects restoration of vegetation gao et al 2013 hu et al 2009 for example overuse of soil water by vegetation resulted in soil desiccation and formation of dry soil layers chen et al 2008b fu et al 2012 replenishment of soil water by rainwater failed to replenish soil water completely jian et al 2015 duan et al 2016a b consequently restored vegetation is subject to frequent periods of water stress therefore soil moisture dynamics and its dominant controls should be considered when restoring vegetation mcvicar et al 2010 the dominant controls of soil moisture dynamics must be identified for sustainable vegetation management three hillslopes with different climates soil textures and vegetation types were selected from northeast to southwest on the loess plateau and a series of soil moisture profiles were measured for two years along the transect of each hillslope climate variables soil properties topography and vegetation factors were collected and three spatial scales of plot hillslope and region were considered this study aimed 1 to characterize temporal and spatial variations in soil moisture at different depths and at different scales and 2 to detect dominant controls of soil moisture at different depths and at different scales using plsr 2 materials and methods 2 1 study site the study was conducted on the loess plateau of china which covers a total area of 620 000 km2 at an elevation of 200 3000 m above mean sea level amsl loess paleosol deposits are 30 200 m in thickness he et al 2003 shi and shao 2000 fig 1 soils are sandier in the northwest and more clayish in the southeast vegetation type from northwest to southeast changes from steppe desert through desert steppe to typical steppe forest steppe and then to forest zones the loess plateau belongs to a continental monsoon region where annual precipitation ranges from 150 mm in the northwest to 800 mm in the southeast and with 55 78 falling during june september 1951 2001 annual average temperature ranges from 3 6 c in the northwest to 14 3 c in the southeast he et al 2003 wang et al 2011 2 2 experimental design to study the effects of climate vegetation type soil texture and their combinations on soil moisture dynamics three hillslopes named shenmu ansai and changwu were selected from northeast to southwest in the central part of the loess plateau fig 1 the shenmu hillslope represented a semiarid steppe with mean annual precipitation of 437 mm and featured sandy loam as its primary soil texture korshinsk peashrub caragana korshinskii kom was the main plant species in shenmu the ansai hillslope was a semiarid forest steppe region with a mean annual precipitation of 505 mm and that featured silty loam as the main soil texture plant species in ansai were mainly comprised of a mixture of sea buckthorn hippophae rhamnoides l and black locust robinia pseudoacacia l the changwu hillslope was a semi humid forest zone with mean annual precipitation of 582 mm that included silty clay loam as the main soil texture changwu was populated primarily by black locust the groundwater table was approximately 18 110 and 50 m below ground surface at shenmu ansai and changwu hillslopes respectively the detailed information for each hillslope is shown in table 2 the number of sampling locations along transects at shenmu ansai and changwu hillslopes totaled 40 35 and 35 respectively where sampling locations were at uniform intervals of 6 5 6 and 5 4 m respectively an aluminum neutron probe access tube with a length of 5 2 m was installed at each sampling location for soil moisture measurements in this study soil moisture dynamics and dominant controls were analyzed at the three spatial scales plot hillslope and region sampling location in the middle of each transect represented the plot scale which was 6 6 m at the shenmu hillslope 5 6 5 6 m at the ansai hillslope and 5 4 5 4 m at the changwu hillslope each transect represented the hillslope scale shenmu 243 m 20 m ansai 191 m 18 m and changwu 189 m 20 m the combination of three transects depicted the regional scale of 458 km 100 km in the central part of the loess plateau which included the soil types of sandy loam silty loam and silty clay loam climatic conditions with mean annual precipitation of 350 600 mm and vegetative types from steppe to forest zones the plsr model was used to analyze variations in soil moisture dynamics and dominant variables at different scales 2 3 field collection of data soil volumetric moisture content was measured to a depth of 5 m using a neutron probe cnc 503b dr chaoneng china that was calibrated using standard methods huang and gallichand 2006 fu et al 2012 during april november volumetric smc was measured four times per month in the rainy season july september and two times per month in the dry season during the months of april june and october november measurements were made at depth increments of 10 and 20 cm in 0 1 and 1 5 m soil layers respectively mean smc in the 0 5 m profile was determined using depth weighting a total of 800 1050 and 700 data points were collected in two years at the shenmu ansai and changwu hillslopes respectively at each sampling location 25 disturbed soil samples were collected at depth increments of 0 2 m during installation of the access tube and these were used for soil particle analysis soil samples were air dried and passed through a 2 mm sieve soil particle sizes were measured using a mastersizer 2000 malvern instruments malvern england undisturbed soil samples were also collected at a depth of 30 cm and 0 5 m away from the access tube using a cutting ring 5 cm diameter 100 cm3 volume to measure soil bulk density bd g cm 3 bd was measured using a gravimetric method astmc29 c29m 09 2003 leaf area index lai was measured with a plant canopy analyzer lai 2000 li cor usa and synchronized with smc determination elevation at each sampling location slope gradient sg and slope aspect sa were measured using a differential kinematic gps a laser slope meter and an inclinometer respectively long term daily meteorological data were collected from the nearest meteorological station and daily potential evapotranspiration was calculated using the penman equation shuttleworth 1993 mcvicar et al 2012 2 4 soil profile division the soil profile division in smc for three locations of shenmu ansai and changwu was determined using the mean of coefficient of variation cv at each smc measured point the mean of cv at each measured point was calculated as follows 1 cv cv 1 j cv 2 j cv 3 j 3 2 cv i j sd i j θ i j 3 sd i j 1 n 1 k 1 n θ i j k θ i j 2 where i is the location i 1 2 3 j is the measured point in soil profile j 1 2 30 k is each smc measurement k 1 2 n n is the total number of smc measurements sdi j and θ i j are the standard deviation and the mean of all smc measurements at the j th measured point for the location i during the observation period cm3 cm 3 respectively and θi j k is the individual smc measurement during the observation period cm3 cm 3 on the loess plateau the standards of soil profile division were defined by chen et al 2008a as follows cv 30 is a fast changing layer cv 20 30 is an active layer cv 10 20 is a sub active layer and cv 10 is a stable layer 2 5 potential soil moisture controls we selected 15 potential environmental factors that affected soil moisture and they included smc of neighboring layers climatic variables soil properties vegetation factors and topographic attributes table 3 with the exception of 7 day antecedent precipitation a7p 7 day antecedent potential evapotranspiration a7e and the smc of neighboring layers the remaining potential environmental factors have been investigated frequently in previous studies gómez plaza et al 2001 qiu et al 2001 brocca et al 2011 smc of neighboring layers was selected in this study because the dried soil layer which often appears in the soil profile of 100 cm 500 cm has become a serious environmental problem for sustainable vegetation management on the loess plateau wang et al 2011 this dried soil layer occurs when water consumption by vegetation is greater than water replenishment by precipitation chen et al 2008a b fu et al 2012 using the factor of smc of neighboring layers the effect of different vegetation conditions on smc at different soil layers could be simulated vegetation types plant biomass vegetative cover plant density lai and fine root distribution in the soil profile are very important variables that affect smc however lai is one of the most important vegetation factors and it can integrate the effects of other vegetation factors on smc such as vegetation biomass coverage and density zhao et al 2011 gómez plaza et al 2001 cantón et al 2004 cheng et al 2009 han et al 2007 duan et al 2016a b and zhang 2018 measured fine root distributions in soil profiles at shenmu ansai and changwu hillslopes respectively and they found that the maximum root depth varied 3 5 4 5 m for three vegetation types their results also indicated that approximately 86 of the fine root system was in the 0 100 cm layer the variable of fine root distribution is difficult to obtain at the regional scale whereas the spatial variability in this variable was a function of amsl bd and soil texture at the hillslope scale duan et al 2017 duan 2017 zhang 2018 because amsl bd and soil texture were environmental factors of smc lai was used only to represent the effect of vegetation factors on smc in this study climatic controls included mean annual precipitation map mean annual potential evapotranspiration mae a7p and a7e soil controls consisted of bd soil median grain size d 0 5 sand content sac silt content sic and clay content clc vegetation control included lai and topographical attributes included amsl sg and sa at the plot scale five potential factors were selected from all environmental factors table 3 these factors included upper layer smc usmc lower layer smc lsmc a7p a7e and lai at the hillslope scale in addition to the five environmental controls at the plot scale terrain attributes and soil properties affected spatial variation in soil moisture although sac sic clc d 0 5 bd and amsl potentially controlled soil moisture dynamics at the regional scale map mae sg and sa were considered because of their corresponding variation with increasing space scale then 15 environmental factors were analyzed to predict smc at the regional scale table 3 2 6 partial least squares regression plsr the plsr technique was used to determine the relationship between two sets of variables the matrix xm n that consisted of m variables columns and n objects rows and a response vector yn 1 plsr uses a few important linear combinations components or factors of the original x values that describe most of the inherent variable information in y the most important linear combinations are identified mathematically by maximizing the covariance between y and all possible linear functions of x shi et al 2013 hence plsr is an alternative to ordinary regression for problems with partly or highly co linear predictor variables wold et al 2001 carrascal et al 2009 and it has been used widely in the fields of hydrology environment and ecology ai et al 2015 huang et al 2016 plsr can be used to identify significant variables from environmental factors of smc that have remarkable correlation details on theory principles and applications of plsr can be found in carrascal et al 2009 and abdi 2010 to overcome the problem of over fitting and to achieve an optimal balance between the explained variation in responses r2 and the predictive ability of the model goodness of prediction q2 an appropriate number of significant components in the plsr model are determined by the cross validation abdi 2010 r2 and q2 are computed using the following equations 4 r 2 i 1 n y i y 2 i 1 n y i y 2 5 q 2 1 press ss a a 1 2 3 m where y i and y i are the predicted and measured values respectively y is the mean the press is the prediction error sum of squares the ss is the residual sum of squares and m is the number of plsr components when r2 and q2 are 0 5 and 0 0975 respectively then the model has significantly predictive ability trap et al 2013 two important parameters of variable importance in project vip and regression coefficients rc explain the relative influence of each independent variable the vip scores summarize the influence of individual x variable on the plsr model and provide a useful method to select variables that contribute the most to the y variance explanation the vip score for the j th variable is given as 6 vip j f 1 f w jf 2 ssy f j ssy total f where wjf is the weight value for the j variable and the f component ssyf is the sum of squares explained by the variance for the f th component and j number of x variables ssytotal is the total sum of squares explained by the dependent variable and f is the total number of components the equations for calculating ssyf and ssytotal are given by farrés et al 2015 the w2 jf gives the importance of the j th variable in each f th component and wjf is calculated using the plsr algorithm wold et al 2001 farrés et al 2015 in general an independent variable with a vip value 1 is the most relevant in explaining the dependent variables of smc whereas a value 0 5 indicates that the variable does not explain dependent variables significantly the interval between 0 5 and 1 is a gray zone where the significance level depends on the vip value trap et al 2013 jia et al 2009 the rc value represents the change in the response when the scaled variable varies from 0 to 1 in coded units while other variables are kept at their averages abdi 2010 the rc is significant above the noise when the confidence interval does not cross zero carrascal et al 2009 farrés et al 2015 the rc reveals the impact direction and strength of each variable in the plsr model all environmental characteristics were not necessarily included in this model because redundant variables can lead to plsr models with low statistical significance therefore plsr analysis was used to obtain an optimal model first simulation was conducted using the plsr model with all predictors next a series of simulations used new plsr models where each new plsr analysis eliminated a variable the new plsr model with the highest q2 was selected this procedure was repeated until two predictor variables remained finally the model with the highest q2 was selected as the optimal smc model at plot scale all measured smcs at four soil layers of 0 40 40 100 100 200 and 200 500 cm during the observation period were used to implement plsr analysis while all measured smcs at four soil layers and at all sampling locations along each transect were used for plsr analysis at hillslope scale at regional scale plsr analysis included all individual measurements at all three hillslopes 3 results 3 1 soil water dynamics and soil profile division fig 2 shows smc variations in the 500 cm soil profile at each plot during the growing season april october 2015 at the three plots smc at the changwu plot had the highest mean of 0 15 cm3 cm 3 smc at the ansai plot had a lower mean of 0 11 cm3 cm 3 and both were higher than that at the shenmu plot 0 08 cm3 cm 3 smc exhibited variation over the entire profile but the degree of variation declined with increasing soil depth in the surface soil layer smc showed more significant variation in a range of 0 1 0 3 cm3 cm 3 at ansai and changwu plots with increasing depth degree of changes in smc decreased and it stabilized below the depth of 200 cm therefore soil moisture variation showed a remarkable division in the profile according to the mean cv along the soil profile at three locations vertical changes in the soil profile at 0 500 cm were divided into four layers a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm with cv values of 32 4 49 4 19 8 28 4 10 3 17 8 and 3 4 9 5 respectively 3 2 soil water dynamics in different layers at three hillslopes fig 3 shows the time series of hillslope mean smc for four soil layers at the three hillslopes at the shenmu hillslope the soil layer at 100 200 cm depth had the highest smc and showed minimal fluctuation over time however the surface layer exhibited higher smc than the underlying layers at ansai and changwu hillslopes and displayed more significant irregularity with time as annual precipitation and clay soil texture increased table 2 sds on each measured date also exhibited some differences among the three hillslopes in general sd values at the shenmu hillslope were less than those at ansai and changwu hillslopes with ranges of 0 005 0 051 0 002 0 101 and 0 017 0 044 cm3 cm 3 respectively during the rainy season from july to september the temporal variation in smc in the 0 40 cm and 40 100 cm soil layers were large at all three hillslopes due to the effects of large rainfall events and evapotranspiration in contrast the temporal changes in smc at the deeper soil layers of 100 200 cm and 200 500 cm were relatively stable due to lower rates of both rainfall infiltration and root water uptake therefore large rainfall events and evapotranspiration strongly influenced soil moisture dynamics in the 0 40 cm and 40 100 cm soil layers 3 3 dominant controls of soil moisture dynamics at different scales instead of the climatic controls that we selected in previous studies a7p and a7e were used to replace the 3 day antecedent precipitation a3p and 3 day potential evapotranspiration a3e which proved to influence soil moisture dynamics significantly in humid regions huang et al 2016 in this study a7p and a7e had a more significant influence on soil moisture dynamics in semiarid and semi humid regions of the loess plateau p 0 05 in comparison with a3p and a3e after comparing pearson correlation coefficients between smc and its antecedent precipitation and potential evapotranspiration during the 1 30 days therefore we used 15 potential environmental factors to investigate dominant controls of smc at different scales table 3 3 3 1 plot scale table 4 summarizes individual plsr models for soil moisture at each measured layer of the three plots including r2 and q2 in optimal models for depths of 40 100 cm and 100 200 cm q2 and r2 reached at least 0 5 and most reached 0 8 table 4 in other layers q2 was a little lower than 0 5 except the depth of 0 40 cm at shenmu plot and the depth of 200 500 cm at changwu plot plsr models predicted smc accurately at the plot scale fig 4 illustrates vip values for smc at the plot scale with rcs plotted against predictors in separate models for 0 40 cm depths lsmc had the highest vip values followed by three other factors that exhibited minimal difference in models for other layers usmc and lsmc were also the highest but a7p was not included in plsr models at 40 100 cm and 200 500 cm and a7p had small values in the 100 200 cm model vip of a7e and lai showed minimal variation and some differences among the three locations rcs of usmc lsmc and a7p were correlated positively to smc at rc values 0 but a7e and lai were correlated negatively to smc at rc 0 smc measurements at the depth of 100 200 cm of ansai plot were used as an example to conduct the sensitivity analysis of optimal plsr model to rc values the plsr model indicated that usmc lsmc a7p a7e and lai were dominant factors of smc at the depth of 100 200 cm of ansai plot fig 4 and the optimal equation was as follows 7 smcs 100 200 0 364 x usmc 0 527 x lsmc 0 086 x a 7 p 0 206 x a 7 e 0 210 x lai q 2 0 782 r 2 0 855 rmse 0 385 where smcs is the scaled smc coded by the mean of smc and xusmc xlsmc xa7p xa7e and xlai are the scaled variables of usmc lsmc a7p a7e and lai coded by the mean of each variable respectively the performance of optimal plsr model toward the changes of each rc value whose r2 and rmse varied with the rc of each dominant factor being either increased or decreased 10 from the optimized one in eq 7 is shown in table 5 among five rc values the rc of variable lsmc was the most sensitive whereas the rc of variable lai appeared to be the least sensitive 3 3 2 hillslope scale table 6 summarizes individual plsr models constructed for soil moisture at each layer for the three hillslopes similar to the plot scale q2 and r2 values at depths of 40 100 cm and 100 200 cm were higher than those in other layers but the value was relatively low except for that of the changwu hillslope r2 and q2 were minimal at the shenmu hillslope at a depth of 200 500 cm followed by the 0 40 cm depth only two models yielded r2 0 5 fig 5 depicts vip values separately calculated for smc at the hillslope scale with rcs plotted against predictors lsmc and usmc appeared as the most important controls for smc at all measurement depths but other controls varied at different layers a7p and a7e were the second dominant controls at a depth of 0 40 cm with vip values 1 however the influence of a7p and a7e weakened with increasing depth other factors which included bd lai and amsl showed a little variability among the three hillslopes with good performance at one or two sites vip values for soil texture were the lowest among all factors and most of them were 0 5 for all layers rcs of usmc and lsmc were correlated positively with smc at values 0 5 values of a7p and a7e varied at different layers at the 0 40 cm layer smc increased with high a7p indicated by a positive rc but a7e indicated by a negative rc contributed to low smcs no clear connection existed between smc and a7p or a7p in the other layers where their values approached 0 few rc values of other factors such as bd at the shenmu hillslope at a depth of 40 100 cm and amsl at the changwu hillslope at a depth of 200 500 cm showed a significant correlation with smc 3 3 3 regional scale table 7 summarizes individual plsr models constructed for soil moisture at each measurement layer at the regional scale models with r2 and q2 0 75 at all layers displayed good performances and models for all layers identified four components that reached maximum q2 table 7 the first component explained 59 1 of the variance in changes in smc at a depth of 0 40 cm and approximately 70 variance at the three other layers the addition of second components explained approximately 10 of the variance the first four components explained 75 5 86 1 90 2 and 84 1 of total variance at measurement depths of 0 40 40 100 100 200 200 500 cm respectively further addition of components to plsr models did not substantially increase the percentage of variance that was explained as linear combinations of original variables that define scores plsr weights can describe the quantitative relationship between predictors and results abdi 2010 usmc and lsmc showed the highest vip values followed by map mae sac and sic fig 6 six factors and a7e at the 0 40 cm depth were the most important factors with values 1 rcs indicated that smc was lower for higher mae sac and lai values and higher for increased usmc lsmc map and sic values vip values of other predictors were 1 therefore these predictors barely influenced the smc prediction cumulative q2 values of plsr models were 0 75 which indicated their good predictive ability and robustness validation of the best plsr prediction model was illustrated using a scatter diagram that compared the measured and predicted smc values fig 7 at a depth of 0 40 cm points of smc were the most scattered as depth increased points of smc became more aggregated and appeared closer to one line as reflected by the low root mean square error rmse rmse for 100 200 and 200 500 cm at 0 014 and 0 015 cm3 cm 3 were considerably lower than that for 0 40 and 40 100 cm at 0 035 and 0 022 cm3 cm 3 respectively slopes of linear equations for all soil layers showed high values within a range of 0 756 0 904 these results further showed the accuracy of simulation by the plsr model 4 discussion 4 1 soil water dynamics and soil profile division according to profile variations in smc and its cv values vertical changes in the 0 500 cm soil profile were divided into four layers fig 2 this pattern was attributed primarily to seasonal distribution of annual precipitation gómez plaza et al 2001 in the loess plateau 55 78 of precipitation occurred during july september shi and shao 2000 yang and shao 2000 which led to variations in smc in our study areas similarly li 2001 also observed that soil moisture depth which was affected by rainfall infiltration usually occurred at 100 300 cm with an average of 200 cm depth of soil moisture 500 cm only appeared during the wettest years in the loess plateau where the frequency of such wet years was about 10 which means a recurrence of this event occurred only once every 10 years fu et al 2012 4 2 soil moisture dynamics of different layers at three hillslopes at the shenmu hillslope smc was low and showed no significant difference between surface and underlying layers these results were attributed mainly to less precipitation and high potential evapotranspiration and sandy loam soil in this area reduced the storage of moisture table 1 with increasing amounts of rainfall and decreasing potential evapotranspiration surface layers yielded higher smc than the underlying layers at the ansai and changwu hillslopes soil moisture dynamics also displayed a high and low change during the growing season may november at the three hillslopes especially for deep soil layers which were less influenced by antecedent precipitation and potential evapotranspiration at the beginning of the growing season may july soil moisture dynamics showed a decreased trend and then reached the minimum near july which resulted from high water consumption by growing vegetation the significant increase in lai from 0 22 to 2 89 from 0 35 to 3 5 and from 0 41 to 2 9 at the shenmu ansai and changwu hillslopes respectively supported this trend with the arrival of monsoon rains during july september the supply of soil moisture was higher than consumption and soil moisture dynamics increased from august to november which indicated that soil moisture dynamics depended directly on uptake of moisture by vegetation and interactions between rainfall and potential evapotranspiration on the loess plateau 4 3 dominant controls of soil smc at different scales 4 3 1 modeling results of smc at different scales at the plot scale smc changed with time and was related to five potential temporal factors the model results at depths of 40 100 cm and 100 200 cm were better than at depths of 0 40 cm and 200 500 cm at the hillslope scale smc exhibited spatial variation and factors soil texture and amsl that were related to smc spatial variation were considered in plsr models although the number of environmental factors increased modeling results were poorer than at the plot scale as indicated by lower r2 and q2 this condition possibly resulted from soil water redistribution along the hillslope and spatial variation in soil textures and water uptake by roots at the regional scale all spatiotemporal factors were included in the plsr model table 1 all r2 and q2 values were 0 75 in all layers which indicated good simulation results two important factors of map and mae were considered in the model at the regional scale with vips 1 fig 6 4 3 2 dominant controls of smc at different scales usmc and lsmc had the highest vip values and they were the most important factors across the three scales which indicated a close relationship of soil moisture between neighboring soil layers figs 4 6 in contrast to the stability of smc between neighboring layers a7p and a7e showed significant differences among various soil layers at the three scales a7p and a7e influenced smc significantly with most vip values 1 at depths of 0 40 cm but they exhibited little influence in the other layers figs 4 6 hence most rainfall penetrated the soil profile at 0 40 cm and a7e exhibited a significant influence on smc in that layer these results were attributed mainly to supply and consumption of soil moisture rainfall events infiltration processes soil evaporation vegetation transpiration and lateral flows contributed to soil moisture in a highly variable pattern zhao et al 2011 zhu et al 2014 however the climate in our study area is semiarid and sub humid with annual precipitation of 437 582 mm and potential evapotranspiration of 1233 1398 mm precipitation and potential evapotranspiration showed limited influence in deep soils thus smc in the top layer only had the closest relationship with a7p and a7e though most factors that we examined were stable at all three scales the vip of soil texture differed remarkably between the hillslope and regional scales especially for sac and sic which were the most important factors with values 1 fig 6 zhu and lin 2011 obtained similar values and observed that soil properties became less significant to smc in the hillslope transects that had comparatively homogenous texture the main reason was that topographic attributes dominated over soil properties in controlling smc variation however soil properties had an obvious influence on smc when soil properties exhibited significant variation at the regional scale soil properties mainly included texture organic matter content porosity structure and macroporosity the impact of soil properties on spatial variation in soil moisture has been well documented with large scale data sets that were acquired from soil moisture field campaigns crow et al 2012 this study detected the dominant controls of smc in diverse soil layers at different spatial scales and identified their relative significance to impact soil moisture dynamics these results are useful to develop eco hydrological models and optimize soil water and vegetation management over semiarid and semi humid areas of the loess plateau china this region is well known for its serious soil erosion largely caused by cultivation of marginal lands and destruction of natural vegetation huang and gallichand 2006 chen et al 2008a b wang et al 2011 in order to control serious soil erosion a great effort has been made to plant trees and grasses on slope land since the end of 1950 s indeed annual sediment loss has reduced from 1760 mt in 1950 s to 360 mt in 2000 s and vegetation coverage has increased from 6 5 in 1970 s to 38 in 2006 guo et al 2014 however large scale vegetation restoration also aggravated water scarcity and gradually led to the formation of a dry soil layer in the soil profile of 100 600 cm in which smc was less than stable field water capacity chen et al 2008a b wang et al 2011 the dry soil layer prevented water interchanges between upper soil layer and groundwater conversely reduced the anti drought capability of deep rooted plants and heavily influenced the growth and natural succession of vegetation wang et al 2011 studied the thickness forming depth and smc of dry soil layer in the loess plateau their results indicated that the thickness of dry soil layer varied from 126 cm to 500 cm with an average of 289 cm the forming depth from 100 cm to 198 cm with an average of 139 cm and the smc from 1 7 to 11 7 with an average of 7 5 the distribution pattern of dry soil layer was mainly related to climate region soil properties vegetation type and growth age shangguan 2007 wang et al 2011 chen et al 2008b different from previous investigations this study used plsr to detect dominant controls of smc among 15 selected environmental factors the results indicated that dominant controls of smc varied in diverse soil layers at different spatial scales understanding the dominant factors of smc at different scales enables scientifically based policies to be made that would mediate the formation of dry soil layer and sustain development of the economy and restoration of the natural environment in the semiarid and semi humid regions of the loess plateau and in similar regions elsewhere 5 conclusion this study identified the main controlling factors of smc in various soil layers at different scales on the loess plateau using plsr models and the conclusions of this study are as follows 1 temporal variation in soil water decreased with increasing soil depth according to sd and cv of smc and vertical changes in the 0 500 cm soil profile were divided into a fast changing layer 0 40 cm an active layer 40 100 cm a sub active layer 100 200 cm and a relatively stable layer 200 500 cm 2 plsr models simulated smc accurately in various soil layers at different scales with almost all r2 and q2 values 0 5 and 0 0975 respectively 3 usmc and lsmc which had the highest vip values were the most important factors in various soil layers at three scales a7p and a7e contributed significantly to smc at a depth of 0 40 cm but they showed little relationship with the other layers vip of soil properties showed a notable increase as the scale of measurement increased specifically sac and sic showed strong effects on smc and map and mae also had a significant influence on smc at the regional scale acknowledgements financial support for this work was provided by nsfc rcuk nerc no 41571130082 and the national natural science foundation of china nos 41390463 and 41571213 the authors thank the members of the changwu ansai and shenmu ecology stations chinese academy of sciences and ministry of water resources for their assistance special thanks are given to the editor in chief prof mcvicar and two anonymous reviewers for their comments and suggestions 
7248,this paper examines the frequency distribution tails and peak over threshold pot of extreme floods through analysis that centers on the october 2015 flooding in north carolina nc and south carolina sc united states us the most striking features of the october 2015 flooding were a short time to peak tp and a multi hour continuous flood peak which caused intensive and widespread damages to human lives properties and infrastructure the 2015 flooding was produced by a sequence of intense rainfall events which originated from category 4 hurricane joaquin over a period of four days here the probability distribution and distribution parameters i e location scale and shape of floods were investigated by comparing the upper part of empirical distributions of the annual maximum flood amf and pot with light to heavy theoretical tails fréchet pareto gumbel weibull beta and exponential specifically four sets of u s geological survey usgs gauging data from the central carolinas with record lengths from approximately 65 125 years were used analysis suggests that heavier tailed distributions are in better agreement with the pot and somewhat amf data than more often used exponential light tailed probability distributions further the threshold selection and record length affect the heaviness of the tail and fluctuations of the parent distributions the shape parameter and its evolution in the period of record play a critical and poorly understood role in determining the scaling of flood response to intense rainfall keywords flood frequency analysis return period distribution tails the carolinas 1 introduction the october 3 5 2015 historic rains caused by hurricane joaquin released more than 500 mm of rain in south carolina sc and north carolina nc united states us the flood peak of many u s geological survey usgs gauges including those located in the center of sc were almost twice the previous maximum from a record of over 65 years the spatial extent of flooding in this portion was also unprecedented with more record flood peaks at usgs stream gauging stations in urban areas such as columbia the capital of sc than for any other rural catchments such an extraordinary flood lies within the fundamental issue of infrastructure safety and raises the crucial question of how to proceed if this event is not visible for a given dataset and if it is too rare for design applications although recently significant progress has been made to predict short term flood for operational purposes e g pourreza bilondi et al 2017 long term prediction on which infrastructure design is based is difficult in deterministic terms e g papalexiou and koutsoyiannis 2013 thus it is common to treat this event in a probabilistic manner i e as a random variable that is governed by a distribution law such a distribution enables the modeler to capture the probability of exceedance and assign a return period to any flood event the procedure called flood frequency analysis ffa in design hydrology assessment of flood probability has been an active research topic yet a less understood concept however the analysis is well rooted in an extensive literature dating back to the work of nicolaus bernoulli three centuries ago mentioned in gumbel 1958 extreme value theory evt was the first and widely accepted method for ffa that has rapidly evolved and found applications in engineering hydrology fuller s 1914 study was probably the first application of extreme value distributions some recent studies such as papalexiou and koutsoyiannis 2013 and serinaldi and kilsby 2014 2015 expanded the evt concepts for hydrological design applications specifically evt has stimulated an extensive investigation to estimate the parent distribution e g michele and rosso 2001 bernardara et al 2008 and upper tail behaviors of flood properties papalexiou and koutsoyiannis 2013 serinaldi and kilsby 2015 just to mention a few recent studies focusing on evt and referring to renard et al 2013 and martinkova 2013 for a recent review of the evt applications in hydrology this theory captures the asymptotic distributional behavior of two types of data namely the so called block maxima bm and peak over threshold pot bm extracts the maximum values from subsets i e blocks of observations whilst pot performs observations exceeding a certain threshold when the size of the blocks approaches infinity the distribution of bm converges to three types of extreme value distribution families gumbel fréchet and reverse weibull fisher and tippett 1928 gnedenko 1943 where the parameters scale with the information dimension these three types of extreme families can be described by the so called generalized extreme value gev distribution with the location scale and shape parameters e g coles 2001 as defined by the unified von mises jenkinson parameterization jenkinson 1955 if the threshold of exceedance increases the gev then converges to the so called generalized pareto gp distribution as described by the pickands balkema de haan theorem pickands iii 1975 balkema and de haan 1974 in many cases gp yields a more accurate approximation to the distribution of absolute and relative excesses as well as distribution tails in addition it represents distribution tails obliquely but rigorously by letting the data decide the function in practice a way to verify the validity of gp is to check whether the estimates of the shape parameter are stable when the model is fitted to excesses over a range of thresholds from a theoretical point of view absence of the stability can be explained by a slow rate of convergence in the pickands balkema de haan theorem the fitted model can then be used to compute any tail related risk measure such as tail probabilities tail quantiles or value at risk etc there is an established link between gp and gev in the evt modeling in practice if block maxima follow a gev distribution then the threshold excesses have a corresponding approximate distribution within the gp family e g coles 2001 and vice versa gev parameterization can be estimated using gp such as poisson distribution for the occurrence frequency of the pot e g goda 2011 recently the probabilistic fitting of these extreme distributions to hydrological variables signifies major progress in design hydrology as it quantifies risk and disputes arbitrary notions e g koutsoyiannis 2004 although in spite of the extensive literature on eva model fitting and goodness of fit testing only few studies have recently tackled the practical problems of flood frequency analysis facing real time application and uncertainty e g vogel et al 2011 stedinger and griffis 2011 rootzén and katz 2013 papalexiou and koutsoyiannis 2013 obeysekera and salas 2014 serinaldi and kilsby 2015 mondal and mujumdar 2015 the application of extreme theory on various real world applications is essential for risk assessment and water resources planning which demand long time horizons with no other rational scientific basis than probability therefore the aim of this paper is to compute ffa and return periods for annual and instantaneous floods in the center of the carolinas with special attention to the pot approach and to place the october 2015 flood in a flood frequency analysis context an important class of questions addressed in this study concerns the impact of peak rates and thresholds on the upper tails of flood distributions the goal was to investigate the distribution fitting model and the upper tail distribution of maxima and to provide a better answer to the question of how extreme was the october 2015 flood in the carolinas to address aforementioned question four different applications across the carolinas were used to infer various procedures and to relate these analyses to properties of the october 2015 flooding spatial and temporal variability of flood events and the uncertainty associated with flood properties were also addressed during the period of analysis the underlying parent distributions were also re assessed with the inclusion of the 2015 flood event in order to characterize distributional changes associated with the fitting parameters this study quantified the sampling uncertainty via confidence intervals cis in the evt framework to highlight its fundamental role for a fair comparison between models and a fair assessment of the output reliability this paper is organized as follows in section 2 the study region and flood data used in this study are explained the theoretical concept and mathematical structures of probability distributions distribution parameters and pot are explained in section 3 these methodologies were then examined in four different case studies explained in section 4 each application example has its own merit emphasizing particular aspects of extreme analysis conclusions are provided in section 5 as critical guidelines to address the limitation and criticism in judging the magnitude and frequency of floods 2 study area and data the study area is located in the eastern us covering portions of the mountain and piedmont regions of nc and sc the area is comprised of the wateree the upper and the lower broad the upper and the lower catawba the south fork catawba the congaree the tyger the saluda and the enoree sub basins two metropolitan cities charlotte and columbia are located in the study region see fig 1 flood data were analyzed to determine appropriate usgs gauges that exhibited a long term period of record as well as a consistent hourly sub hourly record of the october 2015 flood event due to limited data availability and large missing values this approach limited the analysis to four usgs monitoring gauges the flood data of usgs 02147500 rocky creek at great falls sc usgs 02160700 enoree river at whitmire sc usgs 02167000 saluda river at chappells sc and usgs 02169500 congaree river at columbia sc were used fig 1 historical instantaneous floods data for each of the usgs gauging stations were also collected from october 2 2015 to october 10 2015 as well as the period of record data for observed instantaneous flows and peak annual maximum flows although the rainfall event produced from the hurricane event lasted approximately 3 days the flood hydrograph spanned for several days as the drainage system responded to intense rainfall for a longer period further the october 2015 intense rainfall produced different runoff volumes although the time to peak and flood duration are approximately the same among the four hydrographs fig 2 it should be noted that all four hydrographs presented in fig 2 have a short time to peak tp and a continuous peak rate which caused intensive and widespread damages to human lives properties and infrastructure here flood data analysis placed emphasis on the concept of water year which is often designated as the hydrological year beginning on october 1st and ending on september 30th accordingly two alternative approaches of data are selected i annual maximum flood amf series spanning from 1892 to 2015 with no missing values and ii daily maximum flood dmf or pot series spanning from october 01 1984 to september 30 2015 with 3 of missing values 3 methodology the data sets described in the previous section were employed to fit different distribution functions this study tested i the significance of lag 1 correlation for two subsequent values by the kendall correlation coefficient k acf ii possible monotonic trends by the mann kendall m k test and iii distributional hypotheses by goodness of fit and ad hoc diagnostics specifically the suitability of a probability distribution is assessed by four goodness of fit tests namely kolmogorov smirnov k s cramer von mises c vm anderson darling a d and the pearson product moment correlation coefficient ppmcc kottegoda and rosso 2008 k acf and m k tests are necessary to compute the temporal dependence and monotonic trends in data these trends can also affect the outcome of the goodness of fit tests which rely on the hypothesis of independent observations serinaldi and kilsby 2014 in addition to a hypothesis based goodness of fit information based criteria also known as bayesian statistics for probability model selection laio et al 2009 was used which has shown to aid modeler in identifying the best probability distribution for hypothesis testing e g di baldassarre et al 2009 chen et al 2017 more specifically the akaike information criterion aic and bayes information criterion bic were adopted as the information based criterions bic and aic reward a model for a higher likelihood and penalize a model for overfitting lower aic and bic indicate a better fit these techniques were implemented using continuous to discrete distributions explained in the next section 3 1 theoretical concept of probability distribution continuous to more skewed distributions were employed to capture flood characteristics over time gumbel is tested as a continuous probability distribution whereas extreme value theory was used to implement discrete probability distributions continuous distributions have been widely applied in hydrology so we refer the readers to vogel and wilson 1996 koutsoyiannis 2005 mcmahon et al 2007 and el adlouni et al 2008 among others in the classical extreme value theory if a random variable rv x follows the distribution fx x the distribution function of the maximum of n is independent and identically distributed iid that can be described by 1 g y n f x x n if n the distribution of n can converge to three limiting laws of the gumbel the fréchet or the reversed weibull which can be described by the unified von mises jenkinson parameterization jenkinson 1955 of the so called gev distribution coles 2001 2 g z exp 1 ξ z μ σ 1 ξ where z 1 ξ z µ σ 0 µ is the location parameter σ 0 is the scale parameter and µ is the shape parameter depending on the sign of the shape parameter gev can envelop three types of distribution functions i ξ 0 the heavy tailed fréchet ii ξ 0 the upper bounded weibull and iii ξ 0 the gumbel distribution function based on the pickands balkema de haan theorem pickands iii 1975 balkema and de haan 1974 a gev distribution coverages to a gp distribution if the extreme threshold increases over time coles 2001 which is given as 3 h y 1 1 ξ y σ 1 ξ where y y 0 and 1 ξy σ 0 and σ σ ξ u µ ξ determines three types of distribution functions with the same interpretation as for gev heavy tail when ξ 0 i e pareto upper bound when ξ 0 i e beta and exponential in the limit as ξ among three parameters in the eva theory ξ plays a key role in hydrological frequency analysis since it determines the upper tail behavior of floods that is important for the design of hydraulic structures the point process pp model is applied to the distribution of discharge excess following that of the gev parameterization as outlined by coles 2001 and approximated by the gp distribution pp treats the data points as the occurrence of extreme events and marks their associated size briefly if the parent distribution does not evolve over time i e stationary and satisfies an asymptotic lack of clustering condition for values that exceed a high threshold then the distribution s limiting form is non homogeneous poisson it is noted that gev pp gp and gumbel distributions were fitted to the amf data while only gp and pp models were tested for the pot dataset application of the gp and pp distributions to amf data is not typically applied in engineering practice or research but have found merit in previous studies mohssen 2009 papalexiou et al 2013 as a result this study incorporated the distributions as an exploratory case to better understand if the october 2015 flood shifted the tail behavior of maxima towards a heavier tail distribution in the case where the limiting form is an excess distribution such as the gp or pp comparative annual design quantiles were estimated by considering the threshold exceedance rate and number of raw observations per year coles 2001 and therefore can be practically applied in the infrastructure design setting the supporting parameterization of the cumulative distribution and their inherent parameters implemented within the study are provided in table 1 a fair comparison between various fitting models requires the assessment of the uncertainty of probabilities return periods and design quantiles indeed when evaluating design properties the differences of modeling estimates should be significant for operational use in this respect there are several methodologies that have recently been applied to various extreme fitting models including the delta method obeysekera and salas 2014 the bootstrap resampling method efron and tibshirani 1993 samadi et al 2013 and the profile likelihood function method obeysekera and salas 2014 the delta method relies on the asymptotic properties of the maximum likelihood estimates of the model parameters and their covariance matrix e g serinaldi and kilsby 2014 obeysekera and salas 2014 it calculates symmetric cis under the hypothesis that the distribution of the quantiles is reasonably described by a gaussian distribution the bootstrap method resamples the observed series using nonparametric bootstrapping or parametric bootstrapping also so called monte carlo simulation to simulate iid realizations of a random variable y and to draw a suitable standardized distribution the bootstrap method provides an assessment of the sampling and parameter estimation uncertainties by realistic asymmetric cis serinaldi and kilsby 2014 unlike the bootstrap method the profile likelihood function relies on the asymptotic properties of maximum likelihood estimators although this method is difficult to implement and can be quite computationally burdensome e g obeysekera and salas 2014 in this study the bootstrap method is used to calculate the uncertainty of cis because it is independent of the probability estimation method and provides more reliable simulation of iid realization 3 2 parameter estimation the method of moments and method of l moments have been traditionally employed to estimate distribution parameters in hydrological datasets hosking et al 1985 maidment 1993 gubareva and gartsman 2010 in recent years due to advancements in computing the method of maximum likelihood estimation mle and generalized mle gmle martins and stedinger 2000 2001 have a growing interest these methods provide unbiased parameter estimates compared to the method of moments thus the mle method was adopted for the estimation of distribution parameters for the results presented herein for the case where the random variable x has probability density function f x θ0 the likelihood function is given as 4 l θ i 1 n f x i θ eq 4 represents the likelihood function of a set of independent realization of the random variable x given a set of parameters θ which describes the underlying nature of the probability density function coles 2001 usually the optimization of the parameter set is addressed in terms of the log likelihood function given by 5 l θ log l θ i 1 n log f x i θ during the maximum likelihood approach eq 5 is implemented using optimization procedures such that the log likelihood function is maximized resulting in the maximum likelihood estimates of the distribution parameter set θ because of the monotonic behavior of the log likelihood function the likelihood and log likelihood functions take on the maximum given the same set of likelihood estimators coles 2001 the model likelihood functions implemented within this study are outlined in table 2 3 3 threshold selection a critical component in the implementation of the gp and pp excess distribution models in hydrological frequency analysis is the selection of a threshold value villarini et al 2011 saeed far and wahab 2016 in this study mean residual life plots and threshold range plots coles 2001 were applied for the evaluation of threshold values each of these exploratory techniques aim to guide in the selection of a minimum threshold such that there is linearity below the threshold in the mean residual life plot and there is stability beyond the threshold in the distribution parameters coles 2001 saeed far and wahab 2016 further evaluation of threshold selection for the pot and its influence on the performance of the gp and pp models was analyzed by taking into consideration the significance level for the goodness of fit tests outlined earlier in this study threshold values were selected such that appropriate significance levels were achieved with the lowest possible threshold as to limit the resulting variance in distribution parameter estimation while achieving agreement among the three goodness of fit two sided p values each of the above techniques was employed along with goodness of fit tests kolmogorov smirnov k s cramer von mises c vm and anderson darling a d to derive a threshold value for each usgs gauging station it is important to note that in the case of block maxima data the threshold value was set slightly lower than the absolute minimum of the amf data set such that all events are considered this is done because by nature that is the natural threshold of the amf time series as a result the gp and pp distributions consider the amf data in the same facet as the gev and gumbel distributions thereby providing consistent sample sizes for which the distribution parameters can be estimated and prevent additional bias 4 applications 4 1 analysis of rocky creek rocky creek drains approximately 502 5 square kilometers sqkm just north of the catawba river into lake wateree sc the drainage area above the usgs monitoring station can be classified as moderately rural composed of smaller communities and residential areas in this drainage system the peak flow rate during the october 2015 flood event was approximately 59 cubic meters per second cms with a sustained peak rate for approximately 8 h analysis begins with the diagnostic plots fig 3 a presents the results for the fitted cumulative distribution function p p probabilities plot and q q quantile quantile plots for the gev gp gumbel and pp models using historical amf records without the inclusion of the 2015 flood event i e 1952 2015 for clarity these data will be referred to as amf herein while historical records with the 2015 event will be referred to as amf 2015 the diagnostic plots reveal that both the gev and the gumbel models more precisely predicted the empirical distribution and probabilities when compared to the gp and the pp models both the gev and gumbel distributions appeared to have comparable quantile accuracy below 250 cms with the gev model showing more variability in its upper tail bound visual fitting results of the gp and pp models indicate that each model under over predicted the observed probability p p and increasingly deviated from the observed quantiles with increasing probabilities a more detailed review of the fitting performance data provides a summary of the four models for the amf data table 3 presents two sided p values for the k s c vm and a d tests the pearson correlation coefficient ρ p p coefficient of determination r2 aic and bic the upper tail performance of the gev model as indicated by the a d test proves a slightly higher significance compared to the gumbel model while the overall fitting performance indicated by the k s test is much lower than that of the gumbel the cumulative consideration of the goodness of fit test results and the information based criteria indicated that the gumbel distribution provided the best fit for the rocky creek amf data followed by the gev gp and pp models respectively it is important to note that the selection of the most appropriate parent distribution within this study was based on the distribution which produced the highest and most consistent significance level i e two sided p value for the k s c vm and a d tests while also generating the lowest information based criteria set forth by the aic and bic statistics next the analysis proceeded using amf 2015 data results of the amf 2015 analysis are based on the hypothesis that the parent distribution of amf i e gumbel would not hold true for amf 2015 inclusion of the 2015 flood event within the amf dataset i e amf 2015 served as an investigatory analysis into the underlying evaluation of the potential abnormality of the 2015 hurricane event and its impact on the underlying parent distribution of amf referring to table 3 the performance of the probability models was not generally affected when including the october 2015 flood event however the fitted models of the evt family showed slightly asymmetric behavior but they were not prominent as the upper tails of gp gev and pp models are approximately exponential i e their shape parameters are close to zero in this respect the gumbel model deemed more accurate than those obtained by power law as expected the only model which experienced a significant impact on performance was the gev k s 0 05 the fitted distribution parameters presented in table 4 highlight the superiority of the gumbel model for the amf 2015 series when compared to the rest of the models the gumbel distribution computed the second lowest scale parameter thus constituting a much lower variance in the predicted runoff level for any given design period this result indicates that the hurricane event more so follows the underlying distribution of the annual flood data rejecting the hypothesis of power law distribution for the rocky creek amf 2015 dataset as the second part of data analysis for rocky creek a continuous time series i e sub hourly of dmf was extracted for a period from 10 01 1986 09 30 2015 gp and pp are the two most appropriate distributions to describe the exceedance over threshold thus they were implemented for pot probability analysis diagnostic plots of the resulting pot frequency analysis without the inclusion of the 2015 flood i e pot are presented in fig 3b examination of the cdf and p p plots suggest a decreased performance in the parent distribution i e gumbel due to the variation between the model and observed data an assessment of the gp and pp diagnostic plots show arguably exceptional fits with a very fine degree of variation between the model and observed data with little to no visual deviance between the two model results table 5 overall the gp and pp models provided between 0 85 and 0 93 significance levels with a well computed upper tail bound i e a d 0 93 the gp and pp models generated nearly identical results given the goodness of fit tests and coefficient of determination but the pp model dominated after review of the bayesian information criteria inclusion of the 2015 flood event i e pot 2015 appeared to have a marginally significant effect on the central distribution behavior as indicated by the k s test however the upper tail behavior outlined by the a d significance levels showed negligible impact the resulting distribution parameters for the pot analysis are presented in table 6 the pp model distribution parameters for the pot with the 2015 event clearly show an increase for example the variation in predicted runoff as indicated by the scale parameter increased from 43 4 cms to 45 9 cms while the rate at which runoff generation grows with more intense events set forth by the shape parameter increased from 0 017 to 0 031 although the increase in the parameters is small the increase in the positive shape parameter indicates the possibility of the return level growth and non linear increase over larger return periods thus the inclusion of additional data within the pot time series appears to have a considerable effect to a degree of the ultimate distribution shape and hence the underlying vulnerability of current drainage infrastructure addressing the implications of the flood event through the amf and pot analysis was complimented by return level predictions generated from the amf and pot parent distribution parameters these estimates with the bootstrapped 95 cis are presented in fig 4 the results indicate that the bm s parent distribution replicated the theoretical return level function quite well see raw data points the amf computed return levels from the gumbel distribution had little to no dispersion below the 25 year return period unlike the amf the pot return model had much more variation between the modeled and empirical results since the annual time series was best represented with a gumbel model there is a strong linearity in return level estimation with less uncertainty above the 50 year level when compared to the pot model i e pp distribution the width of the cis confirms that a large uncertainty characterizes the results obtained by the pot analysis for instance the amf 100 year return level ranged between approximately 420 to 650 cms whereas a significant width of the ci for lower i e 10 year and upper i e 100 year return levels reflect that the model is less informative this further implies that the increase of extreme data is paid in terms of the increase of uncertainty and extreme complexity in both cases the shape and underlying distribution were different however a peak flow rate of 59 cms only registered as a 1 1 year and 1 0 year return period for the amf and pot model respectively thus confirming that the 2015 hurricane event cannot be classified as an extreme event from a truly probabilistic point of view 4 2 analysis of the enoree river the enoree river lies in the heart of the sumter national forest draining more than 1150 sqkm of rural and urban land and eventually discharging into the broad river available historical datasets for this basin provided amf data from 1974 to 2015 in addition to continuous i e sub hourly flow data from 04 05 1985 09 30 2015 fig 5 a presents the diagnostic plot results of the amf analysis the results indicate that the gev and gumbel models appeared to be good indicators for predicting the annual maxima with little deviation between model and empirical estimates the cdf plots of these models show similar characteristics in the lower tail behavior of the distribution but a review of the probability plots reveals that the gev had much less variation between the model and the observed data than that of the gumbel the quantile plots confirm this result and show that the gev model had more accuracy in predicting the upper tail behavior of the amf distribution when compared to the gumbel a significant increase in the shape parameter of amf 2015 denotes a tendency to heavier tailed fréchet law unlike the gev model both the gp and pp resulted in slight deviations in the model fitting the amf results table 7 showed a superior performance of the gev model for all goodness of fit results 0 87 0 90 however each of the models proved to have similar accuracy in predicting the probability of exceedance with the gev being however the most realistic among the competitors as illustrated by the diagnostic plots see fig 5a the gumbel model represented the distribution of data well but falls short of the gev with significance levels approximately 0 22 to 0 42 lower than the gev the central tendency of the gp and pp models was slightly lower than that of the gumbel nonetheless the tail performance was at most satisfactory 0 16 the consideration of each of the performance standards led to the adoption of the gev model as a proxy for the amf data however interestingly the gev gp and pp models had similar bayesian criterion with the latter being the most realistic model including the 2015 flood event within the amf dataset i e amf 2015 predominantly affected the gev and the gumbel models although around the 5 significance level in both cases the performance of the goodness of fit tests improved with the most improvement observed in the tail behavior of the distributions for example prior to the 2015 hurricane the a d statistic for the gev model was 0 90 while following the event this statistic increased to approximately 0 95 improved performance was also noted in the gp and pp models although affected less than the 2 ci as a result the gev model maintained the stance of representing the amf distribution however as cooley 2013 stated any method for generating cis has drawbacks therefore caution should be exercised when interpreting such results a review of the fitted distribution parameters is warranted to further summarize the response of the probability distribution to the 2015 flood event the results presented in table 8 do not suggest a larger rate of variability of the runoff prediction for the gev gp and pp models as indicated by the shape parameter however the gp and pp models did tend to show much more fluctuation in the model when comparing the scale parameter during pre and post hurricane event for instance the gev model showed an increase in the scale parameter of approximately 1 6 cms while the gp and pp models increased by approximately 5 cms in this basin incorporation of amf excess models i e gp and pp presented more uncertainty in design variables as opposed to the selected gev parent distribution the distribution fitting performance increased with the addition of the 2015 hurricane event however many of the estimated distribution parameters experienced a slight increase for example the gev location and scale parameters increased by approximately 2 cms and 1 6 cms respectively while the shape parameter remained constant in this regard the central tendency and the anticipated variation of the discharge increase such an increase is considered minor and practically immeasurable in engineering practice most importantly the upper tail behavior i e shape parameter of the distribution remained unchanged by the hurricane event as a result the parent distribution did not show significant deviations during the post event analysis indicating no abnormalities or outlier prone evidence of the 2015 hurricane event in this drainage system it is however important to note that the lack of abnormalities indicated by the distribution performance and parameters does not necessarily negate the potential extreme intensity of the event further evaluation of the extremeness of the 2015 flood event was continued for the enoree river with the development of probability models using the partial duration series prior to the 2015 flood event visual inspection of the diagnostic plots for these models fig 5b indicated exceptional fits for both the gp and pp in terms of the predicted distribution function i e cdf the results of goodness of fit tests revealed that the gp model simply provides a proxy for the probabilities of exceedance qi with the pp model trailing however bayesian statistics highlight the superiority of the pp model see table 9 evaluation proceeded by analysis of the pot 2015 series the results presented in table 9 prove that the inclusion of the 2015 flood event significantly affected the distribution performance both the gp and pp models show improvement in model adequacy in the case of the pot 2015 analysis it appears the hurricane event significantly affected the parent distribution in terms of the goodness of fit tests however both the gp and pp models showed little to no degree of variability in the distribution parameters see table 10 the most adequate fit the pp model showed the highest increase in the estimated scale and shape parameters when compared to the gp distribution fig 6 shows the 95 cis obtained by the gev and pp models for the amf and pot data sets respectively in both cases the predicted return levels for lower return periods i e 25 year event showed close to each other though a high variability existed at higher frequencies between two formulations further both series cis were slightly asymmetric and deemed more accurate based on performance criterion however such asymmetry was more prominent for the pot model as the upper tail of the pp distribution was not exponential i e the gev shape parameter is not close to zero the fitted model for pot indeed showed more nonlinear temporal patterns compare to the amf as illustrated the width of the cis of higher frequencies return periods confirms that a significant uncertainty characterizes the results obtained by the pp distribution this further demonstrates that the increase in model complexity i e moving from gev to pp distribution is paid in terms of increasing uncertainty and more complex probability models cannot replace information if this is not available estimated return periods for the enoree river discharge i e 303 cms during the 2015 event were 8 8 years and 5 8 years for the amf and pot datasets respectively the modeled return periods show larger intensities compared to the previous application but still much lower than typical bridge and road design settings in the study region i e 25 year return period although the pot distribution fit and parameter values did experience more significant variations when compared to the amf analysis these results corroborate and substantiate the previous ffa results and provide reason to disregard the runoff event as a truly extreme sample from the underlying distribution of flood events 4 3 analysis of the saluda river the saluda river is situated below lake greenwood draining a diverse set of landscapes and river systems totaling more than 3522 sqkm and eventually discharging to lake murray outside of lexington sc due to the size and slope of the saluda river basin a maximum peak flow of nearly 1000 cms was recorded to have caused considerable damages to water control structures e g dams and ponds during hurricane joaquin the usgs station at the saluda river monitors flow since 1906 i e 108 year data set for bm however a continuous sub daily maximum flood data set is not readily available until 07 31 1986 diagnostic plots for the amf series are shown in fig 7 a as illustrated the gp and pp modeled adequately in terms of predicting most of the observed flood data when compared to the gev and the gumbel models inspection of the p p plots show each model had a degree of dispersion with the gp and pp being less biased a review of the q q plots reveals the effectiveness of the gp and pp models in all cases apart from the single most extreme flood event a much different result from the previous applications of the rocky creek and the enoree river basins for example the gp and pp models showed an excellent fit to the observed discharge levels below approximately 1700 cms although in some intervals the models over predicted large flood events the gumbel distribution visually appeared to offer the most concise quantile estimates i e less dispersion when compared to empirical data inspection of the performance indicators table 11 of the four probability models show that the gp model outperformed the pp model followed by the gev and gumbel respectively unlike the two previous applications the gp and pp statistical significance of the fits as indicated by the k s c vm and a d two sided p values were substantially higher i e 0 70 0 93 significance levels statistically the gev model did provide evidence that the flood data could come from the distribution with confidence levels of approximately 0 24 0 30 however the selected parent distribution of the gp had more than twice the confidence level similarly the gumbel did show minor confidence in the distribution of the flood data but with satisfactory results in the tail estimate around 0 11 extension of the annual time series to include the 2015 flood event i e amf 2015 is summarized in table 11 the performance summary results did not change significantly for the gev and gumbel models however the gp and pp models experienced a slight improvement in the overall fitting the fitted distribution parameters table 12 substantiate this hypothesis although there was an increased degree of variability introduced into each model for example the gp model most appropriately represented the behavior of annual extreme events for the saluda river basin inclusion of the 2015 flood event within the amf series increased the scale parameter by nearly 11 cms in this case a larger degree of uncertainty in the estimated runoff is present when compared to the previous applications especially at larger runoff rates in addition the block maxima approach yielded much more appropriate performance for the gp and pp models in comparison to the gev and gumbel models however the small size of the samples usually 50 amf tends to hide the heavy tail behavior as seen elsewhere e g serinaldi and kilsby 2014 2015 evaluation of the frequency analysis using partial duration series led to a more abundant supply of extreme data points although over only a portion of the annual period of record hence the threshold models i e gp and pp provided superiority over the rest of the models fig 7b presents the diagnostic plots for the extreme value distributions considered for the saluda river pot time series like the block maxima dataset the gp and pp models characterized the extreme flood events well below the 90 exceedance level while overpredicting events above this threshold further investigation of the gp and pp probability plots revealed a less degree of dispersion when compared to the amf results fig 7a moreover the pot frequency analysis slightly affected the central tendency of the probability models however the upper tail performance was significantly affected by this frequency approach table 13 for example the amf analysis resulted in approximately 70 cis for the upper tail behavior of the gp model i e a d while pot yielded a 95 significance level for the adequacy of the gp model based on the goodness of fit test considering different goodness of fit statistics the pp model provided the best overall fit although the effect on the upper tail behavior was negligible the results presented in table 13 show that 2015 flood event appeared to have a marginally significant effect i e 5 on the overall performance of the pot models for example the a d test prior to the 2015 event was estimated at 0 9504 while the test results with the hurricane event produced a significance level of 0 9532 therefore the inclusion of the 2015 flood data appears to be less statistically significant the fitted distribution parameters for the pot analysis presented in table 14 indicate that the 2015 flood event increased the scale parameter by approximately 15 cms such increases led to a larger variance in the predicted runoff and increased the non linearity of the quantile function for which design discharges can be estimated the k s test of the pp model produced a more significant influence from the event this result is observed with the increase in the location parameter from approximately 411 cms to approximately 427 cms although the location parameter is a vital distribution parameter the shape and scale parameters play a more influential role in the runoff estimation for the infrastructure design setting the gp and pp models presented in fig 7a and b were used to construct return level predictions for the saluda river this result is presented in fig 8 like the two previous applications the model return level function of each model appropriately represented the estimated discharge below the 10 year event with more variation between the modeled and empirical results above the 10 year event see fig 9 the bm approach predicted higher return level estimates when compared to the pot approach although both followed the empirical data well and have comparable shape parameters further reduced confidence intervals were generated from the pot model versus the amf model for example the 100 year storm ci for the amf model would be approximately 900 3200 cms while the pot ci would be approximately 700 1700 cms hence for a common design storm such as the 100 year event the amf model has approximately 1300 cms more variation in the predicted design discharge on the other hand the 100 year design discharge for the amf model would be approximately 1700 cms while the pot model would be 1100 cms thereby representing a difference in the design discharges by approximately half the difference in the discharge variation in this case the amf model has more than four times the available data of that compared to the pot model making the selection of the amf model much more appealing to infrastructure safety and reliability as opposed to the pot model the return period framework was extended to estimate the intensity of the 2015 event to be approximately 11 2 years and 51 7 years for the amf and pot model respectively this result further exemplifies the uncertainty in design level estimates for this basin using the pot techniques presented herein when compared to the amf model for example the amf analysis resulted in a slightly negative shape parameter which did not experience a significant change with the inclusion of the 2015 event however the pot model experienced a positive shape parameter which doubled during the post event analysis hence increasing the non linearity in the return level estimation in this context the amf model showed less influence and severity due to the hurricane event when compared to the pot model although with a much larger sampling size 4 4 analysis of the congaree river the congaree river is located at the heart of columbia sc and drains more than 20 331 sqkm block maxima i e 1982 2015 and pot i e 10 01 1984 09 30 2015 datasets were developed from historical usgs datasets and used to construct probabilistic models of extreme flood events fig 10 reveals the results for the gev gp gumbel and pp models through constructed cdf and probability plots without the 2015 event the amf results show that each model performed well when comparing the modeled probability to the empirical probabilities of which the gev appeared to be the best fit the gev and gumbel models produced comparable distributions i e cdfs but a review of the q q plots show much more variability in the upper tail behavior of the gumbel model i e 5000 cms to 10 000 cms the gp and pp models provided satisfactory results below 10 000 cms for the distribution and quantile predictions but well overpredicted the major flood event by more than 10 000 cms it is evident that the observed patterns i e for the mean and standard deviation in the different quantile plots are not in the middle of the simulated bundle of curves but are compatible with the range of extreme fluctuations and persistent however the quantile estimation method like any statistical method can be affected by the sampling uncertainty the amf performance summary outlined in table 15 indicates that the gev model was appropriately considered the parent distribution for the congaree river flood data that represented the upper tail behavior of the empirical distribution quite well when compared to the rest of models the upper tail behavior of the gp and pp models demonstrated in the amf diagnostic plots likely indicate the inadequacy of these models in approximating the block maxima time series for the congaree river basin although the length of amf for the congaree river is more than 120 years the behavior of the extreme flood data did not follow fréchet law as observed at the saluda river basin inclusion of the 2015 flood event within the amf analysis i e amf 2015 had the least amount of effect on the performance of the probability models compared to the three previous applications in this case the congaree river basin is the largest of the four study areas thus exhibiting larger runoff lag times and response levels for the same meteorological setting the longer time series and larger basin size is expected to enable the flood distribution to be less affected by additional extreme events no significant i e 0 05 changes were observed in the amf shape parameter estimates with the inclusion of the 2015 flood event table 16 thus suggesting the event does not appear to affect the distribution or cause a larger degree of uncertainty in the model fitting the gev served as the most appropriate amf distribution with a minimal increase in the estimated shape parameter by 0 003 conversely the scale parameter experienced an increase of approximately 11 cms in this case the 2015 flood is indicated to be a larger event increase in location parameter and introduces more variability in runoff deviation nonetheless the runoff rate for any given return level is nearly constant the performance summary for the congaree pot time series excluding the 2015 flood provided in table 17 shows a strong consideration for the gp or pp as a parent distribution when considering the significance level of the cumulative frequency distribution however bayesian criterion suggests that the pp model should be selected in this case the traditional hypothesis tests produced nearly identical results but a fair comparison of the information based criteria i e bic aic quickly differentiates the underlying distribution extension of the pot time series to include the entirety of the 2015 flood event did not significantly affect the pp model for example the pp scale and shape parameters prior to the 2015 event were 636 62 cms and 0 1810 respectively while the influence of the 2015 flood increased the scale and shape to 677 86 cms and 0 2085 correspondingly see table 18 while the shape parameter is only marginally affected the scale parameter does experience an increase by more than 6 5 an important result demonstrated in table 17 is the fact that the performance of each model slightly improved with the inclusion of the 2015 flood event data thus there is ultimately a degree of influence fig 10 shows the predicted return levels with bootstrapped 95 cis assuming the gev and pp models as parent distributions for the amf and pot analyses respectively in this case the amf frequency analysis resulted in a return period function which more adequately mimicked the empirical behavior of the underlying distribution when compared to the pot frequency analysis it is important to note that the pp parameterization used herein is given in terms of the gev block maxima parameters and then further re parameterized as a threshold model in terms of the gp derivation both the amf and pot return level functions had comparable underlying function shapes i e amf 0 25 vs pot 0 18 as with the enoree river and the saluda river applications while the underlying scale of the amf model was much higher i e amf 868 6 cms vs pot 636 6 cms for example the estimated 100 year runoff for the amf model was just below 10 000 cms while the pot model estimated slightly less than 7500 cms fig 10 in this case the design level discharge is approximately 2500 cms less for the pot approach this result is like that of the saluda river application in that the amf model produces consistently higher design discharges but based on more than 100 years of historical data as opposed to the pot model which is based on approximately 35 years of historical records as stated in the analysis of other applications the small size of the samples tend to hide the true distribution and the heavy tail fluctuation a peak flow rate of 5239 cms registered for the congaree river during hurricane joaquin the models presented herein i e gev and pp estimated a return period of 16 9 years and 29 2 years for the amf and pot series respectively the intense rainfall which was categorized as a 1000 year storm did not directly correlate to such an extreme event for the direct runoff i e river discharge within the congaree basin however extensive hydraulics structures e g dams ponds levees etc in the study region can delay and mitigate peak runoff and substantially affect distribution family 5 conclusions this paper provided a critical analysis of stochastic flood frequency analysis using four real world applications from north carolina and south carolina usa the aim was to explore the inferences involved in extreme modeling the available methods tools and explicitly highlight the modeling difficulties and the role of uncertainty in flood frequency analysis two types of time series were evaluated the first method employed historical annual maximum flood data amf collected by usgs while the second incorporated daily maximum flood data or pot the amf incorporated all annual data points while the pot series incorporated all daily maximum data above a specific threshold defined based on statistical goodness of fit tests four extreme value probability distributions were selected and tested based on statistical fitting i generalized extreme value ii generalized pareto iii gumbel and iv point process of these methods the gp and pp models are parameterized in terms of threshold exceedances for the pot dataset evaluation of the extreme events was first based on statistical testing of the data with and without the inclusion of the 2015 flood event that occurred in the carolinas the results indicated that the 2015 flood event does affect some of the probability distributions but does not appear to cause statistically significant influences on the selected parent distribution focusing on the geographical distribution of the fitted distributions in the study region the analyses suggest that large urban and rural areas share approximately the same distributions i e gev whereas less developed areas exhibit light to heavy tailed behaviors i e gumbel and gp the amf analysis of rocky creek favors the use of the gumbel distribution and does not provide any evidence of its outperformance over the fréchet law saluda river on the other hand showed the best fit to gp with a negative shape parameter here we would suggest that this should not be used in the case where flood data propose an extreme distribution with a negative shape parameter instead it makes more sense and seems more reasonable to use a gumbel distribution as recommended by papalexiou and koutsoyiannis 2013 in addition the analysis of the estimated gev shape parameters of the amf data for the enoree and congaree basins revealed a close relationship between these two basins and suggests that the distribution of the gev shape parameter that would emerge if extremely large samples were available is slightly shifted toward a heavier tail when the october 2015 flood was included 0 25 however as stated by papalexiou and koutsoyiannis 2013 only very large samples can reveal the true distribution of the shape parameter and actual variability of the underlying process further modeling results of the pot series showed the best fit with the pp distribution for four applications in all cases the average value of the pp shape parameter increased and tended to a positive value as the october 2015 pot values were included to the time series i e the record length increased under the hypothesis of the existence of an asymptotic distribution for the shape parameter it has been demonstrated that the apparent exponential decay of the upper tail of the pot distribution observed in short time series is coherent with an asymptotic process which fluctuates around an average heavy tail behavior and affects return level estimation and its uncertainty dealing with extreme analysis uncertainty affects not only the distribution model but also the exploratory diagnostics this study used the nonparametric and parametric bootstrap method or monte carlo simulation model to compute cis of distribution models and the estimators the empirical distribution of the bootstrap method showed a close relationship with the actual distribution i e asymptotic assumptions and easily met specific requirements of the four applications therefore we suggest the bootstrap method as a practical approach to obtain cis however more advanced methods such as bayesian approaches can be applied to reduce the uncertainty by incorporating exogenous information i e a variable originates externally but has influences within a drainage system based on these analyses the answer for the research question of how extreme was the october 2015 flood in the carolinas raised earlier is that there is insufficient evidence to show that the 2015 flood event affected the parent distribution model of the annual series but certainty affected the pot series in terms of shifting the shape parameter towards a heavier tail however tail fluctuations that are caused by i mixture of extreme observational data and their thresholds and ii temporal fluctuations of the parent distribution and or its parameters over long time scales may introduce significant uncertainty and make extreme inference difficult while this research tends to diminish the impact of the first fluctuation by analysis of different applications time series and thresholds the latter one is correlated with the fluctuation of physical mechanisms driving the runoff process such as dam construction land use changes site development etc as recently advocated by samadi and meadows 2017 further research is required to understand the true behavior of runoff in the study region for instance the inclusion of independent variables e g reservoir factor can be employed as covariates to check for monotonic trends abrupt changes or more complex nonlinear temporal patterns and relationships between flood dynamics and watershed factors in this context possible deterministic predictable mechanisms can be applied to identify temporal permanent evolution of drainage systems if necessary such an assessment should be complemented by other criteria such as risk of failure in the design life and cost benefit analyses considerations by accounting for the different sources of uncertainty e g distribution parameters quantile estimates and sampling uncertainty finally this research provides a consistent and practical procedure to model floods which can be applied to other hydrological extremes e g rainfall the key implication of this analysis is that increasing the magnitude and frequency of runoff particularly for peak over threshold events is more probable at least for the four applications presented here thus the classical frequency analysis may underestimate the flood magnitude this result is seen in the context of the saluda and congaree basins which experience major changes in the estimated shape parameters for the pot analysis and should not be blindly assumed to occur elsewhere however a similar analysis could be thoroughly applied to study regions around the globe to grasp a greater understanding of changing flood distributional behavior in response to extreme events and aid in advancing resilient infrastructure design engineering design and practice need to move from simple methods to more practical advanced approaches that acknowledge the shifting of extremes from exponential law toward heavier tailed probability distributions acknowledgments this research was supported by sc sea grant consortium grant 15520 ga11 and the university of south carolina grant 15520 16 40787 and 15520 17 44716 the analyses were performed using the ismev and extremes packages in r r core team 2013 the authors and maintainers of these tools are gratefully acknowledged 
7248,this paper examines the frequency distribution tails and peak over threshold pot of extreme floods through analysis that centers on the october 2015 flooding in north carolina nc and south carolina sc united states us the most striking features of the october 2015 flooding were a short time to peak tp and a multi hour continuous flood peak which caused intensive and widespread damages to human lives properties and infrastructure the 2015 flooding was produced by a sequence of intense rainfall events which originated from category 4 hurricane joaquin over a period of four days here the probability distribution and distribution parameters i e location scale and shape of floods were investigated by comparing the upper part of empirical distributions of the annual maximum flood amf and pot with light to heavy theoretical tails fréchet pareto gumbel weibull beta and exponential specifically four sets of u s geological survey usgs gauging data from the central carolinas with record lengths from approximately 65 125 years were used analysis suggests that heavier tailed distributions are in better agreement with the pot and somewhat amf data than more often used exponential light tailed probability distributions further the threshold selection and record length affect the heaviness of the tail and fluctuations of the parent distributions the shape parameter and its evolution in the period of record play a critical and poorly understood role in determining the scaling of flood response to intense rainfall keywords flood frequency analysis return period distribution tails the carolinas 1 introduction the october 3 5 2015 historic rains caused by hurricane joaquin released more than 500 mm of rain in south carolina sc and north carolina nc united states us the flood peak of many u s geological survey usgs gauges including those located in the center of sc were almost twice the previous maximum from a record of over 65 years the spatial extent of flooding in this portion was also unprecedented with more record flood peaks at usgs stream gauging stations in urban areas such as columbia the capital of sc than for any other rural catchments such an extraordinary flood lies within the fundamental issue of infrastructure safety and raises the crucial question of how to proceed if this event is not visible for a given dataset and if it is too rare for design applications although recently significant progress has been made to predict short term flood for operational purposes e g pourreza bilondi et al 2017 long term prediction on which infrastructure design is based is difficult in deterministic terms e g papalexiou and koutsoyiannis 2013 thus it is common to treat this event in a probabilistic manner i e as a random variable that is governed by a distribution law such a distribution enables the modeler to capture the probability of exceedance and assign a return period to any flood event the procedure called flood frequency analysis ffa in design hydrology assessment of flood probability has been an active research topic yet a less understood concept however the analysis is well rooted in an extensive literature dating back to the work of nicolaus bernoulli three centuries ago mentioned in gumbel 1958 extreme value theory evt was the first and widely accepted method for ffa that has rapidly evolved and found applications in engineering hydrology fuller s 1914 study was probably the first application of extreme value distributions some recent studies such as papalexiou and koutsoyiannis 2013 and serinaldi and kilsby 2014 2015 expanded the evt concepts for hydrological design applications specifically evt has stimulated an extensive investigation to estimate the parent distribution e g michele and rosso 2001 bernardara et al 2008 and upper tail behaviors of flood properties papalexiou and koutsoyiannis 2013 serinaldi and kilsby 2015 just to mention a few recent studies focusing on evt and referring to renard et al 2013 and martinkova 2013 for a recent review of the evt applications in hydrology this theory captures the asymptotic distributional behavior of two types of data namely the so called block maxima bm and peak over threshold pot bm extracts the maximum values from subsets i e blocks of observations whilst pot performs observations exceeding a certain threshold when the size of the blocks approaches infinity the distribution of bm converges to three types of extreme value distribution families gumbel fréchet and reverse weibull fisher and tippett 1928 gnedenko 1943 where the parameters scale with the information dimension these three types of extreme families can be described by the so called generalized extreme value gev distribution with the location scale and shape parameters e g coles 2001 as defined by the unified von mises jenkinson parameterization jenkinson 1955 if the threshold of exceedance increases the gev then converges to the so called generalized pareto gp distribution as described by the pickands balkema de haan theorem pickands iii 1975 balkema and de haan 1974 in many cases gp yields a more accurate approximation to the distribution of absolute and relative excesses as well as distribution tails in addition it represents distribution tails obliquely but rigorously by letting the data decide the function in practice a way to verify the validity of gp is to check whether the estimates of the shape parameter are stable when the model is fitted to excesses over a range of thresholds from a theoretical point of view absence of the stability can be explained by a slow rate of convergence in the pickands balkema de haan theorem the fitted model can then be used to compute any tail related risk measure such as tail probabilities tail quantiles or value at risk etc there is an established link between gp and gev in the evt modeling in practice if block maxima follow a gev distribution then the threshold excesses have a corresponding approximate distribution within the gp family e g coles 2001 and vice versa gev parameterization can be estimated using gp such as poisson distribution for the occurrence frequency of the pot e g goda 2011 recently the probabilistic fitting of these extreme distributions to hydrological variables signifies major progress in design hydrology as it quantifies risk and disputes arbitrary notions e g koutsoyiannis 2004 although in spite of the extensive literature on eva model fitting and goodness of fit testing only few studies have recently tackled the practical problems of flood frequency analysis facing real time application and uncertainty e g vogel et al 2011 stedinger and griffis 2011 rootzén and katz 2013 papalexiou and koutsoyiannis 2013 obeysekera and salas 2014 serinaldi and kilsby 2015 mondal and mujumdar 2015 the application of extreme theory on various real world applications is essential for risk assessment and water resources planning which demand long time horizons with no other rational scientific basis than probability therefore the aim of this paper is to compute ffa and return periods for annual and instantaneous floods in the center of the carolinas with special attention to the pot approach and to place the october 2015 flood in a flood frequency analysis context an important class of questions addressed in this study concerns the impact of peak rates and thresholds on the upper tails of flood distributions the goal was to investigate the distribution fitting model and the upper tail distribution of maxima and to provide a better answer to the question of how extreme was the october 2015 flood in the carolinas to address aforementioned question four different applications across the carolinas were used to infer various procedures and to relate these analyses to properties of the october 2015 flooding spatial and temporal variability of flood events and the uncertainty associated with flood properties were also addressed during the period of analysis the underlying parent distributions were also re assessed with the inclusion of the 2015 flood event in order to characterize distributional changes associated with the fitting parameters this study quantified the sampling uncertainty via confidence intervals cis in the evt framework to highlight its fundamental role for a fair comparison between models and a fair assessment of the output reliability this paper is organized as follows in section 2 the study region and flood data used in this study are explained the theoretical concept and mathematical structures of probability distributions distribution parameters and pot are explained in section 3 these methodologies were then examined in four different case studies explained in section 4 each application example has its own merit emphasizing particular aspects of extreme analysis conclusions are provided in section 5 as critical guidelines to address the limitation and criticism in judging the magnitude and frequency of floods 2 study area and data the study area is located in the eastern us covering portions of the mountain and piedmont regions of nc and sc the area is comprised of the wateree the upper and the lower broad the upper and the lower catawba the south fork catawba the congaree the tyger the saluda and the enoree sub basins two metropolitan cities charlotte and columbia are located in the study region see fig 1 flood data were analyzed to determine appropriate usgs gauges that exhibited a long term period of record as well as a consistent hourly sub hourly record of the october 2015 flood event due to limited data availability and large missing values this approach limited the analysis to four usgs monitoring gauges the flood data of usgs 02147500 rocky creek at great falls sc usgs 02160700 enoree river at whitmire sc usgs 02167000 saluda river at chappells sc and usgs 02169500 congaree river at columbia sc were used fig 1 historical instantaneous floods data for each of the usgs gauging stations were also collected from october 2 2015 to october 10 2015 as well as the period of record data for observed instantaneous flows and peak annual maximum flows although the rainfall event produced from the hurricane event lasted approximately 3 days the flood hydrograph spanned for several days as the drainage system responded to intense rainfall for a longer period further the october 2015 intense rainfall produced different runoff volumes although the time to peak and flood duration are approximately the same among the four hydrographs fig 2 it should be noted that all four hydrographs presented in fig 2 have a short time to peak tp and a continuous peak rate which caused intensive and widespread damages to human lives properties and infrastructure here flood data analysis placed emphasis on the concept of water year which is often designated as the hydrological year beginning on october 1st and ending on september 30th accordingly two alternative approaches of data are selected i annual maximum flood amf series spanning from 1892 to 2015 with no missing values and ii daily maximum flood dmf or pot series spanning from october 01 1984 to september 30 2015 with 3 of missing values 3 methodology the data sets described in the previous section were employed to fit different distribution functions this study tested i the significance of lag 1 correlation for two subsequent values by the kendall correlation coefficient k acf ii possible monotonic trends by the mann kendall m k test and iii distributional hypotheses by goodness of fit and ad hoc diagnostics specifically the suitability of a probability distribution is assessed by four goodness of fit tests namely kolmogorov smirnov k s cramer von mises c vm anderson darling a d and the pearson product moment correlation coefficient ppmcc kottegoda and rosso 2008 k acf and m k tests are necessary to compute the temporal dependence and monotonic trends in data these trends can also affect the outcome of the goodness of fit tests which rely on the hypothesis of independent observations serinaldi and kilsby 2014 in addition to a hypothesis based goodness of fit information based criteria also known as bayesian statistics for probability model selection laio et al 2009 was used which has shown to aid modeler in identifying the best probability distribution for hypothesis testing e g di baldassarre et al 2009 chen et al 2017 more specifically the akaike information criterion aic and bayes information criterion bic were adopted as the information based criterions bic and aic reward a model for a higher likelihood and penalize a model for overfitting lower aic and bic indicate a better fit these techniques were implemented using continuous to discrete distributions explained in the next section 3 1 theoretical concept of probability distribution continuous to more skewed distributions were employed to capture flood characteristics over time gumbel is tested as a continuous probability distribution whereas extreme value theory was used to implement discrete probability distributions continuous distributions have been widely applied in hydrology so we refer the readers to vogel and wilson 1996 koutsoyiannis 2005 mcmahon et al 2007 and el adlouni et al 2008 among others in the classical extreme value theory if a random variable rv x follows the distribution fx x the distribution function of the maximum of n is independent and identically distributed iid that can be described by 1 g y n f x x n if n the distribution of n can converge to three limiting laws of the gumbel the fréchet or the reversed weibull which can be described by the unified von mises jenkinson parameterization jenkinson 1955 of the so called gev distribution coles 2001 2 g z exp 1 ξ z μ σ 1 ξ where z 1 ξ z µ σ 0 µ is the location parameter σ 0 is the scale parameter and µ is the shape parameter depending on the sign of the shape parameter gev can envelop three types of distribution functions i ξ 0 the heavy tailed fréchet ii ξ 0 the upper bounded weibull and iii ξ 0 the gumbel distribution function based on the pickands balkema de haan theorem pickands iii 1975 balkema and de haan 1974 a gev distribution coverages to a gp distribution if the extreme threshold increases over time coles 2001 which is given as 3 h y 1 1 ξ y σ 1 ξ where y y 0 and 1 ξy σ 0 and σ σ ξ u µ ξ determines three types of distribution functions with the same interpretation as for gev heavy tail when ξ 0 i e pareto upper bound when ξ 0 i e beta and exponential in the limit as ξ among three parameters in the eva theory ξ plays a key role in hydrological frequency analysis since it determines the upper tail behavior of floods that is important for the design of hydraulic structures the point process pp model is applied to the distribution of discharge excess following that of the gev parameterization as outlined by coles 2001 and approximated by the gp distribution pp treats the data points as the occurrence of extreme events and marks their associated size briefly if the parent distribution does not evolve over time i e stationary and satisfies an asymptotic lack of clustering condition for values that exceed a high threshold then the distribution s limiting form is non homogeneous poisson it is noted that gev pp gp and gumbel distributions were fitted to the amf data while only gp and pp models were tested for the pot dataset application of the gp and pp distributions to amf data is not typically applied in engineering practice or research but have found merit in previous studies mohssen 2009 papalexiou et al 2013 as a result this study incorporated the distributions as an exploratory case to better understand if the october 2015 flood shifted the tail behavior of maxima towards a heavier tail distribution in the case where the limiting form is an excess distribution such as the gp or pp comparative annual design quantiles were estimated by considering the threshold exceedance rate and number of raw observations per year coles 2001 and therefore can be practically applied in the infrastructure design setting the supporting parameterization of the cumulative distribution and their inherent parameters implemented within the study are provided in table 1 a fair comparison between various fitting models requires the assessment of the uncertainty of probabilities return periods and design quantiles indeed when evaluating design properties the differences of modeling estimates should be significant for operational use in this respect there are several methodologies that have recently been applied to various extreme fitting models including the delta method obeysekera and salas 2014 the bootstrap resampling method efron and tibshirani 1993 samadi et al 2013 and the profile likelihood function method obeysekera and salas 2014 the delta method relies on the asymptotic properties of the maximum likelihood estimates of the model parameters and their covariance matrix e g serinaldi and kilsby 2014 obeysekera and salas 2014 it calculates symmetric cis under the hypothesis that the distribution of the quantiles is reasonably described by a gaussian distribution the bootstrap method resamples the observed series using nonparametric bootstrapping or parametric bootstrapping also so called monte carlo simulation to simulate iid realizations of a random variable y and to draw a suitable standardized distribution the bootstrap method provides an assessment of the sampling and parameter estimation uncertainties by realistic asymmetric cis serinaldi and kilsby 2014 unlike the bootstrap method the profile likelihood function relies on the asymptotic properties of maximum likelihood estimators although this method is difficult to implement and can be quite computationally burdensome e g obeysekera and salas 2014 in this study the bootstrap method is used to calculate the uncertainty of cis because it is independent of the probability estimation method and provides more reliable simulation of iid realization 3 2 parameter estimation the method of moments and method of l moments have been traditionally employed to estimate distribution parameters in hydrological datasets hosking et al 1985 maidment 1993 gubareva and gartsman 2010 in recent years due to advancements in computing the method of maximum likelihood estimation mle and generalized mle gmle martins and stedinger 2000 2001 have a growing interest these methods provide unbiased parameter estimates compared to the method of moments thus the mle method was adopted for the estimation of distribution parameters for the results presented herein for the case where the random variable x has probability density function f x θ0 the likelihood function is given as 4 l θ i 1 n f x i θ eq 4 represents the likelihood function of a set of independent realization of the random variable x given a set of parameters θ which describes the underlying nature of the probability density function coles 2001 usually the optimization of the parameter set is addressed in terms of the log likelihood function given by 5 l θ log l θ i 1 n log f x i θ during the maximum likelihood approach eq 5 is implemented using optimization procedures such that the log likelihood function is maximized resulting in the maximum likelihood estimates of the distribution parameter set θ because of the monotonic behavior of the log likelihood function the likelihood and log likelihood functions take on the maximum given the same set of likelihood estimators coles 2001 the model likelihood functions implemented within this study are outlined in table 2 3 3 threshold selection a critical component in the implementation of the gp and pp excess distribution models in hydrological frequency analysis is the selection of a threshold value villarini et al 2011 saeed far and wahab 2016 in this study mean residual life plots and threshold range plots coles 2001 were applied for the evaluation of threshold values each of these exploratory techniques aim to guide in the selection of a minimum threshold such that there is linearity below the threshold in the mean residual life plot and there is stability beyond the threshold in the distribution parameters coles 2001 saeed far and wahab 2016 further evaluation of threshold selection for the pot and its influence on the performance of the gp and pp models was analyzed by taking into consideration the significance level for the goodness of fit tests outlined earlier in this study threshold values were selected such that appropriate significance levels were achieved with the lowest possible threshold as to limit the resulting variance in distribution parameter estimation while achieving agreement among the three goodness of fit two sided p values each of the above techniques was employed along with goodness of fit tests kolmogorov smirnov k s cramer von mises c vm and anderson darling a d to derive a threshold value for each usgs gauging station it is important to note that in the case of block maxima data the threshold value was set slightly lower than the absolute minimum of the amf data set such that all events are considered this is done because by nature that is the natural threshold of the amf time series as a result the gp and pp distributions consider the amf data in the same facet as the gev and gumbel distributions thereby providing consistent sample sizes for which the distribution parameters can be estimated and prevent additional bias 4 applications 4 1 analysis of rocky creek rocky creek drains approximately 502 5 square kilometers sqkm just north of the catawba river into lake wateree sc the drainage area above the usgs monitoring station can be classified as moderately rural composed of smaller communities and residential areas in this drainage system the peak flow rate during the october 2015 flood event was approximately 59 cubic meters per second cms with a sustained peak rate for approximately 8 h analysis begins with the diagnostic plots fig 3 a presents the results for the fitted cumulative distribution function p p probabilities plot and q q quantile quantile plots for the gev gp gumbel and pp models using historical amf records without the inclusion of the 2015 flood event i e 1952 2015 for clarity these data will be referred to as amf herein while historical records with the 2015 event will be referred to as amf 2015 the diagnostic plots reveal that both the gev and the gumbel models more precisely predicted the empirical distribution and probabilities when compared to the gp and the pp models both the gev and gumbel distributions appeared to have comparable quantile accuracy below 250 cms with the gev model showing more variability in its upper tail bound visual fitting results of the gp and pp models indicate that each model under over predicted the observed probability p p and increasingly deviated from the observed quantiles with increasing probabilities a more detailed review of the fitting performance data provides a summary of the four models for the amf data table 3 presents two sided p values for the k s c vm and a d tests the pearson correlation coefficient ρ p p coefficient of determination r2 aic and bic the upper tail performance of the gev model as indicated by the a d test proves a slightly higher significance compared to the gumbel model while the overall fitting performance indicated by the k s test is much lower than that of the gumbel the cumulative consideration of the goodness of fit test results and the information based criteria indicated that the gumbel distribution provided the best fit for the rocky creek amf data followed by the gev gp and pp models respectively it is important to note that the selection of the most appropriate parent distribution within this study was based on the distribution which produced the highest and most consistent significance level i e two sided p value for the k s c vm and a d tests while also generating the lowest information based criteria set forth by the aic and bic statistics next the analysis proceeded using amf 2015 data results of the amf 2015 analysis are based on the hypothesis that the parent distribution of amf i e gumbel would not hold true for amf 2015 inclusion of the 2015 flood event within the amf dataset i e amf 2015 served as an investigatory analysis into the underlying evaluation of the potential abnormality of the 2015 hurricane event and its impact on the underlying parent distribution of amf referring to table 3 the performance of the probability models was not generally affected when including the october 2015 flood event however the fitted models of the evt family showed slightly asymmetric behavior but they were not prominent as the upper tails of gp gev and pp models are approximately exponential i e their shape parameters are close to zero in this respect the gumbel model deemed more accurate than those obtained by power law as expected the only model which experienced a significant impact on performance was the gev k s 0 05 the fitted distribution parameters presented in table 4 highlight the superiority of the gumbel model for the amf 2015 series when compared to the rest of the models the gumbel distribution computed the second lowest scale parameter thus constituting a much lower variance in the predicted runoff level for any given design period this result indicates that the hurricane event more so follows the underlying distribution of the annual flood data rejecting the hypothesis of power law distribution for the rocky creek amf 2015 dataset as the second part of data analysis for rocky creek a continuous time series i e sub hourly of dmf was extracted for a period from 10 01 1986 09 30 2015 gp and pp are the two most appropriate distributions to describe the exceedance over threshold thus they were implemented for pot probability analysis diagnostic plots of the resulting pot frequency analysis without the inclusion of the 2015 flood i e pot are presented in fig 3b examination of the cdf and p p plots suggest a decreased performance in the parent distribution i e gumbel due to the variation between the model and observed data an assessment of the gp and pp diagnostic plots show arguably exceptional fits with a very fine degree of variation between the model and observed data with little to no visual deviance between the two model results table 5 overall the gp and pp models provided between 0 85 and 0 93 significance levels with a well computed upper tail bound i e a d 0 93 the gp and pp models generated nearly identical results given the goodness of fit tests and coefficient of determination but the pp model dominated after review of the bayesian information criteria inclusion of the 2015 flood event i e pot 2015 appeared to have a marginally significant effect on the central distribution behavior as indicated by the k s test however the upper tail behavior outlined by the a d significance levels showed negligible impact the resulting distribution parameters for the pot analysis are presented in table 6 the pp model distribution parameters for the pot with the 2015 event clearly show an increase for example the variation in predicted runoff as indicated by the scale parameter increased from 43 4 cms to 45 9 cms while the rate at which runoff generation grows with more intense events set forth by the shape parameter increased from 0 017 to 0 031 although the increase in the parameters is small the increase in the positive shape parameter indicates the possibility of the return level growth and non linear increase over larger return periods thus the inclusion of additional data within the pot time series appears to have a considerable effect to a degree of the ultimate distribution shape and hence the underlying vulnerability of current drainage infrastructure addressing the implications of the flood event through the amf and pot analysis was complimented by return level predictions generated from the amf and pot parent distribution parameters these estimates with the bootstrapped 95 cis are presented in fig 4 the results indicate that the bm s parent distribution replicated the theoretical return level function quite well see raw data points the amf computed return levels from the gumbel distribution had little to no dispersion below the 25 year return period unlike the amf the pot return model had much more variation between the modeled and empirical results since the annual time series was best represented with a gumbel model there is a strong linearity in return level estimation with less uncertainty above the 50 year level when compared to the pot model i e pp distribution the width of the cis confirms that a large uncertainty characterizes the results obtained by the pot analysis for instance the amf 100 year return level ranged between approximately 420 to 650 cms whereas a significant width of the ci for lower i e 10 year and upper i e 100 year return levels reflect that the model is less informative this further implies that the increase of extreme data is paid in terms of the increase of uncertainty and extreme complexity in both cases the shape and underlying distribution were different however a peak flow rate of 59 cms only registered as a 1 1 year and 1 0 year return period for the amf and pot model respectively thus confirming that the 2015 hurricane event cannot be classified as an extreme event from a truly probabilistic point of view 4 2 analysis of the enoree river the enoree river lies in the heart of the sumter national forest draining more than 1150 sqkm of rural and urban land and eventually discharging into the broad river available historical datasets for this basin provided amf data from 1974 to 2015 in addition to continuous i e sub hourly flow data from 04 05 1985 09 30 2015 fig 5 a presents the diagnostic plot results of the amf analysis the results indicate that the gev and gumbel models appeared to be good indicators for predicting the annual maxima with little deviation between model and empirical estimates the cdf plots of these models show similar characteristics in the lower tail behavior of the distribution but a review of the probability plots reveals that the gev had much less variation between the model and the observed data than that of the gumbel the quantile plots confirm this result and show that the gev model had more accuracy in predicting the upper tail behavior of the amf distribution when compared to the gumbel a significant increase in the shape parameter of amf 2015 denotes a tendency to heavier tailed fréchet law unlike the gev model both the gp and pp resulted in slight deviations in the model fitting the amf results table 7 showed a superior performance of the gev model for all goodness of fit results 0 87 0 90 however each of the models proved to have similar accuracy in predicting the probability of exceedance with the gev being however the most realistic among the competitors as illustrated by the diagnostic plots see fig 5a the gumbel model represented the distribution of data well but falls short of the gev with significance levels approximately 0 22 to 0 42 lower than the gev the central tendency of the gp and pp models was slightly lower than that of the gumbel nonetheless the tail performance was at most satisfactory 0 16 the consideration of each of the performance standards led to the adoption of the gev model as a proxy for the amf data however interestingly the gev gp and pp models had similar bayesian criterion with the latter being the most realistic model including the 2015 flood event within the amf dataset i e amf 2015 predominantly affected the gev and the gumbel models although around the 5 significance level in both cases the performance of the goodness of fit tests improved with the most improvement observed in the tail behavior of the distributions for example prior to the 2015 hurricane the a d statistic for the gev model was 0 90 while following the event this statistic increased to approximately 0 95 improved performance was also noted in the gp and pp models although affected less than the 2 ci as a result the gev model maintained the stance of representing the amf distribution however as cooley 2013 stated any method for generating cis has drawbacks therefore caution should be exercised when interpreting such results a review of the fitted distribution parameters is warranted to further summarize the response of the probability distribution to the 2015 flood event the results presented in table 8 do not suggest a larger rate of variability of the runoff prediction for the gev gp and pp models as indicated by the shape parameter however the gp and pp models did tend to show much more fluctuation in the model when comparing the scale parameter during pre and post hurricane event for instance the gev model showed an increase in the scale parameter of approximately 1 6 cms while the gp and pp models increased by approximately 5 cms in this basin incorporation of amf excess models i e gp and pp presented more uncertainty in design variables as opposed to the selected gev parent distribution the distribution fitting performance increased with the addition of the 2015 hurricane event however many of the estimated distribution parameters experienced a slight increase for example the gev location and scale parameters increased by approximately 2 cms and 1 6 cms respectively while the shape parameter remained constant in this regard the central tendency and the anticipated variation of the discharge increase such an increase is considered minor and practically immeasurable in engineering practice most importantly the upper tail behavior i e shape parameter of the distribution remained unchanged by the hurricane event as a result the parent distribution did not show significant deviations during the post event analysis indicating no abnormalities or outlier prone evidence of the 2015 hurricane event in this drainage system it is however important to note that the lack of abnormalities indicated by the distribution performance and parameters does not necessarily negate the potential extreme intensity of the event further evaluation of the extremeness of the 2015 flood event was continued for the enoree river with the development of probability models using the partial duration series prior to the 2015 flood event visual inspection of the diagnostic plots for these models fig 5b indicated exceptional fits for both the gp and pp in terms of the predicted distribution function i e cdf the results of goodness of fit tests revealed that the gp model simply provides a proxy for the probabilities of exceedance qi with the pp model trailing however bayesian statistics highlight the superiority of the pp model see table 9 evaluation proceeded by analysis of the pot 2015 series the results presented in table 9 prove that the inclusion of the 2015 flood event significantly affected the distribution performance both the gp and pp models show improvement in model adequacy in the case of the pot 2015 analysis it appears the hurricane event significantly affected the parent distribution in terms of the goodness of fit tests however both the gp and pp models showed little to no degree of variability in the distribution parameters see table 10 the most adequate fit the pp model showed the highest increase in the estimated scale and shape parameters when compared to the gp distribution fig 6 shows the 95 cis obtained by the gev and pp models for the amf and pot data sets respectively in both cases the predicted return levels for lower return periods i e 25 year event showed close to each other though a high variability existed at higher frequencies between two formulations further both series cis were slightly asymmetric and deemed more accurate based on performance criterion however such asymmetry was more prominent for the pot model as the upper tail of the pp distribution was not exponential i e the gev shape parameter is not close to zero the fitted model for pot indeed showed more nonlinear temporal patterns compare to the amf as illustrated the width of the cis of higher frequencies return periods confirms that a significant uncertainty characterizes the results obtained by the pp distribution this further demonstrates that the increase in model complexity i e moving from gev to pp distribution is paid in terms of increasing uncertainty and more complex probability models cannot replace information if this is not available estimated return periods for the enoree river discharge i e 303 cms during the 2015 event were 8 8 years and 5 8 years for the amf and pot datasets respectively the modeled return periods show larger intensities compared to the previous application but still much lower than typical bridge and road design settings in the study region i e 25 year return period although the pot distribution fit and parameter values did experience more significant variations when compared to the amf analysis these results corroborate and substantiate the previous ffa results and provide reason to disregard the runoff event as a truly extreme sample from the underlying distribution of flood events 4 3 analysis of the saluda river the saluda river is situated below lake greenwood draining a diverse set of landscapes and river systems totaling more than 3522 sqkm and eventually discharging to lake murray outside of lexington sc due to the size and slope of the saluda river basin a maximum peak flow of nearly 1000 cms was recorded to have caused considerable damages to water control structures e g dams and ponds during hurricane joaquin the usgs station at the saluda river monitors flow since 1906 i e 108 year data set for bm however a continuous sub daily maximum flood data set is not readily available until 07 31 1986 diagnostic plots for the amf series are shown in fig 7 a as illustrated the gp and pp modeled adequately in terms of predicting most of the observed flood data when compared to the gev and the gumbel models inspection of the p p plots show each model had a degree of dispersion with the gp and pp being less biased a review of the q q plots reveals the effectiveness of the gp and pp models in all cases apart from the single most extreme flood event a much different result from the previous applications of the rocky creek and the enoree river basins for example the gp and pp models showed an excellent fit to the observed discharge levels below approximately 1700 cms although in some intervals the models over predicted large flood events the gumbel distribution visually appeared to offer the most concise quantile estimates i e less dispersion when compared to empirical data inspection of the performance indicators table 11 of the four probability models show that the gp model outperformed the pp model followed by the gev and gumbel respectively unlike the two previous applications the gp and pp statistical significance of the fits as indicated by the k s c vm and a d two sided p values were substantially higher i e 0 70 0 93 significance levels statistically the gev model did provide evidence that the flood data could come from the distribution with confidence levels of approximately 0 24 0 30 however the selected parent distribution of the gp had more than twice the confidence level similarly the gumbel did show minor confidence in the distribution of the flood data but with satisfactory results in the tail estimate around 0 11 extension of the annual time series to include the 2015 flood event i e amf 2015 is summarized in table 11 the performance summary results did not change significantly for the gev and gumbel models however the gp and pp models experienced a slight improvement in the overall fitting the fitted distribution parameters table 12 substantiate this hypothesis although there was an increased degree of variability introduced into each model for example the gp model most appropriately represented the behavior of annual extreme events for the saluda river basin inclusion of the 2015 flood event within the amf series increased the scale parameter by nearly 11 cms in this case a larger degree of uncertainty in the estimated runoff is present when compared to the previous applications especially at larger runoff rates in addition the block maxima approach yielded much more appropriate performance for the gp and pp models in comparison to the gev and gumbel models however the small size of the samples usually 50 amf tends to hide the heavy tail behavior as seen elsewhere e g serinaldi and kilsby 2014 2015 evaluation of the frequency analysis using partial duration series led to a more abundant supply of extreme data points although over only a portion of the annual period of record hence the threshold models i e gp and pp provided superiority over the rest of the models fig 7b presents the diagnostic plots for the extreme value distributions considered for the saluda river pot time series like the block maxima dataset the gp and pp models characterized the extreme flood events well below the 90 exceedance level while overpredicting events above this threshold further investigation of the gp and pp probability plots revealed a less degree of dispersion when compared to the amf results fig 7a moreover the pot frequency analysis slightly affected the central tendency of the probability models however the upper tail performance was significantly affected by this frequency approach table 13 for example the amf analysis resulted in approximately 70 cis for the upper tail behavior of the gp model i e a d while pot yielded a 95 significance level for the adequacy of the gp model based on the goodness of fit test considering different goodness of fit statistics the pp model provided the best overall fit although the effect on the upper tail behavior was negligible the results presented in table 13 show that 2015 flood event appeared to have a marginally significant effect i e 5 on the overall performance of the pot models for example the a d test prior to the 2015 event was estimated at 0 9504 while the test results with the hurricane event produced a significance level of 0 9532 therefore the inclusion of the 2015 flood data appears to be less statistically significant the fitted distribution parameters for the pot analysis presented in table 14 indicate that the 2015 flood event increased the scale parameter by approximately 15 cms such increases led to a larger variance in the predicted runoff and increased the non linearity of the quantile function for which design discharges can be estimated the k s test of the pp model produced a more significant influence from the event this result is observed with the increase in the location parameter from approximately 411 cms to approximately 427 cms although the location parameter is a vital distribution parameter the shape and scale parameters play a more influential role in the runoff estimation for the infrastructure design setting the gp and pp models presented in fig 7a and b were used to construct return level predictions for the saluda river this result is presented in fig 8 like the two previous applications the model return level function of each model appropriately represented the estimated discharge below the 10 year event with more variation between the modeled and empirical results above the 10 year event see fig 9 the bm approach predicted higher return level estimates when compared to the pot approach although both followed the empirical data well and have comparable shape parameters further reduced confidence intervals were generated from the pot model versus the amf model for example the 100 year storm ci for the amf model would be approximately 900 3200 cms while the pot ci would be approximately 700 1700 cms hence for a common design storm such as the 100 year event the amf model has approximately 1300 cms more variation in the predicted design discharge on the other hand the 100 year design discharge for the amf model would be approximately 1700 cms while the pot model would be 1100 cms thereby representing a difference in the design discharges by approximately half the difference in the discharge variation in this case the amf model has more than four times the available data of that compared to the pot model making the selection of the amf model much more appealing to infrastructure safety and reliability as opposed to the pot model the return period framework was extended to estimate the intensity of the 2015 event to be approximately 11 2 years and 51 7 years for the amf and pot model respectively this result further exemplifies the uncertainty in design level estimates for this basin using the pot techniques presented herein when compared to the amf model for example the amf analysis resulted in a slightly negative shape parameter which did not experience a significant change with the inclusion of the 2015 event however the pot model experienced a positive shape parameter which doubled during the post event analysis hence increasing the non linearity in the return level estimation in this context the amf model showed less influence and severity due to the hurricane event when compared to the pot model although with a much larger sampling size 4 4 analysis of the congaree river the congaree river is located at the heart of columbia sc and drains more than 20 331 sqkm block maxima i e 1982 2015 and pot i e 10 01 1984 09 30 2015 datasets were developed from historical usgs datasets and used to construct probabilistic models of extreme flood events fig 10 reveals the results for the gev gp gumbel and pp models through constructed cdf and probability plots without the 2015 event the amf results show that each model performed well when comparing the modeled probability to the empirical probabilities of which the gev appeared to be the best fit the gev and gumbel models produced comparable distributions i e cdfs but a review of the q q plots show much more variability in the upper tail behavior of the gumbel model i e 5000 cms to 10 000 cms the gp and pp models provided satisfactory results below 10 000 cms for the distribution and quantile predictions but well overpredicted the major flood event by more than 10 000 cms it is evident that the observed patterns i e for the mean and standard deviation in the different quantile plots are not in the middle of the simulated bundle of curves but are compatible with the range of extreme fluctuations and persistent however the quantile estimation method like any statistical method can be affected by the sampling uncertainty the amf performance summary outlined in table 15 indicates that the gev model was appropriately considered the parent distribution for the congaree river flood data that represented the upper tail behavior of the empirical distribution quite well when compared to the rest of models the upper tail behavior of the gp and pp models demonstrated in the amf diagnostic plots likely indicate the inadequacy of these models in approximating the block maxima time series for the congaree river basin although the length of amf for the congaree river is more than 120 years the behavior of the extreme flood data did not follow fréchet law as observed at the saluda river basin inclusion of the 2015 flood event within the amf analysis i e amf 2015 had the least amount of effect on the performance of the probability models compared to the three previous applications in this case the congaree river basin is the largest of the four study areas thus exhibiting larger runoff lag times and response levels for the same meteorological setting the longer time series and larger basin size is expected to enable the flood distribution to be less affected by additional extreme events no significant i e 0 05 changes were observed in the amf shape parameter estimates with the inclusion of the 2015 flood event table 16 thus suggesting the event does not appear to affect the distribution or cause a larger degree of uncertainty in the model fitting the gev served as the most appropriate amf distribution with a minimal increase in the estimated shape parameter by 0 003 conversely the scale parameter experienced an increase of approximately 11 cms in this case the 2015 flood is indicated to be a larger event increase in location parameter and introduces more variability in runoff deviation nonetheless the runoff rate for any given return level is nearly constant the performance summary for the congaree pot time series excluding the 2015 flood provided in table 17 shows a strong consideration for the gp or pp as a parent distribution when considering the significance level of the cumulative frequency distribution however bayesian criterion suggests that the pp model should be selected in this case the traditional hypothesis tests produced nearly identical results but a fair comparison of the information based criteria i e bic aic quickly differentiates the underlying distribution extension of the pot time series to include the entirety of the 2015 flood event did not significantly affect the pp model for example the pp scale and shape parameters prior to the 2015 event were 636 62 cms and 0 1810 respectively while the influence of the 2015 flood increased the scale and shape to 677 86 cms and 0 2085 correspondingly see table 18 while the shape parameter is only marginally affected the scale parameter does experience an increase by more than 6 5 an important result demonstrated in table 17 is the fact that the performance of each model slightly improved with the inclusion of the 2015 flood event data thus there is ultimately a degree of influence fig 10 shows the predicted return levels with bootstrapped 95 cis assuming the gev and pp models as parent distributions for the amf and pot analyses respectively in this case the amf frequency analysis resulted in a return period function which more adequately mimicked the empirical behavior of the underlying distribution when compared to the pot frequency analysis it is important to note that the pp parameterization used herein is given in terms of the gev block maxima parameters and then further re parameterized as a threshold model in terms of the gp derivation both the amf and pot return level functions had comparable underlying function shapes i e amf 0 25 vs pot 0 18 as with the enoree river and the saluda river applications while the underlying scale of the amf model was much higher i e amf 868 6 cms vs pot 636 6 cms for example the estimated 100 year runoff for the amf model was just below 10 000 cms while the pot model estimated slightly less than 7500 cms fig 10 in this case the design level discharge is approximately 2500 cms less for the pot approach this result is like that of the saluda river application in that the amf model produces consistently higher design discharges but based on more than 100 years of historical data as opposed to the pot model which is based on approximately 35 years of historical records as stated in the analysis of other applications the small size of the samples tend to hide the true distribution and the heavy tail fluctuation a peak flow rate of 5239 cms registered for the congaree river during hurricane joaquin the models presented herein i e gev and pp estimated a return period of 16 9 years and 29 2 years for the amf and pot series respectively the intense rainfall which was categorized as a 1000 year storm did not directly correlate to such an extreme event for the direct runoff i e river discharge within the congaree basin however extensive hydraulics structures e g dams ponds levees etc in the study region can delay and mitigate peak runoff and substantially affect distribution family 5 conclusions this paper provided a critical analysis of stochastic flood frequency analysis using four real world applications from north carolina and south carolina usa the aim was to explore the inferences involved in extreme modeling the available methods tools and explicitly highlight the modeling difficulties and the role of uncertainty in flood frequency analysis two types of time series were evaluated the first method employed historical annual maximum flood data amf collected by usgs while the second incorporated daily maximum flood data or pot the amf incorporated all annual data points while the pot series incorporated all daily maximum data above a specific threshold defined based on statistical goodness of fit tests four extreme value probability distributions were selected and tested based on statistical fitting i generalized extreme value ii generalized pareto iii gumbel and iv point process of these methods the gp and pp models are parameterized in terms of threshold exceedances for the pot dataset evaluation of the extreme events was first based on statistical testing of the data with and without the inclusion of the 2015 flood event that occurred in the carolinas the results indicated that the 2015 flood event does affect some of the probability distributions but does not appear to cause statistically significant influences on the selected parent distribution focusing on the geographical distribution of the fitted distributions in the study region the analyses suggest that large urban and rural areas share approximately the same distributions i e gev whereas less developed areas exhibit light to heavy tailed behaviors i e gumbel and gp the amf analysis of rocky creek favors the use of the gumbel distribution and does not provide any evidence of its outperformance over the fréchet law saluda river on the other hand showed the best fit to gp with a negative shape parameter here we would suggest that this should not be used in the case where flood data propose an extreme distribution with a negative shape parameter instead it makes more sense and seems more reasonable to use a gumbel distribution as recommended by papalexiou and koutsoyiannis 2013 in addition the analysis of the estimated gev shape parameters of the amf data for the enoree and congaree basins revealed a close relationship between these two basins and suggests that the distribution of the gev shape parameter that would emerge if extremely large samples were available is slightly shifted toward a heavier tail when the october 2015 flood was included 0 25 however as stated by papalexiou and koutsoyiannis 2013 only very large samples can reveal the true distribution of the shape parameter and actual variability of the underlying process further modeling results of the pot series showed the best fit with the pp distribution for four applications in all cases the average value of the pp shape parameter increased and tended to a positive value as the october 2015 pot values were included to the time series i e the record length increased under the hypothesis of the existence of an asymptotic distribution for the shape parameter it has been demonstrated that the apparent exponential decay of the upper tail of the pot distribution observed in short time series is coherent with an asymptotic process which fluctuates around an average heavy tail behavior and affects return level estimation and its uncertainty dealing with extreme analysis uncertainty affects not only the distribution model but also the exploratory diagnostics this study used the nonparametric and parametric bootstrap method or monte carlo simulation model to compute cis of distribution models and the estimators the empirical distribution of the bootstrap method showed a close relationship with the actual distribution i e asymptotic assumptions and easily met specific requirements of the four applications therefore we suggest the bootstrap method as a practical approach to obtain cis however more advanced methods such as bayesian approaches can be applied to reduce the uncertainty by incorporating exogenous information i e a variable originates externally but has influences within a drainage system based on these analyses the answer for the research question of how extreme was the october 2015 flood in the carolinas raised earlier is that there is insufficient evidence to show that the 2015 flood event affected the parent distribution model of the annual series but certainty affected the pot series in terms of shifting the shape parameter towards a heavier tail however tail fluctuations that are caused by i mixture of extreme observational data and their thresholds and ii temporal fluctuations of the parent distribution and or its parameters over long time scales may introduce significant uncertainty and make extreme inference difficult while this research tends to diminish the impact of the first fluctuation by analysis of different applications time series and thresholds the latter one is correlated with the fluctuation of physical mechanisms driving the runoff process such as dam construction land use changes site development etc as recently advocated by samadi and meadows 2017 further research is required to understand the true behavior of runoff in the study region for instance the inclusion of independent variables e g reservoir factor can be employed as covariates to check for monotonic trends abrupt changes or more complex nonlinear temporal patterns and relationships between flood dynamics and watershed factors in this context possible deterministic predictable mechanisms can be applied to identify temporal permanent evolution of drainage systems if necessary such an assessment should be complemented by other criteria such as risk of failure in the design life and cost benefit analyses considerations by accounting for the different sources of uncertainty e g distribution parameters quantile estimates and sampling uncertainty finally this research provides a consistent and practical procedure to model floods which can be applied to other hydrological extremes e g rainfall the key implication of this analysis is that increasing the magnitude and frequency of runoff particularly for peak over threshold events is more probable at least for the four applications presented here thus the classical frequency analysis may underestimate the flood magnitude this result is seen in the context of the saluda and congaree basins which experience major changes in the estimated shape parameters for the pot analysis and should not be blindly assumed to occur elsewhere however a similar analysis could be thoroughly applied to study regions around the globe to grasp a greater understanding of changing flood distributional behavior in response to extreme events and aid in advancing resilient infrastructure design engineering design and practice need to move from simple methods to more practical advanced approaches that acknowledge the shifting of extremes from exponential law toward heavier tailed probability distributions acknowledgments this research was supported by sc sea grant consortium grant 15520 ga11 and the university of south carolina grant 15520 16 40787 and 15520 17 44716 the analyses were performed using the ismev and extremes packages in r r core team 2013 the authors and maintainers of these tools are gratefully acknowledged 
7249,prediction of peak discharge of floods has attracted great attention for researchers and engineers in present study nine typical nonlinear mathematical models are established based on database of 40 historical dam failures the first eight models that were developed with a series of regression analyses are purely empirical while the last one is a semi analytical approach that was derived from an analytical solution of dam break floods in a trapezoidal channel water depth above breach invert hw volume of water stored above breach invert vw embankment length el and average embankment width ew are used as independent variables to develop empirical formulas of estimating the peak outflow from breached embankment dams it is indicated from the multiple regression analysis that a function using the former two variables i e hw and vw produce considerably more accurate results than that using latter two variables i e el and ew it is shown that the semi analytical approach works best in terms of both prediction accuracy and uncertainty and the established empirical models produce considerably reasonable results except the model only using el moreover present models have been compared with other models available in literature for estimating peak discharge keywords embankment dam failure peak discharge empirical model semi analytical model 1 introduction the safety for modern dams has attracted more concern than early dams because the surrounding areas for modern dams were more densely populated and industrialized and these dams were generally larger than early dams the analysis of modern dam safety was initiated in the 1970s during 1972 and 1977 four notable dam failures occurred in the united states i e buffalo creek canyon lake teton and kelly barnes and two were in china i e banqiao reservoir and shimantan reservoir compared with other types of dams i e gravity dams buttress dams barrages and arch dams et al an embankment dam termed an earthfill or rockfill dam has attracted more attention because an embankment dam is the most common type of dam in use for example nearly 86 about 75 000 of more than 87 000 dams located in the united states and its territories are embankment dam usace 2013 another reason is that embankment dam has higher risk of failure for instance during 1954 2006 93 of the 3498 dam failures occurred in china are embankment dams xie and sun 2009 so it is essential for risk assessment to accurately and quickly estimate the peak outflows from a potentially breached dam since it is a vital basis for both hazard classification and emergency planning among the numerous models which are developed to estimate the peak outflows caused by dam failures empirical and semi theoretical models that were based on the case study data have attracted much attention e g kirkpatrick 1977 scs 1981 hagen 1982 bureau of reclamation 1982 macdonald and langridge monopolis 1984 scs 1985 costa 1985 evans 1986 froehlich 1995 webby 1996 walder and o connor 1997 pierce 2008 macchione 2008 macchione and rino 2008 xu and zhang 2009 pierce et al 2010 thornton et al 2011 gupta and singh 2012 hooshyaripor et al 2014 de lorenzo and macchione 2014 azimi et al 2015 and froehlich 2016 wang et al 2016 it is noted that these models are parametric and to usually use the data of historical dam failures to develop empirically linear curvilinear or multiple regression relationships relating the peak discharge to one or more parameters of dam and reservoir characteristics i e dam height hd water depth above breach invert at time of failure hw volume of water stored above breach invert at time of failure vw reservoir storage vs or the produce of vs hd or hw vw recently more features of dam i e embankment length el average embankment width ew have been involved into the prediction models in conjunction with hd and vs to determine the peak discharge i e thornton et al 2011 gupta and singh 2012 froehlich 2016 however these models have a common defect that the empirical relationships were derived from a limited case study database and show insufficient accuracy wahl 2004 reviewed the models of predicting peak outflows developed between 1977 and 1997 and concluded that the uncertainty bands were about 0 5 to 1 order of magnitude except that the relation by froehlich 1995 which had an uncertainty of 1 3 order of magnitude the uncertainty band used by wahl 2004 is defined as 2 se i e standard deviation that approximately represents a 95 confidence band it is noted that xu and zhang 2009 incorporated dam erodibility and failure mode in the prediction model in addition to hw and vw wahl 2014 evaluated the method by xu and zhang 2009 and showed that this method produced reasonable results of peak breach outflow rates for medium and high erodibility dams but it is not clear whether it is applicable for low erodibility dams due to insufficient data available the semi theoretical models of walder and o connor 1997 and macchione 2008 were developed based on the continuity equation applied to reservoir which describes the emptying of the reservoir due to the discharge outflowing through the breach a constant downcutting rate for the breach deepening over the entire development of the breach was simply assumed in the walder and o connor 1997 method and its value is very difficult to be determined froehlich 2017 this issue may be solved using the breach formation time formula proposed by froehlich 2008 differently to the constant downcutting rate used by walder and o connor 1997 in the model of macchione 2008 a time variable rate of breach opening depending on the eroding flow capacity was introduced to address the dam erosion based on the model of macchione 2008 macchione and rino 2008 proposed an analytical method for predicting the whole outflow hydrograph for overtopping failures in addition de lorenzo and macchione 2014 proposed some formulas for peak discharge both for overtopping and piping failures obtained by regression analysis of the numerical results provided by the model proposed by macchione 2008 the formula of froehlich 2016 is based on a semi theoretical approach that reduces the maximum possible peak discharge through an instantaneous partial breach of prescribed dimensions that forms in the shape of a trapezoid assuming the final geometry of the breach as a trapezoidal shape wang et al 2016 developed a semi analytical model of predicting floods peak discharge caused by embankment dam failures which was based on an analytical solution of dam break floods in a trapezoidal channel the method by wang et al 2016 has a high coefficient of determination and a small standard error among the considered 15 models however a coefficient determined by the data of historical dam failures in their model was taken as a constant due to the limit of the case study data i e only 27 dam breach cases were used in wang et al 2016 the objectives of present study are to 1 develop new empirical relationships based on regression analysis of the case study database and 2 propose a new method of determining the coefficient in the method by wang et al 2016 to improve the predictive capability by using a larger database of historical dam failures 2 database wahl 1998 presented a database containing 108 cases 43 of them contained data describing hw and vw or hd and vs as well as reported peak outflow through the dam breach qp macchione 2008 presented a database of 15 cases with earthfill dam failure providing data describing the main characteristics of the reservoir dam and breach i e maximum storage volume surface area of reservoir average value of upstream and downstream embankment slopes overall outflow volume from the breach water depth of reservoir before failure observed breach top widths and breach average widths and the observed peak discharges xu and zhang 2009 compiled a database composed of 75 cases with earth and rockfill dam failure 39 of them containing data describing hw vw and qp pierce et al 2010 compiled a database of 87 cases by combining the pierce 2008 s database 44 cases with the wahl 1998 s data 43 cases 38 of them reported embankment length el average embankment width ew or both of them gupta and singh 2012 employed a database of 35 dam break cases to develop an expression including hw vw el and ew for predicting peak outflow however the data used were not presented in that paper hooshyaripor and tahershamsi 2012 presented a dataset composed of 93 embankment failure cases which contains data involving hw vw and qp froehlich 2016 compiled a database containing 41 cases with hw vw and qp as well as 40 cases including el and ew in present study the dam breach case data involving all four variables i e hw vw el and ew and reported peak outflow qp have been used to develop an empirical models of predicting peak discharge the database used in this study is comprised of 40 dam failure cases all of them are from the work of froehlich 2016 probability distributions of various parameters involved in this database are presented in figs 1 6 these diagrams show that the distributions are right skewed and scattered especially in the cases of vw qp and el moreover the mass of the distributions are concentrated on the left vw reported here is in the range of 0 0133 701 million m3 vw with less than 50 million m3 are accounted for 85 qp ranges from 30 to 65 120 m3 s and 75 of them is less than 5000 m3 s el is in the range of 40 and 4100 m while for 30 of 40 breached embankments el is less than 500 m ew is in the range of 9 63 and 250 m and only three cases have ew with larger than 100 m two parameters hw and hb of measuring flow potential energy have similar distribution the number of cases with hw 40 and hb 40 are 39 and 38 respectively based on the findings of international commission on large dams 1974 the embankment failures were classified into two modes froehlich 2016 overtopping and internal erosion among the breached embankments showed in table 1 one half 18 cases failed due to overtopping and another half failed due to internal erosion 22 cases 3 peak discharge relations in this section nine mathematical models are proposed to predict peak discharge from gradually breached embankment dams the first eight models are developed with a series of regression analyses using embankment and reservoir properties as predictors the main difference among those models lies on the consideration of el and ew the last model is an enhanced semi analytical formula based on the previous work by wang et al 2016 3 1 empirical formula the peak discharge qp is commonly expected to be a function of vw and hw pierce et al 2010 performed a series of regression analyses to develop empirical expressions of predicting peak outflow from a breached embankment dam it is shown that the models based on a linear regression analysis i e q p f h w q p f v w and q p f v w h w have lower coefficient of determination r2 when compared to those developed by a multiple regression analysis i e q p f v w h w hence only the multiple regression analysis was carried out to produce the predictor for peak discharge in present study considering the outflow from a breach as the flow on a weir and using the technique of dimensional analysis webby 1996 suggested the functional relationship as follow 1 q p f g v w h w choosing vw and g as the repeating variables a dimensionless function can be obtained as 2 q p gv w 5 3 f h w v w 1 3 in order to use the regression technique eq 2 can be rewritten as 3 q p gv w 5 3 α v w h w 3 β where α and β are regression coefficients a multivariate regression analysis was performed using vw and hw as dependent variables of predicting qp a first order regression model was applied to the logarithmic transform of the each term of eq 3 and the best fit expression eq 4 can be developed as 4 q p gv w 5 3 0 0370 v w h w 3 0 4262 the predicted peak outflow by eq 4 is presented in fig 7 it can be seen that r2 for eq 4 is 0 9620 and the relative root mean square error rrmse of ln qp is 0 0555 two other parameters of dam i e embankment length el average embankment width ew should be considered in the functional relation of qp the reasons are that el should be used to account for the effects of a partial breach i e a breach does not extend across the entire width of an approach channel and ew is taken to account for the breach formation process i e a larger embankment width would certainly lead to a smaller peak discharge because the breach formation process will be slowed down these two parameters are seldom involved in other models of estimating peak outflow except those by thornton et al 2011 and gupta and singh 2012 a linear regression analysis was performed to determine whether el and ew could be used as a reliable predictor of estimating dam breach peak discharge or not a regression analysis was conducted on the logarithmic transformation of the data 40 cases and resulted in the relationships of eqs 5 and 6 5 q p 3 9031 e l 1 0727 6 q p 0 8041 e w 2 1128 the predictions of the peak outflow by eqs 5 and 6 are presented in figs 8 and 9 it can be seen that r2 for eqs 5 and 6 are 0 2982 and 0 6911 respectively while rrmse of ln qp are 0 2680 and 0 1620 respectively it can be seen that the peak outflow relation using el as the dependent variable yielded a poor correlation hence embankment length was not considered to be a significant peak outflow predictor when used as the only dependent variable although the peak outflow relation only using ew produces a higher r2 but it needs to be calibrated by more data thornton et al 2011 developed an empirical expression r2 0 909 of predicting peak outflow relating to el using 14 case studies and proposed the relations of qp and ew from 25 cases r2 0 29 a multivariate regression analysis was also performed using both el and ew as the dependent variables a first order regression model was applied to the logarithmic transform of the variables and a best fit expression of eq 7 can be obtained as 7 q p 0 1413 e l 0 4675 e w 1 8579 eq 7 yields an r2 of 0 7377 and a rrmse of 0 1491 the peak outflows calculated by using eq 7 are compared to the observed values in fig 10 the addition of el as a dependent variable improves slightly the prediction of the peak outflow over the using only ew it is not clear whether including el and ew in prediction model such as q p f v w h w can improve the prediction so a multivariate regression analysis has been used for models such as q p f v w h w e l q p f v w h w e w q p f v w h w e l e w in order to dimensionless analysis acceleration of gravity has been added in these models incorporating el and ew into eq 1 respectively the following relationships can be obtained 8 q p f g v w h w e l 9 q p f g v w h w e w the corresponding dimensionless functions can be shown as 10 q p gv w 5 3 f v w h w 3 e l h w 11 q p gv w 5 3 f v w h w 3 e w h w eqs 10 and 11 can be rewritten as 12 q p gv w 5 3 α v w h w 3 β e l h w γ 13 q p gv w 5 3 α v w h w 3 β e w h w γ where α β and γ are regression coefficients a multivariate regression analysis was performed using not only vw and hw but also the el or ew the resulted models of predicting the peak outflow as a function of vw hw and el and a function of vw hw and ew are they can be expressed as 14 q p gv w 5 3 0 0350 v w h w 3 0 4554 e l h w 0 0899 15 q p gv w 5 3 0 0370 v w h w 3 0 4264 e w h w 0 0028 predictions by eqs 14 and 15 are compared with observation in figs 11 and 12 respectively it can be seen that r2 for eqs 14 and 15 are 0 9633 and 0 9620 respectively and the rrmse of ln qp are 0 0549 and 0 0555 compared with eq 4 el and ew are added in eqs 14 and 15 but the prediction is not improved obviously besides the eqs 14 and 15 also show that the predicted discharge will become larger as el increases or ew reduces and this founding is the same as that by thornton et al 2011 however gupta and singh 2012 indicated that the predicted discharge will become larger as ew increases and this founding is obviously contradictory to the facts moreover it can be clear from eqs 14 and 15 that the exponential number of ew and el are relatively small compared with vw and hw so ew and el cannot play a leading role in predicting flood peak discharge incorporating both el and ew into eq 1 the following functional relationship can be obtained 16 q p f g v w h w e l e w so two dimensionless functions can be obtained 17 q p gv w 5 3 f v w h w 3 e l h w e w h w 18 q p gv w 5 3 f v w h w 3 e l e w eqs 17 and 18 can be rewritten as 19 q p gv w 5 3 α v w h w 3 β e l h w γ e w h w δ 20 q p gv w 5 3 α v w h w 3 β e l e w γ a multivariate regression analysis was also performed using all of the four parameters vw hw el and ew the resulted relationship of predicting the peak outflow as a function of vw hw and el and a function of vw hw and ew can be expressed as 21 q p gv w 5 3 0 0372 v w h w 3 0 4193 e l h w 0 0266 e w h w 0 0256 22 q p gv w 5 3 0 0367 v w h w 3 0 4533 e l e w 0 1041 the predictions by eqs 21 and 23 are shown in figs 13 and 14 it can be seen that r2 for eqs 14 and 15 are 0 9611 and 0 9635 respectively and the rrmse of ln qp are 0 0560 and 0 0551 it is indicated that the addition of both el and ew as dependent variables does not improve substantially the prediction of the peak outflow over the model using only hw and vw 3 2 semi analytical formula assuming that the final embankment breach forms in the shape of a trapezoid having left and right slope ratios of ml and mr as well as a bottom width b wang et al 2016 presented a semi analytical formula of predicting the peak discharge as following 23 q p ζ 2 2 gq 5 m l m r w 3 w 2 1 3 2 w 2 1 where ζ is a coefficient that can be determined by the data of historical dam failures q is a combinative parameter of the cross section w is a characteristic parameter of the flow depth more details on eq 23 can be found in the work of wang et al 2016 the breach dimensions i e average breach width and side slope ratio used in the solution procedure for eq 23 are estimated with the method by froehlich 2008 the coefficient ζ was specified as a constant ζ 0 66 in the study by wang et al 2016 but there are about 78 of cases are overtopping failures so the coefficient ζ should be calibrated as follows 24 ξ 0 82 for overtopping failures 0 78 for non overtopping failures eq 23 has r2 of 0 9584 and the rrsme of ln qp is 0 0621 the predicted peak outflows by eq 23 are plotted against the observed values in fig 15 all in all the predictions by eqs 4 7 14 15 21 23 are compared with the observed peak outflows in fig 16 it is shown that eq 5 only with el for estimating the peak discharge does not predict reasonable results and eq 7 where both el and ew are used as the dependent variables works better besides prediction by eqs 4 14 15 21 22 are close each other which demonstrates again that the addition of both el and ew as dependent variables cannot improve essentially the prediction of peak outflow over the models using only hw and vw moreover eq 23 provides predictions which are close to the observed values so its performance is best among proposed models 4 comparison with other peak discharge formulas the present models were compared with other models developed for predicting peak discharge from breached embankment dams with the assembled data table 2 summarizes the methods presented in this paper as well as the models by kirkpatrick 1977 bureau of reclamation 1982 macdonald and langridge monopolis 1984 costa 1985 scs 1985 evans 1986 froehlich 1995 webby 1996 walder and o connor 1997 pierce et al 2010 thornton et al 2011 gupta and singh 2012 hooshyaripor et al 2014 de lorenzo and macchione 2014 azimi et al 2015 and froehlich 2016 and presents the values of r2 and rrmse of ln qp predicted for the failures the formula given by webby 1996 eq 2 of froehlich 2016 and eqs 4 14 15 21 23 have a comparatively larger coefficient of determination r2 0 95 and a lower relative root mean square error i e the rrmse is about 0 06 when compared to other models the uncertainty of each model considered here was also evaluated using the quantitative analysis method proposed by wahl 2004 wahl 2004 calculated the errors in terms of the number of log cycles separating the predicted value qpp and observed value qpo ei log10 qpp log10 qpo log10 qpp qpo and considered their mean em and the standard deviation se the results of this analysis are also reported in table 2 it is expected that eqs 4 7 14 15 21 22 yield an em of zero because of the development of these formulas based on the least squares method for the same reason it is expected that the equations here proposed and the froehlich 2016 formula are advantaged since they use the same database both for calibration and for performance evaluation in addition they have a comparatively small uncertainty band except eqs 5 7 among the remainder methods eq 23 has a minimum value of em 0 0080 and a bandwidth of 2se 0 3338 a box and whisker diagram is used to illustrate the performance of present models and other models available from literature as shown in fig 17 it is convenient to graphically depict the prediction errors through five values the upper and lower extremes not including outliers the upper and lower quartiles i e q 1 and q 3 and the median i e q 2 outliers are defined as the data that fall below q 1 1 5 iqr or above q 3 1 5 iqr where iqr q 3 q 1 interquartile range iqr the degree of dispersion spread and skew asymmetry of the prediction errors is illustrated through the spacing between different parts of the boxes the following conclusions can be obtained from the fig 17 predictions by macdonald and langridge monopolis 1984 eq 1 costa 1985 gupta and singh 2012 eq 1 de lorenzo and macchione 2014 froehlich 2016 eqs 1 and eqs 4 5 7 14 15 4 and 5 have a median with a deviation of 10 around one eq 23 works best with a consistent accuracy the iqr extends from 0 8181 to 1 2642 and a median of 0 9845 as well as little skew it is followed by eqs 4 14 15 21 22 predictions by eqs 4 14 15 21 22 provide good results and a similar feature is found that the median is about 0 92 0 95 and the iqr extends from approximate 0 72 to 1 39 with a little skew a single outlier is generated by each of eqs 4 14 15 21 23 all of the other models produce results that are lower in accuracy than those just mentioned notably poor predictions are given by models of the macdonald and langridge monopolis 1984 scs 1985 and thornton et al 2011 predictions by eq 7 have a little skew with a slightly large iqr and the obvious improvement of eq 7 over eqs 5 and 6 is expected eq 1 of froehlich 2016 with a slightly smaller iqr extending from 0 4754 to 1 3524 and a median of 0 9179 works better than eq 2 of froehlich 2016 besides the models developed by kirkpatrick 1977 gupta and singh 2012 hooshyaripor et al 2014 pierce et al 2010 distinctly underestimate peak discharge especially the method proposed by hooshyaripor et al 2014 underestimates peak discharge by about 50 and produces a comparatively large rrmse although it has the smallest iqr extending from 0 4019 to 0 5676 of all models among the three semi analytical models considered in this study the one developed by de lorenzo and macchione 2014 works best although only 30 of 40 embankment failures were used due to the applicability of the formula itself table 3 shows the dam failures which were excluded and the reasons for their exclusion the method given by walder and o connor 1997 overestimates peak discharge and has a comparatively larger interquartile range the last one proposed by froehlich 2016 produce better results 5 conclusions based on a database of 40 historical dam failures two kinds of nonlinear mathematical models were developed to predict the peak discharge of floods for breached embankment dams the first kind of models are purely empirical making use of a series of regression analyses the second is semi analytical which was an enhanced version of a previous prediction by wang et al 2016 the coefficient in wang et al 2016 for the semi analytical method was specified as a constant however it is determined on the base of the failure modes in present study four independent variables are used to develop empirical formulas of estimating the peak outflow from breached embankment dams and they include water depth above breach invert hw volume of water stored above breach invert vw embankment length el and average embankment width ew moreover the effects of el and ew on peak discharge are discussed it is indicated from the multiple regression analysis that the models including hw and vw produce considerably more accurate predictions of peak discharge than those including el and ew furthermore the addition of both el and ew as dependent variables does not improve substantially the prediction of the peak outflow over the models using only hw and vw it is shown that the semi analytical approach produces best results among all models proposed in this paper in terms of both prediction accuracy and uncertainty the proposed empirical models produce considerably reasonable results except the model using only el the application of the semi analytical model for predicting peak discharge proposed here is more complicated when compared to the empirical models however it can be applied widely because it is derived theoretically and has clear physical meaning the present models have been compared with other available models in literature of course the results of this comparison might be different if independent databases for regression and comparison were used acknowledgements the first author would like to thank for the financial support of national natural science foundation of china grant no 51209155 the national key research and development program of china grant no 2016yfc0401705 and youth science and technology innovation team plan funding of sichuan grant no 2016td0020 the fourth author would like to thank national natural science foundation of china grant nos 51409183 51579166 and 51611130203 for the financial support 
7249,prediction of peak discharge of floods has attracted great attention for researchers and engineers in present study nine typical nonlinear mathematical models are established based on database of 40 historical dam failures the first eight models that were developed with a series of regression analyses are purely empirical while the last one is a semi analytical approach that was derived from an analytical solution of dam break floods in a trapezoidal channel water depth above breach invert hw volume of water stored above breach invert vw embankment length el and average embankment width ew are used as independent variables to develop empirical formulas of estimating the peak outflow from breached embankment dams it is indicated from the multiple regression analysis that a function using the former two variables i e hw and vw produce considerably more accurate results than that using latter two variables i e el and ew it is shown that the semi analytical approach works best in terms of both prediction accuracy and uncertainty and the established empirical models produce considerably reasonable results except the model only using el moreover present models have been compared with other models available in literature for estimating peak discharge keywords embankment dam failure peak discharge empirical model semi analytical model 1 introduction the safety for modern dams has attracted more concern than early dams because the surrounding areas for modern dams were more densely populated and industrialized and these dams were generally larger than early dams the analysis of modern dam safety was initiated in the 1970s during 1972 and 1977 four notable dam failures occurred in the united states i e buffalo creek canyon lake teton and kelly barnes and two were in china i e banqiao reservoir and shimantan reservoir compared with other types of dams i e gravity dams buttress dams barrages and arch dams et al an embankment dam termed an earthfill or rockfill dam has attracted more attention because an embankment dam is the most common type of dam in use for example nearly 86 about 75 000 of more than 87 000 dams located in the united states and its territories are embankment dam usace 2013 another reason is that embankment dam has higher risk of failure for instance during 1954 2006 93 of the 3498 dam failures occurred in china are embankment dams xie and sun 2009 so it is essential for risk assessment to accurately and quickly estimate the peak outflows from a potentially breached dam since it is a vital basis for both hazard classification and emergency planning among the numerous models which are developed to estimate the peak outflows caused by dam failures empirical and semi theoretical models that were based on the case study data have attracted much attention e g kirkpatrick 1977 scs 1981 hagen 1982 bureau of reclamation 1982 macdonald and langridge monopolis 1984 scs 1985 costa 1985 evans 1986 froehlich 1995 webby 1996 walder and o connor 1997 pierce 2008 macchione 2008 macchione and rino 2008 xu and zhang 2009 pierce et al 2010 thornton et al 2011 gupta and singh 2012 hooshyaripor et al 2014 de lorenzo and macchione 2014 azimi et al 2015 and froehlich 2016 wang et al 2016 it is noted that these models are parametric and to usually use the data of historical dam failures to develop empirically linear curvilinear or multiple regression relationships relating the peak discharge to one or more parameters of dam and reservoir characteristics i e dam height hd water depth above breach invert at time of failure hw volume of water stored above breach invert at time of failure vw reservoir storage vs or the produce of vs hd or hw vw recently more features of dam i e embankment length el average embankment width ew have been involved into the prediction models in conjunction with hd and vs to determine the peak discharge i e thornton et al 2011 gupta and singh 2012 froehlich 2016 however these models have a common defect that the empirical relationships were derived from a limited case study database and show insufficient accuracy wahl 2004 reviewed the models of predicting peak outflows developed between 1977 and 1997 and concluded that the uncertainty bands were about 0 5 to 1 order of magnitude except that the relation by froehlich 1995 which had an uncertainty of 1 3 order of magnitude the uncertainty band used by wahl 2004 is defined as 2 se i e standard deviation that approximately represents a 95 confidence band it is noted that xu and zhang 2009 incorporated dam erodibility and failure mode in the prediction model in addition to hw and vw wahl 2014 evaluated the method by xu and zhang 2009 and showed that this method produced reasonable results of peak breach outflow rates for medium and high erodibility dams but it is not clear whether it is applicable for low erodibility dams due to insufficient data available the semi theoretical models of walder and o connor 1997 and macchione 2008 were developed based on the continuity equation applied to reservoir which describes the emptying of the reservoir due to the discharge outflowing through the breach a constant downcutting rate for the breach deepening over the entire development of the breach was simply assumed in the walder and o connor 1997 method and its value is very difficult to be determined froehlich 2017 this issue may be solved using the breach formation time formula proposed by froehlich 2008 differently to the constant downcutting rate used by walder and o connor 1997 in the model of macchione 2008 a time variable rate of breach opening depending on the eroding flow capacity was introduced to address the dam erosion based on the model of macchione 2008 macchione and rino 2008 proposed an analytical method for predicting the whole outflow hydrograph for overtopping failures in addition de lorenzo and macchione 2014 proposed some formulas for peak discharge both for overtopping and piping failures obtained by regression analysis of the numerical results provided by the model proposed by macchione 2008 the formula of froehlich 2016 is based on a semi theoretical approach that reduces the maximum possible peak discharge through an instantaneous partial breach of prescribed dimensions that forms in the shape of a trapezoid assuming the final geometry of the breach as a trapezoidal shape wang et al 2016 developed a semi analytical model of predicting floods peak discharge caused by embankment dam failures which was based on an analytical solution of dam break floods in a trapezoidal channel the method by wang et al 2016 has a high coefficient of determination and a small standard error among the considered 15 models however a coefficient determined by the data of historical dam failures in their model was taken as a constant due to the limit of the case study data i e only 27 dam breach cases were used in wang et al 2016 the objectives of present study are to 1 develop new empirical relationships based on regression analysis of the case study database and 2 propose a new method of determining the coefficient in the method by wang et al 2016 to improve the predictive capability by using a larger database of historical dam failures 2 database wahl 1998 presented a database containing 108 cases 43 of them contained data describing hw and vw or hd and vs as well as reported peak outflow through the dam breach qp macchione 2008 presented a database of 15 cases with earthfill dam failure providing data describing the main characteristics of the reservoir dam and breach i e maximum storage volume surface area of reservoir average value of upstream and downstream embankment slopes overall outflow volume from the breach water depth of reservoir before failure observed breach top widths and breach average widths and the observed peak discharges xu and zhang 2009 compiled a database composed of 75 cases with earth and rockfill dam failure 39 of them containing data describing hw vw and qp pierce et al 2010 compiled a database of 87 cases by combining the pierce 2008 s database 44 cases with the wahl 1998 s data 43 cases 38 of them reported embankment length el average embankment width ew or both of them gupta and singh 2012 employed a database of 35 dam break cases to develop an expression including hw vw el and ew for predicting peak outflow however the data used were not presented in that paper hooshyaripor and tahershamsi 2012 presented a dataset composed of 93 embankment failure cases which contains data involving hw vw and qp froehlich 2016 compiled a database containing 41 cases with hw vw and qp as well as 40 cases including el and ew in present study the dam breach case data involving all four variables i e hw vw el and ew and reported peak outflow qp have been used to develop an empirical models of predicting peak discharge the database used in this study is comprised of 40 dam failure cases all of them are from the work of froehlich 2016 probability distributions of various parameters involved in this database are presented in figs 1 6 these diagrams show that the distributions are right skewed and scattered especially in the cases of vw qp and el moreover the mass of the distributions are concentrated on the left vw reported here is in the range of 0 0133 701 million m3 vw with less than 50 million m3 are accounted for 85 qp ranges from 30 to 65 120 m3 s and 75 of them is less than 5000 m3 s el is in the range of 40 and 4100 m while for 30 of 40 breached embankments el is less than 500 m ew is in the range of 9 63 and 250 m and only three cases have ew with larger than 100 m two parameters hw and hb of measuring flow potential energy have similar distribution the number of cases with hw 40 and hb 40 are 39 and 38 respectively based on the findings of international commission on large dams 1974 the embankment failures were classified into two modes froehlich 2016 overtopping and internal erosion among the breached embankments showed in table 1 one half 18 cases failed due to overtopping and another half failed due to internal erosion 22 cases 3 peak discharge relations in this section nine mathematical models are proposed to predict peak discharge from gradually breached embankment dams the first eight models are developed with a series of regression analyses using embankment and reservoir properties as predictors the main difference among those models lies on the consideration of el and ew the last model is an enhanced semi analytical formula based on the previous work by wang et al 2016 3 1 empirical formula the peak discharge qp is commonly expected to be a function of vw and hw pierce et al 2010 performed a series of regression analyses to develop empirical expressions of predicting peak outflow from a breached embankment dam it is shown that the models based on a linear regression analysis i e q p f h w q p f v w and q p f v w h w have lower coefficient of determination r2 when compared to those developed by a multiple regression analysis i e q p f v w h w hence only the multiple regression analysis was carried out to produce the predictor for peak discharge in present study considering the outflow from a breach as the flow on a weir and using the technique of dimensional analysis webby 1996 suggested the functional relationship as follow 1 q p f g v w h w choosing vw and g as the repeating variables a dimensionless function can be obtained as 2 q p gv w 5 3 f h w v w 1 3 in order to use the regression technique eq 2 can be rewritten as 3 q p gv w 5 3 α v w h w 3 β where α and β are regression coefficients a multivariate regression analysis was performed using vw and hw as dependent variables of predicting qp a first order regression model was applied to the logarithmic transform of the each term of eq 3 and the best fit expression eq 4 can be developed as 4 q p gv w 5 3 0 0370 v w h w 3 0 4262 the predicted peak outflow by eq 4 is presented in fig 7 it can be seen that r2 for eq 4 is 0 9620 and the relative root mean square error rrmse of ln qp is 0 0555 two other parameters of dam i e embankment length el average embankment width ew should be considered in the functional relation of qp the reasons are that el should be used to account for the effects of a partial breach i e a breach does not extend across the entire width of an approach channel and ew is taken to account for the breach formation process i e a larger embankment width would certainly lead to a smaller peak discharge because the breach formation process will be slowed down these two parameters are seldom involved in other models of estimating peak outflow except those by thornton et al 2011 and gupta and singh 2012 a linear regression analysis was performed to determine whether el and ew could be used as a reliable predictor of estimating dam breach peak discharge or not a regression analysis was conducted on the logarithmic transformation of the data 40 cases and resulted in the relationships of eqs 5 and 6 5 q p 3 9031 e l 1 0727 6 q p 0 8041 e w 2 1128 the predictions of the peak outflow by eqs 5 and 6 are presented in figs 8 and 9 it can be seen that r2 for eqs 5 and 6 are 0 2982 and 0 6911 respectively while rrmse of ln qp are 0 2680 and 0 1620 respectively it can be seen that the peak outflow relation using el as the dependent variable yielded a poor correlation hence embankment length was not considered to be a significant peak outflow predictor when used as the only dependent variable although the peak outflow relation only using ew produces a higher r2 but it needs to be calibrated by more data thornton et al 2011 developed an empirical expression r2 0 909 of predicting peak outflow relating to el using 14 case studies and proposed the relations of qp and ew from 25 cases r2 0 29 a multivariate regression analysis was also performed using both el and ew as the dependent variables a first order regression model was applied to the logarithmic transform of the variables and a best fit expression of eq 7 can be obtained as 7 q p 0 1413 e l 0 4675 e w 1 8579 eq 7 yields an r2 of 0 7377 and a rrmse of 0 1491 the peak outflows calculated by using eq 7 are compared to the observed values in fig 10 the addition of el as a dependent variable improves slightly the prediction of the peak outflow over the using only ew it is not clear whether including el and ew in prediction model such as q p f v w h w can improve the prediction so a multivariate regression analysis has been used for models such as q p f v w h w e l q p f v w h w e w q p f v w h w e l e w in order to dimensionless analysis acceleration of gravity has been added in these models incorporating el and ew into eq 1 respectively the following relationships can be obtained 8 q p f g v w h w e l 9 q p f g v w h w e w the corresponding dimensionless functions can be shown as 10 q p gv w 5 3 f v w h w 3 e l h w 11 q p gv w 5 3 f v w h w 3 e w h w eqs 10 and 11 can be rewritten as 12 q p gv w 5 3 α v w h w 3 β e l h w γ 13 q p gv w 5 3 α v w h w 3 β e w h w γ where α β and γ are regression coefficients a multivariate regression analysis was performed using not only vw and hw but also the el or ew the resulted models of predicting the peak outflow as a function of vw hw and el and a function of vw hw and ew are they can be expressed as 14 q p gv w 5 3 0 0350 v w h w 3 0 4554 e l h w 0 0899 15 q p gv w 5 3 0 0370 v w h w 3 0 4264 e w h w 0 0028 predictions by eqs 14 and 15 are compared with observation in figs 11 and 12 respectively it can be seen that r2 for eqs 14 and 15 are 0 9633 and 0 9620 respectively and the rrmse of ln qp are 0 0549 and 0 0555 compared with eq 4 el and ew are added in eqs 14 and 15 but the prediction is not improved obviously besides the eqs 14 and 15 also show that the predicted discharge will become larger as el increases or ew reduces and this founding is the same as that by thornton et al 2011 however gupta and singh 2012 indicated that the predicted discharge will become larger as ew increases and this founding is obviously contradictory to the facts moreover it can be clear from eqs 14 and 15 that the exponential number of ew and el are relatively small compared with vw and hw so ew and el cannot play a leading role in predicting flood peak discharge incorporating both el and ew into eq 1 the following functional relationship can be obtained 16 q p f g v w h w e l e w so two dimensionless functions can be obtained 17 q p gv w 5 3 f v w h w 3 e l h w e w h w 18 q p gv w 5 3 f v w h w 3 e l e w eqs 17 and 18 can be rewritten as 19 q p gv w 5 3 α v w h w 3 β e l h w γ e w h w δ 20 q p gv w 5 3 α v w h w 3 β e l e w γ a multivariate regression analysis was also performed using all of the four parameters vw hw el and ew the resulted relationship of predicting the peak outflow as a function of vw hw and el and a function of vw hw and ew can be expressed as 21 q p gv w 5 3 0 0372 v w h w 3 0 4193 e l h w 0 0266 e w h w 0 0256 22 q p gv w 5 3 0 0367 v w h w 3 0 4533 e l e w 0 1041 the predictions by eqs 21 and 23 are shown in figs 13 and 14 it can be seen that r2 for eqs 14 and 15 are 0 9611 and 0 9635 respectively and the rrmse of ln qp are 0 0560 and 0 0551 it is indicated that the addition of both el and ew as dependent variables does not improve substantially the prediction of the peak outflow over the model using only hw and vw 3 2 semi analytical formula assuming that the final embankment breach forms in the shape of a trapezoid having left and right slope ratios of ml and mr as well as a bottom width b wang et al 2016 presented a semi analytical formula of predicting the peak discharge as following 23 q p ζ 2 2 gq 5 m l m r w 3 w 2 1 3 2 w 2 1 where ζ is a coefficient that can be determined by the data of historical dam failures q is a combinative parameter of the cross section w is a characteristic parameter of the flow depth more details on eq 23 can be found in the work of wang et al 2016 the breach dimensions i e average breach width and side slope ratio used in the solution procedure for eq 23 are estimated with the method by froehlich 2008 the coefficient ζ was specified as a constant ζ 0 66 in the study by wang et al 2016 but there are about 78 of cases are overtopping failures so the coefficient ζ should be calibrated as follows 24 ξ 0 82 for overtopping failures 0 78 for non overtopping failures eq 23 has r2 of 0 9584 and the rrsme of ln qp is 0 0621 the predicted peak outflows by eq 23 are plotted against the observed values in fig 15 all in all the predictions by eqs 4 7 14 15 21 23 are compared with the observed peak outflows in fig 16 it is shown that eq 5 only with el for estimating the peak discharge does not predict reasonable results and eq 7 where both el and ew are used as the dependent variables works better besides prediction by eqs 4 14 15 21 22 are close each other which demonstrates again that the addition of both el and ew as dependent variables cannot improve essentially the prediction of peak outflow over the models using only hw and vw moreover eq 23 provides predictions which are close to the observed values so its performance is best among proposed models 4 comparison with other peak discharge formulas the present models were compared with other models developed for predicting peak discharge from breached embankment dams with the assembled data table 2 summarizes the methods presented in this paper as well as the models by kirkpatrick 1977 bureau of reclamation 1982 macdonald and langridge monopolis 1984 costa 1985 scs 1985 evans 1986 froehlich 1995 webby 1996 walder and o connor 1997 pierce et al 2010 thornton et al 2011 gupta and singh 2012 hooshyaripor et al 2014 de lorenzo and macchione 2014 azimi et al 2015 and froehlich 2016 and presents the values of r2 and rrmse of ln qp predicted for the failures the formula given by webby 1996 eq 2 of froehlich 2016 and eqs 4 14 15 21 23 have a comparatively larger coefficient of determination r2 0 95 and a lower relative root mean square error i e the rrmse is about 0 06 when compared to other models the uncertainty of each model considered here was also evaluated using the quantitative analysis method proposed by wahl 2004 wahl 2004 calculated the errors in terms of the number of log cycles separating the predicted value qpp and observed value qpo ei log10 qpp log10 qpo log10 qpp qpo and considered their mean em and the standard deviation se the results of this analysis are also reported in table 2 it is expected that eqs 4 7 14 15 21 22 yield an em of zero because of the development of these formulas based on the least squares method for the same reason it is expected that the equations here proposed and the froehlich 2016 formula are advantaged since they use the same database both for calibration and for performance evaluation in addition they have a comparatively small uncertainty band except eqs 5 7 among the remainder methods eq 23 has a minimum value of em 0 0080 and a bandwidth of 2se 0 3338 a box and whisker diagram is used to illustrate the performance of present models and other models available from literature as shown in fig 17 it is convenient to graphically depict the prediction errors through five values the upper and lower extremes not including outliers the upper and lower quartiles i e q 1 and q 3 and the median i e q 2 outliers are defined as the data that fall below q 1 1 5 iqr or above q 3 1 5 iqr where iqr q 3 q 1 interquartile range iqr the degree of dispersion spread and skew asymmetry of the prediction errors is illustrated through the spacing between different parts of the boxes the following conclusions can be obtained from the fig 17 predictions by macdonald and langridge monopolis 1984 eq 1 costa 1985 gupta and singh 2012 eq 1 de lorenzo and macchione 2014 froehlich 2016 eqs 1 and eqs 4 5 7 14 15 4 and 5 have a median with a deviation of 10 around one eq 23 works best with a consistent accuracy the iqr extends from 0 8181 to 1 2642 and a median of 0 9845 as well as little skew it is followed by eqs 4 14 15 21 22 predictions by eqs 4 14 15 21 22 provide good results and a similar feature is found that the median is about 0 92 0 95 and the iqr extends from approximate 0 72 to 1 39 with a little skew a single outlier is generated by each of eqs 4 14 15 21 23 all of the other models produce results that are lower in accuracy than those just mentioned notably poor predictions are given by models of the macdonald and langridge monopolis 1984 scs 1985 and thornton et al 2011 predictions by eq 7 have a little skew with a slightly large iqr and the obvious improvement of eq 7 over eqs 5 and 6 is expected eq 1 of froehlich 2016 with a slightly smaller iqr extending from 0 4754 to 1 3524 and a median of 0 9179 works better than eq 2 of froehlich 2016 besides the models developed by kirkpatrick 1977 gupta and singh 2012 hooshyaripor et al 2014 pierce et al 2010 distinctly underestimate peak discharge especially the method proposed by hooshyaripor et al 2014 underestimates peak discharge by about 50 and produces a comparatively large rrmse although it has the smallest iqr extending from 0 4019 to 0 5676 of all models among the three semi analytical models considered in this study the one developed by de lorenzo and macchione 2014 works best although only 30 of 40 embankment failures were used due to the applicability of the formula itself table 3 shows the dam failures which were excluded and the reasons for their exclusion the method given by walder and o connor 1997 overestimates peak discharge and has a comparatively larger interquartile range the last one proposed by froehlich 2016 produce better results 5 conclusions based on a database of 40 historical dam failures two kinds of nonlinear mathematical models were developed to predict the peak discharge of floods for breached embankment dams the first kind of models are purely empirical making use of a series of regression analyses the second is semi analytical which was an enhanced version of a previous prediction by wang et al 2016 the coefficient in wang et al 2016 for the semi analytical method was specified as a constant however it is determined on the base of the failure modes in present study four independent variables are used to develop empirical formulas of estimating the peak outflow from breached embankment dams and they include water depth above breach invert hw volume of water stored above breach invert vw embankment length el and average embankment width ew moreover the effects of el and ew on peak discharge are discussed it is indicated from the multiple regression analysis that the models including hw and vw produce considerably more accurate predictions of peak discharge than those including el and ew furthermore the addition of both el and ew as dependent variables does not improve substantially the prediction of the peak outflow over the models using only hw and vw it is shown that the semi analytical approach produces best results among all models proposed in this paper in terms of both prediction accuracy and uncertainty the proposed empirical models produce considerably reasonable results except the model using only el the application of the semi analytical model for predicting peak discharge proposed here is more complicated when compared to the empirical models however it can be applied widely because it is derived theoretically and has clear physical meaning the present models have been compared with other available models in literature of course the results of this comparison might be different if independent databases for regression and comparison were used acknowledgements the first author would like to thank for the financial support of national natural science foundation of china grant no 51209155 the national key research and development program of china grant no 2016yfc0401705 and youth science and technology innovation team plan funding of sichuan grant no 2016td0020 the fourth author would like to thank national natural science foundation of china grant nos 51409183 51579166 and 51611130203 for the financial support 
