index,text
26020,the great barrier reef is a unesco world heritage area that has been assessed as having a very poor outlook and needing urgent intervention the ereefs hydrodynamic biogeochemical models are used to complement monitoring facilitate data interpretation and support policy decisions management and policy for the great barrier reef are politically contentious so a high standard of model evaluation is required we report the application of a recently developed four level csps concept state process system model evaluation framework to the ereefs biogeochemical model the framework considers level 0 conceptual evaluation level 1 simulated state variables level 2 process rates and level 3 system level patterns and emergent properties this paper is the first complete application of this model assessment framework it highlights both strengths and weaknesses of the ereefs biogeochemical model that might not have been apparent from a traditional model evaluation the framework can be applied to evaluation of any complex process based numerical model keywords model evaluation assessment ecosystem model hydrodynamic model emergent properties great barrier reef 1 introduction the great barrier reef gbr has been identified by unesco as a world heritage area of outstanding universal value lucas et al 1997 it is one of the most biodiverse regions in the world has enormous cultural value and has been assessed as adding at least au 5 7b in uncorrected 2012 australian dollars and 64 000 jobs to australia s economy deloitte access economics 2013 the great barrier reef experiences diverse pressures that threaten these values these include changes in water quality attributed to agricultural clearing bainbridge et al 2018 kroon et al 2016 recurrent outbreaks of coral eating crown of thorns starfish which may also be due to water quality changes brodie et al 2005 structural damage due to tropical cyclones and other storms massel and done 1993 and devastating coral bleaching events associated with climate change and marine heatwaves in 2016 2017 coral bleaching events caused the death of a substantial proportion estimates vary from 10 to 50 of coral on the great barrier reef babcock et al 2019 hughes et al 2018 kennedy et al 2018 lewis and mallela 2018 stuart smith et al 2018 this was part of the wider global coral bleaching event that occurred between 2014 and 2017 eakin et al 2019 in light of these threats the great barrier reef marine park authority has recently assessed the outlook for the reef as very poor great barrier reef marine park authority 2019 policy and intervention options to preserve and restore the great barrier reef and improve its resilience to ongoing climate change are now being urgently explored the need for marine models to support policy and management decisions for the great barrier reef motivated the ereefs project a collaboration between csiro the australian institute of marine science the bureau of meteorology the government of queensland and the great barrier reef foundation chen et al 2011 schiller et al 2015 the ereefs project developed a tailored suite of nested hydrodynamic sediment dynamic and biogeochemical models that simulate physical conditions and water quality in three dimensions in near real time throughout the great barrier reef world heritage area and beyond baird et al 2016a 2016b 2018 herzfeld et al 2016 jones et al 2016 robson et al 2013b 2018 the ereefs models have been applied to investigations of the effects of ocean acidification on the great barrier reef mongin and baird 2014 mongin et al 2016b the effects of rivers on marine water quality margvelashvili et al 2018 robson et al 2014 wolff et al 2018 the physical oceanography that drives the exposure of reefs to hot water schiller et al 2015 and the interactions between physical conditions and biological mechanisms that lead to coral bleaching baird et al 2018 the models are now being used to test the likely effects of interventions e g albright et al 2016 mongin et al 2016a and to help set new more ecologically relevant targets for water quality and land runoff brodie et al 2017 monitoring and adaptive management are not straightforward in the region because of the size of the great barrier reef 300 000 km2 its remoteness from major population centres and the large inter annual variability associated with the tropical climate of the region including cyclone events wolff et al 2016 and effects of fluctuations in the el nino southern oscillation index leonard et al 2016 the health and condition of the system is therefore monitored through a combination of quarterly water quality surveys mostly in the inshore region waterhouse et al 2018a regular annual and alternate year ecological surveys of reef thompson et al 2018 and seagrass mckenzie et al 2018 habitats in situ logging of the physical properties fluorescence and turbidity of water at a few sites lynch et al 2014 and opportunistic sampling of major flood plume events e g cherukuru et al 2017 howley et al 2018 this in situ sampling regime is supplemented by satellite observation brando et al 2015a 2015b devlin et al 2015 and more recently by the ereefs marine models to provide synoptic near real time reporting of hydrodynamic and biochemical conditions on the gbr steven et al 2019 in this context of policy urgency data sparsity and politically contentious management and regulation it is important to understand the performance of the models so that they can be used with confidence where we lack direct observational data yet with due recognition of their limitations a traditional evaluation of the ereefs biogeochemical model against routine marine monitoring observations following the principles laid out by bennett et al 2013 has been presented by herzfeld et al 2016 with respect to water quality and physical conditions and skerratt et al 2019 with respect to plankton these evaluations directly compare observed water quality at marine monitoring program mmp and integrated marine observing system imos monitoring sites using time series comparisons over several years 2011 2016 along with measures of bias correlation error and skill willmott et al 1985 for each observed water quality and plankton variable at each monitoring site results generally indicate strong model performance although this evaluation should be treated with caution for several reasons a monitoring sites are predominantly shallow inshore sampling and thus represent only a small fraction of the area modelled b the water quality variables and sites used in the evaluation were the same as those used for model calibration although evaluation extended beyond the calibration period and to more variables than used in the calibration i e secchi depth c many variables included in the model are not directly observed these unobserved variables include for instance intracellular concentrations of carbon nitrogen phosphorus and chlorophyll within phytoplankton cells concentrations of labile and refractory particulate organic material and concentrations of small zooplankton recently hipsey et al 2020 proposed a new four level framework hereafter referred to as the concept state process system or csps framework for evaluation of aquatic ecosystem models in brief it describes level 0 concept the sanity check does the model produce plausible behaviour and reproduce key expected system properties level 1 state traditional evaluation achieved by comparing values of model state variables e g nutrient and chlorophyll concentrations with observed values of these state variables at matched times and locations this evaluation should ideally include measures of error bias and correlation for key state variables with weighting functions that consider how the models will be used and therefore which aspects of state variable responses are most important bennett et al 2013 level 2 process evaluation of process rates and ecosystem function for example nutrient flux rates and system metabolism while maximum process rates are typically represented in biogeochemical models as assigned parameter values bainbridge et al 2018 actual process rates arise from interactions among state variables and environmental conditions and these are rarely validated against observational data hipsey et al 2020 level 3 system evaluation of system scale emergent properties that is properties and relationships that emerge from complex system dynamics in a way that is not obvious from simple inspection of the equations underlying the model the behaviour of flocks of birds in flight has often been presented as a classic example of an emergent property of simple rules governing the individual behaviour of each animal phelan 1999 when a complex process based model is able to reproduce emergent properties without additional calibration after level 1 state validation it gives confidence that the model is getting the right results for the right reasons robson et al 2017 here we demonstrate for the first time an application of the csps framework to evaluate the ereefs marine biogeochemical model we show how this approach can be used to highlight the weaknesses as well as the strengths of the model which is important to build trust and encourage appropriate use we also synthesise a body of previously published and in press work that describes the development of the ereefs marine models their calibration and evaluation and the application of these models to enhance understanding of drivers of water quality in the great barrier reef world heritage area in addition we show several new evaluations that were necessary to complete the full csps model assessment and provide a narrative of the modelling and model delivery process 2 methods 2 1 models the ereefs marine models are a complex system of three dimensional hydrodynamic biogeochemical and sediment dynamics models driven by hydrological monitoring in 22 rivers feeding into the great barrier reef near real time meteorological models and global ocean models that provide the outer boundary conditions the ereefs marine models are an implementation of the csiro environmental modelling suite ems which has previously been applied on a smaller scale in a range of coastal and estuary contexts including on the great barrier reef the fitzroy estuary and keppel bay margvelashvili et al 2003 2013 robson et al 2006 webster et al 2003 2005 2006 the initial vision for the ereefs project was described by chen et al 2011 the full system of coupled biogeochemical hydrodynamic sediment dynamic models developed in subsequent years the final models and their implementation have been described in detail by herzfeld et al 2016 and baird et al in review and the information architecture supporting the model and other data products has been described by car 2013 and yu et al 2016 steven et al 2019 provide an overview of overall operational architecture of the modelling platform and describe several of the ways in which they have been applied the ereefs biogeochemical model includes several components that were developed specifically for application to the great barrier reef herzfeld and gillibrand 2015 describe a new open boundary forcing scheme used to facilitate two way nesting of regional submodels within the broader great barrier reef domain baird et al 2016a describe the development and application of the seagrass submodel which considers above and below ground biomass for three seagrass functional groups and their responses to spectrally resolved light conditions the optical submodel used to simulate these conditions is described by baird et al 2016b who also demonstrates how a spectrally resolved optical model can be used to generate simulated true colour images of the ocean as from a satellite and improve comparison with satellite remote sensing observations margvelashvili et al 2016 describe the use of a model emulator to facilitate calibration of the sediment dynamic submodel against in situ observations model development also included a submodel for the marine cyanobacterium trichodesmium important for its role in fixing atmospheric nitrogen robson et al 2013b 2 2 evaluation metrics 2 2 1 level 0 concept conceptual evaluation sanity check a conceptual evaluation of the biogeochemical model was conducted at each of three stages of the modelling process 1 project inception 2 end user workshops 3 project final delivery in addition 4 the model software was validated to check for mass conservation and numerical integration errors in stage 1 the csiro based biogeochemical modelling team met with researchers at the australian institute of marine science who have significant expertise in the biogeochemistry and plankton dynamics of the great barrier reef the csiro ems model which had previously been applied to a range of estuarine and coastal systems around australia baird et al 2003 robson et al 2006 2008 2013c skerratt et al 2013 wild allen et al 2010 2013 was presented and its suitability for application to the great barrier reef was discussed at the end of the workshop a list of priorities for further development was drawn up in this stage the conceptual framework structure and processes represented by the original csiro ems model were evaluated by disciplinary experts who were not part of the modelling team where this evaluation revealed deficiencies further development was prioritised stage 2 involved a series of end user workshops as model development progressed in these workshops representatives of the modelling team met with representatives of end user organisations the queensland government department of natural resource and land management the office of the great barrier reef and the great barrier reef marine park authority to ensure that end users understood how the model worked at a conceptual level and what they could expect from the finished models the end user workshops also ensured modellers understood how the model could inform environmental management and policy decisions and what would be required to achieve end user acceptance of the model suite as a practical tool to support adaptive management of the great barrier reef while this workshop was not part of the evaluation of the model s structure or performance it did provide an evaluation of utility of the model s intended delivery mechanisms and capabilities stage 3 was a project delivery workshop in which the model its outputs and level 2 evaluations were presented and discussed over the course of two days with great barrier reef experts from both research and policy domains including representatives from several organisations the workshop involved vigorous discussion of aspects of the model s conceptualisation and functional behaviour and analysis of model outputs in response to queries from researchers about whether it could reproduce specific observed water quality patterns the participating experts proposed specific level 3 evaluations that would test the model s ability to reproduce their conceptual understanding of functional relationships in gbr water quality we are not able to publish detailed results of check 2 end user workshops or check 3 project delivery workshop as workshop discussions are considered to constitute research involving human subjects under csiro s research ethics policy publication of workshop outcomes as part of the modelling process was not considered at the time the workshops were held so it was not possible to obtain prior written consent from the disciplinary subject experts and government stakeholders who participated in these discussions however we are able to summarise the improvements to the model and its evaluation that arose from these discussions 2 2 2 level 1 state state variables level 1 state evaluation of the biogeochemical model was completed by visual inspection of time series comparisons of all available water quality monitoring data along with calculation of bias root mean squared error mean absolute error correlation and willmott skill scores for each variable at each site willmott scores were chosen because of their appropriate weighting of differences between predicted and observed variations from the mean delivering numerically comparable scores between 0 no match and 1 a perfect fit for each state variable willmott et al 1985 for reference the recorded match between observed chlorophyll by pigment analysis and observed chlorophyll by fluorescence at the imos nrs sites is around 0 6 because we cannot expect to achieve a better match between modelled water quality and observed water quality than the match achieved between two widely used methods of estimating the same water quality variable from in situ measurements we use this value 0 6 as a benchmark representing an excellent score for modelled water quality variables skerratt et al 2019 data considered included chlorophyll a concentrations measured by pigment extraction three times per year at each of the nearshore mmp fig 1 sites waterhouse et al 2018a and the two imos national reference station sites yongala and north stradbroke island surface chlorophyll a estimated by continuous fluorescence monitoring at 18 mmp sites surface turbidity measured using continuous loggers at the same 18 sites depth integrated zooplankton and phytoplankton counts at two imos national reference station sites within the model domain yongala and north stradbroke island chlorophyll a by pigment extraction and by fluorescence at the two imos sites at the surface and at 20 m below the surface nitrate concentrations measured at the surface 10 m and 20 m monthly at each of the two imos sites concentrations of ammonium nh4 nitrate nitrite nox dissolved inorganic phosphorus dip dissolved organic phosphorus dop dissolved organic nitrogen don total suspended solids tss and dissolved inorganic carbon dic measured three times per year at each of the 14 mmp sites along with alkalinity salinity by conductivity and temperature this time series comparison was supplemented by spatial comparisons of simulated and satellite observed ocean colour over the model domain baird et al 2016b spatial comparisons of simulated and satellite observed sea surface temperature herzfeld et al 2016 and opportunistic comparisons of the model with two dimensional vertically resolved ocean glider transects of optically estimated chlorophyll and nitrate concentrations here we present a few examples of level 1 state validation with analyses not previously presented by skerratt et al 2019 specifically we will present 1 summary figures showing willmott scores willmott et al 1985 bias and rmse for several water quality variables included in the mmp 2 figures showing the distribution and variance of chlorophyll a at representative sites 3 visual comparisons of two dimensional transects from glider autonomous underwater vehicle chlorophyll a from fluorescence with modelled chlorophyll a 2 2 3 level 2 process process rates measurement of process rates is usually more labour intensive expensive and uncertain than measurement of state variables i e concentrations or biomass so relatively few relevant process rate measurements are available here however we consider two data sets 1 dinitrogen fixation rates measured at ten locations that covered both north south and inshore offshore gradients in the great barrier reef in july 2014 as reported by messer et al 2017 2 remineralisation rates of particulate and dissolved organic matter in the central inshore part of the great barrier reef in september 2014 as reported by lønborg et al 2018 2 2 4 level 3 system system level evaluation including emergent properties we consider four system level model properties this evaluation of emergent properties and large scale patterns expands upon the brief evaluation presented by robson et al 2017 which considered the first three of these evaluations the system level properties considered here are 1 phytoplankton community structure we randomly sample 10 000 points from the surface layer of the january 2011 to october 2018 biogeochemical model output and examine how the percentage of chlorophyll a contained in picophytoplankton represented in the model as cells with a radius of 1 μm versus nanophytoplankton 4 μm varies as a function of chlorophyll this overall relationship is compared with the expected relationship i e the observed relationship reported by hirata et al 2011 and brewin et al 2010 from analysis of a global marine phytoplankton database 2 the relationship between modelled chlorophyll a and zooplankton biomass comparing 10 000 points randomly sampled from the surface layer of the biogeochemical model between january 2011 and october 2018 with a relationship obtained from an analysis of 996 in situ observations in the great barrier reef from imos data sets measured at two national reference stations yongala and north stradbroke island between 2008 and 2019 supplemented by ten ship of opportunity continuous plankton recorder cpr transects between 2013 and 2015 see richardson et al 2006 for details and net samples taken by the australian institute of marine science between 1987 and 2014 sampling for the two national reference stations and the australian institute of marine science was conducted with a 100 μm mesh net converting measured dry mass to estimated carbon content following the method of moriarty and o brien 2013 and from carbon to nitrogen content assuming a c n ratio of 6 1 carlotti et al 2000 3 relationships between annual river flow and photic depth in each of several regions of the great barrier reef marine park we compare total wet season river flow to each region as reported by waterhouse et al 2018b following the method described by lewis et al 2014 and with mean modelled photic depth in the north wet tropics mid shelf north wet tropics outer shelf south wet tropics mid shelf south wet tropics outer shelf and central great barrier reef mid and outer shelf we compare model results with remote sensing results of fabricius et al 2016 using the same region boundaries they used 4 the relationship between observed or modelled optical flood plume class and observed or modelled water quality comparing randomly sampled model output from 2011 to 2018 wet seasons with relationships obtained from opportunistic flood plume sampling reported since 1991 as described by devlin et al 2015 optical plume class is determined by classification of ocean colour remote sensing images using rgb hsi signatures and ereefs simulated true colour baird et al 2016b using the algorithm described by devlin et al 2012 3 results 3 1 level 0 conceptual evaluation check 1 inception workshop involving authors of this paper a number of improvements to the model previously developed for application to estuarine ecosystems e g margvelashvili et al 2003 were identified as important for achieving plausibility and credibility in its application to the great barrier reef priority improvements included development of a coral metabolism and carbon cycle submodel mongin and baird 2014 development of a submodel to represent the nitrogen fixing marine cyanobacterium trichodesmium robson et al 2013b and development of an improved seagrass submodel baird et al 2016a these developments were all subsequently implemented and documented the workshops identified a range of challenges regarding the design and communication of the model that needed to be addressed to gain user acceptance and peer reviewer support addressing these challenges resulted in further improvements to the model and its evaluation a summary of these improvements is provided in table 1 check 4 mass conservation gillibrand and herzfeld 2016 have documented the advection scheme used in the model and demonstrated its mass conserving properties 3 2 level 1 state variables model performance in reproducing observed water quality monitoring observations showed no spatial trend along the north south gradient fig 2 and generally met the project requirements willmott scores for ammonium nitrate dissolved inorganic phosphorus chlorophyll and total suspended solids tss ranged between 0 3 and 0 8 at most sites bias metrics shows that the model tends to under estimate observed nutrient and sediment concentrations in the nearshore region across the board performance is better for particulate constituents chlorophyll and tss than dissolved nutrients reflecting the strong physical forcing of particulate concentrations by tides and flood plumes and the influence of localised sub grid scale processes on fluctuations in dissolved nutrient concentrations in the gbr time series comparisons of modelled and observed chlorophyll and nitrate concentrations large phytoplankton and large zooplankton counts along with spatial comparisons of simulated and ocean colour derived chlorophyll estimates have previously been presented by skerratt et al 2019 to which we refer readers interested in additional level 2 evaluations comparisons of simulated suspended sediments with suspended sediment concentrations estimated from turbidity records have been presented by margvelashvili et al 2016 additional level 2 skill assessments including metrics for secchi depth dissolved organic nitrogen dissolved organic phosphorus and dissolved inorganic carbon are available via https ereefs info and in particular at https research csiro au ereefs wp content uploads sites 34 2019 04 gbr4 h2p0 b3p0 chyd dcrt 4april19 pdf box plots of simulated and observed concentrations show that the model generally reproduces the statistical distribution of observed state variable values e g fig 3 again we see that the model under estimates observed chlorophyll concentrations at these nearshore sites and also under estimates the variability in chlorophyll concentrations at most sites a scatter plot comparing instantaneous model predictions with observational chlorophyll a fig 4 reveals the weakness of model performance when assessed at this scale while the relatively strong willmott scores show that the model can reproduce longer term especially seasonal patterns it is not able to reproduce exact concentrations at a particular point in time in the macrotidal environment of our nearshore coastal sites concentrations of particulate materials vary by multiple orders of magnitude in the space of a few hours due to tidal resuspension as the sediment and biogeochemical models are resolved with an hourly time step it is not possible to reproduce variations on this scale while monthly monitoring observations are primarily near surface data from the autonomous underwater vehicle transects provide two dimensional vertical observations that are shown alongside matching slices of model outputs in fig 5 3 3 level 2 process rates a comparison between ereefs modelled nitrogen fixation rates for july 2016 and rates reported by messer et al 2017 is given in fig 6 the model predicts much lower nitrogen fixation rates and a narrow range at each site the results however also suggest that the model is under estimating total nitrogen fixation while remineralisation rates for organic matter are specified as parameters in the ereefs biogeochemical model modified as a function of temperature observations of particulate and dissolved organic matter remineralisation rates in gbr waters have only recently and after completion of model calibration become available lønborg et al 2018 reports degradation rates for marine dissolved organic matter in the gbr of between 0 04 and 0 33 d 1 and rates for particulate organic matter of between 0 2 and 0 36 d 1 these rates are relatively uncertain they have wide variability and many are not inconsistent with the assigned modelled rate of decay for phytoplankton derived marine detritus which ranges from 0 04 to 0 09 d 1 within the range of sea surface temperatures predicted by the model 34 4 level 3 system level evaluation including emergent properties the comparison between size distribution of phytoplankton in the ereefs biogeochemical model and from observed empirical relationships is shown in fig 7 the ereefs model captures the non linearities between the relative proportion of phytoplankton cell size and chlorohyll this match gives us a high degree of confidence in the model s capacity to simulate the size spectrum of phytoplankton the comparison between the observed and simulated relationship between zooplankton biomass and chlorophyll shows similar general patterns with increasing zooplankton biomass with chlorophyll the model over predicts observed zooplankton biomass and also over estimates the rate at which zooplankton increases with chlorophyll fig 8 the modelled large zooplankton concentration has a mean of 2 09 1 16 mg n m 3 while the observational data has a mean of 1 42 2 23 mg n m 3 although observational measurements are an underestimate of true zooplankton biomass the extent of under estimation is difficult to estimate overall the agreement between the emergent patterns of zooplankton and chlorophyll between the model and field data are encouraging the model correctly predicts reduced water clarity in years following high river discharge even well beyond the boundaries of the observed river plumes but over estimates water clarity in most regions fig 9 fig 10 shows the observed relationship between remote sensing optical plume class and in situ water quality from hundreds of opportunistic and routine monitoring observations that coincided with flood events in the great barrier reef between 2007 and 2015 from devlin et al 2015 by randomly sampling simulated water quality within each plume class we have produced comparable plots for model output although the time frames of observations and modelling do not match up exactly and the opportunistic observational data do not provide a complete snapshot of any one individual flood event comparing the system scale results allows an additional model assessment by showing the statistical distribution of water quality variables in flood plumes over periods of several years in each case this comparison reveals that 1 the model and opportunistic observations produce comparable ranges and variabilities of din dip and tss within flood plumes 2 the model tends to under estimate flood plume chlorophyll and light attenuation 3 model and observations show a comparable improvement in water quality reduced sediment and nutrient concentrations with increasing plume colour class 4 discussion we have demonstrated the application of the csps model evaluation framework to a complex biogeochemical model the framework allowed us to identify the aspects of the ereef model suite that agree well with observations and those that require further work it also provided an approach for model development while traditional evaluation focuses on the comparison of observed and modelled state variable concentrations e g chlorophyll a total nitrogen and temperature a more comprehensive model evaluation that formally considers other aspects of model performance provides a more thorough understanding of model performance 4 1 the need for better evaluation of complex models complex biogeochemical models are widely used but are sometimes criticised as being over parameterised relative to the data available for model evaluation mooij et al 2010 one counter to this criticism is that a typical model calibration exercise does not involve tuning every model parameter that represents a biogeochemical process rate or biological trait that may potentially vary rather experienced biogeochemical modellers typically focus their calibration efforts on a handful of parameters that have been identified as both sensitive i e model outputs respond strongly to variations in these parameters and under prescribed i e available information from laboratory or field studies is not sufficient to support specification of an exact value hence a biogeochemical model described as having 100 parameters may in fact have only 6 parameters used for tuning of these some may be tuned within a tightly constrained range informed by a physiological understanding of the process while others may be treated as free parameters better practise in biogeochemical modelling could be to ensure that the selection of parameters used for calibration is fully documented e g janse et al 2010 and perhaps to use an alternative word to describe parameters that are excluded from tuning and treated as constants another frequent criticism of biogeochemical and ecosystem modelling e g arhonditsis and brett 2004 robson 2014 is the frequency with which all or most of the available observational data is used for calibration leaving insufficient independent data for a robust model evaluation colloquially validation while best modelling practise avoids this jakeman et al 2006 robson et al 2008 the constraints of applied modelling outside academia sometimes make best practise impracticable when there are insufficient data to support independent calibration and validation of state variable time series analysis of system level properties especially emergent properties that were not considered during calibration provides an alternative and powerful means of independent model evaluation further with the move toward more observational data being made available in online databases there is the opportunity for greater model evaluation in the future 4 2 emergent properties versus aggregate system properties definitions of the term emergent property in ecosystem science vary and have long been a point of contention salt 1979 argued that an emergent property must be wholly unpredictable from observation of components of that unit if a mechanistic model can predict the occurrence of a system level property then it follows that the system level property does not meet this strict standard as subsequently noted by edson et al 1981 however predictability is always relative we prefer a definition that rests on the emergent system scale relationship being neither an obvious consequence of the low level interactions directly represented in the model algorithms nor a simple aggregation of lower level properties and components by this definition relationships between optical flood plume class and concentrations of nutrients and sediments may not represent true emergent properties as they fail the simple aggregation test though this point is arguable because of the non additive relationship between the inherent optical properties iops of water quality constituents and the apparent optical properties that results in observed ocean colour the relationship between river flow and water clarity in the great barrier reef may be obvious when applied to wet season and near shore conditions but it is not obvious that these relationships should still hold in the dry season and further offshore hence these relationships may be considered emergent according to our definition planktonic community structures emerge as a function of complex relationships amongst many processes simulated by the model including growth of each separate phytoplankton group itself a function of variable intracellular carbon nitrogen phosphorus and chlorophyll concentrations cell size and temperature sinking a function of cell size and density and grazing which varies due to predator prey relationships between the two zooplankton size classes and the two phytoplankton size classes hence we consider phytoplankton community size structures and the shape of the relationship between zooplankton biomass and chlorophyll to be genuine and non trivial emergent properties of underlying model performance 4 3 performance of the ereefs biogeochemical model the evaluation described here revealed both strengths of the ereefs biogeochemical model and areas for improvement as has been previously reported skerratt et al 2019 steven et al 2019 the performance at level 1 state i e the match against state variable in this case water quality monitoring is adequate for its application to a wide range of scientific and management applications and comparable or better than results obtained elsewhere for biogeochemical models of similar scope the deeper evaluation here sheds additional light on model performance 4 3 1 level 1 state the systematic negative bias in simulated dissolved nutrient and chlorophyll concentrations was not evident during the first few years of the simulation herzfeld et al 2016 but appeared over time this suggests that the model is consistently over estimating net nutrient losses or under estimating sources by a small amount because both nitrogen and phosphorus are affected ruling out nitrogen fixation and denitrification as the source of the problem and we do not observe an increase in sediment stores this is most likely a boundary condition issue at present the model relies on climatological data to specify biogeochemical outer boundary conditions as global biogeochemical models continue to improve in skill and resolution they may in future offer an opportunity for improved boundary conditions the observed under estimation of variability in chlorophyll concentrations is to be expected given the relatively course 4 km square resolution of the model under estimating natural variability is common amongst many models refs the model is able to reproduce vertical temperature profiles along a transect see quinlan et al 2019 for a more detailed analysis of model skill in reproducing vertical temperature profiles and also reproduced an observed sub surface chlorophyll maximum albeit with an exaggerated vertical extent probably attributable to the model s limited vertical and horizontal resolution this vertical stretching of the deep chlorophyll maximum also results in the maximum being too close to the surface at least some of the time which has implications for estimated primary productivity further examination of data from glider tracks is indicated 4 3 2 level 2 process this analysis has shown an important potential weakness in the model s simulation of nitrogen fixation rates fig 6 nitrogen fixation in the model is represented as a physiological function of trichodesmium chlorophyll production energy and nutrient requirements baird et al in review robson et al 2013a one possible explanation for the apparent under estimation of nitrogen fixation by the model is that it does not include other nitrogen fixing organisms alternatively the model may not be adequately capturing the dynamics of trichodesmium nitrogen fixation because it omits potentially important factors such as the role of iron in nitrogenase production the variations in trichodesmium colony and filament size and shape and the physical dynamics of surface blooms while only a handful of nitrogen fixation measurements for the great barrier reef are currently available to suggest that this might be a problem it has implications for how the system may respond to changes in catchment nutrient loads and to climate change and therefore warrants further investigation as well as caution in interpretation of model results for this type of scenario 4 3 3 level 3 system the excellent match between the model and observed emergent properties of phytoplankton community structure fig 7 provides confidence that the model s detailed physiological representation of phytoplankton carbon fixation nutrient uptake population growth and chlorophyll production is not over fitted and represents key features of the system well we did not peek at this emergent relationship until after model calibration which was restricted to level 1 state metrics such as direct comparisons of observed and simulated chlorophyll concentrations had been finalised so we are confident that they genuinely emerge from the model structure this provides greater confidence that the model is likely to predict changes in phytoplankton and chlorophyll responses with reasonable accuracy the empirical models provided by brewin et al 2010 and hirata et al 2011 differ markedly in their predictions at low chlorophyll a the ereefs predictions are more in agreement with hirata et al 2011 the difference between the empirical models at low chlorophyll a concentrations is important for energy transfer in the food web as copepods an important component of zooplankton can graze large phytoplankton but not smaller phytoplankton picophytoplankton both empirical models like the ereefs model results show a large scatter around the fit lines the expected relationship between zooplankton nitrogen content and chlorophyll is not as well matched by the model fig 8 this could be due either to model error or observational error if the error is in the model either simulated zooplankton mortality or of nitrogen lost due to sloppy eating in grazing processes may be too low alternatively it could be due to the necessary structural simplification of the model zooplankton is used as a closure term and hence effectively represents the whole food web above it see libralato and solidoro 2009 for further exploration of this point observational error may also play a part net based methods under estimate true zooplankton biomass as smaller zooplankton can pass through the 100 μm mesh and larger zooplankton can escape the net by active avoidance barnes and tranter 1965 the model s ability to represent qualitatively the previously reported system scale relationships between wet season river flow an annual average light attenuation or secchi depth in the mid and outer shelf fig 9 provides confirmation of earlier remote sensing results and motivation for continued river water quality improvement efforts the ereefs hydrodynamic sediment dynamic and biogeochemical models also now provide a tool to explore the mechanism of this relationship impacts of river nutrient delivery on phytoplankton productivity and impacts of fine dust carried with river water may both play a role as may enhanced mixing associated with monsoonal storms that the match between observed and simulated relationships is imperfect with the biogeochemical model tending to over estimate water clarity especially in high flow years has motivated improvements in model structure since the run reported here baird et al in review margvelashvili et al 2018 this includes the addition of a very fine dust sediment class which is carried further from the river mouth the model also over estimates the clarity of water in river flood plumes and tends to under estimate nearshore chlorophyll concentrations figs 3 and 10 which suggests a need for closer examination of ecosystem processes in river flood plumes the observational data suggest qualitatively that chlorophyll may decline with increasing plume class number while the model shows no such decline fig 10 and predicts lower chlorophyll concentrations in flood waters this is likely due to the limited range of phytoplankton functional groups included in the model the large phytoplankton small phytoplankton and trichodesmium groups in the model represent typical characteristics of marine phytoplankton but may not adequately represent the properties of fast growing opportunistic and mesohaline phytoplankton species in the relatively nutrient rich waters of flood plumes work is now in progress to assess phytoplankton community composition in gbr waters in flood conditions and compare it with phytoplankton community composition during the dry season this will inform future model developments fig 10 also reveals unexpectedly high din and dip concentrations in waters classified as optical plume class 5 this suggests that either the biogeochemical function or the optical colour of water in this plume class towards the outer edge of flood plumes is not well represented by the model in particular there may be an interaction between the under prediction of chlorophyll in flood waters and an under prediction of dissolved nutrient uptake by phytoplankton 5 conclusions the csps model evaluation framework demonstrated here provides a more systematic way than traditional model assessment approaches to consider multiple aspects of model performance this framework may help to improve the evaluation of any complex mechanistic environmental model especially where the number of free parameters the computational requirements of the model or the sparsity of data limit the application of robust parameter estimation uncertainty analysis and or independent model evaluation procedures unlike some other approaches the framework described here does not require a large number of model runs and it can make use of sparse data that may not be precisely aligned with model outputs in space and time in application to the ereefs biogeochemical model this approach has shed light on both strengths and weaknesses of the model providing confidence for a range of applications while also demonstrating a need for caution in some applications and setting directions for future model improvements in particular level 0 concept evaluation showed that to achieve end user acceptance it was important to include additional sediment size classes a more detailed seagrass model coral metabolism ocean acidification and nitrogen fixation these were all implemented in the final version of the model in direct response to concept evaluation of the pilot model level 1 state evaluation showed that the model is fit for purpose with respect to application to regional scale land use change scenario applications and can be used to complement in situ measurements especially in the context of monitoring seasonal and inter annual variations however the model does not accurately predict variations in water quality at smaller timescales hourly to daily and should not be used in applications where this capability is required in addition end users should be aware of a negative bias in nutrient concentrations i e the tendency of the model to under predict observed values currently ongoing model development is intended to reduce this bias and further improve level 1 metrics level 2 process evaluation showed that the model very likely under estimates nitrogen fixation further process rate measurements are needed to confirm this and provide the data needed to correct it although this evaluation was limited due to a paucity of process rate measurements improved understanding of nitrogen fixation in the gbr is highlighted as a priority to support improved predictive capability level 3 system evaluation showed that the model is able to predict system scale properties correctly including the relationship between phytoplankton cell size and chlorophyll the evolution of water quality in flood plumes and the existence of a negative relationship between river discharge and gbr photic depth the model is therefore fit for purpose as an aid to understanding large scale processes and patterns in the gbr discrepancies in the details of these patterns including the slope of the relationship between discharge and photic depth and the slope of the curve relating zooplankton to phytoplankton concentrations point to additional areas for improvement specifically we are investigating improvements in the parameterisation of inherent optical properties in the optical submodel and the specification of zooplankton traits in the grazing submodel given sparse in situ observational data for photic depth and zooplankton concentrations these conclusions would not have been possible without the system level evaluations software and data availability the csiro ems hydrodynamic sediment and biogeochemical model software is available open source via https github com csiro coasts ems detailed science and user manuals can be found at https research csiro au cem software ems ems documentation both near real time and archived model outputs for the ereefs application of ems are made freely available in standard netcdf format via https research csiro au ereefs to facilitate third party scientific and technical applications to support extraction plotting and analysis of netcdf model outputs example matlab scripts are available on the ereefs info website and an r package is available via https github com aims ereefs model outputs are also available as prepared visualisations for policy end users through http aims ereefs org au aims ereefs https aims ereefs org au aims ereefs this paper refers to imos observational data which are available via https portal aodn org au and to water quality data from the marine monitoring program detailed descriptions of which are available through https eatlas org au rrmmp gbr aims inshore water quality for this paper we present results for the 4 km resolution ereefs biogeochemical model version 2 0 run reference h2p0 b2p0 chyd dcrt an explanation of the naming convention and a description of the boundary conditions and model version used for this run is given at https research csiro au ereefs models models about biogeochemical simulation naming protocol declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements ereefs simulations were developed as part of the ereefs project http ereefs org au ereefs a public private collaboration between australia s leading operational and scientific research agencies government and corporate australia integrated marine observing system imos supplied imos mooring data imos is supported by the australian government through the national collaborative research infrastructure strategy and the super science initiative the marine monitoring program mmp managed by the gbr marine park authority with funding from the department of the environment and energy and co funding from research partners supplied mmp moorings and tri annual water sampling which was conducted by the australian institute of marine science james cook university university of queensland queensland parks and wildlife service reef catchments and community volunteers we thank our many colleagues involved in developing the ereefs model particularly rob ellis and david waters for the catchment modelling as part of the queensland and australian government s paddock to reef program that is funded by the queensland department of natural resources and mines queensland regional natural resource management investment program 2013 2018 with support from the department of science information technology innovation and the arts dsitia this analysis was funded by the australian institute of marine science and csiro with contributions from the imos zoom zooplankton ocean observations and modelling task particular thanks also to our colleagues jessica benthyusen miles furnas david mckinnon john andrewartha philip gillibrand ruth eriksen jonathon hodge sharon ticknell richard brinkman and cedric robillot for their contributions to ereefs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104707 
26020,the great barrier reef is a unesco world heritage area that has been assessed as having a very poor outlook and needing urgent intervention the ereefs hydrodynamic biogeochemical models are used to complement monitoring facilitate data interpretation and support policy decisions management and policy for the great barrier reef are politically contentious so a high standard of model evaluation is required we report the application of a recently developed four level csps concept state process system model evaluation framework to the ereefs biogeochemical model the framework considers level 0 conceptual evaluation level 1 simulated state variables level 2 process rates and level 3 system level patterns and emergent properties this paper is the first complete application of this model assessment framework it highlights both strengths and weaknesses of the ereefs biogeochemical model that might not have been apparent from a traditional model evaluation the framework can be applied to evaluation of any complex process based numerical model keywords model evaluation assessment ecosystem model hydrodynamic model emergent properties great barrier reef 1 introduction the great barrier reef gbr has been identified by unesco as a world heritage area of outstanding universal value lucas et al 1997 it is one of the most biodiverse regions in the world has enormous cultural value and has been assessed as adding at least au 5 7b in uncorrected 2012 australian dollars and 64 000 jobs to australia s economy deloitte access economics 2013 the great barrier reef experiences diverse pressures that threaten these values these include changes in water quality attributed to agricultural clearing bainbridge et al 2018 kroon et al 2016 recurrent outbreaks of coral eating crown of thorns starfish which may also be due to water quality changes brodie et al 2005 structural damage due to tropical cyclones and other storms massel and done 1993 and devastating coral bleaching events associated with climate change and marine heatwaves in 2016 2017 coral bleaching events caused the death of a substantial proportion estimates vary from 10 to 50 of coral on the great barrier reef babcock et al 2019 hughes et al 2018 kennedy et al 2018 lewis and mallela 2018 stuart smith et al 2018 this was part of the wider global coral bleaching event that occurred between 2014 and 2017 eakin et al 2019 in light of these threats the great barrier reef marine park authority has recently assessed the outlook for the reef as very poor great barrier reef marine park authority 2019 policy and intervention options to preserve and restore the great barrier reef and improve its resilience to ongoing climate change are now being urgently explored the need for marine models to support policy and management decisions for the great barrier reef motivated the ereefs project a collaboration between csiro the australian institute of marine science the bureau of meteorology the government of queensland and the great barrier reef foundation chen et al 2011 schiller et al 2015 the ereefs project developed a tailored suite of nested hydrodynamic sediment dynamic and biogeochemical models that simulate physical conditions and water quality in three dimensions in near real time throughout the great barrier reef world heritage area and beyond baird et al 2016a 2016b 2018 herzfeld et al 2016 jones et al 2016 robson et al 2013b 2018 the ereefs models have been applied to investigations of the effects of ocean acidification on the great barrier reef mongin and baird 2014 mongin et al 2016b the effects of rivers on marine water quality margvelashvili et al 2018 robson et al 2014 wolff et al 2018 the physical oceanography that drives the exposure of reefs to hot water schiller et al 2015 and the interactions between physical conditions and biological mechanisms that lead to coral bleaching baird et al 2018 the models are now being used to test the likely effects of interventions e g albright et al 2016 mongin et al 2016a and to help set new more ecologically relevant targets for water quality and land runoff brodie et al 2017 monitoring and adaptive management are not straightforward in the region because of the size of the great barrier reef 300 000 km2 its remoteness from major population centres and the large inter annual variability associated with the tropical climate of the region including cyclone events wolff et al 2016 and effects of fluctuations in the el nino southern oscillation index leonard et al 2016 the health and condition of the system is therefore monitored through a combination of quarterly water quality surveys mostly in the inshore region waterhouse et al 2018a regular annual and alternate year ecological surveys of reef thompson et al 2018 and seagrass mckenzie et al 2018 habitats in situ logging of the physical properties fluorescence and turbidity of water at a few sites lynch et al 2014 and opportunistic sampling of major flood plume events e g cherukuru et al 2017 howley et al 2018 this in situ sampling regime is supplemented by satellite observation brando et al 2015a 2015b devlin et al 2015 and more recently by the ereefs marine models to provide synoptic near real time reporting of hydrodynamic and biochemical conditions on the gbr steven et al 2019 in this context of policy urgency data sparsity and politically contentious management and regulation it is important to understand the performance of the models so that they can be used with confidence where we lack direct observational data yet with due recognition of their limitations a traditional evaluation of the ereefs biogeochemical model against routine marine monitoring observations following the principles laid out by bennett et al 2013 has been presented by herzfeld et al 2016 with respect to water quality and physical conditions and skerratt et al 2019 with respect to plankton these evaluations directly compare observed water quality at marine monitoring program mmp and integrated marine observing system imos monitoring sites using time series comparisons over several years 2011 2016 along with measures of bias correlation error and skill willmott et al 1985 for each observed water quality and plankton variable at each monitoring site results generally indicate strong model performance although this evaluation should be treated with caution for several reasons a monitoring sites are predominantly shallow inshore sampling and thus represent only a small fraction of the area modelled b the water quality variables and sites used in the evaluation were the same as those used for model calibration although evaluation extended beyond the calibration period and to more variables than used in the calibration i e secchi depth c many variables included in the model are not directly observed these unobserved variables include for instance intracellular concentrations of carbon nitrogen phosphorus and chlorophyll within phytoplankton cells concentrations of labile and refractory particulate organic material and concentrations of small zooplankton recently hipsey et al 2020 proposed a new four level framework hereafter referred to as the concept state process system or csps framework for evaluation of aquatic ecosystem models in brief it describes level 0 concept the sanity check does the model produce plausible behaviour and reproduce key expected system properties level 1 state traditional evaluation achieved by comparing values of model state variables e g nutrient and chlorophyll concentrations with observed values of these state variables at matched times and locations this evaluation should ideally include measures of error bias and correlation for key state variables with weighting functions that consider how the models will be used and therefore which aspects of state variable responses are most important bennett et al 2013 level 2 process evaluation of process rates and ecosystem function for example nutrient flux rates and system metabolism while maximum process rates are typically represented in biogeochemical models as assigned parameter values bainbridge et al 2018 actual process rates arise from interactions among state variables and environmental conditions and these are rarely validated against observational data hipsey et al 2020 level 3 system evaluation of system scale emergent properties that is properties and relationships that emerge from complex system dynamics in a way that is not obvious from simple inspection of the equations underlying the model the behaviour of flocks of birds in flight has often been presented as a classic example of an emergent property of simple rules governing the individual behaviour of each animal phelan 1999 when a complex process based model is able to reproduce emergent properties without additional calibration after level 1 state validation it gives confidence that the model is getting the right results for the right reasons robson et al 2017 here we demonstrate for the first time an application of the csps framework to evaluate the ereefs marine biogeochemical model we show how this approach can be used to highlight the weaknesses as well as the strengths of the model which is important to build trust and encourage appropriate use we also synthesise a body of previously published and in press work that describes the development of the ereefs marine models their calibration and evaluation and the application of these models to enhance understanding of drivers of water quality in the great barrier reef world heritage area in addition we show several new evaluations that were necessary to complete the full csps model assessment and provide a narrative of the modelling and model delivery process 2 methods 2 1 models the ereefs marine models are a complex system of three dimensional hydrodynamic biogeochemical and sediment dynamics models driven by hydrological monitoring in 22 rivers feeding into the great barrier reef near real time meteorological models and global ocean models that provide the outer boundary conditions the ereefs marine models are an implementation of the csiro environmental modelling suite ems which has previously been applied on a smaller scale in a range of coastal and estuary contexts including on the great barrier reef the fitzroy estuary and keppel bay margvelashvili et al 2003 2013 robson et al 2006 webster et al 2003 2005 2006 the initial vision for the ereefs project was described by chen et al 2011 the full system of coupled biogeochemical hydrodynamic sediment dynamic models developed in subsequent years the final models and their implementation have been described in detail by herzfeld et al 2016 and baird et al in review and the information architecture supporting the model and other data products has been described by car 2013 and yu et al 2016 steven et al 2019 provide an overview of overall operational architecture of the modelling platform and describe several of the ways in which they have been applied the ereefs biogeochemical model includes several components that were developed specifically for application to the great barrier reef herzfeld and gillibrand 2015 describe a new open boundary forcing scheme used to facilitate two way nesting of regional submodels within the broader great barrier reef domain baird et al 2016a describe the development and application of the seagrass submodel which considers above and below ground biomass for three seagrass functional groups and their responses to spectrally resolved light conditions the optical submodel used to simulate these conditions is described by baird et al 2016b who also demonstrates how a spectrally resolved optical model can be used to generate simulated true colour images of the ocean as from a satellite and improve comparison with satellite remote sensing observations margvelashvili et al 2016 describe the use of a model emulator to facilitate calibration of the sediment dynamic submodel against in situ observations model development also included a submodel for the marine cyanobacterium trichodesmium important for its role in fixing atmospheric nitrogen robson et al 2013b 2 2 evaluation metrics 2 2 1 level 0 concept conceptual evaluation sanity check a conceptual evaluation of the biogeochemical model was conducted at each of three stages of the modelling process 1 project inception 2 end user workshops 3 project final delivery in addition 4 the model software was validated to check for mass conservation and numerical integration errors in stage 1 the csiro based biogeochemical modelling team met with researchers at the australian institute of marine science who have significant expertise in the biogeochemistry and plankton dynamics of the great barrier reef the csiro ems model which had previously been applied to a range of estuarine and coastal systems around australia baird et al 2003 robson et al 2006 2008 2013c skerratt et al 2013 wild allen et al 2010 2013 was presented and its suitability for application to the great barrier reef was discussed at the end of the workshop a list of priorities for further development was drawn up in this stage the conceptual framework structure and processes represented by the original csiro ems model were evaluated by disciplinary experts who were not part of the modelling team where this evaluation revealed deficiencies further development was prioritised stage 2 involved a series of end user workshops as model development progressed in these workshops representatives of the modelling team met with representatives of end user organisations the queensland government department of natural resource and land management the office of the great barrier reef and the great barrier reef marine park authority to ensure that end users understood how the model worked at a conceptual level and what they could expect from the finished models the end user workshops also ensured modellers understood how the model could inform environmental management and policy decisions and what would be required to achieve end user acceptance of the model suite as a practical tool to support adaptive management of the great barrier reef while this workshop was not part of the evaluation of the model s structure or performance it did provide an evaluation of utility of the model s intended delivery mechanisms and capabilities stage 3 was a project delivery workshop in which the model its outputs and level 2 evaluations were presented and discussed over the course of two days with great barrier reef experts from both research and policy domains including representatives from several organisations the workshop involved vigorous discussion of aspects of the model s conceptualisation and functional behaviour and analysis of model outputs in response to queries from researchers about whether it could reproduce specific observed water quality patterns the participating experts proposed specific level 3 evaluations that would test the model s ability to reproduce their conceptual understanding of functional relationships in gbr water quality we are not able to publish detailed results of check 2 end user workshops or check 3 project delivery workshop as workshop discussions are considered to constitute research involving human subjects under csiro s research ethics policy publication of workshop outcomes as part of the modelling process was not considered at the time the workshops were held so it was not possible to obtain prior written consent from the disciplinary subject experts and government stakeholders who participated in these discussions however we are able to summarise the improvements to the model and its evaluation that arose from these discussions 2 2 2 level 1 state state variables level 1 state evaluation of the biogeochemical model was completed by visual inspection of time series comparisons of all available water quality monitoring data along with calculation of bias root mean squared error mean absolute error correlation and willmott skill scores for each variable at each site willmott scores were chosen because of their appropriate weighting of differences between predicted and observed variations from the mean delivering numerically comparable scores between 0 no match and 1 a perfect fit for each state variable willmott et al 1985 for reference the recorded match between observed chlorophyll by pigment analysis and observed chlorophyll by fluorescence at the imos nrs sites is around 0 6 because we cannot expect to achieve a better match between modelled water quality and observed water quality than the match achieved between two widely used methods of estimating the same water quality variable from in situ measurements we use this value 0 6 as a benchmark representing an excellent score for modelled water quality variables skerratt et al 2019 data considered included chlorophyll a concentrations measured by pigment extraction three times per year at each of the nearshore mmp fig 1 sites waterhouse et al 2018a and the two imos national reference station sites yongala and north stradbroke island surface chlorophyll a estimated by continuous fluorescence monitoring at 18 mmp sites surface turbidity measured using continuous loggers at the same 18 sites depth integrated zooplankton and phytoplankton counts at two imos national reference station sites within the model domain yongala and north stradbroke island chlorophyll a by pigment extraction and by fluorescence at the two imos sites at the surface and at 20 m below the surface nitrate concentrations measured at the surface 10 m and 20 m monthly at each of the two imos sites concentrations of ammonium nh4 nitrate nitrite nox dissolved inorganic phosphorus dip dissolved organic phosphorus dop dissolved organic nitrogen don total suspended solids tss and dissolved inorganic carbon dic measured three times per year at each of the 14 mmp sites along with alkalinity salinity by conductivity and temperature this time series comparison was supplemented by spatial comparisons of simulated and satellite observed ocean colour over the model domain baird et al 2016b spatial comparisons of simulated and satellite observed sea surface temperature herzfeld et al 2016 and opportunistic comparisons of the model with two dimensional vertically resolved ocean glider transects of optically estimated chlorophyll and nitrate concentrations here we present a few examples of level 1 state validation with analyses not previously presented by skerratt et al 2019 specifically we will present 1 summary figures showing willmott scores willmott et al 1985 bias and rmse for several water quality variables included in the mmp 2 figures showing the distribution and variance of chlorophyll a at representative sites 3 visual comparisons of two dimensional transects from glider autonomous underwater vehicle chlorophyll a from fluorescence with modelled chlorophyll a 2 2 3 level 2 process process rates measurement of process rates is usually more labour intensive expensive and uncertain than measurement of state variables i e concentrations or biomass so relatively few relevant process rate measurements are available here however we consider two data sets 1 dinitrogen fixation rates measured at ten locations that covered both north south and inshore offshore gradients in the great barrier reef in july 2014 as reported by messer et al 2017 2 remineralisation rates of particulate and dissolved organic matter in the central inshore part of the great barrier reef in september 2014 as reported by lønborg et al 2018 2 2 4 level 3 system system level evaluation including emergent properties we consider four system level model properties this evaluation of emergent properties and large scale patterns expands upon the brief evaluation presented by robson et al 2017 which considered the first three of these evaluations the system level properties considered here are 1 phytoplankton community structure we randomly sample 10 000 points from the surface layer of the january 2011 to october 2018 biogeochemical model output and examine how the percentage of chlorophyll a contained in picophytoplankton represented in the model as cells with a radius of 1 μm versus nanophytoplankton 4 μm varies as a function of chlorophyll this overall relationship is compared with the expected relationship i e the observed relationship reported by hirata et al 2011 and brewin et al 2010 from analysis of a global marine phytoplankton database 2 the relationship between modelled chlorophyll a and zooplankton biomass comparing 10 000 points randomly sampled from the surface layer of the biogeochemical model between january 2011 and october 2018 with a relationship obtained from an analysis of 996 in situ observations in the great barrier reef from imos data sets measured at two national reference stations yongala and north stradbroke island between 2008 and 2019 supplemented by ten ship of opportunity continuous plankton recorder cpr transects between 2013 and 2015 see richardson et al 2006 for details and net samples taken by the australian institute of marine science between 1987 and 2014 sampling for the two national reference stations and the australian institute of marine science was conducted with a 100 μm mesh net converting measured dry mass to estimated carbon content following the method of moriarty and o brien 2013 and from carbon to nitrogen content assuming a c n ratio of 6 1 carlotti et al 2000 3 relationships between annual river flow and photic depth in each of several regions of the great barrier reef marine park we compare total wet season river flow to each region as reported by waterhouse et al 2018b following the method described by lewis et al 2014 and with mean modelled photic depth in the north wet tropics mid shelf north wet tropics outer shelf south wet tropics mid shelf south wet tropics outer shelf and central great barrier reef mid and outer shelf we compare model results with remote sensing results of fabricius et al 2016 using the same region boundaries they used 4 the relationship between observed or modelled optical flood plume class and observed or modelled water quality comparing randomly sampled model output from 2011 to 2018 wet seasons with relationships obtained from opportunistic flood plume sampling reported since 1991 as described by devlin et al 2015 optical plume class is determined by classification of ocean colour remote sensing images using rgb hsi signatures and ereefs simulated true colour baird et al 2016b using the algorithm described by devlin et al 2012 3 results 3 1 level 0 conceptual evaluation check 1 inception workshop involving authors of this paper a number of improvements to the model previously developed for application to estuarine ecosystems e g margvelashvili et al 2003 were identified as important for achieving plausibility and credibility in its application to the great barrier reef priority improvements included development of a coral metabolism and carbon cycle submodel mongin and baird 2014 development of a submodel to represent the nitrogen fixing marine cyanobacterium trichodesmium robson et al 2013b and development of an improved seagrass submodel baird et al 2016a these developments were all subsequently implemented and documented the workshops identified a range of challenges regarding the design and communication of the model that needed to be addressed to gain user acceptance and peer reviewer support addressing these challenges resulted in further improvements to the model and its evaluation a summary of these improvements is provided in table 1 check 4 mass conservation gillibrand and herzfeld 2016 have documented the advection scheme used in the model and demonstrated its mass conserving properties 3 2 level 1 state variables model performance in reproducing observed water quality monitoring observations showed no spatial trend along the north south gradient fig 2 and generally met the project requirements willmott scores for ammonium nitrate dissolved inorganic phosphorus chlorophyll and total suspended solids tss ranged between 0 3 and 0 8 at most sites bias metrics shows that the model tends to under estimate observed nutrient and sediment concentrations in the nearshore region across the board performance is better for particulate constituents chlorophyll and tss than dissolved nutrients reflecting the strong physical forcing of particulate concentrations by tides and flood plumes and the influence of localised sub grid scale processes on fluctuations in dissolved nutrient concentrations in the gbr time series comparisons of modelled and observed chlorophyll and nitrate concentrations large phytoplankton and large zooplankton counts along with spatial comparisons of simulated and ocean colour derived chlorophyll estimates have previously been presented by skerratt et al 2019 to which we refer readers interested in additional level 2 evaluations comparisons of simulated suspended sediments with suspended sediment concentrations estimated from turbidity records have been presented by margvelashvili et al 2016 additional level 2 skill assessments including metrics for secchi depth dissolved organic nitrogen dissolved organic phosphorus and dissolved inorganic carbon are available via https ereefs info and in particular at https research csiro au ereefs wp content uploads sites 34 2019 04 gbr4 h2p0 b3p0 chyd dcrt 4april19 pdf box plots of simulated and observed concentrations show that the model generally reproduces the statistical distribution of observed state variable values e g fig 3 again we see that the model under estimates observed chlorophyll concentrations at these nearshore sites and also under estimates the variability in chlorophyll concentrations at most sites a scatter plot comparing instantaneous model predictions with observational chlorophyll a fig 4 reveals the weakness of model performance when assessed at this scale while the relatively strong willmott scores show that the model can reproduce longer term especially seasonal patterns it is not able to reproduce exact concentrations at a particular point in time in the macrotidal environment of our nearshore coastal sites concentrations of particulate materials vary by multiple orders of magnitude in the space of a few hours due to tidal resuspension as the sediment and biogeochemical models are resolved with an hourly time step it is not possible to reproduce variations on this scale while monthly monitoring observations are primarily near surface data from the autonomous underwater vehicle transects provide two dimensional vertical observations that are shown alongside matching slices of model outputs in fig 5 3 3 level 2 process rates a comparison between ereefs modelled nitrogen fixation rates for july 2016 and rates reported by messer et al 2017 is given in fig 6 the model predicts much lower nitrogen fixation rates and a narrow range at each site the results however also suggest that the model is under estimating total nitrogen fixation while remineralisation rates for organic matter are specified as parameters in the ereefs biogeochemical model modified as a function of temperature observations of particulate and dissolved organic matter remineralisation rates in gbr waters have only recently and after completion of model calibration become available lønborg et al 2018 reports degradation rates for marine dissolved organic matter in the gbr of between 0 04 and 0 33 d 1 and rates for particulate organic matter of between 0 2 and 0 36 d 1 these rates are relatively uncertain they have wide variability and many are not inconsistent with the assigned modelled rate of decay for phytoplankton derived marine detritus which ranges from 0 04 to 0 09 d 1 within the range of sea surface temperatures predicted by the model 34 4 level 3 system level evaluation including emergent properties the comparison between size distribution of phytoplankton in the ereefs biogeochemical model and from observed empirical relationships is shown in fig 7 the ereefs model captures the non linearities between the relative proportion of phytoplankton cell size and chlorohyll this match gives us a high degree of confidence in the model s capacity to simulate the size spectrum of phytoplankton the comparison between the observed and simulated relationship between zooplankton biomass and chlorophyll shows similar general patterns with increasing zooplankton biomass with chlorophyll the model over predicts observed zooplankton biomass and also over estimates the rate at which zooplankton increases with chlorophyll fig 8 the modelled large zooplankton concentration has a mean of 2 09 1 16 mg n m 3 while the observational data has a mean of 1 42 2 23 mg n m 3 although observational measurements are an underestimate of true zooplankton biomass the extent of under estimation is difficult to estimate overall the agreement between the emergent patterns of zooplankton and chlorophyll between the model and field data are encouraging the model correctly predicts reduced water clarity in years following high river discharge even well beyond the boundaries of the observed river plumes but over estimates water clarity in most regions fig 9 fig 10 shows the observed relationship between remote sensing optical plume class and in situ water quality from hundreds of opportunistic and routine monitoring observations that coincided with flood events in the great barrier reef between 2007 and 2015 from devlin et al 2015 by randomly sampling simulated water quality within each plume class we have produced comparable plots for model output although the time frames of observations and modelling do not match up exactly and the opportunistic observational data do not provide a complete snapshot of any one individual flood event comparing the system scale results allows an additional model assessment by showing the statistical distribution of water quality variables in flood plumes over periods of several years in each case this comparison reveals that 1 the model and opportunistic observations produce comparable ranges and variabilities of din dip and tss within flood plumes 2 the model tends to under estimate flood plume chlorophyll and light attenuation 3 model and observations show a comparable improvement in water quality reduced sediment and nutrient concentrations with increasing plume colour class 4 discussion we have demonstrated the application of the csps model evaluation framework to a complex biogeochemical model the framework allowed us to identify the aspects of the ereef model suite that agree well with observations and those that require further work it also provided an approach for model development while traditional evaluation focuses on the comparison of observed and modelled state variable concentrations e g chlorophyll a total nitrogen and temperature a more comprehensive model evaluation that formally considers other aspects of model performance provides a more thorough understanding of model performance 4 1 the need for better evaluation of complex models complex biogeochemical models are widely used but are sometimes criticised as being over parameterised relative to the data available for model evaluation mooij et al 2010 one counter to this criticism is that a typical model calibration exercise does not involve tuning every model parameter that represents a biogeochemical process rate or biological trait that may potentially vary rather experienced biogeochemical modellers typically focus their calibration efforts on a handful of parameters that have been identified as both sensitive i e model outputs respond strongly to variations in these parameters and under prescribed i e available information from laboratory or field studies is not sufficient to support specification of an exact value hence a biogeochemical model described as having 100 parameters may in fact have only 6 parameters used for tuning of these some may be tuned within a tightly constrained range informed by a physiological understanding of the process while others may be treated as free parameters better practise in biogeochemical modelling could be to ensure that the selection of parameters used for calibration is fully documented e g janse et al 2010 and perhaps to use an alternative word to describe parameters that are excluded from tuning and treated as constants another frequent criticism of biogeochemical and ecosystem modelling e g arhonditsis and brett 2004 robson 2014 is the frequency with which all or most of the available observational data is used for calibration leaving insufficient independent data for a robust model evaluation colloquially validation while best modelling practise avoids this jakeman et al 2006 robson et al 2008 the constraints of applied modelling outside academia sometimes make best practise impracticable when there are insufficient data to support independent calibration and validation of state variable time series analysis of system level properties especially emergent properties that were not considered during calibration provides an alternative and powerful means of independent model evaluation further with the move toward more observational data being made available in online databases there is the opportunity for greater model evaluation in the future 4 2 emergent properties versus aggregate system properties definitions of the term emergent property in ecosystem science vary and have long been a point of contention salt 1979 argued that an emergent property must be wholly unpredictable from observation of components of that unit if a mechanistic model can predict the occurrence of a system level property then it follows that the system level property does not meet this strict standard as subsequently noted by edson et al 1981 however predictability is always relative we prefer a definition that rests on the emergent system scale relationship being neither an obvious consequence of the low level interactions directly represented in the model algorithms nor a simple aggregation of lower level properties and components by this definition relationships between optical flood plume class and concentrations of nutrients and sediments may not represent true emergent properties as they fail the simple aggregation test though this point is arguable because of the non additive relationship between the inherent optical properties iops of water quality constituents and the apparent optical properties that results in observed ocean colour the relationship between river flow and water clarity in the great barrier reef may be obvious when applied to wet season and near shore conditions but it is not obvious that these relationships should still hold in the dry season and further offshore hence these relationships may be considered emergent according to our definition planktonic community structures emerge as a function of complex relationships amongst many processes simulated by the model including growth of each separate phytoplankton group itself a function of variable intracellular carbon nitrogen phosphorus and chlorophyll concentrations cell size and temperature sinking a function of cell size and density and grazing which varies due to predator prey relationships between the two zooplankton size classes and the two phytoplankton size classes hence we consider phytoplankton community size structures and the shape of the relationship between zooplankton biomass and chlorophyll to be genuine and non trivial emergent properties of underlying model performance 4 3 performance of the ereefs biogeochemical model the evaluation described here revealed both strengths of the ereefs biogeochemical model and areas for improvement as has been previously reported skerratt et al 2019 steven et al 2019 the performance at level 1 state i e the match against state variable in this case water quality monitoring is adequate for its application to a wide range of scientific and management applications and comparable or better than results obtained elsewhere for biogeochemical models of similar scope the deeper evaluation here sheds additional light on model performance 4 3 1 level 1 state the systematic negative bias in simulated dissolved nutrient and chlorophyll concentrations was not evident during the first few years of the simulation herzfeld et al 2016 but appeared over time this suggests that the model is consistently over estimating net nutrient losses or under estimating sources by a small amount because both nitrogen and phosphorus are affected ruling out nitrogen fixation and denitrification as the source of the problem and we do not observe an increase in sediment stores this is most likely a boundary condition issue at present the model relies on climatological data to specify biogeochemical outer boundary conditions as global biogeochemical models continue to improve in skill and resolution they may in future offer an opportunity for improved boundary conditions the observed under estimation of variability in chlorophyll concentrations is to be expected given the relatively course 4 km square resolution of the model under estimating natural variability is common amongst many models refs the model is able to reproduce vertical temperature profiles along a transect see quinlan et al 2019 for a more detailed analysis of model skill in reproducing vertical temperature profiles and also reproduced an observed sub surface chlorophyll maximum albeit with an exaggerated vertical extent probably attributable to the model s limited vertical and horizontal resolution this vertical stretching of the deep chlorophyll maximum also results in the maximum being too close to the surface at least some of the time which has implications for estimated primary productivity further examination of data from glider tracks is indicated 4 3 2 level 2 process this analysis has shown an important potential weakness in the model s simulation of nitrogen fixation rates fig 6 nitrogen fixation in the model is represented as a physiological function of trichodesmium chlorophyll production energy and nutrient requirements baird et al in review robson et al 2013a one possible explanation for the apparent under estimation of nitrogen fixation by the model is that it does not include other nitrogen fixing organisms alternatively the model may not be adequately capturing the dynamics of trichodesmium nitrogen fixation because it omits potentially important factors such as the role of iron in nitrogenase production the variations in trichodesmium colony and filament size and shape and the physical dynamics of surface blooms while only a handful of nitrogen fixation measurements for the great barrier reef are currently available to suggest that this might be a problem it has implications for how the system may respond to changes in catchment nutrient loads and to climate change and therefore warrants further investigation as well as caution in interpretation of model results for this type of scenario 4 3 3 level 3 system the excellent match between the model and observed emergent properties of phytoplankton community structure fig 7 provides confidence that the model s detailed physiological representation of phytoplankton carbon fixation nutrient uptake population growth and chlorophyll production is not over fitted and represents key features of the system well we did not peek at this emergent relationship until after model calibration which was restricted to level 1 state metrics such as direct comparisons of observed and simulated chlorophyll concentrations had been finalised so we are confident that they genuinely emerge from the model structure this provides greater confidence that the model is likely to predict changes in phytoplankton and chlorophyll responses with reasonable accuracy the empirical models provided by brewin et al 2010 and hirata et al 2011 differ markedly in their predictions at low chlorophyll a the ereefs predictions are more in agreement with hirata et al 2011 the difference between the empirical models at low chlorophyll a concentrations is important for energy transfer in the food web as copepods an important component of zooplankton can graze large phytoplankton but not smaller phytoplankton picophytoplankton both empirical models like the ereefs model results show a large scatter around the fit lines the expected relationship between zooplankton nitrogen content and chlorophyll is not as well matched by the model fig 8 this could be due either to model error or observational error if the error is in the model either simulated zooplankton mortality or of nitrogen lost due to sloppy eating in grazing processes may be too low alternatively it could be due to the necessary structural simplification of the model zooplankton is used as a closure term and hence effectively represents the whole food web above it see libralato and solidoro 2009 for further exploration of this point observational error may also play a part net based methods under estimate true zooplankton biomass as smaller zooplankton can pass through the 100 μm mesh and larger zooplankton can escape the net by active avoidance barnes and tranter 1965 the model s ability to represent qualitatively the previously reported system scale relationships between wet season river flow an annual average light attenuation or secchi depth in the mid and outer shelf fig 9 provides confirmation of earlier remote sensing results and motivation for continued river water quality improvement efforts the ereefs hydrodynamic sediment dynamic and biogeochemical models also now provide a tool to explore the mechanism of this relationship impacts of river nutrient delivery on phytoplankton productivity and impacts of fine dust carried with river water may both play a role as may enhanced mixing associated with monsoonal storms that the match between observed and simulated relationships is imperfect with the biogeochemical model tending to over estimate water clarity especially in high flow years has motivated improvements in model structure since the run reported here baird et al in review margvelashvili et al 2018 this includes the addition of a very fine dust sediment class which is carried further from the river mouth the model also over estimates the clarity of water in river flood plumes and tends to under estimate nearshore chlorophyll concentrations figs 3 and 10 which suggests a need for closer examination of ecosystem processes in river flood plumes the observational data suggest qualitatively that chlorophyll may decline with increasing plume class number while the model shows no such decline fig 10 and predicts lower chlorophyll concentrations in flood waters this is likely due to the limited range of phytoplankton functional groups included in the model the large phytoplankton small phytoplankton and trichodesmium groups in the model represent typical characteristics of marine phytoplankton but may not adequately represent the properties of fast growing opportunistic and mesohaline phytoplankton species in the relatively nutrient rich waters of flood plumes work is now in progress to assess phytoplankton community composition in gbr waters in flood conditions and compare it with phytoplankton community composition during the dry season this will inform future model developments fig 10 also reveals unexpectedly high din and dip concentrations in waters classified as optical plume class 5 this suggests that either the biogeochemical function or the optical colour of water in this plume class towards the outer edge of flood plumes is not well represented by the model in particular there may be an interaction between the under prediction of chlorophyll in flood waters and an under prediction of dissolved nutrient uptake by phytoplankton 5 conclusions the csps model evaluation framework demonstrated here provides a more systematic way than traditional model assessment approaches to consider multiple aspects of model performance this framework may help to improve the evaluation of any complex mechanistic environmental model especially where the number of free parameters the computational requirements of the model or the sparsity of data limit the application of robust parameter estimation uncertainty analysis and or independent model evaluation procedures unlike some other approaches the framework described here does not require a large number of model runs and it can make use of sparse data that may not be precisely aligned with model outputs in space and time in application to the ereefs biogeochemical model this approach has shed light on both strengths and weaknesses of the model providing confidence for a range of applications while also demonstrating a need for caution in some applications and setting directions for future model improvements in particular level 0 concept evaluation showed that to achieve end user acceptance it was important to include additional sediment size classes a more detailed seagrass model coral metabolism ocean acidification and nitrogen fixation these were all implemented in the final version of the model in direct response to concept evaluation of the pilot model level 1 state evaluation showed that the model is fit for purpose with respect to application to regional scale land use change scenario applications and can be used to complement in situ measurements especially in the context of monitoring seasonal and inter annual variations however the model does not accurately predict variations in water quality at smaller timescales hourly to daily and should not be used in applications where this capability is required in addition end users should be aware of a negative bias in nutrient concentrations i e the tendency of the model to under predict observed values currently ongoing model development is intended to reduce this bias and further improve level 1 metrics level 2 process evaluation showed that the model very likely under estimates nitrogen fixation further process rate measurements are needed to confirm this and provide the data needed to correct it although this evaluation was limited due to a paucity of process rate measurements improved understanding of nitrogen fixation in the gbr is highlighted as a priority to support improved predictive capability level 3 system evaluation showed that the model is able to predict system scale properties correctly including the relationship between phytoplankton cell size and chlorophyll the evolution of water quality in flood plumes and the existence of a negative relationship between river discharge and gbr photic depth the model is therefore fit for purpose as an aid to understanding large scale processes and patterns in the gbr discrepancies in the details of these patterns including the slope of the relationship between discharge and photic depth and the slope of the curve relating zooplankton to phytoplankton concentrations point to additional areas for improvement specifically we are investigating improvements in the parameterisation of inherent optical properties in the optical submodel and the specification of zooplankton traits in the grazing submodel given sparse in situ observational data for photic depth and zooplankton concentrations these conclusions would not have been possible without the system level evaluations software and data availability the csiro ems hydrodynamic sediment and biogeochemical model software is available open source via https github com csiro coasts ems detailed science and user manuals can be found at https research csiro au cem software ems ems documentation both near real time and archived model outputs for the ereefs application of ems are made freely available in standard netcdf format via https research csiro au ereefs to facilitate third party scientific and technical applications to support extraction plotting and analysis of netcdf model outputs example matlab scripts are available on the ereefs info website and an r package is available via https github com aims ereefs model outputs are also available as prepared visualisations for policy end users through http aims ereefs org au aims ereefs https aims ereefs org au aims ereefs this paper refers to imos observational data which are available via https portal aodn org au and to water quality data from the marine monitoring program detailed descriptions of which are available through https eatlas org au rrmmp gbr aims inshore water quality for this paper we present results for the 4 km resolution ereefs biogeochemical model version 2 0 run reference h2p0 b2p0 chyd dcrt an explanation of the naming convention and a description of the boundary conditions and model version used for this run is given at https research csiro au ereefs models models about biogeochemical simulation naming protocol declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements ereefs simulations were developed as part of the ereefs project http ereefs org au ereefs a public private collaboration between australia s leading operational and scientific research agencies government and corporate australia integrated marine observing system imos supplied imos mooring data imos is supported by the australian government through the national collaborative research infrastructure strategy and the super science initiative the marine monitoring program mmp managed by the gbr marine park authority with funding from the department of the environment and energy and co funding from research partners supplied mmp moorings and tri annual water sampling which was conducted by the australian institute of marine science james cook university university of queensland queensland parks and wildlife service reef catchments and community volunteers we thank our many colleagues involved in developing the ereefs model particularly rob ellis and david waters for the catchment modelling as part of the queensland and australian government s paddock to reef program that is funded by the queensland department of natural resources and mines queensland regional natural resource management investment program 2013 2018 with support from the department of science information technology innovation and the arts dsitia this analysis was funded by the australian institute of marine science and csiro with contributions from the imos zoom zooplankton ocean observations and modelling task particular thanks also to our colleagues jessica benthyusen miles furnas david mckinnon john andrewartha philip gillibrand ruth eriksen jonathon hodge sharon ticknell richard brinkman and cedric robillot for their contributions to ereefs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104707 
26021,a general formulation of a surface water hydrological model is posed that enables multiple numerical schemes model structures discretization schemes and interpolation approaches to be tested compared and or used for assessment of structural uncertainty and algorithm skill this formulation is the basis for the object oriented and open source hydrological modelling framework raven raven is uniquely capable of fully emulating and then modifying or extending a number of existing hydrological models of differing design including the ubc watershed model gr4j mohyse hmets and hbv ec more than 100 compatible process algorithm options and more than 80 interchangeable options for both routing and the estimation of forcings support the testing of trillions of possible model configurations it supports general spatial discretization including grids subbasin hydrologic response units lumped or triangulated irregular network models raven s utility for examining the impact of model choices and sensitivity to structural model uncertainty is demonstrated with a set of simple test cases graphical abstract keywords watershed model distributed hydrological modelling modelling framework hydrology object oriented surface water models software availability the raven hydrologic modelling framework along with tutorials documentation source code utilities and example files is freely available for download at http www raven uwaterloo ca 1 introduction computational hydrological models under continuous development for the last half century have become increasingly complex and varied in their description of the hydrologic cycle the sophisticated physics of mixed saturation soil water flow evapotranspiration snow melt overland flow and heat transfer at multiple interacting time and space scales often precludes representation using simple physical laws beven 1989 beven and germann 2013 rather when applied at the basin scale a component based approach is generally applied each individual hydrologic process e g evapotranspiration infiltration etc can be represented using any number of mixed physical statistical and empirical process models developed in the literature these process models are here formally defined as algorithms that relate fluxes of mass or energy between or within storage units to the current state of the hydrologic system many of these process models are site or terrain specific many are only valid for hourly or daily or monthly estimates and many have a considerable number of parameters which are typically unknown unmeasurable or both all process models have a significant number of assumptions associated with their use the net effect of this variety of plausible descriptions of hydrological processes is the existence of dozens of hydrological models regularly used in practice singh and woolhiser 2002 documented more than 70 in 2002 and this number has risen since while many of the process algorithms used in these existing models have been individually validated against data from tightly controlled field or laboratory experiments the cumulative effects of multiple interacting non linear processes in the real world are still far from being replicable let alone wholly predictable likewise it is recognized that the unknowable nature of the phenomena occurring at scales of interest introduces a significant amount of structural uncertainty that is uncertainty in model predictions linked to uncertainty regarding the description of the hydrologic system there is a recognized need clark et al 2015 weiler and beven 2015 mendoza et al 2015 addor and melsen 2019 for flexible and modular models which can be used to assess the impacts of structural uncertainty wagener et al 2001 test hypotheses about system functioning clark et al 2011a b euser et al 2013 estimate the uncertainty linked to structural inadequacy clark et al 2008 identify optimal model configurations marshall et al 2005 and support stepwise conceptual model improvement supported by available data fenicia et al 2008 such modular models may also be used for identification of best practices for discretization interpolation porting of parameters and other testing of the various choices made in application of hydrologic models while some recent progress has been made in the development of flexible models primarily for research applications clark et al 2008 fenicia et al 2011 clark et al 2015 these developments have primarily focused on providing flexibility in model structure for example both fuse clark et al 2008 and superflex fenicia et al 2011 can represent numerous conceptual box models of surface water processes enabling different model structures and structural combinations the recently developed summa modelling framework clark et al 2015 additionally supports physically based continuum models of hydrological phenomena including e g richards equation for redistributing shallow soil moisture to date no model allows for the vast array of individual model choices beyond model structure to be individually or collectively assessed nor do the existing flexible model frameworks support contaminant transport reservoir routing or a number of other features often deemed necessary in operational hydrologic modelling for forecasting or water management however all of these various choices and augmentations influence the performance adequacy and skill of such models and therefore it is additionally desirable to provide flexibility in the methods used for spatial and temporal discretization forcing function interpolation orographic correction rain snow partitioning in catchment in channel reservoir routing and other critical model choices one solution to these issues is to implement a flexible hydrologic modelling framework that can support and help to test a wide variety of modelling paradigms while avoiding the pitfalls of excessive abstraction this paper outlines the general structure of such a framework and its implementation as raven specifically this paper reports on a number of critical issues that impact the development and operation of a modular hydrological framework demonstrates the ability of the raven code to emulate a number of popular existing models and deploys the raven framework for basic assessments of sub model and discretization choice 2 background current modular frameworks existing hydrological models range from fully spatially distributed 4 d finite element simulators e g mikeshe abbott et al 1986 inhm vanderkwaak 1999 or parflow ashby and falgout 1996 to simple lumped tank models of an entire watershed e g the sacramento model burnash et al 1973 they may be spatially discretized using a cell based approach for use with digital elevation models as done inde roo et al 2000 or using a subbasin approach via the concept of a representative elementary area rea wood et al 1988 the more common approach used in swat arnold et al 1998 hspf bicknell et al 1997 agnps young et al 1989 etc because of this variability in model structure and implementation it is exceedingly difficult to compare and contrast results between models smith et al 2004 and often challenging even to simply select an appropriate model for any given purpose flexible codes which can support a range of modelling approaches of different complexity can afford a user a direct mechanism for comparison of model choices and can likewise support a range of complexity where simple and fast conceptual models can be used for data poor systems or preliminary watershed assessment and fully distributed models can be deployed for more complex problems supported by richer data sets likewise flexible modelling codes can support place based customization in the same model potentially using different physical representations of the water cycle on different landscapes lastly they can have the capacity to support stepwise modelling whereby models are increased in complexity only when justified by the available data and where added complexity leads to higher predictive skill and or system understanding various researchers have developed flexible modular or object oriented hydrologic modelling codes which rest upon the inherent modularity of hydrologic processes the general evolution over the last 30years has been from modular models with some plug and play support to fully realized modular modelling frameworks the first such model is likely prms leavesley et al 1983 leavesley and stannard 1995 a fortran code where individual modules correspond to specific hydrologic processes algorithms which could be interchangeably used or excluded by the user the implementation approach of prms was to have each module directly modify the relevant state variables during each time step at the end of the modules operation all local and global mass balance constraints were resolved while this approach is successful at small time steps processes are necessarily treated as serial rather than concurrent in addition the order of processes could not be altered and both the model discretization approach and structure were fixed as is typical for most hydrological models however with the successor to the prms model mms leavesley et al 2002 a good deal more attention was paid to the abstraction of hydrological processes with the intention of understanding the differences between subprocess models and means of iterating between modules during a timestep were included more recently clark et al 2008 developed a software package fuse for lumped surface water modelling which is able to mix and match process models for ensemble basin simulation and therefore directly investigate the impact of structural error a descendent of the modular rainfall runoff toolkit rrmt wagener et al 2001 the degree of abstraction was here improved because the fuse model was intended to emulate 4 distinct lumped model structural representations and the various permutations and combinations of those models this fuse model was later used to test the impact of different numerical methods on model calibration and the interpretation of model output clark and kavetski 2010 kavetski and clark 2010 fuse has since been superseded by superflex fenicia et al 2011 and the more physically based summa land surface model clark et al 2015 the cold regions hydrologic model crhm ofpomeroy et al 2007 is another recent attempt at generality based upon the process module approach with an emphasis upon a toolkit approach for snow modelling in smaller basins crhm is quite adept at handling a variety of methods of lateral transfer between landscape units and therefore provides flexibility in both model process representation and hru discretization the framework ofcoxon et al 2019 provides similar flexibility in spatial discretization but without comparable structural flexibility other examples include the topography based modelling package of wang et al 2005 the generic object modelling system oms kralisch et al 2005 the openmi component based approach ofcastronova and goodall 2010 the parallel framework seims zhu et al 2019 and the rapid development framework ofkneis 2015 as noted bykneis 2015 the use of a modular framework rather than a specific hydrological model is needed for application to a range of problems also as noted bymendoza et al 2015 the availability of a wide range of model options helps to avoid unnecessary constraints upon model agility the raven software framework builds upon some of these earlier successes and is able to replicate many of the features of these modular models and supplement them with additional unique functionality as will be discussed below 3 formulation here the structure of a generic lumped semi distributed distributed hydrological model is uniquely formalized this formalization is used as the basis for a modelling framework that is compatible with the wide range of process representations model structures discretization approaches parameterizations and numerical algorithms that can be found in individual hydrological models many of these structures closely echo and further build upon the blueprint offenicia et al 2011 but the means by which processes forcing function generation and the global local numerical algorithm are organized and exchange information are unique this formalization is implemented in the object oriented open source software code raven which was developed using the c programming language a number of design philosophies were adhered to during the construction of raven objectives were to 1 separate the global numerical solution algorithm from the individual process model representations in order to allow for improved runtime and model simulation performance 2 be able to plug and play different algorithmic representations of the same physical process for inter model comparison hypothesis testing clark et al 2011a and ensemble simulation vrugt et al 2005 3 be capable of emulating multiple existing hydrological codes model approaches for benchmarking inter model comparison improvement and evaluation purposes this includes both continuum submodels e g richards equation for soil moisture redistribution and compartment based models 4 formally and universally distinguish between state variables forcing functions model parameters and derived parameters 5 accommodate various methods of lateral and vertical spatial discretization in a general fashion e g grid cell land surface tile triangulated irregular network tin or basin representation 6 accommodate generic temporal discretization schemes for individual processes including variable time stepping and sub stepping 7 handle standard treatment of conceptual model routing e g convolution and transfer functions fenicia et al 2011 8 accommodate specification of as large or small a number of parameters as needed all parameters may be either directly specified by the user some may be auto generated from empirical relationships e g pseudotransfer functions all parameters can be fixed scalars or user specified time series 9 accommodate a wide variety of interpolation autogeneration orographic correction and downscaling schemes for applying gridded netcdf or gauge interpolated model forcings such as precipitation temperature and radiation models should be able to run with as little information as daily precipitation and temperature but also have the capacity to use rarer data sets such as user specified relative humidity and or longwave radiation when available 10 ensure that the model is completely deterministic i e the state of the system at any point in time wholly dictates its future state as is consistent with a mathematical initial value problem 11 support advective transport of constituents e g contaminants nutrients or tracers without having to modify transport code with the addition of new hydrologic process algorithms 12 find a balance between over abstraction a common pitfall of some modular frameworks and systems models and under abstraction such that the framework is flexible and powerful but not at the expense of usability extendability raven enables the investigation of a number of troublesome issues in the science of hydrologic modelling for example the framework is supportive of the systematic scrutiny of competing hypotheses advocated byclark et al 2016 whereby individual model hypotheses or algorithms are explicitly assessed for robustness and appropriateness with raven this scrutiny of model choices may further be extended beyond model structure to interpolation forcing specification or discretization choices as inhaghenegadar et al 2015 orliu et al 2016 as such raven has been applied to evaluate different means for constraining flow partitioning in hydrologic models shafii et al 2017 for evaluating swe estimation methods yao et al 2018 and evaluating model workflow choices chernos et al 2017 3 1 spatial discretization no general model framework can be completely flexible in its representation of the hydrologic cycle without sacrificing usability some basic ground rules need to apply in order to simplify code development avoid over abstraction and ensure model consistency most of these ground rules may be established by fixing the means of model spatial discretization i e determining the level at which parameters state variables lateral vertical processes and forcing functions are assumed to be homogeneous and determining the level at which lumping of fluxes and lateral transfer occur notably these decisions are often implicit in hydrological model development but determining the actual choices made in individual models typically requires close investigation of the source code the fundamental spatial discretization approach used in raven is depicted in fig 1 and is consistent with most modern semi distributed surface water models in raven a watershed scale model is composed of a number of subbasins through which water is routed laterally downstream on the land surface and laterally between subbasin groundwater reservoirs each subbasin contains a stream river channel discretized into a user specified number of reaches each subbasin is subdivided into one or more contiguous or non contiguous hydrological response units hrus where the vertical water and energy balance is applied producing runoff interflow and baseflow that is aggregated and distributed along the reach and the recharge that is aggregated and deposited to the subbasin groundwater reservoir these hrus are assumed to have a homogeneous response to external forcings a specified vertical soil profile and unique membership in land use terrain aquifer and vegetation classes these class memberships uniquely define the physical properties of the hru hrus possess their own set of state variables such that variables such as soil moisture may be tracked at the resolution of hrus state variables may be statistical in nature representing the key moments of storage quantities for example or represent only part of an hru e g wetland storage may only correspond to the 30 of the hru covered by wetlands but explicit spatial locations within the subbasin should be functionally irrelevant a notable exception is the case of lateral transfer driven by topography e g the uphill region of an hru could transfer water to the downhill via some process but explicit location or direction cannot be considered forcings such as precipitation or radiation are likewise hru specific and can be generated in any number of ways see section3 5 note that this structure allows for the basin modelling approach of e g swat neitsch et al 2001 fig 1a the cartesian cell based structure of atmospheric land surface schemes such as class verseghy 1991 fig 1b a lumped basin approach such as in gr4j perrin et al 2003 fig 1d or a triangular irregular network discretization fig 1c note that this approach fully supports the goals ofclark et al 2015 with regard to arbitrary spatial discretization albeit in a somewhat different manner 3 2 mathematical structure since the computational simulation of a hydrologic system is fundamentally a mathematical problem the underpinnings of the software outlined here are constructed based upon a formalization of this mathematical problem while often ignored or circumvented in both software documentation and hydrology textbooks the problem being solved by all hydrological models is that of a coupled system of ordinary and partial differential equations odes and pdes within each computational element hru in this case it has been shown kavetski and clark 2010 kavetski et al 2011 that not giving due consideration to the numerical solution approach in hydrological models can lead to issues in model calibration and performance these odes and pdes individually describe either 1 the accumulation of mass or energy within a given storage compartment or continuum i e a mass or energy balance or 2 the temporal change in some auxiliary system property e g density crop height or albedo these parameters are collectively the dependent variables of the problem here referred to as either primary state variables those corresponding to mean storage of intrinsic properties such as mass or energy or auxiliary state variables those corresponding to extrinsic properties higher order statistical moments of intrinsic properties etc another category derived variables is here reserved for parameters which depend upon the current state of the model but can be calculated directly from a finite set of primary and auxiliary state variables for example snow depth a derived variable may be calculated from the combined knowledge of the snow water equivalent a primary variable representing water storage and the snow density an auxiliary variable at any snapshot in time as should be clear from this example the distinction between derived and auxiliary variables is not unique and the two are ontologically interchangeable the mathematical problem statement solved in hydrology is often obfuscated in the literature by the mixed communication of the mass energy balance problem statement couched in terms of the solution algorithm as noticed byclark and kavetski 2010 for example the accumulation of water in a closed pond is most appropriately expressed as a continuous ode t s t p t e t where s is the water storage in the pond an implicitly positive quantity p is the precipitation rate and e is the evaporation rate all are continuous functions of time as they are in nature and are here stated in the state space formulation ofclark et al 2008 many texts describe the problem pre cast as a numerical solution over a discrete time step δ t e g s i s 0 δ t p i min s 0 δ t e i where now the implicit constraint that the storage is positive is made explicit an advantage but the ability to choose the most appropriate numerical algorithm for solution as part of a coupled system of odes has been removed a marked disadvantage this is worsened by the common implicit assumption that the time step is in days in fact many common process and routing models e g muskingum routing are strictly defined in terms of their numerical algorithms rather than their implicit mathematical formulation this makes it difficult but still possible to mix and match them with other processes under a single consistent representation here in every hru each state variable is subject to the influence of a number of hydrological processes increases or decreases in a primary state variable are simply the additive combination of influx or outflux terms i e the ode or pde corresponding to a primary state variable is built up from mass or energy balance considerations increases or decreases in auxiliary variables are likewise assumed to be written as the additive combination of terms we can therefore write an individual differential equation for the change in the j th state variable of n s total state variables ϕ j as 1 ϕ j t p 1 n p i 1 n s k 0 n c p m i j k ϕ p f where m i j k is the change in state variable j due to subprocess k each of the n p processes can have n c p sub processes which is linked to another state variable i this linkage typically communicates flow direction e g a subprocess m i j k moves mass or energy from storage compartment i to compartment j a subprocess m i i k i e i j represents an independent rate of change for an auxiliary variable and does not connote exchange of mass or energy between compartments though simple processes e g evapotranspiration from topsoil to atmosphere may be represented by just a single sub process moving water from one state variable to another more complex processes e g snowpack evolution may move mass and energy between multiple compartments and are represented by a matrix of constituent sub processes m p m i j k the fluxes or rates of change returned by each set of sub processes are a function of the current vector of state variables ϕ system parameters p and forcing functions f the full system of equations describing the influence of all processes in an hru can be written in matrix form in a manner similar tokneis 2015 2 ϕ t m g 1 p 1 n p m p 1 where ϕ is the complete vector of state variables m g is a n s x n s global matrix of composite rate of change functions and 1 is a column vector filled with ones the global process matrix m g is the sum of contributions from each individual process matrix i e m g m p the individual functions m i j k may be linear or highly non linear all of these functions return representative rates of change over the global time step δ t any full hydrologic model structure can be assembled from its constituent processes described by the process functional matrix m p as seen in fig 2 the antisymmetric matrices in fig 2d and e correspond to the individual processes of infiltration runoff and precipitation each of which work on a different subset of state variables ponded water p surface water sw and topsoil s in the case of infiltration atmospheric precipitation a depression d canopy c and ponded water in the case of precipitation the summation of the two matrices fig 2f represents the combined influence of both processes this notation is useful for a number of reasons first of all for processes which only operate on primary state variables the function matrix m k for that process has to be anti symmetric i e m i j k m j i k i e mass or energy balance is perfectly preserved by requiring that the mass entering storage compartment i is equivalent to that leaving storage compartment j likewise for the same reason diagonal terms of the matrix which correspond to mass storage variables must be zero m i i k 0 equivalent to the statement that mass or energy cannot be created or destroyed it must have an external source or sink the notation above also simplifies the description of global numerical algorithms which control the coupling of individual processes over a time step the explicit euler algorithm can be written as 3 ϕ n 1 ϕ n δ t m g t n ϕ n 1 where n denotes the current time step because of the presence of constraints on state variable values the euler method is typically not used in hydrology instead numerical hydrological models are typically solved using an ordered series approach where processes that physically occur in parallel are mathematically treated as if they occur in series over the time step snowdon 2010 i e ϕ ϕ n δ t m 1 t n ϕ n 1 ϕ ϕ δ t m 2 t n ϕ 1 4 ϕ n 1 ϕ δ t m n p t n ϕ 1 where ϕ is an intermediate vector of state variables using this ordered series approach the order of operation is inevitably hard coded into hydrologic modelling software packages the notable exceptions being those of fuse clark et al 2008 superflex fenicia et al 2011 and summa clark et al 2015 which can use a variety of implicit and explicit numerical algorithms note that the numerical methods used to calculate individual process matrices may be more sophisticated than this for example within raven vertical redistribution of water in a discretized soil column may optionally be simulated using an iterative finite volume solution to richards equation however as the process matrices are quite generic there are no constraints upon how individual terms of those matrices are defined or calculated this flexibility enables the use of finer temporal discretization to resolve faster or more highly coupled processes without burdening the rest of the simulation it is also important to note that this formalization supports both a conceptual type and continuum type representation of hydrologic processes as advocated byweiler and beven 2015 in addition to the euler and ordered series approaches indicated above raven also supports an iterative predictor corrector method for solving the set of odes snowdon 2010 3 3 hydrologic process representation some hydrologic processes can be simply treated as the movement of water mass or energy from one storage compartment to another for example evaporation from open water is a well understood physical phenomenon which moves water from one storage compartment e g a lake to another the atmosphere the process alone is simply described by two symmetric ordinary differential equations the net accumulation of water in the atmosphere due to evaporation is equal to the net loss from surface storage and is primarily a function of net incoming radiation unfortunately many processes are too coupled to be represented in such a simple fashion the snowmelt process for example directly influences snow depth density liquid water content cold content temperature and possibly additional state variables these variables have multiple constraints e g positivity of depth negativity of temperature upper and lower bounds on water content and their interrelationships are highly coupled e g melt directly causes corresponding changes in depth density and heat energy the corresponding process matrix m melt is therefore quite complex in structure containing both diagonal and off diagonal terms some of which are individually quite complicated however due to the significant coupling between the concurrent subprocesses of ripening shrinking warming etc decoupling a snowmelt model into simpler problems i e into smaller process matrices has the potential to lead to significant numerical error in the raven framework process algorithm code is not allowed to modify state variable values only return what the desired modification would be when provided information about the initial state of the system this decoupling of process calculation and state variable updating is an essential ingredient for separating process algorithms from the global algorithm for example with this approach the process algorithm may be applied repeatedly in an iterative fashion to account for the influence of simultaneous processes i e the implicit approaches advocated bykavetski and clark 2010 this approach also accommodates multiple numerical methods by allowing process fluxes to be independently estimated at the start and end of the time step with the same code lastly the approach truly encapsulates individual process algorithm code which is critical for extendible development raven currently supports over 100 hydrologic process algorithms documented in the user s manual craig and the raven development team 2019 a subset of these algorithms are included in table1 uniquely it has been designed to support conditional application of these processes via control through the input file e g different infiltration algorithms may be applied in different parts of the watershed depending upon land use or soil profile type this is in support of the place based hydrologic representation discussed bybeven 2000 ormcdonnell et al 2007 an example of the assembly of a model from process algorithms in the raven input file is shown in fig 3 each command indicates the process name the algorithm the set of from compartments and the set of to compartments note the conditional application statements used to locally apply some hydrologic processes in this case the conditional ensures different processes are applied to glacier hrus 3 4 parameter assignment and granularity it is often recognized that as models move from lumped to distributed the data needs to parameterize the model and necessary preprocessing to meet these needs of a model increase dramatically in raven this is somewhat alleviated through the use of hru classification schemes each hru in addition to being defined as having a unique representative area centroid slope aspect and elevation is assumed to belong to a unique set of non overlapping classifications listed and briefly defined in table2 the benefit of this classification approach is that parameters need only be specified on a class by class basis rather than an hru by hru basis simplifying the mechanics of calibration each parameter shows up once in the model input files the data storage parameters are linked to class instances rather than directly to the hru and improving the portability of parameters to ungauged basins this also enables examination of discretization and parameter assignment choices as done by e g haghenegadar et al 2015 the hru specific parameters slope aspect area latitude and longitude are part of the hru definition as these are not portable between hrus as classified parameters may be this classification approach is more obviously beneficial for physically based modelling approaches than conceptual box models as parameters have physical meaning that can port from one place to another however it does not limit conceptual models which may have watershed specific parameters that could be mapped to watershed specific vegetation and soil classes nor does it limit the users ability to locally adjust parameters in a distributed model if needed in the extreme though likely unnecessary case each individual hru could be assigned its own vegetation soil land use class and parameters would then be fully distributed a unique feature of raven is that each and every model parameter can if desired be linked to a user specified time series i e all parameters can vary temporally this can be used for e g land use change studies or to represent the impacts of seasonal variation in vegetative cover wagener et al 2003 have shown the value of being able to use time varying parameters as a diagnostic tool in the identification of model structural deficiencies 3 5 forcing function estimation and interpolation forcing data e g precipitation incident radiation etc is distributed to hrus using either by area weighted mapping of gridded data in netcdf format to hrus or by using a generic form of interpolation between gauge stations 5 f x c y c t i 1 n g ω i x c y c f i t where ω i is the interpolation function associated with the i th gauge f i is the orographically corrected forcing function value at gauge i of n g gauges and x c is the hru centroid this is analogous to the approach ofkneis 2015 typically the constraint that ω i 1 is required generalized in this fashion this easily accommodates standard interpolation algorithms such as inverse distance weighting nearest neighbour kriging or radial basis functions the first two are internally supported by raven it also reduces the storage requirements for forcing inputs which if generated externally may require time series of each forcing function for each hru over the entire model duration while it is still possible to use such an approach the data requirements for standard model application are relatively small raw forcing data required by raven includes at the very least time series of total precipitation and minimum maximum or average daily temperature either in gridded netcdf format or at the set of gauges raven includes a suite of algorithms for determining related forcings potential evapotranspiration potential melt shortwave radiation longwave radiation snow rain partitioning cloud cover relative humidity wind speed etc from these basic data products a subset of these algorithms are indicated in table3 while there are multiple software products that have options for switching between evapotranspiration methods the authors are not aware of any other modelling framework which provides the degree of forcing generation flexibility made available through raven and enables the explicit testing of the impact of these modelling choices on model output generation of synthetic forcing functions and filling in data gaps in observed meteorological records are tasks that are appropriately handled using preprocessing utilities as such raven has only limited support for this type of functionality 3 6 temporal discretization because the global solution algorithm is wholly decoupled from the individual process algorithms raven hydrological simulations are characterized by two independent levels of temporal discretization at the top level a user specified global time step is used this time step is typically but not necessarily fixed at one day or smaller intervals at the next level down individual process algorithms can use smaller independent time steps to either meet stability constraints e g courant or peclet constraints or adaptively react to occurrences that warrant finer temporal discretization e g storm events or other rapid changes in forcings this formalism minimizes the need to explicitly address complicated temporal discretization when building simpler process models while still accommodating complex processes which by definition require fine temporal discretization this approach also minimizes computational cost by only requiring fine temporal discretization for processes whose numerical stability or accuracy require it note that this approach requires the inclusion of metadata which indicates whether algorithms are appropriate for sub daily simulation a critical consideration for any model with variable time stepping is the allocation of meteorological forcing data here all forcing functions e g rainfall incoming solar radiation or pet are specified as a time series of piecewise constant rates rather than linearly interpolated between times of measurement the temporal discretization of these time series are independent of the model time step i e forcing data does not have to be pre processed to conform to the temporal discretization of the model as is required by many hydrologic models rather raven handles the mapping from the temporal discretization of the data to that of the model minimum maximum or average values over any given time interval may be extracted from the time series data by any individual process or at the global algorithm level the only requirement is that the duration of the time series covers the entire duration of the simulation 3 7 routing to facilitate flexibility in choice of routing algorithms the routing process was generalized to both accommodate the functional form of the quickflow baseflow output from the constituent hrus an average flow rate over each time step and to consistently represent the state variables of volumetric flow along the basin reach as with hru state variables the flows from each channel segment are assumed to represent instantaneous values at the start of each time step and are not time step averaged quantities despite the apparent inconsistency with the hru water energy balance where volumetric fluxes are piecewise constant averages over each time step this approach has a number of advantages firstly it is common practice in flood routing to treat q t as the dependent variable most algorithms rely on this second it is more common to have flow measurements than measurements of total reach water storage the alternative and ostensibly more consistent state variable for routing the transition from piecewise constant to piecewise linear fluxes from the hru water balance to the channel water balance is eased in part by considering in catchment routing in channel routing and reservoir routing separately the in catchment and in channel routing model is visually described in fig 4 reservoirs lakes may be added to the outlet of any channel reach and are simulated using a level pool routing scheme with support of temporally variable reservoir outflow relationships and user specified reservoir withdrawals which can mimic reservoir operations in catchment routing is predominantly handled using a unit hydrograph uh approach where the uh for each catchment is either user specified or generated from basin characteristics the immediate history of quickflow and baseflow output to surface water is stored in memory as an array of time step averaged outflow rates to off channel tributaries q l a t the duration of this history is determined by the subbasins time of concentration t c to transfer this water to either the channel segments within the subbasin or the subbasin outflow the pulse hydrograph is convolved with the unit hydrograph represented as a piecewise linear function water and energy is transferred to the downstream ends of channel segments within the reach in the simplest case a dirac hydrograph may be used to dump the water directly into the channel without a time lag in channel routing for each time step is assumed to be completely characterized by a finite history of upstream inflow stored as a vector of flow values at fixed time intervals of δ t q i n and the outflow at the start of the time step the duration of this history is determined by the minimum flood celerity and the length of the reach segment during each time step moving from upstream to downstream at both the watershed level basin to basin and subbasin level reach segment to reach segment a generalized routing algorithm is used to generate the outflow from each reach based upon the time history of upstream inflows i e 6 q o u t n 1 f r o u t e q o u t n q i n p s where f r o u t e is the routing algorithm p s is a vector of channel parameters typically a number of stored channel rating curves primary channel and bank roughness and if applicable weir or reservoir relationships this formalization supports both common lumped and distributed flow routing methods that dictate the form of f including muskingum cunge lag and route level pool transfer function unit hydrograph convolution or diffusive wave approaches all of which raven currently supports if desired a more complex kinematic wave approach may later be implemented in this framework but it cannot support hydraulic routing with significant backwater effects notably sub time stepping for routing is also enabled with this formulation if a lake or reservoir is included at the end of the reach not shown in fig 4 the non linear reservoir mass balance is exclusively solved using newton s method after the upstream flows for the time step are calculated time variable minimum maximum and target constraints on stage and flow may be imposed to represent reservoir operations as can specification of downstream irrigation demand 3 8 transport to the authors knowledge raven is the only hydrologic modelling framework that supports general transport of user defined constituents through the watershed though fuse was modified for support of tracers mcmillan et al 2012 but without routing the advective transport model is incorporated in the code in such a way that the water balance simulation is wholly independent of the transport simulation when new hydrological process algorithms are added they automatically support advective transport this is because the transport algorithm is built upon a completely abstracted treatment of in hru in catchment and in channel routing of water and mass between storage compartments only the fluxes and changes in water storage volumes are needed over a time step to resolve the advective fluxes of mass the in hru advective transport solver currently works exclusively with the ordered series flow solver eq 4 and relies upon operator splitting after all of the fluxes m i j k between all in hru storage compartments are calculated for each time step the following fully upstream weighted advective transport problem is solved 7 m j t p 1 n p i 1 n s k 0 n c p min m i j k 0 m j ϕ j max m i j k 0 m i ϕ i for j i where m i is the constituent mass density in mg m 2 in water storage compartment i n p n s n c p and ϕ j are defined as ineq 1 the first term in the summation represents mass gains from compartment j m i j k is negative the second represents losses from compartment i as in the ordered series approach this mass transfer is applied piecewise in the same order as the flow calculation but in this case one flow exchange at a time 8 m i m i δ t j i j m j m j δ t j i j where m i is the mass in storage compartment i after the single mass transfer connection has been considered and the mass flow from unit i to unit j j i j mg m 2 d may be calculated as j i j m i j k m i ϕ i if m i j k 0 m j ϕ j if m i j k 0 where m i m j ϕ i and ϕ j represent mass and storage just prior to the water mass transfer in the ordered series approach volumetric concentrations of constituents in mg l may be calculated as c i α m i ϕ i where α accounts for the units conversion note that this approach accommodates any flow model representation without intervention even those process algorithms yet to be implemented in raven it is also wholly mass conservative is unconditionally stable and can readily handle storage compartments drying out but consequentially is subject to a higher degree of numerical dispersion than e g an implicit solution toeq 7 water storage compartments can have fixed concentrations or user specified mass fluxes applied in channel routing is handled using a transfer function approach if the flow routing is also using transfer functions e g the analytical diffusive wave approach for other in channel routing methods advective transport is not supported the utility of the transport tool is significant for instance baseflow may be tracked in the model by tagging the groundwater stores with an inert tracer the transport tool has likewise been used to discriminate between rainfall sourced and snowmelt sourced contributions to streamflow and to determine the relative proportion of streamflow generated by different parts of the landscape development of algorithms for supporting more complex transport mechanisms such as dispersion diffusion sorption and geochemical reactions is ongoing 4 software implementation raven is a fully object oriented code written in c it successfully compiles for 32 bit and 64 bit windows application and on both macos and linux systems the code is open source under artistic license 2 0 and is available at http raven uwaterloo ca the class structure for the object oriented software is depicted in fig 5 the most important container class is the cmodel class of which there is only one instance the cmodel class houses all of the information regarding the global numerical algorithm the lists of state variables and processes of interest the meteorological gauge data of class cgauge and the collections of subbasins of class csubbasin and hrus of class chydrounit which define the spatial extent of the problem the watershed model also housed in the cmodel class operates by for each time step first updating all forcing functions and sifting through the constituent hrus in the model and applying all hydrological processes included in the model effectively solving the set of odes and pdes defined byeq 1 this is followed by separate solution of the in catchment and in channel routing equations to move water downstream mass balance checks and updates are applied in each time step as are storage of intermediate diagnostics e g model output to be compared to observations after simulation has commenced a solution file is written to output which represents the full current state of the system and can be used as initial states to resume a model run this process is depicted in fig 6 4 1 memory and speed considerations hydrological models have a tendency to be memory intensive for a number of reasons the raven framework has used a number of approaches to avoid this internal memory only stores the state of the system at the start and end of the current time step therefore the total memory storage is roughly o n h r u 2 n s v n f f where n f f is the fixed number of forcing functions n h r u is the number of hrus and n s v is the number of state variables in the model many models internally store hydrographs for the entire simulation duration which propagate downstream model forcing input is in the form of time series specified at gauge locations and is propagated to hrus using a weighting scheme if needed direct specification of forcings can be applied on an hru by hru basis by generating a gauge for each hru however this will increase computational cost in the case where gridded forcings are provided in netcdf format an adaptive buffering scheme is used to reduce i o and storage requirements recognizing that much of the computational and memory cost of hydrological models is in i o only outputs requested by the user are generated in an extreme case raven file output could be minimized to e g a single nash sutcliffe metric representing the match between an observed and simulated hydrograph this is especially beneficial for automatic calibration individual subsets of the model domain may be simulated independently by specifying a portion of the domain to disable this can be done for headwater basins without additional intervention or the upstream streamflows may be supplied to downstream subbasins as a time series e g from an independent model run or observation data in addition to minimizing memory needs the raven code has been extensively optimized and is able to simulate approximately 400 000 hru time steps second on a standard desktop computer with a reasonable process configuration and the default output files 4 2 adding process algorithms forcing estimators because of raven s class structure and encapsulation approach adding new processes or forcing estimators is relatively straightforward requiring revision typically to only three locations in the code for example to add a new baseflow algorithm a developer would have to 1 enable support for parsing of the input command for that algorithm one line of code 2 would have to generate the body of the algorithm to generate m i j given the snapshot state of the system the bulk of the effort and 3 would have to indicate a which state variables are modified by and b which soil parameters are required by the algorithm and add these state variables parameters if they are not already recognized by the model because the global model solver works with abstracted instances of a generic hydrologic process it does not need to be modified in any way some modular platforms can require user specification of the connections between processes creating additional development overhead and leading to potential incompatibility of hydrologic processes this is avoided in raven via conforming to the mathematical structure outlined in section3 2 which ensures that new algorithms can be added without any explicit knowledge of other processes used in the user specified model configuration abstracted coding practices for forcing function estimators interpolation schemes and routing algorithms ensure that these may be added with similarly little impact on the body of code strategies for implementing new process algorithms forcing estimators and adding new parameters are fully documented craig and the raven development team 2019 4 3 input and output raven is a completely text based model which supports both unix and dos based line ending conventions the input is segregated into only five primary files 1 the primary input rvi file in which the model structure is specified and simulation details time step model duration output options are indicated 2 the discretization rvh file which defines the subbasin and hru topology and membership in land use vegetation soil profile classes 3 the parameterization rvp file which defines soil vegetation land use classes and their corresponding model parameters 4 the time series rvt file which describes all of the time series based forcings to the file but also observation time series user specified stream inflows etc and 5 the initial conditions rvc file there are very few changes to these input files needed when adjusting model structure or algorithms which is advantageous for pre processing and model testing most of the primary input file is portable as it defines a specific model configuration only the simulation duration and output diagnostics might vary from simulation to simulation for parameter estimation only the parameterization file would typically be revised for forecasting with ensemble forcings only the time series and perhaps initial conditions files would be modified as a deterministic model is wholly defined by the primary discretization and parameterization file this organization of input facilitates porting of model configuration and parameter specification but also facilitates ease of integration into forecasting and data management systems for instance raven has been integrated into the deltares delft fews open data handling platform and the wiski water information system via adapter scripts for output raven provides complete control over the output generated thus minimizing the overhead of extraneous file writing statistical spatial and temporal aggregation of any state variable forcing function or auxiliary variable may be written to output e g histograms or monthly averages of daily snow depth grouped by subbasin in support of automatic parameter estimation irregularly spaced or continuous observation time series of any simulated state variable may be supplied for internal comparison with model results eighteen different metrics and hydrologic signatures may be generated for reporting of model to observation fit to an unlimited number of observation time series calibration parameter estimation and visualization are to be handled externally to the code using post processing there are multiple general purpose parameter estimation software packages e g doherty 2005 matott 2016 poeter et al 2005 duan et al 1992 that are capable of utilizing arbitrary text based numerical engines with a single input flag raven will work in silent mode only generating a single file of diagnostic results enabling exceedingly fast calibration runs 4 4 summary of contribution while it possesses many similar traits to existing recent flexible modelling frameworks such as superflex and summa the raven framework is distinct in a number of notable ways these distinctions are summarized here to clarify the scope of the contribution represented by raven first it is the first flexible modelling framework to include a generic in catchment and in channel advective transport routine which is independent of the hydrological model configuration as noted in section3 8 it may therefore be used to test additional hypotheses related to model skill and function that can e g be verified using isotopic tracers second raven was designed for operational application as well as research deployment and therefore directly supports simulation of lakes wetlands and reservoirs irrigation demand injection and extraction from reservoirs and streams straightforward i o for data assimilation and calibration and integration with existing forecasting systems the framework may therefore be applied to basins with significant human interference enabling flexible conceptual models to be tested in altered landscapes and operational models to be steadily upgraded as additional data is made available its speed also aids in operational calibration and uncertainty analysis exercises most raven models take seconds to run and the framework able to simulate 100 000 500 000 hru time steps second on an intel i7 3 2ghz processor depending upon model configuration third it supports a range of stream network routing algorithms both superflex and summa support routing only with basic transfer functions or by using external routing models fourth with regard to software implementation raven is the only framework which is sufficiently abstracted to mix and match any subset of process algorithms and forcing representations i e process and forcing algorithm compatibility is ensured by design fifth raven is the only framework that enables testing of both an extensive range of forcing function estimators and process algorithms and therefore enables investigation of the important interplay between forcing estimation and model structure lastly raven enables simultaneous comparison and evaluation of routing forcings parameter uncertainty and structural uncertainty while providing sufficient computational abstraction to evolve considerably in the future as new capabilities are added 5 demonstration in this section some basic results generated by raven are presented to demonstrate some of the capabilities of the software 5 1 emulation capabilities in support of a model stewardship programme supported by the national research council of canada the functionality of the ubc watershed model ubcwm quick 1995 was ported over into raven the ubc watershed model originally developed in the late 1970s and gradually improved over the following decades has been successfully deployed for continuous hydrologic simulation of both rainfall and snowmelt dominated mountainous watersheds in british columbia alberta and the himalayas the programme includes rather advanced treatment of the snow energy balance place specific orographic correction algorithms and unique treatment of infiltration and evaporation from the topsoil documented inquick 1995 the available code for the original ubcwm was written in vb net and is significantly more complicated than many of the general code structures replicated in essence but not in specific detail in the efforts ofclark et al 2008 orfenicia et al 2011 what distinguishes these past emulation efforts from the one presented here is that an effort was made to nearly perfectly replicate the output of the existing code not just match the theoretical documentation because the complex code was not simply translated but also re organized into a highly abstracted modelling framework this was a non trivial effort the ubcwm model configuration as formally implemented in raven is depicted in fig 7 note that each of the hydrologic processes corresponds to a individual process matrix m p as depicted in fig 2 fig 8 depicts the snow depths as snow water equivalent swe generated by raven and by the original ubc watershed model using a daily time step for a forested low elevation band of the revelstoke watershed in souther british columbia canada for the period from oct 1 1989 to sep 30 1992 the maximum error in simulated snow depths during this time is less than one percent indicating a quite successful emulation it is noteworthy that the raven model is not calibrated to match ubcwm results the internal model mechanics are identical given the same input parameters this emulation extends the ubcwm model to now be used in river networks with multiple subbasins and with explicit simulation of reservoirs and support for transport it likewise enables the individual algorithms comprising the classical ubcwm model to be tested and if desirable upgraded lastly it enables other custom raven models to be built using some of the strengths of the ubcwm model for instance its strong support for orographic corrections to snow deposition and melt processes note that raven similarly supports exact emulation of the models gr4j perrin et al 2003 hbv ec bergström 1995 hamilton et al 2000 hmets martel et al 2017 and mohyse fortin and turcotte 2006 5 2 ensemble simulation and algorithm comparison one of the goals of the raven framework is to provide a tool with which to assess and test the various algorithms used to support hydrological simulation here a simple deployment of raven in this capacity is demonstrated fig 9 depicts the testing of the infiltration algorithm in the hourly time step implementation of the raven ubcwm model recognizing that single deterministic simulations are insufficient for comparing model performance populations of model realizations with perturbed versions of the observed hydrograph data were used as calibration targets for two different infiltration percolation baseflow algorithms with the rest of the model being identical the perturbed observation hydrographs were used to represent the range of potential real hydrographs which are consistent with the measured river stage data after applying an error model to the stage discharge curve this new approach developed bysgro 2016 was developed to more rigorously assess proposed model improvements in light of data uncertainty it is introduced here simply as an illustration of the types of model comparison exercises that can be performed with the flexible raven framework the test was performed using a base hourly model of the 202 km 2 alouette river basin in british columbia canada the first algorithm used is the original ubcwm quick 1995 formulation while the second is loosely based upon the infiltration percolation and base flow routines of hbv bergström 1995 which raven is also capable of emulating in this algorithm infiltration is based on saturation of the topsoil and percolation is a constant rate from topsoil down to groundwater the baseflow is calculated using a power law relationship with groundwater reservoir storage a two year period from 2011 2013 was used for calibration following a one year warm up period using 100 realizations of perturbed observation data the calibration was performed using the dds algorithm tolson and shoemaker 2007 with rmse as the objective function the standard raven ubcwm had 14 calibrated parameters while the hbv based infiltration model had only 13 another two year period from 2014 2015 was used as the validation period a subset of which is shown in fig 9 the utility of using the distribution of perturbed observation hydrographs is that populations of simulated hydrographs could be compared to the unperturbed hydrograph during a validation period thus instead of comparing two individual model results to assess whether one model configuration was better distributions can be compared with formal statistical tests the results of the comparison in terms of nash sutcliffe efficiency in the validation period is shown in fig 10 to simplify the analysis uncertainty in the discharges in the validation phase was not considered though it is likely that as more sources of uncertainty are included in such an analysis the harder it would be to discriminate between model performances it is clear from the figure that with this test the hbv formulation consistently outperforms the original this is also reflected in results of a non parametric wilcoxon mann whitney two sample rank sum test montgomery and runger 1994 which indicates that the two distributions are a statistically distinguishable at the 95 confidence level note that these distributions also reflect some degree of parameter uncertainty as there is a random component to the calibration algorithm and some of the parameter sets may not be considered optimal with respect to the calibration objective function the details of this approach and its applications to other model improvements are documented bysgro 2016 5 3 transport demonstration to demonstrate the transport capabilities of raven the alouette model from the previous section was modified to track the quantity of snowmelt released from each elevation band using the algorithm described in section3 8 the results depicted as a partitioned hydrograph from november 1993 to july 1994 are shown in fig 11 the total snowmelt hydrograph is decomposed into the portion of the hydrograph generated via snowmelt from each of 7 elevation bands 0 250 250 450 450 650 650 850 850 1050 1050 1350 and greater than 1350 masl it is noteworthy that the sum of contributions from each band is significantly less than the total outlet discharge not shown the remainder is contributed through rainfall sourced runoff glacier melt and water released from groundwater stores in this rainfall dominated basin in the pacific northwest snow accounts for a relatively small amount of runoff tracer mass balance for the system is satisfied to machine precision as might be expected contributions to the total runoff hydrograph proceed from lower elevations to higher elevations with the lower elevation bands melting earlier from this figure one can clearly identify two warming events in february and early march where all bands contributed meltwater to the outlet whereas after april only the upper elevation bands have a sufficient amount of snow cover to contribute melt with raven tracers could readily be applied to sub areas within each band and can likewise track concentrations of tracers in any storage unit within the system the algorithm which preserves mass exactly is subject to some numerical dispersion as a trade off for flexibility and stability but is nonetheless extremely useful for assessing internal model partitioning as shown here 5 4 discretization evaluation the flexibility in discretization provided by raven enables the direct evaluation and comparison of different discretization strategies while keeping the rest of the model choices fixed here a simple demonstrative example is provided where three discretization approaches are applied to the 7 2 km 2 mount head basin in the canadian rockies and the value of incorporating fine resolution slope aspect and routing detail into a model is assessed the finest resolution model model a is a fully distributed tin model of the watershed with 13 228 individual hru surfaces with unique slope and aspect split into 6 281 local drainage areas comprising the flow network network shown in fig 12a a second model model b lumped the basin into 3 hrus corresponding to 3 primary drainage areas using an area averaged representative slope and aspect for each of the 3 regions fig 12b finally in a third model model c retained the same number of high resolution hrus as model a for generating snowmelt but aggregated them into the 3 primary drainage areas of model c for routing only snow accumulation and snow melt are simulated using a simple single layer energy balance snowmelt approach forced with the shortwave radiation estimated using the algorithm ofallen et al 2006 and a diffusive wave routing model it is presumed for the sake of this example that model a is the most accurate representation of the basin and the reduction in model fidelity is assessed for the other two models we do not attempt to calibrate the model to observations the hydrographs from the three different models during the freshet season of 2008 are shown in fig 12c it is noteworthy that the computational cost of this hourly model was 518 5 and 221s for models a b and c respectively on an intel i7 3 2 ghz processor it is clear from the hydrographs shown in fig 12c that there is relatively little impact of the routing aggregation from model a to model c as the timing of snowmelt release is controlled primarily by energetics rather than transit through the routing network the minor reduction in fidelity for the significant computational cost reduction is a reasonable tradeoff the aggregation of all of the hrus into three zones in model b however led to a significant loss of model fidelity and creates massive errors in the timing of the freshet and a total disappearance of late season events presumably because high elevation and north facing slopes are averaged out in the lumping process raven enables the generation of a number of intermediate discretization setups to identify the optimal tradeoff between fidelity and computational cost and could furthermore evaluate different means of lumping hrus by elevation slope or aspect bins 6 conclusions a robust and highly generalized object oriented hydrological modelling platform raven has been developed the structure and organization of the model has been formalized so as to accommodate a wide range of model structures process representations spatial discretization approaches interpolation methods and numerical algorithms adding additional hydrologic process or forcing function estimation algorithms is straightforward and unhampered by common issues of incompatibility of algorithm modules the software has been demonstrated to successfully emulate the disparate models gr4j hbv ec mohyse hmets and ubcwm and includes over 100 hydrological process algorithms to emulate the approaches included in a number of other popular hydrological models it may therefore also be considered a historical archive of hydrologic algorithms the modelling framework supports the science motivated community modelling efforts advocated by cuahsi 2009 and others e g weiler and beven 2015 additionally it has a number of practice motivated features which support the concerns of consultants and modelling practitioners including means of explicitly simulating water management practices reservoir operations and land use modifications it has successfully been deployed for use in operational reservoir forecasting by multiple power generation and flood forecasting organizations within canada current research with raven is aimed at using its flexibility to test methods for improving model structure assessing and comparing between model choices and examining the impact of various forms of model uncertainty the model is also being extended to further support contaminant transport simulation declaration of competing interest dr craig has occasionally performed paid technical consulting for users of the open source modelling framework presented in this research paper which may be deemed a perceived conflict of interest however this has been to support practical application of the research tool to real watersheds and integration of the tool into operational forecasting systems and does not influence the results of this paper which are primarily demonstrations of model capability no other paper authors have potential conflicts of interest to report acknowledgements the authors would like to thank the many student members of the raven development team who have participated in developing code documentation or additional utilities to the raven framework this work was partially funded through the nserc industrial postgraduate scholarship program which supported authors a snowdon and n sgro and two nserc discovery grants awarded to the first author we acknowledge significant financial and technical support from bc hydro and the national research council s ocean coastal and river engineering division we would further like to thank the variety of organizational partners which have financially contributed to its development including ontario power generation alberta watersmart mac hydro northwest hydraulics inc associated engineering and the city of calgary we would also like to acknowledge funding partners and organizational users that have otherwise supported raven driven research and consulting projects leading to improvements in the software including alberta environment and parks canarie deltares environment and climate change canada the floodnet nserc strategic research network the government of the northwest territories natural resources canada new brunswick environment and local government ouranos and transalta lastly we thank lieke melsen and four anonymous reviewers for helping to improve this paper 
26021,a general formulation of a surface water hydrological model is posed that enables multiple numerical schemes model structures discretization schemes and interpolation approaches to be tested compared and or used for assessment of structural uncertainty and algorithm skill this formulation is the basis for the object oriented and open source hydrological modelling framework raven raven is uniquely capable of fully emulating and then modifying or extending a number of existing hydrological models of differing design including the ubc watershed model gr4j mohyse hmets and hbv ec more than 100 compatible process algorithm options and more than 80 interchangeable options for both routing and the estimation of forcings support the testing of trillions of possible model configurations it supports general spatial discretization including grids subbasin hydrologic response units lumped or triangulated irregular network models raven s utility for examining the impact of model choices and sensitivity to structural model uncertainty is demonstrated with a set of simple test cases graphical abstract keywords watershed model distributed hydrological modelling modelling framework hydrology object oriented surface water models software availability the raven hydrologic modelling framework along with tutorials documentation source code utilities and example files is freely available for download at http www raven uwaterloo ca 1 introduction computational hydrological models under continuous development for the last half century have become increasingly complex and varied in their description of the hydrologic cycle the sophisticated physics of mixed saturation soil water flow evapotranspiration snow melt overland flow and heat transfer at multiple interacting time and space scales often precludes representation using simple physical laws beven 1989 beven and germann 2013 rather when applied at the basin scale a component based approach is generally applied each individual hydrologic process e g evapotranspiration infiltration etc can be represented using any number of mixed physical statistical and empirical process models developed in the literature these process models are here formally defined as algorithms that relate fluxes of mass or energy between or within storage units to the current state of the hydrologic system many of these process models are site or terrain specific many are only valid for hourly or daily or monthly estimates and many have a considerable number of parameters which are typically unknown unmeasurable or both all process models have a significant number of assumptions associated with their use the net effect of this variety of plausible descriptions of hydrological processes is the existence of dozens of hydrological models regularly used in practice singh and woolhiser 2002 documented more than 70 in 2002 and this number has risen since while many of the process algorithms used in these existing models have been individually validated against data from tightly controlled field or laboratory experiments the cumulative effects of multiple interacting non linear processes in the real world are still far from being replicable let alone wholly predictable likewise it is recognized that the unknowable nature of the phenomena occurring at scales of interest introduces a significant amount of structural uncertainty that is uncertainty in model predictions linked to uncertainty regarding the description of the hydrologic system there is a recognized need clark et al 2015 weiler and beven 2015 mendoza et al 2015 addor and melsen 2019 for flexible and modular models which can be used to assess the impacts of structural uncertainty wagener et al 2001 test hypotheses about system functioning clark et al 2011a b euser et al 2013 estimate the uncertainty linked to structural inadequacy clark et al 2008 identify optimal model configurations marshall et al 2005 and support stepwise conceptual model improvement supported by available data fenicia et al 2008 such modular models may also be used for identification of best practices for discretization interpolation porting of parameters and other testing of the various choices made in application of hydrologic models while some recent progress has been made in the development of flexible models primarily for research applications clark et al 2008 fenicia et al 2011 clark et al 2015 these developments have primarily focused on providing flexibility in model structure for example both fuse clark et al 2008 and superflex fenicia et al 2011 can represent numerous conceptual box models of surface water processes enabling different model structures and structural combinations the recently developed summa modelling framework clark et al 2015 additionally supports physically based continuum models of hydrological phenomena including e g richards equation for redistributing shallow soil moisture to date no model allows for the vast array of individual model choices beyond model structure to be individually or collectively assessed nor do the existing flexible model frameworks support contaminant transport reservoir routing or a number of other features often deemed necessary in operational hydrologic modelling for forecasting or water management however all of these various choices and augmentations influence the performance adequacy and skill of such models and therefore it is additionally desirable to provide flexibility in the methods used for spatial and temporal discretization forcing function interpolation orographic correction rain snow partitioning in catchment in channel reservoir routing and other critical model choices one solution to these issues is to implement a flexible hydrologic modelling framework that can support and help to test a wide variety of modelling paradigms while avoiding the pitfalls of excessive abstraction this paper outlines the general structure of such a framework and its implementation as raven specifically this paper reports on a number of critical issues that impact the development and operation of a modular hydrological framework demonstrates the ability of the raven code to emulate a number of popular existing models and deploys the raven framework for basic assessments of sub model and discretization choice 2 background current modular frameworks existing hydrological models range from fully spatially distributed 4 d finite element simulators e g mikeshe abbott et al 1986 inhm vanderkwaak 1999 or parflow ashby and falgout 1996 to simple lumped tank models of an entire watershed e g the sacramento model burnash et al 1973 they may be spatially discretized using a cell based approach for use with digital elevation models as done inde roo et al 2000 or using a subbasin approach via the concept of a representative elementary area rea wood et al 1988 the more common approach used in swat arnold et al 1998 hspf bicknell et al 1997 agnps young et al 1989 etc because of this variability in model structure and implementation it is exceedingly difficult to compare and contrast results between models smith et al 2004 and often challenging even to simply select an appropriate model for any given purpose flexible codes which can support a range of modelling approaches of different complexity can afford a user a direct mechanism for comparison of model choices and can likewise support a range of complexity where simple and fast conceptual models can be used for data poor systems or preliminary watershed assessment and fully distributed models can be deployed for more complex problems supported by richer data sets likewise flexible modelling codes can support place based customization in the same model potentially using different physical representations of the water cycle on different landscapes lastly they can have the capacity to support stepwise modelling whereby models are increased in complexity only when justified by the available data and where added complexity leads to higher predictive skill and or system understanding various researchers have developed flexible modular or object oriented hydrologic modelling codes which rest upon the inherent modularity of hydrologic processes the general evolution over the last 30years has been from modular models with some plug and play support to fully realized modular modelling frameworks the first such model is likely prms leavesley et al 1983 leavesley and stannard 1995 a fortran code where individual modules correspond to specific hydrologic processes algorithms which could be interchangeably used or excluded by the user the implementation approach of prms was to have each module directly modify the relevant state variables during each time step at the end of the modules operation all local and global mass balance constraints were resolved while this approach is successful at small time steps processes are necessarily treated as serial rather than concurrent in addition the order of processes could not be altered and both the model discretization approach and structure were fixed as is typical for most hydrological models however with the successor to the prms model mms leavesley et al 2002 a good deal more attention was paid to the abstraction of hydrological processes with the intention of understanding the differences between subprocess models and means of iterating between modules during a timestep were included more recently clark et al 2008 developed a software package fuse for lumped surface water modelling which is able to mix and match process models for ensemble basin simulation and therefore directly investigate the impact of structural error a descendent of the modular rainfall runoff toolkit rrmt wagener et al 2001 the degree of abstraction was here improved because the fuse model was intended to emulate 4 distinct lumped model structural representations and the various permutations and combinations of those models this fuse model was later used to test the impact of different numerical methods on model calibration and the interpretation of model output clark and kavetski 2010 kavetski and clark 2010 fuse has since been superseded by superflex fenicia et al 2011 and the more physically based summa land surface model clark et al 2015 the cold regions hydrologic model crhm ofpomeroy et al 2007 is another recent attempt at generality based upon the process module approach with an emphasis upon a toolkit approach for snow modelling in smaller basins crhm is quite adept at handling a variety of methods of lateral transfer between landscape units and therefore provides flexibility in both model process representation and hru discretization the framework ofcoxon et al 2019 provides similar flexibility in spatial discretization but without comparable structural flexibility other examples include the topography based modelling package of wang et al 2005 the generic object modelling system oms kralisch et al 2005 the openmi component based approach ofcastronova and goodall 2010 the parallel framework seims zhu et al 2019 and the rapid development framework ofkneis 2015 as noted bykneis 2015 the use of a modular framework rather than a specific hydrological model is needed for application to a range of problems also as noted bymendoza et al 2015 the availability of a wide range of model options helps to avoid unnecessary constraints upon model agility the raven software framework builds upon some of these earlier successes and is able to replicate many of the features of these modular models and supplement them with additional unique functionality as will be discussed below 3 formulation here the structure of a generic lumped semi distributed distributed hydrological model is uniquely formalized this formalization is used as the basis for a modelling framework that is compatible with the wide range of process representations model structures discretization approaches parameterizations and numerical algorithms that can be found in individual hydrological models many of these structures closely echo and further build upon the blueprint offenicia et al 2011 but the means by which processes forcing function generation and the global local numerical algorithm are organized and exchange information are unique this formalization is implemented in the object oriented open source software code raven which was developed using the c programming language a number of design philosophies were adhered to during the construction of raven objectives were to 1 separate the global numerical solution algorithm from the individual process model representations in order to allow for improved runtime and model simulation performance 2 be able to plug and play different algorithmic representations of the same physical process for inter model comparison hypothesis testing clark et al 2011a and ensemble simulation vrugt et al 2005 3 be capable of emulating multiple existing hydrological codes model approaches for benchmarking inter model comparison improvement and evaluation purposes this includes both continuum submodels e g richards equation for soil moisture redistribution and compartment based models 4 formally and universally distinguish between state variables forcing functions model parameters and derived parameters 5 accommodate various methods of lateral and vertical spatial discretization in a general fashion e g grid cell land surface tile triangulated irregular network tin or basin representation 6 accommodate generic temporal discretization schemes for individual processes including variable time stepping and sub stepping 7 handle standard treatment of conceptual model routing e g convolution and transfer functions fenicia et al 2011 8 accommodate specification of as large or small a number of parameters as needed all parameters may be either directly specified by the user some may be auto generated from empirical relationships e g pseudotransfer functions all parameters can be fixed scalars or user specified time series 9 accommodate a wide variety of interpolation autogeneration orographic correction and downscaling schemes for applying gridded netcdf or gauge interpolated model forcings such as precipitation temperature and radiation models should be able to run with as little information as daily precipitation and temperature but also have the capacity to use rarer data sets such as user specified relative humidity and or longwave radiation when available 10 ensure that the model is completely deterministic i e the state of the system at any point in time wholly dictates its future state as is consistent with a mathematical initial value problem 11 support advective transport of constituents e g contaminants nutrients or tracers without having to modify transport code with the addition of new hydrologic process algorithms 12 find a balance between over abstraction a common pitfall of some modular frameworks and systems models and under abstraction such that the framework is flexible and powerful but not at the expense of usability extendability raven enables the investigation of a number of troublesome issues in the science of hydrologic modelling for example the framework is supportive of the systematic scrutiny of competing hypotheses advocated byclark et al 2016 whereby individual model hypotheses or algorithms are explicitly assessed for robustness and appropriateness with raven this scrutiny of model choices may further be extended beyond model structure to interpolation forcing specification or discretization choices as inhaghenegadar et al 2015 orliu et al 2016 as such raven has been applied to evaluate different means for constraining flow partitioning in hydrologic models shafii et al 2017 for evaluating swe estimation methods yao et al 2018 and evaluating model workflow choices chernos et al 2017 3 1 spatial discretization no general model framework can be completely flexible in its representation of the hydrologic cycle without sacrificing usability some basic ground rules need to apply in order to simplify code development avoid over abstraction and ensure model consistency most of these ground rules may be established by fixing the means of model spatial discretization i e determining the level at which parameters state variables lateral vertical processes and forcing functions are assumed to be homogeneous and determining the level at which lumping of fluxes and lateral transfer occur notably these decisions are often implicit in hydrological model development but determining the actual choices made in individual models typically requires close investigation of the source code the fundamental spatial discretization approach used in raven is depicted in fig 1 and is consistent with most modern semi distributed surface water models in raven a watershed scale model is composed of a number of subbasins through which water is routed laterally downstream on the land surface and laterally between subbasin groundwater reservoirs each subbasin contains a stream river channel discretized into a user specified number of reaches each subbasin is subdivided into one or more contiguous or non contiguous hydrological response units hrus where the vertical water and energy balance is applied producing runoff interflow and baseflow that is aggregated and distributed along the reach and the recharge that is aggregated and deposited to the subbasin groundwater reservoir these hrus are assumed to have a homogeneous response to external forcings a specified vertical soil profile and unique membership in land use terrain aquifer and vegetation classes these class memberships uniquely define the physical properties of the hru hrus possess their own set of state variables such that variables such as soil moisture may be tracked at the resolution of hrus state variables may be statistical in nature representing the key moments of storage quantities for example or represent only part of an hru e g wetland storage may only correspond to the 30 of the hru covered by wetlands but explicit spatial locations within the subbasin should be functionally irrelevant a notable exception is the case of lateral transfer driven by topography e g the uphill region of an hru could transfer water to the downhill via some process but explicit location or direction cannot be considered forcings such as precipitation or radiation are likewise hru specific and can be generated in any number of ways see section3 5 note that this structure allows for the basin modelling approach of e g swat neitsch et al 2001 fig 1a the cartesian cell based structure of atmospheric land surface schemes such as class verseghy 1991 fig 1b a lumped basin approach such as in gr4j perrin et al 2003 fig 1d or a triangular irregular network discretization fig 1c note that this approach fully supports the goals ofclark et al 2015 with regard to arbitrary spatial discretization albeit in a somewhat different manner 3 2 mathematical structure since the computational simulation of a hydrologic system is fundamentally a mathematical problem the underpinnings of the software outlined here are constructed based upon a formalization of this mathematical problem while often ignored or circumvented in both software documentation and hydrology textbooks the problem being solved by all hydrological models is that of a coupled system of ordinary and partial differential equations odes and pdes within each computational element hru in this case it has been shown kavetski and clark 2010 kavetski et al 2011 that not giving due consideration to the numerical solution approach in hydrological models can lead to issues in model calibration and performance these odes and pdes individually describe either 1 the accumulation of mass or energy within a given storage compartment or continuum i e a mass or energy balance or 2 the temporal change in some auxiliary system property e g density crop height or albedo these parameters are collectively the dependent variables of the problem here referred to as either primary state variables those corresponding to mean storage of intrinsic properties such as mass or energy or auxiliary state variables those corresponding to extrinsic properties higher order statistical moments of intrinsic properties etc another category derived variables is here reserved for parameters which depend upon the current state of the model but can be calculated directly from a finite set of primary and auxiliary state variables for example snow depth a derived variable may be calculated from the combined knowledge of the snow water equivalent a primary variable representing water storage and the snow density an auxiliary variable at any snapshot in time as should be clear from this example the distinction between derived and auxiliary variables is not unique and the two are ontologically interchangeable the mathematical problem statement solved in hydrology is often obfuscated in the literature by the mixed communication of the mass energy balance problem statement couched in terms of the solution algorithm as noticed byclark and kavetski 2010 for example the accumulation of water in a closed pond is most appropriately expressed as a continuous ode t s t p t e t where s is the water storage in the pond an implicitly positive quantity p is the precipitation rate and e is the evaporation rate all are continuous functions of time as they are in nature and are here stated in the state space formulation ofclark et al 2008 many texts describe the problem pre cast as a numerical solution over a discrete time step δ t e g s i s 0 δ t p i min s 0 δ t e i where now the implicit constraint that the storage is positive is made explicit an advantage but the ability to choose the most appropriate numerical algorithm for solution as part of a coupled system of odes has been removed a marked disadvantage this is worsened by the common implicit assumption that the time step is in days in fact many common process and routing models e g muskingum routing are strictly defined in terms of their numerical algorithms rather than their implicit mathematical formulation this makes it difficult but still possible to mix and match them with other processes under a single consistent representation here in every hru each state variable is subject to the influence of a number of hydrological processes increases or decreases in a primary state variable are simply the additive combination of influx or outflux terms i e the ode or pde corresponding to a primary state variable is built up from mass or energy balance considerations increases or decreases in auxiliary variables are likewise assumed to be written as the additive combination of terms we can therefore write an individual differential equation for the change in the j th state variable of n s total state variables ϕ j as 1 ϕ j t p 1 n p i 1 n s k 0 n c p m i j k ϕ p f where m i j k is the change in state variable j due to subprocess k each of the n p processes can have n c p sub processes which is linked to another state variable i this linkage typically communicates flow direction e g a subprocess m i j k moves mass or energy from storage compartment i to compartment j a subprocess m i i k i e i j represents an independent rate of change for an auxiliary variable and does not connote exchange of mass or energy between compartments though simple processes e g evapotranspiration from topsoil to atmosphere may be represented by just a single sub process moving water from one state variable to another more complex processes e g snowpack evolution may move mass and energy between multiple compartments and are represented by a matrix of constituent sub processes m p m i j k the fluxes or rates of change returned by each set of sub processes are a function of the current vector of state variables ϕ system parameters p and forcing functions f the full system of equations describing the influence of all processes in an hru can be written in matrix form in a manner similar tokneis 2015 2 ϕ t m g 1 p 1 n p m p 1 where ϕ is the complete vector of state variables m g is a n s x n s global matrix of composite rate of change functions and 1 is a column vector filled with ones the global process matrix m g is the sum of contributions from each individual process matrix i e m g m p the individual functions m i j k may be linear or highly non linear all of these functions return representative rates of change over the global time step δ t any full hydrologic model structure can be assembled from its constituent processes described by the process functional matrix m p as seen in fig 2 the antisymmetric matrices in fig 2d and e correspond to the individual processes of infiltration runoff and precipitation each of which work on a different subset of state variables ponded water p surface water sw and topsoil s in the case of infiltration atmospheric precipitation a depression d canopy c and ponded water in the case of precipitation the summation of the two matrices fig 2f represents the combined influence of both processes this notation is useful for a number of reasons first of all for processes which only operate on primary state variables the function matrix m k for that process has to be anti symmetric i e m i j k m j i k i e mass or energy balance is perfectly preserved by requiring that the mass entering storage compartment i is equivalent to that leaving storage compartment j likewise for the same reason diagonal terms of the matrix which correspond to mass storage variables must be zero m i i k 0 equivalent to the statement that mass or energy cannot be created or destroyed it must have an external source or sink the notation above also simplifies the description of global numerical algorithms which control the coupling of individual processes over a time step the explicit euler algorithm can be written as 3 ϕ n 1 ϕ n δ t m g t n ϕ n 1 where n denotes the current time step because of the presence of constraints on state variable values the euler method is typically not used in hydrology instead numerical hydrological models are typically solved using an ordered series approach where processes that physically occur in parallel are mathematically treated as if they occur in series over the time step snowdon 2010 i e ϕ ϕ n δ t m 1 t n ϕ n 1 ϕ ϕ δ t m 2 t n ϕ 1 4 ϕ n 1 ϕ δ t m n p t n ϕ 1 where ϕ is an intermediate vector of state variables using this ordered series approach the order of operation is inevitably hard coded into hydrologic modelling software packages the notable exceptions being those of fuse clark et al 2008 superflex fenicia et al 2011 and summa clark et al 2015 which can use a variety of implicit and explicit numerical algorithms note that the numerical methods used to calculate individual process matrices may be more sophisticated than this for example within raven vertical redistribution of water in a discretized soil column may optionally be simulated using an iterative finite volume solution to richards equation however as the process matrices are quite generic there are no constraints upon how individual terms of those matrices are defined or calculated this flexibility enables the use of finer temporal discretization to resolve faster or more highly coupled processes without burdening the rest of the simulation it is also important to note that this formalization supports both a conceptual type and continuum type representation of hydrologic processes as advocated byweiler and beven 2015 in addition to the euler and ordered series approaches indicated above raven also supports an iterative predictor corrector method for solving the set of odes snowdon 2010 3 3 hydrologic process representation some hydrologic processes can be simply treated as the movement of water mass or energy from one storage compartment to another for example evaporation from open water is a well understood physical phenomenon which moves water from one storage compartment e g a lake to another the atmosphere the process alone is simply described by two symmetric ordinary differential equations the net accumulation of water in the atmosphere due to evaporation is equal to the net loss from surface storage and is primarily a function of net incoming radiation unfortunately many processes are too coupled to be represented in such a simple fashion the snowmelt process for example directly influences snow depth density liquid water content cold content temperature and possibly additional state variables these variables have multiple constraints e g positivity of depth negativity of temperature upper and lower bounds on water content and their interrelationships are highly coupled e g melt directly causes corresponding changes in depth density and heat energy the corresponding process matrix m melt is therefore quite complex in structure containing both diagonal and off diagonal terms some of which are individually quite complicated however due to the significant coupling between the concurrent subprocesses of ripening shrinking warming etc decoupling a snowmelt model into simpler problems i e into smaller process matrices has the potential to lead to significant numerical error in the raven framework process algorithm code is not allowed to modify state variable values only return what the desired modification would be when provided information about the initial state of the system this decoupling of process calculation and state variable updating is an essential ingredient for separating process algorithms from the global algorithm for example with this approach the process algorithm may be applied repeatedly in an iterative fashion to account for the influence of simultaneous processes i e the implicit approaches advocated bykavetski and clark 2010 this approach also accommodates multiple numerical methods by allowing process fluxes to be independently estimated at the start and end of the time step with the same code lastly the approach truly encapsulates individual process algorithm code which is critical for extendible development raven currently supports over 100 hydrologic process algorithms documented in the user s manual craig and the raven development team 2019 a subset of these algorithms are included in table1 uniquely it has been designed to support conditional application of these processes via control through the input file e g different infiltration algorithms may be applied in different parts of the watershed depending upon land use or soil profile type this is in support of the place based hydrologic representation discussed bybeven 2000 ormcdonnell et al 2007 an example of the assembly of a model from process algorithms in the raven input file is shown in fig 3 each command indicates the process name the algorithm the set of from compartments and the set of to compartments note the conditional application statements used to locally apply some hydrologic processes in this case the conditional ensures different processes are applied to glacier hrus 3 4 parameter assignment and granularity it is often recognized that as models move from lumped to distributed the data needs to parameterize the model and necessary preprocessing to meet these needs of a model increase dramatically in raven this is somewhat alleviated through the use of hru classification schemes each hru in addition to being defined as having a unique representative area centroid slope aspect and elevation is assumed to belong to a unique set of non overlapping classifications listed and briefly defined in table2 the benefit of this classification approach is that parameters need only be specified on a class by class basis rather than an hru by hru basis simplifying the mechanics of calibration each parameter shows up once in the model input files the data storage parameters are linked to class instances rather than directly to the hru and improving the portability of parameters to ungauged basins this also enables examination of discretization and parameter assignment choices as done by e g haghenegadar et al 2015 the hru specific parameters slope aspect area latitude and longitude are part of the hru definition as these are not portable between hrus as classified parameters may be this classification approach is more obviously beneficial for physically based modelling approaches than conceptual box models as parameters have physical meaning that can port from one place to another however it does not limit conceptual models which may have watershed specific parameters that could be mapped to watershed specific vegetation and soil classes nor does it limit the users ability to locally adjust parameters in a distributed model if needed in the extreme though likely unnecessary case each individual hru could be assigned its own vegetation soil land use class and parameters would then be fully distributed a unique feature of raven is that each and every model parameter can if desired be linked to a user specified time series i e all parameters can vary temporally this can be used for e g land use change studies or to represent the impacts of seasonal variation in vegetative cover wagener et al 2003 have shown the value of being able to use time varying parameters as a diagnostic tool in the identification of model structural deficiencies 3 5 forcing function estimation and interpolation forcing data e g precipitation incident radiation etc is distributed to hrus using either by area weighted mapping of gridded data in netcdf format to hrus or by using a generic form of interpolation between gauge stations 5 f x c y c t i 1 n g ω i x c y c f i t where ω i is the interpolation function associated with the i th gauge f i is the orographically corrected forcing function value at gauge i of n g gauges and x c is the hru centroid this is analogous to the approach ofkneis 2015 typically the constraint that ω i 1 is required generalized in this fashion this easily accommodates standard interpolation algorithms such as inverse distance weighting nearest neighbour kriging or radial basis functions the first two are internally supported by raven it also reduces the storage requirements for forcing inputs which if generated externally may require time series of each forcing function for each hru over the entire model duration while it is still possible to use such an approach the data requirements for standard model application are relatively small raw forcing data required by raven includes at the very least time series of total precipitation and minimum maximum or average daily temperature either in gridded netcdf format or at the set of gauges raven includes a suite of algorithms for determining related forcings potential evapotranspiration potential melt shortwave radiation longwave radiation snow rain partitioning cloud cover relative humidity wind speed etc from these basic data products a subset of these algorithms are indicated in table3 while there are multiple software products that have options for switching between evapotranspiration methods the authors are not aware of any other modelling framework which provides the degree of forcing generation flexibility made available through raven and enables the explicit testing of the impact of these modelling choices on model output generation of synthetic forcing functions and filling in data gaps in observed meteorological records are tasks that are appropriately handled using preprocessing utilities as such raven has only limited support for this type of functionality 3 6 temporal discretization because the global solution algorithm is wholly decoupled from the individual process algorithms raven hydrological simulations are characterized by two independent levels of temporal discretization at the top level a user specified global time step is used this time step is typically but not necessarily fixed at one day or smaller intervals at the next level down individual process algorithms can use smaller independent time steps to either meet stability constraints e g courant or peclet constraints or adaptively react to occurrences that warrant finer temporal discretization e g storm events or other rapid changes in forcings this formalism minimizes the need to explicitly address complicated temporal discretization when building simpler process models while still accommodating complex processes which by definition require fine temporal discretization this approach also minimizes computational cost by only requiring fine temporal discretization for processes whose numerical stability or accuracy require it note that this approach requires the inclusion of metadata which indicates whether algorithms are appropriate for sub daily simulation a critical consideration for any model with variable time stepping is the allocation of meteorological forcing data here all forcing functions e g rainfall incoming solar radiation or pet are specified as a time series of piecewise constant rates rather than linearly interpolated between times of measurement the temporal discretization of these time series are independent of the model time step i e forcing data does not have to be pre processed to conform to the temporal discretization of the model as is required by many hydrologic models rather raven handles the mapping from the temporal discretization of the data to that of the model minimum maximum or average values over any given time interval may be extracted from the time series data by any individual process or at the global algorithm level the only requirement is that the duration of the time series covers the entire duration of the simulation 3 7 routing to facilitate flexibility in choice of routing algorithms the routing process was generalized to both accommodate the functional form of the quickflow baseflow output from the constituent hrus an average flow rate over each time step and to consistently represent the state variables of volumetric flow along the basin reach as with hru state variables the flows from each channel segment are assumed to represent instantaneous values at the start of each time step and are not time step averaged quantities despite the apparent inconsistency with the hru water energy balance where volumetric fluxes are piecewise constant averages over each time step this approach has a number of advantages firstly it is common practice in flood routing to treat q t as the dependent variable most algorithms rely on this second it is more common to have flow measurements than measurements of total reach water storage the alternative and ostensibly more consistent state variable for routing the transition from piecewise constant to piecewise linear fluxes from the hru water balance to the channel water balance is eased in part by considering in catchment routing in channel routing and reservoir routing separately the in catchment and in channel routing model is visually described in fig 4 reservoirs lakes may be added to the outlet of any channel reach and are simulated using a level pool routing scheme with support of temporally variable reservoir outflow relationships and user specified reservoir withdrawals which can mimic reservoir operations in catchment routing is predominantly handled using a unit hydrograph uh approach where the uh for each catchment is either user specified or generated from basin characteristics the immediate history of quickflow and baseflow output to surface water is stored in memory as an array of time step averaged outflow rates to off channel tributaries q l a t the duration of this history is determined by the subbasins time of concentration t c to transfer this water to either the channel segments within the subbasin or the subbasin outflow the pulse hydrograph is convolved with the unit hydrograph represented as a piecewise linear function water and energy is transferred to the downstream ends of channel segments within the reach in the simplest case a dirac hydrograph may be used to dump the water directly into the channel without a time lag in channel routing for each time step is assumed to be completely characterized by a finite history of upstream inflow stored as a vector of flow values at fixed time intervals of δ t q i n and the outflow at the start of the time step the duration of this history is determined by the minimum flood celerity and the length of the reach segment during each time step moving from upstream to downstream at both the watershed level basin to basin and subbasin level reach segment to reach segment a generalized routing algorithm is used to generate the outflow from each reach based upon the time history of upstream inflows i e 6 q o u t n 1 f r o u t e q o u t n q i n p s where f r o u t e is the routing algorithm p s is a vector of channel parameters typically a number of stored channel rating curves primary channel and bank roughness and if applicable weir or reservoir relationships this formalization supports both common lumped and distributed flow routing methods that dictate the form of f including muskingum cunge lag and route level pool transfer function unit hydrograph convolution or diffusive wave approaches all of which raven currently supports if desired a more complex kinematic wave approach may later be implemented in this framework but it cannot support hydraulic routing with significant backwater effects notably sub time stepping for routing is also enabled with this formulation if a lake or reservoir is included at the end of the reach not shown in fig 4 the non linear reservoir mass balance is exclusively solved using newton s method after the upstream flows for the time step are calculated time variable minimum maximum and target constraints on stage and flow may be imposed to represent reservoir operations as can specification of downstream irrigation demand 3 8 transport to the authors knowledge raven is the only hydrologic modelling framework that supports general transport of user defined constituents through the watershed though fuse was modified for support of tracers mcmillan et al 2012 but without routing the advective transport model is incorporated in the code in such a way that the water balance simulation is wholly independent of the transport simulation when new hydrological process algorithms are added they automatically support advective transport this is because the transport algorithm is built upon a completely abstracted treatment of in hru in catchment and in channel routing of water and mass between storage compartments only the fluxes and changes in water storage volumes are needed over a time step to resolve the advective fluxes of mass the in hru advective transport solver currently works exclusively with the ordered series flow solver eq 4 and relies upon operator splitting after all of the fluxes m i j k between all in hru storage compartments are calculated for each time step the following fully upstream weighted advective transport problem is solved 7 m j t p 1 n p i 1 n s k 0 n c p min m i j k 0 m j ϕ j max m i j k 0 m i ϕ i for j i where m i is the constituent mass density in mg m 2 in water storage compartment i n p n s n c p and ϕ j are defined as ineq 1 the first term in the summation represents mass gains from compartment j m i j k is negative the second represents losses from compartment i as in the ordered series approach this mass transfer is applied piecewise in the same order as the flow calculation but in this case one flow exchange at a time 8 m i m i δ t j i j m j m j δ t j i j where m i is the mass in storage compartment i after the single mass transfer connection has been considered and the mass flow from unit i to unit j j i j mg m 2 d may be calculated as j i j m i j k m i ϕ i if m i j k 0 m j ϕ j if m i j k 0 where m i m j ϕ i and ϕ j represent mass and storage just prior to the water mass transfer in the ordered series approach volumetric concentrations of constituents in mg l may be calculated as c i α m i ϕ i where α accounts for the units conversion note that this approach accommodates any flow model representation without intervention even those process algorithms yet to be implemented in raven it is also wholly mass conservative is unconditionally stable and can readily handle storage compartments drying out but consequentially is subject to a higher degree of numerical dispersion than e g an implicit solution toeq 7 water storage compartments can have fixed concentrations or user specified mass fluxes applied in channel routing is handled using a transfer function approach if the flow routing is also using transfer functions e g the analytical diffusive wave approach for other in channel routing methods advective transport is not supported the utility of the transport tool is significant for instance baseflow may be tracked in the model by tagging the groundwater stores with an inert tracer the transport tool has likewise been used to discriminate between rainfall sourced and snowmelt sourced contributions to streamflow and to determine the relative proportion of streamflow generated by different parts of the landscape development of algorithms for supporting more complex transport mechanisms such as dispersion diffusion sorption and geochemical reactions is ongoing 4 software implementation raven is a fully object oriented code written in c it successfully compiles for 32 bit and 64 bit windows application and on both macos and linux systems the code is open source under artistic license 2 0 and is available at http raven uwaterloo ca the class structure for the object oriented software is depicted in fig 5 the most important container class is the cmodel class of which there is only one instance the cmodel class houses all of the information regarding the global numerical algorithm the lists of state variables and processes of interest the meteorological gauge data of class cgauge and the collections of subbasins of class csubbasin and hrus of class chydrounit which define the spatial extent of the problem the watershed model also housed in the cmodel class operates by for each time step first updating all forcing functions and sifting through the constituent hrus in the model and applying all hydrological processes included in the model effectively solving the set of odes and pdes defined byeq 1 this is followed by separate solution of the in catchment and in channel routing equations to move water downstream mass balance checks and updates are applied in each time step as are storage of intermediate diagnostics e g model output to be compared to observations after simulation has commenced a solution file is written to output which represents the full current state of the system and can be used as initial states to resume a model run this process is depicted in fig 6 4 1 memory and speed considerations hydrological models have a tendency to be memory intensive for a number of reasons the raven framework has used a number of approaches to avoid this internal memory only stores the state of the system at the start and end of the current time step therefore the total memory storage is roughly o n h r u 2 n s v n f f where n f f is the fixed number of forcing functions n h r u is the number of hrus and n s v is the number of state variables in the model many models internally store hydrographs for the entire simulation duration which propagate downstream model forcing input is in the form of time series specified at gauge locations and is propagated to hrus using a weighting scheme if needed direct specification of forcings can be applied on an hru by hru basis by generating a gauge for each hru however this will increase computational cost in the case where gridded forcings are provided in netcdf format an adaptive buffering scheme is used to reduce i o and storage requirements recognizing that much of the computational and memory cost of hydrological models is in i o only outputs requested by the user are generated in an extreme case raven file output could be minimized to e g a single nash sutcliffe metric representing the match between an observed and simulated hydrograph this is especially beneficial for automatic calibration individual subsets of the model domain may be simulated independently by specifying a portion of the domain to disable this can be done for headwater basins without additional intervention or the upstream streamflows may be supplied to downstream subbasins as a time series e g from an independent model run or observation data in addition to minimizing memory needs the raven code has been extensively optimized and is able to simulate approximately 400 000 hru time steps second on a standard desktop computer with a reasonable process configuration and the default output files 4 2 adding process algorithms forcing estimators because of raven s class structure and encapsulation approach adding new processes or forcing estimators is relatively straightforward requiring revision typically to only three locations in the code for example to add a new baseflow algorithm a developer would have to 1 enable support for parsing of the input command for that algorithm one line of code 2 would have to generate the body of the algorithm to generate m i j given the snapshot state of the system the bulk of the effort and 3 would have to indicate a which state variables are modified by and b which soil parameters are required by the algorithm and add these state variables parameters if they are not already recognized by the model because the global model solver works with abstracted instances of a generic hydrologic process it does not need to be modified in any way some modular platforms can require user specification of the connections between processes creating additional development overhead and leading to potential incompatibility of hydrologic processes this is avoided in raven via conforming to the mathematical structure outlined in section3 2 which ensures that new algorithms can be added without any explicit knowledge of other processes used in the user specified model configuration abstracted coding practices for forcing function estimators interpolation schemes and routing algorithms ensure that these may be added with similarly little impact on the body of code strategies for implementing new process algorithms forcing estimators and adding new parameters are fully documented craig and the raven development team 2019 4 3 input and output raven is a completely text based model which supports both unix and dos based line ending conventions the input is segregated into only five primary files 1 the primary input rvi file in which the model structure is specified and simulation details time step model duration output options are indicated 2 the discretization rvh file which defines the subbasin and hru topology and membership in land use vegetation soil profile classes 3 the parameterization rvp file which defines soil vegetation land use classes and their corresponding model parameters 4 the time series rvt file which describes all of the time series based forcings to the file but also observation time series user specified stream inflows etc and 5 the initial conditions rvc file there are very few changes to these input files needed when adjusting model structure or algorithms which is advantageous for pre processing and model testing most of the primary input file is portable as it defines a specific model configuration only the simulation duration and output diagnostics might vary from simulation to simulation for parameter estimation only the parameterization file would typically be revised for forecasting with ensemble forcings only the time series and perhaps initial conditions files would be modified as a deterministic model is wholly defined by the primary discretization and parameterization file this organization of input facilitates porting of model configuration and parameter specification but also facilitates ease of integration into forecasting and data management systems for instance raven has been integrated into the deltares delft fews open data handling platform and the wiski water information system via adapter scripts for output raven provides complete control over the output generated thus minimizing the overhead of extraneous file writing statistical spatial and temporal aggregation of any state variable forcing function or auxiliary variable may be written to output e g histograms or monthly averages of daily snow depth grouped by subbasin in support of automatic parameter estimation irregularly spaced or continuous observation time series of any simulated state variable may be supplied for internal comparison with model results eighteen different metrics and hydrologic signatures may be generated for reporting of model to observation fit to an unlimited number of observation time series calibration parameter estimation and visualization are to be handled externally to the code using post processing there are multiple general purpose parameter estimation software packages e g doherty 2005 matott 2016 poeter et al 2005 duan et al 1992 that are capable of utilizing arbitrary text based numerical engines with a single input flag raven will work in silent mode only generating a single file of diagnostic results enabling exceedingly fast calibration runs 4 4 summary of contribution while it possesses many similar traits to existing recent flexible modelling frameworks such as superflex and summa the raven framework is distinct in a number of notable ways these distinctions are summarized here to clarify the scope of the contribution represented by raven first it is the first flexible modelling framework to include a generic in catchment and in channel advective transport routine which is independent of the hydrological model configuration as noted in section3 8 it may therefore be used to test additional hypotheses related to model skill and function that can e g be verified using isotopic tracers second raven was designed for operational application as well as research deployment and therefore directly supports simulation of lakes wetlands and reservoirs irrigation demand injection and extraction from reservoirs and streams straightforward i o for data assimilation and calibration and integration with existing forecasting systems the framework may therefore be applied to basins with significant human interference enabling flexible conceptual models to be tested in altered landscapes and operational models to be steadily upgraded as additional data is made available its speed also aids in operational calibration and uncertainty analysis exercises most raven models take seconds to run and the framework able to simulate 100 000 500 000 hru time steps second on an intel i7 3 2ghz processor depending upon model configuration third it supports a range of stream network routing algorithms both superflex and summa support routing only with basic transfer functions or by using external routing models fourth with regard to software implementation raven is the only framework which is sufficiently abstracted to mix and match any subset of process algorithms and forcing representations i e process and forcing algorithm compatibility is ensured by design fifth raven is the only framework that enables testing of both an extensive range of forcing function estimators and process algorithms and therefore enables investigation of the important interplay between forcing estimation and model structure lastly raven enables simultaneous comparison and evaluation of routing forcings parameter uncertainty and structural uncertainty while providing sufficient computational abstraction to evolve considerably in the future as new capabilities are added 5 demonstration in this section some basic results generated by raven are presented to demonstrate some of the capabilities of the software 5 1 emulation capabilities in support of a model stewardship programme supported by the national research council of canada the functionality of the ubc watershed model ubcwm quick 1995 was ported over into raven the ubc watershed model originally developed in the late 1970s and gradually improved over the following decades has been successfully deployed for continuous hydrologic simulation of both rainfall and snowmelt dominated mountainous watersheds in british columbia alberta and the himalayas the programme includes rather advanced treatment of the snow energy balance place specific orographic correction algorithms and unique treatment of infiltration and evaporation from the topsoil documented inquick 1995 the available code for the original ubcwm was written in vb net and is significantly more complicated than many of the general code structures replicated in essence but not in specific detail in the efforts ofclark et al 2008 orfenicia et al 2011 what distinguishes these past emulation efforts from the one presented here is that an effort was made to nearly perfectly replicate the output of the existing code not just match the theoretical documentation because the complex code was not simply translated but also re organized into a highly abstracted modelling framework this was a non trivial effort the ubcwm model configuration as formally implemented in raven is depicted in fig 7 note that each of the hydrologic processes corresponds to a individual process matrix m p as depicted in fig 2 fig 8 depicts the snow depths as snow water equivalent swe generated by raven and by the original ubc watershed model using a daily time step for a forested low elevation band of the revelstoke watershed in souther british columbia canada for the period from oct 1 1989 to sep 30 1992 the maximum error in simulated snow depths during this time is less than one percent indicating a quite successful emulation it is noteworthy that the raven model is not calibrated to match ubcwm results the internal model mechanics are identical given the same input parameters this emulation extends the ubcwm model to now be used in river networks with multiple subbasins and with explicit simulation of reservoirs and support for transport it likewise enables the individual algorithms comprising the classical ubcwm model to be tested and if desirable upgraded lastly it enables other custom raven models to be built using some of the strengths of the ubcwm model for instance its strong support for orographic corrections to snow deposition and melt processes note that raven similarly supports exact emulation of the models gr4j perrin et al 2003 hbv ec bergström 1995 hamilton et al 2000 hmets martel et al 2017 and mohyse fortin and turcotte 2006 5 2 ensemble simulation and algorithm comparison one of the goals of the raven framework is to provide a tool with which to assess and test the various algorithms used to support hydrological simulation here a simple deployment of raven in this capacity is demonstrated fig 9 depicts the testing of the infiltration algorithm in the hourly time step implementation of the raven ubcwm model recognizing that single deterministic simulations are insufficient for comparing model performance populations of model realizations with perturbed versions of the observed hydrograph data were used as calibration targets for two different infiltration percolation baseflow algorithms with the rest of the model being identical the perturbed observation hydrographs were used to represent the range of potential real hydrographs which are consistent with the measured river stage data after applying an error model to the stage discharge curve this new approach developed bysgro 2016 was developed to more rigorously assess proposed model improvements in light of data uncertainty it is introduced here simply as an illustration of the types of model comparison exercises that can be performed with the flexible raven framework the test was performed using a base hourly model of the 202 km 2 alouette river basin in british columbia canada the first algorithm used is the original ubcwm quick 1995 formulation while the second is loosely based upon the infiltration percolation and base flow routines of hbv bergström 1995 which raven is also capable of emulating in this algorithm infiltration is based on saturation of the topsoil and percolation is a constant rate from topsoil down to groundwater the baseflow is calculated using a power law relationship with groundwater reservoir storage a two year period from 2011 2013 was used for calibration following a one year warm up period using 100 realizations of perturbed observation data the calibration was performed using the dds algorithm tolson and shoemaker 2007 with rmse as the objective function the standard raven ubcwm had 14 calibrated parameters while the hbv based infiltration model had only 13 another two year period from 2014 2015 was used as the validation period a subset of which is shown in fig 9 the utility of using the distribution of perturbed observation hydrographs is that populations of simulated hydrographs could be compared to the unperturbed hydrograph during a validation period thus instead of comparing two individual model results to assess whether one model configuration was better distributions can be compared with formal statistical tests the results of the comparison in terms of nash sutcliffe efficiency in the validation period is shown in fig 10 to simplify the analysis uncertainty in the discharges in the validation phase was not considered though it is likely that as more sources of uncertainty are included in such an analysis the harder it would be to discriminate between model performances it is clear from the figure that with this test the hbv formulation consistently outperforms the original this is also reflected in results of a non parametric wilcoxon mann whitney two sample rank sum test montgomery and runger 1994 which indicates that the two distributions are a statistically distinguishable at the 95 confidence level note that these distributions also reflect some degree of parameter uncertainty as there is a random component to the calibration algorithm and some of the parameter sets may not be considered optimal with respect to the calibration objective function the details of this approach and its applications to other model improvements are documented bysgro 2016 5 3 transport demonstration to demonstrate the transport capabilities of raven the alouette model from the previous section was modified to track the quantity of snowmelt released from each elevation band using the algorithm described in section3 8 the results depicted as a partitioned hydrograph from november 1993 to july 1994 are shown in fig 11 the total snowmelt hydrograph is decomposed into the portion of the hydrograph generated via snowmelt from each of 7 elevation bands 0 250 250 450 450 650 650 850 850 1050 1050 1350 and greater than 1350 masl it is noteworthy that the sum of contributions from each band is significantly less than the total outlet discharge not shown the remainder is contributed through rainfall sourced runoff glacier melt and water released from groundwater stores in this rainfall dominated basin in the pacific northwest snow accounts for a relatively small amount of runoff tracer mass balance for the system is satisfied to machine precision as might be expected contributions to the total runoff hydrograph proceed from lower elevations to higher elevations with the lower elevation bands melting earlier from this figure one can clearly identify two warming events in february and early march where all bands contributed meltwater to the outlet whereas after april only the upper elevation bands have a sufficient amount of snow cover to contribute melt with raven tracers could readily be applied to sub areas within each band and can likewise track concentrations of tracers in any storage unit within the system the algorithm which preserves mass exactly is subject to some numerical dispersion as a trade off for flexibility and stability but is nonetheless extremely useful for assessing internal model partitioning as shown here 5 4 discretization evaluation the flexibility in discretization provided by raven enables the direct evaluation and comparison of different discretization strategies while keeping the rest of the model choices fixed here a simple demonstrative example is provided where three discretization approaches are applied to the 7 2 km 2 mount head basin in the canadian rockies and the value of incorporating fine resolution slope aspect and routing detail into a model is assessed the finest resolution model model a is a fully distributed tin model of the watershed with 13 228 individual hru surfaces with unique slope and aspect split into 6 281 local drainage areas comprising the flow network network shown in fig 12a a second model model b lumped the basin into 3 hrus corresponding to 3 primary drainage areas using an area averaged representative slope and aspect for each of the 3 regions fig 12b finally in a third model model c retained the same number of high resolution hrus as model a for generating snowmelt but aggregated them into the 3 primary drainage areas of model c for routing only snow accumulation and snow melt are simulated using a simple single layer energy balance snowmelt approach forced with the shortwave radiation estimated using the algorithm ofallen et al 2006 and a diffusive wave routing model it is presumed for the sake of this example that model a is the most accurate representation of the basin and the reduction in model fidelity is assessed for the other two models we do not attempt to calibrate the model to observations the hydrographs from the three different models during the freshet season of 2008 are shown in fig 12c it is noteworthy that the computational cost of this hourly model was 518 5 and 221s for models a b and c respectively on an intel i7 3 2 ghz processor it is clear from the hydrographs shown in fig 12c that there is relatively little impact of the routing aggregation from model a to model c as the timing of snowmelt release is controlled primarily by energetics rather than transit through the routing network the minor reduction in fidelity for the significant computational cost reduction is a reasonable tradeoff the aggregation of all of the hrus into three zones in model b however led to a significant loss of model fidelity and creates massive errors in the timing of the freshet and a total disappearance of late season events presumably because high elevation and north facing slopes are averaged out in the lumping process raven enables the generation of a number of intermediate discretization setups to identify the optimal tradeoff between fidelity and computational cost and could furthermore evaluate different means of lumping hrus by elevation slope or aspect bins 6 conclusions a robust and highly generalized object oriented hydrological modelling platform raven has been developed the structure and organization of the model has been formalized so as to accommodate a wide range of model structures process representations spatial discretization approaches interpolation methods and numerical algorithms adding additional hydrologic process or forcing function estimation algorithms is straightforward and unhampered by common issues of incompatibility of algorithm modules the software has been demonstrated to successfully emulate the disparate models gr4j hbv ec mohyse hmets and ubcwm and includes over 100 hydrological process algorithms to emulate the approaches included in a number of other popular hydrological models it may therefore also be considered a historical archive of hydrologic algorithms the modelling framework supports the science motivated community modelling efforts advocated by cuahsi 2009 and others e g weiler and beven 2015 additionally it has a number of practice motivated features which support the concerns of consultants and modelling practitioners including means of explicitly simulating water management practices reservoir operations and land use modifications it has successfully been deployed for use in operational reservoir forecasting by multiple power generation and flood forecasting organizations within canada current research with raven is aimed at using its flexibility to test methods for improving model structure assessing and comparing between model choices and examining the impact of various forms of model uncertainty the model is also being extended to further support contaminant transport simulation declaration of competing interest dr craig has occasionally performed paid technical consulting for users of the open source modelling framework presented in this research paper which may be deemed a perceived conflict of interest however this has been to support practical application of the research tool to real watersheds and integration of the tool into operational forecasting systems and does not influence the results of this paper which are primarily demonstrations of model capability no other paper authors have potential conflicts of interest to report acknowledgements the authors would like to thank the many student members of the raven development team who have participated in developing code documentation or additional utilities to the raven framework this work was partially funded through the nserc industrial postgraduate scholarship program which supported authors a snowdon and n sgro and two nserc discovery grants awarded to the first author we acknowledge significant financial and technical support from bc hydro and the national research council s ocean coastal and river engineering division we would further like to thank the variety of organizational partners which have financially contributed to its development including ontario power generation alberta watersmart mac hydro northwest hydraulics inc associated engineering and the city of calgary we would also like to acknowledge funding partners and organizational users that have otherwise supported raven driven research and consulting projects leading to improvements in the software including alberta environment and parks canarie deltares environment and climate change canada the floodnet nserc strategic research network the government of the northwest territories natural resources canada new brunswick environment and local government ouranos and transalta lastly we thank lieke melsen and four anonymous reviewers for helping to improve this paper 
26022,various methods have been developed for the calibration of cellular automata ca which can produce a plausible cell by cell fit between modeling results and observed land use data however traditional cell based ca models still fail to characterize the aggregate landscape patterns of multiple land use changes to address this problem we introduced a landscape driven multiple ca model that can consider landscape patterns during the calibration procedure genetic algorithm was used to search for the optimal calibration parameters we further investigated the performance of five important landscape metrics in calibration comparisons with two well accepted cell based cas indicated that the modeling results of the proposed method are closer to the observed land use data furthermore we found that patch cohesion and edge density are appropriate landscape objectives for ca calibration in this study more importantly our method can effectively evaluate the performance of different landscape metrics which could provide useful information for land use planning keywords cellular automata landscape pattern multiple lulc change land use planning 1 introduction rapid land use and land cover lulc changes have placed heavy pressure on land resources and exerted profound impacts on environmental conditions he et al 2016 seppelt et al 2013 van vliet et al 2016b verburg et al 2015 monitoring and modeling lulc changes are increasingly important tasks that can provide valuable information for regional planning jakeman et al 2006 tayyebi et al 2016 van delden et al 2010 verstegen et al 2017 voinov 2010 computer based lulc models are useful tools for these tasks because they can effectively simulate the evolution of complex geographical processes athanasiadis and mitkas 2004 clarke et al 1997 mcgarigal et al 2018 omrani et al 2017 stevens et al 2007 verburg et al 2002 notably cellular automata ca are popular tools that have proliferated in studies on lulc changes over the last two decades feng et al 2018a newland et al 2018b van vliet et al 2016a verstegen et al 2014 white et al 1997 significant improvements have been continuously introduced by researchers to make ca better for practical applications since the structure of ca is simple and intrinsically spatial barreira gonzález and barros 2017 batty et al 1999 clarke et al 2007 roodposhti et al 2019 white et al 2012 ca models should be calibrated if they are utilized for simulating and predicting natural phenomena the purpose of calibration is to minimize the differences between the modeling results and observed land use data which can be controlled by a set of parameters within transition rules feng et al 2018b li et al 2017 long and wu 2017 for a long time cell by cell agreement has been the only objective for calibration various studies have been conducted to increase the accuracies of the modeling results using manual statistical e g logistic regression and heuristic methods e g intelligent algorithms barreira gonzález et al 2015 garcía et al 2012 newland et al 2018a verburg and overmars 2009 however ca calibration can still be improved because traditional cell based methods ignore the spatial homogeneity of land use development at local scales recently feng et al 2018b developed a spatial autoregressive based ca for simulating coastal land use changes roodposhti et al 2020 presented an automatic neighborhood detection procedure for ca models he et al 2018 developed a deep learning based method for mining transition rules of ca wang et al 2019 optimized the stochastic component of ca by using a maximum entropy method such current state of art studies found that some landscape metrics of their simulation results are close to those of the observed land use data nevertheless the selected landscape metrics vary quite differently in these attempts and most of them did not discuss the rationale behind the selection moreover they did not clearly demonstrate the underlying relationships between the model settings modifications and those selected metrics in addition the results of some other landscape metrics are not satisfactory the main reason is because these studies were not intrinsically dedicated to the improvement of landscape patterns to address these problems many studies argue that landscape metrics should be considered during calibration to ensure aggregate spatial patterns are captured brown et al 2005 dezhkam et al 2017 li et al 2013 given the existence of stochastic uncertainty in lulc systems lulc models could be overfitted if they produce results in which the locations of changes are strongly consistent with the observed data in other words an overemphasis on cell by cell agreement could substantially affect the generalization and predictive abilities of lulc models brown et al 2005 chen et al 2014 as mentioned above the spatial patterns of lulc changes can be characterized by a series of landscape metrics herold et al 2003 mcgarigal et al 2012 mcgarigal and marks 1995 landscape metrics have been popularly used to evaluate the performance of lulc modeling aguilera et al 2011 herold et al 2002 previous studies have demonstrated that landscape metrics can link economic processes with the associated land use morphology dezhkam et al 2017 herold et al 2005 recently landscape metrics have also been considered during the calibration of lulc models garcía et al 2013 calibrated an urban ca model by considering three landscape metrics including the number of patches mean patch size and edge density li et al 2013 developed a pattern calibrated ca model by taking into account overall accuracy the percentage of landscape largest patch index and landscape division index van delden et al 2012 2011 manually calibrated a land use model by combining kappa coefficient and landscape metrics nevertheless most previous attempts were only tested for simulating binary lulc changes i e from non urban to urban areas in fact the modeling of multiple lulc changes remains a great challenge given the much more complicated non linear relationships of lulc conversion lin et al 2018 liu et al 2017 tayyebi and pijanowski 2014 zhang et al 2019 to date a number of statistical and heuristic techniques have been introduced in the literature such as land use allocation artificial neural networks classification and regression trees e g barredo et al 2003 li and yeh 2002 tayyebi and pijanowski 2014 the land use allocation based ca lua ca is a commonly used model because it is easy to understand and construct barredo et al 2003 moreover many improvements have been made based on this model owing to its flexibility and simplicity he et al 2005 2006 for example the transition rules can be calibrated using either statistical or heuristic methods santé et al 2010 however there are very limited studies that have considered landscape metrics during the calibration of multiple lulc models since a considerable number of geographical variables and parameters will be involved more importantly it is not easy to maintain the similarity of landscape patterns for various land use classes simultaneously in addition another critical concern is that the selected landscape metrics vary quite differently in previous applications as reviewed above a number of different metrics e g number of patches mean patch size edge density largest patch index landscape division index have been used as calibration objectives in different studies it is obvious that the performance of lulc models relies heavily on the calibration objectives designed by policy makers unfortunately the selection of landscape metrics is highly subject to expert experience and preferences the influence of landscape metric selection on modeling results is still rarely discussed to improve the goodness of fit of multiple lulc modeling the performance of landscape metrics in model calibration should be investigated therefore this study aims to shed light on the following two questions 1 does a heuristic calibration method combining landscape metrics and cell based accuracies for a development probability based model i e derived from wu 2002 with multiple land use classes outperform traditional heuristic calibration methods applied to such models and 2 does the selection of landscape metrics matter to model calibration 2 methods in this study we first employed the commonly used land use allocation ca for modeling multiple lulc changes because landscape metrics can be easily incorporated into the calibration procedure building upon this cell based model we introduced landscape driven ca that can consider both landscape patterns and cell by cell accuracy the details of these two models are presented in the following subsections we repeated all the ca models 10 times and took the average of the 10 repeated runs as the final result the methodology is the same if other multiple lulc methods are selected as the basic model 2 1 study area and data the study area guangzhou is the third largest city in china this city has experienced unprecedented rapid urbanization since the adoption of reform and opening up policy these dramatic lulc changes have brought about severe ecological and environmental problems therefore modeling lulc changes in these metropolises becomes an urgent task in this study all the ca models were calibrated based on the observed land use data during 2000 2005 and were validated based on the data during 2005 2010 these datasets were rescaled to a spatial resolution of 500m to reduce the computational cost the final results contain six land use classes farmland forest grassland water body urban land and unused land ca is a development probability based model that adopts a number of spatial variables to estimate the potential of lulc changes although spatial variables are not directly related to landscape metrics the differences in terms of cell based accuracies and or landscape metrics between simulation results and observed data can be minimized through tuning the parameters of spatial variables therefore a series of spatial variables reflecting the driving forces for lulc changes were also prepared equation 2 below building upon previous studies related to the study area three major types of variables were considered chen et al 2014 feng et al 2018b liu et al 2017 1 proximity to centers distance to city center distance to district centers 2 proximity to transportation networks distance to metro stations distance to highways distance to roads and 3 slope representing the physical condition of a location although different spatial variables may lead to different results the comparison between the proposed landscape driven and traditional cell based methods will not be affected because all the ca models were calibrated under the same conditions moreover the calibration objective plays a much more important role than spatial variables in lulc modeling 2 2 cell based land use allocation ca the modeling of multiple lulc changes is a difficult task due to the complicated non linear relationships of lulc conversion to facilitate this process land use allocation ca lua ca has been developed barredo et al 2003 he et al 2005 in this cell based ca model the development probability for each grid cell is estimated as follows 1 p k t l 1 1 exp z k l ω k t l 1 ln γ α where p k t l is the development probability for cell k at time t from the current land use class to class l and z k l denotes the composite score of cell k for class l 2 z k l i d k i b l i b l 0 where d k i is the ith spatial variable value for cell k introduced in section 3 1 b l i is the corresponding parameter for land use class l and b l 0 is a constant ω k t l denotes the neighborhood effect for land use class l on cell k at time t 3 ω k t l i 1 n c o n s i t l n where s i t denotes the land use class of the neighbor cell i at time t if s i t l the con function returns 1 otherwise it returns 0 n is the number of cells within the neighborhood we chose the commonly used 3 3 moore neighborhood in accordance with previous studies li et al 2017 liu et al 2018 the parameters in equation 2 can be calibrated using statistical or heuristic methods feng et al 2018b he et al 2006 however statistical calibration methods that are only based on randomly selected training samples can hardly incorporate landscape metrics into the calibration procedure therefore we adopted heuristic methods for further experiments as a well known approach genetic algorithm ga was selected in this study ga allows landscape metrics calculated from the modeling results to be embedded within its fitness function in fact ga was designed to solve complex mathematical optimization problems based on the theories of natural selection and survival of the fittest golberg 1989 holland 1975 ga has been extensively applied in many disciplines liu et al 2015 maier et al 2019 in this study each group of candidate solutions parameters is encoded as a chromosome individual 4 chromosome a 1 a 2 a m where a i denotes the ith parameter value encoded in the binary system the evolution process starts with an initial population of randomly generated individuals at each generation we evaluate the fitness of every individual using the cell by cell indicator figure of merit fom fom is calculated as follows pontius et al 2008 5 fom c a b c d 100 where a means errors caused by observed change simulated as persistence b means errors caused by observed persistence simulated as change c means agreements caused by observed change simulated as change and d means errors caused by observed change simulated as wrong gaining land use class based on the fitness values these individuals are chosen as parents to reproduce offspring that is the probability of being chosen corresponds to the individual s fitness value next crossover and mutation operations are conducted for evolution the former randomly chooses a subsection of the chromosome and two individuals interchange the numbers therein while the latter randomly alters a subsection of a chromosome the evolution process is largely influenced by the crossover operation although the mutation only introduces a tiny perturbation it can bring new information into the whole population moreover we adopted the elitism strategy to guarantee that the best individual always survives to the succeeding generation the optimal solution can be obtained after numerous iterative runs of selection crossover and mutation operations golberg 1989 holland 1975 the parameters for running the ga were determined in accordance with previous related studies for example studies indicated that a population size ranging from 20 to 200 could generate a desirable optimization result lin et al 2019 liu et al 2015 in addition the crossover operation is assigned with a high probability 0 90 while the mutation is assigned with an extremely low probability 0 01 the methodology is the same if other intelligent algorithms are adopted after calibration we calculated the change probabilities for each grid cell using equation 1 finally the lulc changes were simulated based on the following criteria barredo et al 2003 he et al 2005 1 the area requirements for different land use classes were satisfied one by one in the order of water body urban land forest farmland grassland and unused land this order which is subject to local conditions was determined through trial and error for example if the land use allocation of water body starts after the allocation of urban land then the former will easily fall apart since the majority of the water body crosses through the city proper 2 for each land use class l we focused on the cells with the highest probability of changing to l on the basis of their probability values from high to low the land use class of those cells will be changed to l subject to the area requirements 2 3 landscape driven land use allocation ca cell based ca can hardly characterize the complex landscape patterns of lulc changes to address this problem our study introduced a landscape driven ca for multiple lulc changes as shown in fig 1 first two time series observed land use maps t1 and t2 are used for calibration and simulation in this step landscape metrics are calculated from the t2 map as the reference the main difference between cell based and landscape driven ca is that the former only considers cell based accuracy during the calibration procedure while the latter will also take into account landscape metrics in the second step t2 and a third observed land use map t3 are used for validating the above calibrated ca model various statistics have been developed to measure landscape patterns for every single patch and class patch type in the landscape and for the landscape as a whole correspondingly landscape metrics are grouped into patch class and landscape levels however it is unnecessary and unrealistic to use all the metrics in a single case first the metrics at the patch level were not selected because land use patches may merge and new patches may appear at each run of the ca for example the ground truth land use data and final modeling result will almost always contain different total numbers of patches therefore the spatial characteristics and context of every individual patch can hardly be compared second we did not consider landscape level metrics that cannot distinguish the differences among multiple land use classes specifically the exact value e g patch number for each land use class can vary considerably even if the summation at landscape level total patch number is fixed third a number of metrics could have certain limitations for example perimeter area fractal dimension is subject to small sample issues this metric may greatly exceed the theoretical range in values when the number of patches is small mcgarigal et al 2012 fourth several metrics are redundant for example effective mesh size is inversely correlated with landscape division index and landscape shape index is a standardized measure of edge density in addition some studies have indicated that clumpiness index is positively associated with patch cohesion index cushman et al 2008 fifth user specified files parameters are needed for some metrics e g core area and contrast metrics for example the calculation of core area depends heavily on user specified edge depth after considering these points as a prescreen we selected largest patch index edge density mean patch size landscape division index and patch cohesion index for detailed examination based on local conditions and previous findings garcía et al 2013 herold et al 2005 li et al 2013 in our study area the dramatic lulc changes have caused serious ecological and environmental issues as mentioned above therefore the selection of landscape metrics should also connect lulc change modeling with ecological conservation first we selected largest patch index because this metric shows a strong correlation with species richness and abundance second many studies indicated that edge based metrics can capture complexities in landscape structure that cannot be detected by area based metrics for example edge density can quantify the dynamics in the attributes of specific types of edges ecotones and infer the associated ecological effects third mean patch size was also considered because the study area consists of multiple land use patches fourth landscape division index a useful measure of landscape fragmentation was selected because this fast growing city is easily affected by extension of settlement areas and transportation networks fifth habitat coverage in such landscapes dominated by urban development is so low that the persistence of various species depends heavily on the cohesion of the habitat networks therefore patch cohesion index which can measure the physical connectedness of land use patches was adopted these selected metrics cover the area edge and aggregation aspects of landscape patterns it is desirable to use the smallest number of metrics that could sufficiently characterize landscape patterns for example we selected patch cohesion index instead of clumpiness index because the latter is slightly biased under certain conditions mcgarigal et al 2012 nevertheless the feasibility of landscape driven ca remains unchanged if different landscape metrics are considered in other applications detailed information on the selected metrics is introduced below the first metric is largest patch index lpi an effective measure of dominance in landscape ecology patch refers to a relatively homogeneous area that differs from its surroundings e g an area of grassland surrounded by urban land lpi can easily quantify the percentage of total land use area comprised by the largest land use patch therefore lpi is calculated as follows mcgarigal et al 2012 6 lpi max a i a 100 where max a i is the area of the largest patch of land use class i and a is the total land use area the second metric is edge density ed ed is a useful measure of edge length for a particular land use class mcgarigal et al 2012 7 ed k 1 m e i k a 10000 where k 1 m e i k is the total length of edge involving land use class i and a is the total land use area the third metric is mean patch size mps which combines the information on both patch number and patch area mps is calculated as follows garcía et al 2013 8 mps a i n i 100 where a i is the total land use area for land use class i and n i is the number of patches for land use class i the fourth metric is landscape division index division which represents the probability that two randomly chosen land use cells are not situated in the same patch of a particular land use class therefore division is calculated based on the cumulative patch area distribution jaeger 2000 9 division 1 j 1 n a i j a 2 where a ij is the area of patch j of land use class i n is the number of patches for land use class i and a is the total land use area the last metric is patch cohesion index cohesion which can measure the physical connectedness of every land use class cohesion increases as the grid cells become more aggregated and clumped in their spatial distribution mcgarigal et al 2012 10 cohesion 1 j 1 n p i j j 1 n p i j a i j 1 1 z 1 100 where p i j is the perimeter in terms of cell numbers of patch j of land use class i a i j is the area in terms of cell numbers of patch j of land use class i z is the total number of cells in a given area and n is the number of patches for land use class i since landscape driven ca involves two equally important objectives cell based agreement and landscape similarity a weighted linear combination could be used to produce a composite score with equal consideration eastman et al 1998 malczewski 2006 although such a combination may have limitations it is still an effective and convenient measure for tackling multi objective optimization problems we first incorporated each landscape metric into the fitness function of ga one by one and then simultaneously incorporated all these metrics therefore the fitness function combined error f x in landscape driven ca is improved as follows 11 f x w f 1 fom w l le where w f and w l denote the weights for the two objectives and le denotes the landscape error which is a normalized combination of the above landscape metrics 12 le i 1 m j 1 n lm i j lm i j actual lm i j actual m n where lm ij is the ith landscape metric value for land use class j calculated from the modeling result lm i j actual is the actual value calculated from the observed land use data m is the number of landscape metrics and n is the number of land use classes all these metrics were calculated at class level the two weights in equation 11 can be determined based on land use planning strategies according to li et al 2013 if w f is much greater than w l then this model is similar to the traditional cell based lua ca which completely ignores landscape patterns by contrast the cell by cell agreements of modeling results cannot be guaranteed if w l is much greater than w f therefore these two weights are both set to 0 5 so that the two calibration objectives can be considered equally 2 4 cell based artificial neural network ca for a better comparison we also built the well known artificial neural network ca ann ca li and yeh 2002 the idea of ann is to imitate the learning and recalling abilities of humans at each iteration the ca module simulates and predicts lulc changes according to the change probabilities estimated by the ann module in this study the input layer of the ann module contains m neurons to represent the variables related to lulc changes while the hidden layer includes 2 3m neurons n neurons are needed in the output layer to generate development probability values corresponding to n land use classes we randomly selected 20 of samples from the observed land use data the training samples were used for calibrating the ann ca while the testing samples were used to examine whether the model was overfitted more detailed information on the ann ca can be found in the literature li and yeh 2002 tayyebi et al 2011 unfortunately it is very difficult to incorporate landscape metrics into the calibration procedure since this model involves a huge number of black box weights particularly in multiple lulc change modeling we also used the normal ann ca for comparison because the assessment of lulc models in many previous studies e g dezhkam et al 2017 herold et al 2005 santé et al 2010 is based on a combined use of cell based agreement and landscape metrics even if landscape patterns are completely ignored during calibration this comparison could directly assess the performance of the proposed landscape driven strategy and traditional cell based strategy to thoroughly compare the goodness of fit of the above three ca models we calculated additional landscape metrics and cell based accuracies of all the modeling results in addition to the five landscape metrics mentioned above the remaining class level metrics introduced in the fragstats package were also considered for a more comprehensive evaluation these new metrics include the perimeter area fractal dimension pafrac splitting index split effective mesh size mesh clumpiness clumpy landscape shape index lsi aggregation index ai interspersion and juxtaposition index iji and percentage of like adjacencies pladj the detailed definitions of these metrics can be found in mcgarigal et al 2012 in addition kappa coefficient and overall accuracy were adopted to further quantify the cell by cell agreement of different modeling results visser and de nijs 2006 the kappa coefficient is calculated as follows 13 kappa p o p e 1 p e where p o is the percentage of samples that are correctly simulated out of the total number of samples i e overall accuracy and p e is the hypothetical probability of chance agreement we just used fom in the calculation of combined error because pontius et al 2008 pointed out that the metrics affected by large numbers of persistence land use grid cells e g kappa are potentially misleading fom the ratio of the intersection of observed changes and simulated changes to the union of observed changes and simulated changes is therefore a more reasonable metric for lulc model assessment pontius et al 2008 3 implementation and results 3 1 assessment of the cell based lua ca landscape driven lua ca and ann ca first we built the cell based lua ca in the matlab platform ga was used to search for the optimal parameters equation 2 between the dependent variable lulc change and the independent variables spatial variables after calibration we used this ca model to simulate the lulc changes in guangzhou as illustrated in fig 2 the simulation result was compared with the observed land use data in 2005 to evaluate the performance of the cell based lua ca second the proposed landscape driven ca was also constructed in the matlab platform similar to the cell based lua ca we used ga to search for the optimal transition rule the difference is that landscape metrics were also considered in the fitness function of the ga i e equations 11 and 12 after calibration we simulated the multiple lulc changes from 2000 to 2005 based on the calibrated model the simulation result and its accuracies are presented in fig 2c and table 1 respectively third we also simulated the multiple lulc changes during 2000 2005 based on the calibrated ann ca with the result shown in fig 2d the higher cell by cell accuracies for the ann ca may be attributed to its ability to capture complex non linear relationships between independent and dependent variables through iterative learning recall operations li and yeh 2002 pijanowski et al 2002 however from a macroscopic perspective circled parts in fig 2 we found that our simulation results are more similar to the observed land use data in terms of aggregate landscape patterns in addition we can observe from the inset maps that the result of the cell based lua ca includes more grassland and water bodies while the urban land farmland and forest simulated by the cell based ann ca are more agglomerated than expected these visual comparisons indicated that the landscape driven lua ca performs better than the other two methods in addition the goodness of fit of the three results is summarized in table 1 with the detailed calculation presented in the supplementary material we found that the landscape driven ca improved the simulation performance in terms of landscape similarity even though the three results share relatively similar cell by cell accuracies as expected the landscape driven ca yields the smallest errors regarding the five calibration metrics more importantly this proposed method also shows the highest similarity in terms of the other eight landscape metrics landscape error in table 1 in addition the combined errors calculated via equation 11 of cell based lua ca and ann ca are 0 6267 and 0 5723 while the proposed method only has an error of 0 5324 15 05 and 6 97 lower respectively this comparison demonstrated that the landscape driven ca performs better than the traditional cell based ca since the eight new landscape metrics were not considered during the calibration to validate the prediction performance of the landscape driven ca we further simulated the multiple lulc changes during 2005 2010 based on the three calibrated models cell based lua ca ann ca and landscape driven ca this experiment can compare their generalization abilities the prediction results and their corresponding errors are displayed in fig 3 and table 2 respectively we found that the landscape driven ca still outperforms the other two despite the five calibration metrics the proposed method still yields the smallest landscape error via equation 12 regarding the eight new landscape metrics compared with the observed land use data in 2010 the combined errors via equation 11 of the cell based lua ca and ann ca are 0 6729 and 0 6561 respectively however the landscape driven ca model can reduce this error to merely 0 5421 the same conclusion can be reached through the visual comparison in fig 3 we can observe from both the inset maps and the circled parts that the prediction result of the landscape driven lua ca shares more similar aggregate landscape patterns with the observed land use data as shown in the inset maps for example this proposed method almost replicates the ring shaped configuration in the ground truth data moreover nearly every land use class predicted by the two cell based methods is still more agglomerated than expected more importantly the differences in cell by cell accuracies between the landscape driven lua ca and the other two methods are much lower compared with the differences in the calibration period see table 1 last but not least the combined error of our proposed method is still 19 44 and 17 38 lower than those of the cell based lua ca and ann ca respectively all these comparisons indicated that the landscape driven lua ca has much better generalization ability 3 2 assessment of the landscape driven ca with different landscape metrics to further investigate the performance of those five landscape metrics equations 6 10 in ca calibration we also designed five simulation scenarios that used each metric individually all the simulation results are presented in fig 4 and their corresponding errors are summarized in table 3 although the differences in landscape patterns among various simulation results cannot be easily distinguished through a visual comparison the error values still considerably vary as expected the best simulation result was obtained with the support of all five metrics in addition the model calibrated using cohesion yielded the second best result patch cohesion is sensitive to the aggregation of the focal class and will increase as the land use cells become more physically connected this experiment suggested that patch cohesion is an informative metric for calibrating the ca model in this study next the third and fourth best results were obtained using ed and lpi respectively the length of the edge in a landscape is critical to various ecological phenomena therefore it is not surprising that ed can offer abundant information on landscape patterns moreover the landscape patterns of dominant land use patches may also play important roles in land use development last we found that the mere consideration of patch number and size division mps can neither well characterize the whole landscape pattern nor easily get the cell by cell locations right overall the above experiments indicated that cohesion ed and lpi are appropriate landscape objectives for ca calibration in this study finally we further calibrated a ca model during 2000 2005 using the best two landscape metrics i e cohesion and ed simultaneously as presented in table 4 the comparison has indicated that this simulation result in 2005 is comparable with the result calibrated using all five landscape metrics for example the combined errors of these two results are similar 0 5381 and 0 5324 therefore it is possible to build a desirable lulc model when using only a small number of landscape metrics as the calibration objective 3 3 discussion 3 3 1 advantages of landscape driven ca cell based agreement and landscape similarity are two equally important objectives for ca calibration although a simulation result will perfectly match with the landscape metrics of the observed land use data if it has a cell based agreement of 100 this is rarely the case in practice therefore a question arises as to whether the increase in the similarity of landscape patterns is worth the decrease in cell based accuracies to shed some light on this question we take urban expansion the dominant type of land use changes in the study area as an example see fig 5 in general the newly urbanized grid cells simulated by the traditional cell based cas are almost evenly distributed around preexisting urban land this observation explains why the urban land of cell based methods is more agglomerated than expected however not all land use grid cells surrounded by urban land will be urbanized in the real world for example some water bodies and green spaces that are essential to the local environment will remain unchanged over time although these small grid cells simulated by the proposed landscape driven ca may not exactly fall within the ground truth location the simulation results can still call policy makers attention to those tiny but fundamental elements during land use planning processes policy makers could also manually correct the locations for these grid cells in this regard the landscape driven ca should be much more informative for land use planners in a rapidly changing landscape in addition one of the most important applications of calibrated lulc models is that their prediction results could support policy making in long term land use planning an overemphasis on cell by cell agreement could substantially affect the generalization and predictive abilities of lulc models since there exist no ground truth land use data for years to come the ability to capture and reproduce aggregate landscape patterns is especially valuable for policy makers to assess the potential influences of various land use development strategies for example numerous studies have demonstrated that the land use morphology of a region could exert profound influences on its carbon emissions land surface temperature environmental conservation and so on chen et al 2011 privitera et al 2018 moreover spatial configuration e g compactness and connectedness is one of the most important criteria in land use zoning and management lin and li 2019 therefore it is necessary for land use planners to appropriately consider landscape patterns especially those important metrics during land use planning and optimization processes in particular we could incorporate some specific landscape metrics for ca calibration and then predict future lulc change scenarios based on the calibrated model some important aspects of urban development e g compact city low carbon city can be systematically explored in this manner it should also be noted that not all landscape metrics are effective for the calibration of lulc models even though the metrics were selected with reference in this study for example the simulation results are even worse than those simulated by traditional cell based methods if inappropriate metrics are adopted e g the comparison between tables 1 and 3 nevertheless the proposed landscape driven ca still works if inexperienced modelers don t know how to choose a set of metrics it is because our method can effectively evaluate the performance of different landscape metrics in calibration in future applications any landscape metric that is potentially important for the calibration of multiple lulc models can be incorporated in equations 11 and 12 for detailed evaluation 3 3 2 disadvantages of landscape driven ca we noticed that some anomalies occur in the comparisons of landscape similarity for example the landscape driven ca has the highest error values of interspersion and juxtaposition index table 1 compared with the other two methods we referred to the detailed calculation in the supplementary material and found that this error is mainly attributed to unused land a small land use class that is difficult to control similarly although the model calibrated using only ed does not return the highest similarity of this metric table 3 the error is mainly attributed to a certain land use class in addition this model and the model calibrated using only mps share similar errors of ed and mps these two metrics are probably related in this case therefore we think that these phenomena are inevitable and acceptable since not all landscape metrics were considered during the calibration procedures nevertheless the proposed method still has some limitations that need to be addressed in the future first while the similarity of aggregate landscape patterns can be considerably increased by the proposed landscape driven ca cell based agreement cannot be guaranteed in future studies we will try to simultaneously improve these two equally important objectives second only five relevant and commonly used landscape metrics have been examined in this study we could investigate different metrics in future applications given the methodology is the same third we did not choose the ann ca as the basic model because it is very difficult to optimize a huge number of weights e g 171 in this study using intelligent algorithms within an acceptable timeframe finally the potential influence of spatial resolution scale on the results remains to be investigated on the one hand if the spatial resolution of land use data is too coarse then the geometrical information of landscape patterns will be largely lost and the risk of redundancies in landscape metrics may increase on the other hand the computational burden will increase if land use data with a very fine resolution are used 4 conclusions since ca models were introduced for analyzing geographical phenomena various methods have been developed to facilitate the modeling of lulc changes although ca can produce an acceptable cell by cell fit between the modeling results and observed land use data this study has indicated that the commonly used cell based cas still cannot characterize the complex landscape patterns of multiple lulc changes in our experiments the landscape metrics of the traditional modeling results are substantially different from those of the observed data to address this problem we introduced a landscape driven multiple ca that can consider landscape patterns during calibration this proposed model builds upon the cell based lua ca in which ga is used to search for the optimal transition rule both the landscape metrics and cell by cell accuracy are incorporated into the fitness function of the ga we also investigated the performance of five important landscape metrics in calibration the proposed method was applied to the modeling of multiple lulc changes in guangzhou during 2000 2010 two well accepted cell based ca models were also calibrated for comparison we found that cohesion ed and lpi are appropriate landscape objectives for ca calibration in this study in addition a number of experiments demonstrated that landscape driven ca can significantly improve the simulation and prediction performances in terms of landscape similarity our modeling results are closer to the observed land use data according to the combined errors we therefore argue that the proposed method is promising for providing valuable information for land use planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china grant no 41801307 no 41871290 no 41531176 we thank the anonymous reviewers for their constructive comments and suggestions that greatly improved this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104719 
26022,various methods have been developed for the calibration of cellular automata ca which can produce a plausible cell by cell fit between modeling results and observed land use data however traditional cell based ca models still fail to characterize the aggregate landscape patterns of multiple land use changes to address this problem we introduced a landscape driven multiple ca model that can consider landscape patterns during the calibration procedure genetic algorithm was used to search for the optimal calibration parameters we further investigated the performance of five important landscape metrics in calibration comparisons with two well accepted cell based cas indicated that the modeling results of the proposed method are closer to the observed land use data furthermore we found that patch cohesion and edge density are appropriate landscape objectives for ca calibration in this study more importantly our method can effectively evaluate the performance of different landscape metrics which could provide useful information for land use planning keywords cellular automata landscape pattern multiple lulc change land use planning 1 introduction rapid land use and land cover lulc changes have placed heavy pressure on land resources and exerted profound impacts on environmental conditions he et al 2016 seppelt et al 2013 van vliet et al 2016b verburg et al 2015 monitoring and modeling lulc changes are increasingly important tasks that can provide valuable information for regional planning jakeman et al 2006 tayyebi et al 2016 van delden et al 2010 verstegen et al 2017 voinov 2010 computer based lulc models are useful tools for these tasks because they can effectively simulate the evolution of complex geographical processes athanasiadis and mitkas 2004 clarke et al 1997 mcgarigal et al 2018 omrani et al 2017 stevens et al 2007 verburg et al 2002 notably cellular automata ca are popular tools that have proliferated in studies on lulc changes over the last two decades feng et al 2018a newland et al 2018b van vliet et al 2016a verstegen et al 2014 white et al 1997 significant improvements have been continuously introduced by researchers to make ca better for practical applications since the structure of ca is simple and intrinsically spatial barreira gonzález and barros 2017 batty et al 1999 clarke et al 2007 roodposhti et al 2019 white et al 2012 ca models should be calibrated if they are utilized for simulating and predicting natural phenomena the purpose of calibration is to minimize the differences between the modeling results and observed land use data which can be controlled by a set of parameters within transition rules feng et al 2018b li et al 2017 long and wu 2017 for a long time cell by cell agreement has been the only objective for calibration various studies have been conducted to increase the accuracies of the modeling results using manual statistical e g logistic regression and heuristic methods e g intelligent algorithms barreira gonzález et al 2015 garcía et al 2012 newland et al 2018a verburg and overmars 2009 however ca calibration can still be improved because traditional cell based methods ignore the spatial homogeneity of land use development at local scales recently feng et al 2018b developed a spatial autoregressive based ca for simulating coastal land use changes roodposhti et al 2020 presented an automatic neighborhood detection procedure for ca models he et al 2018 developed a deep learning based method for mining transition rules of ca wang et al 2019 optimized the stochastic component of ca by using a maximum entropy method such current state of art studies found that some landscape metrics of their simulation results are close to those of the observed land use data nevertheless the selected landscape metrics vary quite differently in these attempts and most of them did not discuss the rationale behind the selection moreover they did not clearly demonstrate the underlying relationships between the model settings modifications and those selected metrics in addition the results of some other landscape metrics are not satisfactory the main reason is because these studies were not intrinsically dedicated to the improvement of landscape patterns to address these problems many studies argue that landscape metrics should be considered during calibration to ensure aggregate spatial patterns are captured brown et al 2005 dezhkam et al 2017 li et al 2013 given the existence of stochastic uncertainty in lulc systems lulc models could be overfitted if they produce results in which the locations of changes are strongly consistent with the observed data in other words an overemphasis on cell by cell agreement could substantially affect the generalization and predictive abilities of lulc models brown et al 2005 chen et al 2014 as mentioned above the spatial patterns of lulc changes can be characterized by a series of landscape metrics herold et al 2003 mcgarigal et al 2012 mcgarigal and marks 1995 landscape metrics have been popularly used to evaluate the performance of lulc modeling aguilera et al 2011 herold et al 2002 previous studies have demonstrated that landscape metrics can link economic processes with the associated land use morphology dezhkam et al 2017 herold et al 2005 recently landscape metrics have also been considered during the calibration of lulc models garcía et al 2013 calibrated an urban ca model by considering three landscape metrics including the number of patches mean patch size and edge density li et al 2013 developed a pattern calibrated ca model by taking into account overall accuracy the percentage of landscape largest patch index and landscape division index van delden et al 2012 2011 manually calibrated a land use model by combining kappa coefficient and landscape metrics nevertheless most previous attempts were only tested for simulating binary lulc changes i e from non urban to urban areas in fact the modeling of multiple lulc changes remains a great challenge given the much more complicated non linear relationships of lulc conversion lin et al 2018 liu et al 2017 tayyebi and pijanowski 2014 zhang et al 2019 to date a number of statistical and heuristic techniques have been introduced in the literature such as land use allocation artificial neural networks classification and regression trees e g barredo et al 2003 li and yeh 2002 tayyebi and pijanowski 2014 the land use allocation based ca lua ca is a commonly used model because it is easy to understand and construct barredo et al 2003 moreover many improvements have been made based on this model owing to its flexibility and simplicity he et al 2005 2006 for example the transition rules can be calibrated using either statistical or heuristic methods santé et al 2010 however there are very limited studies that have considered landscape metrics during the calibration of multiple lulc models since a considerable number of geographical variables and parameters will be involved more importantly it is not easy to maintain the similarity of landscape patterns for various land use classes simultaneously in addition another critical concern is that the selected landscape metrics vary quite differently in previous applications as reviewed above a number of different metrics e g number of patches mean patch size edge density largest patch index landscape division index have been used as calibration objectives in different studies it is obvious that the performance of lulc models relies heavily on the calibration objectives designed by policy makers unfortunately the selection of landscape metrics is highly subject to expert experience and preferences the influence of landscape metric selection on modeling results is still rarely discussed to improve the goodness of fit of multiple lulc modeling the performance of landscape metrics in model calibration should be investigated therefore this study aims to shed light on the following two questions 1 does a heuristic calibration method combining landscape metrics and cell based accuracies for a development probability based model i e derived from wu 2002 with multiple land use classes outperform traditional heuristic calibration methods applied to such models and 2 does the selection of landscape metrics matter to model calibration 2 methods in this study we first employed the commonly used land use allocation ca for modeling multiple lulc changes because landscape metrics can be easily incorporated into the calibration procedure building upon this cell based model we introduced landscape driven ca that can consider both landscape patterns and cell by cell accuracy the details of these two models are presented in the following subsections we repeated all the ca models 10 times and took the average of the 10 repeated runs as the final result the methodology is the same if other multiple lulc methods are selected as the basic model 2 1 study area and data the study area guangzhou is the third largest city in china this city has experienced unprecedented rapid urbanization since the adoption of reform and opening up policy these dramatic lulc changes have brought about severe ecological and environmental problems therefore modeling lulc changes in these metropolises becomes an urgent task in this study all the ca models were calibrated based on the observed land use data during 2000 2005 and were validated based on the data during 2005 2010 these datasets were rescaled to a spatial resolution of 500m to reduce the computational cost the final results contain six land use classes farmland forest grassland water body urban land and unused land ca is a development probability based model that adopts a number of spatial variables to estimate the potential of lulc changes although spatial variables are not directly related to landscape metrics the differences in terms of cell based accuracies and or landscape metrics between simulation results and observed data can be minimized through tuning the parameters of spatial variables therefore a series of spatial variables reflecting the driving forces for lulc changes were also prepared equation 2 below building upon previous studies related to the study area three major types of variables were considered chen et al 2014 feng et al 2018b liu et al 2017 1 proximity to centers distance to city center distance to district centers 2 proximity to transportation networks distance to metro stations distance to highways distance to roads and 3 slope representing the physical condition of a location although different spatial variables may lead to different results the comparison between the proposed landscape driven and traditional cell based methods will not be affected because all the ca models were calibrated under the same conditions moreover the calibration objective plays a much more important role than spatial variables in lulc modeling 2 2 cell based land use allocation ca the modeling of multiple lulc changes is a difficult task due to the complicated non linear relationships of lulc conversion to facilitate this process land use allocation ca lua ca has been developed barredo et al 2003 he et al 2005 in this cell based ca model the development probability for each grid cell is estimated as follows 1 p k t l 1 1 exp z k l ω k t l 1 ln γ α where p k t l is the development probability for cell k at time t from the current land use class to class l and z k l denotes the composite score of cell k for class l 2 z k l i d k i b l i b l 0 where d k i is the ith spatial variable value for cell k introduced in section 3 1 b l i is the corresponding parameter for land use class l and b l 0 is a constant ω k t l denotes the neighborhood effect for land use class l on cell k at time t 3 ω k t l i 1 n c o n s i t l n where s i t denotes the land use class of the neighbor cell i at time t if s i t l the con function returns 1 otherwise it returns 0 n is the number of cells within the neighborhood we chose the commonly used 3 3 moore neighborhood in accordance with previous studies li et al 2017 liu et al 2018 the parameters in equation 2 can be calibrated using statistical or heuristic methods feng et al 2018b he et al 2006 however statistical calibration methods that are only based on randomly selected training samples can hardly incorporate landscape metrics into the calibration procedure therefore we adopted heuristic methods for further experiments as a well known approach genetic algorithm ga was selected in this study ga allows landscape metrics calculated from the modeling results to be embedded within its fitness function in fact ga was designed to solve complex mathematical optimization problems based on the theories of natural selection and survival of the fittest golberg 1989 holland 1975 ga has been extensively applied in many disciplines liu et al 2015 maier et al 2019 in this study each group of candidate solutions parameters is encoded as a chromosome individual 4 chromosome a 1 a 2 a m where a i denotes the ith parameter value encoded in the binary system the evolution process starts with an initial population of randomly generated individuals at each generation we evaluate the fitness of every individual using the cell by cell indicator figure of merit fom fom is calculated as follows pontius et al 2008 5 fom c a b c d 100 where a means errors caused by observed change simulated as persistence b means errors caused by observed persistence simulated as change c means agreements caused by observed change simulated as change and d means errors caused by observed change simulated as wrong gaining land use class based on the fitness values these individuals are chosen as parents to reproduce offspring that is the probability of being chosen corresponds to the individual s fitness value next crossover and mutation operations are conducted for evolution the former randomly chooses a subsection of the chromosome and two individuals interchange the numbers therein while the latter randomly alters a subsection of a chromosome the evolution process is largely influenced by the crossover operation although the mutation only introduces a tiny perturbation it can bring new information into the whole population moreover we adopted the elitism strategy to guarantee that the best individual always survives to the succeeding generation the optimal solution can be obtained after numerous iterative runs of selection crossover and mutation operations golberg 1989 holland 1975 the parameters for running the ga were determined in accordance with previous related studies for example studies indicated that a population size ranging from 20 to 200 could generate a desirable optimization result lin et al 2019 liu et al 2015 in addition the crossover operation is assigned with a high probability 0 90 while the mutation is assigned with an extremely low probability 0 01 the methodology is the same if other intelligent algorithms are adopted after calibration we calculated the change probabilities for each grid cell using equation 1 finally the lulc changes were simulated based on the following criteria barredo et al 2003 he et al 2005 1 the area requirements for different land use classes were satisfied one by one in the order of water body urban land forest farmland grassland and unused land this order which is subject to local conditions was determined through trial and error for example if the land use allocation of water body starts after the allocation of urban land then the former will easily fall apart since the majority of the water body crosses through the city proper 2 for each land use class l we focused on the cells with the highest probability of changing to l on the basis of their probability values from high to low the land use class of those cells will be changed to l subject to the area requirements 2 3 landscape driven land use allocation ca cell based ca can hardly characterize the complex landscape patterns of lulc changes to address this problem our study introduced a landscape driven ca for multiple lulc changes as shown in fig 1 first two time series observed land use maps t1 and t2 are used for calibration and simulation in this step landscape metrics are calculated from the t2 map as the reference the main difference between cell based and landscape driven ca is that the former only considers cell based accuracy during the calibration procedure while the latter will also take into account landscape metrics in the second step t2 and a third observed land use map t3 are used for validating the above calibrated ca model various statistics have been developed to measure landscape patterns for every single patch and class patch type in the landscape and for the landscape as a whole correspondingly landscape metrics are grouped into patch class and landscape levels however it is unnecessary and unrealistic to use all the metrics in a single case first the metrics at the patch level were not selected because land use patches may merge and new patches may appear at each run of the ca for example the ground truth land use data and final modeling result will almost always contain different total numbers of patches therefore the spatial characteristics and context of every individual patch can hardly be compared second we did not consider landscape level metrics that cannot distinguish the differences among multiple land use classes specifically the exact value e g patch number for each land use class can vary considerably even if the summation at landscape level total patch number is fixed third a number of metrics could have certain limitations for example perimeter area fractal dimension is subject to small sample issues this metric may greatly exceed the theoretical range in values when the number of patches is small mcgarigal et al 2012 fourth several metrics are redundant for example effective mesh size is inversely correlated with landscape division index and landscape shape index is a standardized measure of edge density in addition some studies have indicated that clumpiness index is positively associated with patch cohesion index cushman et al 2008 fifth user specified files parameters are needed for some metrics e g core area and contrast metrics for example the calculation of core area depends heavily on user specified edge depth after considering these points as a prescreen we selected largest patch index edge density mean patch size landscape division index and patch cohesion index for detailed examination based on local conditions and previous findings garcía et al 2013 herold et al 2005 li et al 2013 in our study area the dramatic lulc changes have caused serious ecological and environmental issues as mentioned above therefore the selection of landscape metrics should also connect lulc change modeling with ecological conservation first we selected largest patch index because this metric shows a strong correlation with species richness and abundance second many studies indicated that edge based metrics can capture complexities in landscape structure that cannot be detected by area based metrics for example edge density can quantify the dynamics in the attributes of specific types of edges ecotones and infer the associated ecological effects third mean patch size was also considered because the study area consists of multiple land use patches fourth landscape division index a useful measure of landscape fragmentation was selected because this fast growing city is easily affected by extension of settlement areas and transportation networks fifth habitat coverage in such landscapes dominated by urban development is so low that the persistence of various species depends heavily on the cohesion of the habitat networks therefore patch cohesion index which can measure the physical connectedness of land use patches was adopted these selected metrics cover the area edge and aggregation aspects of landscape patterns it is desirable to use the smallest number of metrics that could sufficiently characterize landscape patterns for example we selected patch cohesion index instead of clumpiness index because the latter is slightly biased under certain conditions mcgarigal et al 2012 nevertheless the feasibility of landscape driven ca remains unchanged if different landscape metrics are considered in other applications detailed information on the selected metrics is introduced below the first metric is largest patch index lpi an effective measure of dominance in landscape ecology patch refers to a relatively homogeneous area that differs from its surroundings e g an area of grassland surrounded by urban land lpi can easily quantify the percentage of total land use area comprised by the largest land use patch therefore lpi is calculated as follows mcgarigal et al 2012 6 lpi max a i a 100 where max a i is the area of the largest patch of land use class i and a is the total land use area the second metric is edge density ed ed is a useful measure of edge length for a particular land use class mcgarigal et al 2012 7 ed k 1 m e i k a 10000 where k 1 m e i k is the total length of edge involving land use class i and a is the total land use area the third metric is mean patch size mps which combines the information on both patch number and patch area mps is calculated as follows garcía et al 2013 8 mps a i n i 100 where a i is the total land use area for land use class i and n i is the number of patches for land use class i the fourth metric is landscape division index division which represents the probability that two randomly chosen land use cells are not situated in the same patch of a particular land use class therefore division is calculated based on the cumulative patch area distribution jaeger 2000 9 division 1 j 1 n a i j a 2 where a ij is the area of patch j of land use class i n is the number of patches for land use class i and a is the total land use area the last metric is patch cohesion index cohesion which can measure the physical connectedness of every land use class cohesion increases as the grid cells become more aggregated and clumped in their spatial distribution mcgarigal et al 2012 10 cohesion 1 j 1 n p i j j 1 n p i j a i j 1 1 z 1 100 where p i j is the perimeter in terms of cell numbers of patch j of land use class i a i j is the area in terms of cell numbers of patch j of land use class i z is the total number of cells in a given area and n is the number of patches for land use class i since landscape driven ca involves two equally important objectives cell based agreement and landscape similarity a weighted linear combination could be used to produce a composite score with equal consideration eastman et al 1998 malczewski 2006 although such a combination may have limitations it is still an effective and convenient measure for tackling multi objective optimization problems we first incorporated each landscape metric into the fitness function of ga one by one and then simultaneously incorporated all these metrics therefore the fitness function combined error f x in landscape driven ca is improved as follows 11 f x w f 1 fom w l le where w f and w l denote the weights for the two objectives and le denotes the landscape error which is a normalized combination of the above landscape metrics 12 le i 1 m j 1 n lm i j lm i j actual lm i j actual m n where lm ij is the ith landscape metric value for land use class j calculated from the modeling result lm i j actual is the actual value calculated from the observed land use data m is the number of landscape metrics and n is the number of land use classes all these metrics were calculated at class level the two weights in equation 11 can be determined based on land use planning strategies according to li et al 2013 if w f is much greater than w l then this model is similar to the traditional cell based lua ca which completely ignores landscape patterns by contrast the cell by cell agreements of modeling results cannot be guaranteed if w l is much greater than w f therefore these two weights are both set to 0 5 so that the two calibration objectives can be considered equally 2 4 cell based artificial neural network ca for a better comparison we also built the well known artificial neural network ca ann ca li and yeh 2002 the idea of ann is to imitate the learning and recalling abilities of humans at each iteration the ca module simulates and predicts lulc changes according to the change probabilities estimated by the ann module in this study the input layer of the ann module contains m neurons to represent the variables related to lulc changes while the hidden layer includes 2 3m neurons n neurons are needed in the output layer to generate development probability values corresponding to n land use classes we randomly selected 20 of samples from the observed land use data the training samples were used for calibrating the ann ca while the testing samples were used to examine whether the model was overfitted more detailed information on the ann ca can be found in the literature li and yeh 2002 tayyebi et al 2011 unfortunately it is very difficult to incorporate landscape metrics into the calibration procedure since this model involves a huge number of black box weights particularly in multiple lulc change modeling we also used the normal ann ca for comparison because the assessment of lulc models in many previous studies e g dezhkam et al 2017 herold et al 2005 santé et al 2010 is based on a combined use of cell based agreement and landscape metrics even if landscape patterns are completely ignored during calibration this comparison could directly assess the performance of the proposed landscape driven strategy and traditional cell based strategy to thoroughly compare the goodness of fit of the above three ca models we calculated additional landscape metrics and cell based accuracies of all the modeling results in addition to the five landscape metrics mentioned above the remaining class level metrics introduced in the fragstats package were also considered for a more comprehensive evaluation these new metrics include the perimeter area fractal dimension pafrac splitting index split effective mesh size mesh clumpiness clumpy landscape shape index lsi aggregation index ai interspersion and juxtaposition index iji and percentage of like adjacencies pladj the detailed definitions of these metrics can be found in mcgarigal et al 2012 in addition kappa coefficient and overall accuracy were adopted to further quantify the cell by cell agreement of different modeling results visser and de nijs 2006 the kappa coefficient is calculated as follows 13 kappa p o p e 1 p e where p o is the percentage of samples that are correctly simulated out of the total number of samples i e overall accuracy and p e is the hypothetical probability of chance agreement we just used fom in the calculation of combined error because pontius et al 2008 pointed out that the metrics affected by large numbers of persistence land use grid cells e g kappa are potentially misleading fom the ratio of the intersection of observed changes and simulated changes to the union of observed changes and simulated changes is therefore a more reasonable metric for lulc model assessment pontius et al 2008 3 implementation and results 3 1 assessment of the cell based lua ca landscape driven lua ca and ann ca first we built the cell based lua ca in the matlab platform ga was used to search for the optimal parameters equation 2 between the dependent variable lulc change and the independent variables spatial variables after calibration we used this ca model to simulate the lulc changes in guangzhou as illustrated in fig 2 the simulation result was compared with the observed land use data in 2005 to evaluate the performance of the cell based lua ca second the proposed landscape driven ca was also constructed in the matlab platform similar to the cell based lua ca we used ga to search for the optimal transition rule the difference is that landscape metrics were also considered in the fitness function of the ga i e equations 11 and 12 after calibration we simulated the multiple lulc changes from 2000 to 2005 based on the calibrated model the simulation result and its accuracies are presented in fig 2c and table 1 respectively third we also simulated the multiple lulc changes during 2000 2005 based on the calibrated ann ca with the result shown in fig 2d the higher cell by cell accuracies for the ann ca may be attributed to its ability to capture complex non linear relationships between independent and dependent variables through iterative learning recall operations li and yeh 2002 pijanowski et al 2002 however from a macroscopic perspective circled parts in fig 2 we found that our simulation results are more similar to the observed land use data in terms of aggregate landscape patterns in addition we can observe from the inset maps that the result of the cell based lua ca includes more grassland and water bodies while the urban land farmland and forest simulated by the cell based ann ca are more agglomerated than expected these visual comparisons indicated that the landscape driven lua ca performs better than the other two methods in addition the goodness of fit of the three results is summarized in table 1 with the detailed calculation presented in the supplementary material we found that the landscape driven ca improved the simulation performance in terms of landscape similarity even though the three results share relatively similar cell by cell accuracies as expected the landscape driven ca yields the smallest errors regarding the five calibration metrics more importantly this proposed method also shows the highest similarity in terms of the other eight landscape metrics landscape error in table 1 in addition the combined errors calculated via equation 11 of cell based lua ca and ann ca are 0 6267 and 0 5723 while the proposed method only has an error of 0 5324 15 05 and 6 97 lower respectively this comparison demonstrated that the landscape driven ca performs better than the traditional cell based ca since the eight new landscape metrics were not considered during the calibration to validate the prediction performance of the landscape driven ca we further simulated the multiple lulc changes during 2005 2010 based on the three calibrated models cell based lua ca ann ca and landscape driven ca this experiment can compare their generalization abilities the prediction results and their corresponding errors are displayed in fig 3 and table 2 respectively we found that the landscape driven ca still outperforms the other two despite the five calibration metrics the proposed method still yields the smallest landscape error via equation 12 regarding the eight new landscape metrics compared with the observed land use data in 2010 the combined errors via equation 11 of the cell based lua ca and ann ca are 0 6729 and 0 6561 respectively however the landscape driven ca model can reduce this error to merely 0 5421 the same conclusion can be reached through the visual comparison in fig 3 we can observe from both the inset maps and the circled parts that the prediction result of the landscape driven lua ca shares more similar aggregate landscape patterns with the observed land use data as shown in the inset maps for example this proposed method almost replicates the ring shaped configuration in the ground truth data moreover nearly every land use class predicted by the two cell based methods is still more agglomerated than expected more importantly the differences in cell by cell accuracies between the landscape driven lua ca and the other two methods are much lower compared with the differences in the calibration period see table 1 last but not least the combined error of our proposed method is still 19 44 and 17 38 lower than those of the cell based lua ca and ann ca respectively all these comparisons indicated that the landscape driven lua ca has much better generalization ability 3 2 assessment of the landscape driven ca with different landscape metrics to further investigate the performance of those five landscape metrics equations 6 10 in ca calibration we also designed five simulation scenarios that used each metric individually all the simulation results are presented in fig 4 and their corresponding errors are summarized in table 3 although the differences in landscape patterns among various simulation results cannot be easily distinguished through a visual comparison the error values still considerably vary as expected the best simulation result was obtained with the support of all five metrics in addition the model calibrated using cohesion yielded the second best result patch cohesion is sensitive to the aggregation of the focal class and will increase as the land use cells become more physically connected this experiment suggested that patch cohesion is an informative metric for calibrating the ca model in this study next the third and fourth best results were obtained using ed and lpi respectively the length of the edge in a landscape is critical to various ecological phenomena therefore it is not surprising that ed can offer abundant information on landscape patterns moreover the landscape patterns of dominant land use patches may also play important roles in land use development last we found that the mere consideration of patch number and size division mps can neither well characterize the whole landscape pattern nor easily get the cell by cell locations right overall the above experiments indicated that cohesion ed and lpi are appropriate landscape objectives for ca calibration in this study finally we further calibrated a ca model during 2000 2005 using the best two landscape metrics i e cohesion and ed simultaneously as presented in table 4 the comparison has indicated that this simulation result in 2005 is comparable with the result calibrated using all five landscape metrics for example the combined errors of these two results are similar 0 5381 and 0 5324 therefore it is possible to build a desirable lulc model when using only a small number of landscape metrics as the calibration objective 3 3 discussion 3 3 1 advantages of landscape driven ca cell based agreement and landscape similarity are two equally important objectives for ca calibration although a simulation result will perfectly match with the landscape metrics of the observed land use data if it has a cell based agreement of 100 this is rarely the case in practice therefore a question arises as to whether the increase in the similarity of landscape patterns is worth the decrease in cell based accuracies to shed some light on this question we take urban expansion the dominant type of land use changes in the study area as an example see fig 5 in general the newly urbanized grid cells simulated by the traditional cell based cas are almost evenly distributed around preexisting urban land this observation explains why the urban land of cell based methods is more agglomerated than expected however not all land use grid cells surrounded by urban land will be urbanized in the real world for example some water bodies and green spaces that are essential to the local environment will remain unchanged over time although these small grid cells simulated by the proposed landscape driven ca may not exactly fall within the ground truth location the simulation results can still call policy makers attention to those tiny but fundamental elements during land use planning processes policy makers could also manually correct the locations for these grid cells in this regard the landscape driven ca should be much more informative for land use planners in a rapidly changing landscape in addition one of the most important applications of calibrated lulc models is that their prediction results could support policy making in long term land use planning an overemphasis on cell by cell agreement could substantially affect the generalization and predictive abilities of lulc models since there exist no ground truth land use data for years to come the ability to capture and reproduce aggregate landscape patterns is especially valuable for policy makers to assess the potential influences of various land use development strategies for example numerous studies have demonstrated that the land use morphology of a region could exert profound influences on its carbon emissions land surface temperature environmental conservation and so on chen et al 2011 privitera et al 2018 moreover spatial configuration e g compactness and connectedness is one of the most important criteria in land use zoning and management lin and li 2019 therefore it is necessary for land use planners to appropriately consider landscape patterns especially those important metrics during land use planning and optimization processes in particular we could incorporate some specific landscape metrics for ca calibration and then predict future lulc change scenarios based on the calibrated model some important aspects of urban development e g compact city low carbon city can be systematically explored in this manner it should also be noted that not all landscape metrics are effective for the calibration of lulc models even though the metrics were selected with reference in this study for example the simulation results are even worse than those simulated by traditional cell based methods if inappropriate metrics are adopted e g the comparison between tables 1 and 3 nevertheless the proposed landscape driven ca still works if inexperienced modelers don t know how to choose a set of metrics it is because our method can effectively evaluate the performance of different landscape metrics in calibration in future applications any landscape metric that is potentially important for the calibration of multiple lulc models can be incorporated in equations 11 and 12 for detailed evaluation 3 3 2 disadvantages of landscape driven ca we noticed that some anomalies occur in the comparisons of landscape similarity for example the landscape driven ca has the highest error values of interspersion and juxtaposition index table 1 compared with the other two methods we referred to the detailed calculation in the supplementary material and found that this error is mainly attributed to unused land a small land use class that is difficult to control similarly although the model calibrated using only ed does not return the highest similarity of this metric table 3 the error is mainly attributed to a certain land use class in addition this model and the model calibrated using only mps share similar errors of ed and mps these two metrics are probably related in this case therefore we think that these phenomena are inevitable and acceptable since not all landscape metrics were considered during the calibration procedures nevertheless the proposed method still has some limitations that need to be addressed in the future first while the similarity of aggregate landscape patterns can be considerably increased by the proposed landscape driven ca cell based agreement cannot be guaranteed in future studies we will try to simultaneously improve these two equally important objectives second only five relevant and commonly used landscape metrics have been examined in this study we could investigate different metrics in future applications given the methodology is the same third we did not choose the ann ca as the basic model because it is very difficult to optimize a huge number of weights e g 171 in this study using intelligent algorithms within an acceptable timeframe finally the potential influence of spatial resolution scale on the results remains to be investigated on the one hand if the spatial resolution of land use data is too coarse then the geometrical information of landscape patterns will be largely lost and the risk of redundancies in landscape metrics may increase on the other hand the computational burden will increase if land use data with a very fine resolution are used 4 conclusions since ca models were introduced for analyzing geographical phenomena various methods have been developed to facilitate the modeling of lulc changes although ca can produce an acceptable cell by cell fit between the modeling results and observed land use data this study has indicated that the commonly used cell based cas still cannot characterize the complex landscape patterns of multiple lulc changes in our experiments the landscape metrics of the traditional modeling results are substantially different from those of the observed data to address this problem we introduced a landscape driven multiple ca that can consider landscape patterns during calibration this proposed model builds upon the cell based lua ca in which ga is used to search for the optimal transition rule both the landscape metrics and cell by cell accuracy are incorporated into the fitness function of the ga we also investigated the performance of five important landscape metrics in calibration the proposed method was applied to the modeling of multiple lulc changes in guangzhou during 2000 2010 two well accepted cell based ca models were also calibrated for comparison we found that cohesion ed and lpi are appropriate landscape objectives for ca calibration in this study in addition a number of experiments demonstrated that landscape driven ca can significantly improve the simulation and prediction performances in terms of landscape similarity our modeling results are closer to the observed land use data according to the combined errors we therefore argue that the proposed method is promising for providing valuable information for land use planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china grant no 41801307 no 41871290 no 41531176 we thank the anonymous reviewers for their constructive comments and suggestions that greatly improved this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104719 
26023,land cover maps are key elements for understanding global climate and land use they are often created by automatically classifying satellite imagery however inconsistencies in classification may be introduced inadvertently experts can reconcile classification discrepancies by viewing satellite and high resolution images taken on the ground we present and evaluate a framework to filter relevant geo tagged photos from social network sites for land cover classification tasks social network sites offer massive amounts of potentially relevant data but its quality and fitness for research purposes must be verified our framework uses computer vision to analyse the content of geo tagged photos on social network sites to generate descriptive tags these are used to train artificial neural networks to predict a photo s relevance for land cover classification we apply our models to four african case studies and their neighbours the framework has been implemented within geo wiki to fetch relevant photos from flickr keywords land cover social network geo tagged photos computer vision machine learning 1 introduction world wide web data have been broadly applied to research applications including public opinion measurement o connor et al 2010 and election forecasting tumasjan et al 2010 epidemiology culotta 2010 and even to philosophy cheong 2018 geo tagged visual media in particular as a form of volunteered geographic information vgi has seen a strong interest in scientific research as they allowed scientists to make firsthand observations from remote sparsely populated places that would be otherwise impractical or expensive for data collection barve 2014 daume 2016 elqadi et al 2017 estima and painho 2013 this ability to make such omnipresent observations is particularly useful in land cover research to improve land cover maps land cover impacts global climate by changing biogeochemical cycles and consequently the composition of the atmosphere as well as changing the biogeophysical processes that affect energy absorption at the earth s surface feddema et al 2005 an understanding of changes in land cover and the associated monitoring of human land use such as agriculture mining and urban development also enables us to better grasp human encroachment on natural ecosystems and habitats mapping land cover is therefore essential for understanding and simulating anthropogenic climate change and our impact on earth s ecosystems feddema et al 2005 land cover maps are usually created by automatic classification of satellite imagery a process that results in discrepancies between global land cover products fritz et al 2011 mccallum et al 2006 to aid in solving discrepancies citizen science is a high value resource for example the international institute for applied systems analysis iiasa has created geo wiki an information portal that allows volunteers to improve data on land cover using satellite imagery from google earth fritz et al 2017 see et al 2015 results show volunteer contributed data to be generally equivalent to expert data see et al 2013 satellite imagery interpretation frequently depends on the existence of images taken in situ requiring researchers to seek alternative sources of visual data to build a robust model of the environment estima and painho 2013 explored the adequacy of flickr images to help the quality control of the corine land cover clc they concluded there is potential for such use admitting however that their study has not looked into the content of the images and their adequacy for this purpose nevertheless elqadi et al 2017 demonstrated that since flickr contains such a massive amount of photos even if only a small fraction are relevant there is potential to achieve high value outcomes from open data on social network sites sns by careful filtering which avoids some of the issues associated with raw image data barve 2014 social network sites sns geo tagged photographs were used in previous research to determine land cover classes for instance estima et al 2014 compared corine land cover information obtained from flickr geo tagged photos against classification based on satellite imagery they concluded that the sns geotagged photos are a valuable supplementary data source oba et al 2014 retrieved photos from flickr using text tags corresponding to land cover classes they then classified regions to land cover types based on image feature classification using a support vector machine svm as well as photos titles and tags xu et al 2017 and xing et al 2018 used convolutional neural networks cnn to classify geotagged photos from the global geo referenced field photo library and flickr into land cover classes our work seeks to support remote sensing scientists decision making on land cover type rather than to automate the decision process this distinction is made primarily because our research problem stems from discrepancies in land cover satellite imagery consequently our work provides a practical framework to solve existing problems in land cover classification in understudied regions rather than suggesting theory to be validated in well studied regions as was the focus of previous work to achieve our objectives we developed a framework that uses computer vision to describe image visual content and an artificial neural network classification model to decide whether or not the image is relevant based on this description the main outcome of our study is a reusable framework to find and filter imagery that can help determine land cover types therefore we investigate whether particular countries should have customised models or whether data from all countries can be used to build one generalised model to build the suggested framework human labour is required to label images for training the machine learning algorithm it is therefore worthwhile testing whether models can be reused in countries other than those from where the training data originates we developed country based and generalised models for africa and compared the results to establish the most appropriate ways to use them our framework was integrated into geo wiki although our framework is independent of the social network site data source we chose flickr for its favourable application programming interface api since it accepts queries simultaneously filtered spatially temporally and textually we used the python flickr api mignon 2016 in the next sections we present our methodology and discuss the results of the different models we developed finally we present our publicly available api allowing other users to invoke our models 2 methodology 2 1 study area the chosen study area consists of four african countries with varied geographic demographic and economic characteristics egypt kenya zambia and côte d ivoire fig 1 these are all subjects of a practical interest in obtaining land cover photos because although they fall within regions where high resolution data is available it has previously been noted lesiv et al 2017 that the accuracy of this data is as low as 65 also these four countries offer valuable case studies due to their variations in climate and geography 2 2 process overview our process uses cloud based computer vision services to analyse the visual content of geotagged photos on social network sites and generate descriptive tags for that content we then use these tags to train an artificial neural network to predict a photo s adequacy for land cover classification fig 2 demonstrates the workflow first we collect photos in the specified study area we then filter the collected photos to exclude those in urban regions since built up land cover is already well known and mapped while we are primarily interested in natural and other non urban regions we then select a random sample of photos from the filtered data set that we subject to automatic computer vision classification programs to generate descriptive text tags for the samples the sample set of photos is also manually labelled by expert researchers as either relevant or irrelevant based on the photo utility to land cover determination e g outdoor outside settlement areas containing trees shrubs grassland rocks sand or agricultural areas etc the text tags and associated relevant irrelevant boolean labels are used to build a classification model that can predict whether a photo described by a certain set of tags is relevant for land cover classification the model is later applied to filter photos based on visual content by users classifying land cover fig 3 2 3 building the classification models 2 3 1 photo collection the method we describe can be applied to any social network site that offers an application programming interface api to retrieve geotagged photos in addition to the convenience of flickr s api already noted we chose flickr www flickr com for its wealth of photos with at one stage about 2 million new photos being added every day michel 2016 flickr can be queried for photos in a particular geographic bounding box however for any given query the api only returns 3600 unique results in order to collect a larger number of photos we divided each of our test case countries into a grid where its cells are then used sequentially as the bounding box for photo queries to create the grid the outline of a country is divided into cells of side length of 1 decimal degree the grid cells intersecting the urban centres are replaced by a smaller grid of a cell side length of 0 2 decimal degrees to allow for high density retrieval of photos we then delete the grid cells that are totally contained within urban areas since we are only interested in land cover in non urban areas grid cells containing smaller urban settlements i e cells that are not totally covered by urban settlements are used to query flickr the full set of collected photos in a country is filtered using an urban settlement mask based on the copernicus land cover dataset european union copernicus land monitoring service 2018 european environment agency eea leaving only photos outside registered urban settlements a sample of the remaining photos is randomly selected for the following steps of visual tagging and data labelling table 3 although the flickr api allows searching by location and keyword simultaneously we didn t limit our queries by any keywords and only searched by geographic bounding box because the text tags supplied by the original image posters can sometimes be misleading elqadi et al 2017 xing et al 2018 2 3 2 computer vision tagging in this step we use computer vision to assign tags to the photos based on their visual content to achieve this we used the computer vision cognitive services from microsoft azure microsoft azure https azure microsoft com is a set of cloud based services which include cognitive services microsoft 2018 machine learning models and artificial intelligence algorithms among these computer vision is one of the available cognitive services in the cognitive services api the image function can be applied for a given image where more than one tag is returned for the image with varied confidence levels with unity being the highest confidence level we ran the analyse image service on every image in the selected sample and only chose the tags with confidence levels higher than 0 5 this is an arbitrary value that corresponds to the computer vision system being more confident than not about the generated tag a lower threshold would have simply given more tags in which the computer vision system had poor confidence we excluded images that had no tags with enough confidence from the machine learning model training for each country we compiled a list of all tag occurrences for all photos in that country to build the classification models we first order tags returned from our set of images in order of decreasing frequency next starting with tags that are most frequent we selected tags for inclusion in our classifier stopping when we have the set of tags present in 95 of the records these selected tags served as our classification model features if we were to consider the minimum set of tags occurring in a larger number of records say 99 we would have a much larger set of tags table 1 but many of these tags would only be present in very few photos this consequently would result in a higher number of classification model features many of which would carry very little information preliminary analysis in this step showed that the frequent tags are different in each country despite the fact that photos in all cases were outside urban centres for example in egypt where people primarily visit to see monuments and practice water sports the tags include outdoor sky nature person water building ground indoor etc fig 4 conversely in kenya where safari is prominent the tags include outdoor grass animal field sky mammal tree etc fig 5 2 3 3 manual data labelling the sample set of photos that have been automatically tagged based on their visual content are then manually labelled by expert researchers from iiasa as relevant if they perceived the image content presented potentially meaningful land cover information otherwise the images are labelled as irrelevant the relevance ratio is different among countries as shown in table 4 that these ratios differ among countries may be attributed to the different photographic activity profile of each country as discussed in 2 3 2 2 3 4 classification model training artificial neural networks anns were chosen to build the classification model anns are capable of classifying patterns unseen in training data they are tolerant to noisy data and appropriate when little is known about the relationships in input data han et al 2012 and they are ubiquitous in the remote sensing community e g xing et al 2018 in order to train an ann that can predict whether a photo is relevant based on its visual tags we created a table containing the label we applied i e whether the photo is relevant or not next the tags identified as selection features in 2 3 2 were added to the table as binary columns showing whether each tag was present in each row i e photo see table 2 to build our classification models we used fully connected neural networks with one hidden layer consisting of n neurons where n t r a i n i n g s e t s i z e 2 i n p u t f e a t u r e s o u t p u t the number of neurons in the hidden layer is based on experimentation with the rules of thumb suggested in heaton 2008 our anns had a 0 1 learning rate 0 1 initial learning weight and a min max normaliser we randomly selected 70 of the data set to train the model and the remaining 30 for scoring the machine learning experiment was fully implemented in microsoft s azure machine learning studio a browser based graphical user interface that runs machine learning algorithms and models in the cloud 2 4 using the classification models the driver behind this work is the practical need for geotagged photos to help determine land cover in understudied areas geo wiki users are presented with a web interface this follows the workflow shown in fig 3 for a given location viewed on the map images are retrieved from sns next tags are generated for all photos using the computer vision api the tags are then sent to our machine learning model that responds with a boolean value whether the photo is relevant to land cover determination only relevant photos are displayed to the user it is worth noting that in the image retrieval step the user can choose to retrieve images from sns based on associated text tags which are proved to be valuable in predicting ecological features jeawak et al 2019 however our system may be used in under studied regions as described in the case studies of this paper in such regions no a priori assumptions are made about the coverage and different languages may be used in text tags associated with the photos hence the user may opt to retrieve images from sns based on location only forgoing the text tags and depend on our filtration tool to find the relevant photos consequently we trained our filtration models based on visual content of images from study regions without considering associated text tags the user retains the freedom to query sns images using text tags the retrieved images in all cases are subject to our filtration models 2 5 experiments an overview of these experiments is outlined below and in fig 6 1 for each country in set 1 egypt kenya côte d ivoire and zambia we collected images 2 3 1 tagged them 2 3 2 labelled them 2 3 3 and trained a model 2 3 4 2 a union of data drawn from across set 1 to ensure a spread of representation from each country was used to train and test a single generalised model through the process outlined in 2 3 1 to 2 3 4 3 for each country in set 2 libya ghana and tanzania which neighbour set 1 countries we collected images 2 3 1 tagged them 2 3 2 labelled them 2 3 3 but did not train a model the first experiment enabled us to test our models in the countries from which the data were obtained the second experiment tests the performance of a unified generalised model the third experiment enabled us to test model reusability i e can a model from one country filter or predict the relevance of data from another 3 results and discussion 3 1 experiment 1 country specific models the results of the data collection process described in 2 3 1 are summarised in table 3 here we report total number of available photos which is the count of records reported by the flickr web interface when searching for geotagged images in a country total collected is the number of photos collected outside large urban areas using the grid cells we calculated in 2 3 1 the api limits the number of downloaded photos in grid cells with high photos density these would mostly be urban areas non urban is the count of remaining records after masking out urban areas sample is the count of records randomly selected for automatic visual tagging and manual labelling the sample size is about 2 in egypt and kenya however since fewer data are available in côte d ivoire and zambia the sample size was taken to be 50 and 10 respectively to allow for a representative sample from these relatively small data sets that was sufficiently large to allow for training and scoring the photos in the sample were tagged and labelled as described in sections 2 3 2 and 2 3 3 some of the photos couldn t be tagged or labelled with sufficient confidence and so were removed from the analysis hence the difference between dataset size in table 4 and the sample size in table 3 the ratio between the two classification classes relevant and irrelevant are shown in table 4 the table also reports on the configuration of our ann classification model section 2 3 4 and the performance metrics for classifying the scoring dataset 3 2 experiment 2 a generalised model although the distribution of tags differs by country as we discussed in 2 3 2 we checked whether there is sufficient value in spending resources on creating separate models we thus collated records from every dataset to train a generalised model with records from each country the number of columns i e selection features was the superset of all datasets 70 of the data was used to train the generalised model the validation dataset from each country was scored against the generalised model as well as the original country specific model results shown in table 5 demonstrate that differences in model accuracy between per country models and the generalised model are statistically significant when the per country models perform better than the generalised model egypt and côte d ivoire in cases where the generalised model slightly outperformed the per country models kenya and zambia the difference was statistically insignificant 3 3 experiment 3 set 2 countries here we address the question of model reusability could we use a model from one country to filter predict relevance of data from another country and given that country specific models are better than the generalised model how would the generalised model perform against data from countries not used in model training to answer these questions we collected new test data from a second set of countries libya sharing a border with egypt tanzania sharing borders with both kenya and zambia and ghana sharing a border with côte d ivoire samples from these data were manually labelled 156 from libya 165 from tanzania and 162 from ghana then scored using all individual country models and the generalised model results are shown in table 6 the generalised model s performance with data from a new country is the best or at least as good as the model from a neighbouring country 3 4 related costs and technology the machine learning logic was implemented in the azure machine learning cloud service from microsoft which not only masked many of the low level details of the neural network that are of little interest to our research but also helped in the automatic generation of consumable web services allowing geo wiki users to benefit from our models as a service in geo wiki photos are retrieved from social network sources using a search module then every photo is subjected to the process shown in fig 3 the time taken to process each photo depends on multiple factors such as the image resolution the speed of the computer vision api the classification model speed the throughput of the web application host server and the bandwidth between these components we empirically measured the mean processing time of computer vision tagging to be 1 4 s image sd 0 4 n 100 and the mean processing time of our classification model to be 0 24 s image sd 0 12 n 100 in our experiments we opted for the highest available resolution of the flickr photos used however the overall processing speed might be enhanced through optimising between resolution and computer vision accuracy for specific user s requirements our framework weeds out the majority of irrelevant photos accuracy around 0 8 from the massive amounts of sns photos noting that the ratio of relevant photos in samples we checked was between 0 3 and 0 7 egypt and kenya respectively our framework is saving researchers valuable time they would otherwise spend browsing many irrelevant photos we have opted to include text tags in a descending order of frequency 2 3 2 until we had included tags appearing in at least 95 of the records table 1 however the accuracy of our classification filter can be further enhanced by optimising the number of input features text tags participating in the model users may elect to explore the possibility of better training their classifier to give more accurate assessments however this will incur a time cost it requires more data it may result in issues related to overtraining and in fact it may not save the users sufficient amounts of human time to be worthwhile microsoft cognitive services computer vision is implemented using deep learning tran et al 2016 these are often mis calibrated i e overly confident in their own results guo et al 2017 although microsoft cognitive services computer vision contains a dedicated module for confidence estimation tran et al 2016 we arbitrarily chose to consider tags with confidence 0 5 empirical calibration of this confidence cut off may prove useful to the overall framework performance in the future 4 conclusions social network sites provide a wealth of data for researchers in many disciplines data quality is a primary concern in leveraging this rich data source photos on sns are not always fit for research purposes in land cover mapping sns can provide much needed high resolution geotagged photos which can be manually or automatically classified to determine the type of land cover in a photo however to retrieve and classify geotagged photos from sns filtration methods are needed to remove irrelevant photos from the classification pipeline in this work we suggested developed and tested a framework to filter sns photos relevant to land cover classification our framework uses commercially available apis to favour simple implementation by interdisciplinary researchers who may need to reproduce it technology is improving over time we are sure the methods and frameworks we suggest in this research would be improved further when paired with more capable computer vision and machine learning in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the project was part of the iiasa young scientist summer program yssp the participation of the first author in this program was funded by potsdam institute for climate impact research the experiments used apis and services from microsoft azure supported by a microsoft ai for earth grant ad and a g d were supported by australian research council discovery project grant dp160100161 m m e was supported by a monash university faculty of information technology apa award the authors would also like to thank christoph perger for his help at yssp with geo wiki development 
26023,land cover maps are key elements for understanding global climate and land use they are often created by automatically classifying satellite imagery however inconsistencies in classification may be introduced inadvertently experts can reconcile classification discrepancies by viewing satellite and high resolution images taken on the ground we present and evaluate a framework to filter relevant geo tagged photos from social network sites for land cover classification tasks social network sites offer massive amounts of potentially relevant data but its quality and fitness for research purposes must be verified our framework uses computer vision to analyse the content of geo tagged photos on social network sites to generate descriptive tags these are used to train artificial neural networks to predict a photo s relevance for land cover classification we apply our models to four african case studies and their neighbours the framework has been implemented within geo wiki to fetch relevant photos from flickr keywords land cover social network geo tagged photos computer vision machine learning 1 introduction world wide web data have been broadly applied to research applications including public opinion measurement o connor et al 2010 and election forecasting tumasjan et al 2010 epidemiology culotta 2010 and even to philosophy cheong 2018 geo tagged visual media in particular as a form of volunteered geographic information vgi has seen a strong interest in scientific research as they allowed scientists to make firsthand observations from remote sparsely populated places that would be otherwise impractical or expensive for data collection barve 2014 daume 2016 elqadi et al 2017 estima and painho 2013 this ability to make such omnipresent observations is particularly useful in land cover research to improve land cover maps land cover impacts global climate by changing biogeochemical cycles and consequently the composition of the atmosphere as well as changing the biogeophysical processes that affect energy absorption at the earth s surface feddema et al 2005 an understanding of changes in land cover and the associated monitoring of human land use such as agriculture mining and urban development also enables us to better grasp human encroachment on natural ecosystems and habitats mapping land cover is therefore essential for understanding and simulating anthropogenic climate change and our impact on earth s ecosystems feddema et al 2005 land cover maps are usually created by automatic classification of satellite imagery a process that results in discrepancies between global land cover products fritz et al 2011 mccallum et al 2006 to aid in solving discrepancies citizen science is a high value resource for example the international institute for applied systems analysis iiasa has created geo wiki an information portal that allows volunteers to improve data on land cover using satellite imagery from google earth fritz et al 2017 see et al 2015 results show volunteer contributed data to be generally equivalent to expert data see et al 2013 satellite imagery interpretation frequently depends on the existence of images taken in situ requiring researchers to seek alternative sources of visual data to build a robust model of the environment estima and painho 2013 explored the adequacy of flickr images to help the quality control of the corine land cover clc they concluded there is potential for such use admitting however that their study has not looked into the content of the images and their adequacy for this purpose nevertheless elqadi et al 2017 demonstrated that since flickr contains such a massive amount of photos even if only a small fraction are relevant there is potential to achieve high value outcomes from open data on social network sites sns by careful filtering which avoids some of the issues associated with raw image data barve 2014 social network sites sns geo tagged photographs were used in previous research to determine land cover classes for instance estima et al 2014 compared corine land cover information obtained from flickr geo tagged photos against classification based on satellite imagery they concluded that the sns geotagged photos are a valuable supplementary data source oba et al 2014 retrieved photos from flickr using text tags corresponding to land cover classes they then classified regions to land cover types based on image feature classification using a support vector machine svm as well as photos titles and tags xu et al 2017 and xing et al 2018 used convolutional neural networks cnn to classify geotagged photos from the global geo referenced field photo library and flickr into land cover classes our work seeks to support remote sensing scientists decision making on land cover type rather than to automate the decision process this distinction is made primarily because our research problem stems from discrepancies in land cover satellite imagery consequently our work provides a practical framework to solve existing problems in land cover classification in understudied regions rather than suggesting theory to be validated in well studied regions as was the focus of previous work to achieve our objectives we developed a framework that uses computer vision to describe image visual content and an artificial neural network classification model to decide whether or not the image is relevant based on this description the main outcome of our study is a reusable framework to find and filter imagery that can help determine land cover types therefore we investigate whether particular countries should have customised models or whether data from all countries can be used to build one generalised model to build the suggested framework human labour is required to label images for training the machine learning algorithm it is therefore worthwhile testing whether models can be reused in countries other than those from where the training data originates we developed country based and generalised models for africa and compared the results to establish the most appropriate ways to use them our framework was integrated into geo wiki although our framework is independent of the social network site data source we chose flickr for its favourable application programming interface api since it accepts queries simultaneously filtered spatially temporally and textually we used the python flickr api mignon 2016 in the next sections we present our methodology and discuss the results of the different models we developed finally we present our publicly available api allowing other users to invoke our models 2 methodology 2 1 study area the chosen study area consists of four african countries with varied geographic demographic and economic characteristics egypt kenya zambia and côte d ivoire fig 1 these are all subjects of a practical interest in obtaining land cover photos because although they fall within regions where high resolution data is available it has previously been noted lesiv et al 2017 that the accuracy of this data is as low as 65 also these four countries offer valuable case studies due to their variations in climate and geography 2 2 process overview our process uses cloud based computer vision services to analyse the visual content of geotagged photos on social network sites and generate descriptive tags for that content we then use these tags to train an artificial neural network to predict a photo s adequacy for land cover classification fig 2 demonstrates the workflow first we collect photos in the specified study area we then filter the collected photos to exclude those in urban regions since built up land cover is already well known and mapped while we are primarily interested in natural and other non urban regions we then select a random sample of photos from the filtered data set that we subject to automatic computer vision classification programs to generate descriptive text tags for the samples the sample set of photos is also manually labelled by expert researchers as either relevant or irrelevant based on the photo utility to land cover determination e g outdoor outside settlement areas containing trees shrubs grassland rocks sand or agricultural areas etc the text tags and associated relevant irrelevant boolean labels are used to build a classification model that can predict whether a photo described by a certain set of tags is relevant for land cover classification the model is later applied to filter photos based on visual content by users classifying land cover fig 3 2 3 building the classification models 2 3 1 photo collection the method we describe can be applied to any social network site that offers an application programming interface api to retrieve geotagged photos in addition to the convenience of flickr s api already noted we chose flickr www flickr com for its wealth of photos with at one stage about 2 million new photos being added every day michel 2016 flickr can be queried for photos in a particular geographic bounding box however for any given query the api only returns 3600 unique results in order to collect a larger number of photos we divided each of our test case countries into a grid where its cells are then used sequentially as the bounding box for photo queries to create the grid the outline of a country is divided into cells of side length of 1 decimal degree the grid cells intersecting the urban centres are replaced by a smaller grid of a cell side length of 0 2 decimal degrees to allow for high density retrieval of photos we then delete the grid cells that are totally contained within urban areas since we are only interested in land cover in non urban areas grid cells containing smaller urban settlements i e cells that are not totally covered by urban settlements are used to query flickr the full set of collected photos in a country is filtered using an urban settlement mask based on the copernicus land cover dataset european union copernicus land monitoring service 2018 european environment agency eea leaving only photos outside registered urban settlements a sample of the remaining photos is randomly selected for the following steps of visual tagging and data labelling table 3 although the flickr api allows searching by location and keyword simultaneously we didn t limit our queries by any keywords and only searched by geographic bounding box because the text tags supplied by the original image posters can sometimes be misleading elqadi et al 2017 xing et al 2018 2 3 2 computer vision tagging in this step we use computer vision to assign tags to the photos based on their visual content to achieve this we used the computer vision cognitive services from microsoft azure microsoft azure https azure microsoft com is a set of cloud based services which include cognitive services microsoft 2018 machine learning models and artificial intelligence algorithms among these computer vision is one of the available cognitive services in the cognitive services api the image function can be applied for a given image where more than one tag is returned for the image with varied confidence levels with unity being the highest confidence level we ran the analyse image service on every image in the selected sample and only chose the tags with confidence levels higher than 0 5 this is an arbitrary value that corresponds to the computer vision system being more confident than not about the generated tag a lower threshold would have simply given more tags in which the computer vision system had poor confidence we excluded images that had no tags with enough confidence from the machine learning model training for each country we compiled a list of all tag occurrences for all photos in that country to build the classification models we first order tags returned from our set of images in order of decreasing frequency next starting with tags that are most frequent we selected tags for inclusion in our classifier stopping when we have the set of tags present in 95 of the records these selected tags served as our classification model features if we were to consider the minimum set of tags occurring in a larger number of records say 99 we would have a much larger set of tags table 1 but many of these tags would only be present in very few photos this consequently would result in a higher number of classification model features many of which would carry very little information preliminary analysis in this step showed that the frequent tags are different in each country despite the fact that photos in all cases were outside urban centres for example in egypt where people primarily visit to see monuments and practice water sports the tags include outdoor sky nature person water building ground indoor etc fig 4 conversely in kenya where safari is prominent the tags include outdoor grass animal field sky mammal tree etc fig 5 2 3 3 manual data labelling the sample set of photos that have been automatically tagged based on their visual content are then manually labelled by expert researchers from iiasa as relevant if they perceived the image content presented potentially meaningful land cover information otherwise the images are labelled as irrelevant the relevance ratio is different among countries as shown in table 4 that these ratios differ among countries may be attributed to the different photographic activity profile of each country as discussed in 2 3 2 2 3 4 classification model training artificial neural networks anns were chosen to build the classification model anns are capable of classifying patterns unseen in training data they are tolerant to noisy data and appropriate when little is known about the relationships in input data han et al 2012 and they are ubiquitous in the remote sensing community e g xing et al 2018 in order to train an ann that can predict whether a photo is relevant based on its visual tags we created a table containing the label we applied i e whether the photo is relevant or not next the tags identified as selection features in 2 3 2 were added to the table as binary columns showing whether each tag was present in each row i e photo see table 2 to build our classification models we used fully connected neural networks with one hidden layer consisting of n neurons where n t r a i n i n g s e t s i z e 2 i n p u t f e a t u r e s o u t p u t the number of neurons in the hidden layer is based on experimentation with the rules of thumb suggested in heaton 2008 our anns had a 0 1 learning rate 0 1 initial learning weight and a min max normaliser we randomly selected 70 of the data set to train the model and the remaining 30 for scoring the machine learning experiment was fully implemented in microsoft s azure machine learning studio a browser based graphical user interface that runs machine learning algorithms and models in the cloud 2 4 using the classification models the driver behind this work is the practical need for geotagged photos to help determine land cover in understudied areas geo wiki users are presented with a web interface this follows the workflow shown in fig 3 for a given location viewed on the map images are retrieved from sns next tags are generated for all photos using the computer vision api the tags are then sent to our machine learning model that responds with a boolean value whether the photo is relevant to land cover determination only relevant photos are displayed to the user it is worth noting that in the image retrieval step the user can choose to retrieve images from sns based on associated text tags which are proved to be valuable in predicting ecological features jeawak et al 2019 however our system may be used in under studied regions as described in the case studies of this paper in such regions no a priori assumptions are made about the coverage and different languages may be used in text tags associated with the photos hence the user may opt to retrieve images from sns based on location only forgoing the text tags and depend on our filtration tool to find the relevant photos consequently we trained our filtration models based on visual content of images from study regions without considering associated text tags the user retains the freedom to query sns images using text tags the retrieved images in all cases are subject to our filtration models 2 5 experiments an overview of these experiments is outlined below and in fig 6 1 for each country in set 1 egypt kenya côte d ivoire and zambia we collected images 2 3 1 tagged them 2 3 2 labelled them 2 3 3 and trained a model 2 3 4 2 a union of data drawn from across set 1 to ensure a spread of representation from each country was used to train and test a single generalised model through the process outlined in 2 3 1 to 2 3 4 3 for each country in set 2 libya ghana and tanzania which neighbour set 1 countries we collected images 2 3 1 tagged them 2 3 2 labelled them 2 3 3 but did not train a model the first experiment enabled us to test our models in the countries from which the data were obtained the second experiment tests the performance of a unified generalised model the third experiment enabled us to test model reusability i e can a model from one country filter or predict the relevance of data from another 3 results and discussion 3 1 experiment 1 country specific models the results of the data collection process described in 2 3 1 are summarised in table 3 here we report total number of available photos which is the count of records reported by the flickr web interface when searching for geotagged images in a country total collected is the number of photos collected outside large urban areas using the grid cells we calculated in 2 3 1 the api limits the number of downloaded photos in grid cells with high photos density these would mostly be urban areas non urban is the count of remaining records after masking out urban areas sample is the count of records randomly selected for automatic visual tagging and manual labelling the sample size is about 2 in egypt and kenya however since fewer data are available in côte d ivoire and zambia the sample size was taken to be 50 and 10 respectively to allow for a representative sample from these relatively small data sets that was sufficiently large to allow for training and scoring the photos in the sample were tagged and labelled as described in sections 2 3 2 and 2 3 3 some of the photos couldn t be tagged or labelled with sufficient confidence and so were removed from the analysis hence the difference between dataset size in table 4 and the sample size in table 3 the ratio between the two classification classes relevant and irrelevant are shown in table 4 the table also reports on the configuration of our ann classification model section 2 3 4 and the performance metrics for classifying the scoring dataset 3 2 experiment 2 a generalised model although the distribution of tags differs by country as we discussed in 2 3 2 we checked whether there is sufficient value in spending resources on creating separate models we thus collated records from every dataset to train a generalised model with records from each country the number of columns i e selection features was the superset of all datasets 70 of the data was used to train the generalised model the validation dataset from each country was scored against the generalised model as well as the original country specific model results shown in table 5 demonstrate that differences in model accuracy between per country models and the generalised model are statistically significant when the per country models perform better than the generalised model egypt and côte d ivoire in cases where the generalised model slightly outperformed the per country models kenya and zambia the difference was statistically insignificant 3 3 experiment 3 set 2 countries here we address the question of model reusability could we use a model from one country to filter predict relevance of data from another country and given that country specific models are better than the generalised model how would the generalised model perform against data from countries not used in model training to answer these questions we collected new test data from a second set of countries libya sharing a border with egypt tanzania sharing borders with both kenya and zambia and ghana sharing a border with côte d ivoire samples from these data were manually labelled 156 from libya 165 from tanzania and 162 from ghana then scored using all individual country models and the generalised model results are shown in table 6 the generalised model s performance with data from a new country is the best or at least as good as the model from a neighbouring country 3 4 related costs and technology the machine learning logic was implemented in the azure machine learning cloud service from microsoft which not only masked many of the low level details of the neural network that are of little interest to our research but also helped in the automatic generation of consumable web services allowing geo wiki users to benefit from our models as a service in geo wiki photos are retrieved from social network sources using a search module then every photo is subjected to the process shown in fig 3 the time taken to process each photo depends on multiple factors such as the image resolution the speed of the computer vision api the classification model speed the throughput of the web application host server and the bandwidth between these components we empirically measured the mean processing time of computer vision tagging to be 1 4 s image sd 0 4 n 100 and the mean processing time of our classification model to be 0 24 s image sd 0 12 n 100 in our experiments we opted for the highest available resolution of the flickr photos used however the overall processing speed might be enhanced through optimising between resolution and computer vision accuracy for specific user s requirements our framework weeds out the majority of irrelevant photos accuracy around 0 8 from the massive amounts of sns photos noting that the ratio of relevant photos in samples we checked was between 0 3 and 0 7 egypt and kenya respectively our framework is saving researchers valuable time they would otherwise spend browsing many irrelevant photos we have opted to include text tags in a descending order of frequency 2 3 2 until we had included tags appearing in at least 95 of the records table 1 however the accuracy of our classification filter can be further enhanced by optimising the number of input features text tags participating in the model users may elect to explore the possibility of better training their classifier to give more accurate assessments however this will incur a time cost it requires more data it may result in issues related to overtraining and in fact it may not save the users sufficient amounts of human time to be worthwhile microsoft cognitive services computer vision is implemented using deep learning tran et al 2016 these are often mis calibrated i e overly confident in their own results guo et al 2017 although microsoft cognitive services computer vision contains a dedicated module for confidence estimation tran et al 2016 we arbitrarily chose to consider tags with confidence 0 5 empirical calibration of this confidence cut off may prove useful to the overall framework performance in the future 4 conclusions social network sites provide a wealth of data for researchers in many disciplines data quality is a primary concern in leveraging this rich data source photos on sns are not always fit for research purposes in land cover mapping sns can provide much needed high resolution geotagged photos which can be manually or automatically classified to determine the type of land cover in a photo however to retrieve and classify geotagged photos from sns filtration methods are needed to remove irrelevant photos from the classification pipeline in this work we suggested developed and tested a framework to filter sns photos relevant to land cover classification our framework uses commercially available apis to favour simple implementation by interdisciplinary researchers who may need to reproduce it technology is improving over time we are sure the methods and frameworks we suggest in this research would be improved further when paired with more capable computer vision and machine learning in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the project was part of the iiasa young scientist summer program yssp the participation of the first author in this program was funded by potsdam institute for climate impact research the experiments used apis and services from microsoft azure supported by a microsoft ai for earth grant ad and a g d were supported by australian research council discovery project grant dp160100161 m m e was supported by a monash university faculty of information technology apa award the authors would also like to thank christoph perger for his help at yssp with geo wiki development 
26024,spatial discretization is the cornerstone of all spatially distributed numerical simulations including watershed hydrology traditional square grid spatial discretization has several limitations including inability to represent adjacency uniformly in this study we developed a watershed delineation model hexwatershed based on the hexagon grid spatial discretization we applied this model to two different types of watershed in the us and we evaluated its performance against the traditional method the comparisons show that the hexagon grid spatial discretization exhibits many advantages over the tradition method we propose that spatially distributed hydrologic simulations should consider using a hexagon grid spatial discretization keywords hydrology hexagon watershed delineation digital global grid system 1 introduction spatial discretization is the cornerstone of all spatially distributed numerical simulations including hydrologic simulations in hydrologic modeling the study domain is commonly discretized using a square grid spatial discretization sgsd few studies have investigated the performance of other spatial discretizations such as hexagon grid spatial discretization hgsd in hydrology weller et al 2009 wang and ai 2018 by definition spatial discretization is the representation of the continuous real world with discrete information in geographic information system gis sgsd is the most widely used approach to represent spatial information for example a raster digital elevation model dem dataset is usually used to describe the surface elevation of a region of interest roi on the earth s surface gesch et al 2002 because sgsd data structure can be represented by a rectangular array matrix directly it is convenient for computation analysis visualization and storage however sgsd has several limitations first sgsd cannot represent adjacency uniformly de sousa et al 2006 in a cartesian coordinate system each grid has two types of neighbors direct and diagonal the distances between the center of a grid and the center of its diagonal neighbors are further than that of the direct ones fig 1 a as a result hydrologic models have to assign different weights arbitrarily or empirically to account for the differences in travel distances these differences are also not always treated consistently for example in a coupled surface and subsurface hydrologic simulation water flow in the diagonal direction may be considered in the surface hydrology component whereas it is ignored in the groundwater hydrology component harbaugh 2005 henson et al 2013 maxwell et al 2009 langevin et al 2017 consequently it becomes one of the major model uncertainty sources input data model structure and parameters in hydrologic modeling in the remainder of the paper the term uncertainty refers to model uncertainty caused by model structure unless otherwise specified because the diagonal neighbors are connected through the vertices instead of faces they cannot represent stream width information correctly fig 1a fan et al 2007 similarly flow width information of the surface runoff is misrepresented second sgsd will create island effect due to the differences in d4 and d8 neighbor definitions which causes problems for numerical simulations birch et al 2007 in this study we define a single or group of grids that are connected through diagonal path at the edge of boundaries as an island for this reason watershed delineation results usually require tedious manual correction to eliminate these diagonal islands between subbasin boundaries johnston et al 2009 besides because most groundwater flow models do not consider d8 neighbors we cannot couple them with surface hydrology models directly harbaugh 2005 maxwell et al 2009 for example we need to set the grid 1 as inactive in a coupled groundwater and surface water flow model gsflow simulation fig 2 markstrom et al 2008 third sgsd cannot effectively represent a spherical topology which will introduce significant spatial distortions fig 3 hydrologic simulations at global scale that use longitude latitude mesh grid will be undermined especially when the roi is also the most distorted areas for this reason longitude latitude based river routing models e g model for scale adaptive river transport mosart may contain larger uncertainty at high latitudes renssen and knoop 2000 li et al 2013 furthermore as global scale oceanic models do not use sgsd method it becomes cumbersome to couple land surface hydrologic models with oceanic models other flow direction methods based on the sgsd also have similar limitations for example the d infinity flow direction method can describe the flow direction in 360 and improve the partition of water flow in different directions tarboton 1997 however these methods do not resolve the sgsd limitations fundamentally and they are relatively less used in hydrologic simulations triangular irregular networks tin spatial discretization tinsd is also used in hydrologic models one advantage of the tinsd method is that points of a tin are distributed variably so it can provide high resolution near roi whereas low resolution elsewhere francipane et al 2012 however this method is less popular because of the complex data structure because it has two types of neighbor connectivity 3 face neighbors and multiple vertice neighbors it may also introduce uncertainty for example the penn state integrated hydrologic model pihm only considers water flow through the face neighbors but ignores the vertice neighbors qu 2005 hereafter our discussion excludes tinsd unless otherwise specified watershed boundary based spatial discretization wbsd is also used in large scale hydrologic simulations tesfa et al 2014a 2014b however this method essentially depends on the availability of watershed boundaries which mostly come from sgsd based watershed delineation processes in contrast the hgsd method can resolve these limitations 1 in hgsd each grid has only one type of neighbor with the same connectivity and distance fig 1b as a result we can route both surface and subsurface water flow consistently without using different weights thus getting rid of the decadal old assumption on travel length this will improve spatially distributed hydrologic models that rely on grid connectivity liao et al 2019 guo 2 the island effect is automatically eliminated because all neighbors are connected through faces no manual corrections are needed to resolve the diagonal traveling path issue johnston et al 2009 3 it can provide continental to global coverage at consistent or variable spatial resolutions fig 3b sahr 2011 it can be used to couple land surface hydrologic models with oceanic models using a unified mesh grid e g the voronoi tessellation of the model for prediction across scale mpas du et al 2003 additionally it has other advantages 1 it can be used for coupled surface d6 and subsurface 9 point structured connectivity hydrologic modeling to resolve the inconsistency in connectivity 2 the conceptual model is more compatible with the flow width information because the flow path can be contained within the grid boundary fig 1b 3 it can improve model performance as many studies show that numerical simulations based on hexagon grid perform better when compared with other mesh grids de sousa et al 2006 weller et al 2009 2012 hamiltonalberto 2014 4 other flow direction methods e g d infinity can also be implemented on hgsd with modifications to improve flow direction and partitions tarboton 1997 in recent decades hgsd is widely used in discrete global grid system dggs weller et al 2009 randall et al 2002 for example the icosahedral snyder equal area isea tessellation method is used to generate a geodesic grid system within which most grids can be hexagons sahr 2011 despite all the advantages the hgsd method can provide it is not widely used in numerical simulations partially it is because most existing datasets were generated in the sgsd format and numerical models were not designed to use the hgsd datasets for example nearly all the spatially distributed hydrologic models were developed based on square or unstructured grid discretization and very few studies have used hgsd in this study we made the first attempt to develop a watershed delineation model hexwatershed with a set of algorithms based on the hgsd method in section 2 we introduce the model algorithms in section 3 we apply the model to two different types of watersheds and analyze the model outputs in section 4 we evaluate the model performance against outputs from the traditional sgsd method in section 5 and 6 we discuss the limitations and future work 2 model algorithm 2 1 overview following the traditional watershed delineation algorithms we developed a list of algorithms for the hgsd method fig a 15 because these algorithms are fundamentally similar in principle we mainly focus on the differences that were introduced in the new model last we describe the software requirements to run the hexwatershed model 2 2 hexagon grid resolution the hexagon grid resolution is defined using grid area instead of edge length so it is comparable with the traditional square grid wang and ai 2018 for example if the edge length of a hexagon is 10 m its area is approximately 259 81 m 2 then its equivalent effective square grid resolution is 16 11 m which is the square root of its area equations 1 and 2 1 a 3 3 2 l 2 2 r e a where a is the hexagon area l is the actual hexagon edge length and r e is the effective hexagon resolution although applications of hexwatershed in this study use a constant resolution hexagonal mesh section 3 it also supports variable resolution hexagonal mesh all algorithms within hexwatershed are designed and implemented independent of resolution for example the stream grid algorithm considers the total drainage area because area of each hexagon grid may be different section 2 9 2 3 grid topology in sgsd grid is often referred by its array matrix indices i j and its neighbors can be referred by moving up down the indices i 1 j 1 however in hgsd we cannot use array indices directly unless a dedicated hexagon grid index system is available sahr 2019 as a result an algorithm is required to obtain the topology depending on how the hexagon grid was generated there are different ways to obtain the grid topology for the sake of generality we assume that there is no prior grid topology stored within the hexagon grid hexwatershed rebuilds the topology with three steps 1 assign a unique global id for each hexagon 2 for each hexagon identify hexagons that share the same vertex edge as neighbors and 3 save the global ids of these neighbors into a look up table for each hexagon the final look up table of this algorithm is illustrated in fig 4 2 4 dem resampling similar to traditional raster datasets that use grid center to store information hexwatershed uses the hexagon center to store elevation theoratically hexagonal dem can be obtained by resampling from either high resolution traditional dem or high resolution hexagonal dem in this study the former approach was used because most available dem datasets are stored in the sgsd format to further simplify this process the nearest neighbor resampling method is used 2 5 dem depression filling similar to traditional dem hexagonal dem could potentially have local depressions when generated we developed an algorithm following the method proposed by richard barnes which uses the priority flood method to fill the depressions in any grid system barnes et al 2014 priority flood is an efficient algorithm to fill dem depressions by sequentially flooding the domain from the boundary inward to adjust elevations to assure that surface will drain olivier and frédéric 2002 to ensure an absolute drain a minimal slope 0 01 in percentage is added when applicable wang and liu 2006 the step by step instructions are provided in appendix b and illustrated in fig 5 2 6 flow direction flow direction is defined from the hexagon center to the center of neighbor hexagon which has the lowest elevation in other words flow direction is the flow path which has the steepest slope the global id of this downslope neighbor is stored in an attribute table unlike the traditional sgsd method that uses indices 1 2 4 8 etc to represent flow direction hexwatershed currently represents the flow direction using a flow routing map 2 7 flow accumulation we developed a flow accumulation algorithm based on the concept from arcgis flow accumulation tarboton et al 1991 in short this algorithm scans all the hexagon grids and sums up the accumulations once all the accumulations of upslope hexagons are calculated it runs recursively until accumulations of all hexagons are calculated fig c17 the flow accumulation algorithm also provides the option to consider variable resolution hexagonal mesh 2 8 watershed boundary the hexagon grid that has the highest flow accumulation is defined as the watershed outlet this algorithm scans all the hexagon grids and identifies all the hexagons that contribute to this outlet using the flow routing map among them those at the edge with less than 6 neighbors are used to define the watershed boundary 2 9 stream grid a hexagon grid is defined as a stream grid if its total drainage area exceeds the minimal drainage area threshold for a constant resolution hexagonal mesh each grid s drainage area is proportional to its accumulation value for a variable resolution hexagonal mesh each grid s drainage area is summarized from its upslope grids plus its own area in hexwatershed a stream grid is also named a stream reach which makes up a stream segment section 2 11 and fig 6 2 10 stream confluence stream confluences are defined based on the flow routing map and stream grids in short if a hexagon grid is a stream grid and it has multiple upslope stream reaches it is defined as a stream confluence in rare scenarios a stream confluence may have three or more upslope stream reaches 2 11 stream networks in hexwatershed a stream segment is defined as the stream component between headwater outlet and confluence or between two confluences to maintain the ascending order from upstream to downstream we developed an algorithm to define the stream segments reversely from watershed outlet to headwaters starting from the watershed outlet the algorithm searches for stream confluence following the stream grids once a stream confluence is found all of its upslope stream reaches are identified and the algorithm continues to search recursively until all stream segments are identified this algorithm works in the following steps 1 calculate the total number n of stream segments based on stream confluences information 2 set current outlet and current segment index as the watershed outlet and n respectively 3 starting from the current outlet search upstream and assign the current segment index to each stream reach 4 if a confluence is found set the current outlet and current segment index as this confluence and n n 1 respectively 5 repeat step 3 and loop through all the upstream segments of this confluence 6 stop until all confluences and segments are treated n 1 at the end of this algorithm a stream segment is made up of a list of stream reaches which have the same segment index the topology information of these stream reaches within each stream segment is also defined fig 6 2 12 stream topology stream topology is defined based on the stream reaches information if the first stream reach of a stream segment does not have a positive or valid upstream segment index this stream reach is headwater reach 1 in fig 6 similarly the segment index segment 6 of the downstream of the last stream reach reach 3 of a stream segment segment 5 is a downstream of segment 5 details of stream topology are explained in fig 6 a stream segment always has only one downstream segment but it may have multiple upstream segments unless it is headwater besides both stream segment indices and stream reach indices within the same segment have ascending orders 2 13 stream order stream order is defined following the classical stream order definition tarboton et al 1991 first the stream order of all the headwater stream segments are defined as 1 then the stream orders of remaining stream segments are defined based on stream topology 2 14 subbasin boundary similar to stream networks subbasins are defined reversely the algorithm works as follows 1 set the last stream segment and watershed outlet as the current stream segment index n and outlet respectively 2 scan all the grids that contribute to the current outlet based on the flow routing map set their subbasin indices to n 3 go to stream segment n n 1 repeat step 2 4 stop until all stream segments are treated n 1 2 15 software requirements and file i o hexwatershed was written in c 11 with openmp enabled fig a 16 it can be applied to both regional and global scales it is platform independent and parallel computing ready for high performance computing hpc barnes 2017 to run the hexwatershed model the minimal software requirements include 1 gnu compiler collection gcc 4 9 and above and 2 geospatial data abstraction library gdal 2 3 the required model inputs include a a high resolution traditional dem raster file and b a corresponding hexagonal mesh file which can be generated by any mesh generator in our study we used the qgis mmqgis plugin the input data must be prepared with the same spatial reference map projection after a successful model simulation hexwatershed produces a list of products including flow direction and stream networks these products have the same spatial reference as the input data because currently a standard file format for hgsd datasets is unavailable all the model inputs and outputs are stored using the esri shapefile format esri and white paperdjuly 1998 3 application 3 1 study area to test the performance of hexwatershed model we applied the model to two different types of watersheds in the western us specifically a mountainous area watershed and a flat area watershed are used to demonstrate the capability of hexwatershed model then we analyzed the model outputs the tin pan tp watershed is located near the northern border of new mexico this is a mountainous area watershed with relatively high average surface slope fig 7 the columbia basin flat cbf watershed is located near the columbia river washington this is a flat area watershed with relatively low average surface slope fig 8 characteristics of the two watersheds are listed in table 1 3 2 model setup first we collected raster dem for both tin pan 10 m resolution and columbia basin flat 90 m resolution watersheds then we generated the hexagonal mesh files for tin pan 30 m resolution and columbia basin flat 90 m resolution watersheds last we ran the hexwatershed model for both watersheds 3 3 results because of the unique structure we mainly use visualization to present the model outputs to provide a clear view of the data structure we provide zoom in views of the whole datasets and the full views of these datasets are provided in appendix d 3 3 1 tin pan zoom in views upper left of model outputs in the tin pan watershed are illustrated in fig 9 these results show that hexwatershed is able to produce all the traditional watershed delineation characteristics 3 3 2 columbia basin flat zoom in views lower left of model outputs in the columbia basin flat watershed are illustrated in fig 10 simulation results from the columbia basin flat watershed demonstrate that hexwatershed is robust in basins with flat terrain 3 4 comparisons to evaluate the performance of the hexwatershed model we compared the model outputs against outputs from the traditional sgsd method to produce the traditional watershed delineation characteristics we used the arcswat watershed delineation tool arcswat is an arcgis extension for the soil water assessment tool swat which is widely used in basin scale hydrologic simulations neitsch et al 2005 winchell et al 2007 although hexwatershed is able to produce all the watershed delineation characteristics we only compared characteristics that are commonly used in hydrologic simulations because hexwatershed is robust in different watersheds we mainly provide comparison in the tin pan watershed and show only selected results from the columbia basin flat watershed to evaluate the sensitivity of hexwatershed to mesh resolution we ran an addition simulation at tin pan watershed using a 100 m resolution hexagonal mesh 3 5 hexagonal dem the depression filled hexagonal dem fig 9a has the same spatial pattern as the traditional dem fig 7 and it fits the land surface reasonably well 3 6 flow direction because hexwatershed does not use indices 1 2 4 etc to represent flow direction we cannot compare its flow direction against arcswat flow direction output directly 3 7 flow accumulation the spatial patterns of flow accumulation from arcswat and hexwatershed are similar however their spatial distributions are different for example arcswat produces more grids with accumulation value at 1 2 and 3 whereas hexwatershed produces more between 4 and 10 fig 11 because of this hexwatershed produces less spatial variability in flow accumulation 3 8 subbasin boundary arcswat produces diagonal travel path at the subbasin interfaces for example grids with circles are defined within subbasin 17 when grids with accumulation 0 1 2 and 3 should be in either subbasin 11 or 12 fig 12 a meanwhile hexwatershed is able to eliminate the diagonal travel path and the corresponding hexagon grids are clearly defined within subbasin 2 2 2 because arcswat and hexwatershed use different index systems subbasin indices are different fig 12b 3 9 stream networks stream networks produced by hexwatershed are very close to the arcswat produced stream networks to further compare the differences we calculated the enclosed area of differences between modelled stream networks and the national hydrography dataset nhd flowline datasets we treat the nhd flowline as the true flow path in theory the smaller the total enclosed area is the closer the stream networks are to the nhd flowline to test the robustness we compared model outputs at different spatial resolutions fig 13 and table 2 statistics of areas of differences due to changing spatial resolutions are provided in table 2 these statistics show that hexwatershed performs much better at coarse resolutions 4 discussion based on model outputs and comparisons hexwatershed can provide equivalent and potentially even better performance than the traditional method because of the close relationship between watershed delineation and surface hydrology most hydrologic processes will be affected because we didn t present results on a sphere our discussion will focus on watershed scale only first hexwatershed has successfully eliminated the island effect and tedious manual corrections are no longer needed because both subbasin boundary and watershed boundary are improved stream discharge in hydrologic simulations will be improved fig 14 second although we didn t compare flow direction directly which essentially determines flow accumulation and subsequently stream networks subbasin boundary it is potentially improved because the latter ones are improved for example due to the uniform connectivity flow accumulation is smoother in spatial transitions figs 9c and 11 consequently surface runoff evapotranspiration infiltration and stream discharge will be improved fig 14 last comparisons of stream networks suggest that spatial resolution has an impact on model performance our analysis shows that hexwatershed performs much better at coarse resolutions taken together hexwatershed should be applied to hydrologic models to improve hydrologic simulations 5 limitations based on our analysis we have identified a few limitations in this study 1 currently a standard file format to manage hgsd based datasets is unavailable in this study we rely on esri shapefile for storage and visualization however due to the limitations of shapefile we can use netcdf or hdf to improve performance for example netcdf is the file format currently used by mpas mesh grid 2 the hexwatershed model relies on the accuracy of dem resampling there are challenges in converting sgsd based raster dem to hgsd based dem currently we use the nearest resampling method because it will not introduce new values into the system and it s computationally efficient other advanced resampling methods should be used in the future her and yuan 1994 3 our model currently only considers the steepest slope as the single flow direction in some scenarios multiple flow directions should be considered and we can implement the d infinity algorithm on the hgsd method henson et al 2013 4 we didn t implement the stream burn in capability in the current version which could further improve the performance under certain circumstances hellweger and maidment 1997 5 currently there is not a tool that can be used to convert existing sgsd based datasets to the hgsd based format but similar function is already available in many coupled earth system models in the future we plan to provide a tool to resolve this limitation 6 currently we only ran hexwatershed model at 30 m 90 m and 100 m spatial resolutions more simulations at different resolutions are needed to evaluate the model sensitivity to spatial resolution 6 conclusion we have developed a watershed delineation model hexwatershed using the hexagon grid spatial discretization method we have applied this model to two different types of watersheds in the western us featuring steep and flat terrain model outputs have shown that hexwatershed can reproduce all the watershed delineation characteristics comparisons between outputs from hexwatershed and the traditional square grid spatial discretization method have shown that the hgsd method has multiple advantages including removal of island effect and improvement of flow direction and all watershed characteristics such as subbasin boundary that depend on the flow direction because of the consistent connectivity analysis also suggests that spatially distributed hydrologic simulations which rely on connectivity routing can be improved if the hgsd method is used our model can be applied to continental or global scale to improve large scale hydrologic simulations declaration of competing interest the authors chang liao teklu tesfa zhuoran duan l ruby leung certify that they have no affiliations with or involvement in any organization or entity with any financial interest such as honoraria educational grants participation in speakers bureaus membership employment consultancies stock ownership or other equity interest and expert testimony or patent licensing arrangements or non financial interest such as personal or professional relationships affiliations knowledge or beliefs in the subject matter or materials discussed in this manuscript acknowledgement the research described in this paper was primarily funded by a laboratory directed research and development ldrd program quickstarter project at pacific northwest national laboratory cl and lrl were also partly supported by u s department of energy office of science biological and environmental research through the earth and environmental system modeling program as part of the energy exascale earth system model e3sm project the data used for model simulations are listed in the tables and all input data and model outputs are archived on the computers at pnnl and will be available by contacting chang liao chang liao pnnl gov the hexwatershed program can be accessed through github hexwatershed some datasets in the tin pan watershed were produced from an effort of hydrology based design of geomorphic evapotranspiration covers for reclamation of mine land a portion of this research was performed using pnnl research computing at pacific northwest national laboratory pnnl is operated for doe by battelle memorial institute under contract de ac05 76rl01830 appendix a model structure fig a 15 the work flow of the hexwatershed model the red tiles are start and end the yellow tiles are processing steps the green and blue tiles are major model inputs and outputs respectively fig a 15 fig a 16 the structure of hexwatershed model generated by doxygen fig a 16 appendix b depression filling 1 find the boundary of the grid system and push them into a queue q 2 find the grid a which has the lowest elevation in q 3 find all the untreated neighbors of grid a and put them into array b 4 if any member of b has a lower elevation than a increase its elevation to higher than a s 5 push b into q and remove a from q 6 if there are still untreated grids go to step 2 appendix c flow accumulation fig c 17 illustration of the flow accumulation algorithm in a hexagon grid system this algorithm loops through grids using the global ids and calculates accumulation once its upslope accumulations are finished fig c 17 appendix d model results appendix d 1 tin pan fig d 18 the digital elevation model using the hgsd method m fig d 18 fig d 19 the spatial distribution of simulated flow direction fig d 19 fig d 20 the spatial distribution of simulated flow accumulation fig d 20 fig d 21 the spatial distribution of simulated stream networks fig d 21 fig d 22 the spatial distribution of simulated stream order fig d 22 fig d 23 the spatial distribution of simulated subbasin boundary the colored polygons represent hexagons in the same subbasin fig d 23 appendix d 2 columbia basin flat fig d 24 the digital elevation model using the hgsd method m fig d 24 fig d 25 the spatial distribution of simulated flow direction fig d 25 fig d 26 the spatial distribution of simulated flow accumulation fig d 26 fig d 27 the spatial distribution of simulated stream networks fig d 27 fig d 28 the spatial distribution of simulated stream order fig d 28 fig d 29 the spatial distribution of simulated subbasin boundary the colored polygons represent hexagons in the same subbasin fig d 29 
26024,spatial discretization is the cornerstone of all spatially distributed numerical simulations including watershed hydrology traditional square grid spatial discretization has several limitations including inability to represent adjacency uniformly in this study we developed a watershed delineation model hexwatershed based on the hexagon grid spatial discretization we applied this model to two different types of watershed in the us and we evaluated its performance against the traditional method the comparisons show that the hexagon grid spatial discretization exhibits many advantages over the tradition method we propose that spatially distributed hydrologic simulations should consider using a hexagon grid spatial discretization keywords hydrology hexagon watershed delineation digital global grid system 1 introduction spatial discretization is the cornerstone of all spatially distributed numerical simulations including hydrologic simulations in hydrologic modeling the study domain is commonly discretized using a square grid spatial discretization sgsd few studies have investigated the performance of other spatial discretizations such as hexagon grid spatial discretization hgsd in hydrology weller et al 2009 wang and ai 2018 by definition spatial discretization is the representation of the continuous real world with discrete information in geographic information system gis sgsd is the most widely used approach to represent spatial information for example a raster digital elevation model dem dataset is usually used to describe the surface elevation of a region of interest roi on the earth s surface gesch et al 2002 because sgsd data structure can be represented by a rectangular array matrix directly it is convenient for computation analysis visualization and storage however sgsd has several limitations first sgsd cannot represent adjacency uniformly de sousa et al 2006 in a cartesian coordinate system each grid has two types of neighbors direct and diagonal the distances between the center of a grid and the center of its diagonal neighbors are further than that of the direct ones fig 1 a as a result hydrologic models have to assign different weights arbitrarily or empirically to account for the differences in travel distances these differences are also not always treated consistently for example in a coupled surface and subsurface hydrologic simulation water flow in the diagonal direction may be considered in the surface hydrology component whereas it is ignored in the groundwater hydrology component harbaugh 2005 henson et al 2013 maxwell et al 2009 langevin et al 2017 consequently it becomes one of the major model uncertainty sources input data model structure and parameters in hydrologic modeling in the remainder of the paper the term uncertainty refers to model uncertainty caused by model structure unless otherwise specified because the diagonal neighbors are connected through the vertices instead of faces they cannot represent stream width information correctly fig 1a fan et al 2007 similarly flow width information of the surface runoff is misrepresented second sgsd will create island effect due to the differences in d4 and d8 neighbor definitions which causes problems for numerical simulations birch et al 2007 in this study we define a single or group of grids that are connected through diagonal path at the edge of boundaries as an island for this reason watershed delineation results usually require tedious manual correction to eliminate these diagonal islands between subbasin boundaries johnston et al 2009 besides because most groundwater flow models do not consider d8 neighbors we cannot couple them with surface hydrology models directly harbaugh 2005 maxwell et al 2009 for example we need to set the grid 1 as inactive in a coupled groundwater and surface water flow model gsflow simulation fig 2 markstrom et al 2008 third sgsd cannot effectively represent a spherical topology which will introduce significant spatial distortions fig 3 hydrologic simulations at global scale that use longitude latitude mesh grid will be undermined especially when the roi is also the most distorted areas for this reason longitude latitude based river routing models e g model for scale adaptive river transport mosart may contain larger uncertainty at high latitudes renssen and knoop 2000 li et al 2013 furthermore as global scale oceanic models do not use sgsd method it becomes cumbersome to couple land surface hydrologic models with oceanic models other flow direction methods based on the sgsd also have similar limitations for example the d infinity flow direction method can describe the flow direction in 360 and improve the partition of water flow in different directions tarboton 1997 however these methods do not resolve the sgsd limitations fundamentally and they are relatively less used in hydrologic simulations triangular irregular networks tin spatial discretization tinsd is also used in hydrologic models one advantage of the tinsd method is that points of a tin are distributed variably so it can provide high resolution near roi whereas low resolution elsewhere francipane et al 2012 however this method is less popular because of the complex data structure because it has two types of neighbor connectivity 3 face neighbors and multiple vertice neighbors it may also introduce uncertainty for example the penn state integrated hydrologic model pihm only considers water flow through the face neighbors but ignores the vertice neighbors qu 2005 hereafter our discussion excludes tinsd unless otherwise specified watershed boundary based spatial discretization wbsd is also used in large scale hydrologic simulations tesfa et al 2014a 2014b however this method essentially depends on the availability of watershed boundaries which mostly come from sgsd based watershed delineation processes in contrast the hgsd method can resolve these limitations 1 in hgsd each grid has only one type of neighbor with the same connectivity and distance fig 1b as a result we can route both surface and subsurface water flow consistently without using different weights thus getting rid of the decadal old assumption on travel length this will improve spatially distributed hydrologic models that rely on grid connectivity liao et al 2019 guo 2 the island effect is automatically eliminated because all neighbors are connected through faces no manual corrections are needed to resolve the diagonal traveling path issue johnston et al 2009 3 it can provide continental to global coverage at consistent or variable spatial resolutions fig 3b sahr 2011 it can be used to couple land surface hydrologic models with oceanic models using a unified mesh grid e g the voronoi tessellation of the model for prediction across scale mpas du et al 2003 additionally it has other advantages 1 it can be used for coupled surface d6 and subsurface 9 point structured connectivity hydrologic modeling to resolve the inconsistency in connectivity 2 the conceptual model is more compatible with the flow width information because the flow path can be contained within the grid boundary fig 1b 3 it can improve model performance as many studies show that numerical simulations based on hexagon grid perform better when compared with other mesh grids de sousa et al 2006 weller et al 2009 2012 hamiltonalberto 2014 4 other flow direction methods e g d infinity can also be implemented on hgsd with modifications to improve flow direction and partitions tarboton 1997 in recent decades hgsd is widely used in discrete global grid system dggs weller et al 2009 randall et al 2002 for example the icosahedral snyder equal area isea tessellation method is used to generate a geodesic grid system within which most grids can be hexagons sahr 2011 despite all the advantages the hgsd method can provide it is not widely used in numerical simulations partially it is because most existing datasets were generated in the sgsd format and numerical models were not designed to use the hgsd datasets for example nearly all the spatially distributed hydrologic models were developed based on square or unstructured grid discretization and very few studies have used hgsd in this study we made the first attempt to develop a watershed delineation model hexwatershed with a set of algorithms based on the hgsd method in section 2 we introduce the model algorithms in section 3 we apply the model to two different types of watersheds and analyze the model outputs in section 4 we evaluate the model performance against outputs from the traditional sgsd method in section 5 and 6 we discuss the limitations and future work 2 model algorithm 2 1 overview following the traditional watershed delineation algorithms we developed a list of algorithms for the hgsd method fig a 15 because these algorithms are fundamentally similar in principle we mainly focus on the differences that were introduced in the new model last we describe the software requirements to run the hexwatershed model 2 2 hexagon grid resolution the hexagon grid resolution is defined using grid area instead of edge length so it is comparable with the traditional square grid wang and ai 2018 for example if the edge length of a hexagon is 10 m its area is approximately 259 81 m 2 then its equivalent effective square grid resolution is 16 11 m which is the square root of its area equations 1 and 2 1 a 3 3 2 l 2 2 r e a where a is the hexagon area l is the actual hexagon edge length and r e is the effective hexagon resolution although applications of hexwatershed in this study use a constant resolution hexagonal mesh section 3 it also supports variable resolution hexagonal mesh all algorithms within hexwatershed are designed and implemented independent of resolution for example the stream grid algorithm considers the total drainage area because area of each hexagon grid may be different section 2 9 2 3 grid topology in sgsd grid is often referred by its array matrix indices i j and its neighbors can be referred by moving up down the indices i 1 j 1 however in hgsd we cannot use array indices directly unless a dedicated hexagon grid index system is available sahr 2019 as a result an algorithm is required to obtain the topology depending on how the hexagon grid was generated there are different ways to obtain the grid topology for the sake of generality we assume that there is no prior grid topology stored within the hexagon grid hexwatershed rebuilds the topology with three steps 1 assign a unique global id for each hexagon 2 for each hexagon identify hexagons that share the same vertex edge as neighbors and 3 save the global ids of these neighbors into a look up table for each hexagon the final look up table of this algorithm is illustrated in fig 4 2 4 dem resampling similar to traditional raster datasets that use grid center to store information hexwatershed uses the hexagon center to store elevation theoratically hexagonal dem can be obtained by resampling from either high resolution traditional dem or high resolution hexagonal dem in this study the former approach was used because most available dem datasets are stored in the sgsd format to further simplify this process the nearest neighbor resampling method is used 2 5 dem depression filling similar to traditional dem hexagonal dem could potentially have local depressions when generated we developed an algorithm following the method proposed by richard barnes which uses the priority flood method to fill the depressions in any grid system barnes et al 2014 priority flood is an efficient algorithm to fill dem depressions by sequentially flooding the domain from the boundary inward to adjust elevations to assure that surface will drain olivier and frédéric 2002 to ensure an absolute drain a minimal slope 0 01 in percentage is added when applicable wang and liu 2006 the step by step instructions are provided in appendix b and illustrated in fig 5 2 6 flow direction flow direction is defined from the hexagon center to the center of neighbor hexagon which has the lowest elevation in other words flow direction is the flow path which has the steepest slope the global id of this downslope neighbor is stored in an attribute table unlike the traditional sgsd method that uses indices 1 2 4 8 etc to represent flow direction hexwatershed currently represents the flow direction using a flow routing map 2 7 flow accumulation we developed a flow accumulation algorithm based on the concept from arcgis flow accumulation tarboton et al 1991 in short this algorithm scans all the hexagon grids and sums up the accumulations once all the accumulations of upslope hexagons are calculated it runs recursively until accumulations of all hexagons are calculated fig c17 the flow accumulation algorithm also provides the option to consider variable resolution hexagonal mesh 2 8 watershed boundary the hexagon grid that has the highest flow accumulation is defined as the watershed outlet this algorithm scans all the hexagon grids and identifies all the hexagons that contribute to this outlet using the flow routing map among them those at the edge with less than 6 neighbors are used to define the watershed boundary 2 9 stream grid a hexagon grid is defined as a stream grid if its total drainage area exceeds the minimal drainage area threshold for a constant resolution hexagonal mesh each grid s drainage area is proportional to its accumulation value for a variable resolution hexagonal mesh each grid s drainage area is summarized from its upslope grids plus its own area in hexwatershed a stream grid is also named a stream reach which makes up a stream segment section 2 11 and fig 6 2 10 stream confluence stream confluences are defined based on the flow routing map and stream grids in short if a hexagon grid is a stream grid and it has multiple upslope stream reaches it is defined as a stream confluence in rare scenarios a stream confluence may have three or more upslope stream reaches 2 11 stream networks in hexwatershed a stream segment is defined as the stream component between headwater outlet and confluence or between two confluences to maintain the ascending order from upstream to downstream we developed an algorithm to define the stream segments reversely from watershed outlet to headwaters starting from the watershed outlet the algorithm searches for stream confluence following the stream grids once a stream confluence is found all of its upslope stream reaches are identified and the algorithm continues to search recursively until all stream segments are identified this algorithm works in the following steps 1 calculate the total number n of stream segments based on stream confluences information 2 set current outlet and current segment index as the watershed outlet and n respectively 3 starting from the current outlet search upstream and assign the current segment index to each stream reach 4 if a confluence is found set the current outlet and current segment index as this confluence and n n 1 respectively 5 repeat step 3 and loop through all the upstream segments of this confluence 6 stop until all confluences and segments are treated n 1 at the end of this algorithm a stream segment is made up of a list of stream reaches which have the same segment index the topology information of these stream reaches within each stream segment is also defined fig 6 2 12 stream topology stream topology is defined based on the stream reaches information if the first stream reach of a stream segment does not have a positive or valid upstream segment index this stream reach is headwater reach 1 in fig 6 similarly the segment index segment 6 of the downstream of the last stream reach reach 3 of a stream segment segment 5 is a downstream of segment 5 details of stream topology are explained in fig 6 a stream segment always has only one downstream segment but it may have multiple upstream segments unless it is headwater besides both stream segment indices and stream reach indices within the same segment have ascending orders 2 13 stream order stream order is defined following the classical stream order definition tarboton et al 1991 first the stream order of all the headwater stream segments are defined as 1 then the stream orders of remaining stream segments are defined based on stream topology 2 14 subbasin boundary similar to stream networks subbasins are defined reversely the algorithm works as follows 1 set the last stream segment and watershed outlet as the current stream segment index n and outlet respectively 2 scan all the grids that contribute to the current outlet based on the flow routing map set their subbasin indices to n 3 go to stream segment n n 1 repeat step 2 4 stop until all stream segments are treated n 1 2 15 software requirements and file i o hexwatershed was written in c 11 with openmp enabled fig a 16 it can be applied to both regional and global scales it is platform independent and parallel computing ready for high performance computing hpc barnes 2017 to run the hexwatershed model the minimal software requirements include 1 gnu compiler collection gcc 4 9 and above and 2 geospatial data abstraction library gdal 2 3 the required model inputs include a a high resolution traditional dem raster file and b a corresponding hexagonal mesh file which can be generated by any mesh generator in our study we used the qgis mmqgis plugin the input data must be prepared with the same spatial reference map projection after a successful model simulation hexwatershed produces a list of products including flow direction and stream networks these products have the same spatial reference as the input data because currently a standard file format for hgsd datasets is unavailable all the model inputs and outputs are stored using the esri shapefile format esri and white paperdjuly 1998 3 application 3 1 study area to test the performance of hexwatershed model we applied the model to two different types of watersheds in the western us specifically a mountainous area watershed and a flat area watershed are used to demonstrate the capability of hexwatershed model then we analyzed the model outputs the tin pan tp watershed is located near the northern border of new mexico this is a mountainous area watershed with relatively high average surface slope fig 7 the columbia basin flat cbf watershed is located near the columbia river washington this is a flat area watershed with relatively low average surface slope fig 8 characteristics of the two watersheds are listed in table 1 3 2 model setup first we collected raster dem for both tin pan 10 m resolution and columbia basin flat 90 m resolution watersheds then we generated the hexagonal mesh files for tin pan 30 m resolution and columbia basin flat 90 m resolution watersheds last we ran the hexwatershed model for both watersheds 3 3 results because of the unique structure we mainly use visualization to present the model outputs to provide a clear view of the data structure we provide zoom in views of the whole datasets and the full views of these datasets are provided in appendix d 3 3 1 tin pan zoom in views upper left of model outputs in the tin pan watershed are illustrated in fig 9 these results show that hexwatershed is able to produce all the traditional watershed delineation characteristics 3 3 2 columbia basin flat zoom in views lower left of model outputs in the columbia basin flat watershed are illustrated in fig 10 simulation results from the columbia basin flat watershed demonstrate that hexwatershed is robust in basins with flat terrain 3 4 comparisons to evaluate the performance of the hexwatershed model we compared the model outputs against outputs from the traditional sgsd method to produce the traditional watershed delineation characteristics we used the arcswat watershed delineation tool arcswat is an arcgis extension for the soil water assessment tool swat which is widely used in basin scale hydrologic simulations neitsch et al 2005 winchell et al 2007 although hexwatershed is able to produce all the watershed delineation characteristics we only compared characteristics that are commonly used in hydrologic simulations because hexwatershed is robust in different watersheds we mainly provide comparison in the tin pan watershed and show only selected results from the columbia basin flat watershed to evaluate the sensitivity of hexwatershed to mesh resolution we ran an addition simulation at tin pan watershed using a 100 m resolution hexagonal mesh 3 5 hexagonal dem the depression filled hexagonal dem fig 9a has the same spatial pattern as the traditional dem fig 7 and it fits the land surface reasonably well 3 6 flow direction because hexwatershed does not use indices 1 2 4 etc to represent flow direction we cannot compare its flow direction against arcswat flow direction output directly 3 7 flow accumulation the spatial patterns of flow accumulation from arcswat and hexwatershed are similar however their spatial distributions are different for example arcswat produces more grids with accumulation value at 1 2 and 3 whereas hexwatershed produces more between 4 and 10 fig 11 because of this hexwatershed produces less spatial variability in flow accumulation 3 8 subbasin boundary arcswat produces diagonal travel path at the subbasin interfaces for example grids with circles are defined within subbasin 17 when grids with accumulation 0 1 2 and 3 should be in either subbasin 11 or 12 fig 12 a meanwhile hexwatershed is able to eliminate the diagonal travel path and the corresponding hexagon grids are clearly defined within subbasin 2 2 2 because arcswat and hexwatershed use different index systems subbasin indices are different fig 12b 3 9 stream networks stream networks produced by hexwatershed are very close to the arcswat produced stream networks to further compare the differences we calculated the enclosed area of differences between modelled stream networks and the national hydrography dataset nhd flowline datasets we treat the nhd flowline as the true flow path in theory the smaller the total enclosed area is the closer the stream networks are to the nhd flowline to test the robustness we compared model outputs at different spatial resolutions fig 13 and table 2 statistics of areas of differences due to changing spatial resolutions are provided in table 2 these statistics show that hexwatershed performs much better at coarse resolutions 4 discussion based on model outputs and comparisons hexwatershed can provide equivalent and potentially even better performance than the traditional method because of the close relationship between watershed delineation and surface hydrology most hydrologic processes will be affected because we didn t present results on a sphere our discussion will focus on watershed scale only first hexwatershed has successfully eliminated the island effect and tedious manual corrections are no longer needed because both subbasin boundary and watershed boundary are improved stream discharge in hydrologic simulations will be improved fig 14 second although we didn t compare flow direction directly which essentially determines flow accumulation and subsequently stream networks subbasin boundary it is potentially improved because the latter ones are improved for example due to the uniform connectivity flow accumulation is smoother in spatial transitions figs 9c and 11 consequently surface runoff evapotranspiration infiltration and stream discharge will be improved fig 14 last comparisons of stream networks suggest that spatial resolution has an impact on model performance our analysis shows that hexwatershed performs much better at coarse resolutions taken together hexwatershed should be applied to hydrologic models to improve hydrologic simulations 5 limitations based on our analysis we have identified a few limitations in this study 1 currently a standard file format to manage hgsd based datasets is unavailable in this study we rely on esri shapefile for storage and visualization however due to the limitations of shapefile we can use netcdf or hdf to improve performance for example netcdf is the file format currently used by mpas mesh grid 2 the hexwatershed model relies on the accuracy of dem resampling there are challenges in converting sgsd based raster dem to hgsd based dem currently we use the nearest resampling method because it will not introduce new values into the system and it s computationally efficient other advanced resampling methods should be used in the future her and yuan 1994 3 our model currently only considers the steepest slope as the single flow direction in some scenarios multiple flow directions should be considered and we can implement the d infinity algorithm on the hgsd method henson et al 2013 4 we didn t implement the stream burn in capability in the current version which could further improve the performance under certain circumstances hellweger and maidment 1997 5 currently there is not a tool that can be used to convert existing sgsd based datasets to the hgsd based format but similar function is already available in many coupled earth system models in the future we plan to provide a tool to resolve this limitation 6 currently we only ran hexwatershed model at 30 m 90 m and 100 m spatial resolutions more simulations at different resolutions are needed to evaluate the model sensitivity to spatial resolution 6 conclusion we have developed a watershed delineation model hexwatershed using the hexagon grid spatial discretization method we have applied this model to two different types of watersheds in the western us featuring steep and flat terrain model outputs have shown that hexwatershed can reproduce all the watershed delineation characteristics comparisons between outputs from hexwatershed and the traditional square grid spatial discretization method have shown that the hgsd method has multiple advantages including removal of island effect and improvement of flow direction and all watershed characteristics such as subbasin boundary that depend on the flow direction because of the consistent connectivity analysis also suggests that spatially distributed hydrologic simulations which rely on connectivity routing can be improved if the hgsd method is used our model can be applied to continental or global scale to improve large scale hydrologic simulations declaration of competing interest the authors chang liao teklu tesfa zhuoran duan l ruby leung certify that they have no affiliations with or involvement in any organization or entity with any financial interest such as honoraria educational grants participation in speakers bureaus membership employment consultancies stock ownership or other equity interest and expert testimony or patent licensing arrangements or non financial interest such as personal or professional relationships affiliations knowledge or beliefs in the subject matter or materials discussed in this manuscript acknowledgement the research described in this paper was primarily funded by a laboratory directed research and development ldrd program quickstarter project at pacific northwest national laboratory cl and lrl were also partly supported by u s department of energy office of science biological and environmental research through the earth and environmental system modeling program as part of the energy exascale earth system model e3sm project the data used for model simulations are listed in the tables and all input data and model outputs are archived on the computers at pnnl and will be available by contacting chang liao chang liao pnnl gov the hexwatershed program can be accessed through github hexwatershed some datasets in the tin pan watershed were produced from an effort of hydrology based design of geomorphic evapotranspiration covers for reclamation of mine land a portion of this research was performed using pnnl research computing at pacific northwest national laboratory pnnl is operated for doe by battelle memorial institute under contract de ac05 76rl01830 appendix a model structure fig a 15 the work flow of the hexwatershed model the red tiles are start and end the yellow tiles are processing steps the green and blue tiles are major model inputs and outputs respectively fig a 15 fig a 16 the structure of hexwatershed model generated by doxygen fig a 16 appendix b depression filling 1 find the boundary of the grid system and push them into a queue q 2 find the grid a which has the lowest elevation in q 3 find all the untreated neighbors of grid a and put them into array b 4 if any member of b has a lower elevation than a increase its elevation to higher than a s 5 push b into q and remove a from q 6 if there are still untreated grids go to step 2 appendix c flow accumulation fig c 17 illustration of the flow accumulation algorithm in a hexagon grid system this algorithm loops through grids using the global ids and calculates accumulation once its upslope accumulations are finished fig c 17 appendix d model results appendix d 1 tin pan fig d 18 the digital elevation model using the hgsd method m fig d 18 fig d 19 the spatial distribution of simulated flow direction fig d 19 fig d 20 the spatial distribution of simulated flow accumulation fig d 20 fig d 21 the spatial distribution of simulated stream networks fig d 21 fig d 22 the spatial distribution of simulated stream order fig d 22 fig d 23 the spatial distribution of simulated subbasin boundary the colored polygons represent hexagons in the same subbasin fig d 23 appendix d 2 columbia basin flat fig d 24 the digital elevation model using the hgsd method m fig d 24 fig d 25 the spatial distribution of simulated flow direction fig d 25 fig d 26 the spatial distribution of simulated flow accumulation fig d 26 fig d 27 the spatial distribution of simulated stream networks fig d 27 fig d 28 the spatial distribution of simulated stream order fig d 28 fig d 29 the spatial distribution of simulated subbasin boundary the colored polygons represent hexagons in the same subbasin fig d 29 
