index,text
1775,in situ remediation of contaminated low permeability sites remains to be a critical challenge due to severe mass transfer limitations this work developed an analytical model for enhanced in situ remediation by hydraulic fracturing in low permeability sites considering the mechanisms of convection diffusion adsorption and degradation of amendments in this model amendment filled fractures were conceptualized as mass release sources with the release process controlled by a dissolution diffusion equation combining the laplace and fourier cosine transformation techniques the semi analytic solutions in the two dimensional spatial domain were obtained and then validated by comsol multiphysics 6 0 based on this model the influence range effective longevity degradability amendment reserves and fracture spacing design of reactive fractures were investigated results show that 1 three reaction zones with different degradability will potentially form around the reactive fractures 2 a larger reservoir of amendments effectively increases the influence range and effective longevity of reactive fractures 3 the synergy of multiple fracture systems outweigh the sum of individual fracture properties but the reasonable spacing design is decisive 4 ignoring adsorption in low permeability site remediation may overestimate the influence range of reactive fractures leading to remediation failure this work comprehensively analyzed the properties of reactive fractures providing practitioners with a reference for the enhanced remediation of low permeability contaminated sites keywords enhanced delivery hydraulic fracturing reactive fractures low permeability soil analytical model data availability data will be made available on request 1 introduction there is growing awareness of the potential risks to air surface water groundwater and human life from contaminated soils chowdhury et al 2017 denison et al 2022 ding et al 2021 liu et al 2013 song et al 2017 and hence remediation is required however due to the severe mass transfer restrictions the delivery of amendments into and throughout the low permeability media is often hindered leading to limited accessibility and inefficient contact chen et al 2022b chowdhury et al 2017 head et al 2020 therefore there are potential challenges in implementing effective in situ remediation techniques for contaminants in low permeability media such as silt or clay deposits various enhanced mass transfer techniques of soils have been proposed of which hydraulic fracturing is one of the most promising hossain et al 2000 murdoch 2002 nilsson et al 2011 chen and feng 2023 hydraulic fracturing involves injecting high pressure fluid into the soil formation and fracturing around the wellbore to create a nucleated fracture when the pressure exceeds a critical value in this process sand and fluid are mixed to form an injected slurry that fills the created fracture as it propagates away forming a highly permeable sand interlayer after the pressure stops bradner and murdoch 2005 murdoch 2002 if created fractures have mass outlets the amendment can be injected into the hydraulic fracture by installing wells with reasonable spacing to achieve efficient delivery of amendments chen et al 2022b otherwise the amendment injection remains difficult instead the amendment may be injected by mixing with sand as a prop pant and hence amendment interlayer can be obtained known as reactive fractures due to fick diffusion a reaction zone of a few centimeters to tens of centimeters is formed around the fracture degrading the surrounding contaminants siegrist et al 1999 christiansen et al 2008 reactive fractures installed in targeted contaminated areas can reduce the migration distance of amendments see fig 1 and hence accelerate the remediation however the properties of reactive fractures have not been fully studied depending on the transport mechanism of amendments in a fracture there have been numerous conceptual models suggested to capture the flow and transport mechanism in fractured media e g dual or multiple continua models warren and root 1963 wei and zhang 2010 and discrete fracture networks makedonska et al 2016 neuman 2005 the dual continuum model treats the matrix and fracture as separate continuums with their own pore size distribution and hydraulic properties chen and feng 2022 chen et al 2022a depending on the types of connection between fracture and matrix cells one can obtain the dual porosity model gerke and van genuchten 1993 gerke and van genuchten 1996 the dual permeability model vogel et al 2000 or their hybrid models wei and zhang 2010 known as multiple continua models the discrete fracture network focuses on characterizing the properties of a single fracture e g fracture aperture length and density therefore when examining mass transfer through sparsely fractured media is that they can capture more detailed transport mechanisms deng and spycher 2019 for instance for a single fracture tang et al 1981 developed an analytical model to study solute transport that considers the mass loss from the fracture to the surrounding matrix roubinet et al 2012 generalized tang s model by considering the two dimensional dispersion and fully coupled mass exchange recently arshadi and rajaram 2019 presented an analytical solution for mass transport in a fracture embedded within low permeability matrix accounting for bimolecular reactions note that in existing analytical models 1 an infinite length of fracture and matrix was assumed and the fracture spanned the entire matrix domain 2 the solute is always injected from one end of the fracture with the other end being the outlet it is clear that existing analytical models are not applicable for the scenario of reactive fractures in fig 1 because the length of reactive fractures is limited and fully embedded in the soil matrix besides the amendment is not injected from one end of the fracture but as a proppant during the fracture nucleation process e g siegrist et al 1999 christiansen et al 2008 therefore the reactive fractures should be considered as amendment interlayers in low permeability sites acting as a mass release source however to the authors knowledge such an analytical model has not been found in the published literature furthermore these analytical models focus on the transport of contaminants and do not address any features of amendment transport during remediation hence the properties of reactive fractures remain unclear e g the effective longevity and fracture spacing design the objective of this work is to develop an analytical model to comprehensively study the properties of reactive fractures in the in situ remediation of low permeability contaminated soils in this model the reactive fractures are treated as linear mass sources with a finite length and their mass exchange with the surrounding soil matrix is controlled by a dissolution diffusion equation and then the influence range effective longevity and degradation capacity and the spacing design of reactive fractures will be addressed besides the effect of non equilibrium adsorption on amendment transport and fracture spacing will also be discussed 2 mathematical model 2 1 conceptualization and problem description fig 1 is the conceptual illustration of the enhanced delivery of amendments in contaminated low permeability sites through hydraulic fracturing the process can be summarized in the following steps 1 using hydraulic fracturing equipment creates hydraulic fractures in the contaminated low permeability layer 2 a mixture of amendments and sand is injected into the fracture as the prop pant 3 amendments in hydraulic fractures dissolves and diffuses into the surrounding soil matrix to form 4 reactive zone in which contaminants can be rapidly degraded e g siegrist et al 1999 christiansen et al 2008 generally the hydraulic fracture length varied from 1 to 8 m to 13 26 m while the fracture aperture is much smaller ranging from 2 5 mm to 14 mm nilsson et al 2011 therefore from a mathematical modeling perspective the amendment filled fracture can be conceptualized as a soluble line source where the fracture aperture can be ignored see fig 2 as shown in this figure the two dimensional domain represents the contaminated low permeability site that needs to be remediated with a width of l and a depth of h the upper boundary of the domain is the ground surface the bottom is the bedrock the left and right are influent and effluent boundaries respectively the x axis coincides with the ground surface the origin of the coordinate system is at the left end of the domain and the z axis is perpendicular to the x axis see fig 2 hydraulic fractures can be installed anywhere in the 2d domain as a release source of amendments note that to reduce the number of input parameters this study neglects the distribution and transport of substrate considering the advection diffusion degradation and adsorption of amendments the governing equations for the described problem can be written as 1 θ c t ρ b s t d x 2 c x 2 v c x d z 2 c z 2 λ w θ c λ s ρ b s i 1 n j 1 m k 1 w q i j t h x i h x i η k δ z z j 2 s t a k d c s λ s s where c m l3 and s m m are the dissolved and adsorbed concentration of amendments respectively θ is the soil porosity ρb is the bulk density of the soil matrix m l3 t is time t v is advection velocity of groundwater lt 1 λw and λs are the first order degradation rate of dissolved and adsorbed amendments t 1 respectively xi is the location of the ith fracture at the same z coordinate zj is the location of the jth fracture at the same coordinate ηk is the kth length of hydraulic fractures h is the heaviside step function and h xi 0 when xi otherwise h xi 1 δ is the delta function and δ z zj 1 when z zj otherwise δ z zj 0 q t is the time variant function of amendments dissolves and releases source a is the first order mass transfer rate between dissolved and adsorbed amendments t 1 kd is the partitioning coefficient between dissolved and adsorbed amendments l3 m dx and dz are hydrodynamic dispersion coefficients l2t 1 and can be calculated by 3 d x α x v d m 4 d z α z v d m where αx and αz are the dispersivities in the x axis and z axis directions l dm is the molecular diffusion coefficient l2t 1 initial conditions initially since the amendment in fractures is not released the dissolved and adsorbed concentrations of amendments are zero in the two dimensional domain that is 5 c x z t 0 0 0 x l 0 z h 6 s x z t 0 0 0 x l 0 z h boundary conditions as shown in fig 2 since the upper boundary of the two dimensional domain is the ground surface and the bottom is the bedrock the neumann boundary is adopted to constrain the boundary of the z axis direction of eq 1 that is chen et al 2022b 7 c x z t z z 0 0 0 x l t 0 8 c x z t z z h 0 0 x l t 0 during low permeability site remediation we assumed that both the upper gradient plume and the downstream plume were blocked by impermeable vertical walls to prevent the leakage of pollutants usepa 1995 therefore the mass flux of left and right boundaries is zero and hence the neumann boundary also applies 9 c x z t x x 0 0 0 z h t 0 10 c x z t x x l 0 0 z h t 0 dissolution mass source of amendment filled fractures the dissolution of amendment slurry in the fracture with a first order rate law can be described by imhoff et al 2003 imhoff and miller 1996 11 ρ a v a t k a v a σ c as c where ρa is the density of amendments m l3 va is the volume fraction of amendments in the fracture characterizing the amendment reserve ka is the mass transfer rate 1 t in low permeability soils σ is a model parameter σ 0 cas is the concentration of amendment slurry in hydraulic fractures m l3 c is the amendment concentration in soil water due to c cas eq 11 may be simplified as 12 ρ a v a t k a v a σ c as where the initial volume fraction of va is va0 therefore introducing the initial condition va t 0 va0 and integrating eq 12 yields 13 ρ a v a t k a c as v a 0 exp k a c as t ρ a σ 1 k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 the source function of amendments in hydraulic fractures can be written as 14 q t θ ρ a v a t θ k a c as v a 0 exp k a c as t ρ a σ 1 θ k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 for condition σ 1 amendment source dissolution will continue forever for condition σ 1 the dissolution of amendment sources will be maintained for a limited time and then disappear specifically when t v a 0 1 σ ρ a 1 σ k a c as the amendments in hydraulic fractures would be totally released 2 2 dimensionless transform dimensionless transform can simplify the model and improve the solution efficiency and the transform method of chen et al 2022b is adopted here as follows 15 z d z h x d x l t d t θ t λ wd θ t λ w t l v p e x vl d x p e z vl d z h l 2 ρ bd ρ b θ λ sd θ t λ s a d θ t a where the subscript d represents the dimensionless term pex and pez are the peclet numbers in the x axis and z axis directions respectively t is the characteristic travel time t introducing the above dimensionless variables eqs 1 and 2 can be rewritten as 16 c t d ρ bd s t d 1 p e x 2 c x d 2 c x d 1 p e z 2 c z d 2 λ wd c λ sd ρ bd s t i 1 n j 1 m k 1 w q i j θ t t d h x di h x di η k l δ z d z dj 17 s t d a d k d c s λ sd s where q θrttd can be calculated by 18 q θ t t d θ k a c as v a 0 exp k a c as θ t t d ρ a σ 1 θ k a c as σ 1 k a c as θ t t d ρ a v a 0 1 σ σ 1 σ σ 1 similarly the initial and boundary conditions can be rewritten as 19 c x d z d t d 0 0 0 x d 1 0 z d 1 20 s x d z d t d 0 0 0 x d 1 0 z d 1 21 c x d z d t d x d x d 0 0 0 z d 1 t d 0 22 c x d z d t d x d x d 1 0 0 z d 1 t d 0 23 c x d z d t d x d z d 0 0 0 x d 1 t d 0 24 c x d z d t d x d z d 1 0 0 x d 1 t d 0 2 3 solution in the laplace domain applying the laplace transform to the dimensionless variable td and the finite fourier cosine transform to the dimensionless variable zd eqs 8a and 8b can be written as 25 2 c x d 2 p e x c x d p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd c p e x θ i 1 n j 1 m k 1 w q i j p θ t h x di h x di η k l cos ω π z dj 26 s a d k d p λ sd a d c where c and c is the laplace and finite fourier cosine transforms of c respectively defined as 27 c x d z d p 0 c x d z d t d exp p t d d t d 28 c x d ω t d 0 1 c x d z d t d cos ω π z d d z d the general solution to eq 25 can be written as see appendix a for details 29 c x d ω p c x d 0 ω p e α x d y x d 0 ω p α β e α x d e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ where 30 g j 1 m p e x θ q i j p θ t cos ω π z dj 31 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 2 0 32 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd 33 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd according to eqs 21 22 27 28 and a 9 we can obtain 34 y x d 0 ω p c x d 0 ω p x d α c x d 0 ω p 35 c x d ω p x d x d 0 0 36 c x d ω p x d x d 1 0 substituting eqs 34 36 into eq 29 yields 37a c x d 0 ω p α β α β e α e β g β γ 37b γ i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ α e α 1 x d i δ ξ ζ e ξ ζ 1 x d i δ α ξ ζ therefore eq 29 can be rewritten as 38 c x d ω p g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ the inverse finite fourier cosine transform can be written as 39 c x d z d p f 1 c x d ω p c x d ω 0 p 2 ω 1 c x d ω p cos ω π z d by substituting eq 38 into eq 39 the solution of concentration distribution in soil water can be written in the laplace domain as 40 c x d z d p mp e x q i j p θ t γ θ r α β e α e β e α x d mp e x q i j p θ t γ θ r β 2 e α e β e β x d mp e x q i j p θ t θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d finally combining eqs 26 and 40 the solution of concentration distribution adsorbed on soil particles can be expressed in the laplace domain as 41 s x d z d p α d k d p λ sd α d mp e x q i j p θ t γ θ α β e α e β e α x d mp e x q i j p θ t γ θ β 2 e α e β e β x d mp e x q i j p θ t θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d where 42 γ i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ α e α 1 x d i δ ξ ζ e ξ ζ 1 x d i δ α ξ ζ 43 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 1 β ξ 2 ξ 2 0 44 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p λ wd a d k d ρ bd p λ sd a d λ sd 45 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p λ wd a d k d ρ bd p λ sd a d λ sd eqs 40 and 41 are the solutions of amendment distribution in low permeability soils in the laplace domain including dissolved and adsorbed components due to efficiency and ease of use this study will adopt talbot s inversion algorithm talbot 1979 to inverse the laplace solutions into real time solutions wang and zhan 2015 3 verification of the analytical solutions in this section numerical simulations using comsol multiphysics 6 0 were performed to verify the correctness of the developed semi analytical solution for the cases of σ 1 and σ 1 the specified solutions can be found in appendix b as in the previous analysis when σ 1 the release of amendments in the fracture will last forever when σ 1 the amendment in the fracture is completely released in a limited time obviously the latter is more realistic and was therefore adopted in this study i e eqs b 5 and b 6 in appendix b for simplicity the reactive fracture was installed at the center of the two dimensional domain the reference points were located at 0 2 and 0 3 m directly above the fracture respectively using parameters listed in table 1 fig 3 presents time varying curves of the concentration at the reference points including the dissolved and adsorbed components it is observed that the match between the results of the analytical solution and that of the numerical solution is excellent for two locations i e δz 0 2 and 0 3 m in the low permeability soils for both dissolved and adsorbed components indicating the accuracy of the new solution besides we also calculated the results for the case of σ 1 see eqs b 3 and b 4 in appendix b as shown in fig s1 the goodness of the comparisons depicted in fig a 1 a and a 1 b corresponding to dissolved and adsorbed concentrations give us further confidence in the correctness of the developed analytical solution 4 results and discussion although the mass transfer efficiency of low permeability soils is very limited a diffuse reaction zone could be formed around amendments filled fractures due to the molecular diffusion and weak advection siegrist et al 1999 christiansen et al 2008 the reaction zone has a high and sustained oxidative degradation potential and actively remediates the contaminated soil around the hydraulic fracture therefore if the concentration distribution of contaminants is available it may be possible to remediate through the rational design of amendments filled fractures including the type and the expected amount of amendments take tce as the contaminant napl and potassium permanganate as the amendment as an example assuming that the concentration of tce in the contaminated site is distributed between 0 8 and 2 g l based on a stoichiometric requirement the concentration of potassium permanganate is at least 2 5 g l therefore the diffuse reaction zone around fractures potentially forms three concentration zones that is 5 g l strong reactive zone srz 2 5 g l weak reactive zone wrz and 2 g l non reactive zone nrz in srz amendments are excessive and contaminants will be completely degraded in wrz contaminants will be mostly degraded in nrz contaminants are only degraded in small amounts therefore the characteristics of the three reactive zones can serve as important references for reactive fracture design and evaluation note that the concentration range of the reactive area can be adjusted by actual engineering parameters and the range here is only for general discussion 4 1 amendment reserve influence range degradability and longevity of reactive hydraulic fractures the viability of emplacing reactive fractures to remediate contaminated low permeability sites requires consideration of amendment reserves influence range degradability and longevity of hydraulic fractures fig 4 a shows the spatial distribution characteristics of the amendment concentration after the amendments filled fracture is installed for 100 d due to the dissolution and diffusion of the amendment three distinct reaction zones are formed around the fracture that is srz wrz and nrz this is consistent with the experimental observation that reaction zones with different degradation capacities are formed near the mass source e g siegrist et al 1999 struse et al 2002 christenson et al 2012 the area of srz and wrz can be used to evaluate the influence range ir of amendments filled fractures defined as 46 i r t 0 h 0 l g x z t d x d z 47 g x z t 1 c x z t ϖ 0 o t h e r s where ϖ is the required concentration domain of amendments and it is assumed as 2 5 g l for wrz and 5 g l for srz in this study as seen the srz is preferentially formed near the fracture and the amendment concentration gradually decreases with increasing distance until the concentration decreases to 5 g l where the reaction zone transitions to wrz after 100d of fracture installation the influence range ir of srz is 1 37 m2 and that of wrz is 1 34 m2 they are symmetrically distributed on both sides of the fracture due to the neglect of gravity as time goes on the ir srr gradually decreases to disappear 240d while the ir wrz approaches the maximum value ir wrz max 2 74 m2 as shown in fig 4 b the time varying curves of the influence range of srz and wrz can be seen in fig 4 c due to the lower concentration range wrz could remain active for a longer time on the other hand the concentration range for wrz is also the minimum concentration required for site remediation therefore the maximum and duration of ir wrz can be used to describe the influence range and effective longevity of a reactive fracture for example the influence range ir max of the reactive fracture shown in fig 4 is 2 74 m2 and the effective longevity el is 420d amendment reserve in a hydraulic fracture is an important design parameter for site remediation which potentially affects the influence range and effective longevity of reactive fractures in this work the amendment reserve can be described by volume fraction va0 of amendments in a fracture ideally setting va0 to 1 maximizes the use of the fracture space however it should be noted that after hydraulic fracturing the amendment needs to be mixed with the prop pant to form a slurry for injection into the hydraulic fracture therefore to ensure the fluidity of the slurry va0 is usually not able to reach 1 but varies between 0 and 1 fig 5 a shows the time varying curve of release flux of amendment filled fracture as seen for the case of va0 0 7 initially the release flux of amendments is the largest q 12 3 g l d and then the flux begins to decrease after 31 d of fracture installation the flux is halved and decreases to 0 after 232 7 d meaning that amendments in the fracture are completely released on the other hand with the increase of the amendment volume fraction va0 0 3 0 5 0 7 the peak value of the release flux increased from 6 2 to 8 7 and 12 3 g l d and the continuous release time increased from 196 5 to 217 6 and 232 7d the influence of amendment reserves on the influence range and effective longevity of the fracture can be seen in fig 5 b as shown the effective longevity el increases with the increase of va0 and the curve can be described by an exponential relationship i e el 645 3va0 0 65 r 2 0 9979 the ir max va0 curve also presents a similar exponential characteristic i e ir max 6 446va0 0 90 r 2 0 9945 note that unless otherwise specified the curve equations shown in figures are primarily intended to describe the mathematical characteristics of the data rather than to give accurate predictions 4 2 vertical overlap at different depths and horizontal continuity of reactive fractures the viability of emplacing reactive fractures to remediate contaminated low permeability sites also requires consideration of vertical overlap at different depths and horizontal continuity of fractures this facilitates sound engineering decisions in the design of reactive fracture vertical and horizontal spacing a single reactive fracture due to its small aperture and limited length has a limited range of influence see fig 4 larger reactive zones can be formed but multiple amendment filled fractures need to be installed to ensure effective degradation of contaminants in the targeted reaction zone fig 6 a shows the characteristics of the reactive zone after installation for 140 d for overlapping fractures at different depths a complete reaction zone around the two fractures can be observed it can be expected that the nrr fronts of two reaction fractures meet and potentially form wrzs with stronger degradation ability and longer effective longevity fig 6 c shows a comparison of the influence range and effective longevity between vertically overlapping fractures and a single fracture as seen the maximum influence range of the vertically overlapping fracture system is increased by a factor of 2 36 2 and the effective longevity is increased by 182 days on the other hand for a single fracture see fig 4 the amendment molecules in the fracture migrate out by diffusion forming a reactive area at least a few centimeters wide on both sides of the fracture therefore when two discrete fractures are at the same depth the discontinuity between adjacent fractures potentially forms a healing zone due to molecular diffusion see fig 6 b in this figure the complete reaction zone surrounding the two horizontal fractures can also be seen which makes up for the limited length of a single fracture similarly compared with a single fracture in fig 6 c the horizontal double fracture system significantly increases the influence range 2 51 times larger and effective longevity 140 days larger in short the overlap of concentration fronts in a multi fracture system increases the influence range of reactive fractures and potentially enhances the utilization of the amendment for multi fracture systems the design of fracture spacing horizontal and vertical involves the cost and efficiency of contaminated site remediation fig 7 a shows the time varying curve of the amendment concentration at the reference point the midpoint position under different vertical spacing conditions as the spacing increases 0 32 0 42 0 52 m the concentration at the reference point decreases at different moments especially the peak concentration 6 3 5 0 4 1 g l it is worth emphasizing that when the peak concentration at the reference point just reaches the upper limit of the wrz 5 g l in this study the peak concentration at any point between the vertical overlapping fractures will be greater than this limit to ensure that the contaminants are fully degraded therefore for simplicity whether the concentration of amendments at the reference point reached 5 g l can be used as the basis for the design of the vertical spacing the case of discrete horizontal fractures has similar characteristics see fig 7 b the reference point is located in the middle of two adjacent horizontal fractures as seen as the horizontal spacing increases 0 32 0 42 0 52 m the concentration at the reference point decreases at different moments especially the peak concentration 6 1 5 0 4 3 g l similarly for discrete horizontal fracture systems whether the concentration at the reference point reaches 5 g l may be chosen as the basis for horizontal spacing design based on these analyses fig 7 c shows the effect of different amendment reserves on the vertical and horizontal spacing of the fractures with the increase of the volume fraction of amendments in fractures both the horizontal spacing dx and the vertical spacing dz increased significantly characterized by the logarithmic curve note that the increased amendment reserve means a smaller number of fractures need to be installed therefore reducing the cost of hydraulic fracturing however larger fracture spacing is not better as smaller spacing facilitates shorter remediation time therefore the fracture spacing and amendment reserves should be designed within the expected remediation time frame and then optimized for cost 4 3 role of forced convection in the remediation of contaminated low permeability soils in low permeability soils convection is very weak due to higher mass transfer limitations in engineering forced convection can be generated at low permeability sites by applying high head pressure on one side of the site fig 8 a shows the spatial distribution characteristics of the amendment after 100 d of fracture installation under forced convection conditions i e v 2 74e 3 m d comparing fig 4 a and fig 8 a the forced convection intensifies the lateral migration of the amendment and increases the influence range i e t 100d δir srz 0 25 m2 δir wrz 0 45 m2 and hence potentially changing the effective longevity of the reactive fracture as shown in fig 8 b the effective longevity el decreases approximately linearly with increasing convection rate but the influence range increases accordingly it is worth noting that forced convection not only enhances the amendment migration but promotes a more uniform distribution to avoid local accumulation thus potentially improving the utilization of the amendment chen et al 2022b besides the enhanced mass transfer of convection will also influence fracture spacing as shown in fig 8 c the vertical spacing dz gradually increases and then decreases as the convection rate increases this is because in a limited range convection will enhance mass transfer but when the value is too large the flushing effect may become apparent similarly the spacing dx of discontinuous horizontal fractures increases with the increase of flow velocity it is obvious that this increasing trend does not last all the time it is because when the flow rate exceeds a critical value it also leads to significant flushing effects except that the range of flow rates studied in fig 8 c is much smaller than the flushing critical value 4 4 role of diffusion in the remediation of contaminated low permeability soils unlike advection due to pressure gradients the diffusion coefficient is a parameter related to the characteristics of the amendment and the soil material itself e g molecular mean free energy soil porosity feng et al 2022 since advection is extremely weak in low permeability soils under natural hydraulic gradients diffusion is the dominant mechanism of mass transfer chen et al 2022b fig 9 a shows the distribution characteristics of 2 g l contour of the amendment after 100 days of fracture installation it can be seen that as dx increases from 0 3 to 0 6 the range of contours increases accordingly however after the dx increases to 0 9 the contour changes very slightly interestingly the 2 g l contour line contracted slightly when the dx is further increased this is due to the fact that a larger diffusion coefficient significantly increases the amendment migration range see 0 g l contour line in fig 9c and shortens the residence time hence the high concentration band will contract as shown in fig 9 b the effective longevity decreases as dx increases while the influence range first increases and then slightly decreases which is consistent with fig 9 a fig 9 d shows the effect of the diffusion coefficient on fracture spacing including the spacing between horizontal discrete fractures and that of vertical overlapping fractures it can be seen that as the diffusion coefficient increases the fracture spacing increases slightly and then decreases this is also attributed to the fact that a larger diffusion coefficient leads to a shorter residence time of the amendment and hence difficulties in reaching the target concentration at the reference point 4 5 adsorption and concentration trailing clay is a common mineral composition of low permeability soils and the main component is aluminosilicates due to the presence of isomorphous substitutions and broken bonds clay particles usually have a strong adsorption potential therefore the ions diffusing out of the fractures are captured to some extent by the adsorption of clay particles as the concentration of ions in the solution decreases the ions adsorbed on the surface of clay particles are desorbed into the solution to equilibrate the concentration the effect of non equilibrium adsorption on amendment transport and fracture spacing will be discussed in this section the cumulative mass of the amendment released from the hydraulic fracture into the surrounding soil matrix can be calculated by the following equations 48 a c m a s s t 0 h 0 l c x z t d x d z 49 a s m a s s t 0 h 0 l s x z t d x d z where ac mass and as mass are the dissolved mass and adsorbed mass of the amendment in the two dimensional domain respectively fig 10 a shows the time varying characteristic curves of ac mass and as mass in the two dimensional domain the dissolved cumulative mass ac mass in soil solution peaks at t 35d and then decreases a part of it is degraded and another part is adsorbed on the soil particles hence a significant concentration trailing due to adsorption can be clearly observed fig 10 b compares the time varying curves of the normalized concentration at 0 3 m directly above the fracture for the case without and considering adsorption as seen in both cases the peak concentration is reached between 60 and 80 d after fracture installation the concentration at the reference point without adsorption then decreases rapidly roughly halving around 200 d while that with adsorption decreases much more slowly roughly halving around 450 d this means that the adsorption significantly enhances the concentration residence of the amendment and hence leads to concentration trailing in fig 10 b on the other hand with the same amendment reserve the amendment concentration in soil water is significantly reduced due to adsorption for example in fig 10 b cn corresponds to a peak concentration of 10 69 g l for the non adsorption case and cns corresponds to a peak concentration of 3 28 g l for the considering adsorption case fig 10 c shows the effect of the adsorption equilibrium coefficient kd on the influence range and effective longevity of reactive fractures as seen with the increase of kd the influence range of fractures decreases this is to be expected as the adsorption of the amendment molecules by the soil particles apparently inhibits their transport further in the soil water the inhibition effect enhances the residence time of the amendment so the effective longevity of the fracture is extended see the solid line in fig 10c fig 11 shows the effect of adsorption on the vertical and horizontal spacing of reactive fractures as seen the amendment concentration peak concentration at the reference point the location can be found in fig 7 decreases significantly with the increase of kd in order to achieve the required concentration cre the fracture spacing needs to be reduced for example for the required concentration cre 5 g l as the kd increases 0 25 to 0 75 the vertical spacing needs to be reduced from 0 84 m to 0 51 m and the horizontal spacing needs to be reduced from 1 05 m to 0 59 m on the other hand if kd and cre for a contaminated low permeability site are known the design spacing of the fractures can also be quickly obtained from fig 11 to provide a basis for engineering decisions for example if the sorption equilibrium coefficient kd of the site is 0 75 the design values of vertical and horizontal spacing dz dx are 0 80 m 0 98 m 0 51 m 0 59 m and 0 31 m 0 31 m for the required concentrations cre of 2 g l 5 g l and 10 g l respectively in summary active remediation of contaminated low permeability sites using amendment filled hydraulic fracture requires consideration of the effects of adsorption otherwise the influence range of the reactive fracture may be severely overestimated leading to ineffective remediation 4 6 discussion most existing in situ chemical oxidation isco treatments have involved the injection of oxidants as liquids into the aquifer however at low permeability sites the fine texture of the soil can make it difficult for liquid injections to be effective the chemical oxidant may return from the injection borehole as it provides the path of least resistance leading to poor remediation efficiency to overcome this problem hydraulic fracturing can be used to create fractures in the soil which can then be filled with amendments as proppant e g siegrist et al 1999 christiansen et al 2008 this study has demonstrated that amendment filled fractures as mass sources can form reaction zones around them with varying degradation capacities i e srz wrz and nrz this finding is supported by the work of struse et al 2002 they found that the amendments potassium permanganate interacted with the surrounding soil matrix through diffusive transport to form different reaction zones similarly christenson et al 2012 used slow release candles of potassium permanganate as a mass source and also observed different reactive zones certainly other factors such as the effective longevity of reactive fractures the effects of forced convection and fracture spacing design also need to be considered by practitioners therefore more pilot tests and numerical simulations are required in the future to provide more evidence and knowledge besides practitioners may be interested in the adsorption of the soil to the amendment this is because adsorption significantly reduces the distance accessibility of the amendment and reduces the influence range of reactive fractures chelating agents are compounds that can form complexes with some ions leštan et al 2008 for example edta ethylenediaminetetraacetic acid can reduce the sorption capacity of soil for metal ions in the future chelating agents for reducing oxidant adsorption by fine grained soils are expected to be developed and applied this study also suffers some limitations for instance this work has focused solely on the transport properties of the amendment and did not consider the distribution of contaminants or the true chemical reaction process in terms of fracture spacing design we opted to use the maximum required dose instead of the actual amendment demand to ensure enough degradation capacity however this approach may result in excess use of the amendment which can increase costs in the future more comprehensive field tests will be required to advance the understanding of the site contaminants detailed distribution and chemical reaction processes further to improve the utilization of amendments it is essential to consider site heterogeneity as well 5 conclusion two dimensional analytical models have been presented for discrete reactive fractures in low permeability media that simulate advective dissolved dispersive and adsorptive transport processes the amendment filled fractures were conceptualized as line mass sources with a limited length and the mass exchange with the surrounding matrix was described by a dissolution diffusion equation based on laplace and fourier cosine transform techniques we obtained the specific analytical solutions for the dissolved and adsorbed components in the finite two dimensional domain the properties and spacing design of reactive fractures were addressed the main conclusions can be summarized as follows 1 as time goes on three reactive zones with different degradation capabilities i e strong reactive weak reactive and non reactive zones srz wrz and nrz potentially form around the hydraulic fractures filled by the amendment among them wrz has the largest influence range and effective longevity and hence can be used to characterize properties of reactive fractures 2 larger amendment reserves can effectively increase the influence range and effective longevity of reactive fractures reduce fracture spacing both vertical and horizontal spacing and hence potentially reduce fracturing costs 3 the overlap of concentration fronts in a multi fracture system can increase the influence range of reactive fractures and improve the utilization of amendments 4 applying forced convection in a low permeability site effectively enhances mass transport and increases the influence range of reactive fractures however it also reduces effective longevity it is worth noting that excessive convection may cause flushing effects leading to ineffective remediation 5 using reactive fractures to remediate contaminated soils requires consideration of the effects of adsorption otherwise the influence range of reactive fractures may be overestimated leading to poor engineering decisions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments much of the work described in this paper was supported by the national key research and development program of china under grant nos 2020yfc1808104 the shanghai science and technology innovation action plan under grant no 21dz1209601 and the national natural science foundation of china under grant nos 41725012 and 42007249 the writers would like to greatly acknowledge all these financial supports and express their sincerest gratitude appendix a model general solution applying the laplace transform to the dimensionless variable td and the finite fourier cosine transform to the dimensionless variable zd eq 8a can be rewritten as a 1 2 c x d 2 p e x c x d p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd c p e x θ i 1 n j 1 m k 1 w q i j p θ t h x di h x di η k l cos ω π z dj introducing a new variable y eq a 1 can be simplified to the following form a 2 y x d β y i 1 n k 1 w g h x di h x di η k l 0 where a 3 y c x d α c a 4 g j 1 m p e x θ q i j p θ t cos ω π z dj a 5 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd a 6 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd note that from a mathematical point of view the range of xd in eq a 2 can be any value from zero to infinity and the problem studied in this work is mainly aimed at the characteristics of the solution in the range of 0 1 in eqs a 2 and a 3 applying the laplace transform to the dimensionless variable xd yields a 7 s y y x d 0 ω p β y i 1 n k 1 w g e x di s e x di η k l s s 0 a 8 y s c c x d 0 ω p α c where a 9 y x d 0 ω p c x d 0 ω p x d α c x d 0 ω p substituting eqs a 8 into eq a 7 and rearranging eq a 7 yields a 10 c c x d 0 ω p s α y x d 0 ω p s α s β i 1 n k 1 w g e x di s e x di η l s s s α s β the inverse laplace transform can be written as a 11 c x d ω p 1 2 π i x d i x d i c s ω p e st d s substituting eq a 11 into eq a 10 the general solution of eq a 1 can be obtained a 12 c x d ω p c x d 0 ω p e α x d y x d 0 ω p α β e α x d e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ where a 13 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 2 0 appendix b specific solutions for the dissolved source for the dissolution mass source b 1 q t θ k a c as v a 0 exp k a c as t ρ a σ 1 θ k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 introducing the dimensionless transform method see eq 15 and laplace transform to time eq b 1 can be rewritten as b 2 q p θ k a c as v a 0 p k a c as θ t ρ a σ 1 0 θ k a c as σ 1 k a c as θ t t d ρ a v a 0 1 σ σ 1 σ exp p t d d t d σ 1 therefore combining eqs 40 and 41 the specified solution for the dissolution mass source can be written as dissolved component σ 1 b 3 c x d z d p θ k a c as v a 0 p θ t k a c as θ t ρ a mp e x γ θ r α β e α e β e α x d mp e x γ θ r β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d adsorbed component σ 1 b 4 s x d z d p α d k d p λ sd α d θ k a c as v a 0 p θ t k a c as θ t ρ a mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d dissolved component σ 1 b 5 c x d z d p 0 q θ t t d σ 1 exp p t d d t d mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d adsorbed component σ 1 s x d z d p α d k d p λ sd α d 0 q θ t t d σ 1 exp p t d d t d mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d 
1775,in situ remediation of contaminated low permeability sites remains to be a critical challenge due to severe mass transfer limitations this work developed an analytical model for enhanced in situ remediation by hydraulic fracturing in low permeability sites considering the mechanisms of convection diffusion adsorption and degradation of amendments in this model amendment filled fractures were conceptualized as mass release sources with the release process controlled by a dissolution diffusion equation combining the laplace and fourier cosine transformation techniques the semi analytic solutions in the two dimensional spatial domain were obtained and then validated by comsol multiphysics 6 0 based on this model the influence range effective longevity degradability amendment reserves and fracture spacing design of reactive fractures were investigated results show that 1 three reaction zones with different degradability will potentially form around the reactive fractures 2 a larger reservoir of amendments effectively increases the influence range and effective longevity of reactive fractures 3 the synergy of multiple fracture systems outweigh the sum of individual fracture properties but the reasonable spacing design is decisive 4 ignoring adsorption in low permeability site remediation may overestimate the influence range of reactive fractures leading to remediation failure this work comprehensively analyzed the properties of reactive fractures providing practitioners with a reference for the enhanced remediation of low permeability contaminated sites keywords enhanced delivery hydraulic fracturing reactive fractures low permeability soil analytical model data availability data will be made available on request 1 introduction there is growing awareness of the potential risks to air surface water groundwater and human life from contaminated soils chowdhury et al 2017 denison et al 2022 ding et al 2021 liu et al 2013 song et al 2017 and hence remediation is required however due to the severe mass transfer restrictions the delivery of amendments into and throughout the low permeability media is often hindered leading to limited accessibility and inefficient contact chen et al 2022b chowdhury et al 2017 head et al 2020 therefore there are potential challenges in implementing effective in situ remediation techniques for contaminants in low permeability media such as silt or clay deposits various enhanced mass transfer techniques of soils have been proposed of which hydraulic fracturing is one of the most promising hossain et al 2000 murdoch 2002 nilsson et al 2011 chen and feng 2023 hydraulic fracturing involves injecting high pressure fluid into the soil formation and fracturing around the wellbore to create a nucleated fracture when the pressure exceeds a critical value in this process sand and fluid are mixed to form an injected slurry that fills the created fracture as it propagates away forming a highly permeable sand interlayer after the pressure stops bradner and murdoch 2005 murdoch 2002 if created fractures have mass outlets the amendment can be injected into the hydraulic fracture by installing wells with reasonable spacing to achieve efficient delivery of amendments chen et al 2022b otherwise the amendment injection remains difficult instead the amendment may be injected by mixing with sand as a prop pant and hence amendment interlayer can be obtained known as reactive fractures due to fick diffusion a reaction zone of a few centimeters to tens of centimeters is formed around the fracture degrading the surrounding contaminants siegrist et al 1999 christiansen et al 2008 reactive fractures installed in targeted contaminated areas can reduce the migration distance of amendments see fig 1 and hence accelerate the remediation however the properties of reactive fractures have not been fully studied depending on the transport mechanism of amendments in a fracture there have been numerous conceptual models suggested to capture the flow and transport mechanism in fractured media e g dual or multiple continua models warren and root 1963 wei and zhang 2010 and discrete fracture networks makedonska et al 2016 neuman 2005 the dual continuum model treats the matrix and fracture as separate continuums with their own pore size distribution and hydraulic properties chen and feng 2022 chen et al 2022a depending on the types of connection between fracture and matrix cells one can obtain the dual porosity model gerke and van genuchten 1993 gerke and van genuchten 1996 the dual permeability model vogel et al 2000 or their hybrid models wei and zhang 2010 known as multiple continua models the discrete fracture network focuses on characterizing the properties of a single fracture e g fracture aperture length and density therefore when examining mass transfer through sparsely fractured media is that they can capture more detailed transport mechanisms deng and spycher 2019 for instance for a single fracture tang et al 1981 developed an analytical model to study solute transport that considers the mass loss from the fracture to the surrounding matrix roubinet et al 2012 generalized tang s model by considering the two dimensional dispersion and fully coupled mass exchange recently arshadi and rajaram 2019 presented an analytical solution for mass transport in a fracture embedded within low permeability matrix accounting for bimolecular reactions note that in existing analytical models 1 an infinite length of fracture and matrix was assumed and the fracture spanned the entire matrix domain 2 the solute is always injected from one end of the fracture with the other end being the outlet it is clear that existing analytical models are not applicable for the scenario of reactive fractures in fig 1 because the length of reactive fractures is limited and fully embedded in the soil matrix besides the amendment is not injected from one end of the fracture but as a proppant during the fracture nucleation process e g siegrist et al 1999 christiansen et al 2008 therefore the reactive fractures should be considered as amendment interlayers in low permeability sites acting as a mass release source however to the authors knowledge such an analytical model has not been found in the published literature furthermore these analytical models focus on the transport of contaminants and do not address any features of amendment transport during remediation hence the properties of reactive fractures remain unclear e g the effective longevity and fracture spacing design the objective of this work is to develop an analytical model to comprehensively study the properties of reactive fractures in the in situ remediation of low permeability contaminated soils in this model the reactive fractures are treated as linear mass sources with a finite length and their mass exchange with the surrounding soil matrix is controlled by a dissolution diffusion equation and then the influence range effective longevity and degradation capacity and the spacing design of reactive fractures will be addressed besides the effect of non equilibrium adsorption on amendment transport and fracture spacing will also be discussed 2 mathematical model 2 1 conceptualization and problem description fig 1 is the conceptual illustration of the enhanced delivery of amendments in contaminated low permeability sites through hydraulic fracturing the process can be summarized in the following steps 1 using hydraulic fracturing equipment creates hydraulic fractures in the contaminated low permeability layer 2 a mixture of amendments and sand is injected into the fracture as the prop pant 3 amendments in hydraulic fractures dissolves and diffuses into the surrounding soil matrix to form 4 reactive zone in which contaminants can be rapidly degraded e g siegrist et al 1999 christiansen et al 2008 generally the hydraulic fracture length varied from 1 to 8 m to 13 26 m while the fracture aperture is much smaller ranging from 2 5 mm to 14 mm nilsson et al 2011 therefore from a mathematical modeling perspective the amendment filled fracture can be conceptualized as a soluble line source where the fracture aperture can be ignored see fig 2 as shown in this figure the two dimensional domain represents the contaminated low permeability site that needs to be remediated with a width of l and a depth of h the upper boundary of the domain is the ground surface the bottom is the bedrock the left and right are influent and effluent boundaries respectively the x axis coincides with the ground surface the origin of the coordinate system is at the left end of the domain and the z axis is perpendicular to the x axis see fig 2 hydraulic fractures can be installed anywhere in the 2d domain as a release source of amendments note that to reduce the number of input parameters this study neglects the distribution and transport of substrate considering the advection diffusion degradation and adsorption of amendments the governing equations for the described problem can be written as 1 θ c t ρ b s t d x 2 c x 2 v c x d z 2 c z 2 λ w θ c λ s ρ b s i 1 n j 1 m k 1 w q i j t h x i h x i η k δ z z j 2 s t a k d c s λ s s where c m l3 and s m m are the dissolved and adsorbed concentration of amendments respectively θ is the soil porosity ρb is the bulk density of the soil matrix m l3 t is time t v is advection velocity of groundwater lt 1 λw and λs are the first order degradation rate of dissolved and adsorbed amendments t 1 respectively xi is the location of the ith fracture at the same z coordinate zj is the location of the jth fracture at the same coordinate ηk is the kth length of hydraulic fractures h is the heaviside step function and h xi 0 when xi otherwise h xi 1 δ is the delta function and δ z zj 1 when z zj otherwise δ z zj 0 q t is the time variant function of amendments dissolves and releases source a is the first order mass transfer rate between dissolved and adsorbed amendments t 1 kd is the partitioning coefficient between dissolved and adsorbed amendments l3 m dx and dz are hydrodynamic dispersion coefficients l2t 1 and can be calculated by 3 d x α x v d m 4 d z α z v d m where αx and αz are the dispersivities in the x axis and z axis directions l dm is the molecular diffusion coefficient l2t 1 initial conditions initially since the amendment in fractures is not released the dissolved and adsorbed concentrations of amendments are zero in the two dimensional domain that is 5 c x z t 0 0 0 x l 0 z h 6 s x z t 0 0 0 x l 0 z h boundary conditions as shown in fig 2 since the upper boundary of the two dimensional domain is the ground surface and the bottom is the bedrock the neumann boundary is adopted to constrain the boundary of the z axis direction of eq 1 that is chen et al 2022b 7 c x z t z z 0 0 0 x l t 0 8 c x z t z z h 0 0 x l t 0 during low permeability site remediation we assumed that both the upper gradient plume and the downstream plume were blocked by impermeable vertical walls to prevent the leakage of pollutants usepa 1995 therefore the mass flux of left and right boundaries is zero and hence the neumann boundary also applies 9 c x z t x x 0 0 0 z h t 0 10 c x z t x x l 0 0 z h t 0 dissolution mass source of amendment filled fractures the dissolution of amendment slurry in the fracture with a first order rate law can be described by imhoff et al 2003 imhoff and miller 1996 11 ρ a v a t k a v a σ c as c where ρa is the density of amendments m l3 va is the volume fraction of amendments in the fracture characterizing the amendment reserve ka is the mass transfer rate 1 t in low permeability soils σ is a model parameter σ 0 cas is the concentration of amendment slurry in hydraulic fractures m l3 c is the amendment concentration in soil water due to c cas eq 11 may be simplified as 12 ρ a v a t k a v a σ c as where the initial volume fraction of va is va0 therefore introducing the initial condition va t 0 va0 and integrating eq 12 yields 13 ρ a v a t k a c as v a 0 exp k a c as t ρ a σ 1 k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 the source function of amendments in hydraulic fractures can be written as 14 q t θ ρ a v a t θ k a c as v a 0 exp k a c as t ρ a σ 1 θ k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 for condition σ 1 amendment source dissolution will continue forever for condition σ 1 the dissolution of amendment sources will be maintained for a limited time and then disappear specifically when t v a 0 1 σ ρ a 1 σ k a c as the amendments in hydraulic fractures would be totally released 2 2 dimensionless transform dimensionless transform can simplify the model and improve the solution efficiency and the transform method of chen et al 2022b is adopted here as follows 15 z d z h x d x l t d t θ t λ wd θ t λ w t l v p e x vl d x p e z vl d z h l 2 ρ bd ρ b θ λ sd θ t λ s a d θ t a where the subscript d represents the dimensionless term pex and pez are the peclet numbers in the x axis and z axis directions respectively t is the characteristic travel time t introducing the above dimensionless variables eqs 1 and 2 can be rewritten as 16 c t d ρ bd s t d 1 p e x 2 c x d 2 c x d 1 p e z 2 c z d 2 λ wd c λ sd ρ bd s t i 1 n j 1 m k 1 w q i j θ t t d h x di h x di η k l δ z d z dj 17 s t d a d k d c s λ sd s where q θrttd can be calculated by 18 q θ t t d θ k a c as v a 0 exp k a c as θ t t d ρ a σ 1 θ k a c as σ 1 k a c as θ t t d ρ a v a 0 1 σ σ 1 σ σ 1 similarly the initial and boundary conditions can be rewritten as 19 c x d z d t d 0 0 0 x d 1 0 z d 1 20 s x d z d t d 0 0 0 x d 1 0 z d 1 21 c x d z d t d x d x d 0 0 0 z d 1 t d 0 22 c x d z d t d x d x d 1 0 0 z d 1 t d 0 23 c x d z d t d x d z d 0 0 0 x d 1 t d 0 24 c x d z d t d x d z d 1 0 0 x d 1 t d 0 2 3 solution in the laplace domain applying the laplace transform to the dimensionless variable td and the finite fourier cosine transform to the dimensionless variable zd eqs 8a and 8b can be written as 25 2 c x d 2 p e x c x d p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd c p e x θ i 1 n j 1 m k 1 w q i j p θ t h x di h x di η k l cos ω π z dj 26 s a d k d p λ sd a d c where c and c is the laplace and finite fourier cosine transforms of c respectively defined as 27 c x d z d p 0 c x d z d t d exp p t d d t d 28 c x d ω t d 0 1 c x d z d t d cos ω π z d d z d the general solution to eq 25 can be written as see appendix a for details 29 c x d ω p c x d 0 ω p e α x d y x d 0 ω p α β e α x d e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ where 30 g j 1 m p e x θ q i j p θ t cos ω π z dj 31 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 2 0 32 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd 33 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd according to eqs 21 22 27 28 and a 9 we can obtain 34 y x d 0 ω p c x d 0 ω p x d α c x d 0 ω p 35 c x d ω p x d x d 0 0 36 c x d ω p x d x d 1 0 substituting eqs 34 36 into eq 29 yields 37a c x d 0 ω p α β α β e α e β g β γ 37b γ i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ α e α 1 x d i δ ξ ζ e ξ ζ 1 x d i δ α ξ ζ therefore eq 29 can be rewritten as 38 c x d ω p g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ the inverse finite fourier cosine transform can be written as 39 c x d z d p f 1 c x d ω p c x d ω 0 p 2 ω 1 c x d ω p cos ω π z d by substituting eq 38 into eq 39 the solution of concentration distribution in soil water can be written in the laplace domain as 40 c x d z d p mp e x q i j p θ t γ θ r α β e α e β e α x d mp e x q i j p θ t γ θ r β 2 e α e β e β x d mp e x q i j p θ t θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d finally combining eqs 26 and 40 the solution of concentration distribution adsorbed on soil particles can be expressed in the laplace domain as 41 s x d z d p α d k d p λ sd α d mp e x q i j p θ t γ θ α β e α e β e α x d mp e x q i j p θ t γ θ β 2 e α e β e β x d mp e x q i j p θ t θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d where 42 γ i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ α e α 1 x d i δ ξ ζ e ξ ζ 1 x d i δ α ξ ζ 43 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 1 β ξ 2 ξ 2 0 44 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p λ wd a d k d ρ bd p λ sd a d λ sd 45 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p λ wd a d k d ρ bd p λ sd a d λ sd eqs 40 and 41 are the solutions of amendment distribution in low permeability soils in the laplace domain including dissolved and adsorbed components due to efficiency and ease of use this study will adopt talbot s inversion algorithm talbot 1979 to inverse the laplace solutions into real time solutions wang and zhan 2015 3 verification of the analytical solutions in this section numerical simulations using comsol multiphysics 6 0 were performed to verify the correctness of the developed semi analytical solution for the cases of σ 1 and σ 1 the specified solutions can be found in appendix b as in the previous analysis when σ 1 the release of amendments in the fracture will last forever when σ 1 the amendment in the fracture is completely released in a limited time obviously the latter is more realistic and was therefore adopted in this study i e eqs b 5 and b 6 in appendix b for simplicity the reactive fracture was installed at the center of the two dimensional domain the reference points were located at 0 2 and 0 3 m directly above the fracture respectively using parameters listed in table 1 fig 3 presents time varying curves of the concentration at the reference points including the dissolved and adsorbed components it is observed that the match between the results of the analytical solution and that of the numerical solution is excellent for two locations i e δz 0 2 and 0 3 m in the low permeability soils for both dissolved and adsorbed components indicating the accuracy of the new solution besides we also calculated the results for the case of σ 1 see eqs b 3 and b 4 in appendix b as shown in fig s1 the goodness of the comparisons depicted in fig a 1 a and a 1 b corresponding to dissolved and adsorbed concentrations give us further confidence in the correctness of the developed analytical solution 4 results and discussion although the mass transfer efficiency of low permeability soils is very limited a diffuse reaction zone could be formed around amendments filled fractures due to the molecular diffusion and weak advection siegrist et al 1999 christiansen et al 2008 the reaction zone has a high and sustained oxidative degradation potential and actively remediates the contaminated soil around the hydraulic fracture therefore if the concentration distribution of contaminants is available it may be possible to remediate through the rational design of amendments filled fractures including the type and the expected amount of amendments take tce as the contaminant napl and potassium permanganate as the amendment as an example assuming that the concentration of tce in the contaminated site is distributed between 0 8 and 2 g l based on a stoichiometric requirement the concentration of potassium permanganate is at least 2 5 g l therefore the diffuse reaction zone around fractures potentially forms three concentration zones that is 5 g l strong reactive zone srz 2 5 g l weak reactive zone wrz and 2 g l non reactive zone nrz in srz amendments are excessive and contaminants will be completely degraded in wrz contaminants will be mostly degraded in nrz contaminants are only degraded in small amounts therefore the characteristics of the three reactive zones can serve as important references for reactive fracture design and evaluation note that the concentration range of the reactive area can be adjusted by actual engineering parameters and the range here is only for general discussion 4 1 amendment reserve influence range degradability and longevity of reactive hydraulic fractures the viability of emplacing reactive fractures to remediate contaminated low permeability sites requires consideration of amendment reserves influence range degradability and longevity of hydraulic fractures fig 4 a shows the spatial distribution characteristics of the amendment concentration after the amendments filled fracture is installed for 100 d due to the dissolution and diffusion of the amendment three distinct reaction zones are formed around the fracture that is srz wrz and nrz this is consistent with the experimental observation that reaction zones with different degradation capacities are formed near the mass source e g siegrist et al 1999 struse et al 2002 christenson et al 2012 the area of srz and wrz can be used to evaluate the influence range ir of amendments filled fractures defined as 46 i r t 0 h 0 l g x z t d x d z 47 g x z t 1 c x z t ϖ 0 o t h e r s where ϖ is the required concentration domain of amendments and it is assumed as 2 5 g l for wrz and 5 g l for srz in this study as seen the srz is preferentially formed near the fracture and the amendment concentration gradually decreases with increasing distance until the concentration decreases to 5 g l where the reaction zone transitions to wrz after 100d of fracture installation the influence range ir of srz is 1 37 m2 and that of wrz is 1 34 m2 they are symmetrically distributed on both sides of the fracture due to the neglect of gravity as time goes on the ir srr gradually decreases to disappear 240d while the ir wrz approaches the maximum value ir wrz max 2 74 m2 as shown in fig 4 b the time varying curves of the influence range of srz and wrz can be seen in fig 4 c due to the lower concentration range wrz could remain active for a longer time on the other hand the concentration range for wrz is also the minimum concentration required for site remediation therefore the maximum and duration of ir wrz can be used to describe the influence range and effective longevity of a reactive fracture for example the influence range ir max of the reactive fracture shown in fig 4 is 2 74 m2 and the effective longevity el is 420d amendment reserve in a hydraulic fracture is an important design parameter for site remediation which potentially affects the influence range and effective longevity of reactive fractures in this work the amendment reserve can be described by volume fraction va0 of amendments in a fracture ideally setting va0 to 1 maximizes the use of the fracture space however it should be noted that after hydraulic fracturing the amendment needs to be mixed with the prop pant to form a slurry for injection into the hydraulic fracture therefore to ensure the fluidity of the slurry va0 is usually not able to reach 1 but varies between 0 and 1 fig 5 a shows the time varying curve of release flux of amendment filled fracture as seen for the case of va0 0 7 initially the release flux of amendments is the largest q 12 3 g l d and then the flux begins to decrease after 31 d of fracture installation the flux is halved and decreases to 0 after 232 7 d meaning that amendments in the fracture are completely released on the other hand with the increase of the amendment volume fraction va0 0 3 0 5 0 7 the peak value of the release flux increased from 6 2 to 8 7 and 12 3 g l d and the continuous release time increased from 196 5 to 217 6 and 232 7d the influence of amendment reserves on the influence range and effective longevity of the fracture can be seen in fig 5 b as shown the effective longevity el increases with the increase of va0 and the curve can be described by an exponential relationship i e el 645 3va0 0 65 r 2 0 9979 the ir max va0 curve also presents a similar exponential characteristic i e ir max 6 446va0 0 90 r 2 0 9945 note that unless otherwise specified the curve equations shown in figures are primarily intended to describe the mathematical characteristics of the data rather than to give accurate predictions 4 2 vertical overlap at different depths and horizontal continuity of reactive fractures the viability of emplacing reactive fractures to remediate contaminated low permeability sites also requires consideration of vertical overlap at different depths and horizontal continuity of fractures this facilitates sound engineering decisions in the design of reactive fracture vertical and horizontal spacing a single reactive fracture due to its small aperture and limited length has a limited range of influence see fig 4 larger reactive zones can be formed but multiple amendment filled fractures need to be installed to ensure effective degradation of contaminants in the targeted reaction zone fig 6 a shows the characteristics of the reactive zone after installation for 140 d for overlapping fractures at different depths a complete reaction zone around the two fractures can be observed it can be expected that the nrr fronts of two reaction fractures meet and potentially form wrzs with stronger degradation ability and longer effective longevity fig 6 c shows a comparison of the influence range and effective longevity between vertically overlapping fractures and a single fracture as seen the maximum influence range of the vertically overlapping fracture system is increased by a factor of 2 36 2 and the effective longevity is increased by 182 days on the other hand for a single fracture see fig 4 the amendment molecules in the fracture migrate out by diffusion forming a reactive area at least a few centimeters wide on both sides of the fracture therefore when two discrete fractures are at the same depth the discontinuity between adjacent fractures potentially forms a healing zone due to molecular diffusion see fig 6 b in this figure the complete reaction zone surrounding the two horizontal fractures can also be seen which makes up for the limited length of a single fracture similarly compared with a single fracture in fig 6 c the horizontal double fracture system significantly increases the influence range 2 51 times larger and effective longevity 140 days larger in short the overlap of concentration fronts in a multi fracture system increases the influence range of reactive fractures and potentially enhances the utilization of the amendment for multi fracture systems the design of fracture spacing horizontal and vertical involves the cost and efficiency of contaminated site remediation fig 7 a shows the time varying curve of the amendment concentration at the reference point the midpoint position under different vertical spacing conditions as the spacing increases 0 32 0 42 0 52 m the concentration at the reference point decreases at different moments especially the peak concentration 6 3 5 0 4 1 g l it is worth emphasizing that when the peak concentration at the reference point just reaches the upper limit of the wrz 5 g l in this study the peak concentration at any point between the vertical overlapping fractures will be greater than this limit to ensure that the contaminants are fully degraded therefore for simplicity whether the concentration of amendments at the reference point reached 5 g l can be used as the basis for the design of the vertical spacing the case of discrete horizontal fractures has similar characteristics see fig 7 b the reference point is located in the middle of two adjacent horizontal fractures as seen as the horizontal spacing increases 0 32 0 42 0 52 m the concentration at the reference point decreases at different moments especially the peak concentration 6 1 5 0 4 3 g l similarly for discrete horizontal fracture systems whether the concentration at the reference point reaches 5 g l may be chosen as the basis for horizontal spacing design based on these analyses fig 7 c shows the effect of different amendment reserves on the vertical and horizontal spacing of the fractures with the increase of the volume fraction of amendments in fractures both the horizontal spacing dx and the vertical spacing dz increased significantly characterized by the logarithmic curve note that the increased amendment reserve means a smaller number of fractures need to be installed therefore reducing the cost of hydraulic fracturing however larger fracture spacing is not better as smaller spacing facilitates shorter remediation time therefore the fracture spacing and amendment reserves should be designed within the expected remediation time frame and then optimized for cost 4 3 role of forced convection in the remediation of contaminated low permeability soils in low permeability soils convection is very weak due to higher mass transfer limitations in engineering forced convection can be generated at low permeability sites by applying high head pressure on one side of the site fig 8 a shows the spatial distribution characteristics of the amendment after 100 d of fracture installation under forced convection conditions i e v 2 74e 3 m d comparing fig 4 a and fig 8 a the forced convection intensifies the lateral migration of the amendment and increases the influence range i e t 100d δir srz 0 25 m2 δir wrz 0 45 m2 and hence potentially changing the effective longevity of the reactive fracture as shown in fig 8 b the effective longevity el decreases approximately linearly with increasing convection rate but the influence range increases accordingly it is worth noting that forced convection not only enhances the amendment migration but promotes a more uniform distribution to avoid local accumulation thus potentially improving the utilization of the amendment chen et al 2022b besides the enhanced mass transfer of convection will also influence fracture spacing as shown in fig 8 c the vertical spacing dz gradually increases and then decreases as the convection rate increases this is because in a limited range convection will enhance mass transfer but when the value is too large the flushing effect may become apparent similarly the spacing dx of discontinuous horizontal fractures increases with the increase of flow velocity it is obvious that this increasing trend does not last all the time it is because when the flow rate exceeds a critical value it also leads to significant flushing effects except that the range of flow rates studied in fig 8 c is much smaller than the flushing critical value 4 4 role of diffusion in the remediation of contaminated low permeability soils unlike advection due to pressure gradients the diffusion coefficient is a parameter related to the characteristics of the amendment and the soil material itself e g molecular mean free energy soil porosity feng et al 2022 since advection is extremely weak in low permeability soils under natural hydraulic gradients diffusion is the dominant mechanism of mass transfer chen et al 2022b fig 9 a shows the distribution characteristics of 2 g l contour of the amendment after 100 days of fracture installation it can be seen that as dx increases from 0 3 to 0 6 the range of contours increases accordingly however after the dx increases to 0 9 the contour changes very slightly interestingly the 2 g l contour line contracted slightly when the dx is further increased this is due to the fact that a larger diffusion coefficient significantly increases the amendment migration range see 0 g l contour line in fig 9c and shortens the residence time hence the high concentration band will contract as shown in fig 9 b the effective longevity decreases as dx increases while the influence range first increases and then slightly decreases which is consistent with fig 9 a fig 9 d shows the effect of the diffusion coefficient on fracture spacing including the spacing between horizontal discrete fractures and that of vertical overlapping fractures it can be seen that as the diffusion coefficient increases the fracture spacing increases slightly and then decreases this is also attributed to the fact that a larger diffusion coefficient leads to a shorter residence time of the amendment and hence difficulties in reaching the target concentration at the reference point 4 5 adsorption and concentration trailing clay is a common mineral composition of low permeability soils and the main component is aluminosilicates due to the presence of isomorphous substitutions and broken bonds clay particles usually have a strong adsorption potential therefore the ions diffusing out of the fractures are captured to some extent by the adsorption of clay particles as the concentration of ions in the solution decreases the ions adsorbed on the surface of clay particles are desorbed into the solution to equilibrate the concentration the effect of non equilibrium adsorption on amendment transport and fracture spacing will be discussed in this section the cumulative mass of the amendment released from the hydraulic fracture into the surrounding soil matrix can be calculated by the following equations 48 a c m a s s t 0 h 0 l c x z t d x d z 49 a s m a s s t 0 h 0 l s x z t d x d z where ac mass and as mass are the dissolved mass and adsorbed mass of the amendment in the two dimensional domain respectively fig 10 a shows the time varying characteristic curves of ac mass and as mass in the two dimensional domain the dissolved cumulative mass ac mass in soil solution peaks at t 35d and then decreases a part of it is degraded and another part is adsorbed on the soil particles hence a significant concentration trailing due to adsorption can be clearly observed fig 10 b compares the time varying curves of the normalized concentration at 0 3 m directly above the fracture for the case without and considering adsorption as seen in both cases the peak concentration is reached between 60 and 80 d after fracture installation the concentration at the reference point without adsorption then decreases rapidly roughly halving around 200 d while that with adsorption decreases much more slowly roughly halving around 450 d this means that the adsorption significantly enhances the concentration residence of the amendment and hence leads to concentration trailing in fig 10 b on the other hand with the same amendment reserve the amendment concentration in soil water is significantly reduced due to adsorption for example in fig 10 b cn corresponds to a peak concentration of 10 69 g l for the non adsorption case and cns corresponds to a peak concentration of 3 28 g l for the considering adsorption case fig 10 c shows the effect of the adsorption equilibrium coefficient kd on the influence range and effective longevity of reactive fractures as seen with the increase of kd the influence range of fractures decreases this is to be expected as the adsorption of the amendment molecules by the soil particles apparently inhibits their transport further in the soil water the inhibition effect enhances the residence time of the amendment so the effective longevity of the fracture is extended see the solid line in fig 10c fig 11 shows the effect of adsorption on the vertical and horizontal spacing of reactive fractures as seen the amendment concentration peak concentration at the reference point the location can be found in fig 7 decreases significantly with the increase of kd in order to achieve the required concentration cre the fracture spacing needs to be reduced for example for the required concentration cre 5 g l as the kd increases 0 25 to 0 75 the vertical spacing needs to be reduced from 0 84 m to 0 51 m and the horizontal spacing needs to be reduced from 1 05 m to 0 59 m on the other hand if kd and cre for a contaminated low permeability site are known the design spacing of the fractures can also be quickly obtained from fig 11 to provide a basis for engineering decisions for example if the sorption equilibrium coefficient kd of the site is 0 75 the design values of vertical and horizontal spacing dz dx are 0 80 m 0 98 m 0 51 m 0 59 m and 0 31 m 0 31 m for the required concentrations cre of 2 g l 5 g l and 10 g l respectively in summary active remediation of contaminated low permeability sites using amendment filled hydraulic fracture requires consideration of the effects of adsorption otherwise the influence range of the reactive fracture may be severely overestimated leading to ineffective remediation 4 6 discussion most existing in situ chemical oxidation isco treatments have involved the injection of oxidants as liquids into the aquifer however at low permeability sites the fine texture of the soil can make it difficult for liquid injections to be effective the chemical oxidant may return from the injection borehole as it provides the path of least resistance leading to poor remediation efficiency to overcome this problem hydraulic fracturing can be used to create fractures in the soil which can then be filled with amendments as proppant e g siegrist et al 1999 christiansen et al 2008 this study has demonstrated that amendment filled fractures as mass sources can form reaction zones around them with varying degradation capacities i e srz wrz and nrz this finding is supported by the work of struse et al 2002 they found that the amendments potassium permanganate interacted with the surrounding soil matrix through diffusive transport to form different reaction zones similarly christenson et al 2012 used slow release candles of potassium permanganate as a mass source and also observed different reactive zones certainly other factors such as the effective longevity of reactive fractures the effects of forced convection and fracture spacing design also need to be considered by practitioners therefore more pilot tests and numerical simulations are required in the future to provide more evidence and knowledge besides practitioners may be interested in the adsorption of the soil to the amendment this is because adsorption significantly reduces the distance accessibility of the amendment and reduces the influence range of reactive fractures chelating agents are compounds that can form complexes with some ions leštan et al 2008 for example edta ethylenediaminetetraacetic acid can reduce the sorption capacity of soil for metal ions in the future chelating agents for reducing oxidant adsorption by fine grained soils are expected to be developed and applied this study also suffers some limitations for instance this work has focused solely on the transport properties of the amendment and did not consider the distribution of contaminants or the true chemical reaction process in terms of fracture spacing design we opted to use the maximum required dose instead of the actual amendment demand to ensure enough degradation capacity however this approach may result in excess use of the amendment which can increase costs in the future more comprehensive field tests will be required to advance the understanding of the site contaminants detailed distribution and chemical reaction processes further to improve the utilization of amendments it is essential to consider site heterogeneity as well 5 conclusion two dimensional analytical models have been presented for discrete reactive fractures in low permeability media that simulate advective dissolved dispersive and adsorptive transport processes the amendment filled fractures were conceptualized as line mass sources with a limited length and the mass exchange with the surrounding matrix was described by a dissolution diffusion equation based on laplace and fourier cosine transform techniques we obtained the specific analytical solutions for the dissolved and adsorbed components in the finite two dimensional domain the properties and spacing design of reactive fractures were addressed the main conclusions can be summarized as follows 1 as time goes on three reactive zones with different degradation capabilities i e strong reactive weak reactive and non reactive zones srz wrz and nrz potentially form around the hydraulic fractures filled by the amendment among them wrz has the largest influence range and effective longevity and hence can be used to characterize properties of reactive fractures 2 larger amendment reserves can effectively increase the influence range and effective longevity of reactive fractures reduce fracture spacing both vertical and horizontal spacing and hence potentially reduce fracturing costs 3 the overlap of concentration fronts in a multi fracture system can increase the influence range of reactive fractures and improve the utilization of amendments 4 applying forced convection in a low permeability site effectively enhances mass transport and increases the influence range of reactive fractures however it also reduces effective longevity it is worth noting that excessive convection may cause flushing effects leading to ineffective remediation 5 using reactive fractures to remediate contaminated soils requires consideration of the effects of adsorption otherwise the influence range of reactive fractures may be overestimated leading to poor engineering decisions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments much of the work described in this paper was supported by the national key research and development program of china under grant nos 2020yfc1808104 the shanghai science and technology innovation action plan under grant no 21dz1209601 and the national natural science foundation of china under grant nos 41725012 and 42007249 the writers would like to greatly acknowledge all these financial supports and express their sincerest gratitude appendix a model general solution applying the laplace transform to the dimensionless variable td and the finite fourier cosine transform to the dimensionless variable zd eq 8a can be rewritten as a 1 2 c x d 2 p e x c x d p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd c p e x θ i 1 n j 1 m k 1 w q i j p θ t h x di h x di η k l cos ω π z dj introducing a new variable y eq a 1 can be simplified to the following form a 2 y x d β y i 1 n k 1 w g h x di h x di η k l 0 where a 3 y c x d α c a 4 g j 1 m p e x θ q i j p θ t cos ω π z dj a 5 α 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd a 6 β 1 2 p e x p e x 2 4 p e x p a d k d ρ bd p λ sd a d p 1 p e z π 2 ω 2 λ wd a d k d ρ bd p λ sd a d λ sd note that from a mathematical point of view the range of xd in eq a 2 can be any value from zero to infinity and the problem studied in this work is mainly aimed at the characteristics of the solution in the range of 0 1 in eqs a 2 and a 3 applying the laplace transform to the dimensionless variable xd yields a 7 s y y x d 0 ω p β y i 1 n k 1 w g e x di s e x di η k l s s 0 a 8 y s c c x d 0 ω p α c where a 9 y x d 0 ω p c x d 0 ω p x d α c x d 0 ω p substituting eqs a 8 into eq a 7 and rearranging eq a 7 yields a 10 c c x d 0 ω p s α y x d 0 ω p s α s β i 1 n k 1 w g e x di s e x di η l s s s α s β the inverse laplace transform can be written as a 11 c x d ω p 1 2 π i x d i x d i c s ω p e st d s substituting eq a 11 into eq a 10 the general solution of eq a 1 can be obtained a 12 c x d ω p c x d 0 ω p e α x d y x d 0 ω p α β e α x d e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ where a 13 x d i 1 x di x d i 2 x di η k l ξ 1 β ξ 2 0 appendix b specific solutions for the dissolved source for the dissolution mass source b 1 q t θ k a c as v a 0 exp k a c as t ρ a σ 1 θ k a c as σ 1 k a c as t ρ a v a 0 1 σ σ 1 σ σ 1 introducing the dimensionless transform method see eq 15 and laplace transform to time eq b 1 can be rewritten as b 2 q p θ k a c as v a 0 p k a c as θ t ρ a σ 1 0 θ k a c as σ 1 k a c as θ t t d ρ a v a 0 1 σ σ 1 σ exp p t d d t d σ 1 therefore combining eqs 40 and 41 the specified solution for the dissolution mass source can be written as dissolved component σ 1 b 3 c x d z d p θ k a c as v a 0 p θ t k a c as θ t ρ a mp e x γ θ r α β e α e β e α x d mp e x γ θ r β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d adsorbed component σ 1 b 4 s x d z d p α d k d p λ sd α d θ k a c as v a 0 p θ t k a c as θ t ρ a mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d dissolved component σ 1 b 5 c x d z d p 0 q θ t t d σ 1 exp p t d d t d mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d adsorbed component σ 1 s x d z d p α d k d p λ sd α d 0 q θ t t d σ 1 exp p t d d t d mp e x γ θ α β e α e β e α x d mp e x γ θ β 2 e α e β e β x d mp e x θ β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ 2 ω 1 g γ α β e α e β e α x d g γ β 2 e α e β e β x d g β i 1 n k 1 w ζ 1 2 δ 1 2 1 δ ζ e α x d x d i δ e ξ ζ x d x d i δ α ξ ζ h x d x d i δ cos ω π z d 
1776,the murray darling basin is australia s most stressed water resource and most contentious in the sharing of those water resources our hydrological models used for water management have no information on the surface water groundwater interactions within a reach and so implicitly assume that their relationship is stationary this study seeks to test this assumption by looking at the direction of surface water groundwater exchanges and how they have changed over the 49 year period from 1970 to 2019 at the whole of basin scale 492 river reaches this was achieved by comparing the groundwater level at 59 340 bores to the elevation of the surface water at the closest point on the stream network each point evaluated on the stream network was classified as either gaining water from groundwater or losing water to groundwater at an annual scale these point scale observations were aggregated to the reach scale as used in river systems models for analysis of trends though time the proportion of bores predicting losing conditions was falling through the 1970 s and 1980 s before reaching a minimum of 48 in 1995 96 from this period the proportion of bores predicting losing conditions increased to a maximum at the end of the analysis period of 78 in 2018 19 this finding at the point scale is replicated at the reach scale the reaches with greater than 80 of bores predicting losing conditions had a minimum in 1993 94 at 40 and increased to 66 at the end of the analysis period these observations are consistent with the management challenges of the time in the 1980 s and 1990 s gaining conditions leading to increasing salt loads in the streams caused by groundwater discharge were a major problem for downstream water users since the turn of the century drought and water scarcity have been the major challenges with falling groundwater levels leading to increases in losing conditions this study has shown that the surface water groundwater interactions are not stationary and have been changing through time with many reaches having changed direction from gaining to losing throughout the analysis period this is highlighting a deficiency in our hydrological models that cannot replicate this change in flux direction and therefore leading to errors in predicting low flows keywords surface water groundwater interactions river systems models low flow prediction murray darling basin australia data availability the data used is cited in the text and is available from https doi org 10 25919 x453 3g36 1 introduction many of our commonly used hydrological models are conceptualised as gaining streams i e groundwater flows into the stream this stems from the early conceptual understanding of catchment hydrology such as the partial contributing area of dunne and black 1970 or the nested groundwater flow paths of tóth 1963 these hydrological models have successfully been used for decades in water resources planning where the total volume of flow is the most important variable to reproduce but with increased emphasis being placed on low flows for environmental purposes hallouin et al 2020 nicolle et al 2014 these models have been found to be inadequate in some circumstances it has been suggested that the one way coupling of surface water to groundwater results in the poor performance in reproducing low flows jachens et al 2021 because such representation cannot adequately simulate losing conditions surface water groundwater interactions are generally classified into gaining and losing streams where gaining streams are the recipients of groundwater discharge and losing streams contribute to groundwater recharge winter et al 1998 losing streams can be further classified as losing connected or losing disconnected in the losing disconnected case there is an unsaturated zone below the stream bed and the rate of loss from the stream is independent of the depth of groundwater whereas in a losing connected case there is a saturated connection between the surface water and the groundwater and the rate of loss is dependent on the difference in elevation between the surface water and the groundwater brunner et al 2011 winter et al 1998 there are many methods of classifying a stream reach as gaining or losing cook 2015 kalbus et al 2006 at a small scale these can include seepage meters lee 1977 streambed piezometry baxter et al 2003 and heat as a tracer constantz 1998 at larger scales the methods include environmental tracers cook et al 2006 drury et al 1984 differential gauging cey et al 1998 and head gradients freeze and cherry 1979 changes in the surface water groundwater interactions can occur over scales of metres in hyporheic exchange zones harvey and bencala 1993 or many 100 s of kilometres in response to changes in geomorphology and geology konrad 2006 similarly changes in the direction of surface water groundwater exchanges can occur very quickly due to flooding cui et al 2018 or over decades in response to groundwater extraction sophocleous 2000 most of the studies that have enhanced our understanding of surface water groundwater interactions have occurred at a small scale cook 2015 at the regional to continental scale there are few examples some rely solely on surface water data whilst others use both surface and groundwater data differential gauging the difference between upstream and downstream flow measurements has been used in new zealand as a training dataset to upscale to the entire country using machine learning trained on many physical properties of the stream reaches yang et al 2019 analysis of flow duration curves and hydrographs has been used to map gaining and losing reaches in the namoi catchment in australia ivkovic 2009 covering the namoi and the rest of the murray darling basin parsons et al 2008 interpolated groundwater level observations to a water table surface and compared to the surface water elevations to map gaining and losing reaches the largest study has been in the continental united states where the observed groundwater levels were compared to the surface water level at the nearest point in the stream network jasechko et al 2021 these studies have all been of a limited period of time parsons et al 2008 or a long term average ivkovic 2009 jasechko et al 2021 yang et al 2019 none of them have considered a time series or how surface water groundwater interactions have changed through time being able to observe and map the changes in gaining and losing reaches within a basin is the first step to understand the future conditions and provide the process understanding to improve the low flow prediction capability of our hydrological models the aims of this study are to 1 map the gaining and losing reaches of the streams of the murray darling basin used by the river systems models at an annual time step 2 analyse the changes in surface water groundwater interactions through time and 3 to make recommendations on improving the structure of our hydrological models to simulate changes in surface water groundwater interactions 2 study region the murray darling basin mdb is australia s most important river system accounting for 62 of australia s water use abs 2022 and producing 42 of australia s agricultural output abs 2021 for the 2020 21 financial year it covers 1 06x106 km2 and is home to 2 million people and supplies water to over another million people the surface water resources are divided into 19 catchments fig 1 a for administration under the basin plan australian government 2012 and are further grouped into the northern basin catchments flowing into the darling river and the southern basin the catchments flowing into the murray river fig 1c all these rivers have their highest rainfall in the headwaters fig 1b with the climate becoming progressively more arid heading downstream fig 1c most of the major rivers are highly regulated with large dams in their headwaters increasing the reliability of supply of water to the irrigation industry downstream the majority of groundwater use occurs in eight alluvial water sources goulburn murray sedimentary plains vic and the gwydir nsw lachlan nsw macquarie nsw murray nsw murrumbidgee nsw namoi nsw and condamine qld alluvia associated with the major rivers of the basin fu et al 2022 there are also two major sedimentary basins that contain regional groundwater flow paths the murray geological basin in the south and the great artesian basin in the north fig 1d the murray geological basin is a closed basin the flow of groundwater is toward the depocenter in the centre west of the basin around renmark with the only outflow through discharge into the murray river or evapotranspiration evans and kellett 1989 discharge occurs via evapotranspiration in a series of saline lakes where groundwater flow is constricted by basement highs and also discharge to the murray river downstream of mallee cliffs 30 km east of mildura brown 1989 underlying the northern part of the mdb is a portion of the surat and eromanga basins that form part of the great artesian basin the surat basin has its intake beds along the great dividing range but there is little interaction with the rivers of the mdb as the major aquifers are buried below thick aquitards ransley et al 2015 the coonamble embayment of the surat basin is considered to be a closed basin and discharges into the alluvium of the barwon river between walgett and brewarrina fig 1 raiber et al 2022 and also the alluvium of the lower namoi iverach et al 2017 braaten and gates 2003 generalised the surface water groundwater interactions of the alluvial systems of western flowing rivers in new south wales nsw into four components reflecting basin geomorphology from upstream to downstream small upland streams are generally gaining from baseflow in fractured rock aquifers narrow alluvial valleys with highly connected shallow groundwater can be gaining or losing but are generally losing wide arid alluvial floodplains with deep groundwater disconnected from the river have losing conditions approaching the confluence with the murray barwon darling structural controls and finer alluvial material leads to shallow connected systems that can be gaining or losing but generally gaining the simplified conceptualisation of braaten and gates 2003 concurs with the very detailed work of macumber 1991 in the northern victorian catchments the same conceptualisation was used by parsons et al 2008 across the entire mdb who identified that the condamine and namoi alluvial systems consisted of two braaten and gates 2003 sequences as the downstream flow of groundwater was constricted by basement highs fig 1d changes in land use since european settlement have impacted upon the surface water groundwater interactions within the mdb land clearing for agriculture began nearly 200 years ago and resulted in widespread increases in groundwater recharge and consequently rising water tables mdbmc 1999 irrigation using surface water began in the late 19th century and expanded greatly throughout the 20th century with the expansion of engineering works such as dams and canals davidson 1988 leaching under irrigation areas also resulted in increased groundwater recharge and rising water tables rising water tables lead to water logging and salinisation that threatened much of the land and water resources of the southern mdb in the latter decades of the 20th century nlwra 2001 there have been 17 salt interception schemes installed to prevent saline groundwater from reaching the river fig 1d hart et al 2020 groundwater extraction for irrigation expanded from the 1960 s when the technology became economically viable and expanded again from 1995 when a cap was placed on surface water diversions nevill 2009 aquifers that are extensively used as a source of water for irrigation currently have falling groundwater levels as a result of extraction fu et al 2022 superimposed over the impacts on surface water groundwater interactions from water resources development is the impact of climate floods can rapidly raise the groundwater level through recharge across the floodplain macumber 1991 observed that the hinge point of losing to gaining reaches of the lower loddon river moved 20 km upstream after the 1973 1975 floods at the other extreme van dijk et al 2013 showed the groundwater storage depletion due to the millennium drought 2001 2009 this paper aims to investigate the changes in surface water groundwater interactions over the past half a century at a spatial and temporal scale that is commensurate with the river systems models that are used in water resources planning and management in the mdb i e source welsh et al 2013 iqqm simons et al 1996 and realm perera et al 2005 these river systems models are node link models in that water is routed from the upstream node through the link to the downstream node all inputs and outputs from the river are modelled at the scale of a link the stream reaches used for these links are generally between gauges and can vary in length from less than 1 km to many 100 s of kilometres the reaches used by the awra r model of the mdb dutta et al 2015 are used in this study the mdb is discretised into 492 stream reaches for a total length of 36 960 km and a median reach length of 53 km 3 methods the method used in this analysis is comparing the temporally varying groundwater level at bore locations close to the streams with the stream water level that is assumed to be temporally constant to determine if the gradient in water levels is toward the stream gaining or toward the groundwater losing jasechko et al 2021 the point classifications are then aggregated to the stream reach and by year to enable an analysis of the changes in surface water groundwater interactions in space and time throughout the murray darling basin 3 1 source data the point data used for the groundwater bores and levels was extracted from the national groundwater information system ngis bureau of meteorology 2019 at the time of download included data up until mid 2019 for each bore that had a water level record the average standing water level swl was calculated for each year for the 49 year period from 1 6 1971 to 31 5 2019 the natural surface level nsl and top of casing toc are inconsistently recorded in ngis in some cases there is high quality data that has been surveyed in other cases it has been recorded by gps however there are many bores where the location has been estimated from topographic maps the natural surface level estimated from a dem and no record for the top of casing for consistency in the analysis the location of the bore was as recorded in ngis but the natural surface level was extracted from lidar data where available geoscience australia 2015 and the 1 s dem where lidar was not available gallant et al 2011 where the top of casing and natural surface level were recorded in ngis the height of the pipe above land surface could be calculated from the difference from these elevations where the information on the height of the pipe above the land surface was missing the median value of all the bores that had this information was used 0 62 m the annual series of groundwater level gwl for each bore could then be calculated as the natural surface level plus the height of the pipe minus the standing water level fig 2 the groundwater level was then compared to the surface water elevation at the closest point on the stream network the stream network was as used by the awra r river system model dutta et al 2015 with the location of the streams taken from 1 250 k mapping geoscience australia 2006 the stream elevation at the closest point to each bore was taken from the lidar data where available geoscience australia 2015 and the 1 s dem where it was not available gallant et al 2011 the surface water elevation was taken as the minimum within a 100 m buffer distance from the point location on the stream network as the positional accuracy of the 1 250 k mapping was often not commensurate with the higher resolution data from the 5 m lidar dem this process has resulted in an assumption that the surface water elevation is constant in time as an average annual stage height the fluctuations at most gauges are less than 1 m which is less than the threshold used for classifying bores as predicting gaining or losing conditions described in the next section in a field study setting the ideal location for installing a bore would be within the stream bed so the water level can be measured inside and outside the pipe to determine the direction of flow however this is not practical for an analysis of historical data so an evaluation of the distance of the bores to the stream for inclusion in the analysis was necessary if bores are included too far away from the stream they may not be representative of the groundwater level below the stream and if the distance criteria for inclusion was too restrictive there may not be enough data points to complete the analysis 3 2 classification at point and reach scale at the point scale the bore was classified as losing for that year if the average groundwater level was more than 1 m below the surface water elevation at the closest point on the stream network the bore was classified as gaining for that year if the average groundwater level was more than 1 m above the surface water elevation if the groundwater level was within 1 m of the surface water elevation the bore was classified as unsure this 1 m threshold is an allowance for the accumulated errors from the estimation of gaining losing conditions this includes the errors associated with the assumption of the surface water elevation from the lidar survey the in many cases assumed height of the pipe above ground level and the measurement error associated with the standing water level measurement the point scale classification for each bore were aggregated to the river reach scale each bore was associated to a reach based on shortest distance to the stream network for each reach the percentage of bores predicting losing conditions was determined this gave each reach with at least one bore with a water level measurement for the year a value between 0 and 100 losing conditions note that a reach with 50 of bores predicting losing conditions for a given year does not necessarily have 50 of bores predicting gaining conditions for that year due to the third classification category of unsure 3 3 analysis of time series at the basin scale the number of reaches in each quintile of percentage of bores predicting losing conditions were aggregated for each year this allowed a comparison through time of the classification of losing reaches at the basin scale further analysis of these trends through time is possible through grouping reaches into clusters that behave similarly using k means clustering genolini and falissard 2010 each reach is assigned to a cluster that maximises the similarities between reaches within clusters while maximising the differences between clusters the greater the number of clusters the better the fit which leads to overfitting the optimum number of clusters was determined using the akaike information criterion corrected for small sample size aicc between 3 and 15 clusters were trialled two time periods were analysed to test whether the trends seen are cyclical or monotonic these periods were the 20 year period 1980 2000 and the 19 year period 2000 2019 these two periods were chosen as the first period was characterised by above average rainfall across the basin and the second period was characterised by drought the trend in the proportion of bores predicting losing conditions in each reach for each of these time periods was calculated using linear regression they were classified as having a significant trend if the trend was significant at p 0 05 the interpretation of these trends also depends on the starting point if a reach had 0 of bores predicting losing conditions at the start of the period with a significant increase in losing trend then at the reach scale it may be headed for a net change in direction of flow from gaining to losing conditions this would be quite different from a scenario where a reach had 80 of bores predicting losing conditions at the start of the period with a significantly increasing losing trend in this case there would be little difference seen at the reach scale that would impact the stream flow as it was already under losing conditions the reaches were also classified based on their mean over the 19 or 20 year period of the percentage of bores predicting losing conditions in each year always losing 100 of bores predicting losing conditions in every year mostly losing an average of more than two thirds of bores predicting losing conditions but less than 100 calculated over the annual series some losing an average of between one third and two thirds of bores predicting losing conditions calculated over the annual series mostly not losing an average of less than one third of bores predicting losing conditions but greater than 0 calculated over the annual series never losing 0 of bores predicting losing conditions in every year combining the classification of mean and trend resulted in 11 classes overall that could be compared across the two time periods 4 results 4 1 source data there was a total of 59 340 bores within the murray darling basin that had at least one water level measurement within the period 1 6 1970 to 31 5 2019 the majority of these are not observation bores and so do not have a regular time series of measurements these bores had a median distance of 5 6 km from the stream network fig 3 of these 59 340 bores there were 13 603 that were covered by the lidar survey these would be the bores closest to the streams as the lidar surveys were conducted over the floodplains the bores closest to the stream network will be the ones with the most reliable information for this analysis the further from the stream the more chance there is that the groundwater level measurement is affected by local stresses such as pumping there were only 1527 bores within 100 m of the stream network 3 bores for each of 492 reaches as this was not enough to complete the analysis a larger buffer distance was required for including bores buffer distances of 1 km 2 km 5 km and 10 km were trialled for inclusion of bores in the analysis this resulted in the inclusion of 9 779 bores for the 1 km buffer distance 15 290 for 2 km 27 829 for 5 km and 38 984 for 10 km as each bore did not have observed water levels each year the number of bores included in each year was substantially less than the total fig 4 with a buffer distance of 1 km about half of the 492 stream reaches would have been included as a maximum in any year and just over a third as an average across the 49 years of analysis fig 4 if the buffer distance was increased to 10 km then at a maximum two thirds of stream reaches would have been included with an average of over a half it was decided that using a 1 km buffer would not include enough stream reaches to achieve the objectives of the study but using a 10 km buffer would introduce too much uncertainty due the distance over which the water levels were compared a pragmatic choice was made to use a 5 km buffer distance for including bores in the study this included 308 stream reaches as a maximum in 2005 2006 and 240 as an average across the 49 years an analysis of the implications of this choice is shown in the discussion section of this paper 4 2 classification at point and reach scale for each year of analysis the average groundwater level was calculated for each bore and compared to the surface water level at the closest point on the stream network each point was classified as either gaining unsure or losing 3 examples shown in fig 5 a c e these points were then aggregated to the reach scale as the proportion of bores predicting losing conditions 3 examples shown in fig 5b d f at the broad scale the examples shown in fig 5 for the years 1980 81 2000 01 and 2018 19 show similar spatial patterns the headwater catchments in the south east are generally gaining further downstream on the plains are dominantly losing conditions and then the lowest reaches become gaining again consistent with the generalisations of braaten and gates 2003 as all the bores do not have measurements of groundwater level for every year analysed a detailed comparison of the results between years can be confounded by inconsistent data an example is in the warrego river where different bores have water level observations in each of the example years that results in different quintile classifications at the reach scale 4 3 analysis of time series at the basin scale the number of bores predicting losing conditions in the nearest stream steadily increased from 735 in 1970 71 to a maximum of 4020 in 2002 03 and then fell to 1105 in 2018 19 fig 6 a the number of bores predicting gaining conditions followed a similar pattern increasing from 277 bores in 1970 71 to a maximum of 2439 in 2005 06 before falling to 218 bores in 2018 19 this pattern is due to the number of bores measured in a given year rather than changing conditions through time when looking at the proportion of bores predicting losing conditions there is a decrease from 60 in 1970 71 to a minimum of 48 in 1995 96 before increasing through the period of the millennium drought until the 2010 11 floods these floods caused a temporary decrease in the proportion of bores predicting losing conditions before it increased after 2015 16 to a maximum of 78 in 2018 19 fig 6b the number of bores predicting gaining conditions increased from 23 in 1970 71 to a maximum of 33 in 2005 06 and then fell rapidly after the 2010 11 floods to 15 in 2018 19 for the decade from 1995 96 to 2005 06 both the proportion of bores predicting gaining and losing conditions were increasing this apparent contradiction can be explained by the proportion of bores that were classified as unsure decreased from a maximum of 20 in 1993 94 to a minimum of 6 in 2018 19 the number of reaches where the proportion of bores predicting losing conditions was greater than 80 increased from 61 in 1970 71 to a maximum of 157 in 2017 18 the number of reaches where the proportion of bores predicting losing conditions was less than 20 increased from 15 in 1970 71 to a maximum of 66 in 1995 96 and 2000 01 before decreasing to 19 in 2017 18 fig 6c similar to the analysis of the number of bores predicting losing conditions these trends are confounded by the changing number of reaches which have data through time the proportion of reaches where the proportion of bores predicting losing conditions is greater than 80 decreases from 56 in 1970 71 to a minimum of 40 in 1993 94 before increasing to a maximum of 66 of reaches in 2018 19 fig 6d the proportion of reaches where the proportion of bores predicting losing conditions is less than 20 shows the opposite trend through time it starts at 14 in 1970 71 rising to a maximum of 25 in 1995 96 before falling to a minimum of 9 in 2018 19 a temporary decrease in the proportion of reaches with greater than 80 of bores predicting losing conditions is evident after the floods that ended the millennium drought from 2010 to 2012 but this is not mirrored by an increase in the proportion of reaches with less than 20 of bores predicting losing conditions most of the gain in proportion of reaches is in the 40 60 of bores predicting losing conditions category following the basin scale trends groups of reaches can be identified that behave similarly using k means clustering the optimum number of clusters was identified to be seven using the akaike information criterion corrected for small sample size aicc fig 7 a cluster 1 contains about a third of all reaches and has almost all bores predicting losing conditions for all years fig 7b these reaches are mostly located in the lowlands with wide alluvial systems fig 7c cluster 2 contains approximately a fifth of reaches and has few bores predicting losing conditions although with an increasing proportion since 2000 the majority of cluster 2 reaches are in the headwater catchments with some in the lower reaches of the murray river and near the confluence of the murray river and billabong creek the mean for cluster 3 had less than half of bores predicting losing conditions for the majority of the time under study until the most recent few years where more than half of bores were predicting losing conditions most reaches in cluster 3 are in the southern basin but vary in topographic position from the headwater catchments to the lower reaches cluster 4 has a monotonically increasing trend for the proportion of bores predicting losing conditions from less than 40 at the start of the study period to above 70 by the end cluster 5 has most bores at the start of the study period predicting losing conditions but decreases until the early 2000 s before increasing again to the end of the study period clusters 6 and 7 show similar trends in that they have little trend until the mid 1990 s and then increase in their proportion of bores predicting losing conditions the difference between them is that cluster 6 has a higher proportion of bores predicting losing conditions than cluster 7 the analysis at the basin scale fig 6 and the cluster analysis fig 7 showed that in most cases something happened in the trends seen between about the mid 1990 s and mid 2000 s to investigate further an analysis was conducted for two time periods to provide a comparison the 20 year period 1980 2000 and the 19 year period 2000 2019 at the reach scale the time series of the proportion of bores predicting losing conditions was classified based on the mean of the proportion of bores predicting losing conditions and the trend through time an example for each category is shown in fig 8 for the 2000 2019 period some of these reaches are easy to interpret as there are many bores analysed each year and so there is confidence in the results the best examples shown have over 100 bores with water level measurements per year for the 2000 2019 period fig 8c f where there are few bores with measured water levels and the number and presumably location of bores changes through time the analysis of means and trends can become less certain the worst examples fig 8e h show a step change in the number of bores with water level measurements through time and a corresponding change in the mean and trend of the proportion of bores predicting losing conditions comparing the means and trends in the proportion of bores predicting losing conditions for the 20 year period from 1980 to 2000 fig 9 to the 19 year period from 2000 to 2019 fig 10 there are almost the same number of always losing 95 cf 93 and never losing reaches both 40 the largest difference is in the mostly losing category with the 1980 2000 period having 91 reaches and this increases to 109 reaches in the 2000 2019 period there is a corresponding loss of reaches in the some losing 69 cf 58 and mostly not losing 64 cf 57 categories over this time a comparison between the northern and southern basin for the combined never losing and mostly not losing categories was consistent between time periods with 88 in the southern basin for 1980 2000 and 87 for 2000 2019 the combined always losing and mostly losing categories showed a difference between the northern and southern basins with 59 in the northern basin for the 1980 2000 period decreasing to 54 for the 2000 2019 period there was an increase in the number of reaches with a statistically significant increase in the proportion of bores predicting losing conditions 60 in 1980 2000 cf 66 in 2000 2019 although the magnitude was similar between periods the geographical spread was different with 79 of the increasing reaches in the southern basin in 1980 2000 and 92 in 2000 2019 there was a decrease in the number of reaches with a statistically significant decreasing trend 39 cf 18 with 82 of them in the southern basin in 1980 2000 decreasing to 67 for the period 2000 2019 the majority of reaches in both time periods had no statistically significant trend 260 in 1980 2000 and 273 in 2000 2019 when each reach is directly compared between time periods the majority 72 are within the same quintile of proportion of bores predicting losing conditions fig 11 a of those that changed quintiles 65 moved to a higher proportion of bores predicting losing conditions and 35 moved to a lower proportion quintile the trends show a different pattern with only 21 of those reaches with a statistically significant trend for the period 1980 2000 having the same trend for the period 2000 2019 fig 11b there were actually more reaches 22 with the opposing trend in 2000 2019 compared to 1980 2000 these were mainly switching from a decreasing trend to an increasing trend in the proportion of bores predicting losing conditions 5 discussion 5 1 analysis of the method the method applied here has produced results that are consistent spatially with the conceptual understanding of the basin braaten and gates 2003 and consistent temporally with known stressors on the system such as the salinity problems in the latter decades of last century mdbmc 1999 and the recent droughts van dijk et al 2013 however at a more localised scale there can be confounding impacts on the results where the number of bores within a reach included in the analysis changes through time the mean and trends calculated from them can also change in response to the bores analysed rather than a stress on the system e g fig 8e h this can possibly lead to unrepresentative results as well as the temporal availability of bores the spatial arrangement of the bores available can also cause problems when a reach contains both gaining and losing sections the spatial arrangement of the bores may produce a result that is not representative of the reach as a whole the most obvious reach is upstream of river murray at murray bridge that also includes the small tributary of marne river flowing in from the mount lofty ranges in this reach there are more bores along the marne river that are classified as losing than bores along the main trunk of the murray river that are classified as gaining this is causing the reach to be classified as mostly losing in fig 9 and some losing in fig 10 when if the bores were spread evenly along the length of the reach it would have been classified as mostly not losing this is an artefact of the catchment discretisation as used by the awra r model dutta et al 2015 rather than this section of the lower murray river actually being majority losing central to the method as applied was the criteria for including bores in the analysis the choice to use a 5 km buffer distance around the stream network for inclusion of bores in the analysis was a pragmatic choice around data availability rather than a technically sound decision to illustrate why this decision was made and its implications the mean and trends in the proportion of bores predicting losing conditions over the period 2000 2019 shown in fig 10 was recreated for a higher quality subset of the original dataset fig 12 the criteria for inclusion in this higher quality dataset was i bores were only included within 1 km of the stream network ii there was a minimum of 5 bores within a reach and iii the mean and trend over the 19 years was only calculated when there were a minimum of 5 years of data when the proportion of bores predicting losing conditions calculation was performed there were only 170 reaches had data available compared to 306 in fig 10 a visual comparison of fig 10 and fig 12 suggests that the patterns in the classification are similar between the 5 km buffer distance dataset and the higher quality 1 km subset this can be further seen in a difference map fig 13 a quantitative comparison shows that the mean category was the same between the 1 km and 5 km analyses in 75 of stream reaches that were present in the 1 km dataset fig 14 a but this fell to 66 for the trend classification fig 14b the most frequent response from the 1 km dataset to the categories in the 5 km dataset was always the same for the mean but the 1 km dataset tended to have the no trend category represented more often that the 5 km dataset of those reaches that were not classified as the same one reach in the higher quality 1 km dataset had a mean that was more than one quintile different and only three reaches showed an opposing trend this suggests that the methodological choice to include bores within a 5 km buffer rather than the higher quality data with a 1 km buffer had little effect at the reach scale but had the major advantage at the basin scale in that an extra 139 reaches were included the data analysis cannot be extended any further with the data available at this scale to fill in the gaps in space and time will require some form of modelling some examples that have been used elsewhere include characterisation based on geomorphological features ransley et al 2007 numerical groundwater modelling baratelli et al 2016 or upscaling observations using other catchment attributes yang et al 2019 5 2 losing connected versus losing disconnected we have not classified the surface water groundwater interactions of the stream reaches of the mdb further from gaining and losing to the distinction between losing connected and losing disconnected because doing so is not simple at this scale with the information that is available brunner et al 2009 presented the conditions necessary for disconnection to occur which included the presence of a clogging layer and the ratio of its hydraulic conductivity to that of the aquifer below such information are not available at the scale of the mdb taylor et al 2013 lamontagne et al 2014 investigated 12 sites in the mdb for their connection status and found that the sites with losing connected reaches had a groundwater level less than 1 m below the riverbed and losing disconnected reaches had a groundwater level between 6 and 25 m below the riverbed the analysis in this paper is comparing the groundwater level with the surface water elevation without information on the riverbed level parsons et al 2008 were faced with the same lack of information in their classification of surface water groundwater interactions in the mdb but made the simple assessment that losing disconnected streams had a groundwater level more than 10 m below the surface water elevation shown as maximum losing in fig 1d the 31 reaches identified by parsons et al 2008 as being losing disconnected also have the groundwater more than 10 m below the surface water in our analysis for both time periods analysed fig 15 fig 15 shows that the period 1980 2000 had 70 reaches 19 with a groundwater level greater than 10 m below the surface water elevation and that this had expanded to 82 23 reaches for the period 2000 2019 the same trend is seen for the 5 to 10 m class where the number of reaches increased from 65 18 to 76 21 between the 1980 2000 and 2000 2019 time periods this is consistent with the analysis of fu et al 2022 who showed that the groundwater level has been falling in the major alluvial aquifers of the mdb 5 3 comparison to previous studies of surface water groundwater interactions in the mdb the most direct comparison of the results found in this study fig 10 is to that of parsons et al 2008 fig 1d both of these results generally follow the conceptualisation of braaten and gates 2003 gaining conditions are seen in the headwaters losing conditions are seen through the middle reaches where the alluvium is either constricted in narrow valleys or spread out over wide floodplains and then becoming gaining again at the end of the valley due to geological constrictions these generalisations have also been seen in whole of catchment surface water groundwater interactions investigations of the namoi ivkovic 2009 and the border rivers ransley et al 2007 a quantitative comparison is not possible between these studies due to the different reach lengths and the different time periods used for data acquisition the major advance in the current work is that it is conducted at the same reach scale as used by the river system models and so gives information on many more reaches than previous studies and also has the significant benefit of being conducted as a time series rather than as a single point in time there have been many possibly 100 s detailed small scale studies investigating surface water groundwater interactions in the murray darling basin over the past few decades and it is beyond the scope of this paper to review them all a direct comparison with the results of this paper is difficult due to the scale differences between the reaches used here up to 500 km and the small scale detailed work from 10 s to 1000 s m table 1 provides a comparison between a selection of detailed small scale studies around the mdb and the regional reach scale results shown in fig 10 these show that there is general consistency between the results here and the small scale studies even seemingly inconsistent results such as lamontagne et al 2014 showing a gaining section in the dumaresq river compared to the mostly losing classification here can be consistent due to the scales involved a classification of mostly losing means that more than two thirds of bores in that reach predict losing conditions which also mean that up to one third of bores predict gaining conditions 5 4 implications for management of the water resources the results of this analysis have shown that there have been changes in the surface water groundwater interactions across the mdb over the past few decades there was an increasing trend in gaining conditions up until the millennium drought and then an increasing trend in losing conditions thereafter the increasing trend in gaining conditions is coincident with the increase in areas affected by salinity due to rising water tables and the beginning of the increase in losing conditions is coincident with the millennium drought but the losing trend continued even after the drought broke peterson et al 2021 showed that many catchments in victoria have not recovered their rainfall runoff relationship from prior to the millennium drought suggesting a change from a high runoff for a given rainfall stable state to low runoff for a given rainfall stable state hughes et al 2012 showed a similar relationship in western australia but was able to link the changed rainfall runoff relationship to groundwater levels the trends seen in the change in surface water groundwater interactions in this analysis across the entire mdb seem to not be solely caused by land use change i e salinity issues climate droughts then floods or development groundwater extraction it is likely a combination of all three and perhaps other drivers as well with the contribution of each driver a possible subject for future studies recent work in the mdb has shown the magnitude of streamflow depletion that can be expected due to existing groundwater extraction walker et al 2020 and that a future climate is likely to lead to reduced groundwater levels further exacerbating the current trend seen for an increasing proportion of losing streams walker et al 2021 it is not known what effects changes to groundwater interactions with streams will have on groundwater dependent ecosystems gde s boulton and hancock 2006 what is clear is that we will need better predictions of low flows from our hydrological models that are capable of modelling these changes in surface water groundwater interactions if we are to understand and predict changes in gde s 5 5 recommendations for improving the conceptualisation of river systems models the node link river systems models used for planning and managing the water resources of the mdb i e source welsh et al 2013 iqqm simons et al 1996 and realm perera et al 2005 do not explicitly simulate the surface water groundwater interactions but have this exchange flux bundled up with all other errors in an unallocated loss term that is a function of the flow kim et al 2022 the analysis of braaten and gates 2003 showed that the exchange flux between surface water and groundwater is of the same order of magnitude as the gauge errors for two major rivers in the mdb namoi and murrrumbidgee this would suggest that changes in the surface water groundwater exchange flux would not be large enough to cause a material change in the simulated flows for the models intended purpose of simulating the total flows accurately outputs of these simulated flows from the river systems models are also used as an input to ecological models where the low flows become more important rosenfeld 2017 during low flows the flow in the river gauge errors and the surface water groundwater exchange flux are all of the same order of magnitude and therefore changes in the surface water groundwater exchange flux particularly changing from gaining to losing or vice versa become important there are many alternate model structures that are capable of modelling this change in surface water groundwater exchange flux rassam and werner 2008 such as fully integrated hydrological models therrien et al 2006 or coupled river and groundwater models valerio et al 2010 but these do not meet the needs of the river systems models where the water sharing rules and dam releases are a large part of the model s capability a solution to this problem was built into the source model rassam 2011 but has not been adopted by river modellers the solution of rassam 2011 is using analytical equations of stream flow depletion in response to groundwater extraction it is tracking the depletion through time for each groundwater extraction point this solution requires some knowledge of groundwater processes that river modellers may not possess access to groundwater extraction and aquifer property data that may not exist and considerably slows down the model execution times a simpler solution is needed groundwater levels integrate all stressors climate extraction etc upon the system relevant to that point in space and time by making the surface water groundwater exchange flux in the river model dependent upon the surface water level a function of flow and the groundwater level then the changes in the surface water groundwater exchange flux through time can be explicitly represented within the river systems model without the complexity or computational penalty of actually modelling the groundwater system this is an area of research that needs to be explored further 6 conclusions this study has achieved its three aims in investigating the surface water groundwater interactions in the murray darling basin 1 using 59 340 bores with groundwater level measurements over the period 1 6 1949 to 31 5 2019 a point scale classification of gaining and losing conditions was conducted on an annual time step for 49 years these point scale results were aggregated to the stream reach scale as the proportion of bores predicting losing conditions there were 240 reaches on average across the 49 years that had a classification for the proportion of bores predicting losing conditions the maximum was 308 reaches in 2005 2006 these data are available from crosbie et al 2022 this is a significant advance on the previous studies of surface water groundwater interactions in the mdb that were limited in both spatial and temporal extent preventing a whole of basin analysis 2 at the whole of basin scale there was a trend in the latter decades of last century toward an increase in gaining conditions this is consistent with the salinity issues at the time due to rising water tables in the most recent 19 years analysed there has been an increasing trend in losing conditions over this period more than double the number of reaches were classed as always or mostly losing n 198 compared to never or mostly not losing n 94 over this 19 year period the majority of reaches did not have a statistically significant trend in the proportion of bores predicting losing conditions n 265 but there were more reaches that had an increasing trend n 65 compared to those with a decreasing trend n 18 3 the loss functions generally used in our river systems models have no knowledge of surface water groundwater interactions and therefore cannot simulate changes in the magnitude or direction of groundwater exchange fluxes these models are calibrated to reproduce the total annual flows which are dominated by the high flow events and therefore little weight is given to the low flows the ability to reproduce the observed changes in surface water groundwater interactions is most important in simulating low flows further research is needed on the most appropriate way to implement a dependency of the loss term on groundwater level in the simplest possible way credit authorship contribution statement russell crosbie conceptualization methodology writing original draft bill wang data curation shaun kim writing review editing cherry mateo writing review editing jai vaze conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the murray darling basin authority through the murray darling water and environment research program md werp further information is available at https getinvolved mdba gov au murray darling water and environment research program the authors wish to thank the reviewers and editors of the manuscript whose constructive criticisms resulted in improvements throughout the paper research data for this article the dataset presented in this paper is available from crosbie et al 2022 
1776,the murray darling basin is australia s most stressed water resource and most contentious in the sharing of those water resources our hydrological models used for water management have no information on the surface water groundwater interactions within a reach and so implicitly assume that their relationship is stationary this study seeks to test this assumption by looking at the direction of surface water groundwater exchanges and how they have changed over the 49 year period from 1970 to 2019 at the whole of basin scale 492 river reaches this was achieved by comparing the groundwater level at 59 340 bores to the elevation of the surface water at the closest point on the stream network each point evaluated on the stream network was classified as either gaining water from groundwater or losing water to groundwater at an annual scale these point scale observations were aggregated to the reach scale as used in river systems models for analysis of trends though time the proportion of bores predicting losing conditions was falling through the 1970 s and 1980 s before reaching a minimum of 48 in 1995 96 from this period the proportion of bores predicting losing conditions increased to a maximum at the end of the analysis period of 78 in 2018 19 this finding at the point scale is replicated at the reach scale the reaches with greater than 80 of bores predicting losing conditions had a minimum in 1993 94 at 40 and increased to 66 at the end of the analysis period these observations are consistent with the management challenges of the time in the 1980 s and 1990 s gaining conditions leading to increasing salt loads in the streams caused by groundwater discharge were a major problem for downstream water users since the turn of the century drought and water scarcity have been the major challenges with falling groundwater levels leading to increases in losing conditions this study has shown that the surface water groundwater interactions are not stationary and have been changing through time with many reaches having changed direction from gaining to losing throughout the analysis period this is highlighting a deficiency in our hydrological models that cannot replicate this change in flux direction and therefore leading to errors in predicting low flows keywords surface water groundwater interactions river systems models low flow prediction murray darling basin australia data availability the data used is cited in the text and is available from https doi org 10 25919 x453 3g36 1 introduction many of our commonly used hydrological models are conceptualised as gaining streams i e groundwater flows into the stream this stems from the early conceptual understanding of catchment hydrology such as the partial contributing area of dunne and black 1970 or the nested groundwater flow paths of tóth 1963 these hydrological models have successfully been used for decades in water resources planning where the total volume of flow is the most important variable to reproduce but with increased emphasis being placed on low flows for environmental purposes hallouin et al 2020 nicolle et al 2014 these models have been found to be inadequate in some circumstances it has been suggested that the one way coupling of surface water to groundwater results in the poor performance in reproducing low flows jachens et al 2021 because such representation cannot adequately simulate losing conditions surface water groundwater interactions are generally classified into gaining and losing streams where gaining streams are the recipients of groundwater discharge and losing streams contribute to groundwater recharge winter et al 1998 losing streams can be further classified as losing connected or losing disconnected in the losing disconnected case there is an unsaturated zone below the stream bed and the rate of loss from the stream is independent of the depth of groundwater whereas in a losing connected case there is a saturated connection between the surface water and the groundwater and the rate of loss is dependent on the difference in elevation between the surface water and the groundwater brunner et al 2011 winter et al 1998 there are many methods of classifying a stream reach as gaining or losing cook 2015 kalbus et al 2006 at a small scale these can include seepage meters lee 1977 streambed piezometry baxter et al 2003 and heat as a tracer constantz 1998 at larger scales the methods include environmental tracers cook et al 2006 drury et al 1984 differential gauging cey et al 1998 and head gradients freeze and cherry 1979 changes in the surface water groundwater interactions can occur over scales of metres in hyporheic exchange zones harvey and bencala 1993 or many 100 s of kilometres in response to changes in geomorphology and geology konrad 2006 similarly changes in the direction of surface water groundwater exchanges can occur very quickly due to flooding cui et al 2018 or over decades in response to groundwater extraction sophocleous 2000 most of the studies that have enhanced our understanding of surface water groundwater interactions have occurred at a small scale cook 2015 at the regional to continental scale there are few examples some rely solely on surface water data whilst others use both surface and groundwater data differential gauging the difference between upstream and downstream flow measurements has been used in new zealand as a training dataset to upscale to the entire country using machine learning trained on many physical properties of the stream reaches yang et al 2019 analysis of flow duration curves and hydrographs has been used to map gaining and losing reaches in the namoi catchment in australia ivkovic 2009 covering the namoi and the rest of the murray darling basin parsons et al 2008 interpolated groundwater level observations to a water table surface and compared to the surface water elevations to map gaining and losing reaches the largest study has been in the continental united states where the observed groundwater levels were compared to the surface water level at the nearest point in the stream network jasechko et al 2021 these studies have all been of a limited period of time parsons et al 2008 or a long term average ivkovic 2009 jasechko et al 2021 yang et al 2019 none of them have considered a time series or how surface water groundwater interactions have changed through time being able to observe and map the changes in gaining and losing reaches within a basin is the first step to understand the future conditions and provide the process understanding to improve the low flow prediction capability of our hydrological models the aims of this study are to 1 map the gaining and losing reaches of the streams of the murray darling basin used by the river systems models at an annual time step 2 analyse the changes in surface water groundwater interactions through time and 3 to make recommendations on improving the structure of our hydrological models to simulate changes in surface water groundwater interactions 2 study region the murray darling basin mdb is australia s most important river system accounting for 62 of australia s water use abs 2022 and producing 42 of australia s agricultural output abs 2021 for the 2020 21 financial year it covers 1 06x106 km2 and is home to 2 million people and supplies water to over another million people the surface water resources are divided into 19 catchments fig 1 a for administration under the basin plan australian government 2012 and are further grouped into the northern basin catchments flowing into the darling river and the southern basin the catchments flowing into the murray river fig 1c all these rivers have their highest rainfall in the headwaters fig 1b with the climate becoming progressively more arid heading downstream fig 1c most of the major rivers are highly regulated with large dams in their headwaters increasing the reliability of supply of water to the irrigation industry downstream the majority of groundwater use occurs in eight alluvial water sources goulburn murray sedimentary plains vic and the gwydir nsw lachlan nsw macquarie nsw murray nsw murrumbidgee nsw namoi nsw and condamine qld alluvia associated with the major rivers of the basin fu et al 2022 there are also two major sedimentary basins that contain regional groundwater flow paths the murray geological basin in the south and the great artesian basin in the north fig 1d the murray geological basin is a closed basin the flow of groundwater is toward the depocenter in the centre west of the basin around renmark with the only outflow through discharge into the murray river or evapotranspiration evans and kellett 1989 discharge occurs via evapotranspiration in a series of saline lakes where groundwater flow is constricted by basement highs and also discharge to the murray river downstream of mallee cliffs 30 km east of mildura brown 1989 underlying the northern part of the mdb is a portion of the surat and eromanga basins that form part of the great artesian basin the surat basin has its intake beds along the great dividing range but there is little interaction with the rivers of the mdb as the major aquifers are buried below thick aquitards ransley et al 2015 the coonamble embayment of the surat basin is considered to be a closed basin and discharges into the alluvium of the barwon river between walgett and brewarrina fig 1 raiber et al 2022 and also the alluvium of the lower namoi iverach et al 2017 braaten and gates 2003 generalised the surface water groundwater interactions of the alluvial systems of western flowing rivers in new south wales nsw into four components reflecting basin geomorphology from upstream to downstream small upland streams are generally gaining from baseflow in fractured rock aquifers narrow alluvial valleys with highly connected shallow groundwater can be gaining or losing but are generally losing wide arid alluvial floodplains with deep groundwater disconnected from the river have losing conditions approaching the confluence with the murray barwon darling structural controls and finer alluvial material leads to shallow connected systems that can be gaining or losing but generally gaining the simplified conceptualisation of braaten and gates 2003 concurs with the very detailed work of macumber 1991 in the northern victorian catchments the same conceptualisation was used by parsons et al 2008 across the entire mdb who identified that the condamine and namoi alluvial systems consisted of two braaten and gates 2003 sequences as the downstream flow of groundwater was constricted by basement highs fig 1d changes in land use since european settlement have impacted upon the surface water groundwater interactions within the mdb land clearing for agriculture began nearly 200 years ago and resulted in widespread increases in groundwater recharge and consequently rising water tables mdbmc 1999 irrigation using surface water began in the late 19th century and expanded greatly throughout the 20th century with the expansion of engineering works such as dams and canals davidson 1988 leaching under irrigation areas also resulted in increased groundwater recharge and rising water tables rising water tables lead to water logging and salinisation that threatened much of the land and water resources of the southern mdb in the latter decades of the 20th century nlwra 2001 there have been 17 salt interception schemes installed to prevent saline groundwater from reaching the river fig 1d hart et al 2020 groundwater extraction for irrigation expanded from the 1960 s when the technology became economically viable and expanded again from 1995 when a cap was placed on surface water diversions nevill 2009 aquifers that are extensively used as a source of water for irrigation currently have falling groundwater levels as a result of extraction fu et al 2022 superimposed over the impacts on surface water groundwater interactions from water resources development is the impact of climate floods can rapidly raise the groundwater level through recharge across the floodplain macumber 1991 observed that the hinge point of losing to gaining reaches of the lower loddon river moved 20 km upstream after the 1973 1975 floods at the other extreme van dijk et al 2013 showed the groundwater storage depletion due to the millennium drought 2001 2009 this paper aims to investigate the changes in surface water groundwater interactions over the past half a century at a spatial and temporal scale that is commensurate with the river systems models that are used in water resources planning and management in the mdb i e source welsh et al 2013 iqqm simons et al 1996 and realm perera et al 2005 these river systems models are node link models in that water is routed from the upstream node through the link to the downstream node all inputs and outputs from the river are modelled at the scale of a link the stream reaches used for these links are generally between gauges and can vary in length from less than 1 km to many 100 s of kilometres the reaches used by the awra r model of the mdb dutta et al 2015 are used in this study the mdb is discretised into 492 stream reaches for a total length of 36 960 km and a median reach length of 53 km 3 methods the method used in this analysis is comparing the temporally varying groundwater level at bore locations close to the streams with the stream water level that is assumed to be temporally constant to determine if the gradient in water levels is toward the stream gaining or toward the groundwater losing jasechko et al 2021 the point classifications are then aggregated to the stream reach and by year to enable an analysis of the changes in surface water groundwater interactions in space and time throughout the murray darling basin 3 1 source data the point data used for the groundwater bores and levels was extracted from the national groundwater information system ngis bureau of meteorology 2019 at the time of download included data up until mid 2019 for each bore that had a water level record the average standing water level swl was calculated for each year for the 49 year period from 1 6 1971 to 31 5 2019 the natural surface level nsl and top of casing toc are inconsistently recorded in ngis in some cases there is high quality data that has been surveyed in other cases it has been recorded by gps however there are many bores where the location has been estimated from topographic maps the natural surface level estimated from a dem and no record for the top of casing for consistency in the analysis the location of the bore was as recorded in ngis but the natural surface level was extracted from lidar data where available geoscience australia 2015 and the 1 s dem where lidar was not available gallant et al 2011 where the top of casing and natural surface level were recorded in ngis the height of the pipe above land surface could be calculated from the difference from these elevations where the information on the height of the pipe above the land surface was missing the median value of all the bores that had this information was used 0 62 m the annual series of groundwater level gwl for each bore could then be calculated as the natural surface level plus the height of the pipe minus the standing water level fig 2 the groundwater level was then compared to the surface water elevation at the closest point on the stream network the stream network was as used by the awra r river system model dutta et al 2015 with the location of the streams taken from 1 250 k mapping geoscience australia 2006 the stream elevation at the closest point to each bore was taken from the lidar data where available geoscience australia 2015 and the 1 s dem where it was not available gallant et al 2011 the surface water elevation was taken as the minimum within a 100 m buffer distance from the point location on the stream network as the positional accuracy of the 1 250 k mapping was often not commensurate with the higher resolution data from the 5 m lidar dem this process has resulted in an assumption that the surface water elevation is constant in time as an average annual stage height the fluctuations at most gauges are less than 1 m which is less than the threshold used for classifying bores as predicting gaining or losing conditions described in the next section in a field study setting the ideal location for installing a bore would be within the stream bed so the water level can be measured inside and outside the pipe to determine the direction of flow however this is not practical for an analysis of historical data so an evaluation of the distance of the bores to the stream for inclusion in the analysis was necessary if bores are included too far away from the stream they may not be representative of the groundwater level below the stream and if the distance criteria for inclusion was too restrictive there may not be enough data points to complete the analysis 3 2 classification at point and reach scale at the point scale the bore was classified as losing for that year if the average groundwater level was more than 1 m below the surface water elevation at the closest point on the stream network the bore was classified as gaining for that year if the average groundwater level was more than 1 m above the surface water elevation if the groundwater level was within 1 m of the surface water elevation the bore was classified as unsure this 1 m threshold is an allowance for the accumulated errors from the estimation of gaining losing conditions this includes the errors associated with the assumption of the surface water elevation from the lidar survey the in many cases assumed height of the pipe above ground level and the measurement error associated with the standing water level measurement the point scale classification for each bore were aggregated to the river reach scale each bore was associated to a reach based on shortest distance to the stream network for each reach the percentage of bores predicting losing conditions was determined this gave each reach with at least one bore with a water level measurement for the year a value between 0 and 100 losing conditions note that a reach with 50 of bores predicting losing conditions for a given year does not necessarily have 50 of bores predicting gaining conditions for that year due to the third classification category of unsure 3 3 analysis of time series at the basin scale the number of reaches in each quintile of percentage of bores predicting losing conditions were aggregated for each year this allowed a comparison through time of the classification of losing reaches at the basin scale further analysis of these trends through time is possible through grouping reaches into clusters that behave similarly using k means clustering genolini and falissard 2010 each reach is assigned to a cluster that maximises the similarities between reaches within clusters while maximising the differences between clusters the greater the number of clusters the better the fit which leads to overfitting the optimum number of clusters was determined using the akaike information criterion corrected for small sample size aicc between 3 and 15 clusters were trialled two time periods were analysed to test whether the trends seen are cyclical or monotonic these periods were the 20 year period 1980 2000 and the 19 year period 2000 2019 these two periods were chosen as the first period was characterised by above average rainfall across the basin and the second period was characterised by drought the trend in the proportion of bores predicting losing conditions in each reach for each of these time periods was calculated using linear regression they were classified as having a significant trend if the trend was significant at p 0 05 the interpretation of these trends also depends on the starting point if a reach had 0 of bores predicting losing conditions at the start of the period with a significant increase in losing trend then at the reach scale it may be headed for a net change in direction of flow from gaining to losing conditions this would be quite different from a scenario where a reach had 80 of bores predicting losing conditions at the start of the period with a significantly increasing losing trend in this case there would be little difference seen at the reach scale that would impact the stream flow as it was already under losing conditions the reaches were also classified based on their mean over the 19 or 20 year period of the percentage of bores predicting losing conditions in each year always losing 100 of bores predicting losing conditions in every year mostly losing an average of more than two thirds of bores predicting losing conditions but less than 100 calculated over the annual series some losing an average of between one third and two thirds of bores predicting losing conditions calculated over the annual series mostly not losing an average of less than one third of bores predicting losing conditions but greater than 0 calculated over the annual series never losing 0 of bores predicting losing conditions in every year combining the classification of mean and trend resulted in 11 classes overall that could be compared across the two time periods 4 results 4 1 source data there was a total of 59 340 bores within the murray darling basin that had at least one water level measurement within the period 1 6 1970 to 31 5 2019 the majority of these are not observation bores and so do not have a regular time series of measurements these bores had a median distance of 5 6 km from the stream network fig 3 of these 59 340 bores there were 13 603 that were covered by the lidar survey these would be the bores closest to the streams as the lidar surveys were conducted over the floodplains the bores closest to the stream network will be the ones with the most reliable information for this analysis the further from the stream the more chance there is that the groundwater level measurement is affected by local stresses such as pumping there were only 1527 bores within 100 m of the stream network 3 bores for each of 492 reaches as this was not enough to complete the analysis a larger buffer distance was required for including bores buffer distances of 1 km 2 km 5 km and 10 km were trialled for inclusion of bores in the analysis this resulted in the inclusion of 9 779 bores for the 1 km buffer distance 15 290 for 2 km 27 829 for 5 km and 38 984 for 10 km as each bore did not have observed water levels each year the number of bores included in each year was substantially less than the total fig 4 with a buffer distance of 1 km about half of the 492 stream reaches would have been included as a maximum in any year and just over a third as an average across the 49 years of analysis fig 4 if the buffer distance was increased to 10 km then at a maximum two thirds of stream reaches would have been included with an average of over a half it was decided that using a 1 km buffer would not include enough stream reaches to achieve the objectives of the study but using a 10 km buffer would introduce too much uncertainty due the distance over which the water levels were compared a pragmatic choice was made to use a 5 km buffer distance for including bores in the study this included 308 stream reaches as a maximum in 2005 2006 and 240 as an average across the 49 years an analysis of the implications of this choice is shown in the discussion section of this paper 4 2 classification at point and reach scale for each year of analysis the average groundwater level was calculated for each bore and compared to the surface water level at the closest point on the stream network each point was classified as either gaining unsure or losing 3 examples shown in fig 5 a c e these points were then aggregated to the reach scale as the proportion of bores predicting losing conditions 3 examples shown in fig 5b d f at the broad scale the examples shown in fig 5 for the years 1980 81 2000 01 and 2018 19 show similar spatial patterns the headwater catchments in the south east are generally gaining further downstream on the plains are dominantly losing conditions and then the lowest reaches become gaining again consistent with the generalisations of braaten and gates 2003 as all the bores do not have measurements of groundwater level for every year analysed a detailed comparison of the results between years can be confounded by inconsistent data an example is in the warrego river where different bores have water level observations in each of the example years that results in different quintile classifications at the reach scale 4 3 analysis of time series at the basin scale the number of bores predicting losing conditions in the nearest stream steadily increased from 735 in 1970 71 to a maximum of 4020 in 2002 03 and then fell to 1105 in 2018 19 fig 6 a the number of bores predicting gaining conditions followed a similar pattern increasing from 277 bores in 1970 71 to a maximum of 2439 in 2005 06 before falling to 218 bores in 2018 19 this pattern is due to the number of bores measured in a given year rather than changing conditions through time when looking at the proportion of bores predicting losing conditions there is a decrease from 60 in 1970 71 to a minimum of 48 in 1995 96 before increasing through the period of the millennium drought until the 2010 11 floods these floods caused a temporary decrease in the proportion of bores predicting losing conditions before it increased after 2015 16 to a maximum of 78 in 2018 19 fig 6b the number of bores predicting gaining conditions increased from 23 in 1970 71 to a maximum of 33 in 2005 06 and then fell rapidly after the 2010 11 floods to 15 in 2018 19 for the decade from 1995 96 to 2005 06 both the proportion of bores predicting gaining and losing conditions were increasing this apparent contradiction can be explained by the proportion of bores that were classified as unsure decreased from a maximum of 20 in 1993 94 to a minimum of 6 in 2018 19 the number of reaches where the proportion of bores predicting losing conditions was greater than 80 increased from 61 in 1970 71 to a maximum of 157 in 2017 18 the number of reaches where the proportion of bores predicting losing conditions was less than 20 increased from 15 in 1970 71 to a maximum of 66 in 1995 96 and 2000 01 before decreasing to 19 in 2017 18 fig 6c similar to the analysis of the number of bores predicting losing conditions these trends are confounded by the changing number of reaches which have data through time the proportion of reaches where the proportion of bores predicting losing conditions is greater than 80 decreases from 56 in 1970 71 to a minimum of 40 in 1993 94 before increasing to a maximum of 66 of reaches in 2018 19 fig 6d the proportion of reaches where the proportion of bores predicting losing conditions is less than 20 shows the opposite trend through time it starts at 14 in 1970 71 rising to a maximum of 25 in 1995 96 before falling to a minimum of 9 in 2018 19 a temporary decrease in the proportion of reaches with greater than 80 of bores predicting losing conditions is evident after the floods that ended the millennium drought from 2010 to 2012 but this is not mirrored by an increase in the proportion of reaches with less than 20 of bores predicting losing conditions most of the gain in proportion of reaches is in the 40 60 of bores predicting losing conditions category following the basin scale trends groups of reaches can be identified that behave similarly using k means clustering the optimum number of clusters was identified to be seven using the akaike information criterion corrected for small sample size aicc fig 7 a cluster 1 contains about a third of all reaches and has almost all bores predicting losing conditions for all years fig 7b these reaches are mostly located in the lowlands with wide alluvial systems fig 7c cluster 2 contains approximately a fifth of reaches and has few bores predicting losing conditions although with an increasing proportion since 2000 the majority of cluster 2 reaches are in the headwater catchments with some in the lower reaches of the murray river and near the confluence of the murray river and billabong creek the mean for cluster 3 had less than half of bores predicting losing conditions for the majority of the time under study until the most recent few years where more than half of bores were predicting losing conditions most reaches in cluster 3 are in the southern basin but vary in topographic position from the headwater catchments to the lower reaches cluster 4 has a monotonically increasing trend for the proportion of bores predicting losing conditions from less than 40 at the start of the study period to above 70 by the end cluster 5 has most bores at the start of the study period predicting losing conditions but decreases until the early 2000 s before increasing again to the end of the study period clusters 6 and 7 show similar trends in that they have little trend until the mid 1990 s and then increase in their proportion of bores predicting losing conditions the difference between them is that cluster 6 has a higher proportion of bores predicting losing conditions than cluster 7 the analysis at the basin scale fig 6 and the cluster analysis fig 7 showed that in most cases something happened in the trends seen between about the mid 1990 s and mid 2000 s to investigate further an analysis was conducted for two time periods to provide a comparison the 20 year period 1980 2000 and the 19 year period 2000 2019 at the reach scale the time series of the proportion of bores predicting losing conditions was classified based on the mean of the proportion of bores predicting losing conditions and the trend through time an example for each category is shown in fig 8 for the 2000 2019 period some of these reaches are easy to interpret as there are many bores analysed each year and so there is confidence in the results the best examples shown have over 100 bores with water level measurements per year for the 2000 2019 period fig 8c f where there are few bores with measured water levels and the number and presumably location of bores changes through time the analysis of means and trends can become less certain the worst examples fig 8e h show a step change in the number of bores with water level measurements through time and a corresponding change in the mean and trend of the proportion of bores predicting losing conditions comparing the means and trends in the proportion of bores predicting losing conditions for the 20 year period from 1980 to 2000 fig 9 to the 19 year period from 2000 to 2019 fig 10 there are almost the same number of always losing 95 cf 93 and never losing reaches both 40 the largest difference is in the mostly losing category with the 1980 2000 period having 91 reaches and this increases to 109 reaches in the 2000 2019 period there is a corresponding loss of reaches in the some losing 69 cf 58 and mostly not losing 64 cf 57 categories over this time a comparison between the northern and southern basin for the combined never losing and mostly not losing categories was consistent between time periods with 88 in the southern basin for 1980 2000 and 87 for 2000 2019 the combined always losing and mostly losing categories showed a difference between the northern and southern basins with 59 in the northern basin for the 1980 2000 period decreasing to 54 for the 2000 2019 period there was an increase in the number of reaches with a statistically significant increase in the proportion of bores predicting losing conditions 60 in 1980 2000 cf 66 in 2000 2019 although the magnitude was similar between periods the geographical spread was different with 79 of the increasing reaches in the southern basin in 1980 2000 and 92 in 2000 2019 there was a decrease in the number of reaches with a statistically significant decreasing trend 39 cf 18 with 82 of them in the southern basin in 1980 2000 decreasing to 67 for the period 2000 2019 the majority of reaches in both time periods had no statistically significant trend 260 in 1980 2000 and 273 in 2000 2019 when each reach is directly compared between time periods the majority 72 are within the same quintile of proportion of bores predicting losing conditions fig 11 a of those that changed quintiles 65 moved to a higher proportion of bores predicting losing conditions and 35 moved to a lower proportion quintile the trends show a different pattern with only 21 of those reaches with a statistically significant trend for the period 1980 2000 having the same trend for the period 2000 2019 fig 11b there were actually more reaches 22 with the opposing trend in 2000 2019 compared to 1980 2000 these were mainly switching from a decreasing trend to an increasing trend in the proportion of bores predicting losing conditions 5 discussion 5 1 analysis of the method the method applied here has produced results that are consistent spatially with the conceptual understanding of the basin braaten and gates 2003 and consistent temporally with known stressors on the system such as the salinity problems in the latter decades of last century mdbmc 1999 and the recent droughts van dijk et al 2013 however at a more localised scale there can be confounding impacts on the results where the number of bores within a reach included in the analysis changes through time the mean and trends calculated from them can also change in response to the bores analysed rather than a stress on the system e g fig 8e h this can possibly lead to unrepresentative results as well as the temporal availability of bores the spatial arrangement of the bores available can also cause problems when a reach contains both gaining and losing sections the spatial arrangement of the bores may produce a result that is not representative of the reach as a whole the most obvious reach is upstream of river murray at murray bridge that also includes the small tributary of marne river flowing in from the mount lofty ranges in this reach there are more bores along the marne river that are classified as losing than bores along the main trunk of the murray river that are classified as gaining this is causing the reach to be classified as mostly losing in fig 9 and some losing in fig 10 when if the bores were spread evenly along the length of the reach it would have been classified as mostly not losing this is an artefact of the catchment discretisation as used by the awra r model dutta et al 2015 rather than this section of the lower murray river actually being majority losing central to the method as applied was the criteria for including bores in the analysis the choice to use a 5 km buffer distance around the stream network for inclusion of bores in the analysis was a pragmatic choice around data availability rather than a technically sound decision to illustrate why this decision was made and its implications the mean and trends in the proportion of bores predicting losing conditions over the period 2000 2019 shown in fig 10 was recreated for a higher quality subset of the original dataset fig 12 the criteria for inclusion in this higher quality dataset was i bores were only included within 1 km of the stream network ii there was a minimum of 5 bores within a reach and iii the mean and trend over the 19 years was only calculated when there were a minimum of 5 years of data when the proportion of bores predicting losing conditions calculation was performed there were only 170 reaches had data available compared to 306 in fig 10 a visual comparison of fig 10 and fig 12 suggests that the patterns in the classification are similar between the 5 km buffer distance dataset and the higher quality 1 km subset this can be further seen in a difference map fig 13 a quantitative comparison shows that the mean category was the same between the 1 km and 5 km analyses in 75 of stream reaches that were present in the 1 km dataset fig 14 a but this fell to 66 for the trend classification fig 14b the most frequent response from the 1 km dataset to the categories in the 5 km dataset was always the same for the mean but the 1 km dataset tended to have the no trend category represented more often that the 5 km dataset of those reaches that were not classified as the same one reach in the higher quality 1 km dataset had a mean that was more than one quintile different and only three reaches showed an opposing trend this suggests that the methodological choice to include bores within a 5 km buffer rather than the higher quality data with a 1 km buffer had little effect at the reach scale but had the major advantage at the basin scale in that an extra 139 reaches were included the data analysis cannot be extended any further with the data available at this scale to fill in the gaps in space and time will require some form of modelling some examples that have been used elsewhere include characterisation based on geomorphological features ransley et al 2007 numerical groundwater modelling baratelli et al 2016 or upscaling observations using other catchment attributes yang et al 2019 5 2 losing connected versus losing disconnected we have not classified the surface water groundwater interactions of the stream reaches of the mdb further from gaining and losing to the distinction between losing connected and losing disconnected because doing so is not simple at this scale with the information that is available brunner et al 2009 presented the conditions necessary for disconnection to occur which included the presence of a clogging layer and the ratio of its hydraulic conductivity to that of the aquifer below such information are not available at the scale of the mdb taylor et al 2013 lamontagne et al 2014 investigated 12 sites in the mdb for their connection status and found that the sites with losing connected reaches had a groundwater level less than 1 m below the riverbed and losing disconnected reaches had a groundwater level between 6 and 25 m below the riverbed the analysis in this paper is comparing the groundwater level with the surface water elevation without information on the riverbed level parsons et al 2008 were faced with the same lack of information in their classification of surface water groundwater interactions in the mdb but made the simple assessment that losing disconnected streams had a groundwater level more than 10 m below the surface water elevation shown as maximum losing in fig 1d the 31 reaches identified by parsons et al 2008 as being losing disconnected also have the groundwater more than 10 m below the surface water in our analysis for both time periods analysed fig 15 fig 15 shows that the period 1980 2000 had 70 reaches 19 with a groundwater level greater than 10 m below the surface water elevation and that this had expanded to 82 23 reaches for the period 2000 2019 the same trend is seen for the 5 to 10 m class where the number of reaches increased from 65 18 to 76 21 between the 1980 2000 and 2000 2019 time periods this is consistent with the analysis of fu et al 2022 who showed that the groundwater level has been falling in the major alluvial aquifers of the mdb 5 3 comparison to previous studies of surface water groundwater interactions in the mdb the most direct comparison of the results found in this study fig 10 is to that of parsons et al 2008 fig 1d both of these results generally follow the conceptualisation of braaten and gates 2003 gaining conditions are seen in the headwaters losing conditions are seen through the middle reaches where the alluvium is either constricted in narrow valleys or spread out over wide floodplains and then becoming gaining again at the end of the valley due to geological constrictions these generalisations have also been seen in whole of catchment surface water groundwater interactions investigations of the namoi ivkovic 2009 and the border rivers ransley et al 2007 a quantitative comparison is not possible between these studies due to the different reach lengths and the different time periods used for data acquisition the major advance in the current work is that it is conducted at the same reach scale as used by the river system models and so gives information on many more reaches than previous studies and also has the significant benefit of being conducted as a time series rather than as a single point in time there have been many possibly 100 s detailed small scale studies investigating surface water groundwater interactions in the murray darling basin over the past few decades and it is beyond the scope of this paper to review them all a direct comparison with the results of this paper is difficult due to the scale differences between the reaches used here up to 500 km and the small scale detailed work from 10 s to 1000 s m table 1 provides a comparison between a selection of detailed small scale studies around the mdb and the regional reach scale results shown in fig 10 these show that there is general consistency between the results here and the small scale studies even seemingly inconsistent results such as lamontagne et al 2014 showing a gaining section in the dumaresq river compared to the mostly losing classification here can be consistent due to the scales involved a classification of mostly losing means that more than two thirds of bores in that reach predict losing conditions which also mean that up to one third of bores predict gaining conditions 5 4 implications for management of the water resources the results of this analysis have shown that there have been changes in the surface water groundwater interactions across the mdb over the past few decades there was an increasing trend in gaining conditions up until the millennium drought and then an increasing trend in losing conditions thereafter the increasing trend in gaining conditions is coincident with the increase in areas affected by salinity due to rising water tables and the beginning of the increase in losing conditions is coincident with the millennium drought but the losing trend continued even after the drought broke peterson et al 2021 showed that many catchments in victoria have not recovered their rainfall runoff relationship from prior to the millennium drought suggesting a change from a high runoff for a given rainfall stable state to low runoff for a given rainfall stable state hughes et al 2012 showed a similar relationship in western australia but was able to link the changed rainfall runoff relationship to groundwater levels the trends seen in the change in surface water groundwater interactions in this analysis across the entire mdb seem to not be solely caused by land use change i e salinity issues climate droughts then floods or development groundwater extraction it is likely a combination of all three and perhaps other drivers as well with the contribution of each driver a possible subject for future studies recent work in the mdb has shown the magnitude of streamflow depletion that can be expected due to existing groundwater extraction walker et al 2020 and that a future climate is likely to lead to reduced groundwater levels further exacerbating the current trend seen for an increasing proportion of losing streams walker et al 2021 it is not known what effects changes to groundwater interactions with streams will have on groundwater dependent ecosystems gde s boulton and hancock 2006 what is clear is that we will need better predictions of low flows from our hydrological models that are capable of modelling these changes in surface water groundwater interactions if we are to understand and predict changes in gde s 5 5 recommendations for improving the conceptualisation of river systems models the node link river systems models used for planning and managing the water resources of the mdb i e source welsh et al 2013 iqqm simons et al 1996 and realm perera et al 2005 do not explicitly simulate the surface water groundwater interactions but have this exchange flux bundled up with all other errors in an unallocated loss term that is a function of the flow kim et al 2022 the analysis of braaten and gates 2003 showed that the exchange flux between surface water and groundwater is of the same order of magnitude as the gauge errors for two major rivers in the mdb namoi and murrrumbidgee this would suggest that changes in the surface water groundwater exchange flux would not be large enough to cause a material change in the simulated flows for the models intended purpose of simulating the total flows accurately outputs of these simulated flows from the river systems models are also used as an input to ecological models where the low flows become more important rosenfeld 2017 during low flows the flow in the river gauge errors and the surface water groundwater exchange flux are all of the same order of magnitude and therefore changes in the surface water groundwater exchange flux particularly changing from gaining to losing or vice versa become important there are many alternate model structures that are capable of modelling this change in surface water groundwater exchange flux rassam and werner 2008 such as fully integrated hydrological models therrien et al 2006 or coupled river and groundwater models valerio et al 2010 but these do not meet the needs of the river systems models where the water sharing rules and dam releases are a large part of the model s capability a solution to this problem was built into the source model rassam 2011 but has not been adopted by river modellers the solution of rassam 2011 is using analytical equations of stream flow depletion in response to groundwater extraction it is tracking the depletion through time for each groundwater extraction point this solution requires some knowledge of groundwater processes that river modellers may not possess access to groundwater extraction and aquifer property data that may not exist and considerably slows down the model execution times a simpler solution is needed groundwater levels integrate all stressors climate extraction etc upon the system relevant to that point in space and time by making the surface water groundwater exchange flux in the river model dependent upon the surface water level a function of flow and the groundwater level then the changes in the surface water groundwater exchange flux through time can be explicitly represented within the river systems model without the complexity or computational penalty of actually modelling the groundwater system this is an area of research that needs to be explored further 6 conclusions this study has achieved its three aims in investigating the surface water groundwater interactions in the murray darling basin 1 using 59 340 bores with groundwater level measurements over the period 1 6 1949 to 31 5 2019 a point scale classification of gaining and losing conditions was conducted on an annual time step for 49 years these point scale results were aggregated to the stream reach scale as the proportion of bores predicting losing conditions there were 240 reaches on average across the 49 years that had a classification for the proportion of bores predicting losing conditions the maximum was 308 reaches in 2005 2006 these data are available from crosbie et al 2022 this is a significant advance on the previous studies of surface water groundwater interactions in the mdb that were limited in both spatial and temporal extent preventing a whole of basin analysis 2 at the whole of basin scale there was a trend in the latter decades of last century toward an increase in gaining conditions this is consistent with the salinity issues at the time due to rising water tables in the most recent 19 years analysed there has been an increasing trend in losing conditions over this period more than double the number of reaches were classed as always or mostly losing n 198 compared to never or mostly not losing n 94 over this 19 year period the majority of reaches did not have a statistically significant trend in the proportion of bores predicting losing conditions n 265 but there were more reaches that had an increasing trend n 65 compared to those with a decreasing trend n 18 3 the loss functions generally used in our river systems models have no knowledge of surface water groundwater interactions and therefore cannot simulate changes in the magnitude or direction of groundwater exchange fluxes these models are calibrated to reproduce the total annual flows which are dominated by the high flow events and therefore little weight is given to the low flows the ability to reproduce the observed changes in surface water groundwater interactions is most important in simulating low flows further research is needed on the most appropriate way to implement a dependency of the loss term on groundwater level in the simplest possible way credit authorship contribution statement russell crosbie conceptualization methodology writing original draft bill wang data curation shaun kim writing review editing cherry mateo writing review editing jai vaze conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the murray darling basin authority through the murray darling water and environment research program md werp further information is available at https getinvolved mdba gov au murray darling water and environment research program the authors wish to thank the reviewers and editors of the manuscript whose constructive criticisms resulted in improvements throughout the paper research data for this article the dataset presented in this paper is available from crosbie et al 2022 
1777,an operational tidal prediction method based on a combination of nonstationary harmonic analysis and deep learning neural network models is described nonstationary harmonic analysis nsha which was based on classical harmonic analysis cha has been developed for tidal forecasting in tidal rivers and estuaries however the prediction accuracy is poor when the discharge grows very large or changes abruptly therefore in this study we aimed to combine nonstationary harmonic analysis with a deep learning neural network model to improve tidal forecasts in tidal rivers and estuaries the long short term memory lstm neural network model which works well for processing long term time series data was chosen to correct the errors from nsha in addition the traditional feed forward neural network ffnn model was also applied and compared with lstm to determine and optimize the structure of the neural network through experiments the results showed that a two layer network with a fully connected layer on top of an lstm layer that uses discharge in addition to the previous time series as input data exhibited the best performance in predicting the errors of the ns tide model after correction by the proposed model at three stations on the west river a branch river of the pearl river the root mean square error rmse at 24 h by the ns tide model can be reduced from approximately 0 3 m to less than 0 1 m more specifically the results indicated a significant improvement for extremely high level prediction which is crucial for water conservancy administrations finally optimization approaches of the neural network to prevent overfitting and improve efficiency are also discussed keywords estuary tidal prediction deep learning lstm harmonic analysis data availability data will be made available on request 1 introduction predicting tides in estuaries and rivers is significant for the economy and sustainability of estuarine and inland tidal riverine areas coastal engineering endeavors such as levee design dam construction and navigation maintenance all require accurate tidal predictions however accurately predicting tides in estuaries and rivers is a challenge because tides in rivers are nonlinear and nonstationary guo et al 2015 they are the comprehensive result of oceanic tides riverine topography and geometry bottom friction and runoff from upstream and their complex nonlinear interactions the nonlinear hydrodynamic processes in estuaries and tidal rivers introduce new secondary tidal species including superharmonics and subharmonics hoitink and jay 2016 the former results from the increase in the asymmetry of tidal waves and the latter is caused by nonlinear interactions of tidal constituents buschman et al 2009 sassi and hoitink 2013 jay et al 2014 classic harmonic analysis cha has been applied widely for tidal forecasting in ocean and coastal waters demonstrating relatively high accuracy compared to statistical methods such as normal regression and auto regression a primary assumption of cha is that the water level fluctuations are stationary which is true for oceanic tides in deep waters however tides in estuaries and rivers are influenced by the spatial variation in estuarine geometry and the temporal variation in river runoff as a result in tidal rivers the tidal harmonic constants which are considered fixed in cha may vary over the period in addition more secondary tidal constituents must be considered such as supertidal and subtidal constituents produced through the interactions of major astronomical constituents estuarine geometry and river runoff consequently the tides in estuaries and rivers should not be considered stationary direct application of cha in estuaries yields incorrect tidal forecasting to capture the nonstationary characteristics of tides in estuaries and rivers ns tide was developed by matte et al 2013 2014 by modifying t tide based on the stage and tidal fluvial models proposed by jay et al 2011 compared to the continuous wavelet transform cwt which is a popular tool for analyzing nonstationary fluctuations suggested by many scholars jay and flinchem 1999 flinchem and jay 2000 kukulka and jay 2003a 2003b buschman et al 2009 ns tide increases the resolution of tidal constituents within tidal species and can quantitatively reveal the influence of river discharge and tidal forcing on the mean water level as well as the temporal variation in tidal harmonic constants therefore ns tide is a useful tool for nonstationary harmonic analysis nsha in tidal rivers however ns tide has limitations pan et al 2018 revealed that ns tide is not as accurate in representing subtidal water level fluctuations gan et al 2019 also claimed that the errors from ns tide have strong subtidal low frequency variations based on the above findings chen et al 2020 proposed an autoregressive ar model to estimate the errors from ns tide and developed a practical ns tide ar hybrid model to correct the predictions from the ns tide model while confirming the above advances in tidal prediction methods in tidal rivers we believe that in terms of nonlinear fluctuations such as the temporal errors from ns tide deep learning neural network models may be a better choice in combination with nsha for improving tidal predictions artificial neural network ann methods have been widely applied in predicting chaotic and complicated fluctuations because anns have strong self learning ability which helps them find the regularity of data in a large dataset napolitano et al 2011 seo et al 2015 chang et al 2016 in a previous study a feed forward neural network ffnn also called a fully connected layer ann was used to predict astronomical tidal levels lee and jeng 2002 lee 2004 makarynskyy et al 2004 and surges lee 2006 tseng et al 2007 guo et al 2013 kim et al 2016 apart from water level forecasting the ffnn was also applied in combination with hydraulic models for the real time prediction of storm surge and onshore flooding bajo and umgiesser 2010 sahoo and bhaskaran 2019 however the ffnn is not a deep learning neural network and its capability for predicting time series is limited because it cannot remember previous time series information therefore the long short term memory lstm neural network model which is a special kind of recurrent neural network rnn suited for time series prediction was proposed and developed hochreiter and schmidhuber 1997 gers et al 2014 lstm is a well known deep learning model that makes the units form a directed cycle allowing the data to flow both forward and backward within a network thus the previous information can be preserved for future use therefore lstm is particularly suitable for temporal processes because it has a strong memory of past events liu et al 2020 in view of the advantages we used the lstm model to predict the errors from the ns tide model in contrast the ar model and traditional ffnn model have also been used for comparisons the idea that integrating lstm and nonstationary harmonic analysis could enhance the prediction accuracy of tide levels was explored by xu et al 2022 who developed a hybrid model for this purpose while the model showed improved short term prediction performance it was primarily designed for a 48 hour horizon and only considered the water level factor to further enhance the predictive capabilities it would be valuable to conduct additional explorations such as extending the prediction horizon by incorporating more factors optimizing the model structure and addressing overfitting issues the objective of this paper is to explore the applicability of lstm models in correcting the errors from ns tide and to develop a high accuracy tidal prediction model in estuaries and tidal rivers the remainder of this paper is organized as follows the study area and field data are introduced in section 2 the methodology is introduced in section 3 in section 4 the errors from ns tide prediction are analyzed to determine the major factors in section 5 the structure of the neural network is described based on the analysis and tests in section 6 the results of different neural networks are compared with the ar model in section 7 a discussion regarding overfitting issues is provided finally section 8 concludes the paper 2 study area and field data the study area is in the tidal reach of the west river in the pearl river network as shown in fig 1 the most upstream station is wuzhou wz station where the discharge 1760 29100 m3 s and water level are hardly affected by the tide from the ocean the tidal limit site moves periodically during the summer flood season and winter dry season between stations wuzhou wz and gaoyao gy hourly series of river discharge at the wz station and water levels at the makou mk gaoming gm ganzhu gz tianhe th baiqingtou bqt zhuyin zy denglongshan dls and sanzao sz stations are available from 2015 to 2017 as shown in fig 2 the reaches downstream of mk station are characteristic of tidal water level oscillations and simultaneously influenced by runoff from the upstream area as the station locations approach the outlet the influence of runoff becomes weaker and the tidal oscillations are strengthened therefore the wz station is used as the reference location for river discharge and the sz station is used as the reference location for ocean tides in the ns tide model 3 methodology 3 1 ns tide unlike cha nsha ns tide considers the harmonic constant not truly static but variable according to the upstream river discharge and downstream tidal range and their interactions thus the following expression for tidal constituents was proposed by matte et al 2013 2014 1 η η 0 k 1 n a k cos σ k t b k s i n σ k t 2 η 0 c 0 c 1 q 2 3 t τ q c 2 r 2 t τ r q 4 3 t τ q 3 a k d 0 k c d 1 k c q t τ q d 2 k c r 2 t τ r q 1 2 t τ q 4 b k d 0 k s d 1 k s q t τ q d 2 k s r 2 t τ r q 1 2 t τ q where c i i 0 1 2 are the model parameters for the stage model d i i 0 1 2 are the model parameters for the tidal fluvial model q is the upstream discharge r is the downstream tidal range τ r and τ q are the time lags applied to the q and r time series respectively the superscripts c and s refer to the cosine and sine terms respectively and c i and d i i 0 1 2 are regression coefficients that can be determined through the iteratively reweighted least squares irls algorithm through ns tide the variable harmonic constants a k b k can be estimated through 3 and 4 which can be applied to predict the tidal levels in a tidal river and estuary 3 2 lstm a long short term memory network lstm hochreiter and schmidhuber 1997 gers et al 2014 is a special kind of recurrent neural network rnn however rnns cannot solve the long term dependency problem because the information perception ability is weak and the gradient disappears or explodes after multistage backpropagation to address this problem lstm uses gated units to control the flow and loss of information specifically in lstm three gates are introduced an input gate i t forget gate f t and output gate o t the cell state c t that characterizes long term memory and the candidate state c t waiting to be stored in long term memory are introduced the input gate determines how much of the information is stored in the current cell state in contrast the forget gate is applied to selectively forget the information in the cell state and the output gate is applied to selectively output the information in the cell state the expressions of the three gates are as follows 5 i t σ w i h t 1 x t b i 6 f t σ w f h t 1 x t b f 7 o t σ w o h t 1 x t b o where w i w f and w o represent the input gate forget gate and output gate parameter matrices to be trained respectively b i b f and b o are the bias items to be trained and σ represents the sigmoid activation function which controls the range of the threshold to be between zero and unit in addition to the three gates lstm uses a constant error carousel cec cell to prevent the gradient from exploding or vanishing and maintain long term memory the specific expressions are as follows 8 h t o t t a n h c t 9 c t t a n h w c h t 1 x t b c 10 c t f t c t 1 i t c t where h t is the short term memory which is obtained by the current cell state passing through the output gate c t is the long term memory which is the sum of the value of the long term memory at the previous moment passing through the forget gate and the new candidate value c t at the current moment passing through the input gate and the elementwise multiplication of two vectors is denoted with fig 3 shows the structure and workflow of the lstm neural network like rnn lstm computes a mapping from an input sequence x to an output sequence y by calculating the network unit activations using eqs 5 10 from time step t 1 to t τ 3 3 performance evaluation of the model 1 training loss 11 l o s s i 1 n y i y i 2 where y i is the measured value at time i and y i is the predicted value at time i this indicator is mainly used in the training phase of the neural network to evaluate the training results 2 the root mean square error rmse 12 r m s e i 1 n y i y i 2 n where y i is the mean value of y i 3 the coefficient of determination r2 13 r 2 i 1 n y i y i 2 i 1 n y i y i 2 i 1 n y i y i 2 in this study rmse and r2 are used to evaluate the model s accuracy by comparing the real and predicted values r2 measures the degree to which the outcomes are replicated by the model and an r2 score close to 1 is preferred rmse measures the prediction precision by summing up the squared errors and a value close to 0 is preferred in the article the rmse values used to evaluate different computational objects are distinguished by adding subscripts the residual value of the predicted water level elevation errors is expressed by rmser and the rmse value of the total water level elevation is expressed by rmset 4 analysis of the predictive errors of ns tide among the twelve hydrologic stations in fig 1 mk station and its downstream stations are in the typical tidal estuaries featured by the complex interaction of variable runoff and tides one year of the hourly water levels 8760 h in total is applied to regress the coefficients of the ns tide model moreover t tide is also used for comparison the results from the ns tide model with theoretical exponents and those with the optimized exponents and the t tide model are compared for the ns tide model with theoretical exponents the suggested values are based on matte et al 2013 2014 for the ns tide model with the optimized exponents the iterative process is used to determine the exponents with the minimum rmse values by using the iteratively reweighted least squares algorithm the rmse values for the three models are compared in fig 4 this indicates that both ns tide models can capture the fluctuations very well especially the model with the optimized exponents in contrast the t tide model can only obtain satisfying results at stations zy dls and sz in the lower downstream section of the estuary with the stations approaching upstream along the river where the effect of the discharge on water levels increases along the channel the errors of t tide grow rapidly although the errors of ns tide also increase upstream they grow very slowly which illustrates the applicability of the ns tide model in tidal rivers then the temporal variation in the predictive errors of the ns tide model relative to the measurements is investigated at three stations gm gz and th and the predictive errors are significant compared to the downstream stations as shown in fig 5 some short period fluctuations with periods less than 1 day can be found on the error curves the low pass filter is used to remove these short period fluctuations and obtain smoother error curves the maximum errors can reach 0 8 m and 1 1 m at the th station and the other two stations show similar trends but smaller peak errors the timings of the peak error mainly correspond to the peak river discharge these results confirm the findings of pan et al 2018 and gan et al 2019 that showed large predictive errors from ns tide are always related to subtidal fluctuations such as discharge to assess the correlation of the predictive errors of the ns tide model with other dynamic factors and considering the phase difference between the residual value and the input characteristics of each station the lag adjustment is made and a series of correlation analysis is conducted using pearson correlation coefficient the correlated dynamic factors include river discharge q offshore tidal range r principal tidal constituents q1 o1 p1 k1 n2 m2 s2 and k2 subtidal constituents msf and high frequency constituents m4 ms4 m6 and mn4 the results in fig 6 show that river discharge from upstream is the strongest correlated factor at all three stations the second most strongly correlated factor is not the same for the stations for the upstream station gm the subtidal constituent msf is second only to river discharge however for the downstream stations gz and th stations the principal tidal constituents and high frequency constituents are more correlated with the predictive errors of ns tide than the subtidal constituents 5 composition of the neural network 5 1 determination of the input layer as shown in fig 5 the predictive errors of the ns tide model can be considered as short period fluctuations superimposed on low frequency fluctuations these short period fluctuations are characterized by a random and stochastic process which can be predicted by their previous behavior and information a classical method for establishing the relation between further behavior and past behavior for a stochastic process is to use autoregress ar analysis which can determine the order p by calculating akaike s information criterion aic number through the order p we can determine how many previous time series should be used to capture the current and future behavior of the stochastic process fig 7 shows the results of the ar analysis they indicate that there are 2 sharp decreases in the aic number at p equating to 4 and 29 when p 4 and p 29 the clear improvement in model accuracy could be attribute to the influence of the one sixth diurnal tide m6 and the diurnal tidal group d1 when p increases to more than 50 the aic number remains nearly stable we also tested the other stations and the results are similar therefore p 50 is a sufficiently large order to capture the stochastic fluctuations for these time series for the neural network to predict the future values of the model error at time t 1 the past values at time t t 1 t 2 t 49 must be provided in the input layer in addition to the short period stochastic fluctuations the time series is also correlated with low frequency fluctuations such as the discharge q shown in fig 5 and q is also considered an optional node in the input layer therefore the model used in this study employs the predictive errors of the ns tide model and the discharge at the previous 50 times as input variables for predicting the future errors for correction 5 2 determination of the neural network structure it is well known that lstm networks can preserve previous information which can help improve the ability of the model to learn time series data however only one hidden layer is insufficient to fit the complex nonlinear fluctuations therefore we suggest a neural network with two hidden layers although the ffnn model does not have the ability to learn long term time series data it has a strong fitting ability and can be used as a fully connected layer in combination with lstm to obtain better results than those obtained using lstm alone for example zhang et al 2018 proposed a hybrid model composed of an lstm layer with another fully connected layer on top of it and showed that the model performed satisfactorily in predicting the water table depth they claimed that a fully connected layer atop the lstm layer helped the lstm to improve the learning and fitting ability of the model to determine the optimal neural network structure we compose three neural networks with double hidden layers shown in fig 8 to compare their performances a double lstm network is composed of two hidden layers and the other two neural networks are the hybrid of lstm and ffnn for comparison single hidden layer neural networks such as lstm and ffnn are also included in the experiment the performances of the five neural network structures are shown in fig 9 the comparison of the results indicates that all five neural networks predict consistent error results with that by ns tide during the training stage less than 248 days even including the single hidden layer neural networks however it makes a difference during the validation stage when the error curve changes abruptly marked in fig 9 a the prediction error also increases for the neural networks among the five neural networks the local zoomed curves shown in fig 9 b and c indicate that the lstm ffnn network provides the best predictions and the single hidden layer ffnn yields the worst predictions table 1 shows the performance statistics for the neural networks and the lstm ffnn obtains the lowest rmser and highest r2 5 3 determination of the output layer for the output layer the output nodes should be determined the output node number determines the length of the time series that can be predicted one at a time for the neural network for example if the output layer contains 10 nodes the time series values within a duration between t and t 9 can be predicted based on the past time series values when predicting the value at t 10 the previously predicted value at t should be used as input data consequently an increasing number of predicted values is used as the input values for further prediction in this recursive procedure and thus the estimate errors accumulate with time we carried out experiments by varying the output nodes and keeping the other parameters unchanged as shown in fig 10 a when the number of nodes in the output layer is small the model results fit the values by ns tide well within 12 h table 1 demonstrates that the model with only 1 output node obtains the smallest rmser values for the first 12 h however with the increase in the prediction time the errors of the model results with only 1 output node grow dramatically owing to the accumulation of previous errors fig 10 b traditional statistical models such as the ar model are all single valued functions therefore they have similar problems that limit their application in long term time series one of the advantages of the neural network is that it supports multiple output values corresponding to the output nodes by increasing the number of output nodes the accumulated errors can be controlled at a lower level and thus longer time series can be predicted by the model fig 10 shows that more output nodes indeed improve the neural network results especially after the first 12 h however increasing the number of output nodes too more does not improve the prediction results of the model because a model with too many output nodes would have poor fitting precision the rmser value of the model would increase as shown in fig 10 when the number of neuron nodes in the output layer is 9 or 12 there are also large errors between the prediction results and the actual value table 2 also indicates that when the number of output nodes exceeds 8 the rmser values of the predicted value increase markedly through a series of experiments we chose the model with 6 output nodes in the output layer which obtains the minimum rmser values in the overall process of the 72 hour prediction in addition to the specific structure of the model determined above it is necessary to optimize some other hyperparameters in the model such as the number of neuron nodes in hidden layers moreover preventing overfitting is a critical issue for all neural networks we optimize these hyperparameters in the process of composing the network those results will be introduced in the discussion section all relevant codes in this experiment are called and written from the tensorflow library and run on a computer with 16 gb of running memory and an intel core i7 11700 gpu 6 results in the previous section we determined the optimal neural network structure lstm ffnn as well as the number of nodes in the input layer and output layer then in this section a series of tests for assessing the performance of the proposed model and to comparing it with that of other models such as the ar model are described to determine whether and to what extent the discharge which is related to error fluctuations see fig 6 can be used to improve the prediction ability of the neural network iterations of the model including the input of discharge lstm ffnn q and excluding the input of discharge lstm ffnn are also compared in addition the neural network with one hidden layer lstm is included for comparison figs 11 13 depict the performances of the four models that predict the errors of the ns tide model for 3 days 72 h during the flood and dry seasons at the three stations the results indicate that although the errors of the four models grow with time the neural network with lstm ffnn q yields the smallest rmser and the slowest increase rate of rmser with time even up to 72 h the results of the model with lstm ffnn q are satisfactory in comparison the ar model obtains the largest rmser which grows dramatically with elapsed time the reason for this is that the ar model can only predict one value in the next hour at a time and the accumulated errors increase rather quickly in the recursive procedure for 72 h the model with one hidden layer lstm obtains better results than the ar model at the th and gm stations however lstm demonstrates similar performance to the ar model at gz station which is very puzzling because lstm is much more complex than the ar model and was predicted to obtain better results finally we focus on the comparison of models with lstm ffnn and lstm ffnn q this indicates that the model with discharge lstm ffnn q obtains obviously better results than that without discharge lstm ffnn for all three stations during both flood and dry seasons these results confirm our hypothesis that including correlated external information such as discharge into the time series based model can significantly improve the prediction capability of the neural network this is an advantage of neural networks over other statistical models such as ar models based on accurately evaluating the predictive errors of the ns tide model the proposed deep learning neural network lstm ffnn q is applied to correct and improve the prediction of ns tide the main method is to recursively apply the six output values of the model to the prediction of a 24 hour short period to observe the correction effect similar to those in fig 5 the hydrographs at the same stations are used to compare the correction effect using the proposed model according to the hydrodynamics the test period is divided into the dry season and flood season and in each season one typical month is chosen as shown in fig 14 through the short term 24 h correction the prediction accuracy of the ns tide model is greatly improved specifically the maximum predictive errors from the ns tide model without correction can reach up to 0 53 m 0 64 m and 0 71 m for the th gz and gm stations respectively in the dry season after correction using the proposed neural network lstm ffnn q the maximum predictive errors are reduced to 0 06 m 0 08 m and 0 12 m respectively in the flood season the maximum predictive errors can also be decreased from 0 94 m 0 90 m and 0 87 m to 0 40 m 0 46 m and 0 38 m for the th gz and gm stations respectively on average after correction the rmse r can be reduced from approximately 0 3 m to less than 0 1 m more than two thirds of the reduction of the errors from the ns tide model in particular accurate prediction of extreme water levels is of great importance for the water sector in the fields of risk assessment disaster prevention and water management the extreme water level here refers to the maximum and minimum values within a certain time interval when the water level fluctuates we collected these extreme water levels at the th gz and gm stations and compared the predictions with observations fig 15 the results show that without correction ns tide tends to underestimate extreme water levels while after correction by the proposed model the agreement between prediction and observation is significantly improved r2 values from 0 948 to 0 997 therefore the results illustrate that the proposed correction method can capture the dramatic fluctuations of the peak levels during the period of abrupt alteration in discharge and effectively improve the accuracy of the ns tide model in the prediction of extreme water levels in the next 24 72 h 7 discussion in addition to the neural network structure determined in section 5 some other hyperparameters also influence the model performance which will be discussed in this section the first hyperparameter investigated is the number of neuron nodes in the hidden layer theoretically increasing the number of neuron nodes can increase the fitting capability however if the number of neuron nodes is too large the test results of the model may be overfitted taking the model results at gm station as an example as shown in table 3 when the number of neuron nodes increases from 10 to 40 the loss decreases from 3 315 to 1 151 rmser decreases from 0 0578 to 0 0485 and r2 increases from 0 907 to 0 935 however when the number of neuron nodes increases to 60 and 100 although the loss number continues to decrease rmser increases while r2 decreases this indicates that overfitting occurs when the number of neuron nodes is more than 40 fig 16 shows one of the overfitting phenomena at gm station during the calibration process the number of output nodes here is consistent with the best output node of the model used in this paper which is 6 h an obvious feature of overfitting is that the absolute errors in the predicting phase are larger than those in the training phase by using a dropout layer to randomly drop out some neurons in the hidden layer the prediction can be improved and overfitting is prevented see table 4 table 4 shows that when the dropout coefficient is 0 1 the overfitting phenomenon of the model is alleviated to some extent but when the dropout coefficient increases the rmser value of the model also gradually increases and r2 gradually decreases this phenomenon may be caused by insufficient sample data and the high complexity of the model used even when the dropout coefficient is greater than 0 5 there are too many neural nodes that fail in the input layer of the model which gradually leads to the phenomenon of underfitting of the model learning rates also affect the fitting results of the model when the learning rate is high the network may not converge and the optimal value is ignored during the training process when the learning rate is low it may slow down the convergence speed and increase the time required to find the optimal value as shown in table 3 learning rate 0 01 is the optimal value with less rmser and larger r2 values than 0 1 and 0 001 epoch denotes how many times all training data are used in the training process when the training times are small appropriately increasing the training times of the entire training set may improve the fitting accuracy of the model however when there are too many training iterations in the training set the accuracy of the model is not necessarily improved which may also reduce the accuracy of the model and the training time of the model increases the model results at gm station table 3 indicate that epoch 300 is a suitable value for obtaining the lowest rmser and best r2 values in water level prediction the number of lstm layers will also have different effects on the fitting effect of the model in this study three years of data are used to divide the training set and test set data of different lengths due to the limitation of the amount of data the training set and test set are divided in the ratio of 70 and 30 of the whole data this is to judge the impact of the number of lstm layers on the model effect and the data used for testing are not involved in model training as shown in table 5 the single layer lstm model has better prediction performance the model presented in this paper is designed based on the characteristics of the input data which is a time series and the successful performance of lstm in time series prediction lstm model alleviates the problem of gradient disappearance in rnns however it is important to note that when the sequence length is too long the gradient may still disappear hence attention must be given to the length of the data sequence used when selecting and using the model if the amount of data contained is large the model structure suitable for its operation may be more complex as there are more controllable parameters making the complex model more flexible and powerful using a complex model with more parameters for training when the amount of data used is small may result in over fitting which is discussed in the article determining the most suitable model requires considering the specific data used and its characteristics as well as evaluating the results obtained using different models 8 conclusions in this study we proposed an lstm based method to estimate and correct the predictive errors of the ns tide model by analyzing the errors from ns tide we found that the errors were positively correlated with discharge when the discharge was large and changed abruptly the predictive errors of the ns tide model also rose greatly to solve this problem we composed a deep learning neural network through a series of experiments to compare and determine the optimal structure we found that the model with a network structure named lstm ffnn q performed satisfactorily in estimating the short term predictive errors of the ns tide model the rmser and r2 values showed that the model lstm ffnn q obtained much better results than the ar model and the neural networks with single layer lstm and double layer lstm ffnn without q through the correction of our proposed model the accuracy of the ns tide model improved significantly with a reduction of approximately two thirds for rmset on average in particular the prediction of the extremely level which is a crucial indicator for water conservancy management was much more accurate than that before the correction finally we tested and discussed how to optimize the neural network to prevent overfitting and improve efficiency we found that if the neuron number in a hidden layer was too large the dropout layer method could be applied to prevent overfitting credit authorship contribution statement zhuo zhang conceptualization methodology investigation writing original draft lu zhang methodology investigation software writing original draft songshan yue methodology investigation writing review editing jiaxing wu validation investigation software visualization fei guo resources investigation software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china 42171465 42171406 key laboratory of ministry of education for coastal disaster and protection hohai university 202218 and the open research fund of key laboratory of flood drought disaster defense the ministry of water resources kyfb202112010704 
1777,an operational tidal prediction method based on a combination of nonstationary harmonic analysis and deep learning neural network models is described nonstationary harmonic analysis nsha which was based on classical harmonic analysis cha has been developed for tidal forecasting in tidal rivers and estuaries however the prediction accuracy is poor when the discharge grows very large or changes abruptly therefore in this study we aimed to combine nonstationary harmonic analysis with a deep learning neural network model to improve tidal forecasts in tidal rivers and estuaries the long short term memory lstm neural network model which works well for processing long term time series data was chosen to correct the errors from nsha in addition the traditional feed forward neural network ffnn model was also applied and compared with lstm to determine and optimize the structure of the neural network through experiments the results showed that a two layer network with a fully connected layer on top of an lstm layer that uses discharge in addition to the previous time series as input data exhibited the best performance in predicting the errors of the ns tide model after correction by the proposed model at three stations on the west river a branch river of the pearl river the root mean square error rmse at 24 h by the ns tide model can be reduced from approximately 0 3 m to less than 0 1 m more specifically the results indicated a significant improvement for extremely high level prediction which is crucial for water conservancy administrations finally optimization approaches of the neural network to prevent overfitting and improve efficiency are also discussed keywords estuary tidal prediction deep learning lstm harmonic analysis data availability data will be made available on request 1 introduction predicting tides in estuaries and rivers is significant for the economy and sustainability of estuarine and inland tidal riverine areas coastal engineering endeavors such as levee design dam construction and navigation maintenance all require accurate tidal predictions however accurately predicting tides in estuaries and rivers is a challenge because tides in rivers are nonlinear and nonstationary guo et al 2015 they are the comprehensive result of oceanic tides riverine topography and geometry bottom friction and runoff from upstream and their complex nonlinear interactions the nonlinear hydrodynamic processes in estuaries and tidal rivers introduce new secondary tidal species including superharmonics and subharmonics hoitink and jay 2016 the former results from the increase in the asymmetry of tidal waves and the latter is caused by nonlinear interactions of tidal constituents buschman et al 2009 sassi and hoitink 2013 jay et al 2014 classic harmonic analysis cha has been applied widely for tidal forecasting in ocean and coastal waters demonstrating relatively high accuracy compared to statistical methods such as normal regression and auto regression a primary assumption of cha is that the water level fluctuations are stationary which is true for oceanic tides in deep waters however tides in estuaries and rivers are influenced by the spatial variation in estuarine geometry and the temporal variation in river runoff as a result in tidal rivers the tidal harmonic constants which are considered fixed in cha may vary over the period in addition more secondary tidal constituents must be considered such as supertidal and subtidal constituents produced through the interactions of major astronomical constituents estuarine geometry and river runoff consequently the tides in estuaries and rivers should not be considered stationary direct application of cha in estuaries yields incorrect tidal forecasting to capture the nonstationary characteristics of tides in estuaries and rivers ns tide was developed by matte et al 2013 2014 by modifying t tide based on the stage and tidal fluvial models proposed by jay et al 2011 compared to the continuous wavelet transform cwt which is a popular tool for analyzing nonstationary fluctuations suggested by many scholars jay and flinchem 1999 flinchem and jay 2000 kukulka and jay 2003a 2003b buschman et al 2009 ns tide increases the resolution of tidal constituents within tidal species and can quantitatively reveal the influence of river discharge and tidal forcing on the mean water level as well as the temporal variation in tidal harmonic constants therefore ns tide is a useful tool for nonstationary harmonic analysis nsha in tidal rivers however ns tide has limitations pan et al 2018 revealed that ns tide is not as accurate in representing subtidal water level fluctuations gan et al 2019 also claimed that the errors from ns tide have strong subtidal low frequency variations based on the above findings chen et al 2020 proposed an autoregressive ar model to estimate the errors from ns tide and developed a practical ns tide ar hybrid model to correct the predictions from the ns tide model while confirming the above advances in tidal prediction methods in tidal rivers we believe that in terms of nonlinear fluctuations such as the temporal errors from ns tide deep learning neural network models may be a better choice in combination with nsha for improving tidal predictions artificial neural network ann methods have been widely applied in predicting chaotic and complicated fluctuations because anns have strong self learning ability which helps them find the regularity of data in a large dataset napolitano et al 2011 seo et al 2015 chang et al 2016 in a previous study a feed forward neural network ffnn also called a fully connected layer ann was used to predict astronomical tidal levels lee and jeng 2002 lee 2004 makarynskyy et al 2004 and surges lee 2006 tseng et al 2007 guo et al 2013 kim et al 2016 apart from water level forecasting the ffnn was also applied in combination with hydraulic models for the real time prediction of storm surge and onshore flooding bajo and umgiesser 2010 sahoo and bhaskaran 2019 however the ffnn is not a deep learning neural network and its capability for predicting time series is limited because it cannot remember previous time series information therefore the long short term memory lstm neural network model which is a special kind of recurrent neural network rnn suited for time series prediction was proposed and developed hochreiter and schmidhuber 1997 gers et al 2014 lstm is a well known deep learning model that makes the units form a directed cycle allowing the data to flow both forward and backward within a network thus the previous information can be preserved for future use therefore lstm is particularly suitable for temporal processes because it has a strong memory of past events liu et al 2020 in view of the advantages we used the lstm model to predict the errors from the ns tide model in contrast the ar model and traditional ffnn model have also been used for comparisons the idea that integrating lstm and nonstationary harmonic analysis could enhance the prediction accuracy of tide levels was explored by xu et al 2022 who developed a hybrid model for this purpose while the model showed improved short term prediction performance it was primarily designed for a 48 hour horizon and only considered the water level factor to further enhance the predictive capabilities it would be valuable to conduct additional explorations such as extending the prediction horizon by incorporating more factors optimizing the model structure and addressing overfitting issues the objective of this paper is to explore the applicability of lstm models in correcting the errors from ns tide and to develop a high accuracy tidal prediction model in estuaries and tidal rivers the remainder of this paper is organized as follows the study area and field data are introduced in section 2 the methodology is introduced in section 3 in section 4 the errors from ns tide prediction are analyzed to determine the major factors in section 5 the structure of the neural network is described based on the analysis and tests in section 6 the results of different neural networks are compared with the ar model in section 7 a discussion regarding overfitting issues is provided finally section 8 concludes the paper 2 study area and field data the study area is in the tidal reach of the west river in the pearl river network as shown in fig 1 the most upstream station is wuzhou wz station where the discharge 1760 29100 m3 s and water level are hardly affected by the tide from the ocean the tidal limit site moves periodically during the summer flood season and winter dry season between stations wuzhou wz and gaoyao gy hourly series of river discharge at the wz station and water levels at the makou mk gaoming gm ganzhu gz tianhe th baiqingtou bqt zhuyin zy denglongshan dls and sanzao sz stations are available from 2015 to 2017 as shown in fig 2 the reaches downstream of mk station are characteristic of tidal water level oscillations and simultaneously influenced by runoff from the upstream area as the station locations approach the outlet the influence of runoff becomes weaker and the tidal oscillations are strengthened therefore the wz station is used as the reference location for river discharge and the sz station is used as the reference location for ocean tides in the ns tide model 3 methodology 3 1 ns tide unlike cha nsha ns tide considers the harmonic constant not truly static but variable according to the upstream river discharge and downstream tidal range and their interactions thus the following expression for tidal constituents was proposed by matte et al 2013 2014 1 η η 0 k 1 n a k cos σ k t b k s i n σ k t 2 η 0 c 0 c 1 q 2 3 t τ q c 2 r 2 t τ r q 4 3 t τ q 3 a k d 0 k c d 1 k c q t τ q d 2 k c r 2 t τ r q 1 2 t τ q 4 b k d 0 k s d 1 k s q t τ q d 2 k s r 2 t τ r q 1 2 t τ q where c i i 0 1 2 are the model parameters for the stage model d i i 0 1 2 are the model parameters for the tidal fluvial model q is the upstream discharge r is the downstream tidal range τ r and τ q are the time lags applied to the q and r time series respectively the superscripts c and s refer to the cosine and sine terms respectively and c i and d i i 0 1 2 are regression coefficients that can be determined through the iteratively reweighted least squares irls algorithm through ns tide the variable harmonic constants a k b k can be estimated through 3 and 4 which can be applied to predict the tidal levels in a tidal river and estuary 3 2 lstm a long short term memory network lstm hochreiter and schmidhuber 1997 gers et al 2014 is a special kind of recurrent neural network rnn however rnns cannot solve the long term dependency problem because the information perception ability is weak and the gradient disappears or explodes after multistage backpropagation to address this problem lstm uses gated units to control the flow and loss of information specifically in lstm three gates are introduced an input gate i t forget gate f t and output gate o t the cell state c t that characterizes long term memory and the candidate state c t waiting to be stored in long term memory are introduced the input gate determines how much of the information is stored in the current cell state in contrast the forget gate is applied to selectively forget the information in the cell state and the output gate is applied to selectively output the information in the cell state the expressions of the three gates are as follows 5 i t σ w i h t 1 x t b i 6 f t σ w f h t 1 x t b f 7 o t σ w o h t 1 x t b o where w i w f and w o represent the input gate forget gate and output gate parameter matrices to be trained respectively b i b f and b o are the bias items to be trained and σ represents the sigmoid activation function which controls the range of the threshold to be between zero and unit in addition to the three gates lstm uses a constant error carousel cec cell to prevent the gradient from exploding or vanishing and maintain long term memory the specific expressions are as follows 8 h t o t t a n h c t 9 c t t a n h w c h t 1 x t b c 10 c t f t c t 1 i t c t where h t is the short term memory which is obtained by the current cell state passing through the output gate c t is the long term memory which is the sum of the value of the long term memory at the previous moment passing through the forget gate and the new candidate value c t at the current moment passing through the input gate and the elementwise multiplication of two vectors is denoted with fig 3 shows the structure and workflow of the lstm neural network like rnn lstm computes a mapping from an input sequence x to an output sequence y by calculating the network unit activations using eqs 5 10 from time step t 1 to t τ 3 3 performance evaluation of the model 1 training loss 11 l o s s i 1 n y i y i 2 where y i is the measured value at time i and y i is the predicted value at time i this indicator is mainly used in the training phase of the neural network to evaluate the training results 2 the root mean square error rmse 12 r m s e i 1 n y i y i 2 n where y i is the mean value of y i 3 the coefficient of determination r2 13 r 2 i 1 n y i y i 2 i 1 n y i y i 2 i 1 n y i y i 2 in this study rmse and r2 are used to evaluate the model s accuracy by comparing the real and predicted values r2 measures the degree to which the outcomes are replicated by the model and an r2 score close to 1 is preferred rmse measures the prediction precision by summing up the squared errors and a value close to 0 is preferred in the article the rmse values used to evaluate different computational objects are distinguished by adding subscripts the residual value of the predicted water level elevation errors is expressed by rmser and the rmse value of the total water level elevation is expressed by rmset 4 analysis of the predictive errors of ns tide among the twelve hydrologic stations in fig 1 mk station and its downstream stations are in the typical tidal estuaries featured by the complex interaction of variable runoff and tides one year of the hourly water levels 8760 h in total is applied to regress the coefficients of the ns tide model moreover t tide is also used for comparison the results from the ns tide model with theoretical exponents and those with the optimized exponents and the t tide model are compared for the ns tide model with theoretical exponents the suggested values are based on matte et al 2013 2014 for the ns tide model with the optimized exponents the iterative process is used to determine the exponents with the minimum rmse values by using the iteratively reweighted least squares algorithm the rmse values for the three models are compared in fig 4 this indicates that both ns tide models can capture the fluctuations very well especially the model with the optimized exponents in contrast the t tide model can only obtain satisfying results at stations zy dls and sz in the lower downstream section of the estuary with the stations approaching upstream along the river where the effect of the discharge on water levels increases along the channel the errors of t tide grow rapidly although the errors of ns tide also increase upstream they grow very slowly which illustrates the applicability of the ns tide model in tidal rivers then the temporal variation in the predictive errors of the ns tide model relative to the measurements is investigated at three stations gm gz and th and the predictive errors are significant compared to the downstream stations as shown in fig 5 some short period fluctuations with periods less than 1 day can be found on the error curves the low pass filter is used to remove these short period fluctuations and obtain smoother error curves the maximum errors can reach 0 8 m and 1 1 m at the th station and the other two stations show similar trends but smaller peak errors the timings of the peak error mainly correspond to the peak river discharge these results confirm the findings of pan et al 2018 and gan et al 2019 that showed large predictive errors from ns tide are always related to subtidal fluctuations such as discharge to assess the correlation of the predictive errors of the ns tide model with other dynamic factors and considering the phase difference between the residual value and the input characteristics of each station the lag adjustment is made and a series of correlation analysis is conducted using pearson correlation coefficient the correlated dynamic factors include river discharge q offshore tidal range r principal tidal constituents q1 o1 p1 k1 n2 m2 s2 and k2 subtidal constituents msf and high frequency constituents m4 ms4 m6 and mn4 the results in fig 6 show that river discharge from upstream is the strongest correlated factor at all three stations the second most strongly correlated factor is not the same for the stations for the upstream station gm the subtidal constituent msf is second only to river discharge however for the downstream stations gz and th stations the principal tidal constituents and high frequency constituents are more correlated with the predictive errors of ns tide than the subtidal constituents 5 composition of the neural network 5 1 determination of the input layer as shown in fig 5 the predictive errors of the ns tide model can be considered as short period fluctuations superimposed on low frequency fluctuations these short period fluctuations are characterized by a random and stochastic process which can be predicted by their previous behavior and information a classical method for establishing the relation between further behavior and past behavior for a stochastic process is to use autoregress ar analysis which can determine the order p by calculating akaike s information criterion aic number through the order p we can determine how many previous time series should be used to capture the current and future behavior of the stochastic process fig 7 shows the results of the ar analysis they indicate that there are 2 sharp decreases in the aic number at p equating to 4 and 29 when p 4 and p 29 the clear improvement in model accuracy could be attribute to the influence of the one sixth diurnal tide m6 and the diurnal tidal group d1 when p increases to more than 50 the aic number remains nearly stable we also tested the other stations and the results are similar therefore p 50 is a sufficiently large order to capture the stochastic fluctuations for these time series for the neural network to predict the future values of the model error at time t 1 the past values at time t t 1 t 2 t 49 must be provided in the input layer in addition to the short period stochastic fluctuations the time series is also correlated with low frequency fluctuations such as the discharge q shown in fig 5 and q is also considered an optional node in the input layer therefore the model used in this study employs the predictive errors of the ns tide model and the discharge at the previous 50 times as input variables for predicting the future errors for correction 5 2 determination of the neural network structure it is well known that lstm networks can preserve previous information which can help improve the ability of the model to learn time series data however only one hidden layer is insufficient to fit the complex nonlinear fluctuations therefore we suggest a neural network with two hidden layers although the ffnn model does not have the ability to learn long term time series data it has a strong fitting ability and can be used as a fully connected layer in combination with lstm to obtain better results than those obtained using lstm alone for example zhang et al 2018 proposed a hybrid model composed of an lstm layer with another fully connected layer on top of it and showed that the model performed satisfactorily in predicting the water table depth they claimed that a fully connected layer atop the lstm layer helped the lstm to improve the learning and fitting ability of the model to determine the optimal neural network structure we compose three neural networks with double hidden layers shown in fig 8 to compare their performances a double lstm network is composed of two hidden layers and the other two neural networks are the hybrid of lstm and ffnn for comparison single hidden layer neural networks such as lstm and ffnn are also included in the experiment the performances of the five neural network structures are shown in fig 9 the comparison of the results indicates that all five neural networks predict consistent error results with that by ns tide during the training stage less than 248 days even including the single hidden layer neural networks however it makes a difference during the validation stage when the error curve changes abruptly marked in fig 9 a the prediction error also increases for the neural networks among the five neural networks the local zoomed curves shown in fig 9 b and c indicate that the lstm ffnn network provides the best predictions and the single hidden layer ffnn yields the worst predictions table 1 shows the performance statistics for the neural networks and the lstm ffnn obtains the lowest rmser and highest r2 5 3 determination of the output layer for the output layer the output nodes should be determined the output node number determines the length of the time series that can be predicted one at a time for the neural network for example if the output layer contains 10 nodes the time series values within a duration between t and t 9 can be predicted based on the past time series values when predicting the value at t 10 the previously predicted value at t should be used as input data consequently an increasing number of predicted values is used as the input values for further prediction in this recursive procedure and thus the estimate errors accumulate with time we carried out experiments by varying the output nodes and keeping the other parameters unchanged as shown in fig 10 a when the number of nodes in the output layer is small the model results fit the values by ns tide well within 12 h table 1 demonstrates that the model with only 1 output node obtains the smallest rmser values for the first 12 h however with the increase in the prediction time the errors of the model results with only 1 output node grow dramatically owing to the accumulation of previous errors fig 10 b traditional statistical models such as the ar model are all single valued functions therefore they have similar problems that limit their application in long term time series one of the advantages of the neural network is that it supports multiple output values corresponding to the output nodes by increasing the number of output nodes the accumulated errors can be controlled at a lower level and thus longer time series can be predicted by the model fig 10 shows that more output nodes indeed improve the neural network results especially after the first 12 h however increasing the number of output nodes too more does not improve the prediction results of the model because a model with too many output nodes would have poor fitting precision the rmser value of the model would increase as shown in fig 10 when the number of neuron nodes in the output layer is 9 or 12 there are also large errors between the prediction results and the actual value table 2 also indicates that when the number of output nodes exceeds 8 the rmser values of the predicted value increase markedly through a series of experiments we chose the model with 6 output nodes in the output layer which obtains the minimum rmser values in the overall process of the 72 hour prediction in addition to the specific structure of the model determined above it is necessary to optimize some other hyperparameters in the model such as the number of neuron nodes in hidden layers moreover preventing overfitting is a critical issue for all neural networks we optimize these hyperparameters in the process of composing the network those results will be introduced in the discussion section all relevant codes in this experiment are called and written from the tensorflow library and run on a computer with 16 gb of running memory and an intel core i7 11700 gpu 6 results in the previous section we determined the optimal neural network structure lstm ffnn as well as the number of nodes in the input layer and output layer then in this section a series of tests for assessing the performance of the proposed model and to comparing it with that of other models such as the ar model are described to determine whether and to what extent the discharge which is related to error fluctuations see fig 6 can be used to improve the prediction ability of the neural network iterations of the model including the input of discharge lstm ffnn q and excluding the input of discharge lstm ffnn are also compared in addition the neural network with one hidden layer lstm is included for comparison figs 11 13 depict the performances of the four models that predict the errors of the ns tide model for 3 days 72 h during the flood and dry seasons at the three stations the results indicate that although the errors of the four models grow with time the neural network with lstm ffnn q yields the smallest rmser and the slowest increase rate of rmser with time even up to 72 h the results of the model with lstm ffnn q are satisfactory in comparison the ar model obtains the largest rmser which grows dramatically with elapsed time the reason for this is that the ar model can only predict one value in the next hour at a time and the accumulated errors increase rather quickly in the recursive procedure for 72 h the model with one hidden layer lstm obtains better results than the ar model at the th and gm stations however lstm demonstrates similar performance to the ar model at gz station which is very puzzling because lstm is much more complex than the ar model and was predicted to obtain better results finally we focus on the comparison of models with lstm ffnn and lstm ffnn q this indicates that the model with discharge lstm ffnn q obtains obviously better results than that without discharge lstm ffnn for all three stations during both flood and dry seasons these results confirm our hypothesis that including correlated external information such as discharge into the time series based model can significantly improve the prediction capability of the neural network this is an advantage of neural networks over other statistical models such as ar models based on accurately evaluating the predictive errors of the ns tide model the proposed deep learning neural network lstm ffnn q is applied to correct and improve the prediction of ns tide the main method is to recursively apply the six output values of the model to the prediction of a 24 hour short period to observe the correction effect similar to those in fig 5 the hydrographs at the same stations are used to compare the correction effect using the proposed model according to the hydrodynamics the test period is divided into the dry season and flood season and in each season one typical month is chosen as shown in fig 14 through the short term 24 h correction the prediction accuracy of the ns tide model is greatly improved specifically the maximum predictive errors from the ns tide model without correction can reach up to 0 53 m 0 64 m and 0 71 m for the th gz and gm stations respectively in the dry season after correction using the proposed neural network lstm ffnn q the maximum predictive errors are reduced to 0 06 m 0 08 m and 0 12 m respectively in the flood season the maximum predictive errors can also be decreased from 0 94 m 0 90 m and 0 87 m to 0 40 m 0 46 m and 0 38 m for the th gz and gm stations respectively on average after correction the rmse r can be reduced from approximately 0 3 m to less than 0 1 m more than two thirds of the reduction of the errors from the ns tide model in particular accurate prediction of extreme water levels is of great importance for the water sector in the fields of risk assessment disaster prevention and water management the extreme water level here refers to the maximum and minimum values within a certain time interval when the water level fluctuates we collected these extreme water levels at the th gz and gm stations and compared the predictions with observations fig 15 the results show that without correction ns tide tends to underestimate extreme water levels while after correction by the proposed model the agreement between prediction and observation is significantly improved r2 values from 0 948 to 0 997 therefore the results illustrate that the proposed correction method can capture the dramatic fluctuations of the peak levels during the period of abrupt alteration in discharge and effectively improve the accuracy of the ns tide model in the prediction of extreme water levels in the next 24 72 h 7 discussion in addition to the neural network structure determined in section 5 some other hyperparameters also influence the model performance which will be discussed in this section the first hyperparameter investigated is the number of neuron nodes in the hidden layer theoretically increasing the number of neuron nodes can increase the fitting capability however if the number of neuron nodes is too large the test results of the model may be overfitted taking the model results at gm station as an example as shown in table 3 when the number of neuron nodes increases from 10 to 40 the loss decreases from 3 315 to 1 151 rmser decreases from 0 0578 to 0 0485 and r2 increases from 0 907 to 0 935 however when the number of neuron nodes increases to 60 and 100 although the loss number continues to decrease rmser increases while r2 decreases this indicates that overfitting occurs when the number of neuron nodes is more than 40 fig 16 shows one of the overfitting phenomena at gm station during the calibration process the number of output nodes here is consistent with the best output node of the model used in this paper which is 6 h an obvious feature of overfitting is that the absolute errors in the predicting phase are larger than those in the training phase by using a dropout layer to randomly drop out some neurons in the hidden layer the prediction can be improved and overfitting is prevented see table 4 table 4 shows that when the dropout coefficient is 0 1 the overfitting phenomenon of the model is alleviated to some extent but when the dropout coefficient increases the rmser value of the model also gradually increases and r2 gradually decreases this phenomenon may be caused by insufficient sample data and the high complexity of the model used even when the dropout coefficient is greater than 0 5 there are too many neural nodes that fail in the input layer of the model which gradually leads to the phenomenon of underfitting of the model learning rates also affect the fitting results of the model when the learning rate is high the network may not converge and the optimal value is ignored during the training process when the learning rate is low it may slow down the convergence speed and increase the time required to find the optimal value as shown in table 3 learning rate 0 01 is the optimal value with less rmser and larger r2 values than 0 1 and 0 001 epoch denotes how many times all training data are used in the training process when the training times are small appropriately increasing the training times of the entire training set may improve the fitting accuracy of the model however when there are too many training iterations in the training set the accuracy of the model is not necessarily improved which may also reduce the accuracy of the model and the training time of the model increases the model results at gm station table 3 indicate that epoch 300 is a suitable value for obtaining the lowest rmser and best r2 values in water level prediction the number of lstm layers will also have different effects on the fitting effect of the model in this study three years of data are used to divide the training set and test set data of different lengths due to the limitation of the amount of data the training set and test set are divided in the ratio of 70 and 30 of the whole data this is to judge the impact of the number of lstm layers on the model effect and the data used for testing are not involved in model training as shown in table 5 the single layer lstm model has better prediction performance the model presented in this paper is designed based on the characteristics of the input data which is a time series and the successful performance of lstm in time series prediction lstm model alleviates the problem of gradient disappearance in rnns however it is important to note that when the sequence length is too long the gradient may still disappear hence attention must be given to the length of the data sequence used when selecting and using the model if the amount of data contained is large the model structure suitable for its operation may be more complex as there are more controllable parameters making the complex model more flexible and powerful using a complex model with more parameters for training when the amount of data used is small may result in over fitting which is discussed in the article determining the most suitable model requires considering the specific data used and its characteristics as well as evaluating the results obtained using different models 8 conclusions in this study we proposed an lstm based method to estimate and correct the predictive errors of the ns tide model by analyzing the errors from ns tide we found that the errors were positively correlated with discharge when the discharge was large and changed abruptly the predictive errors of the ns tide model also rose greatly to solve this problem we composed a deep learning neural network through a series of experiments to compare and determine the optimal structure we found that the model with a network structure named lstm ffnn q performed satisfactorily in estimating the short term predictive errors of the ns tide model the rmser and r2 values showed that the model lstm ffnn q obtained much better results than the ar model and the neural networks with single layer lstm and double layer lstm ffnn without q through the correction of our proposed model the accuracy of the ns tide model improved significantly with a reduction of approximately two thirds for rmset on average in particular the prediction of the extremely level which is a crucial indicator for water conservancy management was much more accurate than that before the correction finally we tested and discussed how to optimize the neural network to prevent overfitting and improve efficiency we found that if the neuron number in a hidden layer was too large the dropout layer method could be applied to prevent overfitting credit authorship contribution statement zhuo zhang conceptualization methodology investigation writing original draft lu zhang methodology investigation software writing original draft songshan yue methodology investigation writing review editing jiaxing wu validation investigation software visualization fei guo resources investigation software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china 42171465 42171406 key laboratory of ministry of education for coastal disaster and protection hohai university 202218 and the open research fund of key laboratory of flood drought disaster defense the ministry of water resources kyfb202112010704 
1778,extreme hydrological events have been recorded around the world confirming the changing hydroclimate and therefore threatening the reliability over a planning horizon of the existing and projected hydraulic infrastructure unfortunately there is a gap between climate science and engineering planning and design which is partially filled in this study whose aim is to evaluate the expected reliability over a planning horizon of a hydraulic infrastructure and its uncertainty for hydroclimatic projections to achieve this goal we implemented hydrological deep learning models that are driven by the coupled model intercomparison project phase 6 cmip6 after validating and testing the hydrological deep learning models sample series of the annual maximum flood amf were generated for different mountain rivers of chile and used for a standard frequency analysis that updates the exceedance probability of the flood events used for the hydraulic design we found that the mean and the standard deviation of the sample series of the amf are the key parameters that enable quantifying temporal changes in the exceedance probability of the design flow the cumulative effects of these changes are summarized by the projected reliability over the planning horizon of the infrastructure and the age of the infrastructure at which this projected reliability equates to the design reliability quantifies the impact of climate change on the expected security of the hydraulic project we show that the infrastructure that was designed for the smaller exceedance probability is more vulnerable to climate change where the updated planning horizon can be as small as 45 of the planning horizon used for the design keywords projected reliability uncertainty hydraulic infrastructure climate change hydrological deep learning model cmip6 models data availability this article uses data from the cmip6 models and the hydrological observations of the chilean government that is available from https snia mop gob cl bnaconsultas reportes 1 introduction extreme hydrological events have been recorded around the world confirming the hydroclimate is changing e g fischer knutti 2016 moustakis et al 2021 olafsdottir et al 2021 future climate projections indicate possible changes in the frequency duration and intensity of climate extremes barbero et al 2019 olsen 2015 thus raising concerns about traditional engineering designs serinaldi kilsby 2015 in particular hydraulic infrastructures are projected to have long service lives in most cases more than 100 years thus engineers should update the design standards to construct infrastructure with low risks of failure chester et al 2020 olsen 2015 quintero et al 2018 unfortunately there is a gap between climate science and engineering planning and design as more damage from extreme hydrological events has been associated with damage to civil infrastructure this should be a pressing concern quintero et al 2018 then the challenge is to find a balance between high cost low risk overdesign and low cost high risk underdesign wasko et al 2021 which can be achieved by transforming complex hydroclimatic models into simpler rules that can be applied in engineering practices and standards with low uncertainty kundzewicz licznar 2021 under the assumption of stationarity the common engineering practice for hydroclimatic extremes assumes that extreme events are independent and identically distributed salas et al 2018 so that there is no change in the mean and variance of the sample series of extreme events used for the design slater et al 2021 this assumption is no longer valid because beyond climate change streamflow series have been changing due to variations in land use and water infrastructure salas et al 2018 even under natural variability the extension of the recorded time series is not sufficient to capture natural scales of climate variability razavi et al 2020 under this premise many authors have argued that it is no longer valid to use the stationarity assumption in traditional flood frequency analysis quintero et al 2018 salas et al 2018 vogel et al 2011 however there is no consensus in practical guidance for engineering design under changes in hydroclimatic extremes see françois et al 2019 and the references therein under nonstationary conditions the major difficulty is representing a probability of exceedance that is changing over time in this context the nonstationarity of the time series can be assessed by assuming stationarity within a time window of a short length from which the extreme values can be calculated e g emmanouil et al 2022 françois et al 2019 schlef et al 2018 and moving in time repeating the same calculation for which a long series of data greater than the recorded historical data are needed following this idea terms such as the return period lose meaning then it has been recommended to replace it with the term reliability over a planning horizon which means that an extreme event will not occur in a service life of n years read vogel 2015 in fact changes in the frequency duration and intensity of hydroclimate extremes change the reliability of a project thus showing a clearer description of the expected security of the civil infrastructure stephens et al 2022 to obtain larger data series for projected hydrological frequency analysis complex hydroclimatic models which increase the uncertainty are needed serinaldi kilsby 2015 in most cases the top down approach which consists of downscaling general circulation models gcms and then using these results as boundary conditions in hydrological models tramblay et al 2014 is used in this approach the uncertainty of the gcms is transferred into the hydrological model that is already uncertain because of multiple fitting parameters quintero et al 2018 to this end more uncertainty is added from the multiple emission scenarios and from the hydrological frequency analysis quintero et al 2018 in this context to have greater credibility of the model results it is recommended to use an ensemble of different gcms and hydrological models as well as to evaluate if the final results reproduce the historical hydroclimatic conditions of the study area françois et al 2019 in the same way the modeling approach should be as simple as possible according to the parsimony principle serinaldi kilsby 2015 in this context hybrid dynamical statistical approaches are particularly useful for hydroclimatic extremes slater et al 2021 and hybrid dynamical data driven approaches have shown better results and should be used de la fuente et al 2019 the advantages of these new approaches are that they allow the development of models with low error as well as the modeling of ensembles of future projections due to low calculation costs in this way instead of using a limited range of possible outcomes all gcm projections and hydrological models can be evaluated in a particular project which allows calculating uncertainty in a range of model results thus improving the information for decision making stephens et al 2022 in this article we developed a modeling approach for evaluating the expected reliability over a planning horizon of a hydraulic infrastructure and its uncertainty for hydroclimatic future projections this approach was applied in several mountain rivers of chile covering the different hydroclimatic regimes that characterize the andes rivers following the hybrid dynamical data driven approach described in de la fuente et al 2019 we started from a traditional hydrological model which was used to define the conceptual model and the hydrometeorological variables that were used in the training of the data driven models the meteorological variables were obtained through statistical scaling of the cmip6 climate projections eyring et al 2016 for the historical reference period of 1981 2020 to the future projection period of 2021 2060 a deep learning dl approach based on long short term memory lstm cells was implemented historical recorded daily stream flows were used to train the nets which were then validated and tested by passing the results of the hydrological deep learning model hdl model through sequence filters that discarded the trained nets that were not suitable for the frequency analysis of the annual maximum flood amf finally the model results for future projections were compared using frequency analysis for the amf distributions and these results were used to evaluate the time varying reliability and uncertainty the organization of this paper is as follows in section 2 we first introduce the basic concepts of the projected frequency analysis of amf then we describe the studied rivers and detail the deep learning structure and the adopted methodology to train validate and test the nets in the results section the validation and test of the adopted nets is presented then we present the results associated with changes in the exceedance probability of the design flow event and finally we show the cumulative effects of these changes in the exceedance probability that are transferred to the reliability of a hydraulic infrastructure over the planning horizon 2 materials and methods 2 1 projected reliability of a hydraulic infrastructure the standard methodology for obtaining the magnitude of the flow used for designing a hydraulic infrastructure is depicted in fig 2 a which shows a plot of the sample series of the annual maximum flood or the logarithm of this quantity depending on the probability density function pdf as a function of the frequency factor k p the value of k p depends on the exceedance probability p and the pdf such as p and k p are inversely related variables i e kp increases as p decreases naghettini 2016 stedinger et al 1993 on the other hand k p is defined as 1 k p x p μ σ where x p denotes the flow associated with p or the log 10 of the amf if it corresponds and μ and σ are the mean and the standard deviation of the sample series of the amf respectively therefore if the pdf fits to the sample series the resulting plot is a line whose slope is σ and intercept is μ following this the exceedance probability used for the design event herein referred to as p is determined based on techno economic analysis or normative recommendations and usually it is associated with extreme events that have not been observed in the past thus necessitating extrapolation chow et al 2010 furthermore once the design exceedance probability is known the design reliability of the infrastructure over the planning horizon can be computed as 2 r d 1 p n where r d is the design reliability equal to 1 minus the risk and n is the planning horizon of the infrastructure measured in number of years however assuming stationarity is no longer valid the projected reliability of hydraulic infrastructures should be used instead of r d here we adopted the definition by salas obeysekera 2014 see also read vogel 2015 who computed the projected reliability r p as 3 r p i 1 n 1 p i where p i is the exceedance probability of the design flow event for year i which was computed by assuming stationary conditions in the window of time between years i 40 and i therefore the standard frequency analysis can be repeated for the sample series of the amf in this window of time however the relevant question is now what exceedance probability is associated with the design flow x p whose value depends on the μ and σ values of the sample series of the window of time that is different from the corresponding values of the sample series used for the design if the average of the sample series increases with respect to the value of the design series fig 2b the new updated frequency factor is reduced and the exceedance probability increases a similar response is observed with the standard deviation of the sample series as shown in fig 2c the exceedance probability of the design flow increases with σ the length of the window of time used for the frequency analysis was defined based on standard guidelines that defines as 30 years of continuous hydrological data as the minimum information needed for conducting the frequency analysis naghettini 2016 stedinger et al 1993 furthermore the size of the amf limits the minimum exceedance probability that can be extrapolated naghettini 2016 which justifies the use of the longest amf that is available however the availability of information limited us to define 40 years as the size of window of time used for the frequency analysis in both the past and the future projections finally the uncertainty of the computed r p depends on the uncertainty of the values of p i which is determined based on the 95 confidence interval in the calculation of p i as a function of x p if the confidence interval is defined as p i b p i p i t the confidence interval of the computed value of r p r p b r p r p t is estimated by a first order taylor expansion as 4 r p t b r p 1 i 1 n p i b t p i 1 p i note that the upper limit of p i is associated with the smallest values of reliability or higher risks 2 2 study rivers and climate models in this article we analyzed the 15 river stations that are located along the latitudinal gradient of chile as shown in fig 2 each of these stations has observations of the daily average flow between 1980 and 2020 and the geographic features of the catchments are given in table 1 these stations were grouped according to the classification of macrozones of the military geographic institute of chile which subdivides chile into 5 climatic zones from the arid north zone 1 and the austral region where most of the rivers are fed by glaciers the difference between zones 3 and 4 is that the hydrology of zone 4 is modulated by large lakes that regulate the peak flows on the other hand we used the cmip6 multimodel ensemble of the ssp5 rcp8 5 scenarios which are the worst case climate projections and the corresponding historical simulation for the past climate we used 7 cmip6 models with daily air temperature information at different levels and precipitation rates awi cm 1 1 mr bcc csm2 mr canesm5 cmcc cm2 sr5 cnrm cm6 1 hr access sm1 5 and ec earth3 eyring et al 2016 fig 3 summarizes the projected changes in the input information to any hydrological model precipitation and catchment area on the one hand the columns a and b of fig 3 show the temporal evolution of the 40 year window average and standard deviation of the sample series of annual maximum precipitation amp in 24 hrs where each line corresponds to the average of all the climate models respectively as it was shown in fig 1 the exceedance probability of extreme events is controlled by both the average and the standard deviation of the sample series and fig 3 shows that despite of no significant changes are expected in the average of the sample series of amp the standard deviation of the sample series is projected to increase due to the appearance of extreme events that increase the standard deviation of the sample series thus increasing the exceedance probability of extreme events and reducing the reliability or increasing the risk we also computed the elevation of the 0 c isotherm that defines the area of the pluvial catchment and with this information we computed the pluvial area defined as the catchment area below this elevation the annual average of the pluvial area normalized by the catchment area is shown in the third column of fig 3 where the expected increase in the pluvial area due to climate change is observed the increase in the magnitude of the amf is expected in response to the increase in the pluvial area 2 3 hydrological deep learning models here we implemented a hydrological deep learning model hdl model for computing the daily flows associated with each of the cmip6 simulations the structure of the hdl model used in this article is shown in fig 4 and is similar to the structure of de la fuente et al 2019 the structure of the model is divided into three regions the first region captures the temporal correlation with a sequence of n lstm long short term memory lstm layers followed by a dropout layer to avoid model overfitting the second region is a series of n fcl fully connected layers followed by the relu activation function and the last region is the output layer that has one fully connected layer with one output that is followed by a regression layer where the loss is computed other applications of hdl models can be found in hu et al 2018 ma et al 2021 or shen 2018 while the limitations of this approach are on the information needed for training and validating the hdl and the risk of overfitting the model that leads to bad performance in predicting the catchment response under conditions that are observed in the training dataset in this application the availability of information defined the river stations used in this article while the risk of overfitting is limited with the validation procedure that it detailed in the following subsection the aim of the hdl model is to predict the daily average flow for day 8 in response to the meteorological and hydrological information for day 8 and the preceding 7 days consequently the input data are arranged in a matrix of 8 columns and 6 rows where each column is a day and the different rows are the input variables daily precipitation pluvial area elevation of the 0 c isotherm length of the stream in the pluvial area maximum elevation of the pluvial area and average slope of the stream note that the maximum elevation of the pluvial area is not equal to the elevation of the 0 c isotherm when the latter is located above the maximum elevation of the entire catchment in the context of fig 4 the inputs x 1 to x 8 are the different columns of the input matrix the output of the model is one row with 8 flows h 1 to h 8 where the last value h 8 is used as the predicted value of the corresponding day in the calendar the structure of the hdl model is specified by four hyperparameters n lstm n fcl the number of lstm units in each lstm block m lsmt and the number of neurons in the fully connected layers m fcl instead of optimizing the value of these hyperparameters we adopted the random search technique that trains the model with random values of these hyperparameters bergstra bengio 2012 then during the validation and testing of the trained hdl models we evaluated the performance of the models with different combinations of hyperparameters to determine if they are suitable for use in the analysis the hyperparameters n lstm and n fcl take values between 1 and 5 while m lsmt and m fcl take values between 50 and 300 for each river 30 different models with random hyperparameter combinations were trained 2 4 training validating and testing of the hydrological deep learning models the adopted methodology to train validate and test the hydrological deep learning models is described as follows first the era5 reanalysis hersbach et al 2020 was used to obtain the input matrices for training the 30 hdl models of each river and the measured flows at each station were used to compute the error in the simulation that was minimized with the backpropagation algorithm we used the entire set of flow observations for training the hdl models and we validated and tested the resulting model with the cmip6 climate models to define which trained net will be used to compute the hydrological response associated with which cmip6 model there were potentially 210 combined cmip6 and hdl models for each river and each combination was passed through a sequence of filters to determine if it was suitable for use in this article to validate the trained models we first implemented the multivariate bias correction method of cannon 2018 which allows matching the multivariate distribution of the input information of the cmip6 models to the input information obtained from the era5 reanalysis from 1980 to 2020 then we evaluated the trained nets with the cmip6 information and computed the frequency distribution curve of the daily average flows and monthly average flow measured we tested the goodness of fit between the observed and simulated frequency distributions with the kolmogorov smirnov test naghettini 2016 and evaluated the seasonal coherence in the monthly average flows by computing the kling gupta efficiency score gupta et al 2009 the trained models that passed the kolmogorov smirnov test and had kge values larger than 0 7 were finally selected for testing the frequency analysis of the amf the set of these models is hereinafter named the cmip6 hdl model finally the selected cmip6 hdl models were tested by performing a frequency analysis of the extreme events first we tested 7 different pdfs and discarded those that did not fit the observed sample series of amf with the χ 2 test then we selected the pdf that better represented the observed sample series of amf in the river as the one whose predicted values had the largest kge with respect to the observed values seven pdfs normal pearson iii exponential gumbel log normal and log pearson iii were tested chow et al 2010 naghettini 2016 stedinger et al 1993 once a pdf was selected for the observations we conducted a frequency analysis of the extreme events for each of the available cmip6 hdl models and plotted the scatter plot of x p μ σ as a function of k p where μ and σ are the mean and standard deviation of the corresponding sample series of amf of the different cmip6 hdl models and for the historic period between 1980 and 2020 respectively this exercise yielded a cloud of scatter points that follows the same lineal tendency of the observed sample series and then by fitting a line to this cloud the magnitude of flows for the entire set of cmip6 hdp models can be computed for different exceedance probabilities as well as their corresponding uncertainty finally we computed the flows associated with different exceedance probabilities between 0 0001 and 0 2 using the observations and the cmip6 hdl models and the kge of these two datasets quantifies the capability of using the cmip6 hdl models for the frequency analysis of extreme events 3 results 3 1 validation and test of the cmip6 hdl models the results of the validation filter described in the previous section are shown in fig 5 where each column corresponds to different rivers that are representative of the different regions defined in fig 2 the first row plots the monthly average flow and the second row plots the frequency distribution of daily average flow measured and simulated by the selected cmip6 hdl models as a result of this first validation filter a large proportion of the 210 possible cmip6 hdl models that are available for each river were discarded and detailed information is given in the second and third columns of table 2 once the set of cmip6 hdl models was defined their capability for reproducing the frequency analysis of extreme events was tested and the obtained results are shown in fig 6 the selected pdf for each river is detailed in the fourth column of table 2 and the fifth column indicates the value of the kge score of the fit between observations and the selected pdf the gray dots in fig 6 correspond to the simulated amf from the cmip6 hdl model and the goodness of this fit is given in the last column of table 2 note that the magnitude of the flows plotted in fig 6 are expressed in m3 s and not in the dimensionless form of eq 1 for which the average and the magnitude of the measured sample series were used after this test none of the selected cmip6 hdl models were discarded 3 2 projected changes in the exceedance probability of the design events the selected set of cmip6 hdl models was used to evaluate the projected changes in the exceedance probability of extreme events as an example of the changes that can occur fig 7 shows a plot of the same curves as those in fig 6 but for the 2020 2060 period changes in both the magnitude of the extreme events and the uncertainty are observed on the one hand the projected changes in the magnitude of extreme events occur in response to changes in the slope and the mean of the sample series of amf that translate and or rotate the curve as detailed in fig 1 in the context of the fitted lines in fig 7 if the black solid line is located above the gray dashed line an increase in the exceedance probability of the design flow is expected and the magnitude of this change is quantified as the horizontal gap between these two lines the increase in the magnitude of p i is given in the same figure where the ratio p i p can take values as high as 10 6 in river 1 and for the smaller values of p 0 0001 to complement this general description fig 8 a and b show plots of the frequency distributions of the mean and standard variation of all the computed sample series normalized with respect to the values of the sample series for the historical period between 1980 and 2020 the projected changes in μ and σ are expected to be in the range between 0 9 and 1 2 times the values used for the historical period however these relatively small changes in μ and σ have a large impact on the exceedance probability as shown in fig 8c which shows a plot of the relative change in p i for all rivers and years between 2030 and 2060 the adopted p in fig 8c was obtained by setting the design reliability in eq 2 equal to 95 for planning horizons of 5 10 20 50 and 100 years as a summary of these results illustrated in fig 8c we also found that because of changes in σ the magnitude of the change in p i with respect to p depends on the design probability of exceedance p in particular a smaller p has a larger difference between p i and p this pattern suggests that hydraulic infrastructures that were designed for the highest flows smaller exceedance probabilities are those that can be more vulnerable to small changes in the climatic conditions described by μ and σ with respect to conditions observed during the design on the other hand the uncertainty in the calculations also increases in the future the uncertainty in the projected values of p i for the design flow x p increases because of the disagreement among the different projected flows graphically this is reflected by the increase in the size of the cloud compare the cyan and blue points in fig 7 the impact of this disagreement among the cmip6 hdl models with respect to the 95 confidence interval of the updated values of p i is described in fig 8d which shows that the future uncertainty between 2030 and 2060 can be as high as 15 times the current uncertainty 3 3 projected changes in the reliability over a planning horizon the projected reliability defined in eq 3 summarizes the cumulative effects that changes in p i have over the planning horizon of the hydraulic infrastructure while eq 4 enables propagating the confidence interval of p i on the projected reliability the corresponding time series of the design and projected reliability and its confidence interval for the studied rivers are shown in fig 9 the selected value of p for these curves was associated with a design reliability of 95 and a planning horizon of 20 years eq 2 first the reliability of the infrastructure decreases with the age of the infrastructure because of the probability of not having a flow event that exceeds the design event decay with the age according to eq 2 the projected reliability also decreases with the age of the infrastructure however depending on the updated p i the rate of reduction can be faster or slower than the expected rate by design in the context of the studied rivers the reliability of the rivers located in zones 1 2 4 and 5 is expected to decrease in time at a faster rate than the stationary reliability while in zone 3 the nonstationary reliability shows a similar tendency to what is associated with the design conditions one way to summarize the accumulated impact of changes in p i on the reliability of hydraulic infrastructure is by looking at the age of the structure in which the projected reliability is equal to the design reliability computed by eq 2 this age can be understood as the updated planning horizon and the ratio between this age and the planning horizon summarizes the cumulative impact of changes in p i on the hydraulic infrastructure for the studied rivers of this article this ratio is equal to 0 74 0 18 for n 20 yr and 0 46 0 18 for n 100 yr therefore the design reliability is reached on average 5 years before the expected value for structures that are meant to operate for 20 years while for structures designed to last 100 years the design reliability can be reached as early as 50 years before the planning horizon consequently this reinforces the finding in this study that the cumulative impact of changes in p i over the planning horizon of the infrastructure is more pronounced for those structures that were designed for highly infrequent events 4 discussion and concluding remarks in this article the cmip6 models were coupled to deep learning hydrological models to obtain the projected sample series of amf in different mountain rivers of chile these sample series of amf were used to evaluate the cumulative effects that projected changes in the climate have on the reliability of hydraulic infrastructures over their planning horizon and their uncertainty the obtained results of these cmip6 hdl models suggest that the more vulnerable structures in terms of expected changes in the reliability are those that were designed for the larger planning horizon and therefore for the smaller exceedance probability of the design flow event the cumulative effects are summarized by looking at the age of the structure in which the design reliability is reached this value can be understood as the updated planning horizon of the structure and if it is normalized by the planning horizon a simple dimensionless index can be obtained and used to quantify the magnitude of the cumulative impact of climate change on the hydraulic infrastructure which can be used to prioritize sites plan future maintenance and focus surveillance among other operative or normative applications in this context the open challenge that motivates this article is to connect complex hydroclimatic models in a changing climate with the applied engineering practices that are used to define the magnitude of flood events which are finally used to design hydraulic structures that are planned to operate for 100 years or more kundzewicz licznar 2021 in this way a methodology to address the gap between science and engineering practice was derived for accounting for the cumulated changes of p i over the planning horizon of the hydraulic infrastructure and based on the projected reliability definition by salas obeysekera 2014 it was possible to show that the extrapolation needed to obtain the updated exceedance probability of the very infrequent extreme events of the design conditions is very sensible in terms of small changes in the standard deviation and the average of the sample series of amf that can change with the appearance of only one extreme event fig 1 the cumulative changes in p i show that the updated planning horizon of the structure can be reduced to 0 75 n for structures with a design planning horizon of less than 20 years and it can be as small as 0 4 n when the projected reliability is evaluated for infrastructures that are meant to operate for 100 years or more moreover an extended application of this idea of using the temporal evolution of the projected reliability to evaluate the impact of climate change on existing infrastructure is to update p i by simply looking at the new measurements conducted after the construction of the hydraulic infrastructure structures that were built early this century already have 20 years of operation and important changes may have already occurred in the sample series used for the frequency analysis in particular the recent increase in extreme events with magnitudes larger than the magnitude observed in the historical records used for the design simultaneously increases the mean and the standard deviation of the sample needed for the frequency analysis and both changes act in favor of increasing the exceedance probability of the flow event used for the hydraulic design furthermore we used deep learning models that are actively being introduced in hydroclimatic applications as they enable the use of a large amount of data to generate several hydroclimatic projections without significant computational requirements or a deep understanding of the physical processes that explain the catchment response the cmip6 climate models for the worst case scenario ssp5 rcp8 5 were coupled to these hydrological deep learning models that were based on lstm layers which have the advantage of capturing temporal correlations among the model input variables de la fuente et al 2019 hu et al 2018 ma et al 2021 shen 2018 the risk is however overfitting models that lack the flexibility to predict the catchment response under conditions that are observed in the training dataset therefore the validation and test of the trained net are vital stages that should be carefully designed to evaluate the actual performance of the trained net in this application the trained hdl models were used with the cmip6 models as input meteorological variables and the validation and test processes were a sequence of filters that were applied to the generated series of daily average flows 210 in each river 30 trained nets and 7 cmip6 models first we applied a statistical filter that discarded those hdl models whose frequency distribution of daily flow did not fit to the observed curve then we applied a process filter that filtered out the nets that did not reproduce the seasonal variability of the monthly average flow and finally we conducted a frequency analysis of extreme events for testing if the selected cmip6 hdl models were suitable for this analysis after these filters most of the 210 computed time series of daily average flow were discarded in fact we trained with 18 rivers 3 of which did not pass these filters however the adopted validation and test methodology based on filters gave a set of cmip6 hdl models that were shown to be good at reproducing the probabilistic distribution of extreme events fig 6 and are capable of being used for projecting near future conditions of the worst case scenario one of the main conclusions of this adopted validation and test methodology based on filters is that to have a well validated trained net based on observed data era5 in this article it is not sufficient to ensure that the trained net will perform well once it is evaluated with the cmip6 models in some cases the computed monthly average flows had no physical meaning in other cases the simulated curves of the frequency distribution of the daily average flows did not fit at all with the observed curves and in other cases the trained hdl model was suitable to be used only with some of the available cmip6 models therefore the sequence of filters was shown to be a cost efficient methodology that avoids an overfitted model as it selects a collection of several good structures instead of searching for the best structure of the hdl model this approach also enables the quantification of the uncertainty in the future projection that strengthened the frequency analysis in summary in this article we detailed a methodology for quantifying changes in the reliability of hydraulic infrastructure in a changing climate in the context of the applied engineering practices that are needed to compute the magnitude of the flow used for hydraulic design some of the implications of our findings are that in the absence of reliable information climate change can be included in hydraulic design based on recommended factors that amplify μ and σ of the sample series of amf used for the frequency analysis furthermore the updated planning horizon summarizes the cumulative changes in the exceedance probability of the design flow and it can be estimated based on the observed sample series amf after the design of the structure finally scientific efforts should also be made to generate a collection of projected series of flows that can be directly used in applied engineering which can follow the standard methodologies used for hydraulic designs and or maintenance deep learning is a cost effective technology that can be used to generate the projected time series but it should be properly validated and tested for calculation purposes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this article uses data from the cmip6 models and the hydrological observations of the chilean government that is available from https snia mop gob cl bnaconsultas reportes 
1778,extreme hydrological events have been recorded around the world confirming the changing hydroclimate and therefore threatening the reliability over a planning horizon of the existing and projected hydraulic infrastructure unfortunately there is a gap between climate science and engineering planning and design which is partially filled in this study whose aim is to evaluate the expected reliability over a planning horizon of a hydraulic infrastructure and its uncertainty for hydroclimatic projections to achieve this goal we implemented hydrological deep learning models that are driven by the coupled model intercomparison project phase 6 cmip6 after validating and testing the hydrological deep learning models sample series of the annual maximum flood amf were generated for different mountain rivers of chile and used for a standard frequency analysis that updates the exceedance probability of the flood events used for the hydraulic design we found that the mean and the standard deviation of the sample series of the amf are the key parameters that enable quantifying temporal changes in the exceedance probability of the design flow the cumulative effects of these changes are summarized by the projected reliability over the planning horizon of the infrastructure and the age of the infrastructure at which this projected reliability equates to the design reliability quantifies the impact of climate change on the expected security of the hydraulic project we show that the infrastructure that was designed for the smaller exceedance probability is more vulnerable to climate change where the updated planning horizon can be as small as 45 of the planning horizon used for the design keywords projected reliability uncertainty hydraulic infrastructure climate change hydrological deep learning model cmip6 models data availability this article uses data from the cmip6 models and the hydrological observations of the chilean government that is available from https snia mop gob cl bnaconsultas reportes 1 introduction extreme hydrological events have been recorded around the world confirming the hydroclimate is changing e g fischer knutti 2016 moustakis et al 2021 olafsdottir et al 2021 future climate projections indicate possible changes in the frequency duration and intensity of climate extremes barbero et al 2019 olsen 2015 thus raising concerns about traditional engineering designs serinaldi kilsby 2015 in particular hydraulic infrastructures are projected to have long service lives in most cases more than 100 years thus engineers should update the design standards to construct infrastructure with low risks of failure chester et al 2020 olsen 2015 quintero et al 2018 unfortunately there is a gap between climate science and engineering planning and design as more damage from extreme hydrological events has been associated with damage to civil infrastructure this should be a pressing concern quintero et al 2018 then the challenge is to find a balance between high cost low risk overdesign and low cost high risk underdesign wasko et al 2021 which can be achieved by transforming complex hydroclimatic models into simpler rules that can be applied in engineering practices and standards with low uncertainty kundzewicz licznar 2021 under the assumption of stationarity the common engineering practice for hydroclimatic extremes assumes that extreme events are independent and identically distributed salas et al 2018 so that there is no change in the mean and variance of the sample series of extreme events used for the design slater et al 2021 this assumption is no longer valid because beyond climate change streamflow series have been changing due to variations in land use and water infrastructure salas et al 2018 even under natural variability the extension of the recorded time series is not sufficient to capture natural scales of climate variability razavi et al 2020 under this premise many authors have argued that it is no longer valid to use the stationarity assumption in traditional flood frequency analysis quintero et al 2018 salas et al 2018 vogel et al 2011 however there is no consensus in practical guidance for engineering design under changes in hydroclimatic extremes see françois et al 2019 and the references therein under nonstationary conditions the major difficulty is representing a probability of exceedance that is changing over time in this context the nonstationarity of the time series can be assessed by assuming stationarity within a time window of a short length from which the extreme values can be calculated e g emmanouil et al 2022 françois et al 2019 schlef et al 2018 and moving in time repeating the same calculation for which a long series of data greater than the recorded historical data are needed following this idea terms such as the return period lose meaning then it has been recommended to replace it with the term reliability over a planning horizon which means that an extreme event will not occur in a service life of n years read vogel 2015 in fact changes in the frequency duration and intensity of hydroclimate extremes change the reliability of a project thus showing a clearer description of the expected security of the civil infrastructure stephens et al 2022 to obtain larger data series for projected hydrological frequency analysis complex hydroclimatic models which increase the uncertainty are needed serinaldi kilsby 2015 in most cases the top down approach which consists of downscaling general circulation models gcms and then using these results as boundary conditions in hydrological models tramblay et al 2014 is used in this approach the uncertainty of the gcms is transferred into the hydrological model that is already uncertain because of multiple fitting parameters quintero et al 2018 to this end more uncertainty is added from the multiple emission scenarios and from the hydrological frequency analysis quintero et al 2018 in this context to have greater credibility of the model results it is recommended to use an ensemble of different gcms and hydrological models as well as to evaluate if the final results reproduce the historical hydroclimatic conditions of the study area françois et al 2019 in the same way the modeling approach should be as simple as possible according to the parsimony principle serinaldi kilsby 2015 in this context hybrid dynamical statistical approaches are particularly useful for hydroclimatic extremes slater et al 2021 and hybrid dynamical data driven approaches have shown better results and should be used de la fuente et al 2019 the advantages of these new approaches are that they allow the development of models with low error as well as the modeling of ensembles of future projections due to low calculation costs in this way instead of using a limited range of possible outcomes all gcm projections and hydrological models can be evaluated in a particular project which allows calculating uncertainty in a range of model results thus improving the information for decision making stephens et al 2022 in this article we developed a modeling approach for evaluating the expected reliability over a planning horizon of a hydraulic infrastructure and its uncertainty for hydroclimatic future projections this approach was applied in several mountain rivers of chile covering the different hydroclimatic regimes that characterize the andes rivers following the hybrid dynamical data driven approach described in de la fuente et al 2019 we started from a traditional hydrological model which was used to define the conceptual model and the hydrometeorological variables that were used in the training of the data driven models the meteorological variables were obtained through statistical scaling of the cmip6 climate projections eyring et al 2016 for the historical reference period of 1981 2020 to the future projection period of 2021 2060 a deep learning dl approach based on long short term memory lstm cells was implemented historical recorded daily stream flows were used to train the nets which were then validated and tested by passing the results of the hydrological deep learning model hdl model through sequence filters that discarded the trained nets that were not suitable for the frequency analysis of the annual maximum flood amf finally the model results for future projections were compared using frequency analysis for the amf distributions and these results were used to evaluate the time varying reliability and uncertainty the organization of this paper is as follows in section 2 we first introduce the basic concepts of the projected frequency analysis of amf then we describe the studied rivers and detail the deep learning structure and the adopted methodology to train validate and test the nets in the results section the validation and test of the adopted nets is presented then we present the results associated with changes in the exceedance probability of the design flow event and finally we show the cumulative effects of these changes in the exceedance probability that are transferred to the reliability of a hydraulic infrastructure over the planning horizon 2 materials and methods 2 1 projected reliability of a hydraulic infrastructure the standard methodology for obtaining the magnitude of the flow used for designing a hydraulic infrastructure is depicted in fig 2 a which shows a plot of the sample series of the annual maximum flood or the logarithm of this quantity depending on the probability density function pdf as a function of the frequency factor k p the value of k p depends on the exceedance probability p and the pdf such as p and k p are inversely related variables i e kp increases as p decreases naghettini 2016 stedinger et al 1993 on the other hand k p is defined as 1 k p x p μ σ where x p denotes the flow associated with p or the log 10 of the amf if it corresponds and μ and σ are the mean and the standard deviation of the sample series of the amf respectively therefore if the pdf fits to the sample series the resulting plot is a line whose slope is σ and intercept is μ following this the exceedance probability used for the design event herein referred to as p is determined based on techno economic analysis or normative recommendations and usually it is associated with extreme events that have not been observed in the past thus necessitating extrapolation chow et al 2010 furthermore once the design exceedance probability is known the design reliability of the infrastructure over the planning horizon can be computed as 2 r d 1 p n where r d is the design reliability equal to 1 minus the risk and n is the planning horizon of the infrastructure measured in number of years however assuming stationarity is no longer valid the projected reliability of hydraulic infrastructures should be used instead of r d here we adopted the definition by salas obeysekera 2014 see also read vogel 2015 who computed the projected reliability r p as 3 r p i 1 n 1 p i where p i is the exceedance probability of the design flow event for year i which was computed by assuming stationary conditions in the window of time between years i 40 and i therefore the standard frequency analysis can be repeated for the sample series of the amf in this window of time however the relevant question is now what exceedance probability is associated with the design flow x p whose value depends on the μ and σ values of the sample series of the window of time that is different from the corresponding values of the sample series used for the design if the average of the sample series increases with respect to the value of the design series fig 2b the new updated frequency factor is reduced and the exceedance probability increases a similar response is observed with the standard deviation of the sample series as shown in fig 2c the exceedance probability of the design flow increases with σ the length of the window of time used for the frequency analysis was defined based on standard guidelines that defines as 30 years of continuous hydrological data as the minimum information needed for conducting the frequency analysis naghettini 2016 stedinger et al 1993 furthermore the size of the amf limits the minimum exceedance probability that can be extrapolated naghettini 2016 which justifies the use of the longest amf that is available however the availability of information limited us to define 40 years as the size of window of time used for the frequency analysis in both the past and the future projections finally the uncertainty of the computed r p depends on the uncertainty of the values of p i which is determined based on the 95 confidence interval in the calculation of p i as a function of x p if the confidence interval is defined as p i b p i p i t the confidence interval of the computed value of r p r p b r p r p t is estimated by a first order taylor expansion as 4 r p t b r p 1 i 1 n p i b t p i 1 p i note that the upper limit of p i is associated with the smallest values of reliability or higher risks 2 2 study rivers and climate models in this article we analyzed the 15 river stations that are located along the latitudinal gradient of chile as shown in fig 2 each of these stations has observations of the daily average flow between 1980 and 2020 and the geographic features of the catchments are given in table 1 these stations were grouped according to the classification of macrozones of the military geographic institute of chile which subdivides chile into 5 climatic zones from the arid north zone 1 and the austral region where most of the rivers are fed by glaciers the difference between zones 3 and 4 is that the hydrology of zone 4 is modulated by large lakes that regulate the peak flows on the other hand we used the cmip6 multimodel ensemble of the ssp5 rcp8 5 scenarios which are the worst case climate projections and the corresponding historical simulation for the past climate we used 7 cmip6 models with daily air temperature information at different levels and precipitation rates awi cm 1 1 mr bcc csm2 mr canesm5 cmcc cm2 sr5 cnrm cm6 1 hr access sm1 5 and ec earth3 eyring et al 2016 fig 3 summarizes the projected changes in the input information to any hydrological model precipitation and catchment area on the one hand the columns a and b of fig 3 show the temporal evolution of the 40 year window average and standard deviation of the sample series of annual maximum precipitation amp in 24 hrs where each line corresponds to the average of all the climate models respectively as it was shown in fig 1 the exceedance probability of extreme events is controlled by both the average and the standard deviation of the sample series and fig 3 shows that despite of no significant changes are expected in the average of the sample series of amp the standard deviation of the sample series is projected to increase due to the appearance of extreme events that increase the standard deviation of the sample series thus increasing the exceedance probability of extreme events and reducing the reliability or increasing the risk we also computed the elevation of the 0 c isotherm that defines the area of the pluvial catchment and with this information we computed the pluvial area defined as the catchment area below this elevation the annual average of the pluvial area normalized by the catchment area is shown in the third column of fig 3 where the expected increase in the pluvial area due to climate change is observed the increase in the magnitude of the amf is expected in response to the increase in the pluvial area 2 3 hydrological deep learning models here we implemented a hydrological deep learning model hdl model for computing the daily flows associated with each of the cmip6 simulations the structure of the hdl model used in this article is shown in fig 4 and is similar to the structure of de la fuente et al 2019 the structure of the model is divided into three regions the first region captures the temporal correlation with a sequence of n lstm long short term memory lstm layers followed by a dropout layer to avoid model overfitting the second region is a series of n fcl fully connected layers followed by the relu activation function and the last region is the output layer that has one fully connected layer with one output that is followed by a regression layer where the loss is computed other applications of hdl models can be found in hu et al 2018 ma et al 2021 or shen 2018 while the limitations of this approach are on the information needed for training and validating the hdl and the risk of overfitting the model that leads to bad performance in predicting the catchment response under conditions that are observed in the training dataset in this application the availability of information defined the river stations used in this article while the risk of overfitting is limited with the validation procedure that it detailed in the following subsection the aim of the hdl model is to predict the daily average flow for day 8 in response to the meteorological and hydrological information for day 8 and the preceding 7 days consequently the input data are arranged in a matrix of 8 columns and 6 rows where each column is a day and the different rows are the input variables daily precipitation pluvial area elevation of the 0 c isotherm length of the stream in the pluvial area maximum elevation of the pluvial area and average slope of the stream note that the maximum elevation of the pluvial area is not equal to the elevation of the 0 c isotherm when the latter is located above the maximum elevation of the entire catchment in the context of fig 4 the inputs x 1 to x 8 are the different columns of the input matrix the output of the model is one row with 8 flows h 1 to h 8 where the last value h 8 is used as the predicted value of the corresponding day in the calendar the structure of the hdl model is specified by four hyperparameters n lstm n fcl the number of lstm units in each lstm block m lsmt and the number of neurons in the fully connected layers m fcl instead of optimizing the value of these hyperparameters we adopted the random search technique that trains the model with random values of these hyperparameters bergstra bengio 2012 then during the validation and testing of the trained hdl models we evaluated the performance of the models with different combinations of hyperparameters to determine if they are suitable for use in the analysis the hyperparameters n lstm and n fcl take values between 1 and 5 while m lsmt and m fcl take values between 50 and 300 for each river 30 different models with random hyperparameter combinations were trained 2 4 training validating and testing of the hydrological deep learning models the adopted methodology to train validate and test the hydrological deep learning models is described as follows first the era5 reanalysis hersbach et al 2020 was used to obtain the input matrices for training the 30 hdl models of each river and the measured flows at each station were used to compute the error in the simulation that was minimized with the backpropagation algorithm we used the entire set of flow observations for training the hdl models and we validated and tested the resulting model with the cmip6 climate models to define which trained net will be used to compute the hydrological response associated with which cmip6 model there were potentially 210 combined cmip6 and hdl models for each river and each combination was passed through a sequence of filters to determine if it was suitable for use in this article to validate the trained models we first implemented the multivariate bias correction method of cannon 2018 which allows matching the multivariate distribution of the input information of the cmip6 models to the input information obtained from the era5 reanalysis from 1980 to 2020 then we evaluated the trained nets with the cmip6 information and computed the frequency distribution curve of the daily average flows and monthly average flow measured we tested the goodness of fit between the observed and simulated frequency distributions with the kolmogorov smirnov test naghettini 2016 and evaluated the seasonal coherence in the monthly average flows by computing the kling gupta efficiency score gupta et al 2009 the trained models that passed the kolmogorov smirnov test and had kge values larger than 0 7 were finally selected for testing the frequency analysis of the amf the set of these models is hereinafter named the cmip6 hdl model finally the selected cmip6 hdl models were tested by performing a frequency analysis of the extreme events first we tested 7 different pdfs and discarded those that did not fit the observed sample series of amf with the χ 2 test then we selected the pdf that better represented the observed sample series of amf in the river as the one whose predicted values had the largest kge with respect to the observed values seven pdfs normal pearson iii exponential gumbel log normal and log pearson iii were tested chow et al 2010 naghettini 2016 stedinger et al 1993 once a pdf was selected for the observations we conducted a frequency analysis of the extreme events for each of the available cmip6 hdl models and plotted the scatter plot of x p μ σ as a function of k p where μ and σ are the mean and standard deviation of the corresponding sample series of amf of the different cmip6 hdl models and for the historic period between 1980 and 2020 respectively this exercise yielded a cloud of scatter points that follows the same lineal tendency of the observed sample series and then by fitting a line to this cloud the magnitude of flows for the entire set of cmip6 hdp models can be computed for different exceedance probabilities as well as their corresponding uncertainty finally we computed the flows associated with different exceedance probabilities between 0 0001 and 0 2 using the observations and the cmip6 hdl models and the kge of these two datasets quantifies the capability of using the cmip6 hdl models for the frequency analysis of extreme events 3 results 3 1 validation and test of the cmip6 hdl models the results of the validation filter described in the previous section are shown in fig 5 where each column corresponds to different rivers that are representative of the different regions defined in fig 2 the first row plots the monthly average flow and the second row plots the frequency distribution of daily average flow measured and simulated by the selected cmip6 hdl models as a result of this first validation filter a large proportion of the 210 possible cmip6 hdl models that are available for each river were discarded and detailed information is given in the second and third columns of table 2 once the set of cmip6 hdl models was defined their capability for reproducing the frequency analysis of extreme events was tested and the obtained results are shown in fig 6 the selected pdf for each river is detailed in the fourth column of table 2 and the fifth column indicates the value of the kge score of the fit between observations and the selected pdf the gray dots in fig 6 correspond to the simulated amf from the cmip6 hdl model and the goodness of this fit is given in the last column of table 2 note that the magnitude of the flows plotted in fig 6 are expressed in m3 s and not in the dimensionless form of eq 1 for which the average and the magnitude of the measured sample series were used after this test none of the selected cmip6 hdl models were discarded 3 2 projected changes in the exceedance probability of the design events the selected set of cmip6 hdl models was used to evaluate the projected changes in the exceedance probability of extreme events as an example of the changes that can occur fig 7 shows a plot of the same curves as those in fig 6 but for the 2020 2060 period changes in both the magnitude of the extreme events and the uncertainty are observed on the one hand the projected changes in the magnitude of extreme events occur in response to changes in the slope and the mean of the sample series of amf that translate and or rotate the curve as detailed in fig 1 in the context of the fitted lines in fig 7 if the black solid line is located above the gray dashed line an increase in the exceedance probability of the design flow is expected and the magnitude of this change is quantified as the horizontal gap between these two lines the increase in the magnitude of p i is given in the same figure where the ratio p i p can take values as high as 10 6 in river 1 and for the smaller values of p 0 0001 to complement this general description fig 8 a and b show plots of the frequency distributions of the mean and standard variation of all the computed sample series normalized with respect to the values of the sample series for the historical period between 1980 and 2020 the projected changes in μ and σ are expected to be in the range between 0 9 and 1 2 times the values used for the historical period however these relatively small changes in μ and σ have a large impact on the exceedance probability as shown in fig 8c which shows a plot of the relative change in p i for all rivers and years between 2030 and 2060 the adopted p in fig 8c was obtained by setting the design reliability in eq 2 equal to 95 for planning horizons of 5 10 20 50 and 100 years as a summary of these results illustrated in fig 8c we also found that because of changes in σ the magnitude of the change in p i with respect to p depends on the design probability of exceedance p in particular a smaller p has a larger difference between p i and p this pattern suggests that hydraulic infrastructures that were designed for the highest flows smaller exceedance probabilities are those that can be more vulnerable to small changes in the climatic conditions described by μ and σ with respect to conditions observed during the design on the other hand the uncertainty in the calculations also increases in the future the uncertainty in the projected values of p i for the design flow x p increases because of the disagreement among the different projected flows graphically this is reflected by the increase in the size of the cloud compare the cyan and blue points in fig 7 the impact of this disagreement among the cmip6 hdl models with respect to the 95 confidence interval of the updated values of p i is described in fig 8d which shows that the future uncertainty between 2030 and 2060 can be as high as 15 times the current uncertainty 3 3 projected changes in the reliability over a planning horizon the projected reliability defined in eq 3 summarizes the cumulative effects that changes in p i have over the planning horizon of the hydraulic infrastructure while eq 4 enables propagating the confidence interval of p i on the projected reliability the corresponding time series of the design and projected reliability and its confidence interval for the studied rivers are shown in fig 9 the selected value of p for these curves was associated with a design reliability of 95 and a planning horizon of 20 years eq 2 first the reliability of the infrastructure decreases with the age of the infrastructure because of the probability of not having a flow event that exceeds the design event decay with the age according to eq 2 the projected reliability also decreases with the age of the infrastructure however depending on the updated p i the rate of reduction can be faster or slower than the expected rate by design in the context of the studied rivers the reliability of the rivers located in zones 1 2 4 and 5 is expected to decrease in time at a faster rate than the stationary reliability while in zone 3 the nonstationary reliability shows a similar tendency to what is associated with the design conditions one way to summarize the accumulated impact of changes in p i on the reliability of hydraulic infrastructure is by looking at the age of the structure in which the projected reliability is equal to the design reliability computed by eq 2 this age can be understood as the updated planning horizon and the ratio between this age and the planning horizon summarizes the cumulative impact of changes in p i on the hydraulic infrastructure for the studied rivers of this article this ratio is equal to 0 74 0 18 for n 20 yr and 0 46 0 18 for n 100 yr therefore the design reliability is reached on average 5 years before the expected value for structures that are meant to operate for 20 years while for structures designed to last 100 years the design reliability can be reached as early as 50 years before the planning horizon consequently this reinforces the finding in this study that the cumulative impact of changes in p i over the planning horizon of the infrastructure is more pronounced for those structures that were designed for highly infrequent events 4 discussion and concluding remarks in this article the cmip6 models were coupled to deep learning hydrological models to obtain the projected sample series of amf in different mountain rivers of chile these sample series of amf were used to evaluate the cumulative effects that projected changes in the climate have on the reliability of hydraulic infrastructures over their planning horizon and their uncertainty the obtained results of these cmip6 hdl models suggest that the more vulnerable structures in terms of expected changes in the reliability are those that were designed for the larger planning horizon and therefore for the smaller exceedance probability of the design flow event the cumulative effects are summarized by looking at the age of the structure in which the design reliability is reached this value can be understood as the updated planning horizon of the structure and if it is normalized by the planning horizon a simple dimensionless index can be obtained and used to quantify the magnitude of the cumulative impact of climate change on the hydraulic infrastructure which can be used to prioritize sites plan future maintenance and focus surveillance among other operative or normative applications in this context the open challenge that motivates this article is to connect complex hydroclimatic models in a changing climate with the applied engineering practices that are used to define the magnitude of flood events which are finally used to design hydraulic structures that are planned to operate for 100 years or more kundzewicz licznar 2021 in this way a methodology to address the gap between science and engineering practice was derived for accounting for the cumulated changes of p i over the planning horizon of the hydraulic infrastructure and based on the projected reliability definition by salas obeysekera 2014 it was possible to show that the extrapolation needed to obtain the updated exceedance probability of the very infrequent extreme events of the design conditions is very sensible in terms of small changes in the standard deviation and the average of the sample series of amf that can change with the appearance of only one extreme event fig 1 the cumulative changes in p i show that the updated planning horizon of the structure can be reduced to 0 75 n for structures with a design planning horizon of less than 20 years and it can be as small as 0 4 n when the projected reliability is evaluated for infrastructures that are meant to operate for 100 years or more moreover an extended application of this idea of using the temporal evolution of the projected reliability to evaluate the impact of climate change on existing infrastructure is to update p i by simply looking at the new measurements conducted after the construction of the hydraulic infrastructure structures that were built early this century already have 20 years of operation and important changes may have already occurred in the sample series used for the frequency analysis in particular the recent increase in extreme events with magnitudes larger than the magnitude observed in the historical records used for the design simultaneously increases the mean and the standard deviation of the sample needed for the frequency analysis and both changes act in favor of increasing the exceedance probability of the flow event used for the hydraulic design furthermore we used deep learning models that are actively being introduced in hydroclimatic applications as they enable the use of a large amount of data to generate several hydroclimatic projections without significant computational requirements or a deep understanding of the physical processes that explain the catchment response the cmip6 climate models for the worst case scenario ssp5 rcp8 5 were coupled to these hydrological deep learning models that were based on lstm layers which have the advantage of capturing temporal correlations among the model input variables de la fuente et al 2019 hu et al 2018 ma et al 2021 shen 2018 the risk is however overfitting models that lack the flexibility to predict the catchment response under conditions that are observed in the training dataset therefore the validation and test of the trained net are vital stages that should be carefully designed to evaluate the actual performance of the trained net in this application the trained hdl models were used with the cmip6 models as input meteorological variables and the validation and test processes were a sequence of filters that were applied to the generated series of daily average flows 210 in each river 30 trained nets and 7 cmip6 models first we applied a statistical filter that discarded those hdl models whose frequency distribution of daily flow did not fit to the observed curve then we applied a process filter that filtered out the nets that did not reproduce the seasonal variability of the monthly average flow and finally we conducted a frequency analysis of extreme events for testing if the selected cmip6 hdl models were suitable for this analysis after these filters most of the 210 computed time series of daily average flow were discarded in fact we trained with 18 rivers 3 of which did not pass these filters however the adopted validation and test methodology based on filters gave a set of cmip6 hdl models that were shown to be good at reproducing the probabilistic distribution of extreme events fig 6 and are capable of being used for projecting near future conditions of the worst case scenario one of the main conclusions of this adopted validation and test methodology based on filters is that to have a well validated trained net based on observed data era5 in this article it is not sufficient to ensure that the trained net will perform well once it is evaluated with the cmip6 models in some cases the computed monthly average flows had no physical meaning in other cases the simulated curves of the frequency distribution of the daily average flows did not fit at all with the observed curves and in other cases the trained hdl model was suitable to be used only with some of the available cmip6 models therefore the sequence of filters was shown to be a cost efficient methodology that avoids an overfitted model as it selects a collection of several good structures instead of searching for the best structure of the hdl model this approach also enables the quantification of the uncertainty in the future projection that strengthened the frequency analysis in summary in this article we detailed a methodology for quantifying changes in the reliability of hydraulic infrastructure in a changing climate in the context of the applied engineering practices that are needed to compute the magnitude of the flow used for hydraulic design some of the implications of our findings are that in the absence of reliable information climate change can be included in hydraulic design based on recommended factors that amplify μ and σ of the sample series of amf used for the frequency analysis furthermore the updated planning horizon summarizes the cumulative changes in the exceedance probability of the design flow and it can be estimated based on the observed sample series amf after the design of the structure finally scientific efforts should also be made to generate a collection of projected series of flows that can be directly used in applied engineering which can follow the standard methodologies used for hydraulic designs and or maintenance deep learning is a cost effective technology that can be used to generate the projected time series but it should be properly validated and tested for calculation purposes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this article uses data from the cmip6 models and the hydrological observations of the chilean government that is available from https snia mop gob cl bnaconsultas reportes 
1779,to investigate the effects of roughness and seepage fluids on the seepage characteristics of limestone with rough fractures five sets of limestone fracture specimens are obtained using 3d scanning and sculpting techniques in which each set has the same fracture surface morphology both distilled water and sodium sulfate solutions were employed to implement seepage dissolution tests on the five sets of limestone specimens with different joint roughness coefficients jrc a novel method for characterizing the fracture surface morphology has been proposed and the seepage characteristics of individual fractures have been investigated also we analyzed the effects of different jrc and seepage solutions on the seepage parameters including seepage flow equivalent hydraulic aperture and permeability moreover we presented a quantitative relationship between jrc and permeability the results reveal that the seepage flow equivalent hydraulic aperture and permeability of each rough specimen under different seepage fluids decrease rapidly in the early stages and stabilize later the dissolution can enhance the erosion of surface particles in the fracture and promote the seepage flow the permeability and jrc follow a quadratic function in which the empirical parameters can be expressed as a quadratic function and an exponential function of time this study can provide a theoretical basis for estimating the permeability of fracture with different roughness under different water environments keywords limestone fracture joint roughness coefficient dissolution permeability seepage characteristics data availability data will be made available on request 1 introduction rock fracture seepage is a common phenomenon in rock engineering that significantly threatens the safety of miners and the performance of underground facilities the underground water inrush disaster causes a large number of casualties every year zhang and nemcik 2013 zhang et al 2022a zhang et al 2022c limestone as one of the most abundant rock masses in the earth s crust can affect the seepage characteristics in different environments a reasonable assessment of limestone permeability can reduce the potential for engineering leakage zhang et al 2019a zhao et al 2022 accordingly the investigation of the seepage characteristics of limestone fractures under the combined effects of stress and dissolution is of great importance in the design and construction of underground works studies show that rock fractures can affect the seepage properties of rock masses and the seepage in fractures is affected by several factors among which roughness is one of the most effective factors chen et al 2017 as one of the pioneers in this field snow 1969 proposed the cubic law for the seepage in the fracture between smooth parallel plates although remarkable results were obtained the effects of roughness were not taken into account dippenaar and van rooy 2016 yin et al 2018 showed that the presence of surface roughness disturbs the flow in rock fractures therefore the cubic law overestimates the seepage capacity of fractures and is not applicable to rough fractures in nature wang et al 2015 zhang et al 2019b to describe the seepage characteristics of fracture more accurately the cubic law has been modified by introducing the roughness of rock fracture e g babadagli et al 2015 gan et al 2022 guo et al 2020 he et al 2016 he et al 2021 lange et al 1993 maerz et al 1990 wang et al 2018 xiao et al 2013 zhang et al 2022b in this regard modified theories including the local cubic law qian et al 2011 and super cubic and sub cubic laws xu et al 2003 were proposed to describe the seepage characteristics of the rough fractures currently the laser triangulation method wang et al 2000 digital image technique mindaugas et al 2014 and 3d scanning technique chae et al 2011 have been used to measure the fracture surface morphology several parameters such as the root mean square of surface roughness z 2 myers 1962 fractal dimension d lee et al 1990 surface roughness parameter r s lange et al 1993 and roughness profile index r p maerz et al 1990 are used to quantify the roughness of the rock surface the most widely used parameter is the joint roughness coefficient jrc method proposed by barton 1973 barton and choubey 1977 in addition to fracture surface roughness the effect of stress on rough fracture seepage has also been further investigated wang et al 2021a xiao et al 2020 zhang et al 2020 according to the experimental results increasing the confining pressure will cause the fracture aperture to compress or even close decreasing the permeability coefficient in contrast an increase in seepage pressure induces the expansion of fracture and the increase of permeability coefficient ma et al 2013 tian et al 2019 wang et al 2021b di et al 2017 and xiao et al 2019 established the relationship between stress and the permeability coefficient some studies have confirmed that the permeability coefficient exponentially increases with the increase of fracture water pressure and there is a power function relationship between the permeability coefficient and the positive stress under the same water pressure di et al 2017 generally groundwater flows in the rock fracture and there may be acidic erosion in some areas gong et al 2021 zhang et al 2022b the dissolved anions and cations in the fluid would exchange ions with rock minerals such as aluminosilicates and carbonates so that the water flow would dissolve and corrode the fracture surface further research reveals that the seepage characteristics depend on the solute concentration and the pressure distribution ju et al 2019 sadhukhan et al 2012 therefore the factors that affect the fracture seepage also affect the surface roughness stress and corrosion of the rock and in addition solute and flow pressure can affect the rock body more specifically the higher the pressure the stronger the dissolution she et al 2016 the stress chemical coupling tests were carried out to study the characteristics of fracture seepage ameli et al 2014 li et al 2014 yasuhara et al 2011 zhang and yang 2012 zhao et al 2014 limestone is susceptible to dissolution due to its high calcium carbonate content li et al 2014 polak et al 2004 currently the experimental and numerical investigation of seepage mechanism for a rough single fracture under the joint action of stress and dissolution are often concerned li et al 2014 zhang et al 2019b zhao et al 2022 but the quantitative relationships between the fracture surface morphology and seepage parameters were rarely focused the purpose of this study is to experimentally investigate the seepage characteristics of rough fractures in limestone with different roughness and seepage fluids firstly a new method by combining 3d scanning and sculpting techniques was developed to prepare several groups of limestone fracture surfaces with the same 3d morphology which effectively excludes the effect of the splitting specimen preparation process on the tiny cracks in the limestone next a novel method for characterizing the roughness of surface fractures was proposed by introducing jrc and r s and two experimental cases of limestone fracture seepage were designed to reveal the effects of different jrc and seepage solutions on the evolution of fracture seepage flow equivalent hydraulic aperture and fracture permeability finally the quantitative relationships among the fracture permeability jrc and time in different seepage cases were presented the proposed formulas can be used to estimate the fracture permeability of the limestone 2 experimental materials and methodology 2 1 experimental material in the work we collected limestone specimens from the fushan mine in jiaxiang county jining city shandong province china and analyzed the main components and minerals contained in the limestone with an x ray diffractometer the mineral composition of the limestone is as follows calcite and dolomite account for 65 and 33 respectively with small amounts of quartzite and hydromica the basic physical properties of limestone are given in table 1 we designed cylinders of 50 mm in diameter and 100 mm in height for the tests in order to obtain the split specimen with two rough surfaces the splitting test was implemented on a standard cylindrical limestone specimen using the brazilian splitting method astm d3967 16 2016 and the splitting load rate is 0 05 mpa s the rough surfaces were then scanned by a 3d optical scanner to obtain point cloud data which was then engraved the two halves of the fractured surface after carving were assembled and the rock specimens with known values of roughness were obtained fig 1 shows the main preparation process for making the specimens 2 2 experimental procedure in the seepage tests we use a dynamic and static triaxial testing machine for geotechnical materials the machine system is capable of performing multi field coupled tests of seepage stress temperature and chemistry on rock and soil materials fig 2 shows the pressure control system of the device before testing the sides of the prepared cylindrical specimens were coated with 704 silica gel after solidifying the silica gel we placed two layers of filter paper under each specimen and placed the prepared specimens with the described accessories in the test device the specimen was then tightened with a heat shrink film and the upper and lower ends were tied with iron wire the seepage fluid was injected after the specimen was mounted we set the axial pressure confining pressure and seepage pressure to be 5 0 mpa 3 0 mpa and 0 5 mpa respectively and performed all tests at the ambient temperature of 20 c the test data were recorded once an hour and the test procedure is shown in fig 3 2 3 roughness characterization of the fracture surface barton 1973 first defined the fracture roughness coefficient of the rock surface then tse and cruden 1979 maerz et al 1990 and lee et al 1990 established the quantitative relation among the root mean square of slope z 2s r p d and jrc since the seepage characteristics are affected by the degree of undulation we introduce r s to characterize the degree of undulation of the fracture surface it is worth noting that the larger the surface roughness the rougher the fracture surface in this context the following empirical expression between jrc tse and cruden 1979 z 2s myers 1962 and r s lange et al 1993 can be used to jointly characterize the fracture surface roughness 1 jrc 32 2 32 47 lg z 2 s 2 z 2 s 1 δ x 2 i 1 n x 1 j 1 n y 1 z i 1 j 1 z i j 1 2 z i 1 j z i j 2 2 1 δ y 2 j 1 n y 1 i 1 n x 1 z i 1 j 1 z i 1 j 2 z i j 1 z i j 2 2 n x 1 n y 1 3 r s a t a n 4 a t δ x δ y i 1 n x 1 j 1 n y 1 1 z i 1 j z i j δ x 2 z i j 1 z i j δ y 2 where z 2s is the average gradient mode nx and ny are the number of sampling points along the x and y axis respectively δx and δy are the intervals of sampling points along the x and y axis respectively zi and zi 1 are the coordinates of the i th and i 1th fracture discretization points along the z axis i and j denote the serial numbers of the fine viewing planes along the x and y axis respectively a t and a n denote the true surface area and the projected area respectively a n is set to 5000 mm2 in this work the coordinate data of the fracture surfaces obtained from the 3d scanning were processed by the surfer software to obtain a reconstruction mesh model the above equations are used to calculate the coordinate point data of the reconstructed model table 2 presents the characteristic parameters of the fracture surfaces for each specimen and the mean value of the jrc for each fracture surface respectively fig 4 shows the reconstructed mesh model of the fracture surfaces for all specimens notice that surfaces no 1 and no 2 refer to the two rough fracture surfaces of the rock specimens respectively fig 5 shows the fitted relation between jrc and r s for each fracture plane one can observe that the coefficient of determination r 2 of the curve is 0 9395 which suggests that the fit is reasonable according to the fit results the correlation between jrc and r s can be proposed as follows 5 jrc 305 28 ln r s 3 0091 eq 5 is a logarithmic function which can be used to estimate jrc values for limestone fracture surfaces easily to improve the accuracy of the calculation the average value of the calculated results using eqs 1 and 5 can be used to characterize the fracture surface morphology characteristic parameter 2 4 experimental cases to investigate the seepage characteristics of the fracture surfaces under different seepage fluids we designed two cases for testing during testing we selected the fracture surfaces of the j2 j6 j7 j8 and j9 specimens as the original specimens and carved two sets of identical specimens for both cases we calculated the jrc values for each specimen using the roughness method presented in this study and the results are labeled as s1 s5 and b1 b5 from small to large respectively we used distilled water and 0 01 mol l sodium sulphate solution at ph 6 as the seepage fluids in both cases the design cases are introduced in table 3 2 5 calculation of seepage characteristic parameters the equivalent hydraulic aperture b h and permeability k can be calculated from the experimentally measured seepage flow and the cubic law equation snow 1969 lomize 1951 louis 1969 which can be mathematically stated as follows 6 q a υ a k j d b h g b h 2 12 v δ p ρ g l d b h 3 δ p 12 μ l 7 b h 12 μ l q d δ p 3 8 k k μ γ g b h 2 12 v ρ v ρ g b h 2 12 where q is the seepage flow through the fracture m3 s k is the permeability coefficient of the fracture m s d is the diameter of the fracture specimen m l is the length of the fracture specimen m ρ is the density of water kg m3 ν is the kinematic viscosity of water m2 s μ is the dynamic viscosity of water pa s b h is the equivalent hydraulic aperture of the fracture m and δ p is the pressure difference between the two ends of the fracture specimen pa 3 experimental results and discussion 3 1 experimental results in case 1 in the tests with as the seepage fluid we performed the seepage tests for 24 h on five rough single fracture limestone specimens with different values of jrc and recorded the seepage volume once an hour fig 6 presents the variation curves of the seepage characteristic parameters in time for different jrc specimens in the distilled water case 1 the distribution of the seepage flow against time in case 1 was plotted in fig 6a as shown in fig 6a generally the seepage flow of the five sets of specimens decreases rapidly at the early times and slows down to approach a constant value hence the distribution of the seepage flow for different jrc values can be divided into three stages 1 rapidly declining stage 2 slowly declining stage 3 and stabilization stage in the rapidly declining stage the seepage flow decreases at the fastest rate and the initial seepage flow of s1 to s5 specimens are 6 52 10 8 m3 s 5 86 10 8 m3 s 5 50 10 8 m3 s 3 95 10 8 m3 s and 2 47 10 8 m3 s respectively at the end of this phase the seepage flow decreased by 60 39 59 76 61 79 53 26 and 63 83 respectively after 16 h the seepage flow of the fracture specimens gradually gets to a steady state and the seepage flow of s 1 s 5 specimens are 1 3 10 8 m3 s 0 85 10 8 m3 s 0 58 10 8 m3 s 0 52 10 8 m3 s and 0 42 10 8 m3 s respectively it is found that the larger the jrc of the fracture surface the smaller the initial seepage flow plus in the stabilization stage the smaller the decreasing seepage flow and the shorter the time required to reach a steady seepage state more specifically s 1 and s 2 specimens took 2 h longer to reach this state than the others fig 6b and 6c show the variation of the hydraulic aperture and permeability in time for the different jrc specimens in case 1 the figures indicate that the variations of the equivalent hydraulic aperture and permeability are consistent and that both can also be divided into three stages eq 10 reveals that permeability and hydraulic aperture are positively correlated therefore only the fitting equation of permeability k is used in this study to quantify the effect of the initial jrc value on the permeability of the fractured rock the experimental data of 1 h 4 h 8 h 12 h 16 h and 24 h were selected to fit with the values of jrc the 4th and 16th h were the transition time points of the three stages the fitting results are presented in fig 7 and table 4 respectively fig 7 indicates that the permeability and jrc are correlated by quadratic functions with the minimum determination coefficient r2 of 0 9305 table 4 shows the fitting equations of each curve the general fitting equation can be expressed as follows 9 k a j r c 2 b j r c c where a b and c are empirical parameters related to time t which are affected by different factors such as triaxial stress lithology and seepage pressure in this study these parameters are set by eq 9 it should be noted that the empirical parameters a b and t are both quadratic functions while c and t are exponentially dependent the fitting curves of eq 10 are presented in fig 8 in which the minimum r2 of the fitting values is 0 9752 10 a 0 220 t 2 7 591 t 49 0 r 2 0 9752 b 3 930 t 2 130 691 t 531 9 r 2 0 9756 c 16991 t 0 273 r 2 0 9939 hence the relationship between limestone fracture permeability k and jrc for fracture surface roughness less than 18 27 under a confining pressure of 3 0 mpa and a seepage pressure of 0 5 mpa can be written as 11 k 0 220 t 2 7 591 t 49 0 j r c 2 3 930 t 2 130 691 t 531 9 j r c 16991 t 0 273 3 2 experimental results in case 2 in case 2 we considered sodium sulphate solution as the seepage fluid and performed the seepage tests for 135 h on five rough single fracture limestone specimens with different jrc values during the experiment the seepage volume was recorded every hour the seepage flow equivalent hydraulic aperture and permeability were calculated and the results are shown in table 5 moreover fig 9 show the distribution of the three parameters in time for different jrc specimens in case 2 the variation of each parameter is similar to that in case 1 and can be divided into the same three stages as well for the comparative analysis the seepage flow and permeability data from the first 24 h of the test were selected for further investigation in this case we selected the test data obtained and fitted with jrc at 1 h 5 h 10 h 15 h 20 h and 24 h where 15 h was the first transition time point the fitted relations between the permeability and the jrc were presented in fig 10 the figure shows that the permeability and jrc are still related by a quadratic function and the goodness of fit is no less than 0 9625 for the fitted values table 6 shows the fitting results and the general formula can still be expressed in terms of eq 9 fig 11 show the fitted curves for the coefficients and time which can be mathematically expressed in the following form 12 d 0 115 t 2 4 484 t 54 1 r 2 0 9736 e 2 040 t 2 72 148 t 525 6 r 2 0 9263 f 16666 t 0 323 r 2 0 9966 where d e and f are the parameters in the fitting equation for the permeability similar to case 1 in case 2 the permeability k at any moment of limestone fractures with fracture surface roughness less than 18 27 at a confining pressure of 3 0 mpa and a seepage pressure of 0 5 mpa can be obtained from the initial jrc value and the expressions are as follows 13 k 0 115 t 2 4 484 t 54 1 j r c 2 2 040 t 2 72 148 t 525 6 j r c 16666 t 0 323 3 3 discussions by comparing the results of the fitting equations obtained in the different cases both the seepage flow and the permeability calculated for the same specimen at the same time in case 1 are larger than that in case 2 to verify the accuracy of the equations the experimental data under different seepage fluids were compared and analyzed the experimental data from the first 24 h of both cases were used to draw the comparison plots for each parameter which are shown in fig 12 fig 12 shows that the seepage parameters in the stabilization stage of case 1 are smaller than those in case 2 indicating that the dissolving effect of the sodium sulphate solution increases the seepage rate in the fracture thus intensifying the erosion of the fracture surface besides the comparison also shows that the larger the value of jrc the larger the difference between the seepage flow and the permeability of the specimens in cases 1 and 2 this indicates that the dissolution effect and the joint roughness coefficient interact with each other and jointly affect the seepage characteristics of the rock however the fitting equation derived from only five sets of experiments does not prove its applicability to other fracture roughness surfaces hence under the existing conditions two more sets of similar experiments were chosen for verification namely four specimens with jrc of 1 98 and 10 79 were selected to be tested under case 1 and case 2 to obtain the permeability respectively fig 13 shows a comparison between the measured and fitted values of the permeability for both cases as presented in fig 13 the difference between the real and fitted values was small and the relative errors re were less than 5 0 which further verified the fitting equations of plausibility it is important to estimate the morphological characteristics of fracture surfaces and accurately predict the rock permeability to control seepage in engineering applications the fitted equations for the permeability in the different solutions are almost the same except for the different empirical parameters therefore the permeability may be affected by the same factors such as time joint roughness coefficient and the concentration of hydrogen ions which can be stated as follows 14 k f t j r c c h η where c h is the concentration of hydrogen ions moreover η reflects other factors that affect permeability in this study the dominant minerals were caco3 and camg co3 2 under the joint effect of the confining pressure and seepage solution the fracture surface concave and the convex body was damaged by extrusion and reacted with the solution and the seepage path in the fracture became smaller until closure this process is shown in fig 14 a solution of 0 01 mol l sodium sulfate solution with an acidity of ph 6 was used as the seepage fluid and the solution flowed through the fracture surface where the minerals on the fracture surface would react with the solution with this in mind the main chemical reactions can be summarized as follows 15 caco 3 2 h ca 2 h 2 o co 2 16 camg co 3 2 4 h ca 2 mg 2 2h 2 o 2co 2 the experimentally measured concentration of ca2 becomes larger before and after the seepage solution which proves that h plays a key role in the dissolution reaction in other words the changes in the seepage flow and permeability in the test are mainly affected by the concentration of hydrogen ions however further analysis of the quantitative changes produced by the specific concentrations and other factors is needed the study investigated the seepage characteristics of a single fracture under different roughness and seepage fluids the quantitative relationships between permeability and jrc in different seepage cases were proposed through the eqs 11 and 13 respectively these two equations can be used to estimate the permeability of rock formations in conventional and sulfate rich groundwater environments under the conditions of a confusing pressure of 3 0 mpa and a seepage pressure of 0 5 mpa respectively further combining the eqs 6 and 8 the seepage flow of rock formations in the two corresponding cases can be obtained which can provide a theoretical basis for seepage control measures for the geological exploration prior to dam siting and tunnel excavation 4 conclusions in this study the seepage tests with different roughness and seepage fluids were conducted to analyze the effect of jrc and seepage solutions on the seepage parameters of a single rough fracture such as seepage flow equivalent hydraulic aperture and permeability furthermore quantitative relationships between jrc and permeability were proposed to further discuss the seepage characteristics of a single rough fracture in limestone the main findings and conclusions can be summarized as follows 1 a new method by combining 3d scanning and sculpting techniques was developed to prepare the five groups of limestone fracture surfaces with the same jrc values such as 2 13 7 07 12 31 15 67 and 18 27 there is a logarithmic relation between jrc and r s which can be used to characterize the fracture surface morphology parameter 2 the seepage flow equivalent hydraulic aperture and permeability of limestone fracture specimens with different jrc values decrease rapidly at the early times and slow down to approach a constant value the larger the jrc of the fracture surface the smaller the initial and stable values of each seepage parameter and the shorter the time required to reach the stabilization stage the permeability and jrc in the two cases are correlated by quadratic functions with the minimum r2 of 0 9263 3 the quantitative relationships among the permeability time and jrc values at constant confining and seepage pressures were presented the results show that the correlation between the permeability and the jrc can be fitted by quadratic functions for different seepage fluids while the empirical parameters are related to time by quadratic and exponential functions the proposed formulas are suitable to estimate fracture permeability in fractured limestone and reduce the potential for engineering leakage credit authorship contribution statement lei gan writing original draft writing review editing yu liu writing original draft teng xu supervision methodology writing review editing lei xu supervision methodology hongying ma conceptualization methodology writing review editing weichao xu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china no 52179130 no 51609073 and the natural science foundation of jiangsu province china no bk20201312 
1779,to investigate the effects of roughness and seepage fluids on the seepage characteristics of limestone with rough fractures five sets of limestone fracture specimens are obtained using 3d scanning and sculpting techniques in which each set has the same fracture surface morphology both distilled water and sodium sulfate solutions were employed to implement seepage dissolution tests on the five sets of limestone specimens with different joint roughness coefficients jrc a novel method for characterizing the fracture surface morphology has been proposed and the seepage characteristics of individual fractures have been investigated also we analyzed the effects of different jrc and seepage solutions on the seepage parameters including seepage flow equivalent hydraulic aperture and permeability moreover we presented a quantitative relationship between jrc and permeability the results reveal that the seepage flow equivalent hydraulic aperture and permeability of each rough specimen under different seepage fluids decrease rapidly in the early stages and stabilize later the dissolution can enhance the erosion of surface particles in the fracture and promote the seepage flow the permeability and jrc follow a quadratic function in which the empirical parameters can be expressed as a quadratic function and an exponential function of time this study can provide a theoretical basis for estimating the permeability of fracture with different roughness under different water environments keywords limestone fracture joint roughness coefficient dissolution permeability seepage characteristics data availability data will be made available on request 1 introduction rock fracture seepage is a common phenomenon in rock engineering that significantly threatens the safety of miners and the performance of underground facilities the underground water inrush disaster causes a large number of casualties every year zhang and nemcik 2013 zhang et al 2022a zhang et al 2022c limestone as one of the most abundant rock masses in the earth s crust can affect the seepage characteristics in different environments a reasonable assessment of limestone permeability can reduce the potential for engineering leakage zhang et al 2019a zhao et al 2022 accordingly the investigation of the seepage characteristics of limestone fractures under the combined effects of stress and dissolution is of great importance in the design and construction of underground works studies show that rock fractures can affect the seepage properties of rock masses and the seepage in fractures is affected by several factors among which roughness is one of the most effective factors chen et al 2017 as one of the pioneers in this field snow 1969 proposed the cubic law for the seepage in the fracture between smooth parallel plates although remarkable results were obtained the effects of roughness were not taken into account dippenaar and van rooy 2016 yin et al 2018 showed that the presence of surface roughness disturbs the flow in rock fractures therefore the cubic law overestimates the seepage capacity of fractures and is not applicable to rough fractures in nature wang et al 2015 zhang et al 2019b to describe the seepage characteristics of fracture more accurately the cubic law has been modified by introducing the roughness of rock fracture e g babadagli et al 2015 gan et al 2022 guo et al 2020 he et al 2016 he et al 2021 lange et al 1993 maerz et al 1990 wang et al 2018 xiao et al 2013 zhang et al 2022b in this regard modified theories including the local cubic law qian et al 2011 and super cubic and sub cubic laws xu et al 2003 were proposed to describe the seepage characteristics of the rough fractures currently the laser triangulation method wang et al 2000 digital image technique mindaugas et al 2014 and 3d scanning technique chae et al 2011 have been used to measure the fracture surface morphology several parameters such as the root mean square of surface roughness z 2 myers 1962 fractal dimension d lee et al 1990 surface roughness parameter r s lange et al 1993 and roughness profile index r p maerz et al 1990 are used to quantify the roughness of the rock surface the most widely used parameter is the joint roughness coefficient jrc method proposed by barton 1973 barton and choubey 1977 in addition to fracture surface roughness the effect of stress on rough fracture seepage has also been further investigated wang et al 2021a xiao et al 2020 zhang et al 2020 according to the experimental results increasing the confining pressure will cause the fracture aperture to compress or even close decreasing the permeability coefficient in contrast an increase in seepage pressure induces the expansion of fracture and the increase of permeability coefficient ma et al 2013 tian et al 2019 wang et al 2021b di et al 2017 and xiao et al 2019 established the relationship between stress and the permeability coefficient some studies have confirmed that the permeability coefficient exponentially increases with the increase of fracture water pressure and there is a power function relationship between the permeability coefficient and the positive stress under the same water pressure di et al 2017 generally groundwater flows in the rock fracture and there may be acidic erosion in some areas gong et al 2021 zhang et al 2022b the dissolved anions and cations in the fluid would exchange ions with rock minerals such as aluminosilicates and carbonates so that the water flow would dissolve and corrode the fracture surface further research reveals that the seepage characteristics depend on the solute concentration and the pressure distribution ju et al 2019 sadhukhan et al 2012 therefore the factors that affect the fracture seepage also affect the surface roughness stress and corrosion of the rock and in addition solute and flow pressure can affect the rock body more specifically the higher the pressure the stronger the dissolution she et al 2016 the stress chemical coupling tests were carried out to study the characteristics of fracture seepage ameli et al 2014 li et al 2014 yasuhara et al 2011 zhang and yang 2012 zhao et al 2014 limestone is susceptible to dissolution due to its high calcium carbonate content li et al 2014 polak et al 2004 currently the experimental and numerical investigation of seepage mechanism for a rough single fracture under the joint action of stress and dissolution are often concerned li et al 2014 zhang et al 2019b zhao et al 2022 but the quantitative relationships between the fracture surface morphology and seepage parameters were rarely focused the purpose of this study is to experimentally investigate the seepage characteristics of rough fractures in limestone with different roughness and seepage fluids firstly a new method by combining 3d scanning and sculpting techniques was developed to prepare several groups of limestone fracture surfaces with the same 3d morphology which effectively excludes the effect of the splitting specimen preparation process on the tiny cracks in the limestone next a novel method for characterizing the roughness of surface fractures was proposed by introducing jrc and r s and two experimental cases of limestone fracture seepage were designed to reveal the effects of different jrc and seepage solutions on the evolution of fracture seepage flow equivalent hydraulic aperture and fracture permeability finally the quantitative relationships among the fracture permeability jrc and time in different seepage cases were presented the proposed formulas can be used to estimate the fracture permeability of the limestone 2 experimental materials and methodology 2 1 experimental material in the work we collected limestone specimens from the fushan mine in jiaxiang county jining city shandong province china and analyzed the main components and minerals contained in the limestone with an x ray diffractometer the mineral composition of the limestone is as follows calcite and dolomite account for 65 and 33 respectively with small amounts of quartzite and hydromica the basic physical properties of limestone are given in table 1 we designed cylinders of 50 mm in diameter and 100 mm in height for the tests in order to obtain the split specimen with two rough surfaces the splitting test was implemented on a standard cylindrical limestone specimen using the brazilian splitting method astm d3967 16 2016 and the splitting load rate is 0 05 mpa s the rough surfaces were then scanned by a 3d optical scanner to obtain point cloud data which was then engraved the two halves of the fractured surface after carving were assembled and the rock specimens with known values of roughness were obtained fig 1 shows the main preparation process for making the specimens 2 2 experimental procedure in the seepage tests we use a dynamic and static triaxial testing machine for geotechnical materials the machine system is capable of performing multi field coupled tests of seepage stress temperature and chemistry on rock and soil materials fig 2 shows the pressure control system of the device before testing the sides of the prepared cylindrical specimens were coated with 704 silica gel after solidifying the silica gel we placed two layers of filter paper under each specimen and placed the prepared specimens with the described accessories in the test device the specimen was then tightened with a heat shrink film and the upper and lower ends were tied with iron wire the seepage fluid was injected after the specimen was mounted we set the axial pressure confining pressure and seepage pressure to be 5 0 mpa 3 0 mpa and 0 5 mpa respectively and performed all tests at the ambient temperature of 20 c the test data were recorded once an hour and the test procedure is shown in fig 3 2 3 roughness characterization of the fracture surface barton 1973 first defined the fracture roughness coefficient of the rock surface then tse and cruden 1979 maerz et al 1990 and lee et al 1990 established the quantitative relation among the root mean square of slope z 2s r p d and jrc since the seepage characteristics are affected by the degree of undulation we introduce r s to characterize the degree of undulation of the fracture surface it is worth noting that the larger the surface roughness the rougher the fracture surface in this context the following empirical expression between jrc tse and cruden 1979 z 2s myers 1962 and r s lange et al 1993 can be used to jointly characterize the fracture surface roughness 1 jrc 32 2 32 47 lg z 2 s 2 z 2 s 1 δ x 2 i 1 n x 1 j 1 n y 1 z i 1 j 1 z i j 1 2 z i 1 j z i j 2 2 1 δ y 2 j 1 n y 1 i 1 n x 1 z i 1 j 1 z i 1 j 2 z i j 1 z i j 2 2 n x 1 n y 1 3 r s a t a n 4 a t δ x δ y i 1 n x 1 j 1 n y 1 1 z i 1 j z i j δ x 2 z i j 1 z i j δ y 2 where z 2s is the average gradient mode nx and ny are the number of sampling points along the x and y axis respectively δx and δy are the intervals of sampling points along the x and y axis respectively zi and zi 1 are the coordinates of the i th and i 1th fracture discretization points along the z axis i and j denote the serial numbers of the fine viewing planes along the x and y axis respectively a t and a n denote the true surface area and the projected area respectively a n is set to 5000 mm2 in this work the coordinate data of the fracture surfaces obtained from the 3d scanning were processed by the surfer software to obtain a reconstruction mesh model the above equations are used to calculate the coordinate point data of the reconstructed model table 2 presents the characteristic parameters of the fracture surfaces for each specimen and the mean value of the jrc for each fracture surface respectively fig 4 shows the reconstructed mesh model of the fracture surfaces for all specimens notice that surfaces no 1 and no 2 refer to the two rough fracture surfaces of the rock specimens respectively fig 5 shows the fitted relation between jrc and r s for each fracture plane one can observe that the coefficient of determination r 2 of the curve is 0 9395 which suggests that the fit is reasonable according to the fit results the correlation between jrc and r s can be proposed as follows 5 jrc 305 28 ln r s 3 0091 eq 5 is a logarithmic function which can be used to estimate jrc values for limestone fracture surfaces easily to improve the accuracy of the calculation the average value of the calculated results using eqs 1 and 5 can be used to characterize the fracture surface morphology characteristic parameter 2 4 experimental cases to investigate the seepage characteristics of the fracture surfaces under different seepage fluids we designed two cases for testing during testing we selected the fracture surfaces of the j2 j6 j7 j8 and j9 specimens as the original specimens and carved two sets of identical specimens for both cases we calculated the jrc values for each specimen using the roughness method presented in this study and the results are labeled as s1 s5 and b1 b5 from small to large respectively we used distilled water and 0 01 mol l sodium sulphate solution at ph 6 as the seepage fluids in both cases the design cases are introduced in table 3 2 5 calculation of seepage characteristic parameters the equivalent hydraulic aperture b h and permeability k can be calculated from the experimentally measured seepage flow and the cubic law equation snow 1969 lomize 1951 louis 1969 which can be mathematically stated as follows 6 q a υ a k j d b h g b h 2 12 v δ p ρ g l d b h 3 δ p 12 μ l 7 b h 12 μ l q d δ p 3 8 k k μ γ g b h 2 12 v ρ v ρ g b h 2 12 where q is the seepage flow through the fracture m3 s k is the permeability coefficient of the fracture m s d is the diameter of the fracture specimen m l is the length of the fracture specimen m ρ is the density of water kg m3 ν is the kinematic viscosity of water m2 s μ is the dynamic viscosity of water pa s b h is the equivalent hydraulic aperture of the fracture m and δ p is the pressure difference between the two ends of the fracture specimen pa 3 experimental results and discussion 3 1 experimental results in case 1 in the tests with as the seepage fluid we performed the seepage tests for 24 h on five rough single fracture limestone specimens with different values of jrc and recorded the seepage volume once an hour fig 6 presents the variation curves of the seepage characteristic parameters in time for different jrc specimens in the distilled water case 1 the distribution of the seepage flow against time in case 1 was plotted in fig 6a as shown in fig 6a generally the seepage flow of the five sets of specimens decreases rapidly at the early times and slows down to approach a constant value hence the distribution of the seepage flow for different jrc values can be divided into three stages 1 rapidly declining stage 2 slowly declining stage 3 and stabilization stage in the rapidly declining stage the seepage flow decreases at the fastest rate and the initial seepage flow of s1 to s5 specimens are 6 52 10 8 m3 s 5 86 10 8 m3 s 5 50 10 8 m3 s 3 95 10 8 m3 s and 2 47 10 8 m3 s respectively at the end of this phase the seepage flow decreased by 60 39 59 76 61 79 53 26 and 63 83 respectively after 16 h the seepage flow of the fracture specimens gradually gets to a steady state and the seepage flow of s 1 s 5 specimens are 1 3 10 8 m3 s 0 85 10 8 m3 s 0 58 10 8 m3 s 0 52 10 8 m3 s and 0 42 10 8 m3 s respectively it is found that the larger the jrc of the fracture surface the smaller the initial seepage flow plus in the stabilization stage the smaller the decreasing seepage flow and the shorter the time required to reach a steady seepage state more specifically s 1 and s 2 specimens took 2 h longer to reach this state than the others fig 6b and 6c show the variation of the hydraulic aperture and permeability in time for the different jrc specimens in case 1 the figures indicate that the variations of the equivalent hydraulic aperture and permeability are consistent and that both can also be divided into three stages eq 10 reveals that permeability and hydraulic aperture are positively correlated therefore only the fitting equation of permeability k is used in this study to quantify the effect of the initial jrc value on the permeability of the fractured rock the experimental data of 1 h 4 h 8 h 12 h 16 h and 24 h were selected to fit with the values of jrc the 4th and 16th h were the transition time points of the three stages the fitting results are presented in fig 7 and table 4 respectively fig 7 indicates that the permeability and jrc are correlated by quadratic functions with the minimum determination coefficient r2 of 0 9305 table 4 shows the fitting equations of each curve the general fitting equation can be expressed as follows 9 k a j r c 2 b j r c c where a b and c are empirical parameters related to time t which are affected by different factors such as triaxial stress lithology and seepage pressure in this study these parameters are set by eq 9 it should be noted that the empirical parameters a b and t are both quadratic functions while c and t are exponentially dependent the fitting curves of eq 10 are presented in fig 8 in which the minimum r2 of the fitting values is 0 9752 10 a 0 220 t 2 7 591 t 49 0 r 2 0 9752 b 3 930 t 2 130 691 t 531 9 r 2 0 9756 c 16991 t 0 273 r 2 0 9939 hence the relationship between limestone fracture permeability k and jrc for fracture surface roughness less than 18 27 under a confining pressure of 3 0 mpa and a seepage pressure of 0 5 mpa can be written as 11 k 0 220 t 2 7 591 t 49 0 j r c 2 3 930 t 2 130 691 t 531 9 j r c 16991 t 0 273 3 2 experimental results in case 2 in case 2 we considered sodium sulphate solution as the seepage fluid and performed the seepage tests for 135 h on five rough single fracture limestone specimens with different jrc values during the experiment the seepage volume was recorded every hour the seepage flow equivalent hydraulic aperture and permeability were calculated and the results are shown in table 5 moreover fig 9 show the distribution of the three parameters in time for different jrc specimens in case 2 the variation of each parameter is similar to that in case 1 and can be divided into the same three stages as well for the comparative analysis the seepage flow and permeability data from the first 24 h of the test were selected for further investigation in this case we selected the test data obtained and fitted with jrc at 1 h 5 h 10 h 15 h 20 h and 24 h where 15 h was the first transition time point the fitted relations between the permeability and the jrc were presented in fig 10 the figure shows that the permeability and jrc are still related by a quadratic function and the goodness of fit is no less than 0 9625 for the fitted values table 6 shows the fitting results and the general formula can still be expressed in terms of eq 9 fig 11 show the fitted curves for the coefficients and time which can be mathematically expressed in the following form 12 d 0 115 t 2 4 484 t 54 1 r 2 0 9736 e 2 040 t 2 72 148 t 525 6 r 2 0 9263 f 16666 t 0 323 r 2 0 9966 where d e and f are the parameters in the fitting equation for the permeability similar to case 1 in case 2 the permeability k at any moment of limestone fractures with fracture surface roughness less than 18 27 at a confining pressure of 3 0 mpa and a seepage pressure of 0 5 mpa can be obtained from the initial jrc value and the expressions are as follows 13 k 0 115 t 2 4 484 t 54 1 j r c 2 2 040 t 2 72 148 t 525 6 j r c 16666 t 0 323 3 3 discussions by comparing the results of the fitting equations obtained in the different cases both the seepage flow and the permeability calculated for the same specimen at the same time in case 1 are larger than that in case 2 to verify the accuracy of the equations the experimental data under different seepage fluids were compared and analyzed the experimental data from the first 24 h of both cases were used to draw the comparison plots for each parameter which are shown in fig 12 fig 12 shows that the seepage parameters in the stabilization stage of case 1 are smaller than those in case 2 indicating that the dissolving effect of the sodium sulphate solution increases the seepage rate in the fracture thus intensifying the erosion of the fracture surface besides the comparison also shows that the larger the value of jrc the larger the difference between the seepage flow and the permeability of the specimens in cases 1 and 2 this indicates that the dissolution effect and the joint roughness coefficient interact with each other and jointly affect the seepage characteristics of the rock however the fitting equation derived from only five sets of experiments does not prove its applicability to other fracture roughness surfaces hence under the existing conditions two more sets of similar experiments were chosen for verification namely four specimens with jrc of 1 98 and 10 79 were selected to be tested under case 1 and case 2 to obtain the permeability respectively fig 13 shows a comparison between the measured and fitted values of the permeability for both cases as presented in fig 13 the difference between the real and fitted values was small and the relative errors re were less than 5 0 which further verified the fitting equations of plausibility it is important to estimate the morphological characteristics of fracture surfaces and accurately predict the rock permeability to control seepage in engineering applications the fitted equations for the permeability in the different solutions are almost the same except for the different empirical parameters therefore the permeability may be affected by the same factors such as time joint roughness coefficient and the concentration of hydrogen ions which can be stated as follows 14 k f t j r c c h η where c h is the concentration of hydrogen ions moreover η reflects other factors that affect permeability in this study the dominant minerals were caco3 and camg co3 2 under the joint effect of the confining pressure and seepage solution the fracture surface concave and the convex body was damaged by extrusion and reacted with the solution and the seepage path in the fracture became smaller until closure this process is shown in fig 14 a solution of 0 01 mol l sodium sulfate solution with an acidity of ph 6 was used as the seepage fluid and the solution flowed through the fracture surface where the minerals on the fracture surface would react with the solution with this in mind the main chemical reactions can be summarized as follows 15 caco 3 2 h ca 2 h 2 o co 2 16 camg co 3 2 4 h ca 2 mg 2 2h 2 o 2co 2 the experimentally measured concentration of ca2 becomes larger before and after the seepage solution which proves that h plays a key role in the dissolution reaction in other words the changes in the seepage flow and permeability in the test are mainly affected by the concentration of hydrogen ions however further analysis of the quantitative changes produced by the specific concentrations and other factors is needed the study investigated the seepage characteristics of a single fracture under different roughness and seepage fluids the quantitative relationships between permeability and jrc in different seepage cases were proposed through the eqs 11 and 13 respectively these two equations can be used to estimate the permeability of rock formations in conventional and sulfate rich groundwater environments under the conditions of a confusing pressure of 3 0 mpa and a seepage pressure of 0 5 mpa respectively further combining the eqs 6 and 8 the seepage flow of rock formations in the two corresponding cases can be obtained which can provide a theoretical basis for seepage control measures for the geological exploration prior to dam siting and tunnel excavation 4 conclusions in this study the seepage tests with different roughness and seepage fluids were conducted to analyze the effect of jrc and seepage solutions on the seepage parameters of a single rough fracture such as seepage flow equivalent hydraulic aperture and permeability furthermore quantitative relationships between jrc and permeability were proposed to further discuss the seepage characteristics of a single rough fracture in limestone the main findings and conclusions can be summarized as follows 1 a new method by combining 3d scanning and sculpting techniques was developed to prepare the five groups of limestone fracture surfaces with the same jrc values such as 2 13 7 07 12 31 15 67 and 18 27 there is a logarithmic relation between jrc and r s which can be used to characterize the fracture surface morphology parameter 2 the seepage flow equivalent hydraulic aperture and permeability of limestone fracture specimens with different jrc values decrease rapidly at the early times and slow down to approach a constant value the larger the jrc of the fracture surface the smaller the initial and stable values of each seepage parameter and the shorter the time required to reach the stabilization stage the permeability and jrc in the two cases are correlated by quadratic functions with the minimum r2 of 0 9263 3 the quantitative relationships among the permeability time and jrc values at constant confining and seepage pressures were presented the results show that the correlation between the permeability and the jrc can be fitted by quadratic functions for different seepage fluids while the empirical parameters are related to time by quadratic and exponential functions the proposed formulas are suitable to estimate fracture permeability in fractured limestone and reduce the potential for engineering leakage credit authorship contribution statement lei gan writing original draft writing review editing yu liu writing original draft teng xu supervision methodology writing review editing lei xu supervision methodology hongying ma conceptualization methodology writing review editing weichao xu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china no 52179130 no 51609073 and the natural science foundation of jiangsu province china no bk20201312 
