index,text
26270,vars tool is a software toolbox for sensitivity and uncertainty analysis developed primarily around the variogram analysis of response surfaces framework vars tool adopts a multi method approach that enables simultaneous generation of a range of sensitivity indices including ones based on derivative variance and variogram concepts from a single sample other special features of vars tool include 1 novel tools for time varying and time aggregate sensitivity analysis of dynamical systems models 2 highly efficient sampling techniques such as progressive latin hypercube sampling plhs that maximize robustness and rapid convergence to stable sensitivity estimates 3 factor grouping for dealing with high dimensional problems 4 visualization for monitoring stability and convergence 5 model emulation for handling model crashes and 6 an interface that allows working with any model in any programming language and operating system as a test bed for training and research vars tool provides a set of mathematical test functions and the dynamical hbv sask hydrologic model keywords global sensitivity analysis uncertainty analysis variogram analysis of response surface vars sobol morris progressive latin hypercube sampling plhs dynamical systems models sensitivity indices performance metrics software availability the vars tool software package as well as the hbv sask hydrologic model and its case studies can be downloaded from www vars tool com and be used free of charge for non commercial purposes 1 introduction earth and environmental systems models are widely employed for the simulation of complex physical processes that comprise the earth s natural and engineered systems bennett et al 2013 yassin et al 2017 they have become essential tools for management and decision making under uncertainty and non stationarity by providing the capability of prediction and support for scenario analysis regarding the quality and quantity of future earth s resources kwakkel et al 2016 maier et al 2016 these models continue to grow in complexity with our ever growing understanding of underlying system processes their heterogeneity and feedback mechanisms razavi et al 2012 tetzlaff et al 2008 this growth in complexity and presumably model fidelity has however resulted in large computationally intensive models with many sometimes hundreds of uncertain parameters and factors whose effects on model behavior need to be characterized and understood kaizer et al 2015 oreskes 2003 razavi 2017 the pressing need to characterize how uncertainty in model parameters translates into uncertainty in model predictions has spawned development of a range of methods and tools for uncertainty analysis rooted in probability theory in most cases these methods are based on the two traditional forward and inverse problem approaches the former propagates assumptions regarding uncertainties in system inputs or other properties such as parameters and or system structure through the model to obtain some understanding regarding uncertainties in the model predictions e g hong et al 2006 kunstmann et al 2002 conversely the latter uses the information contained in the mismatch between model predictions and data to help identify good values for the model parameters and to characterize their associated posterior uncertainty e g beven and binley 1992 smith and marshall 2008 vrugt et al 2003 a third complementary approach that has gained momentum in recent years is one based on the paradigm of sensitivity analysis sa which seeks to illuminate the controls on model behavior thereby characterizing the dominant controls on predictive uncertainty razavi and gupta 2015 a fundamental basis for sa is an effect called the sparsity of factors principle this principle which originates from the field of statistical design of experiments states that the behavior of a process involving several variables is likely to be driven primarily by a small subset of these variables box and meyer 1986 in sa where one aims to attribute the uncertainty in a model prediction to the uncertainties associated with different factors we seek to answer the critical question when does uncertainty matter imagine for example that sa could tell you the following about two parameters of a model i parameter a is highly uncertain but does not strongly influence the uncertainty in model prediction and ii parameter b is relatively certain while strongly influences and can therefore cause substantial uncertainty in the model prediction it is clear that uncertainty with regards to parameter b albeit small is a dominant control of uncertainty in the problem at hand whereas the large uncertainty in parameter a does not matter much such a characterization of uncertainty sources and their impacts is invaluable in guiding research towards reducing the uncertainties that matter as it may point to the most important aspects of the problem at hand early developments in sa were largely based on the notion of local sensitivity which derives point based sensitivity measures specified locally around a nominal point in the problem space razavi and gupta 2015 this traditional practice commonly referred to as local sensitivity analysis lsa is known to be incomplete and potentially misleading saltelli and annoni 2010 and has therefore evolved into a more advanced paradigm known as global sensitivity analysis gsa saltelli et al 2008 gsa can be defined as a systems theoretic approach to characterizing the overall average sensitivity of one or more model responses across the factor space by attributing the variability of those responses to different controlling but uncertain factors e g model parameters forcings and boundary and initial conditions gsa and sa in general has a variety of applications as outlined in razavi and gupta 2015 including uncertainty apportionment attribution of total uncertainty in model responses to different factors uncertainty sources to identify where best to focus efforts for improved factor characterization so as to reduce the total uncertainty e g chu agor et al 2011 diagnostic testing assessment of similarities between the functioning of the model and the underlying real world system so as to assess the fidelity of the model structure conceptualization and parameterization e g haghnegahdar et al 2017 factor prioritization and function identification of the factors that are more influential and contribute most significantly to variability and other characteristics of model response and to understanding their role and function e g muleta and nicklow 2005 factor and model reduction identification of non influential factors and or insensitive possibly redundant components of model structure and parametrization so that they can be constrained or removed to simplify the model analysis e g touzani and busby 2014 here we introduce a gsa toolbox called vars tool which includes a unique collection of state of the art algorithms and tools for any of the applications outlined above and beyond the toolbox was designed to address the needs of any user beginner to advanced with any level of background knowledge of gsa and computer programming it has been developed to improve upon existing software programs for gsa such as psuade gan et al 2014 safe pianosi et al 2015 dakota adams et al 2009 simlab jrc 2008 and uqlab marelli and sudret 2014 section 2 provides an overview of such improvements and the unique features of vars tool central to the development of vars tool is attention to ease of use and interpretability while vars tool was originally developed using matlab it has now also been written in c and built into the ostrich software toolkit that provides a model independent interface for connecting vars tool with any simulation model matott 2017 being under continuous development new capabilities features and implementations in other computer programming languages are forthcoming a well designed users manual provides detailed descriptions of the different functions within vars tool along with relevant step by step examples our ultimate goal is to promote best practices in gsa applications within the earth and environmental systems modelling community and beyond 2 why vars tool vars tool is a comprehensive multi approach multi algorithm toolbox equipped with a set of tools to enable gsa for any application with a primary focus on dynamical earth and environmental systems models it is developed around the vars variogram analysis of response surfaces theory and methodology which provides a general framework that utilizes directional variogram and covariogram functions to characterize global sensitivity thereby providing a comprehensive set of global sensitivity indices with minimal computational cost razavi and gupta 2016a vars was developed to address two major challenges associated with gsa as outlined in razavi and gupta 2015 ambiguous definition of global sensitivity different gsa methods are based in different philosophies and theoretical definitions of sensitivity leading to different even conflicting assessments of the underlying sensitivities for a given problem computational cost the cost of carrying out gsa can be large even excessive for high dimensional problems and or computationally intensive models where cost or efficiency is commonly assessed in terms of the number of required model runs the vars approach can be seen as a unifying theory for gsa that places the different gsa theories and methods available in the literature on a common foundation we say so because it re defines gsa by characterizing a comprehensive spectrum of information about the underlying sensitivities of a response surface to its factors including 1 local sensitivities i e the partial derivatives of model responses with respect to different factors and their global distributions across the factor space 2 the global distribution of model direct responses and the change in that distribution as a result of fixing one or groups of factors at different values within their uncertainty ranges and 3 the form and covariance structure of the response surface along the directions of different factors in the factor space the information types provided in points 1 and 2 above are respectively the bases for derivative based methods such as elementary effects morris 1991 and its extensions campolongo et al 2007 rakovec et al 2014 sobol and kucherenko 2009 and direct response based methods such as sobol variance based sobol 2001 higher order moment based skewness kurtosis dell oca et al 2017 and general distribution form based pianosi and wagener 2015 methods vars bridges these two philosophically different families of methods and further complements those with the information type provided in point 3 above an information type that is unique to vars accordingly vars introduces a novel and general variogram based paradigm for gsa that unifies and encompasses the pre existing widely used derivative based morris 1991 and variance based sobol 2001 approaches and their extensions as special limiting cases the theoretical relationship between vars and the derivative variance based approaches established in razavi and gupta 2016a enables vars to simultaneously generate both morris and sobol sensitivity indices including elementary effects and total order effects along with the recommended vars based ivars indices using a single common set of sample points in addition vars tackles the issue of sensitivity of the sensitivity analysis results to perturbation scale scale issue in gsa by providing sensitivity information spanning a range of scales across the factor space from small scale features such as roughness noise to large scale features such as multimodality haghnegahdar and razavi 2017 razavi and gupta 2015 a defining feature of vars is its high efficiency and statistical robustness enabling reliable and stable results with 1 2 orders of magnitude fewer sample points model runs than are required by alternative approaches such as the derivative and variance based counterparts razavi and gupta 2016b this computational efficiency is in part due to vars being based on the information contained in pairs of points rather than in individual points as a result vars can be used to effectively and efficiently handle high dimensional problems this feature is important because in practice computational cost is a major reason why most applications of gsa and of uncertainty analysis in general have been limited to low dimensional simple cheap to run models this is also related to the curse of dimensionality in which as the problem dimension e g number of parameters grows the volume of the problem space increases so rapidly exponentially that the available sample density becomes too sparse to be able to properly characterize the problem space as such the size of the sample required i e number of model runs for a stable robust and statistically sound assessment typically grows exponentially with dimension the recent survey by sheikholeslami et al 2018 finds that 70 percent of gsa applications in the environmental modelling literature have focused on models with less than 20 parameters which falls well below the numbers of factors that comprise complex state of the art models this may be seen as paradoxical to the underlying goal of gsa which is to facilitate understanding of the behaviors of complex models nowadays involving tens to hundreds or more of factors in addition to providing the first software implementation of the vars methodology vars tool includes a range of other algorithms and software tools as listed in table 1 and described below the list is continually being updated notably vars tool includes a a suite of gsa algorithms in addition to vars this feature enables the users to use an algorithm that they are most comfortable with and or to compare the performance and results of different algorithms together and with those provided by vars razavi and gupta 2016a 2016b b a suite of sampling strategies a particularly important feature is the implementation of progressive latin hypercube sampling plhs which enables the progressive generation of additional sample points in the factor space while continuing to preserve the distributional properties of interest sheikholeslami and razavi 2017 c novel tools for assessing the time varying nature of sensitivities of dynamic systems models this capability enables the user to compute time varying and time aggregate sensitivity indices of model state and output variables through the generalized global sensitivity matrix ggsm approach gupta and razavi 2018 razavi and gupta in review d a strategy for factor grouping based on their importance and function this feature enables classification of factors into groups which greatly facilitates the analysis of problems that are of very high dimension having large numbers of factors sheikholeslami et al 2018 e a measure of robustness and convergence of sensitivity analysis via statistical bootstrapping this feature enables estimation of confidence intervals on sensitivity indices as well as the reliability robustness and convergence of factor ranking or grouping over time throughout a gsa experiment razavi and gupta 2016b sheikholeslami et al 2018 f model emulation strategies to facilitate model crash handling this feature maintains the reliability and robustness of a gsa experiment when the simulation model used fails to return response values for some sample points in the factor space sheikholeslami et al in prep importantly vars tool comes with a visualization tool that can work in online mode to enable the user to monitor the real time performance and evolution of a gsa experiment and to process its intermediate results while assessing stability and convergence finally the inclusion of several test functions and real world case studies including the hbv sask rainfall runoff model enables the use of vars tool for a range of learning teaching and research purposes by incorporating the aforementioned diversity of tools and features within a single platform vars tool is intended to conveniently provide the user with the ingredients necessary for conducting exploratory research with a view to discovering new directions for advancing the field of sensitivity and uncertainty analysis in the remainder of this paper we provide details on the features and tools outlined above 3 sensitivity analysis algorithms vars tool includes a range of well known model free or model independent gsa algorithms that work with any model of any degree of complexity these algorithms are based on derivative and variance based approaches that are commonly used and reported in the literature and the recently developed more general variogram based approach that bridges across the aforementioned two approaches a brief description of the algorithms is given below and details regarding their numerical implementation sampling strategies and computational costs are provided in section 5 the derivative based approach this approach is a natural extension to local sensitivity analysis wherein the partial derivatives of a model response with respect to different model inputs at one base point are computed numerically and interpreted as indices of local sensitivity the associated algorithm see table 1 generates globally aggregated measures of local sensitivities by computing the partial derivatives called elementary effects by morris 1991 at many sample points and combining them together in some way e g by taking the mean and or standard deviation to generate indices for global sensitivity each index is different in the way that it characterizes the distributional properties of partial derivatives for details see razavi and gupta 2015 the vars tool function main morris m implements the algorithm and computes different variations of elementary effects based sensitivity indices proposed by morris 1991 campolongo et al 2007 and sobol and kucherenko 2009 the user is required to select a step size for numerical approximation of the partial derivatives which is typically recommended arbitrarily to be 1 10 of the input range the variance based approach this approach is based on analysis and decomposition of the variance of model response so as to interpret the contribution of different factors in explaining this variance as an index for global sensitivity the vars tool function main sobol m fig 1 implements the algorithm of saltelli et al 2008 to decompose the total variance of model response when all the factors are varied within user selected ranges into its components arising from individual inputs and their interactions algorithm outputs include the main effects associated with individual factors discarding the role of possible interactions with any other factors and the total order effects that combine the contribution of individual factors and all higher order interactions with other factors typically the total order effect is considered to be an effective index for global factor sensitivity the variogram based approach this approach is based on variogram analysis of response surfaces vars that provides a general comprehensive framework that unifies and extends upon both the derivative and variance based approaches vars was specifically developed to address a major weakness of the aforementioned approaches which is that neither considers or accounts for the spatially ordered structure of the model response in the factor space in other words they ignore the fact that the response values are not randomly distributed throughout the factor space conversely vars recognizes that there is a spatially continuous covariance structure to the model response and hence also to its partial derivatives to extract and analyze this structural information vars uses anisotropic variogram and covariogram functions of the model response to generate directional variograms associated with each of the model factors as a basis for a comprehensive characterization of global sensitivity the directional variogram represents the variance i e rate of variability of the response caused by perturbing that factor across a full range of perturbation scales while all other factors are also varied in the factor space fig 2 shows schematic directional variogram and covariogram functions and how they relate to derivative and variance based approaches by definition the left end point of a variogram where the perturbation scale h is small represents derivative information of the underlying process while its right end point where h is large represents the process variance the variance based total order effect can be estimated via mathematical manipulation of the variogram and covariogram functions for the detailed analytical relationships between the variogram derivative and variance based theories see razavi and gupta 2016a the sensitivity indices in vars are computed by integrating the directional variograms resulting in a comprehensive set of indices for global sensitivity called integrated variograms across a range of scales or ivars the ivars50 index also referred to as the total variogram effect that integrates the variogram across the full range of perturbation scales is the most comprehensive variogram based index for global sensitivity a specific implementation of vars called star vars is incorporated within vars tool this implementation utilizes a form of star based sampling called star developed by razavi and gupta 2016b the vars tool function main vars m computes the ivars indices for global sensitivity along with the aforementioned derivative and variance based indices all together in a single run that utilizes the same sample points therefore this function is a multi method platform that provides the user with a sensitivity assessment that simultaneously includes the derivative variance and variogram based indices fig 3 shows an example input text file to this function where the user can provide the specifications of model factor ranges algorithm parameters etc the text file based interface with this function was designed for ease of use to be comprehensive vars tool also includes an implementation of the so called regional sensitivity analysis rsa approach that is essentially monte carlo filtering hamby 1994 spear et al 1994 the heuristic rsa approach is commonly used to partition the marginal distribution of sample points obtained for each factor into two or more distributions based on empirically selected threshold values for model response the idea is that if the factor does not have a significant impact on model response throughout the factor space the two distributions should be statistically indistinguishable the vars tool function main rsa m implements this approach and utilizes the smirnov test to quantify the extent to which the two distributions are different thereby providing indices for global sensitivity other classic model based gsa approaches such as factorial design correlation and regression based approaches are not included in vars tool because of their limited utility in the design and analysis of computer based simulation experiments these approaches are generally unsuitable for sensitivity analysis of complex dynamical systems models as they are based in the a priori assumption of a particular mathematical form typically linear or polynomial for the underlying model response surface in addition these methods are particularly prone to curse of dimensionality see discussion in razavi and gupta 2015 4 sensitivity analysis of dynamical systems models most approaches to global sensitivity analysis gsa do not adequately account for the dynamical nature of earth and environmental systems models the fact that such models in real world applications produce dynamical time evolving responses to dynamical time evolving perturbations inputs gupta and razavi 2018 highlighted this fact revisited the fundamental basis of gsa for dynamical systems models and developed a sensitivity analysis framework from first principles based on computation of a global sensitivity matrix gsm that quantifies the sensitivity information contained in trajectories of partial derivatives of the dynamical model responses with respect to controlling factors razavi and gupta in review extended and generalized this approach to accommodate a any gsa philosophy including derivative variance and variogram based approaches and b any model response of any type including time series of model state or output variables and their transformations this approach is referred to as generalized global sensitivity matrix ggsm vars tool includes an efficient implementation of the ggsm approach coupled with star vars that enables a multi method gsa of dynamical systems models that accounts for the temporal dynamics of such models while enabling efficient comparison of the results provided by philosophically different approaches e g derivative based versus variance based this implementation generates the following types of indices of global sensitivity a time varying sensitivity indices these indices are in the form of time series that reveal the time dependent sensitivities of model responses over the course of a simulation time period to its controlling factors b time aggregate sensitivity indices these indices are in the form of summary statistics that aggregate and summarize the dynamical sensitivity information of model responses over one or multiple time periods of interest the time varying indices enable the user to better understand the model system behavior over time in response to system forcings boundary conditions this capability facilitates i diagnostic testing and detection of potential defects in different parts of a model thereby helping to improve model realism and ii attribution of variability and therefore uncertainty in the model responses to different factors model parameters forcings boundary conditions etc thereby helping to pinpoint the dominant controls of predictive uncertainty at different points in time when summary information regarding the time varying sensitivities is required the time aggregate indices become more useful a second defining feature of the ggsm approach is that it does not require that observed data on system responses be available and so can be used to assess the internal functioning of a model and the controls exerted by different factors of any of its components including both states and fluxes this is important because 1 our interest when conducting a gsa is not limited to response variables for which observations are available and 2 the sensitivity assessments cannot be obscured or distorted by errors and uncertainties that unavoidably exist in observed response data gupta and razavi 2018 the vars tool function main vars m is enabled with the ggsm approach which can be activated by a switch on line 22 of vars inp txt fig 3 or within the source code upon activation vars tool can generate within a single run i e a single sample set time varying and time aggregate gsa indices based on the derivative based e g morris elementary effects variance based e g sobol total order effects and variogram based e g vars total variogram effects approaches this capability enables a user to explore compare and contrast the assessments provided by these three approaches to gsa 5 sampling strategies sampling strategies are necessary and fundamental components of any algorithm for sensitivity and uncertainty analysis of computer simulation models vars tool includes a variety of sampling strategies as listed in table 1 the derivative variance and variogram based gsa algorithms outlined above all follow a two level sampling procedure in the factor space in the first level they can employ any of the general purpose randomized sampling strategies listed in table 1 to select a set of n base points randomly distributed uniformly throughout the factor space in the second level these points are used as starting points for the algorithm specific structured sampling strategies used to acquire the remaining sample points required for calculating the algorithm specific sensitivity indices vars tool includes both single stage and progressive also called sequential or multi stage strategies for random sampling because single stage sampling strategies such as latin hypercube sampling lhs mckay et al 1979 and symmetric latin hypercube sampling slhs ye et al 2000 generate an entire sample consisting of a pre specified number of points all at once such strategies are effective only if the user is fairly confident about the proper sample size before beginning the analysis as any subsequent enlargement of the sample size will generally fail to preserve the distributional properties of interest in contrast progressive sampling strategies enable sequential or multi stage generation of the sample points examples include the traditional halton and sobol sequences also called low discrepancy sequences and the novel method of progressive latin hypercube sampling plhs recently developed by sheikholeslami and razavi 2017 note that vars tool provides the first available software implementation of plhs plhs sequentially generates sample points while progressively preserving important distributional properties of interest latin hypercube properties space filling etc as the sample size grows fig 4 illustrates how plhs successively generates a series of smaller sub samples slices such that 1 the first slice is latin hypercube 2 the progressive union of slices remains latin hypercube and achieves maximum stratification in any one dimensional projection and as such 3 the entire sample is latin hypercube plhs has been shown to be a superior strategy that scales effectively with the size and dimensionality of the problem under investigation sheikholeslami and razavi 2017 once the set consisting of n base points is generated randomly by any of the sampling strategies mentioned above it is used by the structured sampling strategies as starting points for the generation of the remaining sample points in the derivative based approach campolongo et al 2007 morris 1991 each of the n base points serves as the starting point for a chain of points constructed by changing one factor at a time by a given step size resulting in a total number of n d 1 sample points where d is the number of factors in the variance based approach saltelli et al 2008 structured sampling is conducted via matrix manipulation of the base sample matrix consisting of the n base points resulting in a total number of n d 2 sample points in the variogram based approach and particularly in the star vars implementation the n base points are used as star centers for star sampling razavi and gupta 2016b in this approach equally spaced points are sampled δh apart around each star center along every dimension of the factor space fig 5 resulting in a total number of n d 1 δh 1 1 sample points the distance δh referred to as the resolution of sampling is specified by the user a δh of 0 1 of the factor range is recommended while smaller values can be selected to yield more accurate results 6 dealing with high dimensional problems factor grouping the challenge of conducting a gsa monitoring its convergence and robustness and interpreting the associated results can easily become non trivial when the problem has more than 20 30 factors therefore vars tool includes an innovative factor grouping strategy that employs a clustering mechanism developed by sheikholeslami et al 2018 to handle high dimensional problems involving tens to hundreds of factors this strategy integrated together with bootstrapping see section 7 1 monitors gsa performance and clusters factors into groups of similar properties based on their sensitivity and function the resulting groups can be of any size depending on the problem at hand for example in a problem with 60 factors vars tool can be used to cluster the factors into 5 groups of factors termed strongly influential influential moderately influential weakly influential and non influential the number of groups of interest can be directly specified by the user in vars inp txt as a convenient feature however if the user does not specify a preferred number of groups vars tool will suggest an optimal number of groups based on maximizing the distinctions between the groups through the so called elbow method sheikholeslami et al 2018 fig 6 shows an example dendrogram generated by vars tool and the associated optimal grouping this grouping capability is particularly beneficial when dealing with high dimensional problems where the user typically does not need to obtain an exact ranking for the many factors for example in a problem with 100 factors it may not matter whether a factor is the 30th or the 31st in terms of importance and obtaining a robust answer to this question may require running the model an excessively large number of times instead by assigning factors having similar importance into groups vars tool provides a means for making the problem more tractable within a limited number of model runs this capability scales the computational demand of a gsa algorithm with the users need in terms of precision in factor rankings and as such results in significant improvements in stability and rate of convergence 7 other important features of vars tool 7 1 bootstrapping characterizing confidence and robustness in any analysis based on statistical sampling it is essential to be able to assess the degree of confidence one can place in the results so as to get a sense of reliability robustness and sufficiency of the analysis in vars tool the method of statistical bootstrapping is used to 1 infer confidence intervals around the estimates of sensitivity indices at any user specified confidence level and 2 provide an assessment of the robustness that can be associated with the factor rankings while the use of bootstrapping for the former is common in gsa literature in the latter the definition and implementation of robustness sometimes termed reliability as well of a factor ranking developed by razavi and gupta 2016b and further extended by sheikholeslami et al 2018 for groups of factors is unique to vars tool consider for example a problem with five factors after 200 model runs sample size 200 one may find that the sensitivity index associated with factor 3 has been estimated to have a value of 0 4 but with a confidence interval of 0 3 0 45 at the 90 confidence level meanwhile its factor ranking may be estimated as 2 meaning that it is the second most influential of the 5 factors with an assessed robustness of 70 meaning that factor 3 is exactly rank 2 with 70 probability with larger sample sizes we can expect that confidence intervals will become progressively narrower and the robustness estimates progressively improve thereby increasing our confidence in the results of the statistical analysis note that the reason that we will typically not have 100 confidence in the results is that the analysis can be affected by sampling variability in the generation of the sample points used and so can provide somewhat different results if the experiment is re run with a different but equally representative set of samples however increasing the sample size can help to improve the reliability and robustness of the analysis so for example increasing the sample size to 500 in the example above could alter the sensitivity index of factor 3 to have a value of 0 38 with a 90 confidence interval of 0 35 0 41 and a factor ranking of 2 but with an increased robustness of 90 in this way statistical bootstrapping provides a way of assessing the robustness stability and convergence of a gsa experiment with narrower 90 or 95 etc confidence intervals and higher factor ranking robustness indicating greater reliability of the results as a caveat note that statistical bootstrapping efron 1992 is a procedure based in sampling with replacement from the existing set of sample points to obtain estimates of the underlying statistical properties of a sample and so its validity depends on the assumption that the existing sample is a sufficiently representative sample of the underlying population beran 1997 davison et al 2003 in practice this means that the sample size used for the analysis must be large enough to adequately span the factor space of interest one way to assess this is to repeat the analysis with progressively larger sample sizes which is easily facilitated by the plhs method of progressive sampling discussed in section 5 7 2 reporting and visualization monitoring stability and convergence vars tool provides an on line visualization and reporting capability that enables monitoring the execution of gsa experiments and evaluation of their intermediate progressive performance fig 7 shows example plots generated using vars tool for the razavi gupta s 6d multi scale wavy function developed in razavi and gupta 2016a in on line mode these plots enable the user to monitor how the estimates of factor sensitivities and rankings may be changing with increasing sample size as more model evaluations become available along with comprehensive frequently generated report files provided in text format in the course of a gsa experiment these plots provide the capability to visually monitor robustness stability and convergence and terminate an analysis when required furthermore this capability in conjunction with a sequential sampling strategy e g plhs can resolve the problem that a user will typically not know a priori what a suitable sample size may be for a gsa experiment as a result the gsa can run only for an optimal number of model runs thereby maximizing computational efficiency 7 3 model emulation handling model crashes an issue that is known to arise when running some complex earth and environmental systems models is that they may fail while running under particular factor configurations and values such failures often called crashes can for example be caused by specifying unrealistic combinations for the parameter values that violate model assumptions or by numerical instabilities that arise under some combinations of parameters and forcings in practice if the occurrence of such model failures is not properly dealt with the entire gsa experiment can fail most implementations of gsa algorithms require that meaningful model responses be provided for every sample point generated however execution of the model code may fail to return a response if it crashes when this happens modellers often end up re doing the entire analysis with more conservative tighter factor ranges resulting in extra computational cost model runs and also changing the definition of the original problem to address this issue effectively and efficiently vars tool is enabled with a crash handling module that is designed to complete the gsa while minimizing the impact of model crashes on the analysis for this purpose vars tool employs model emulation techniques razavi et al 2012 to generate surrogate values of model outputs when a model fails theoretical details of this module are available in sheikholeslami et al in prep 7 4 running with models implemented in any programming language and operating system vars tool programs both the matlab version and c version implemented within ostrich have the capability to work with any model implemented in any programming language and running on any operating system vars tool has two modes for execution on line internal this mode can be used when the computer simulation model is set up to be called and run through the matlab or ostrich environments off line external this mode is to be used when the computer simulation model needs to be run externally outside the matlab or ostrich environments in the off line mode the model simulations can be performed independently of the vars tool software programs in this mode 1 vars tool first generates the locations for all the sample points required for the analysis and stores them in a text file 2 then the user should conduct model simulations externally for all of the sampled points and store the respective model response values e g in a text file these runs can be parallelized if multiple processors are available 3 vars tool reads in the model runs results conducts the analysis and generates sensitivity indices importantly the vars tool version within ostrich supports message passing interface mpi based parallel processing on both windows and linux systems which can significantly increase computational efficiency of a gsa experiment by parallelizing model runs 8 test functions test functions are useful for learning understanding testing and benchmarking the performance of different gsa algorithms they have the advantage that 1 they are computationally inexpensive and fast to run and 2 their characteristics are known a priori and in most cases controllable by the user vars tool embeds a diverse set of test functions demonstrating a wide range of possible characteristics such as nonlinearity unimodality and multi modality irregularity roughness and smoothness interaction and high dimensionality these test functions provide artificial response surfaces also called landscapes that may share some characteristics with the response surfaces of real world modelling problems for a discussion of the latter see duan et al 1992 in addition to including test functions designed originally for research on sensitivity and uncertainty analysis vars tool includes functions popular in the field of optimization research the test functions are listed in table 1 and the response surfaces for two of them g function and 6d multi scale wavy function are shown in fig 8 when choosing a test function for a particular analysis users might first consider which test function presents similar structure and characteristics to those of the underlying response surface of the problem at hand this question is non trivial but razavi and gupta 2015 provide some insight 9 hydrologic case studies vars tool includes software of a hydrologic model called hbv sask which is an interpretation of hydrologiska byråns vattenbalansavdelning model lindström et al 1997 developed by the first author for educational and research purposes fig 9 shows the architecture of hbv sask and its inputs process parameters and equations in addition to those process parametrizations daily potential evapotranspiration pet is computed using equation pet 1 etf t tmth petmth where tmth and petmth are long term average monthly temperature and potential evapotranspiration respectively supplied as input data to the model along with daily precipitation p and temperature t table 2 lists the model parameters including etf in the equation above their description and feasible ranges the implementation is modular with each module simulating a different hydrologic process thereby allowing the user to easily investigate the different model flux and state variables see fig 10 hbv sask in vars tool comes with ready to run case studies for two watersheds bow river at banff with area of 2178 53 km2 and oldman river at waldron s corner with an area of 1434 73 km2 these rivers are located in the rocky mountains in alberta canada and flow into the saskatchewan river basin fig 11 historical data is available for the periods 1950 2011 and 1979 2008 respectively from which we estimate average annual precipitation rainfall snowfall to be 795 mm bow and 611 mm oldman and average annual streamflow to be 38 6 m3 s at gauge 05bb001 on the bow river and 11 7 m3 s at gauge 05aa023 on the oldman river the bow and oldman basins differ in their hydrological properties and have runoff ratios of approximately 0 7 and 0 42 respectively vars tool also comes with a ready to use set up of the mesh modélisation environmentale communautaire surface hydrology land surface hydrology model pietroniro et al 2006 mesh couples the canadian land surface scheme class verseghy et al 1993 verseghy 1991 with the hydrologic routing schemes of watflood kouwen et al 1993 this model set up is interfaced with vars tool via the ostrich toolkit matott 2017 the case studies with mesh are adopted from haghnegahdar et al 2017 details on how to run this case study are available in the vars tool manual 10 concluding remarks significant theoretical and practical advances have been made in recent years in the field of sensitivity and uncertainty analyses these analyses need to become an integral part of any model development prediction and decision making process providing insight into various issues such as uncertainty apportionment diagnostic testing planning and management and policy prioritization best practices are however often hampered by computational burden and lack of transparency and interpretability vars tool is designed as a computationally efficient and transparent platform to reconcile the state of the art theories with real world applications that involve complex high dimensional and computationally expensive models of dynamical systems in summary vars tool aims to promote best practices in sensitivity analysis that should be based on a multi method approach that brings together the different theories and aspects of sensitivity analysis thereby providing a more comprehensive assessment of the problem at hand using for example the variogram analysis of response surfaces vars approach see section 3 proper accounting for the dynamical properties of earth and environmental systems models using for example the generalized global sensitivity matrix ggsm approach see section 4 proper sampling of the problem space that effectively scales with the size of the problem at hand and available computational budget using for example progressive latin hypercube sampling plhs see section 5 methods that are capable of handling high dimensional problems such as advanced earth and environmental systems models that tend to involve hundreds of factors whose influence on model response needs to be characterized using for example a factor grouping strategy see section 6 proper characterization of robustness and stability of the algorithms to sampling variability and of the degree of confidence and reliability that users can have in sensitivity results using for example a bootstrap technique see section 7 vars tool is under continuous development to include new theoretical advances in the field of sensitivity and uncertainty analysis forthcoming additions will include but are not limited to a the ability to carry out sensitivity analysis on any sample data of a process however collected see e g borgonovo et al 2016 we believe this ability is needed for two reasons i the utility of sensitivity analysis may not be limited to models and their artifacts and one may directly apply sensitivity analysis to data to characterize the behavior of the underlying system processes ii most sensitivity analysis algorithms require samples taken in a specific manner which limits their utility when a sample set already exists b the ability to handle problems with non uniform and or correlated factors see e g kucherenko et al 2012 most algorithms for sensitivity analysis assume uniform a priori distributions for the factors this assumption limits the applicability and validity of results if the true distribution of factors is non uniform and or correlated c the ability to provide a more informative characterization of interaction effects see e g razavi and gupta 2015 very few existing approaches to sensitivity analysis can provide a meaningful and or easily interpretable assessment of interaction effects there is a need to rethink the fundamentals of interaction effects in relationship with the correlation effects point b above and to develop more informative tools for their characterization in support of model development and prediction the field of sensitivity analysis is still young and evolving it has the potential to provide a solid foundation based on which different sources of uncertainty which may by essence be irreconcilable can be compared and assessed further it has the potential to help us isolate assess and deal with the deep uncertainties in our future earth and environmental systems that occur in the context of climatic environmental and social change as always we invite discussion and collaboration on any of these and other issues related to diagnostic evaluation and improvement of dynamical systems models especially with regard to high dimensional representations of complex systems acknowledgements the first second and fourth authors were supported in part by the first author s nserc natural sciences and engineering research council of canada discovery grant the third author received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 
26270,vars tool is a software toolbox for sensitivity and uncertainty analysis developed primarily around the variogram analysis of response surfaces framework vars tool adopts a multi method approach that enables simultaneous generation of a range of sensitivity indices including ones based on derivative variance and variogram concepts from a single sample other special features of vars tool include 1 novel tools for time varying and time aggregate sensitivity analysis of dynamical systems models 2 highly efficient sampling techniques such as progressive latin hypercube sampling plhs that maximize robustness and rapid convergence to stable sensitivity estimates 3 factor grouping for dealing with high dimensional problems 4 visualization for monitoring stability and convergence 5 model emulation for handling model crashes and 6 an interface that allows working with any model in any programming language and operating system as a test bed for training and research vars tool provides a set of mathematical test functions and the dynamical hbv sask hydrologic model keywords global sensitivity analysis uncertainty analysis variogram analysis of response surface vars sobol morris progressive latin hypercube sampling plhs dynamical systems models sensitivity indices performance metrics software availability the vars tool software package as well as the hbv sask hydrologic model and its case studies can be downloaded from www vars tool com and be used free of charge for non commercial purposes 1 introduction earth and environmental systems models are widely employed for the simulation of complex physical processes that comprise the earth s natural and engineered systems bennett et al 2013 yassin et al 2017 they have become essential tools for management and decision making under uncertainty and non stationarity by providing the capability of prediction and support for scenario analysis regarding the quality and quantity of future earth s resources kwakkel et al 2016 maier et al 2016 these models continue to grow in complexity with our ever growing understanding of underlying system processes their heterogeneity and feedback mechanisms razavi et al 2012 tetzlaff et al 2008 this growth in complexity and presumably model fidelity has however resulted in large computationally intensive models with many sometimes hundreds of uncertain parameters and factors whose effects on model behavior need to be characterized and understood kaizer et al 2015 oreskes 2003 razavi 2017 the pressing need to characterize how uncertainty in model parameters translates into uncertainty in model predictions has spawned development of a range of methods and tools for uncertainty analysis rooted in probability theory in most cases these methods are based on the two traditional forward and inverse problem approaches the former propagates assumptions regarding uncertainties in system inputs or other properties such as parameters and or system structure through the model to obtain some understanding regarding uncertainties in the model predictions e g hong et al 2006 kunstmann et al 2002 conversely the latter uses the information contained in the mismatch between model predictions and data to help identify good values for the model parameters and to characterize their associated posterior uncertainty e g beven and binley 1992 smith and marshall 2008 vrugt et al 2003 a third complementary approach that has gained momentum in recent years is one based on the paradigm of sensitivity analysis sa which seeks to illuminate the controls on model behavior thereby characterizing the dominant controls on predictive uncertainty razavi and gupta 2015 a fundamental basis for sa is an effect called the sparsity of factors principle this principle which originates from the field of statistical design of experiments states that the behavior of a process involving several variables is likely to be driven primarily by a small subset of these variables box and meyer 1986 in sa where one aims to attribute the uncertainty in a model prediction to the uncertainties associated with different factors we seek to answer the critical question when does uncertainty matter imagine for example that sa could tell you the following about two parameters of a model i parameter a is highly uncertain but does not strongly influence the uncertainty in model prediction and ii parameter b is relatively certain while strongly influences and can therefore cause substantial uncertainty in the model prediction it is clear that uncertainty with regards to parameter b albeit small is a dominant control of uncertainty in the problem at hand whereas the large uncertainty in parameter a does not matter much such a characterization of uncertainty sources and their impacts is invaluable in guiding research towards reducing the uncertainties that matter as it may point to the most important aspects of the problem at hand early developments in sa were largely based on the notion of local sensitivity which derives point based sensitivity measures specified locally around a nominal point in the problem space razavi and gupta 2015 this traditional practice commonly referred to as local sensitivity analysis lsa is known to be incomplete and potentially misleading saltelli and annoni 2010 and has therefore evolved into a more advanced paradigm known as global sensitivity analysis gsa saltelli et al 2008 gsa can be defined as a systems theoretic approach to characterizing the overall average sensitivity of one or more model responses across the factor space by attributing the variability of those responses to different controlling but uncertain factors e g model parameters forcings and boundary and initial conditions gsa and sa in general has a variety of applications as outlined in razavi and gupta 2015 including uncertainty apportionment attribution of total uncertainty in model responses to different factors uncertainty sources to identify where best to focus efforts for improved factor characterization so as to reduce the total uncertainty e g chu agor et al 2011 diagnostic testing assessment of similarities between the functioning of the model and the underlying real world system so as to assess the fidelity of the model structure conceptualization and parameterization e g haghnegahdar et al 2017 factor prioritization and function identification of the factors that are more influential and contribute most significantly to variability and other characteristics of model response and to understanding their role and function e g muleta and nicklow 2005 factor and model reduction identification of non influential factors and or insensitive possibly redundant components of model structure and parametrization so that they can be constrained or removed to simplify the model analysis e g touzani and busby 2014 here we introduce a gsa toolbox called vars tool which includes a unique collection of state of the art algorithms and tools for any of the applications outlined above and beyond the toolbox was designed to address the needs of any user beginner to advanced with any level of background knowledge of gsa and computer programming it has been developed to improve upon existing software programs for gsa such as psuade gan et al 2014 safe pianosi et al 2015 dakota adams et al 2009 simlab jrc 2008 and uqlab marelli and sudret 2014 section 2 provides an overview of such improvements and the unique features of vars tool central to the development of vars tool is attention to ease of use and interpretability while vars tool was originally developed using matlab it has now also been written in c and built into the ostrich software toolkit that provides a model independent interface for connecting vars tool with any simulation model matott 2017 being under continuous development new capabilities features and implementations in other computer programming languages are forthcoming a well designed users manual provides detailed descriptions of the different functions within vars tool along with relevant step by step examples our ultimate goal is to promote best practices in gsa applications within the earth and environmental systems modelling community and beyond 2 why vars tool vars tool is a comprehensive multi approach multi algorithm toolbox equipped with a set of tools to enable gsa for any application with a primary focus on dynamical earth and environmental systems models it is developed around the vars variogram analysis of response surfaces theory and methodology which provides a general framework that utilizes directional variogram and covariogram functions to characterize global sensitivity thereby providing a comprehensive set of global sensitivity indices with minimal computational cost razavi and gupta 2016a vars was developed to address two major challenges associated with gsa as outlined in razavi and gupta 2015 ambiguous definition of global sensitivity different gsa methods are based in different philosophies and theoretical definitions of sensitivity leading to different even conflicting assessments of the underlying sensitivities for a given problem computational cost the cost of carrying out gsa can be large even excessive for high dimensional problems and or computationally intensive models where cost or efficiency is commonly assessed in terms of the number of required model runs the vars approach can be seen as a unifying theory for gsa that places the different gsa theories and methods available in the literature on a common foundation we say so because it re defines gsa by characterizing a comprehensive spectrum of information about the underlying sensitivities of a response surface to its factors including 1 local sensitivities i e the partial derivatives of model responses with respect to different factors and their global distributions across the factor space 2 the global distribution of model direct responses and the change in that distribution as a result of fixing one or groups of factors at different values within their uncertainty ranges and 3 the form and covariance structure of the response surface along the directions of different factors in the factor space the information types provided in points 1 and 2 above are respectively the bases for derivative based methods such as elementary effects morris 1991 and its extensions campolongo et al 2007 rakovec et al 2014 sobol and kucherenko 2009 and direct response based methods such as sobol variance based sobol 2001 higher order moment based skewness kurtosis dell oca et al 2017 and general distribution form based pianosi and wagener 2015 methods vars bridges these two philosophically different families of methods and further complements those with the information type provided in point 3 above an information type that is unique to vars accordingly vars introduces a novel and general variogram based paradigm for gsa that unifies and encompasses the pre existing widely used derivative based morris 1991 and variance based sobol 2001 approaches and their extensions as special limiting cases the theoretical relationship between vars and the derivative variance based approaches established in razavi and gupta 2016a enables vars to simultaneously generate both morris and sobol sensitivity indices including elementary effects and total order effects along with the recommended vars based ivars indices using a single common set of sample points in addition vars tackles the issue of sensitivity of the sensitivity analysis results to perturbation scale scale issue in gsa by providing sensitivity information spanning a range of scales across the factor space from small scale features such as roughness noise to large scale features such as multimodality haghnegahdar and razavi 2017 razavi and gupta 2015 a defining feature of vars is its high efficiency and statistical robustness enabling reliable and stable results with 1 2 orders of magnitude fewer sample points model runs than are required by alternative approaches such as the derivative and variance based counterparts razavi and gupta 2016b this computational efficiency is in part due to vars being based on the information contained in pairs of points rather than in individual points as a result vars can be used to effectively and efficiently handle high dimensional problems this feature is important because in practice computational cost is a major reason why most applications of gsa and of uncertainty analysis in general have been limited to low dimensional simple cheap to run models this is also related to the curse of dimensionality in which as the problem dimension e g number of parameters grows the volume of the problem space increases so rapidly exponentially that the available sample density becomes too sparse to be able to properly characterize the problem space as such the size of the sample required i e number of model runs for a stable robust and statistically sound assessment typically grows exponentially with dimension the recent survey by sheikholeslami et al 2018 finds that 70 percent of gsa applications in the environmental modelling literature have focused on models with less than 20 parameters which falls well below the numbers of factors that comprise complex state of the art models this may be seen as paradoxical to the underlying goal of gsa which is to facilitate understanding of the behaviors of complex models nowadays involving tens to hundreds or more of factors in addition to providing the first software implementation of the vars methodology vars tool includes a range of other algorithms and software tools as listed in table 1 and described below the list is continually being updated notably vars tool includes a a suite of gsa algorithms in addition to vars this feature enables the users to use an algorithm that they are most comfortable with and or to compare the performance and results of different algorithms together and with those provided by vars razavi and gupta 2016a 2016b b a suite of sampling strategies a particularly important feature is the implementation of progressive latin hypercube sampling plhs which enables the progressive generation of additional sample points in the factor space while continuing to preserve the distributional properties of interest sheikholeslami and razavi 2017 c novel tools for assessing the time varying nature of sensitivities of dynamic systems models this capability enables the user to compute time varying and time aggregate sensitivity indices of model state and output variables through the generalized global sensitivity matrix ggsm approach gupta and razavi 2018 razavi and gupta in review d a strategy for factor grouping based on their importance and function this feature enables classification of factors into groups which greatly facilitates the analysis of problems that are of very high dimension having large numbers of factors sheikholeslami et al 2018 e a measure of robustness and convergence of sensitivity analysis via statistical bootstrapping this feature enables estimation of confidence intervals on sensitivity indices as well as the reliability robustness and convergence of factor ranking or grouping over time throughout a gsa experiment razavi and gupta 2016b sheikholeslami et al 2018 f model emulation strategies to facilitate model crash handling this feature maintains the reliability and robustness of a gsa experiment when the simulation model used fails to return response values for some sample points in the factor space sheikholeslami et al in prep importantly vars tool comes with a visualization tool that can work in online mode to enable the user to monitor the real time performance and evolution of a gsa experiment and to process its intermediate results while assessing stability and convergence finally the inclusion of several test functions and real world case studies including the hbv sask rainfall runoff model enables the use of vars tool for a range of learning teaching and research purposes by incorporating the aforementioned diversity of tools and features within a single platform vars tool is intended to conveniently provide the user with the ingredients necessary for conducting exploratory research with a view to discovering new directions for advancing the field of sensitivity and uncertainty analysis in the remainder of this paper we provide details on the features and tools outlined above 3 sensitivity analysis algorithms vars tool includes a range of well known model free or model independent gsa algorithms that work with any model of any degree of complexity these algorithms are based on derivative and variance based approaches that are commonly used and reported in the literature and the recently developed more general variogram based approach that bridges across the aforementioned two approaches a brief description of the algorithms is given below and details regarding their numerical implementation sampling strategies and computational costs are provided in section 5 the derivative based approach this approach is a natural extension to local sensitivity analysis wherein the partial derivatives of a model response with respect to different model inputs at one base point are computed numerically and interpreted as indices of local sensitivity the associated algorithm see table 1 generates globally aggregated measures of local sensitivities by computing the partial derivatives called elementary effects by morris 1991 at many sample points and combining them together in some way e g by taking the mean and or standard deviation to generate indices for global sensitivity each index is different in the way that it characterizes the distributional properties of partial derivatives for details see razavi and gupta 2015 the vars tool function main morris m implements the algorithm and computes different variations of elementary effects based sensitivity indices proposed by morris 1991 campolongo et al 2007 and sobol and kucherenko 2009 the user is required to select a step size for numerical approximation of the partial derivatives which is typically recommended arbitrarily to be 1 10 of the input range the variance based approach this approach is based on analysis and decomposition of the variance of model response so as to interpret the contribution of different factors in explaining this variance as an index for global sensitivity the vars tool function main sobol m fig 1 implements the algorithm of saltelli et al 2008 to decompose the total variance of model response when all the factors are varied within user selected ranges into its components arising from individual inputs and their interactions algorithm outputs include the main effects associated with individual factors discarding the role of possible interactions with any other factors and the total order effects that combine the contribution of individual factors and all higher order interactions with other factors typically the total order effect is considered to be an effective index for global factor sensitivity the variogram based approach this approach is based on variogram analysis of response surfaces vars that provides a general comprehensive framework that unifies and extends upon both the derivative and variance based approaches vars was specifically developed to address a major weakness of the aforementioned approaches which is that neither considers or accounts for the spatially ordered structure of the model response in the factor space in other words they ignore the fact that the response values are not randomly distributed throughout the factor space conversely vars recognizes that there is a spatially continuous covariance structure to the model response and hence also to its partial derivatives to extract and analyze this structural information vars uses anisotropic variogram and covariogram functions of the model response to generate directional variograms associated with each of the model factors as a basis for a comprehensive characterization of global sensitivity the directional variogram represents the variance i e rate of variability of the response caused by perturbing that factor across a full range of perturbation scales while all other factors are also varied in the factor space fig 2 shows schematic directional variogram and covariogram functions and how they relate to derivative and variance based approaches by definition the left end point of a variogram where the perturbation scale h is small represents derivative information of the underlying process while its right end point where h is large represents the process variance the variance based total order effect can be estimated via mathematical manipulation of the variogram and covariogram functions for the detailed analytical relationships between the variogram derivative and variance based theories see razavi and gupta 2016a the sensitivity indices in vars are computed by integrating the directional variograms resulting in a comprehensive set of indices for global sensitivity called integrated variograms across a range of scales or ivars the ivars50 index also referred to as the total variogram effect that integrates the variogram across the full range of perturbation scales is the most comprehensive variogram based index for global sensitivity a specific implementation of vars called star vars is incorporated within vars tool this implementation utilizes a form of star based sampling called star developed by razavi and gupta 2016b the vars tool function main vars m computes the ivars indices for global sensitivity along with the aforementioned derivative and variance based indices all together in a single run that utilizes the same sample points therefore this function is a multi method platform that provides the user with a sensitivity assessment that simultaneously includes the derivative variance and variogram based indices fig 3 shows an example input text file to this function where the user can provide the specifications of model factor ranges algorithm parameters etc the text file based interface with this function was designed for ease of use to be comprehensive vars tool also includes an implementation of the so called regional sensitivity analysis rsa approach that is essentially monte carlo filtering hamby 1994 spear et al 1994 the heuristic rsa approach is commonly used to partition the marginal distribution of sample points obtained for each factor into two or more distributions based on empirically selected threshold values for model response the idea is that if the factor does not have a significant impact on model response throughout the factor space the two distributions should be statistically indistinguishable the vars tool function main rsa m implements this approach and utilizes the smirnov test to quantify the extent to which the two distributions are different thereby providing indices for global sensitivity other classic model based gsa approaches such as factorial design correlation and regression based approaches are not included in vars tool because of their limited utility in the design and analysis of computer based simulation experiments these approaches are generally unsuitable for sensitivity analysis of complex dynamical systems models as they are based in the a priori assumption of a particular mathematical form typically linear or polynomial for the underlying model response surface in addition these methods are particularly prone to curse of dimensionality see discussion in razavi and gupta 2015 4 sensitivity analysis of dynamical systems models most approaches to global sensitivity analysis gsa do not adequately account for the dynamical nature of earth and environmental systems models the fact that such models in real world applications produce dynamical time evolving responses to dynamical time evolving perturbations inputs gupta and razavi 2018 highlighted this fact revisited the fundamental basis of gsa for dynamical systems models and developed a sensitivity analysis framework from first principles based on computation of a global sensitivity matrix gsm that quantifies the sensitivity information contained in trajectories of partial derivatives of the dynamical model responses with respect to controlling factors razavi and gupta in review extended and generalized this approach to accommodate a any gsa philosophy including derivative variance and variogram based approaches and b any model response of any type including time series of model state or output variables and their transformations this approach is referred to as generalized global sensitivity matrix ggsm vars tool includes an efficient implementation of the ggsm approach coupled with star vars that enables a multi method gsa of dynamical systems models that accounts for the temporal dynamics of such models while enabling efficient comparison of the results provided by philosophically different approaches e g derivative based versus variance based this implementation generates the following types of indices of global sensitivity a time varying sensitivity indices these indices are in the form of time series that reveal the time dependent sensitivities of model responses over the course of a simulation time period to its controlling factors b time aggregate sensitivity indices these indices are in the form of summary statistics that aggregate and summarize the dynamical sensitivity information of model responses over one or multiple time periods of interest the time varying indices enable the user to better understand the model system behavior over time in response to system forcings boundary conditions this capability facilitates i diagnostic testing and detection of potential defects in different parts of a model thereby helping to improve model realism and ii attribution of variability and therefore uncertainty in the model responses to different factors model parameters forcings boundary conditions etc thereby helping to pinpoint the dominant controls of predictive uncertainty at different points in time when summary information regarding the time varying sensitivities is required the time aggregate indices become more useful a second defining feature of the ggsm approach is that it does not require that observed data on system responses be available and so can be used to assess the internal functioning of a model and the controls exerted by different factors of any of its components including both states and fluxes this is important because 1 our interest when conducting a gsa is not limited to response variables for which observations are available and 2 the sensitivity assessments cannot be obscured or distorted by errors and uncertainties that unavoidably exist in observed response data gupta and razavi 2018 the vars tool function main vars m is enabled with the ggsm approach which can be activated by a switch on line 22 of vars inp txt fig 3 or within the source code upon activation vars tool can generate within a single run i e a single sample set time varying and time aggregate gsa indices based on the derivative based e g morris elementary effects variance based e g sobol total order effects and variogram based e g vars total variogram effects approaches this capability enables a user to explore compare and contrast the assessments provided by these three approaches to gsa 5 sampling strategies sampling strategies are necessary and fundamental components of any algorithm for sensitivity and uncertainty analysis of computer simulation models vars tool includes a variety of sampling strategies as listed in table 1 the derivative variance and variogram based gsa algorithms outlined above all follow a two level sampling procedure in the factor space in the first level they can employ any of the general purpose randomized sampling strategies listed in table 1 to select a set of n base points randomly distributed uniformly throughout the factor space in the second level these points are used as starting points for the algorithm specific structured sampling strategies used to acquire the remaining sample points required for calculating the algorithm specific sensitivity indices vars tool includes both single stage and progressive also called sequential or multi stage strategies for random sampling because single stage sampling strategies such as latin hypercube sampling lhs mckay et al 1979 and symmetric latin hypercube sampling slhs ye et al 2000 generate an entire sample consisting of a pre specified number of points all at once such strategies are effective only if the user is fairly confident about the proper sample size before beginning the analysis as any subsequent enlargement of the sample size will generally fail to preserve the distributional properties of interest in contrast progressive sampling strategies enable sequential or multi stage generation of the sample points examples include the traditional halton and sobol sequences also called low discrepancy sequences and the novel method of progressive latin hypercube sampling plhs recently developed by sheikholeslami and razavi 2017 note that vars tool provides the first available software implementation of plhs plhs sequentially generates sample points while progressively preserving important distributional properties of interest latin hypercube properties space filling etc as the sample size grows fig 4 illustrates how plhs successively generates a series of smaller sub samples slices such that 1 the first slice is latin hypercube 2 the progressive union of slices remains latin hypercube and achieves maximum stratification in any one dimensional projection and as such 3 the entire sample is latin hypercube plhs has been shown to be a superior strategy that scales effectively with the size and dimensionality of the problem under investigation sheikholeslami and razavi 2017 once the set consisting of n base points is generated randomly by any of the sampling strategies mentioned above it is used by the structured sampling strategies as starting points for the generation of the remaining sample points in the derivative based approach campolongo et al 2007 morris 1991 each of the n base points serves as the starting point for a chain of points constructed by changing one factor at a time by a given step size resulting in a total number of n d 1 sample points where d is the number of factors in the variance based approach saltelli et al 2008 structured sampling is conducted via matrix manipulation of the base sample matrix consisting of the n base points resulting in a total number of n d 2 sample points in the variogram based approach and particularly in the star vars implementation the n base points are used as star centers for star sampling razavi and gupta 2016b in this approach equally spaced points are sampled δh apart around each star center along every dimension of the factor space fig 5 resulting in a total number of n d 1 δh 1 1 sample points the distance δh referred to as the resolution of sampling is specified by the user a δh of 0 1 of the factor range is recommended while smaller values can be selected to yield more accurate results 6 dealing with high dimensional problems factor grouping the challenge of conducting a gsa monitoring its convergence and robustness and interpreting the associated results can easily become non trivial when the problem has more than 20 30 factors therefore vars tool includes an innovative factor grouping strategy that employs a clustering mechanism developed by sheikholeslami et al 2018 to handle high dimensional problems involving tens to hundreds of factors this strategy integrated together with bootstrapping see section 7 1 monitors gsa performance and clusters factors into groups of similar properties based on their sensitivity and function the resulting groups can be of any size depending on the problem at hand for example in a problem with 60 factors vars tool can be used to cluster the factors into 5 groups of factors termed strongly influential influential moderately influential weakly influential and non influential the number of groups of interest can be directly specified by the user in vars inp txt as a convenient feature however if the user does not specify a preferred number of groups vars tool will suggest an optimal number of groups based on maximizing the distinctions between the groups through the so called elbow method sheikholeslami et al 2018 fig 6 shows an example dendrogram generated by vars tool and the associated optimal grouping this grouping capability is particularly beneficial when dealing with high dimensional problems where the user typically does not need to obtain an exact ranking for the many factors for example in a problem with 100 factors it may not matter whether a factor is the 30th or the 31st in terms of importance and obtaining a robust answer to this question may require running the model an excessively large number of times instead by assigning factors having similar importance into groups vars tool provides a means for making the problem more tractable within a limited number of model runs this capability scales the computational demand of a gsa algorithm with the users need in terms of precision in factor rankings and as such results in significant improvements in stability and rate of convergence 7 other important features of vars tool 7 1 bootstrapping characterizing confidence and robustness in any analysis based on statistical sampling it is essential to be able to assess the degree of confidence one can place in the results so as to get a sense of reliability robustness and sufficiency of the analysis in vars tool the method of statistical bootstrapping is used to 1 infer confidence intervals around the estimates of sensitivity indices at any user specified confidence level and 2 provide an assessment of the robustness that can be associated with the factor rankings while the use of bootstrapping for the former is common in gsa literature in the latter the definition and implementation of robustness sometimes termed reliability as well of a factor ranking developed by razavi and gupta 2016b and further extended by sheikholeslami et al 2018 for groups of factors is unique to vars tool consider for example a problem with five factors after 200 model runs sample size 200 one may find that the sensitivity index associated with factor 3 has been estimated to have a value of 0 4 but with a confidence interval of 0 3 0 45 at the 90 confidence level meanwhile its factor ranking may be estimated as 2 meaning that it is the second most influential of the 5 factors with an assessed robustness of 70 meaning that factor 3 is exactly rank 2 with 70 probability with larger sample sizes we can expect that confidence intervals will become progressively narrower and the robustness estimates progressively improve thereby increasing our confidence in the results of the statistical analysis note that the reason that we will typically not have 100 confidence in the results is that the analysis can be affected by sampling variability in the generation of the sample points used and so can provide somewhat different results if the experiment is re run with a different but equally representative set of samples however increasing the sample size can help to improve the reliability and robustness of the analysis so for example increasing the sample size to 500 in the example above could alter the sensitivity index of factor 3 to have a value of 0 38 with a 90 confidence interval of 0 35 0 41 and a factor ranking of 2 but with an increased robustness of 90 in this way statistical bootstrapping provides a way of assessing the robustness stability and convergence of a gsa experiment with narrower 90 or 95 etc confidence intervals and higher factor ranking robustness indicating greater reliability of the results as a caveat note that statistical bootstrapping efron 1992 is a procedure based in sampling with replacement from the existing set of sample points to obtain estimates of the underlying statistical properties of a sample and so its validity depends on the assumption that the existing sample is a sufficiently representative sample of the underlying population beran 1997 davison et al 2003 in practice this means that the sample size used for the analysis must be large enough to adequately span the factor space of interest one way to assess this is to repeat the analysis with progressively larger sample sizes which is easily facilitated by the plhs method of progressive sampling discussed in section 5 7 2 reporting and visualization monitoring stability and convergence vars tool provides an on line visualization and reporting capability that enables monitoring the execution of gsa experiments and evaluation of their intermediate progressive performance fig 7 shows example plots generated using vars tool for the razavi gupta s 6d multi scale wavy function developed in razavi and gupta 2016a in on line mode these plots enable the user to monitor how the estimates of factor sensitivities and rankings may be changing with increasing sample size as more model evaluations become available along with comprehensive frequently generated report files provided in text format in the course of a gsa experiment these plots provide the capability to visually monitor robustness stability and convergence and terminate an analysis when required furthermore this capability in conjunction with a sequential sampling strategy e g plhs can resolve the problem that a user will typically not know a priori what a suitable sample size may be for a gsa experiment as a result the gsa can run only for an optimal number of model runs thereby maximizing computational efficiency 7 3 model emulation handling model crashes an issue that is known to arise when running some complex earth and environmental systems models is that they may fail while running under particular factor configurations and values such failures often called crashes can for example be caused by specifying unrealistic combinations for the parameter values that violate model assumptions or by numerical instabilities that arise under some combinations of parameters and forcings in practice if the occurrence of such model failures is not properly dealt with the entire gsa experiment can fail most implementations of gsa algorithms require that meaningful model responses be provided for every sample point generated however execution of the model code may fail to return a response if it crashes when this happens modellers often end up re doing the entire analysis with more conservative tighter factor ranges resulting in extra computational cost model runs and also changing the definition of the original problem to address this issue effectively and efficiently vars tool is enabled with a crash handling module that is designed to complete the gsa while minimizing the impact of model crashes on the analysis for this purpose vars tool employs model emulation techniques razavi et al 2012 to generate surrogate values of model outputs when a model fails theoretical details of this module are available in sheikholeslami et al in prep 7 4 running with models implemented in any programming language and operating system vars tool programs both the matlab version and c version implemented within ostrich have the capability to work with any model implemented in any programming language and running on any operating system vars tool has two modes for execution on line internal this mode can be used when the computer simulation model is set up to be called and run through the matlab or ostrich environments off line external this mode is to be used when the computer simulation model needs to be run externally outside the matlab or ostrich environments in the off line mode the model simulations can be performed independently of the vars tool software programs in this mode 1 vars tool first generates the locations for all the sample points required for the analysis and stores them in a text file 2 then the user should conduct model simulations externally for all of the sampled points and store the respective model response values e g in a text file these runs can be parallelized if multiple processors are available 3 vars tool reads in the model runs results conducts the analysis and generates sensitivity indices importantly the vars tool version within ostrich supports message passing interface mpi based parallel processing on both windows and linux systems which can significantly increase computational efficiency of a gsa experiment by parallelizing model runs 8 test functions test functions are useful for learning understanding testing and benchmarking the performance of different gsa algorithms they have the advantage that 1 they are computationally inexpensive and fast to run and 2 their characteristics are known a priori and in most cases controllable by the user vars tool embeds a diverse set of test functions demonstrating a wide range of possible characteristics such as nonlinearity unimodality and multi modality irregularity roughness and smoothness interaction and high dimensionality these test functions provide artificial response surfaces also called landscapes that may share some characteristics with the response surfaces of real world modelling problems for a discussion of the latter see duan et al 1992 in addition to including test functions designed originally for research on sensitivity and uncertainty analysis vars tool includes functions popular in the field of optimization research the test functions are listed in table 1 and the response surfaces for two of them g function and 6d multi scale wavy function are shown in fig 8 when choosing a test function for a particular analysis users might first consider which test function presents similar structure and characteristics to those of the underlying response surface of the problem at hand this question is non trivial but razavi and gupta 2015 provide some insight 9 hydrologic case studies vars tool includes software of a hydrologic model called hbv sask which is an interpretation of hydrologiska byråns vattenbalansavdelning model lindström et al 1997 developed by the first author for educational and research purposes fig 9 shows the architecture of hbv sask and its inputs process parameters and equations in addition to those process parametrizations daily potential evapotranspiration pet is computed using equation pet 1 etf t tmth petmth where tmth and petmth are long term average monthly temperature and potential evapotranspiration respectively supplied as input data to the model along with daily precipitation p and temperature t table 2 lists the model parameters including etf in the equation above their description and feasible ranges the implementation is modular with each module simulating a different hydrologic process thereby allowing the user to easily investigate the different model flux and state variables see fig 10 hbv sask in vars tool comes with ready to run case studies for two watersheds bow river at banff with area of 2178 53 km2 and oldman river at waldron s corner with an area of 1434 73 km2 these rivers are located in the rocky mountains in alberta canada and flow into the saskatchewan river basin fig 11 historical data is available for the periods 1950 2011 and 1979 2008 respectively from which we estimate average annual precipitation rainfall snowfall to be 795 mm bow and 611 mm oldman and average annual streamflow to be 38 6 m3 s at gauge 05bb001 on the bow river and 11 7 m3 s at gauge 05aa023 on the oldman river the bow and oldman basins differ in their hydrological properties and have runoff ratios of approximately 0 7 and 0 42 respectively vars tool also comes with a ready to use set up of the mesh modélisation environmentale communautaire surface hydrology land surface hydrology model pietroniro et al 2006 mesh couples the canadian land surface scheme class verseghy et al 1993 verseghy 1991 with the hydrologic routing schemes of watflood kouwen et al 1993 this model set up is interfaced with vars tool via the ostrich toolkit matott 2017 the case studies with mesh are adopted from haghnegahdar et al 2017 details on how to run this case study are available in the vars tool manual 10 concluding remarks significant theoretical and practical advances have been made in recent years in the field of sensitivity and uncertainty analyses these analyses need to become an integral part of any model development prediction and decision making process providing insight into various issues such as uncertainty apportionment diagnostic testing planning and management and policy prioritization best practices are however often hampered by computational burden and lack of transparency and interpretability vars tool is designed as a computationally efficient and transparent platform to reconcile the state of the art theories with real world applications that involve complex high dimensional and computationally expensive models of dynamical systems in summary vars tool aims to promote best practices in sensitivity analysis that should be based on a multi method approach that brings together the different theories and aspects of sensitivity analysis thereby providing a more comprehensive assessment of the problem at hand using for example the variogram analysis of response surfaces vars approach see section 3 proper accounting for the dynamical properties of earth and environmental systems models using for example the generalized global sensitivity matrix ggsm approach see section 4 proper sampling of the problem space that effectively scales with the size of the problem at hand and available computational budget using for example progressive latin hypercube sampling plhs see section 5 methods that are capable of handling high dimensional problems such as advanced earth and environmental systems models that tend to involve hundreds of factors whose influence on model response needs to be characterized using for example a factor grouping strategy see section 6 proper characterization of robustness and stability of the algorithms to sampling variability and of the degree of confidence and reliability that users can have in sensitivity results using for example a bootstrap technique see section 7 vars tool is under continuous development to include new theoretical advances in the field of sensitivity and uncertainty analysis forthcoming additions will include but are not limited to a the ability to carry out sensitivity analysis on any sample data of a process however collected see e g borgonovo et al 2016 we believe this ability is needed for two reasons i the utility of sensitivity analysis may not be limited to models and their artifacts and one may directly apply sensitivity analysis to data to characterize the behavior of the underlying system processes ii most sensitivity analysis algorithms require samples taken in a specific manner which limits their utility when a sample set already exists b the ability to handle problems with non uniform and or correlated factors see e g kucherenko et al 2012 most algorithms for sensitivity analysis assume uniform a priori distributions for the factors this assumption limits the applicability and validity of results if the true distribution of factors is non uniform and or correlated c the ability to provide a more informative characterization of interaction effects see e g razavi and gupta 2015 very few existing approaches to sensitivity analysis can provide a meaningful and or easily interpretable assessment of interaction effects there is a need to rethink the fundamentals of interaction effects in relationship with the correlation effects point b above and to develop more informative tools for their characterization in support of model development and prediction the field of sensitivity analysis is still young and evolving it has the potential to provide a solid foundation based on which different sources of uncertainty which may by essence be irreconcilable can be compared and assessed further it has the potential to help us isolate assess and deal with the deep uncertainties in our future earth and environmental systems that occur in the context of climatic environmental and social change as always we invite discussion and collaboration on any of these and other issues related to diagnostic evaluation and improvement of dynamical systems models especially with regard to high dimensional representations of complex systems acknowledgements the first second and fourth authors were supported in part by the first author s nserc natural sciences and engineering research council of canada discovery grant the third author received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 
26271,a freely accessible model aided satellite altimeter based daily water level forecasting system using simple regression analysis is proposed for the mekong river with feasibility study being performed in the mekong delta md where regional flood management and mitigation center rfmmc does not provide forecasting ocean tides strongly impact river levels and were specifically addressed by the sum of 5 term sinusoidal function forecasting skills of our system are quite promising in the md although rfmmc s forecasting system has better skills than ours in the mekong mainstem upstream of md in contrast to current operational system our system circumvents the need of frequent altimeter samplings in the upstream by using the variable infiltration capacity vic macroscale hydrologic model with 0 1 resolution the proposed forecasting system is computationally efficient without the need of complex hydrodynamic modeling making such approach globally applicable for river basins and deltas with comparable forecasting skill and minimal computational cost keywords the mekong river satellite altimetry hydrologic model water level forecasting 1 introduction the mekong river mr is the twelfth longest river in the world and the largest river in southeast asia flowing through six countries china lao pdr myanmar thailand cambodia and vietnam the importance of the mr on a regional and global scale is more than apparent from multiple aspects inhabitants in the mr basin mrb have been extensively relying on resources from the river as their major food supply and source of income by developing floodplain agriculture and fresh water fishery mekong river commission mrc 2011 the mekong delta md located at the mouth of the mr is one of the most populous regions in the mrb there are about 16 million people who live in the md which is about 23 of the population of the mrb food and agriculture organization of the united nations fao 2012 the md together with central highland has a population density of 260 persons km2 which is the highest in the mrb fao 2012 it is also the most important rice production region not only in vietnam but also in the world with about 20 of shares in the global rice export quantity in 2012 fao 2017 in recent years several studies have indicated increases in annual and seasonal river discharges in the mr under future climate change which results in increased flood risks during the wet season in the cambodian and vietnamese floodplain hoang et al 2016 västilä et al 2010 mrc 2010 hoanh et al 2010 developed a climate change analysis framework using the decision support framework models of the mrc to analyze the impacts of future climate change and planned water resources management they concluded an increase of river flow of 2 11 and 18 30 under projected climate change from 2010 to 2050 in high flow and low flow season respectively data from the tide gauges along vietnamese coasts also show that sea level rose at a rate of about 3 mm year during 1993 2008 ministry of natural resources and environment monre 2010 this rising rate is about the same as the global average of about 3 mm year determined from topex poseidon t p jason 1 and jason 2 satellite altimetry observations from 1993 to 2014 ablain et al 2017 wassmann et al 2004 assessed the future flood regime in the md under the scenarios of 20 cm and 45 cm of sea level rises and derived an average water level rises of 11 9 cm and 27 4 cm at the peak of the flood season october respectively as the sea level at the coast of vietnam is projected to continuously rise under the representative concentration pathways rcps adopted by ipcc fifth assessment report ar5 commonwealth scientific and industrial research organization csiro 2014 the md may become more and more vulnerable to flood events in the future threatening the livelihood and food security of millions of people major economic social and environmental changes are also expected in the next 10 years due to urbanization industrialization and sanitation issues boosted by recent and rapid increase in population johnston and kummu 2012 as mason d croz et al 2016 pointed out the importance of tools to explore the potential effects of climatic socioeconomic and environmental changes robust and sustainable flood forecasting can be a great aid in decision making for flood risk reduction and response making the mrb more resilient when facing an uncertain future the regional flood management and mitigation center rfmmc affiliated with the mrc was founded in 2005 after a series of severe floods from 2000 to 2001 it currently issues an up to 5 day lead time of daily water level forecasting at each of 22 locations from chiang saen to tan chau chau doc during the wet season which is from june to october pagano 2014 the flood forecasting system adopted by the rfmmc uses the unified river basin simulator urbs rainfall runoff runoff routing model and isis hydrodynamic model pagano 2014 tospornsampan et al 2009 the operation of the rfmmc s forecasting system is based on the deltares flood early warning system delft fews platform which links data and models in real time to perform daily forecasting werner et al 2013 at the locations of each in situ water level gauge rainfall forecast is first provided by the fifth generation mesoscale model mm5 cox et al 1998 and then used as input of urbs model for discharge estimation which is then converted to forecasted water level via a rating curve the rfmmc also uses the isis hydrodynamic model for water level forecast downstream from stung treng to the ocean pagano 2014 tospornsampan et al 2009 however the isis hydrodynamic model is computationally intensive and therefore is run only for retrospective analyses and when demand arises pagano 2014 the rfmmc also does not issue any forecasting in the md downstream from tan chau chau doc considering the increasing vulnerability of the md under the future climate change a sustainable flood forecasting system which can be operated on routine basis with low computational cost is necessary however complex hydraulic conditions in the md hinder such a flood forecasting system the tonle sap lake tsl has an effect on the md and needs to be considered in addition to the flow from the mr fig 1 in order to perform flood forecasting the tsl is the largest freshwater lake in southeast asia and is called the heart of the lower mekong campbell et al 2009 since most of the population of cambodia relies on it the existence of the tsl leads to a unique flow reversal the water flows from the lake to the cambodian floodplain the bassac river and the md through the tonle sap river in september october and flows back to the lake in may june thus the tsl plays an important role in flood peak attenuation and flow control to the md pagano 2014 heng 2009 the unique flow reversal makes the tsl function as a large natural reservoir which significantly benefits people living in the downstream because it reduces the severity of flooding by moderating peak flow rates during the wet season it also provides additional water resources for irrigational use and eases the extent of salt water intrusion into the md during the dry season mrc 2009 however this flow reversal also poses a challenge from modeling perspective which necessitates the use of a high resolution hydrodynamic model to physically mimic the flow reversal due to the hydrologic drivers running a high resolution hydrodynamic model is often computationally expensive which is also the reason why the rfmmc does not routinely operate it in the md moreover the ocean tide intrusion can reach up to phnom penh in the dry season pagano 2014 the mrc 2005 also pointed out that ocean tide can propagate up to the towns close to the cambodia vietnam border causing about 1 m of tidal variation in river level takagi et al 2013 studied the tidal influence on water levels in can tho and suggested that the interaction between tide and river flow need to be considered together when studying floods in the md recently satellite radar altimetry has been successfully used to observe and forecast river level changes thanks to the advance of waveform retracking techniques satellite radar altimetry has been used to collect water level changes at the satellite ground track and inland water body cross sections the so called virtual stations vss the feasibility of using satellite altimetry derived water level changes in flood forecasting has already been proven biancamaria et al 2011 demonstrated that t p altimetry derived water level observations at upstream vss over the ganges and brahmaputra rivers in india can be used to forecast downstream water levels in bangladesh through regression analysis about every 10 days due to t p s repeat cycle the feasibility test showed that forecasted water levels have a root mean square error rmse of 0 40 m and 0 60 0 80 m at 5 days and 10 days of lead time respectively hossain et al 2014a used jason 2 altimetry derived water levels obtained approximately every 10 days at upstream vss in india to obtain daily 5 day forecasting of downstream water discharges at the upstream most boundary points of the hydrologic engineering center river analysis software hec ras hydrodynamic model setup in bangladesh the forecasted discharges were then used as a boundary condition for the hec ras model to simulate daily 5 day forecasted river levels at 17 locations inside bangladesh the system yielded an rmse ranging from 0 5 to 1 5 m with a bias of 0 25 1 25 m compared with 5 day later nowcast model simulated water levels in bangladesh using in situ water levels at the upstream boundary points as the boundary condition when compared with in situ water levels at 3 locations for up to 5 day lead times it reaches mean errors ranging 0 4 0 4 m with rmses of 0 2 0 7 m however this daily forecasting system relies on a number of jason 2 vss to ensure frequent sampling and data availability hossain et al 2014b since the mr is a river flowing from north to south with only a few number of skillful vss direct implementation of this system becomes challenging to address this issue we propose a model aided altimeter based river level forecasting system called the mekong river flood forecasting tool mr fft it integrates jason 2 altimetry derived water levels and discharge estimations from the variable infiltration capacity hydrologic vic model liang et al 1994 hossain et al 2017 here we present a feasibility study to demonstrate its forecasting skill in the mr especially in the md a multivariate regression model and sum of sinusoidal functions were employed to address the downstream river level changes due to different causes considering multiple variables for a better representation maçaira et al 2018 of water levels in the md the unique features of our flood forecasting system are 1 an easy to set up hydrologic model is implemented which circumvents the need to have frequent altimeter samplings in the upstream 2 the forecasting approach in the delta is based on computationally efficient regression analysis instead of complex hydrodynamic modeling 3 it is freely accessible for stakeholders and users with tolerable computational cost 4 it can be easily extended for other deltas and transboundary rivers in developing countries such as the vietnamese red river ganges brahmaputra meghna river and niger river with skillful upstream altimetry vss a hydrologic model such as the vic model and historic in situ water levels no updated in situ water levels are needed in the downstream our system can be easily implemented as long as no major dams exist between the upstream vss and downstream in situ stations this paper is structured as follows section 2 describes the data we used including altimetry derived water level data and discharges estimations from the vic model section 3 explains the model aided altimeter based water level forecasting system in detail section 4 presents results from the feasibility study section 5 concludes this paper with suggestions for operational flood forecasting section 6 is the software description 2 data 2 1 jason 2 altimetry derived river levels jason 2 launched on june 20th 2008 is the follow up mission of jason 1 operating under the cooperation of the national aeronautics and space administration nasa center national d etudes spatiales cnes national oceanic and atmospheric administration noaa and european organization for the exploitation of meteorological satellites eumetsat it succeeds its predecessors including t p and jason 1 missions to continuously provide highly accurate altimetry data with a 10 day repeat cycle in this feasibility study we used 20 hz ice retracked ranges from the jason 2 geophysical data record gdr to extract river levels at vss located in the mr mainstem and tsl the locations of vss are shown in fig 1 the precisions of altimetry derived water levels over the mr mainstem and tsl were about 0 4 0 5 m and 0 26 m respectively when compared with the nearest in situ gauge data see fig 2 for the jvs p001 02 over the tsl the altimetry water levels have a better rmse since the tsl is a large inland water body where more robust radar altimetry measurements can be collected 2 2 modeled discharges at virtual stations the river discharges at the locations of vss are obtained by the vic hydrological model liang et al 1994 and a streamflow routing model lohmann et al 1996 the vic model is comprised of three layer soil column structure and considers land cover soil type and meteorological forcing data such as precipitation temperature and wind speed to characterize the hydrological mechanism in the soil column of given ground cells to estimate runoffs and baseflows the streamflow routing model lohmann et al 1996 was then used to route the surface runoffs and baseflows estimated by the vic model to the river channels based on the given flow direction map to simulate discharges in our study a 0 1 degree resolution vic model was set up in the mrb hossain et al 2017 using the global land cover characterization glcc land cover dataset and the harmonized world soil database hwsd soil data siddique e akbor et al 2014 the monthly leaf area index and albedo were obtained by the moderate resolution imaging spectro radiometer mission modis data the flow direction map was derived from the shuttle radar topography mission srtm digital elevation model dem over the mrb six sub basin segments were selected for model calibration including chiang sean luang prabang vientiane nakhon phanom pakse and kampong cham the vic model was calibrated using soil parameters including the variable infiltration curve parameter b inf maximum velocity of baseflow ds max fraction of ds max ds and soil moisture ws where non linear baseflow occurs on the other hand the cell impulse response function of the routing model was also calibrated hossain et al 2017 the current vic model output downstream of kampong cham where bidirectional flow due to the tsl and complex river network exist turns out to be less reliable however the vic model is only used to estimate water discharges at vss in the upstream of mekong in this study the meteorological forcings such as maximum and minimum temperatures wind speed and precipitation were obtained from 237 weather station records archived as the global summary of the day gsod by the national climatic data center ncdc the model was run at a daily time step to simulate the discharges from 2002 to 2015 at the locations of jason 2 vss the output of first year 2002 was excluded to avoid spin up error the calibration period was set to be from 2003 to 2008 the validation period was from 2009 to 2013 the simulated water discharges at nakhon phanom and khong chiam which are the nearest stations to jvs p179 01 and jvs p001 01 have nash sutcliffe efficiency nse of 0 82 and 0 86 respectively for more details about the vic model set up in the mrb readers are referred to hossain et al 2017 currently this model runs operationally for nowcast forced with satellite precipitation at several locations along the mr see http depts washington edu saswe 2 3 in situ water levels the in situ water levels in the mr were provided by the asian disaster preparedness center adpc and were used to build and test the feasibility of our model aided altimetry based forecasting system in our study the in situ water level data collected at 6 locations in the middle reach of the mr 3 locations in the cambodian floodplain and 13 locations in the md were used see table 1 fig 1 shows the locations of all of the in situ gauges used a in the middle reach of the mr and cambodian floodplain and b in the md toward the river mouth 3 methods to obtain a daily water level forecasting hossain et al 2014a used jason 2 altimetry derived water levels with 10 day repeat cycle at upstream vss to obtain daily forecasted downstream water discharges at downstream in situ gauges as mentioned above it requires a sufficient number of vss to ensure that at least one of the vss gets sampled in the upstream neighborhood for deriving expected water levels in the downstream hossain et al 2014b however the mr is a north to south flowing river where only a few number of vss can be found hindering the direct implementation of this system hence we attempt to use the vic hydrologic model which can provide daily river discharges at vss to fill the temporal gap of altimeter data at model building step data from october 2008 to december 2010 were used hereafter called as historical at each of vss in the upstream of the mr mainstem a discharge to level rating curve was built using historical vic derived discharges and jason 2 altimetry derived water levels the rating curve was then used to reconstruct historical daily water levels see section 3 1 a relationship between upstream water levels at each of the vss and downstream water levels at each of the in situ gauges was then built through a simple or multivariate linear regression taking the tsl water levels into account depending on the locations see section 3 2 the tidal influence models were also built where necessary see section 3 2 at pseudo forecasting step data from 2011 were used by using the rating curves and models from the model building step the pseudo forecasting was performed and validated to examine the skill of the system the indices for the skill evaluation are described in section 3 3 3 1 daily water level reconstruction at virtual stations the conventional water level to discharge rating curve follows a power law curve in the form of 1 q v i c c h a l t a b where h a l t is jason 2 altimetry derived water level q v i c is discharge simulated from the vic and streamflow routing model and c and b are coefficients to be estimated equation 1 can be transformed into a linear form by taking the logarithm on both sides and then re arranged as 2 l o g h a l t a 1 b l o g q v i c 1 b l o g c a l o g q v i c b which is a linear function where coefficients a and b can be estimated by least squares method while a can be directly solved by johnson s method rantz 1982 world meteorological organization wmo 1980 the logarithm value of daily q v i c can then be used to reconstruct daily water levels h r e c 3 h r e c e a l o g q v i c b a in the tsl where water levels vary smoothly see fig 2 c the daily water levels were reconstructed by linearly interpolating 10 day repeat jason 2 altimetry derived water levels the rating curves and reconstructed water level time series are shown in fig 3 and fig 4 respectively in this study data from october 2008 to december 2010 were used to build the rating curves for historical daily water level reconstruction at the jason 2 vss fig 3 also see fig 5 a for flow chart and data from year 2011 were used for validation with in situ gauge data fig 4 also see fig 6 for flow chart as the bottom panel of fig 3 shows the reconstructed water levels in the middle reach of the mr have rmses of about 1 m with temporal correlation coefficients of 0 95 the rmses of reconstructed water levels at the vss in the mr may depend on the accuracies of altimetry derived water level itself and the vic derived discharge furthermore some spikes in the reconstructed water levels during dry season could result from the errors of in situ precipitation records that are the input data of the vic model for the vs in the tsl the reconstructed water levels show a smaller rmse of about 0 3 m with a temporal correlation of about 1 due to the fact that the tsl is a large water body where a larger number of 20 hz jason 2 measurements are available 3 2 forecasting models considering that the hydraulic conditions of the mr are complex due to its large geographic coverage and various geographic features we built two different forecasting models for 1 the middle reach of the mr and 2 the cambodian floodplain and the md we used the reconstructed water levels at upstream vss and downstream in situ water levels from october 2008 to december 2010 to build the forecasting models and tidal influence models see fig 5 for flow charts data from 2011 were used to perform pseudo forecasting see fig 6 for flow chart the forecasted water levels were then validated with in situ observations for each of the locations from nakhon phanom to kratie see fig 1 a in the middle reach of the mainstem mr the hydraulic condition is relatively simple pagano 2014 thus we performed a simple linear regression analysis using the reconstructed daily water levels at upstream vs and observed historical daily water levels at downstream in situ gauge considering different lead times to build the forecasting model as 4 h i n s i t u t k e h r e c t f where h i n s i t u t is historic water levels at downstream in situ gauge at time t h r e c t is the reconstructed water levels at upstream vs at time t which were derived by equation 3 k is the forecasting lead time day and e and f are coefficients to be estimated for this linear forecasting model for each of the locations from phnom penh to the md see fig 1 b the river flows are dominated by the water exchange between river channel and floodplain and the flow reversal due to the tsl furthermore ocean tides can also intrude up to phnom penh in the dry season pagano 2014 considering these multiple sources of water level changes we propose a two step method to build the forecasting model for the lower reach of the mr the first step is to use the reconstructed water levels at vss in the upstream mr mainstem and the tsl to perform multivariate linear regression analysis with downstream in situ water levels 5 h i n s i t u t k g h r e c m k t h h r e c t s l t i v t k where h r e c m k t is the reconstructed daily water levels in the mr mainstem at time t derived by equation 3 h r e c t s l t is the reconstructed daily water levels in the tsl which were obtained by linearly interpolating jason 2 altimetry water levels as section 3 1 described g h and i are coefficients to be estimated for this multivariate linear regression model k is the forecasting lead time day and v t k is the residual between modeled results and h i n s i t u t k since the model only considers the influence of the mr mainstem and tsl we assume that v here is dominated by the ocean tide influence and hereafter rephrased as h t i d e t for the case of k 0 next we adopted a model of tidal influence as the sum of 5 term sinusoidal functions while considering dominant tidal frequencies including annual semi annual monthly fortnightly and synodic fortnightly tides see table 2 egbert and ray 2003 zheng and zhao 1984 6 h t i d e t i 1 5 m i s i n 2 π ω i t n i where ω is the dominant tidal frequency number of cycles per year while m and n are the magnitude and phase shift to be estimated respectively once m and n are fitted t can be updated as decimal year at the time we want to perform forecasting subsequently the summation of modeled results given by equation 5 and equation 6 provides us with forecasted water levels which now consider influences from the mr mainstem tsl and the ocean tide intrusion 3 3 model evaluation statistics to evaluate model capacity several statistical indices including the mean absolute error mae error of standard deviation estd correlation coefficient cor as well as nse and achievement rate ar were adopted the mae is a measure of error which can be calculated by 7 m a e i 1 n y i o b s y i f c t n where n is the total number of data with i 1 n y i o b s is the in situ observations and y i f c t is the forecasting results a mae value of 0 indicates a perfect forecasting the estd is the standard deviation of difference between forecasting results and observations 8 e s t d i 1 n e i e 2 n where e i y i o b s y i f c t e is the mean of e i this value only reflects how dense the error distributes but does not reflect the average error the cor is a measure of temporal agreement between forecasting and observations 9 c o r i 1 n y i f c t y f c t y i o b s y o b s i 1 n y i f c t y f c t 2 i 1 n y i o b s y o b s 2 where y f c t is the mean of forecasting and y o b s is the mean of the observations the nse is a normalized statistic which reflects the relative magnitude of the residual variance noise compared to the measured data variance information nash and sutcliffe 1970 it indicates how well the plot of observations versus forecasting results fits the 1 1 line and is commonly used to evaluate the performance of the forecasting model 10 n s e 1 i 1 n y i o b s y i f c t 2 i 1 n y i o b s y o b s 2 nse ranges from to 1 0 an nse value of 1 indicates perfect forecasting skill 0 means the forecasting skill is no better than adopting the average of observations as forecasting results and a negative nse means unacceptable skill moriasi et al 2007 the ar is the rate of days in the wet season that meets the satisfactory benchmark in which we used absolute error ae 11 a r s n where s is the number of days in the wet season when the difference between forecasted water levels and in situ observations is smaller than or equal to the benchmark 4 results 4 1 cross comparison with current system for cross comparison of our system with the system adopted by the rfmmc we compared the results of the 5 day forecasting which is the longest forecasting lead time that the rfmmc performs during the wet season june to october the cross comparison was conducted at 6 locations in the middle reach of the mr nakhon phanom mukdahan khong chiam pakse stung treng and kratie 3 locations in the cambodian floodplain phnom penh koh khel and neak luong and 2 locations in the upstream of the md tan chau and chau doc the mae based annual accuracy assessment in the rfmmc s seasonal flood situation report available on http ffw mrcmekong org report seasonal php is used for comparison from 2011 to 2013 year by year rfmmc 2011 2012 2013 on the other hand the estds of our forecasting results during the wet season from 2011 to 2015 were also compared with pagano 2014 s evaluation of performance of the rfmmc s forecasting system using historical data from 2002 to 2012 for a long term evaluation although the time spans of the long term estds are different pagano 2014 s estimation is the only long term accuracy assessment that we can find forecasting at the locations down to khong chiam was performed using jvs p179 01 while jvs p001 01 was used for forecasting at locations from pakse for forecasting at the locations downstream of the tsl both jvs p001 01 and jvs p001 02 were used to represent different water sources from the mr mainstem and the tsl respectively as table 3 shows in the middle reach of the mr our forecasted river levels were in general less accurate than the rfmmc s forecasts we obtained yearly maes of about 1 2 2 m and long term estds of about 0 8 1 5 m during the wet season while the rfmmc s yearly maes are about 0 4 0 7 m with long term estds of about 0 7 0 9 m for the locations in the cambodian floodplain the accuracy of our system improved yearly maes of our system are about 0 4 0 5 m and long term estds are about 0 5 m while the rfmmc s yearly maes are of about 0 2 0 25 m with long term estds of about 0 25 0 3 m overall in the middle reach of the mr and the cambodian floodplain the rfmmc s forecasting showed slightly better accuracy than ours since our system uses vic derived discharges to build the rating curve with altimetry derived water levels for reconstruction of daily water levels at vss both of their accuracies can be leading factors of our forecasting skill for example the rfmmc s forecasting system uses a calibrated set of 51 urbs models which cover over 2244 sub areas 740 000 km2 tospornsampan et al 2009 while the vic model we currently adopted was calibrated at only 6 sub basins in addition the accuracy of altimetry derived water levels can be up to about half of a meter as shown in fig 2 on the other hand currently our forecasting system only assumes that there is a linear relationship between upstream and downstream water levels but does not consider the impact of tributaries and direct rainfalls in between therefore it is expected that our forecasting skill can be further improved by fine tuning the vic model by performing more advanced altimetry waveform retracking and by considering those additional contributions to the downstream water levels however it needs to be emphasized that our forecasting system uses freely accessible and publicly available satellite data and hydrologic model and is able to perform forecasting without updated in situ data on the other hand our system is purely based on simple regression analysis instead of a computationally expensive weather forecast model and hydrodynamic model for example the rfmmc s system uses mm5 to forecast rainfalls as an input of the urbs models which have a computational cost and skill set requirement while our system does not features including strong affordability easy to set up and negligible computational cost make our system operationally sustainable and scalable to other places especially for developing countries it is also worthwhile to mention that in tan chau and chau doc which are close to the upstream boundary of the md in vietnam our system showed yearly maes of about 0 2 0 3 m and long term estds of about 0 3 m which are compatible with the rfmmc s results the smaller maes might be due to the lower amplitude of water levels in the md but it also reflects the fact that the sinusoidal model to represent the tidal influence was effective in the md moreover our system can provide forecasting in the further downstream of tan chau and chau doc inside the md where the rfmmc currently does not provide any forecasting after cross comparison with the rfmmc s report we attempted to perform a pseudo forecasting considering 20 different forecasting lead times from 1 day forecasting to 20 day forecasting in each region and then validated our forecasting results using in situ data in the following section the forecasting skill of our system will be discussed 4 2 flood forecasting in the middle reach of the mr and the cambodian floodplain in the middle reach of the mr and the cambodian floodplain in situ data are available until 2015 which were used to validate our forecasting results fig 7 shows the maes and nses of our system during the wet season from 2011 to 2015 up to 20 day lead time of forecasting was performed forecasting at nakhom phanom mukdahan and khong chiam was performed by using reconstructed water levels at jvs p179 01 we can see that the maes of 1 day forecasting at nakhom phanom and mukdahan are about 0 9 m then gradually increase with lead time while it starts from about 1 2 m at khong chiam the relatively higher maes at khong chiam might be due to its longer distance away from jvs p179 01 which is about 477 km compared to the 141 km and 248 km of nakhom phanom and mukdahan respectively since our forecasting is based on a linear relationship between upstream vs water levels and downstream in situ water levels hydrologic or hydraulic condition changes between upstream vss and downstream in situ stations due to geographic remoteness can influence the forecasting skill on the other hand khong chiam is close to the confluence of the mun river and mr which might be another reason why forecasting at khong chiam has higher maes compared to that at nakhom phanom and mukdahan the nses of forecasting at these locations start from about 0 8 at 1 day forecasting and gradually decrease with longer lead time but can still maintain at 0 7 up to about 5 day lead time forecasting starting from pakse was performed by using reconstructed water levels at jvs p001 01 the maes of 1 day forecasting are 0 9 m and 0 8 m at pakse and stung treng respectively while it is 1 4 m at kratie they increase with longer lead time kratie is 373 km away from jvs p001 01 which is more distant than 44 km and 240 km of pakse and stung treng therefore it can be the reason why the maes at kratie are relatively higher on the other hand kratie is the location where the hydraulic condition changes from a mainstream channel river flow to one where significant water exchange occurs between the river and floodplains this fact may influence the forecasting skill as well at the locations in the cambodian floodplain down from phnom penh the reconstructed water levels of the tsl in addition to jvs p001 01 were used in order to consider the influence of the reverse flow at phnom penh and koh khel the maes of 1 day forecasting are about 0 4 m while it is about 0 3 m at neak luong they also increase with longer lead time the nses of forecasting start from about 0 9 to 0 95 at 1 day forecasting and gradually decrease with longer lead time but can still maintain nses higher than 0 75 to 0 8 up to 20 day lead time since the change of the maes within 20 different days of forecasting can be tens of centimeters to around 1 m it can be misleading to evaluate our forecasting skill if we calculate the yearly averages and stds of the maes and nses from 20 different days of forecasting thus as shown in fig 8 we decided to show the yearly averages and stds of the maes and nses of 1 day to 5 day forecasts at nakhom phanom mukdahan and khong chiam the average maes in 2011 are apparently higher than other years with the lowest average nses at pakse stung treng and kratie the average nses in 2012 are relatively lower than other years at phnom penh the average maes in 2011 2014 and 2015 are relatively higher at neak luong and koh khel the average maes are relatively higher in 2014 and 2011 respectively the higher maes in 2011 might be due to the strike of two tropical storms haima and nokten followed by the influence of southwest monsoons and intertropical convergence zone itcz they led to two flood events with amplitudes over 4 m at stations from nakhom phanom to pakse and another flood event with amplitudes over 2 m at stung treng and over 3 5 m at kratie rfmmc 2011 the average nses are all above 0 8 and some of them are higher than 0 95 with no significant differences the variation of the average nses between years is only less than 0 15 fig 9 shows the correlation coefficients of our forecasting results with in situ observations generally speaking the correlation coefficients of our forecasting results with in situ water levels are about 0 9 at the locations in the middle reach of the mr and gradually decrease to about 0 6 0 7 with longer lead time they become larger than 0 95 at those in the cambodian floodplain in the case of 1 day forecasting then gradually decrease to 0 9 as lead time extends fig 10 shows the time series of 5 day forecasting and in situ water levels table 4 shows averages of years 2011 2015 of ars of our forecasting system based on the satisfactory benchmarks of rfmmc rfmmc 2011 2012 2013 it can be seen that in the middle reach of the mr most ars range about 10 30 except for khong chiam and kratie which range 5 25 and 5 17 respectively the lower ars at these two locations are probably due to the fact that khong chiam is close to the confluence of the mun river and mr and the complicated hydraulic condition at kratie at phnom penh neak luong and koh khel in the cambodian floodplain the values range 12 30 20 40 and 18 33 respectively therefore although our system generally provides generally higher maes than the satisfactory benchmarks of rfmmc based on fig 8 it can still provide satisfactory forecasting at portion of days during the wet season fig 11 shows the ars corresponding to 1 day to 20 day forecasting at 6 locations in the middle reach of the mr and 3 locations in the cambodian floodplain the benchmarks in the two regions are different 0 80 m 1 00 m 1 20 m in the middle reach of the mr and 0 30 m 0 40 m 0 50 m in the cambodian floodplain since they have different levels of forecasting skills generally the ars decrease with longer lead time the maes also have such pattern as fig 7 shows during the wet season of 2011 2015 the ars of the 0 8 m benchmark are about 50 70 at nakhom phanom 55 70 and 55 65 at mukdahan and khong chiam respectively and are 50 66 at pakse 70 78 at stung treng at kratie 55 60 of days can meet the 1 m benchmark at phnom penh 45 62 of days during the period can meet the 0 4 m benchmark and 55 66 and 50 66 of days can meet the 0 3 m benchmark at neak luong and koh khel respectively 4 3 flood forecasting in the md in the md in situ data at tan chau and chau doc spans until 2015 while it spans until 2012 at the other 11 locations pseudo forecasting with 20 different days of forecasting lead times during the wet season of 2011 2015 and 2011 to 2012 was performed at tan chau chau doc and the other 11 locations in the md respectively see fig 1 b for their locations in the md fig 12 shows the change of the maes and nses of our forecasted water levels with respect to 1 day to 20 day lead times from chau doc to can tho the maes generally increase with longer lead time starting from my thuan the maes decrease with the increase of lead time until they reach minimum at about 15 to 17 day lead time perhaps because there are stronger fortnightly and synodic fortnightly tidal influences on the water levels at these locations at chau doc and tan chau the differences between the maximum and minimum of the maes within 1 to 20 day lead times are about 10 cm at vam nao and cho moi they are about 0 04 m and it is about 0 02 m at long xuyen from can tho my thuan to the river mouth they are mostly less than 0 01 0 02 m in summary at locations from can tho my thuan to the river mouth the forecasted river levels have less than or about 0 10 m of maes with even longer than 10 day lead time the average nses range from 0 8 to 0 95 which are also promising and with only less than or about 0 1 of variations within 1 to 20 day lead times fig 13 shows the yearly averages and stds of the maes and nses from 20 different days of forecasting lead time the average maes range from 0 2 to about 0 4 m at chau doc and tan chau at vam nao and cho moi they range from 0 15 to 0 20 m while they range from 0 12 to 0 14 m at long xuyen the average maes then decrease to mostly less than 0 10 m at the locations downstream from can tho my thuan as approaching the river mouth note that at chau doc and tan chau the average maes are of 0 3 m and 0 4 m in 2011 and 2015 respectively which are relatively higher than the other years with the lowest average nses in 2015 moreover at vam nao and cho moi the average mae in 2011 are about 0 07 m higher than that in 2012 while there are only 0 01 0 02 m of differences between the 2 years at the other locations toward the river mouth the higher maes in 2011 may be due to the strike of tropical storm haima and nokten according to the rfmmc s seasonal flood situation report of 2011 rfmmc 2011 from the end of september to early november of 2011 water levels at tan chau and chau doc were recorded to be 3 4 m higher than flood levels as the result of flood water inflows from the upper and middle reaches of the mr it has resulted in extreme peak water levels at vam nao and cho moi as well and impacted the maes fig 14 shows the correlation coefficients of our forecasting results with in situ observations indicating excellent temporal agreement with in situ water levels the correlation coefficients are about or higher than 0 9 fig 15 shows the time series of in situ and 10 day forecasted water levels as marked in the red box of fig 15 from chau doc to cho moi a significant jump of water levels from the end of september to early november can be clearly observed however such extreme peaks did not occur at the locations further downstream to the river mouth therefore the forecasting skills at the locations closer to the river mouth remain consistent on the other hand at chau doc and tan chau the green boxes mark a period of june 2015 when the water level oscillations with an about 14 day period were extremely strong these strong water level oscillations may have resulted from a severe drought in 2015 united nations country team 2016 caused by enso which may have significantly intensified the tidal influence leading to higher maes and lower nses in 2015 as fig 13 shows the tidal influence model we built in this study are not able to consider the influence of enso on water levels which has 3 6 years of period since we used only less than three years of data to build the model however it can be addressed with longer time span of data in the future which would enable us to assimilate long period enso signals into our tidal model fig 16 shows the ars in the md with respect to 1 day to 20 day lead times we calculated the ars using 0 10 m 0 15 m and 0 20 m of aes as the benchmark it can be seen that at chau doc and tan chau about 60 of days meet the 0 20 m benchmark during 2011 2015 wet season for vam nao and cho moi about 60 of days during 2011 2012 wet season can meet the 0 15 m benchmark and are about 75 80 in the case of the 0 20 m benchmark at long xuyen about 66 of days can reach the 0 15 m benchmark and are about 81 of days in the case of the 0 20 m benchmark at the locations downstream from can tho my thuan there are about 55 70 75 85 and 90 93 of days can meet the 0 10 m 0 15 m and 0 20 m benchmarks respectively as fig 16 shows the ars become higher when performing forecasting further toward the river mouth the patterns shown in figs 12 13 and 16 indicate that the forecasted river levels become more accurate near the river mouth as the tidal effects on water levels in the coastal region become dominant and consistent compared to the upper md dang et al 2018 it is expected that the ocean tide model we proposed becomes more effective near the river mouth as the acceptable accuracy of forecasting is 15 cm in the md dr duong du bui nawapi personal communication our system can provide promising forecasting on about 50 80 of days during wet season 5 conclusions and perspectives this study proposes a model aided altimetry based flood forecasting system for the mr including the mr mainstem cambodian floodplain and md the system integrates satellite altimetry the vic hydrological model and the tidal influence models its is freely accessible easy to set up and has negligible computational cost making it operationally sustainable and scalable to other places especially for developing countries currently the vic hydrological model has already been set up in the vietnamese red river basin hiep et al 2018 indus river basin ganges river basin brahmaputra river basin and mangla river basin check http depts washington edu saswe datavis timeseries html with vic derived discharges altimetry derived water levels and historic in situ data our forecasting system can be easily implemented although the cross comparison with the mrc s forecasting shows that the forecasting skill of our system in the region outside of the md still has room for improvement our forecasting system is promising inside the md which has complex hydraulic condition we have performed a 10 day or longer lead time of pseudo forecasting at 13 locations in the md starting from tan chau chau doc which are the last downstream locations where the mrc provides forecasting we obtained a quite promising forecasting skill although due to the strike of tropical storms and strong water level oscillation in june 2015 the peak in situ water levels at chau doc to cho moi in 2011 were not captured in our forecasting results see chau doc to cho moi in fig 15 our system would be relatively easy and computationally inexpensive to be implemented by end users and stakeholders in cambodia and vietnam johnston and kummu 2012 summarized the mrc reports and estimated the cost of developing basin scale models for the mrc to be about usd 20 million banks et al 2014 also pointed out that the base price of the full featured isis model begins with 7680 per year for a single user license with an annual support and maintenance fee starting from 1350 considering the expense of model building and maintenance our freely available forecasting system could be operated as a complementary system to the rfmmc s forecasting especially in the md in our feasibility study we linearly interpolated jason 2 derived 10 day repeat tsl water levels and used them as nowcast of the tsl daily water levels however such interpolation cannot be done in real operational mode therefore for operational purpose up to about 10 day of forecasted water levels in the tsl need to be obtained until the next revisit of altimetry satellite they may be obtained by time series analysis methods such as autoregressive moving average model arma or autoregressive integrated moving average model arima alternatively the smooth and seasonality dominant pattern of the tsl water level changes might be well represented by a fourier series with known dominant frequencies by performing the fourier analysis upon historical tsl water levels for future study we would work on enhancing the forecasting skill and extending the lead time of our forecasting system with the aid of weather based flow forecasting at upstream sikder and hossain 2018 proposed an operationally feasible method called bias correction method to obtain quantitative precipitation forecasting up to 15 days and currently provides up to 7 day of weather based vic derived water discharge forecasting with promising accuracy see http depts washington edu saswe datavis timeseries html since our forecasting system used the vic derived discharges to reconstruct daily water levels at upstream vss the use of forecasted vic derived water discharges can give us forecasted reconstructed daily water levels which then can be used to perform forecast with extended lead times for example if the 7 day vic derived water discharge forecasting at an upstream vs is obtained the 7 day forecasted water level can be reconstructed by using the discharge to level rating curve this reconstructed water level forecasting with 7 day lead time can then be put into the forecasting model of 10 day lead time for example to forecast 17 day later downstream water levels at the locations of downstream in situ gauges although the accuracy of these forecasted water discharges are not as good as nowcasted water discharges as sikder and hossain 2018 indicated 0 77 0 57 of cor with 34 40 of normalized rmses nrmses with 1 7 days of lead times compared with 0 86 of cor and 24 9 of nrmse of nowcasted water discharges at kampong cham in the mr it is expected that the capacity of the forecasting system proposed in this study still has potential to be improved with extended lead times on the other hand local rainfall events at the forecasting locations also need to be addressed in the future as it may be an error source of our forecasting since currently our system is only based on the linear relationship between upstream and downstream water levels moreover it is expected that the enso induced strong water level oscillations in the md can be addressed if data with longer time span is available in the future to build more comprehensive tidal influence models 6 software and data availability the mekong river flood forecasting tool mr fft is developed with cooperation of department of civil and environmental engineering at university of houston department of civil and environmental engineering at university of washington and asian disaster preparedness center under the support of nasa s servir and geo program see acknowledgments as one of the major purposes of nasa servir and geo program the software is free and publicly open the software was originally coded using matlab a graphic user interface has been wrapped up as a package which can be run using freely accessible matlab runtime included in the package on a 64 bit microsoft windows operation system the prototype of the software with sample data which allows users to 1 reconstruct daily water levels at upstream vss 2 build forecasting model and 3 perform pseudo forecasting with user guide can be accessed through https drive google com open id 1lg5w8rw4urhzugroqhyjftsdkbf1vdqb acknowledgments this study was supported by nasa s servir program nnx16an35g and geo program 80nssc18k0423 we also thank the anonymous reviewers for their constructive comments 
26271,a freely accessible model aided satellite altimeter based daily water level forecasting system using simple regression analysis is proposed for the mekong river with feasibility study being performed in the mekong delta md where regional flood management and mitigation center rfmmc does not provide forecasting ocean tides strongly impact river levels and were specifically addressed by the sum of 5 term sinusoidal function forecasting skills of our system are quite promising in the md although rfmmc s forecasting system has better skills than ours in the mekong mainstem upstream of md in contrast to current operational system our system circumvents the need of frequent altimeter samplings in the upstream by using the variable infiltration capacity vic macroscale hydrologic model with 0 1 resolution the proposed forecasting system is computationally efficient without the need of complex hydrodynamic modeling making such approach globally applicable for river basins and deltas with comparable forecasting skill and minimal computational cost keywords the mekong river satellite altimetry hydrologic model water level forecasting 1 introduction the mekong river mr is the twelfth longest river in the world and the largest river in southeast asia flowing through six countries china lao pdr myanmar thailand cambodia and vietnam the importance of the mr on a regional and global scale is more than apparent from multiple aspects inhabitants in the mr basin mrb have been extensively relying on resources from the river as their major food supply and source of income by developing floodplain agriculture and fresh water fishery mekong river commission mrc 2011 the mekong delta md located at the mouth of the mr is one of the most populous regions in the mrb there are about 16 million people who live in the md which is about 23 of the population of the mrb food and agriculture organization of the united nations fao 2012 the md together with central highland has a population density of 260 persons km2 which is the highest in the mrb fao 2012 it is also the most important rice production region not only in vietnam but also in the world with about 20 of shares in the global rice export quantity in 2012 fao 2017 in recent years several studies have indicated increases in annual and seasonal river discharges in the mr under future climate change which results in increased flood risks during the wet season in the cambodian and vietnamese floodplain hoang et al 2016 västilä et al 2010 mrc 2010 hoanh et al 2010 developed a climate change analysis framework using the decision support framework models of the mrc to analyze the impacts of future climate change and planned water resources management they concluded an increase of river flow of 2 11 and 18 30 under projected climate change from 2010 to 2050 in high flow and low flow season respectively data from the tide gauges along vietnamese coasts also show that sea level rose at a rate of about 3 mm year during 1993 2008 ministry of natural resources and environment monre 2010 this rising rate is about the same as the global average of about 3 mm year determined from topex poseidon t p jason 1 and jason 2 satellite altimetry observations from 1993 to 2014 ablain et al 2017 wassmann et al 2004 assessed the future flood regime in the md under the scenarios of 20 cm and 45 cm of sea level rises and derived an average water level rises of 11 9 cm and 27 4 cm at the peak of the flood season october respectively as the sea level at the coast of vietnam is projected to continuously rise under the representative concentration pathways rcps adopted by ipcc fifth assessment report ar5 commonwealth scientific and industrial research organization csiro 2014 the md may become more and more vulnerable to flood events in the future threatening the livelihood and food security of millions of people major economic social and environmental changes are also expected in the next 10 years due to urbanization industrialization and sanitation issues boosted by recent and rapid increase in population johnston and kummu 2012 as mason d croz et al 2016 pointed out the importance of tools to explore the potential effects of climatic socioeconomic and environmental changes robust and sustainable flood forecasting can be a great aid in decision making for flood risk reduction and response making the mrb more resilient when facing an uncertain future the regional flood management and mitigation center rfmmc affiliated with the mrc was founded in 2005 after a series of severe floods from 2000 to 2001 it currently issues an up to 5 day lead time of daily water level forecasting at each of 22 locations from chiang saen to tan chau chau doc during the wet season which is from june to october pagano 2014 the flood forecasting system adopted by the rfmmc uses the unified river basin simulator urbs rainfall runoff runoff routing model and isis hydrodynamic model pagano 2014 tospornsampan et al 2009 the operation of the rfmmc s forecasting system is based on the deltares flood early warning system delft fews platform which links data and models in real time to perform daily forecasting werner et al 2013 at the locations of each in situ water level gauge rainfall forecast is first provided by the fifth generation mesoscale model mm5 cox et al 1998 and then used as input of urbs model for discharge estimation which is then converted to forecasted water level via a rating curve the rfmmc also uses the isis hydrodynamic model for water level forecast downstream from stung treng to the ocean pagano 2014 tospornsampan et al 2009 however the isis hydrodynamic model is computationally intensive and therefore is run only for retrospective analyses and when demand arises pagano 2014 the rfmmc also does not issue any forecasting in the md downstream from tan chau chau doc considering the increasing vulnerability of the md under the future climate change a sustainable flood forecasting system which can be operated on routine basis with low computational cost is necessary however complex hydraulic conditions in the md hinder such a flood forecasting system the tonle sap lake tsl has an effect on the md and needs to be considered in addition to the flow from the mr fig 1 in order to perform flood forecasting the tsl is the largest freshwater lake in southeast asia and is called the heart of the lower mekong campbell et al 2009 since most of the population of cambodia relies on it the existence of the tsl leads to a unique flow reversal the water flows from the lake to the cambodian floodplain the bassac river and the md through the tonle sap river in september october and flows back to the lake in may june thus the tsl plays an important role in flood peak attenuation and flow control to the md pagano 2014 heng 2009 the unique flow reversal makes the tsl function as a large natural reservoir which significantly benefits people living in the downstream because it reduces the severity of flooding by moderating peak flow rates during the wet season it also provides additional water resources for irrigational use and eases the extent of salt water intrusion into the md during the dry season mrc 2009 however this flow reversal also poses a challenge from modeling perspective which necessitates the use of a high resolution hydrodynamic model to physically mimic the flow reversal due to the hydrologic drivers running a high resolution hydrodynamic model is often computationally expensive which is also the reason why the rfmmc does not routinely operate it in the md moreover the ocean tide intrusion can reach up to phnom penh in the dry season pagano 2014 the mrc 2005 also pointed out that ocean tide can propagate up to the towns close to the cambodia vietnam border causing about 1 m of tidal variation in river level takagi et al 2013 studied the tidal influence on water levels in can tho and suggested that the interaction between tide and river flow need to be considered together when studying floods in the md recently satellite radar altimetry has been successfully used to observe and forecast river level changes thanks to the advance of waveform retracking techniques satellite radar altimetry has been used to collect water level changes at the satellite ground track and inland water body cross sections the so called virtual stations vss the feasibility of using satellite altimetry derived water level changes in flood forecasting has already been proven biancamaria et al 2011 demonstrated that t p altimetry derived water level observations at upstream vss over the ganges and brahmaputra rivers in india can be used to forecast downstream water levels in bangladesh through regression analysis about every 10 days due to t p s repeat cycle the feasibility test showed that forecasted water levels have a root mean square error rmse of 0 40 m and 0 60 0 80 m at 5 days and 10 days of lead time respectively hossain et al 2014a used jason 2 altimetry derived water levels obtained approximately every 10 days at upstream vss in india to obtain daily 5 day forecasting of downstream water discharges at the upstream most boundary points of the hydrologic engineering center river analysis software hec ras hydrodynamic model setup in bangladesh the forecasted discharges were then used as a boundary condition for the hec ras model to simulate daily 5 day forecasted river levels at 17 locations inside bangladesh the system yielded an rmse ranging from 0 5 to 1 5 m with a bias of 0 25 1 25 m compared with 5 day later nowcast model simulated water levels in bangladesh using in situ water levels at the upstream boundary points as the boundary condition when compared with in situ water levels at 3 locations for up to 5 day lead times it reaches mean errors ranging 0 4 0 4 m with rmses of 0 2 0 7 m however this daily forecasting system relies on a number of jason 2 vss to ensure frequent sampling and data availability hossain et al 2014b since the mr is a river flowing from north to south with only a few number of skillful vss direct implementation of this system becomes challenging to address this issue we propose a model aided altimeter based river level forecasting system called the mekong river flood forecasting tool mr fft it integrates jason 2 altimetry derived water levels and discharge estimations from the variable infiltration capacity hydrologic vic model liang et al 1994 hossain et al 2017 here we present a feasibility study to demonstrate its forecasting skill in the mr especially in the md a multivariate regression model and sum of sinusoidal functions were employed to address the downstream river level changes due to different causes considering multiple variables for a better representation maçaira et al 2018 of water levels in the md the unique features of our flood forecasting system are 1 an easy to set up hydrologic model is implemented which circumvents the need to have frequent altimeter samplings in the upstream 2 the forecasting approach in the delta is based on computationally efficient regression analysis instead of complex hydrodynamic modeling 3 it is freely accessible for stakeholders and users with tolerable computational cost 4 it can be easily extended for other deltas and transboundary rivers in developing countries such as the vietnamese red river ganges brahmaputra meghna river and niger river with skillful upstream altimetry vss a hydrologic model such as the vic model and historic in situ water levels no updated in situ water levels are needed in the downstream our system can be easily implemented as long as no major dams exist between the upstream vss and downstream in situ stations this paper is structured as follows section 2 describes the data we used including altimetry derived water level data and discharges estimations from the vic model section 3 explains the model aided altimeter based water level forecasting system in detail section 4 presents results from the feasibility study section 5 concludes this paper with suggestions for operational flood forecasting section 6 is the software description 2 data 2 1 jason 2 altimetry derived river levels jason 2 launched on june 20th 2008 is the follow up mission of jason 1 operating under the cooperation of the national aeronautics and space administration nasa center national d etudes spatiales cnes national oceanic and atmospheric administration noaa and european organization for the exploitation of meteorological satellites eumetsat it succeeds its predecessors including t p and jason 1 missions to continuously provide highly accurate altimetry data with a 10 day repeat cycle in this feasibility study we used 20 hz ice retracked ranges from the jason 2 geophysical data record gdr to extract river levels at vss located in the mr mainstem and tsl the locations of vss are shown in fig 1 the precisions of altimetry derived water levels over the mr mainstem and tsl were about 0 4 0 5 m and 0 26 m respectively when compared with the nearest in situ gauge data see fig 2 for the jvs p001 02 over the tsl the altimetry water levels have a better rmse since the tsl is a large inland water body where more robust radar altimetry measurements can be collected 2 2 modeled discharges at virtual stations the river discharges at the locations of vss are obtained by the vic hydrological model liang et al 1994 and a streamflow routing model lohmann et al 1996 the vic model is comprised of three layer soil column structure and considers land cover soil type and meteorological forcing data such as precipitation temperature and wind speed to characterize the hydrological mechanism in the soil column of given ground cells to estimate runoffs and baseflows the streamflow routing model lohmann et al 1996 was then used to route the surface runoffs and baseflows estimated by the vic model to the river channels based on the given flow direction map to simulate discharges in our study a 0 1 degree resolution vic model was set up in the mrb hossain et al 2017 using the global land cover characterization glcc land cover dataset and the harmonized world soil database hwsd soil data siddique e akbor et al 2014 the monthly leaf area index and albedo were obtained by the moderate resolution imaging spectro radiometer mission modis data the flow direction map was derived from the shuttle radar topography mission srtm digital elevation model dem over the mrb six sub basin segments were selected for model calibration including chiang sean luang prabang vientiane nakhon phanom pakse and kampong cham the vic model was calibrated using soil parameters including the variable infiltration curve parameter b inf maximum velocity of baseflow ds max fraction of ds max ds and soil moisture ws where non linear baseflow occurs on the other hand the cell impulse response function of the routing model was also calibrated hossain et al 2017 the current vic model output downstream of kampong cham where bidirectional flow due to the tsl and complex river network exist turns out to be less reliable however the vic model is only used to estimate water discharges at vss in the upstream of mekong in this study the meteorological forcings such as maximum and minimum temperatures wind speed and precipitation were obtained from 237 weather station records archived as the global summary of the day gsod by the national climatic data center ncdc the model was run at a daily time step to simulate the discharges from 2002 to 2015 at the locations of jason 2 vss the output of first year 2002 was excluded to avoid spin up error the calibration period was set to be from 2003 to 2008 the validation period was from 2009 to 2013 the simulated water discharges at nakhon phanom and khong chiam which are the nearest stations to jvs p179 01 and jvs p001 01 have nash sutcliffe efficiency nse of 0 82 and 0 86 respectively for more details about the vic model set up in the mrb readers are referred to hossain et al 2017 currently this model runs operationally for nowcast forced with satellite precipitation at several locations along the mr see http depts washington edu saswe 2 3 in situ water levels the in situ water levels in the mr were provided by the asian disaster preparedness center adpc and were used to build and test the feasibility of our model aided altimetry based forecasting system in our study the in situ water level data collected at 6 locations in the middle reach of the mr 3 locations in the cambodian floodplain and 13 locations in the md were used see table 1 fig 1 shows the locations of all of the in situ gauges used a in the middle reach of the mr and cambodian floodplain and b in the md toward the river mouth 3 methods to obtain a daily water level forecasting hossain et al 2014a used jason 2 altimetry derived water levels with 10 day repeat cycle at upstream vss to obtain daily forecasted downstream water discharges at downstream in situ gauges as mentioned above it requires a sufficient number of vss to ensure that at least one of the vss gets sampled in the upstream neighborhood for deriving expected water levels in the downstream hossain et al 2014b however the mr is a north to south flowing river where only a few number of vss can be found hindering the direct implementation of this system hence we attempt to use the vic hydrologic model which can provide daily river discharges at vss to fill the temporal gap of altimeter data at model building step data from october 2008 to december 2010 were used hereafter called as historical at each of vss in the upstream of the mr mainstem a discharge to level rating curve was built using historical vic derived discharges and jason 2 altimetry derived water levels the rating curve was then used to reconstruct historical daily water levels see section 3 1 a relationship between upstream water levels at each of the vss and downstream water levels at each of the in situ gauges was then built through a simple or multivariate linear regression taking the tsl water levels into account depending on the locations see section 3 2 the tidal influence models were also built where necessary see section 3 2 at pseudo forecasting step data from 2011 were used by using the rating curves and models from the model building step the pseudo forecasting was performed and validated to examine the skill of the system the indices for the skill evaluation are described in section 3 3 3 1 daily water level reconstruction at virtual stations the conventional water level to discharge rating curve follows a power law curve in the form of 1 q v i c c h a l t a b where h a l t is jason 2 altimetry derived water level q v i c is discharge simulated from the vic and streamflow routing model and c and b are coefficients to be estimated equation 1 can be transformed into a linear form by taking the logarithm on both sides and then re arranged as 2 l o g h a l t a 1 b l o g q v i c 1 b l o g c a l o g q v i c b which is a linear function where coefficients a and b can be estimated by least squares method while a can be directly solved by johnson s method rantz 1982 world meteorological organization wmo 1980 the logarithm value of daily q v i c can then be used to reconstruct daily water levels h r e c 3 h r e c e a l o g q v i c b a in the tsl where water levels vary smoothly see fig 2 c the daily water levels were reconstructed by linearly interpolating 10 day repeat jason 2 altimetry derived water levels the rating curves and reconstructed water level time series are shown in fig 3 and fig 4 respectively in this study data from october 2008 to december 2010 were used to build the rating curves for historical daily water level reconstruction at the jason 2 vss fig 3 also see fig 5 a for flow chart and data from year 2011 were used for validation with in situ gauge data fig 4 also see fig 6 for flow chart as the bottom panel of fig 3 shows the reconstructed water levels in the middle reach of the mr have rmses of about 1 m with temporal correlation coefficients of 0 95 the rmses of reconstructed water levels at the vss in the mr may depend on the accuracies of altimetry derived water level itself and the vic derived discharge furthermore some spikes in the reconstructed water levels during dry season could result from the errors of in situ precipitation records that are the input data of the vic model for the vs in the tsl the reconstructed water levels show a smaller rmse of about 0 3 m with a temporal correlation of about 1 due to the fact that the tsl is a large water body where a larger number of 20 hz jason 2 measurements are available 3 2 forecasting models considering that the hydraulic conditions of the mr are complex due to its large geographic coverage and various geographic features we built two different forecasting models for 1 the middle reach of the mr and 2 the cambodian floodplain and the md we used the reconstructed water levels at upstream vss and downstream in situ water levels from october 2008 to december 2010 to build the forecasting models and tidal influence models see fig 5 for flow charts data from 2011 were used to perform pseudo forecasting see fig 6 for flow chart the forecasted water levels were then validated with in situ observations for each of the locations from nakhon phanom to kratie see fig 1 a in the middle reach of the mainstem mr the hydraulic condition is relatively simple pagano 2014 thus we performed a simple linear regression analysis using the reconstructed daily water levels at upstream vs and observed historical daily water levels at downstream in situ gauge considering different lead times to build the forecasting model as 4 h i n s i t u t k e h r e c t f where h i n s i t u t is historic water levels at downstream in situ gauge at time t h r e c t is the reconstructed water levels at upstream vs at time t which were derived by equation 3 k is the forecasting lead time day and e and f are coefficients to be estimated for this linear forecasting model for each of the locations from phnom penh to the md see fig 1 b the river flows are dominated by the water exchange between river channel and floodplain and the flow reversal due to the tsl furthermore ocean tides can also intrude up to phnom penh in the dry season pagano 2014 considering these multiple sources of water level changes we propose a two step method to build the forecasting model for the lower reach of the mr the first step is to use the reconstructed water levels at vss in the upstream mr mainstem and the tsl to perform multivariate linear regression analysis with downstream in situ water levels 5 h i n s i t u t k g h r e c m k t h h r e c t s l t i v t k where h r e c m k t is the reconstructed daily water levels in the mr mainstem at time t derived by equation 3 h r e c t s l t is the reconstructed daily water levels in the tsl which were obtained by linearly interpolating jason 2 altimetry water levels as section 3 1 described g h and i are coefficients to be estimated for this multivariate linear regression model k is the forecasting lead time day and v t k is the residual between modeled results and h i n s i t u t k since the model only considers the influence of the mr mainstem and tsl we assume that v here is dominated by the ocean tide influence and hereafter rephrased as h t i d e t for the case of k 0 next we adopted a model of tidal influence as the sum of 5 term sinusoidal functions while considering dominant tidal frequencies including annual semi annual monthly fortnightly and synodic fortnightly tides see table 2 egbert and ray 2003 zheng and zhao 1984 6 h t i d e t i 1 5 m i s i n 2 π ω i t n i where ω is the dominant tidal frequency number of cycles per year while m and n are the magnitude and phase shift to be estimated respectively once m and n are fitted t can be updated as decimal year at the time we want to perform forecasting subsequently the summation of modeled results given by equation 5 and equation 6 provides us with forecasted water levels which now consider influences from the mr mainstem tsl and the ocean tide intrusion 3 3 model evaluation statistics to evaluate model capacity several statistical indices including the mean absolute error mae error of standard deviation estd correlation coefficient cor as well as nse and achievement rate ar were adopted the mae is a measure of error which can be calculated by 7 m a e i 1 n y i o b s y i f c t n where n is the total number of data with i 1 n y i o b s is the in situ observations and y i f c t is the forecasting results a mae value of 0 indicates a perfect forecasting the estd is the standard deviation of difference between forecasting results and observations 8 e s t d i 1 n e i e 2 n where e i y i o b s y i f c t e is the mean of e i this value only reflects how dense the error distributes but does not reflect the average error the cor is a measure of temporal agreement between forecasting and observations 9 c o r i 1 n y i f c t y f c t y i o b s y o b s i 1 n y i f c t y f c t 2 i 1 n y i o b s y o b s 2 where y f c t is the mean of forecasting and y o b s is the mean of the observations the nse is a normalized statistic which reflects the relative magnitude of the residual variance noise compared to the measured data variance information nash and sutcliffe 1970 it indicates how well the plot of observations versus forecasting results fits the 1 1 line and is commonly used to evaluate the performance of the forecasting model 10 n s e 1 i 1 n y i o b s y i f c t 2 i 1 n y i o b s y o b s 2 nse ranges from to 1 0 an nse value of 1 indicates perfect forecasting skill 0 means the forecasting skill is no better than adopting the average of observations as forecasting results and a negative nse means unacceptable skill moriasi et al 2007 the ar is the rate of days in the wet season that meets the satisfactory benchmark in which we used absolute error ae 11 a r s n where s is the number of days in the wet season when the difference between forecasted water levels and in situ observations is smaller than or equal to the benchmark 4 results 4 1 cross comparison with current system for cross comparison of our system with the system adopted by the rfmmc we compared the results of the 5 day forecasting which is the longest forecasting lead time that the rfmmc performs during the wet season june to october the cross comparison was conducted at 6 locations in the middle reach of the mr nakhon phanom mukdahan khong chiam pakse stung treng and kratie 3 locations in the cambodian floodplain phnom penh koh khel and neak luong and 2 locations in the upstream of the md tan chau and chau doc the mae based annual accuracy assessment in the rfmmc s seasonal flood situation report available on http ffw mrcmekong org report seasonal php is used for comparison from 2011 to 2013 year by year rfmmc 2011 2012 2013 on the other hand the estds of our forecasting results during the wet season from 2011 to 2015 were also compared with pagano 2014 s evaluation of performance of the rfmmc s forecasting system using historical data from 2002 to 2012 for a long term evaluation although the time spans of the long term estds are different pagano 2014 s estimation is the only long term accuracy assessment that we can find forecasting at the locations down to khong chiam was performed using jvs p179 01 while jvs p001 01 was used for forecasting at locations from pakse for forecasting at the locations downstream of the tsl both jvs p001 01 and jvs p001 02 were used to represent different water sources from the mr mainstem and the tsl respectively as table 3 shows in the middle reach of the mr our forecasted river levels were in general less accurate than the rfmmc s forecasts we obtained yearly maes of about 1 2 2 m and long term estds of about 0 8 1 5 m during the wet season while the rfmmc s yearly maes are about 0 4 0 7 m with long term estds of about 0 7 0 9 m for the locations in the cambodian floodplain the accuracy of our system improved yearly maes of our system are about 0 4 0 5 m and long term estds are about 0 5 m while the rfmmc s yearly maes are of about 0 2 0 25 m with long term estds of about 0 25 0 3 m overall in the middle reach of the mr and the cambodian floodplain the rfmmc s forecasting showed slightly better accuracy than ours since our system uses vic derived discharges to build the rating curve with altimetry derived water levels for reconstruction of daily water levels at vss both of their accuracies can be leading factors of our forecasting skill for example the rfmmc s forecasting system uses a calibrated set of 51 urbs models which cover over 2244 sub areas 740 000 km2 tospornsampan et al 2009 while the vic model we currently adopted was calibrated at only 6 sub basins in addition the accuracy of altimetry derived water levels can be up to about half of a meter as shown in fig 2 on the other hand currently our forecasting system only assumes that there is a linear relationship between upstream and downstream water levels but does not consider the impact of tributaries and direct rainfalls in between therefore it is expected that our forecasting skill can be further improved by fine tuning the vic model by performing more advanced altimetry waveform retracking and by considering those additional contributions to the downstream water levels however it needs to be emphasized that our forecasting system uses freely accessible and publicly available satellite data and hydrologic model and is able to perform forecasting without updated in situ data on the other hand our system is purely based on simple regression analysis instead of a computationally expensive weather forecast model and hydrodynamic model for example the rfmmc s system uses mm5 to forecast rainfalls as an input of the urbs models which have a computational cost and skill set requirement while our system does not features including strong affordability easy to set up and negligible computational cost make our system operationally sustainable and scalable to other places especially for developing countries it is also worthwhile to mention that in tan chau and chau doc which are close to the upstream boundary of the md in vietnam our system showed yearly maes of about 0 2 0 3 m and long term estds of about 0 3 m which are compatible with the rfmmc s results the smaller maes might be due to the lower amplitude of water levels in the md but it also reflects the fact that the sinusoidal model to represent the tidal influence was effective in the md moreover our system can provide forecasting in the further downstream of tan chau and chau doc inside the md where the rfmmc currently does not provide any forecasting after cross comparison with the rfmmc s report we attempted to perform a pseudo forecasting considering 20 different forecasting lead times from 1 day forecasting to 20 day forecasting in each region and then validated our forecasting results using in situ data in the following section the forecasting skill of our system will be discussed 4 2 flood forecasting in the middle reach of the mr and the cambodian floodplain in the middle reach of the mr and the cambodian floodplain in situ data are available until 2015 which were used to validate our forecasting results fig 7 shows the maes and nses of our system during the wet season from 2011 to 2015 up to 20 day lead time of forecasting was performed forecasting at nakhom phanom mukdahan and khong chiam was performed by using reconstructed water levels at jvs p179 01 we can see that the maes of 1 day forecasting at nakhom phanom and mukdahan are about 0 9 m then gradually increase with lead time while it starts from about 1 2 m at khong chiam the relatively higher maes at khong chiam might be due to its longer distance away from jvs p179 01 which is about 477 km compared to the 141 km and 248 km of nakhom phanom and mukdahan respectively since our forecasting is based on a linear relationship between upstream vs water levels and downstream in situ water levels hydrologic or hydraulic condition changes between upstream vss and downstream in situ stations due to geographic remoteness can influence the forecasting skill on the other hand khong chiam is close to the confluence of the mun river and mr which might be another reason why forecasting at khong chiam has higher maes compared to that at nakhom phanom and mukdahan the nses of forecasting at these locations start from about 0 8 at 1 day forecasting and gradually decrease with longer lead time but can still maintain at 0 7 up to about 5 day lead time forecasting starting from pakse was performed by using reconstructed water levels at jvs p001 01 the maes of 1 day forecasting are 0 9 m and 0 8 m at pakse and stung treng respectively while it is 1 4 m at kratie they increase with longer lead time kratie is 373 km away from jvs p001 01 which is more distant than 44 km and 240 km of pakse and stung treng therefore it can be the reason why the maes at kratie are relatively higher on the other hand kratie is the location where the hydraulic condition changes from a mainstream channel river flow to one where significant water exchange occurs between the river and floodplains this fact may influence the forecasting skill as well at the locations in the cambodian floodplain down from phnom penh the reconstructed water levels of the tsl in addition to jvs p001 01 were used in order to consider the influence of the reverse flow at phnom penh and koh khel the maes of 1 day forecasting are about 0 4 m while it is about 0 3 m at neak luong they also increase with longer lead time the nses of forecasting start from about 0 9 to 0 95 at 1 day forecasting and gradually decrease with longer lead time but can still maintain nses higher than 0 75 to 0 8 up to 20 day lead time since the change of the maes within 20 different days of forecasting can be tens of centimeters to around 1 m it can be misleading to evaluate our forecasting skill if we calculate the yearly averages and stds of the maes and nses from 20 different days of forecasting thus as shown in fig 8 we decided to show the yearly averages and stds of the maes and nses of 1 day to 5 day forecasts at nakhom phanom mukdahan and khong chiam the average maes in 2011 are apparently higher than other years with the lowest average nses at pakse stung treng and kratie the average nses in 2012 are relatively lower than other years at phnom penh the average maes in 2011 2014 and 2015 are relatively higher at neak luong and koh khel the average maes are relatively higher in 2014 and 2011 respectively the higher maes in 2011 might be due to the strike of two tropical storms haima and nokten followed by the influence of southwest monsoons and intertropical convergence zone itcz they led to two flood events with amplitudes over 4 m at stations from nakhom phanom to pakse and another flood event with amplitudes over 2 m at stung treng and over 3 5 m at kratie rfmmc 2011 the average nses are all above 0 8 and some of them are higher than 0 95 with no significant differences the variation of the average nses between years is only less than 0 15 fig 9 shows the correlation coefficients of our forecasting results with in situ observations generally speaking the correlation coefficients of our forecasting results with in situ water levels are about 0 9 at the locations in the middle reach of the mr and gradually decrease to about 0 6 0 7 with longer lead time they become larger than 0 95 at those in the cambodian floodplain in the case of 1 day forecasting then gradually decrease to 0 9 as lead time extends fig 10 shows the time series of 5 day forecasting and in situ water levels table 4 shows averages of years 2011 2015 of ars of our forecasting system based on the satisfactory benchmarks of rfmmc rfmmc 2011 2012 2013 it can be seen that in the middle reach of the mr most ars range about 10 30 except for khong chiam and kratie which range 5 25 and 5 17 respectively the lower ars at these two locations are probably due to the fact that khong chiam is close to the confluence of the mun river and mr and the complicated hydraulic condition at kratie at phnom penh neak luong and koh khel in the cambodian floodplain the values range 12 30 20 40 and 18 33 respectively therefore although our system generally provides generally higher maes than the satisfactory benchmarks of rfmmc based on fig 8 it can still provide satisfactory forecasting at portion of days during the wet season fig 11 shows the ars corresponding to 1 day to 20 day forecasting at 6 locations in the middle reach of the mr and 3 locations in the cambodian floodplain the benchmarks in the two regions are different 0 80 m 1 00 m 1 20 m in the middle reach of the mr and 0 30 m 0 40 m 0 50 m in the cambodian floodplain since they have different levels of forecasting skills generally the ars decrease with longer lead time the maes also have such pattern as fig 7 shows during the wet season of 2011 2015 the ars of the 0 8 m benchmark are about 50 70 at nakhom phanom 55 70 and 55 65 at mukdahan and khong chiam respectively and are 50 66 at pakse 70 78 at stung treng at kratie 55 60 of days can meet the 1 m benchmark at phnom penh 45 62 of days during the period can meet the 0 4 m benchmark and 55 66 and 50 66 of days can meet the 0 3 m benchmark at neak luong and koh khel respectively 4 3 flood forecasting in the md in the md in situ data at tan chau and chau doc spans until 2015 while it spans until 2012 at the other 11 locations pseudo forecasting with 20 different days of forecasting lead times during the wet season of 2011 2015 and 2011 to 2012 was performed at tan chau chau doc and the other 11 locations in the md respectively see fig 1 b for their locations in the md fig 12 shows the change of the maes and nses of our forecasted water levels with respect to 1 day to 20 day lead times from chau doc to can tho the maes generally increase with longer lead time starting from my thuan the maes decrease with the increase of lead time until they reach minimum at about 15 to 17 day lead time perhaps because there are stronger fortnightly and synodic fortnightly tidal influences on the water levels at these locations at chau doc and tan chau the differences between the maximum and minimum of the maes within 1 to 20 day lead times are about 10 cm at vam nao and cho moi they are about 0 04 m and it is about 0 02 m at long xuyen from can tho my thuan to the river mouth they are mostly less than 0 01 0 02 m in summary at locations from can tho my thuan to the river mouth the forecasted river levels have less than or about 0 10 m of maes with even longer than 10 day lead time the average nses range from 0 8 to 0 95 which are also promising and with only less than or about 0 1 of variations within 1 to 20 day lead times fig 13 shows the yearly averages and stds of the maes and nses from 20 different days of forecasting lead time the average maes range from 0 2 to about 0 4 m at chau doc and tan chau at vam nao and cho moi they range from 0 15 to 0 20 m while they range from 0 12 to 0 14 m at long xuyen the average maes then decrease to mostly less than 0 10 m at the locations downstream from can tho my thuan as approaching the river mouth note that at chau doc and tan chau the average maes are of 0 3 m and 0 4 m in 2011 and 2015 respectively which are relatively higher than the other years with the lowest average nses in 2015 moreover at vam nao and cho moi the average mae in 2011 are about 0 07 m higher than that in 2012 while there are only 0 01 0 02 m of differences between the 2 years at the other locations toward the river mouth the higher maes in 2011 may be due to the strike of tropical storm haima and nokten according to the rfmmc s seasonal flood situation report of 2011 rfmmc 2011 from the end of september to early november of 2011 water levels at tan chau and chau doc were recorded to be 3 4 m higher than flood levels as the result of flood water inflows from the upper and middle reaches of the mr it has resulted in extreme peak water levels at vam nao and cho moi as well and impacted the maes fig 14 shows the correlation coefficients of our forecasting results with in situ observations indicating excellent temporal agreement with in situ water levels the correlation coefficients are about or higher than 0 9 fig 15 shows the time series of in situ and 10 day forecasted water levels as marked in the red box of fig 15 from chau doc to cho moi a significant jump of water levels from the end of september to early november can be clearly observed however such extreme peaks did not occur at the locations further downstream to the river mouth therefore the forecasting skills at the locations closer to the river mouth remain consistent on the other hand at chau doc and tan chau the green boxes mark a period of june 2015 when the water level oscillations with an about 14 day period were extremely strong these strong water level oscillations may have resulted from a severe drought in 2015 united nations country team 2016 caused by enso which may have significantly intensified the tidal influence leading to higher maes and lower nses in 2015 as fig 13 shows the tidal influence model we built in this study are not able to consider the influence of enso on water levels which has 3 6 years of period since we used only less than three years of data to build the model however it can be addressed with longer time span of data in the future which would enable us to assimilate long period enso signals into our tidal model fig 16 shows the ars in the md with respect to 1 day to 20 day lead times we calculated the ars using 0 10 m 0 15 m and 0 20 m of aes as the benchmark it can be seen that at chau doc and tan chau about 60 of days meet the 0 20 m benchmark during 2011 2015 wet season for vam nao and cho moi about 60 of days during 2011 2012 wet season can meet the 0 15 m benchmark and are about 75 80 in the case of the 0 20 m benchmark at long xuyen about 66 of days can reach the 0 15 m benchmark and are about 81 of days in the case of the 0 20 m benchmark at the locations downstream from can tho my thuan there are about 55 70 75 85 and 90 93 of days can meet the 0 10 m 0 15 m and 0 20 m benchmarks respectively as fig 16 shows the ars become higher when performing forecasting further toward the river mouth the patterns shown in figs 12 13 and 16 indicate that the forecasted river levels become more accurate near the river mouth as the tidal effects on water levels in the coastal region become dominant and consistent compared to the upper md dang et al 2018 it is expected that the ocean tide model we proposed becomes more effective near the river mouth as the acceptable accuracy of forecasting is 15 cm in the md dr duong du bui nawapi personal communication our system can provide promising forecasting on about 50 80 of days during wet season 5 conclusions and perspectives this study proposes a model aided altimetry based flood forecasting system for the mr including the mr mainstem cambodian floodplain and md the system integrates satellite altimetry the vic hydrological model and the tidal influence models its is freely accessible easy to set up and has negligible computational cost making it operationally sustainable and scalable to other places especially for developing countries currently the vic hydrological model has already been set up in the vietnamese red river basin hiep et al 2018 indus river basin ganges river basin brahmaputra river basin and mangla river basin check http depts washington edu saswe datavis timeseries html with vic derived discharges altimetry derived water levels and historic in situ data our forecasting system can be easily implemented although the cross comparison with the mrc s forecasting shows that the forecasting skill of our system in the region outside of the md still has room for improvement our forecasting system is promising inside the md which has complex hydraulic condition we have performed a 10 day or longer lead time of pseudo forecasting at 13 locations in the md starting from tan chau chau doc which are the last downstream locations where the mrc provides forecasting we obtained a quite promising forecasting skill although due to the strike of tropical storms and strong water level oscillation in june 2015 the peak in situ water levels at chau doc to cho moi in 2011 were not captured in our forecasting results see chau doc to cho moi in fig 15 our system would be relatively easy and computationally inexpensive to be implemented by end users and stakeholders in cambodia and vietnam johnston and kummu 2012 summarized the mrc reports and estimated the cost of developing basin scale models for the mrc to be about usd 20 million banks et al 2014 also pointed out that the base price of the full featured isis model begins with 7680 per year for a single user license with an annual support and maintenance fee starting from 1350 considering the expense of model building and maintenance our freely available forecasting system could be operated as a complementary system to the rfmmc s forecasting especially in the md in our feasibility study we linearly interpolated jason 2 derived 10 day repeat tsl water levels and used them as nowcast of the tsl daily water levels however such interpolation cannot be done in real operational mode therefore for operational purpose up to about 10 day of forecasted water levels in the tsl need to be obtained until the next revisit of altimetry satellite they may be obtained by time series analysis methods such as autoregressive moving average model arma or autoregressive integrated moving average model arima alternatively the smooth and seasonality dominant pattern of the tsl water level changes might be well represented by a fourier series with known dominant frequencies by performing the fourier analysis upon historical tsl water levels for future study we would work on enhancing the forecasting skill and extending the lead time of our forecasting system with the aid of weather based flow forecasting at upstream sikder and hossain 2018 proposed an operationally feasible method called bias correction method to obtain quantitative precipitation forecasting up to 15 days and currently provides up to 7 day of weather based vic derived water discharge forecasting with promising accuracy see http depts washington edu saswe datavis timeseries html since our forecasting system used the vic derived discharges to reconstruct daily water levels at upstream vss the use of forecasted vic derived water discharges can give us forecasted reconstructed daily water levels which then can be used to perform forecast with extended lead times for example if the 7 day vic derived water discharge forecasting at an upstream vs is obtained the 7 day forecasted water level can be reconstructed by using the discharge to level rating curve this reconstructed water level forecasting with 7 day lead time can then be put into the forecasting model of 10 day lead time for example to forecast 17 day later downstream water levels at the locations of downstream in situ gauges although the accuracy of these forecasted water discharges are not as good as nowcasted water discharges as sikder and hossain 2018 indicated 0 77 0 57 of cor with 34 40 of normalized rmses nrmses with 1 7 days of lead times compared with 0 86 of cor and 24 9 of nrmse of nowcasted water discharges at kampong cham in the mr it is expected that the capacity of the forecasting system proposed in this study still has potential to be improved with extended lead times on the other hand local rainfall events at the forecasting locations also need to be addressed in the future as it may be an error source of our forecasting since currently our system is only based on the linear relationship between upstream and downstream water levels moreover it is expected that the enso induced strong water level oscillations in the md can be addressed if data with longer time span is available in the future to build more comprehensive tidal influence models 6 software and data availability the mekong river flood forecasting tool mr fft is developed with cooperation of department of civil and environmental engineering at university of houston department of civil and environmental engineering at university of washington and asian disaster preparedness center under the support of nasa s servir and geo program see acknowledgments as one of the major purposes of nasa servir and geo program the software is free and publicly open the software was originally coded using matlab a graphic user interface has been wrapped up as a package which can be run using freely accessible matlab runtime included in the package on a 64 bit microsoft windows operation system the prototype of the software with sample data which allows users to 1 reconstruct daily water levels at upstream vss 2 build forecasting model and 3 perform pseudo forecasting with user guide can be accessed through https drive google com open id 1lg5w8rw4urhzugroqhyjftsdkbf1vdqb acknowledgments this study was supported by nasa s servir program nnx16an35g and geo program 80nssc18k0423 we also thank the anonymous reviewers for their constructive comments 
26272,the availability of smart metered high resolution water consumption data introduces through the synchronous readings of the smart meters new perspectives in the proactive approach to the monitoring of water losses measuring or transmission systems problems are unavoidable in real world networks and if not appropriately addressed may compromise the ability to use smart meters to estimate water losses the proposed synchronous water balance methodology allows the near real time assessment of water losses taking into account incomplete readings through a water consumption data validation and reconstruction model the impact on water loss monitoring due to the lack of an increasing number of smart meters is investigated applying a random sampling and evaluating the corresponding error the results tested on a district of the city of fano italy suggest that the availability of near real time synchronous water consumption measures can substantially improve the assessment of water losses in comparison to traditional approaches keywords smart meters water loss water balance missing data synchronous readings water distribution network minimum night flow 1 introduction in a context of increasing water scarcity all over the world schewe et al 2014 water loss reduction in drinking water networks is a key objective in many countries to ensure the sustainable management of water resources kingdom et al 2006 in addition to denote the condition of the infrastructure water losses represent normally the biggest user of water distribution networks wdns and their estimation and control is therefore one of the key water demand management strategies according to the iwa standards alegre et al 2000 water losses are defined as the difference between the system input volume introduced in a water distribution network and the authorised consumption including exported water leaks and overflows after the point of customer metering the increase of water losses in wdns leads to intensify the water subtraction from the environment with an impact on the sustainability of water resources in future decades when water demand is expected to increase not only due to population growth but also to the other expected social economic and climatic changes oecd 2012 roy et al 2012 toth et al 2018 water losses in distribution systems may also cause health risks which may arise from the infiltration of pollutants into pipe systems under low pressure or intermittent supply from the water utility and consumers perspective in addition the losses correspond to high costs for exploiting treating and transporting water that is lost on its way to the customer kingdom et al 2006 oecd 2012 eea 2014 the assessment of water losses in water distribution networks is traditionally based on annual measurements of the consumptions the extended period over which water balance is normally estimated does not allow a prompt control of water losses furthermore the readings of water meters with a half yearly or annual frequency is normally collected for billing purposes and the operator physically reads water meters following a reading plan this introduces a temporal shift in the readings that obviously cannot be all contemporary consequently consumption volume must be adjusted to bring it back to a homogeneous period the other approach adopted by water utilities is the minimum night flow mnf puust et al 2010 in absence of high resolution measurements of the consumption the water losses are estimated as the part of the minimum night input flow that exceeds a predetermined threshold corresponding to an assessment of the night customer use covas et al 2006 mnf is characterized by a high resolution of network inflow data but it suffers by relevant uncertainty in the assumptions on the night time consumption wrc 1994 a smart meter is a meter that stores and transmits measurements at defined time intervals advancements of smart water meters in terms of accuracy and precision in measuring even low water flows and in terms of data communications technology have made it possible to record and transmit household water usage fine resolution data cominola et al 2015 gurung et al 2016 kulkarni and farnham 2016 this opens the possibility to have a near real time knowledge of water consumption data the characterization of the temporal variation of the water demand and finally an estimation of the different components of water balance gurung et al 2016 so far the literature analyzing the data collected by such systems has focused mainly on the development of urban water demand models representing a real revolution in the temporal variability knowledge of user consumptions cominola et al 2015 creaco et al 2015 2016 cardell oliver et al 2016 and cominola et al 2016 propose a method to identify groups of similar households based on their regular high magnitude behaviors of water consumption different approaches are based on the reconstruction of flow time series in water distribution systems quevedo et al 2010 barrela et al 2016 mazzolani et al 2017 taking into account the multiple seasonality using existing time series models barrela et al 2016 the process has led to models that are inevitably tailored to the available database characterized by a different conceptual approach and significantly different in the degree of detail in relation to the technology used sønderlund et al 2016 in fact the case studies from which these models are derived are distinguished by the water smart meter accuracy the acquisition frequency the synchronization time in the measurement and the data transmission mode kim et al 2014 kondratjevs et al 2014 depari et al 2016 advanced knowledge and understanding about smart metering technologies allow the development of novel robust practical and cost effective methodologies and tools to manage urban water demand in households harou et al 2014 savic et al 2014 cominola et al 2015 ribeiro et al 2015 few studies focus instead on systematic approaches to process and analyse water consumption data collected from telemetry systems with the purpose of improving network operation and maintenance mounce et al 2010 loureiro et al 2014 2015 the installation of smart meters cominola et al 2015 cardell oliver et al 2016 made available high resolution water consumption data at the consumer level in addition to this the binomial i segmentation in district meter areas of wdns and ii synchronous readings all smart meters read at the same time introduces new perspectives in the proactive approach to the monitoring of water losses the potentialities of near real time water losses monitoring have yet to be fully explored and the installation of water smart meters will probably proceed with times that are not entirely predictable patchy and with evolving technologies despite this right now it is possible to improve the current methodologies with the available results wu et al 2011 makropoulos et al 2014 kim et al 2014 gurung et al 2016 the availability of fine time scale water consumption data can inform demand models to be used to overcome the lack of direct measurement of water consumption due to either uncomplete spatial coverage of the network or to temporal gaps in the recordings also for water losses monitoring substantial improvements may be expected through the integration of the results of recently developed urban water demand models into water balance methods in particular in presence of water smart meters registering simultaneously in time a short term water balance is possible and will be considered in the proposed approach on the other hand when there is not an overall availability of simultaneous measures urban water demand models adequately calibrated over the specific case study may be applied to improve the short term water losses monitoring strategy information on the accuracy of the measurements and on the main socio demographic drivers may also be considered for characterizing and modelling users behavior to assess the non measured component of water consumption sønderlund et al 2016 the choice of the reading frequency of smart meters is another important decision for data collection in time dependent processes kulkarni and farnham 2016 in fact a fine temporal resolution of the data acquisition is desirable in order to obtain more information as possible at the same time a very high frequency leads to an increase in the cost and robustness of the system battery life bandwidth of the transmission vector potential bottlenecks in data transmission in addition one of the key requirements in the smart meter based water loss assessment is to give continuity also during the unavoidable events of failures in the communication with devices or in the devices themselves some precaution measures can be incorporated within the system architecture embodied both within devices for diagnosis as excessive battery discharge and also within the infrastructure for coordinated recovery action kulkarni and farnham 2016 the installation of smart water meters does not follow homogeneous or regulated criteria sometimes it may concern only the users with greater water consumption in other cases it could be not compulsory for users daiad 2014 therefore the condition in which all users are provided with smart water meters is not always achieved in the real world networks for this in a smart meters based water balance methodology the situation in which part of users is not metered must be considered fortunato et al 2015 another aspect is related to the presence of missing information in the readings of smart water meters and in the data gathering and transmission consequently reconstructing missing data of incomplete readings sets is an important step greatly facilitated by assimilating the periodicity in time that characterizes water consumption as done for example by loureiro et al 2014 who tested a method for the reconstruction of missing data in the total metered consumption based on consumption statistics of each consumer the present work is focused on smart meters able to collect synchronous readings the condition of synchronicity is a fundamental feature to apply smart metering for the dynamic water loss monitoring whereas for the development of water demand models the data transmission can be deferred in time the presented methodology section 2 herein defined as synchronous water balance swb allows the near real time assessment of water losses inside the water distribution network that is not focusing on the assessment of the post meter leakages internal losses that are considered for example in britton et al 2013 the outline adopted in the proposed approach is coherent with the iwa annual water balance approach alegre et al 2000 but it may be applied to measure water losses in near real time in addition the proposed approach overcomes some of the drawbacks of the minimum night flow method puust et al 2010 which was developed assuming to have no measurements of the system output consumption the method is therefore forced to consider only the night hours because at that time it is less difficult to estimate user consumption than in other times of the day synchronous smart readings remove this constraint and the time period in which the water balance may be estimated becomes flexible furthermore incomplete readings of smart meters are taken into account by means of a water consumption data validation and reconstruction model in fact the simultaneous reading of all the water meters installed makes the water balance a powerful operational tool in order to measure water losses in near real time the proposed approach has been tested in one district in the city of fano italy where an automatic meter reading network of water smart meters is registering continuously from 2006 to date in many cases the installation of smart meters derives from research projects as ict4water cluster projects and only recently the water utilities are achieving extensive installations daiad 2014 for this reason an observation period long as that available for the case study of fano is significant the smart meters reading interval adopted by the water utility is approximately monthly the proposed methodology is applicable also at such scale even if obviously this time sampling resolution does not allow a fine monitoring of the water losses which would require at least weekly data the results are presented in section 3 finally section 4 investigates the influence on the water loss assessment due to shortcomings in the data this situation has to be taken into account to make water balance usable in near real time for the water losses monitoring in the operational practice 2 assessment of water losses by synchronous water balance swb 2 1 water loss components water losses in water distribution networks can be divided into real and apparent losses following the classification given in the literature and specifically the terminology adopted by the iwa water balance alegre et al 2000 apparent losses are water volumes illegally subtracted from the network or water volume missing in metered consumption because of measurement inaccuracies in addition to losses along the distribution network other water losses may be due to post meter household leakages britton et al 2013 that can occur in any number of different plumbing fixtures or piping within a residential property downstream of the water meter a real water loss is a water volume that outflows through physical ineffectiveness and it is defined as background loss unless the flow rate is large enough to exceed a measurable and auditable threshold value in which case it is categorized as burst furthermore burst is called reported burst if it emerges on the surface and therefore it is not necessary to detect it because it is visible unreported burst if it runs until it is identified and repaired by active leakage control lambert 1994 real losses magnitude is affected by pressure variations and their activity is continuous for a time given by a sum of awareness location and reparation times wrc 1994 unreported bursts become particularly important when high values of flow rate and running time are associated to them the smart water metering can support the identification of the pipe burst time and detection of leakage farah and shahrour 2017 also combined with other techniques mounce et al 2010 romano et al 2012 laucelli et al 2016 it allows the quantitative measurement of the magnitude of the real losses and it may also help in identifying the apparent loss in the water distribution network or in post meter connections liserra et al 2010 britton et al 2013 loureiro et al 2014 the usefulness of near real time control of water losses is in reducing the awareness time that is the time interval from the start of the event to the time the water utility is aware that a new water loss exists lambert 1994 indeed it is possible to define alert criteria for instance unjustified increment of the daily volume measured in the inlets points of dmas for switching to a more frequent reading schedule in order to better monitor the presence of a new leak the application of the methodology described in the following allows to assess the current water losses and therefore to decide when an intervention in wdn is needed 2 2 synchronous water balance swb the proposed methodology considers a water distribution network wdn or a sector of wdn and allows to calculate the water balance on a variable time period assuming that the readings at each smart water meter are synchronous adopting the terminology proposed by iwa alegre et al 2000 the water losses wl t on a time period t can be evaluated according to eq 1 1 wl t siv t mc t umc t urc t where siv is the system input volume mc is the metered consumption billed and unbilled of users obtained by a set of n available and working smart meters umc is the unmetered consumption billed and unbilled due to the absence of smart meters for either costumers servicing water used for operation purpose by the water utility itself or free water supply not measured for some categories of consumers urc is the unread consumption due to smart meters that are temporarily not operating transmitting or due to measurement errors because both umc and urc have to be estimated the closer they are to zero the more reliable is the assessment of wl thus the accuracy of the water balance eq 1 is strictly dependent on i appropriate state of maintenance of smart meters technology and ii data validation and reconstruction of water consumption incomplete data due to not working smart meters or users without smart meter are normally present in real world networks and a methodology to take into account the lack of data or its uncertainty is necessary this evidence if not properly addressed can compromise the potentiality of smart meters technology to support the monitoring of water losses 2 3 water consumption data validation the framework of the algorithm for validation of the water consumption data in shown in fig 1 furthermore the metacode of the section is reported in appendix 1 the data acquisition frequencies of smart meters can be set depending on the operational needs in the analysis the reference is the housing unit that corresponds to a single house or to one flat of an apartment block because each smart meter can be connected to one housing unit hu for the single houses or to more housing units in case of apartment blocks equipped with a unique device it is convenient to refer the average daily consumption to each housing unit the average consumption hu i δ t per housing unit associated to the smart meter i in the interval of time δ t is obtained from the difference between the two consecutive readings l i t and l i t δ t with the following equation eq 2 2 a v e r a g e c o n s u m p t i o n h u i δ t l i t δ t l i t n h u i δ t where n hu i is the number of housing units of the smart meter i regardless of the reading time interval δ t that can vary adapting to operational requirements water consumption is brought back to the average daily value avg hu i j with j 1 d where d is the number of days of the readings time series therefore the allocation of synchronous measurements in the total inlet vector indd and in the consumption matrix cdd allows to analyse equi spaced daily data series cdd is thus the matrix of daily consumption data pre validation process it is necessary to distinguish the time interval δ t between two consecutive readings of the smart meters and the time horizon t over which computing the water balance for the latter the day may be a good compromise to assess water losses in a quasi real time perspective furthermore the behavior of water distribution networks is generally characterized by a diurnal cycle it was therefore here decided to use a daily time interval even if this is not mandatory for the application of the methodology data validation requires the identification of possible error measurements through the identification of outliers that may correspond to suspicious data that need to be checked considering the daily time series of synchronous readings for each smart meter i n where n is the number of smart meters the average consumption avg hu i d was computed by eq 3 3 a v g h u i d 1 d j d a v e r a g e c o n s u m p t i o n h u i j where d is the number of days of the readings time series in order to identify a suspicious data it is necessary to set a threshold it was here chosen to adopt a monthly varying threshold for each smart meter labelling a daily consumption avg hu i j as suspicious when its value is an outlier in comparison to the average value for the corresponding month avg hu i m that is when it falls outside the range defined by eq 4 4 a v g h u i d 2 s t d d e v h u i d a v g h u i d 2 s t d d e v h u i d where std avg hu i d is the standard deviation of the daily average consumption of each smart meter i n successively a validation procedure based on expert knowledge groups these labeled values in either i measurement errors ii post meter leakages internal losses or iii false alarms when the measure is judged to be reliable and in this last case the suspicious label is removed as a result the matrix of validated consumption cdd n x d at daily time step without measurement errors and internal losses is obtained setting aside the initial preparation of the data set it is expected that when the water utility will apply the methodology to monitor water losses in real time the data are checked as soon as they become available for example every day in order to verify if the monitoring network is working properly in this case the visualization system should identify for example with a color code the few meters where the measure exceeded the threshold in the previous hours so that the person in charge may assess what kind of outlier it is 2 4 water consumption reconstruction model the validated consumption data at daily time step cdd are then used to estimate the average monthly consumption m3 hu day of all the smart meters belonging to the same group apartment blocks or single houses using all the available data in the time series for that month the framework of the algorithm for the reconstruction of the water consumption in shown in fig 1 the average monthly consumption computed over all the meters belonging to the same typology for example single houses or apartment blocks is the value assigned to unmetered consumption umc for the users without smart meters for water uses different from those represented by the metered consumption such as fire flows pipe washing etc the average monthly consumption can be assessed using data from measures obtained in similar contexts possibly other districts of the same city or from literature references vermersch et al 2016 unread consumption urc consisting in either a missing value or a measurement error for each smart meter i n is also estimated using cdd in this case the model is based on the average monthly consumption m3 day of the same meter i calculated with its own available data for that month excluding internal losses along the time series of readings finally the reconstructed water consumption matrix cr n x d is obtained in which urc and umc are reconstructed as described above and the values labeled as post meter leakages are included metacode of the section is shown in appendix 2 2 5 water losses evaluation for a wdn or for one of its sector districts the water loss wl for each day j within the time series is finally evaluated by eq 5 5 w l j i n d d j c t o t r j w i t h c t o t r j i 1 n c r i j where indd j and cr tot are respectively the daily inlet volume and the total water consumption volume in day j and c r i j is the reconstructed consumption cr for the smart meter i at day j for each day j a water loss estimate wl j obtained by eq 5 is considered unreliable when its value is lower than a percentage threshold of the system input volume since some losses are always expected in any real world system the threshold may be set depending on the specific case study as a function of the infrastructure and operational conditions metacode of the section is shown in appendix 3 3 case study the water distribution network of fano the case study refers the city of fano on the northern part of the adriatic coast in central italy with a population of 60000 residents and a density of 462 residents km2 the regional climate is affected by the presence of the adriatic sea to the east and of the apennines mountains chain to the west the mean annual temperature is 13 c july and august are the months with highest temperatures while the lowest ones are generally recorded in january in terms of precipitation the coastal band receives mean precipitation values ranging from 600 to 850 mm year appiotti et al 2014 automatic meter reading amr system started in 2006 in parallel with the creation of district meter areas dma in the wdn proceeding with the installation of smart meters for all the users in order to get measurements homogeneous in respect to the water meter aging all the water meters were replaced amr system installed in fano collects synchronous data directly from smart water meters and delivers them to the central control unit fig 2 moreover it allows acquiring the data by rmr remote meter reading mobile devices that could be used to collect data from the local memory units or in amr setup phases the amr couples standard water meters with optoelectronic impulse devices incoming data are redirected to the concentrator which automatically performs data collection functions from local units and programs them according to the parameters received by the central control unit fig 2 amr system can interrogate the smart water meters at any time and therefore the acquisition of synchronous readings is flexible and adaptable to operational needs concerning the measurement errors it should be noted that they could be due to the accuracy of the meter fontanazza et al 2012 but not in the present case to the round off error imposed by the internal resolution of the impulses emitter in fact the time interval δt between two consecutive readings is variable but always higher than few days and this makes the round off error negligible liserra et al 2010 3 1 water consumption analysis the proposed methodology has been applied to a dmas operating inside the wdn fig 3 all the private connections to wdn are equipped with water smart meters the users are all of residential type and have been distinguished between single houses and apartment blocks in an apartment block a unique smart meter is installed for all the housing units since all the residential users are equipped with smart meters the unmetered consumption umc related to the users is null table 1 points out the main characteristic of the dma with regards to other components of umc street cleaning is carried out with vehicles not supplied with water within the district the hydrants do not have smart meters but no fires have been detected in the observation period some washings of the pipes may have occurred especially during the repairs following to the breakages but detailed data are not available and such uses were neglected housing units have been chosen as reference for the water consumption analysis so to homogenize the readings made for entire apartments blocks with those for single houses the interval between recording day is circa monthly depending on the specific operational necessities that currently are billing consumption and water loss monitoring for this second application a criticality is due to the incomplete readings in some smart meters that may be temporarily unavailable an usual situation in a system with such a high density of equipment throughout the urban area the algorithm see sections 2 3 and 2 4 for validating and reconstructing the water consumption data has been applied to the time series 02 12 2008 03 03 2013 of smart meter readings available for the district meter area shown in fig 3 the first years 2006 2008 are characterized by long periods in which the amr system did not operate due to data transmission problems in the present study when validating the entire data set a manual validation was not too onerous since the data refer to only one district and a limited number of meters if a larger number of meters had been available a manual validation would not have been convenient and we would have devised an automatic procedure in the present case the number of meters is not sufficient to represent a statistically significant sample to propose a reliable automatic procedure for deciding if the outliers are to be considered correct measurement errors or internal losses fig 4 shows the comparison between the water consumption per housing unit m3 day hu before and after the data validation and reconstruction for single houses and apartment blocks the difference between the measured and reconstructed lines for apartment blocks is lower compared to that of single houses this indicates how the smart meters installed in the apartment blocks are more reliable over time than those installed in single houses fig 5 shows the variability in the different seasons of average daily consumption for apartment blocks and single house housing unit after the validation reconstruction procedure cr the boxplots clearly highlight the higher scattering that characterizes the water consumption of single houses in comparison to the housing units of the apartment blocks monthly seasonality of average daily consumption per housing unit m3 day hu in single house blue and apartment block red is exhibited in fig 6 in terms of ratio between the monthly consumption and the yearly average its variation is between 0 409 and 0 547 m3 day hu with an average value of 0 456 m3 day hu in single houses and between 0 353 and 0 385 m3 day hu with an average value of 0 367 m3 day hu in apartment blocks for the observation period the monthly consumption of single houses is higher in comparison to apartment blocks and is characterized by greater seasonal variability this is coherent with the presence of gardens for single houses as it is possible to observe in fig 3 which increases the outdoor water use during the summer for apartment blocks the green area is normally scarce and thus the outdoor consumption is much lower in comparison to the indoor water use for the case study the total private green area belongs for 20 to the apartment blocks and for 80 to the single houses this confirms the need in the analysis to separate apartments blocks from single houses the investigation of the influence of social economic and climatic parameters on the water consumption requires a specific analysis like in toth et al 2018 it was not investigated here because it is not the focus of the present work but it may be a future development 3 2 assessment of water losses by swb the methodology see section 2 5 for the assessment of the water losses has been tested on the district meter area fig 3 metered consumption mc is obtained by a group of n 57 smart meters as indicated in table 1 the assessment of the daily water losses wl is obtained applying the daily synchronous water balance eq 1 over the considered time series d 1553 days from 02 12 2008 to 03 03 2013 fig 7 shows the water loss in percentage and absolute value m3 day the assessment of water loss is considered unreliable when water loss m3 day is negative or water loss percentage is lower than 10 of the system input volume dotted line in fig 7a such relative high value for the threshold chosen in order to identify unbehavioural losses was suggested by the water utility for the case study of fano in fig 7b it is possible to observe the time periods when the information on water losses is considered unreliable such period relatively long in respect to the total length of the available time series 39 may be attributed to the problem to the malfunctioning of the meters on the inlet points to prevent such problems an asset maintenance plan for smart meters and a robust warning system signaling the presence of temporary lack of measurements or anomalies in the data transmission should be foreseen if possible already in the smart meters system design phase the error that may have been introduced over the validated water losses data by the reconstruction process ranges between 0 and 9 1 of the total consumption with an average of 4 7 the average water loss for the reliable data a total of d 947 days shown in blue line in fig 7b is equal to 92 21 m3 day st dev 86 08 m3 day min 20 18 m3 day max 340 97 m3 day in terms of percentage of the lost water volume over the systems input volume the average water loss corresponds to 30 06 st dev 17 36 min 10 08 max 68 15 the unreliable time periods are removed from the data used in the following analyses on the assessment of the water losses thus considering only the days corresponding to the blue line in fig 7b 3 3 comparison with minimum night flow method in the assessment of water losses from the bottom up minimum night flow mnf approach puust et al 2010 the water losses are estimated as the part of mnf that exceeds a predetermined threshold corresponding to an assessment of the customer night use cnu covas et al 2006 as above said mnf is characterized by a high temporal resolution of network inflow data but the estimate of the cnu is affected by a very large uncertainty wrc 1994 proposes a refer value for the cnu equal to 1 7 l hu h recent literature models have tried to improve the efficiency of the mnf method mainly trying to estimate water losses in district meter areas based only on inlet flow data mazzolani et al 2017 or using a multiple linear regression to determine the main factors that contribute to mnf alkasseh et al 2013 but there have not yet been significant improvements in the estimate of cnu with reference to the fano case study the utility provided additional smart water readings referring to night flows and corresponding to a set of two night readings in the same dates at 2 00 a m and 5 00 a m respectively these measures are not available every day but in a discontinuous set of days along the observation period such data have not been used in the herein proposed swb method where the water balance is not limited to the night period when the uncertainties in the measurements are the largest due to the small values of the flows but have allowed a reliable cnu estimate for the present case study according to the night measurements we found an average cnu between 2 00am 5 00am equal to 2 69 l hu h this value even if not too different from the literature reference 1 7 l hu h shows the importance of having actual consumption measurements also for the application of the classic mnf method the water losses identified by the mnf method were reported to daily values and are compared in fig 8 with the assessment obtained with the swb approach the mnf measures are generally available once per month for the same night of the monthly reading used in the swb approach the mnf estimates refer to a single daily flow whereas the swb balance is circa monthly it may thus occur that the daily mnf values dots are in some cases much higher than the corresponding monthly averaged swb volumes solid line overall leaving aside the peaks due to the different time samplings the results show that the two estimates are in good agreement and allow a validation of the proposed procedure through the comparison with a traditional approach 3 4 impact on water loss assessment due to the temporary loss of smart meter data reading this section investigates the influence on the water loss estimate due to the lack of one or more smart meters this situation that can often occur in the real contexts with high number of smart meters and transmitting devices determines shortcoming in the data and thus it does not allow to directly use them to assess water balance in near real time for the water losses monitoring the basic idea is to overtake this gap estimating the expected water consumption data by relying on the seasonality that characterizes water consumption unavailable data are reconstructed as unmetered consumption umc as described in section 2 4 3 5 impact assessment metric statistical sampling fuller 2009 is used to obtain subsets samples of the working smart meters to estimate the impact in the assessment of water losses due to the temporary lack of k smart meters belonging to the population n of smart meters k n a version of the stratified random sampling is herein used fortunato et al 2015 fuller 2009 in which the population is partitioned into non overlapping groups called strata when measurements within strata are sufficiently homogeneous stratification improves the representativeness of the sample by reducing sampling error a simple random sampling in which all members of the population have an equal and independent chance of being selected is applied the most important dissimilarity in the users is here considered as the only stratification and it concerns the grouping of smart meters installed in apartment blocks n ab and in single houses n sh due to the differences in consumption for the presence of green outdoor areas for a given k a set of possible configurations with k missing smart meters is identified for each configuration x the model proposed in section 2 4 is used to reconstruct the unmeasured consumption umc of the k unavailable smart meters and the daily water loss wl k x j with j 1 d is evaluated applying the synchronous water balance wb eq 5 the impact of the lack of k smart meter for each x is then estimated with a comparison between wl k x j and the daily water loss wl 0 j obtained in fact assuming that all smart meters are operating k 0 computing the corresponding rmse k x over the d days eq 6 6 r m s e k x j 1 d w l k x j w l 0 j 2 d the metacode of the section is shown in appendix 4 and the procedure is exemplified in fig 9 3 6 impact of the lack of an increasing number of smart meters for the case study of fano initially the impact of the two strata apartment block and single house is analyzed independently in the assessment of the impact due to the lack of smart meters installed in apartment blocks it is assumed that all the smart meters belonging to the strata single house with n sh 35 are properly working and thus all the water information on their consumption is available it was then supposed that inside the strata apartment block with n ab 22 a number k of smart meters is in turn unavailable with 0 k n ab for each k 10000 random samples x of n ab k measuring smart meter were generated and the corresponding rmse k x calculated according to eq 6 when considering the lack of smart meters installed in single houses the procedure is the same carried out with respect to the apartment blocks in this case it is assumed that all the smart meters belonging to the strata apartment blocks are properly working inside the strata single houses with n sh 35 a given number of smart meters k is in turn considered unavailable with 0 k n sh for each k 10000 random samples of n sh k measuring smart meter are generated and rmse k x calculated according to eq 6 subsequently for each k the average rmse k of the rmse k x random samples is evaluated and the result is shown in fig 10 the maximum value of the standard deviation of the set of rmse k is equal to 1 90 m3 day for the apartment blocks k 1 22 and for the single houses 0 33 m3 day k 1 35 the last points of the two lines represent the rmse of the wl estimate when all the 22 smart meters of the apartment blocks are unavailable k 22 for the red line with a rmse equal to 7 06 m3 day and when all the 35 smart meters of the single houses are unavailable k 35 for the block line with an rmse equal to 0 88 m3 day considering a condition with 50 of smart meters working average rmse becomes equal to 0 95 m3 day for single houses k 18 and 5 11 m3 day for apartment blocks k 11 when apartment blocks meters are missing red line in fig 10 there is a strong deterioration of the estimate whereas the impact of the not availability of single houses on the assessment of water losses is much lower this is due to the fact that each missing smart meter in apartment blocks corresponds to a set of 8 42 housing units and its impact is therefore much larger 3 7 discontinuity or absence of an automatic meter reading system the analysis reported in section 4 2 allows also a comparison towards a baseline representing the lack of all smart meters i e all the meters of the apartments blocks and of the single houses are lacking in this case the rmse of the wl results equal to 7 34 m3 day this value would represent the average error resulting when the amr system is discontinued in a district that was previously metered for a period of time sufficient to characterize its consumptions such period of fine metering allows for the period following the discontinuity a data reconstruction based on past fine time scale readings urc conditions such a situation may occur if the utility decides not to invest any more on the amr technology for example due to the obsolescence of the devices or to the costs for replacing the batteries on the other hand the large majority of the water distribution networks districts have never been equipped with an amr system not even for a limited period of time in such conditions only annual readings of water consumption are available for each meter daily consumption is generally estimated assuming a value constant over the year and equal to the average of the annual reading in the meter itself applying eq 6 under this assumption for the case study of fano the rmse of the water loss estimate amounts to 9 39 m3 day comparing such value with the one obtainable in the case where the existing amr system is discontinued 7 34 m3 day it is shown that even in the case in which the amr system is not working anymore a more reliable characterization of the demand is allowed by its historical implementation such result also suggests that if an utility has availability of only a limited number of portable smart meters not enough for covering all the costumers it may be useful to monitor in turn the different districts in this case even if a real time swb based on actual demand would not be possible after the metering period the collected data would allow to characterize the expected consumption providing information for a more reliable estimate of the water loss 4 conclusion and future research smart meters or automatic meter reading amr systems open to a new near real time knowledge of water consumption the high resolution in its characterization can improve the assessment and reduction of water losses in water distribution networks the attention is herein focalized on the water losses assessment in water distribution networks in which amr systems allows to set readings at the same time synchronism for all the meters the proposed synchronous water balance swb methodology evaluates the water loss on suitable time intervals whose temporal extension can vary adapting to operational requirements differently from the minimum night flow procedure the swb approach does not require night readings that is the period when low water consumption values increase the possibility of measurement errors for a near real time control of water losses the day or multiples of days are sufficient for the swb approach changing the reading acquisition from the night hours to the entire day the probability of reading errors associated to the minimum consumption that in case of residential users is indeed in the night hours is reduced the proposed methodology takes into account the fact that a part of users may not be covered by the measurements and the presence of missing information in the readings of the smart water meters and or in the gathering and transmission of the data a simple data reconstruction algorithm is developed inside the swb methodology assimilating the seasonality that characterizes water consumption the impact on water loss monitoring due to the unavailability of an increasing number of smart meters is then investigated applying a random sampling and evaluating the corresponding error the advantages allowed by a period of fine readings even in the case when the amr system is then discontinued are also presented the length of the measurement period 4 25 years in total as well as the presence of irregular intervals between the readings and the fact that their frequency is circa one per month is a limitation of the case study for near real time water loss assessment a daily or at least weekly reading would be much preferable nevertheless the application to a district of the city of fano italy shows that even a low resolution reading of water consumption can substantially improve the assessment of water losses in addition the analysis highlights that in real world systems the time periods with not reliable information in raw data can be long if compared to the total length of the available time series for the case study of fano the percentage of days in which the water loss is lower than the behavioral threshold of the inlet volume is 39 such situation is far from optimal for real time water loss monitoring and may severely compromise the ability to use smart meters for this purpose furthermore the size of the study area with an overall limited number of smart meters affects the possibility of grouping the set into more homogeneous clusters for improving both the water reconstruction model for unmetered consumption and the sampling procedure for the case study of fano only the most important dissimilarity in the users typology i e apartment blocks versus single houses is used as unique stratification for water supply networks with a larger numbers of smart meters other criteria can be introduced to define the sampling strata i e the number of housing units per apartment block the average water consumption the extension of green areas but also other information might be collected such as for example the number of family members that is not available in the present case study due to privacy reasons provided that more information on the users is available future developments of the proposed methodology might also focus on the improvement in the characterization of user s water consumption which could take advantage from the extensive recent research findings cominola et al 2016 cheifetz et al 2017 software and data availability the analyses have been carried out with scripts developed within the r project free software environmental for statistical computing using the packages xts author jeffrey a ryan joshua m ulrich and hydrogof author mauricio zambrano bigiarini available from the cran comprehensive r archive network platform https cran r project org mirrors html and the scripts are available upon request to the first author the data on inlet flow and water consumptions are not public and have been provided by aset spa acknowledgments the authors are grateful to aset spa for the availability in providing the data in particular to dr danilo galeri for the technical support appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 010 appendix metacode appendix 1 data validation image 1 appendix 2 water consumption reconstruction model image 2 appendix 3 actual water losses evaluation image 3 appendix 4 impact on water loss assessment due to the temporary lack of smart meters image 4 
26272,the availability of smart metered high resolution water consumption data introduces through the synchronous readings of the smart meters new perspectives in the proactive approach to the monitoring of water losses measuring or transmission systems problems are unavoidable in real world networks and if not appropriately addressed may compromise the ability to use smart meters to estimate water losses the proposed synchronous water balance methodology allows the near real time assessment of water losses taking into account incomplete readings through a water consumption data validation and reconstruction model the impact on water loss monitoring due to the lack of an increasing number of smart meters is investigated applying a random sampling and evaluating the corresponding error the results tested on a district of the city of fano italy suggest that the availability of near real time synchronous water consumption measures can substantially improve the assessment of water losses in comparison to traditional approaches keywords smart meters water loss water balance missing data synchronous readings water distribution network minimum night flow 1 introduction in a context of increasing water scarcity all over the world schewe et al 2014 water loss reduction in drinking water networks is a key objective in many countries to ensure the sustainable management of water resources kingdom et al 2006 in addition to denote the condition of the infrastructure water losses represent normally the biggest user of water distribution networks wdns and their estimation and control is therefore one of the key water demand management strategies according to the iwa standards alegre et al 2000 water losses are defined as the difference between the system input volume introduced in a water distribution network and the authorised consumption including exported water leaks and overflows after the point of customer metering the increase of water losses in wdns leads to intensify the water subtraction from the environment with an impact on the sustainability of water resources in future decades when water demand is expected to increase not only due to population growth but also to the other expected social economic and climatic changes oecd 2012 roy et al 2012 toth et al 2018 water losses in distribution systems may also cause health risks which may arise from the infiltration of pollutants into pipe systems under low pressure or intermittent supply from the water utility and consumers perspective in addition the losses correspond to high costs for exploiting treating and transporting water that is lost on its way to the customer kingdom et al 2006 oecd 2012 eea 2014 the assessment of water losses in water distribution networks is traditionally based on annual measurements of the consumptions the extended period over which water balance is normally estimated does not allow a prompt control of water losses furthermore the readings of water meters with a half yearly or annual frequency is normally collected for billing purposes and the operator physically reads water meters following a reading plan this introduces a temporal shift in the readings that obviously cannot be all contemporary consequently consumption volume must be adjusted to bring it back to a homogeneous period the other approach adopted by water utilities is the minimum night flow mnf puust et al 2010 in absence of high resolution measurements of the consumption the water losses are estimated as the part of the minimum night input flow that exceeds a predetermined threshold corresponding to an assessment of the night customer use covas et al 2006 mnf is characterized by a high resolution of network inflow data but it suffers by relevant uncertainty in the assumptions on the night time consumption wrc 1994 a smart meter is a meter that stores and transmits measurements at defined time intervals advancements of smart water meters in terms of accuracy and precision in measuring even low water flows and in terms of data communications technology have made it possible to record and transmit household water usage fine resolution data cominola et al 2015 gurung et al 2016 kulkarni and farnham 2016 this opens the possibility to have a near real time knowledge of water consumption data the characterization of the temporal variation of the water demand and finally an estimation of the different components of water balance gurung et al 2016 so far the literature analyzing the data collected by such systems has focused mainly on the development of urban water demand models representing a real revolution in the temporal variability knowledge of user consumptions cominola et al 2015 creaco et al 2015 2016 cardell oliver et al 2016 and cominola et al 2016 propose a method to identify groups of similar households based on their regular high magnitude behaviors of water consumption different approaches are based on the reconstruction of flow time series in water distribution systems quevedo et al 2010 barrela et al 2016 mazzolani et al 2017 taking into account the multiple seasonality using existing time series models barrela et al 2016 the process has led to models that are inevitably tailored to the available database characterized by a different conceptual approach and significantly different in the degree of detail in relation to the technology used sønderlund et al 2016 in fact the case studies from which these models are derived are distinguished by the water smart meter accuracy the acquisition frequency the synchronization time in the measurement and the data transmission mode kim et al 2014 kondratjevs et al 2014 depari et al 2016 advanced knowledge and understanding about smart metering technologies allow the development of novel robust practical and cost effective methodologies and tools to manage urban water demand in households harou et al 2014 savic et al 2014 cominola et al 2015 ribeiro et al 2015 few studies focus instead on systematic approaches to process and analyse water consumption data collected from telemetry systems with the purpose of improving network operation and maintenance mounce et al 2010 loureiro et al 2014 2015 the installation of smart meters cominola et al 2015 cardell oliver et al 2016 made available high resolution water consumption data at the consumer level in addition to this the binomial i segmentation in district meter areas of wdns and ii synchronous readings all smart meters read at the same time introduces new perspectives in the proactive approach to the monitoring of water losses the potentialities of near real time water losses monitoring have yet to be fully explored and the installation of water smart meters will probably proceed with times that are not entirely predictable patchy and with evolving technologies despite this right now it is possible to improve the current methodologies with the available results wu et al 2011 makropoulos et al 2014 kim et al 2014 gurung et al 2016 the availability of fine time scale water consumption data can inform demand models to be used to overcome the lack of direct measurement of water consumption due to either uncomplete spatial coverage of the network or to temporal gaps in the recordings also for water losses monitoring substantial improvements may be expected through the integration of the results of recently developed urban water demand models into water balance methods in particular in presence of water smart meters registering simultaneously in time a short term water balance is possible and will be considered in the proposed approach on the other hand when there is not an overall availability of simultaneous measures urban water demand models adequately calibrated over the specific case study may be applied to improve the short term water losses monitoring strategy information on the accuracy of the measurements and on the main socio demographic drivers may also be considered for characterizing and modelling users behavior to assess the non measured component of water consumption sønderlund et al 2016 the choice of the reading frequency of smart meters is another important decision for data collection in time dependent processes kulkarni and farnham 2016 in fact a fine temporal resolution of the data acquisition is desirable in order to obtain more information as possible at the same time a very high frequency leads to an increase in the cost and robustness of the system battery life bandwidth of the transmission vector potential bottlenecks in data transmission in addition one of the key requirements in the smart meter based water loss assessment is to give continuity also during the unavoidable events of failures in the communication with devices or in the devices themselves some precaution measures can be incorporated within the system architecture embodied both within devices for diagnosis as excessive battery discharge and also within the infrastructure for coordinated recovery action kulkarni and farnham 2016 the installation of smart water meters does not follow homogeneous or regulated criteria sometimes it may concern only the users with greater water consumption in other cases it could be not compulsory for users daiad 2014 therefore the condition in which all users are provided with smart water meters is not always achieved in the real world networks for this in a smart meters based water balance methodology the situation in which part of users is not metered must be considered fortunato et al 2015 another aspect is related to the presence of missing information in the readings of smart water meters and in the data gathering and transmission consequently reconstructing missing data of incomplete readings sets is an important step greatly facilitated by assimilating the periodicity in time that characterizes water consumption as done for example by loureiro et al 2014 who tested a method for the reconstruction of missing data in the total metered consumption based on consumption statistics of each consumer the present work is focused on smart meters able to collect synchronous readings the condition of synchronicity is a fundamental feature to apply smart metering for the dynamic water loss monitoring whereas for the development of water demand models the data transmission can be deferred in time the presented methodology section 2 herein defined as synchronous water balance swb allows the near real time assessment of water losses inside the water distribution network that is not focusing on the assessment of the post meter leakages internal losses that are considered for example in britton et al 2013 the outline adopted in the proposed approach is coherent with the iwa annual water balance approach alegre et al 2000 but it may be applied to measure water losses in near real time in addition the proposed approach overcomes some of the drawbacks of the minimum night flow method puust et al 2010 which was developed assuming to have no measurements of the system output consumption the method is therefore forced to consider only the night hours because at that time it is less difficult to estimate user consumption than in other times of the day synchronous smart readings remove this constraint and the time period in which the water balance may be estimated becomes flexible furthermore incomplete readings of smart meters are taken into account by means of a water consumption data validation and reconstruction model in fact the simultaneous reading of all the water meters installed makes the water balance a powerful operational tool in order to measure water losses in near real time the proposed approach has been tested in one district in the city of fano italy where an automatic meter reading network of water smart meters is registering continuously from 2006 to date in many cases the installation of smart meters derives from research projects as ict4water cluster projects and only recently the water utilities are achieving extensive installations daiad 2014 for this reason an observation period long as that available for the case study of fano is significant the smart meters reading interval adopted by the water utility is approximately monthly the proposed methodology is applicable also at such scale even if obviously this time sampling resolution does not allow a fine monitoring of the water losses which would require at least weekly data the results are presented in section 3 finally section 4 investigates the influence on the water loss assessment due to shortcomings in the data this situation has to be taken into account to make water balance usable in near real time for the water losses monitoring in the operational practice 2 assessment of water losses by synchronous water balance swb 2 1 water loss components water losses in water distribution networks can be divided into real and apparent losses following the classification given in the literature and specifically the terminology adopted by the iwa water balance alegre et al 2000 apparent losses are water volumes illegally subtracted from the network or water volume missing in metered consumption because of measurement inaccuracies in addition to losses along the distribution network other water losses may be due to post meter household leakages britton et al 2013 that can occur in any number of different plumbing fixtures or piping within a residential property downstream of the water meter a real water loss is a water volume that outflows through physical ineffectiveness and it is defined as background loss unless the flow rate is large enough to exceed a measurable and auditable threshold value in which case it is categorized as burst furthermore burst is called reported burst if it emerges on the surface and therefore it is not necessary to detect it because it is visible unreported burst if it runs until it is identified and repaired by active leakage control lambert 1994 real losses magnitude is affected by pressure variations and their activity is continuous for a time given by a sum of awareness location and reparation times wrc 1994 unreported bursts become particularly important when high values of flow rate and running time are associated to them the smart water metering can support the identification of the pipe burst time and detection of leakage farah and shahrour 2017 also combined with other techniques mounce et al 2010 romano et al 2012 laucelli et al 2016 it allows the quantitative measurement of the magnitude of the real losses and it may also help in identifying the apparent loss in the water distribution network or in post meter connections liserra et al 2010 britton et al 2013 loureiro et al 2014 the usefulness of near real time control of water losses is in reducing the awareness time that is the time interval from the start of the event to the time the water utility is aware that a new water loss exists lambert 1994 indeed it is possible to define alert criteria for instance unjustified increment of the daily volume measured in the inlets points of dmas for switching to a more frequent reading schedule in order to better monitor the presence of a new leak the application of the methodology described in the following allows to assess the current water losses and therefore to decide when an intervention in wdn is needed 2 2 synchronous water balance swb the proposed methodology considers a water distribution network wdn or a sector of wdn and allows to calculate the water balance on a variable time period assuming that the readings at each smart water meter are synchronous adopting the terminology proposed by iwa alegre et al 2000 the water losses wl t on a time period t can be evaluated according to eq 1 1 wl t siv t mc t umc t urc t where siv is the system input volume mc is the metered consumption billed and unbilled of users obtained by a set of n available and working smart meters umc is the unmetered consumption billed and unbilled due to the absence of smart meters for either costumers servicing water used for operation purpose by the water utility itself or free water supply not measured for some categories of consumers urc is the unread consumption due to smart meters that are temporarily not operating transmitting or due to measurement errors because both umc and urc have to be estimated the closer they are to zero the more reliable is the assessment of wl thus the accuracy of the water balance eq 1 is strictly dependent on i appropriate state of maintenance of smart meters technology and ii data validation and reconstruction of water consumption incomplete data due to not working smart meters or users without smart meter are normally present in real world networks and a methodology to take into account the lack of data or its uncertainty is necessary this evidence if not properly addressed can compromise the potentiality of smart meters technology to support the monitoring of water losses 2 3 water consumption data validation the framework of the algorithm for validation of the water consumption data in shown in fig 1 furthermore the metacode of the section is reported in appendix 1 the data acquisition frequencies of smart meters can be set depending on the operational needs in the analysis the reference is the housing unit that corresponds to a single house or to one flat of an apartment block because each smart meter can be connected to one housing unit hu for the single houses or to more housing units in case of apartment blocks equipped with a unique device it is convenient to refer the average daily consumption to each housing unit the average consumption hu i δ t per housing unit associated to the smart meter i in the interval of time δ t is obtained from the difference between the two consecutive readings l i t and l i t δ t with the following equation eq 2 2 a v e r a g e c o n s u m p t i o n h u i δ t l i t δ t l i t n h u i δ t where n hu i is the number of housing units of the smart meter i regardless of the reading time interval δ t that can vary adapting to operational requirements water consumption is brought back to the average daily value avg hu i j with j 1 d where d is the number of days of the readings time series therefore the allocation of synchronous measurements in the total inlet vector indd and in the consumption matrix cdd allows to analyse equi spaced daily data series cdd is thus the matrix of daily consumption data pre validation process it is necessary to distinguish the time interval δ t between two consecutive readings of the smart meters and the time horizon t over which computing the water balance for the latter the day may be a good compromise to assess water losses in a quasi real time perspective furthermore the behavior of water distribution networks is generally characterized by a diurnal cycle it was therefore here decided to use a daily time interval even if this is not mandatory for the application of the methodology data validation requires the identification of possible error measurements through the identification of outliers that may correspond to suspicious data that need to be checked considering the daily time series of synchronous readings for each smart meter i n where n is the number of smart meters the average consumption avg hu i d was computed by eq 3 3 a v g h u i d 1 d j d a v e r a g e c o n s u m p t i o n h u i j where d is the number of days of the readings time series in order to identify a suspicious data it is necessary to set a threshold it was here chosen to adopt a monthly varying threshold for each smart meter labelling a daily consumption avg hu i j as suspicious when its value is an outlier in comparison to the average value for the corresponding month avg hu i m that is when it falls outside the range defined by eq 4 4 a v g h u i d 2 s t d d e v h u i d a v g h u i d 2 s t d d e v h u i d where std avg hu i d is the standard deviation of the daily average consumption of each smart meter i n successively a validation procedure based on expert knowledge groups these labeled values in either i measurement errors ii post meter leakages internal losses or iii false alarms when the measure is judged to be reliable and in this last case the suspicious label is removed as a result the matrix of validated consumption cdd n x d at daily time step without measurement errors and internal losses is obtained setting aside the initial preparation of the data set it is expected that when the water utility will apply the methodology to monitor water losses in real time the data are checked as soon as they become available for example every day in order to verify if the monitoring network is working properly in this case the visualization system should identify for example with a color code the few meters where the measure exceeded the threshold in the previous hours so that the person in charge may assess what kind of outlier it is 2 4 water consumption reconstruction model the validated consumption data at daily time step cdd are then used to estimate the average monthly consumption m3 hu day of all the smart meters belonging to the same group apartment blocks or single houses using all the available data in the time series for that month the framework of the algorithm for the reconstruction of the water consumption in shown in fig 1 the average monthly consumption computed over all the meters belonging to the same typology for example single houses or apartment blocks is the value assigned to unmetered consumption umc for the users without smart meters for water uses different from those represented by the metered consumption such as fire flows pipe washing etc the average monthly consumption can be assessed using data from measures obtained in similar contexts possibly other districts of the same city or from literature references vermersch et al 2016 unread consumption urc consisting in either a missing value or a measurement error for each smart meter i n is also estimated using cdd in this case the model is based on the average monthly consumption m3 day of the same meter i calculated with its own available data for that month excluding internal losses along the time series of readings finally the reconstructed water consumption matrix cr n x d is obtained in which urc and umc are reconstructed as described above and the values labeled as post meter leakages are included metacode of the section is shown in appendix 2 2 5 water losses evaluation for a wdn or for one of its sector districts the water loss wl for each day j within the time series is finally evaluated by eq 5 5 w l j i n d d j c t o t r j w i t h c t o t r j i 1 n c r i j where indd j and cr tot are respectively the daily inlet volume and the total water consumption volume in day j and c r i j is the reconstructed consumption cr for the smart meter i at day j for each day j a water loss estimate wl j obtained by eq 5 is considered unreliable when its value is lower than a percentage threshold of the system input volume since some losses are always expected in any real world system the threshold may be set depending on the specific case study as a function of the infrastructure and operational conditions metacode of the section is shown in appendix 3 3 case study the water distribution network of fano the case study refers the city of fano on the northern part of the adriatic coast in central italy with a population of 60000 residents and a density of 462 residents km2 the regional climate is affected by the presence of the adriatic sea to the east and of the apennines mountains chain to the west the mean annual temperature is 13 c july and august are the months with highest temperatures while the lowest ones are generally recorded in january in terms of precipitation the coastal band receives mean precipitation values ranging from 600 to 850 mm year appiotti et al 2014 automatic meter reading amr system started in 2006 in parallel with the creation of district meter areas dma in the wdn proceeding with the installation of smart meters for all the users in order to get measurements homogeneous in respect to the water meter aging all the water meters were replaced amr system installed in fano collects synchronous data directly from smart water meters and delivers them to the central control unit fig 2 moreover it allows acquiring the data by rmr remote meter reading mobile devices that could be used to collect data from the local memory units or in amr setup phases the amr couples standard water meters with optoelectronic impulse devices incoming data are redirected to the concentrator which automatically performs data collection functions from local units and programs them according to the parameters received by the central control unit fig 2 amr system can interrogate the smart water meters at any time and therefore the acquisition of synchronous readings is flexible and adaptable to operational needs concerning the measurement errors it should be noted that they could be due to the accuracy of the meter fontanazza et al 2012 but not in the present case to the round off error imposed by the internal resolution of the impulses emitter in fact the time interval δt between two consecutive readings is variable but always higher than few days and this makes the round off error negligible liserra et al 2010 3 1 water consumption analysis the proposed methodology has been applied to a dmas operating inside the wdn fig 3 all the private connections to wdn are equipped with water smart meters the users are all of residential type and have been distinguished between single houses and apartment blocks in an apartment block a unique smart meter is installed for all the housing units since all the residential users are equipped with smart meters the unmetered consumption umc related to the users is null table 1 points out the main characteristic of the dma with regards to other components of umc street cleaning is carried out with vehicles not supplied with water within the district the hydrants do not have smart meters but no fires have been detected in the observation period some washings of the pipes may have occurred especially during the repairs following to the breakages but detailed data are not available and such uses were neglected housing units have been chosen as reference for the water consumption analysis so to homogenize the readings made for entire apartments blocks with those for single houses the interval between recording day is circa monthly depending on the specific operational necessities that currently are billing consumption and water loss monitoring for this second application a criticality is due to the incomplete readings in some smart meters that may be temporarily unavailable an usual situation in a system with such a high density of equipment throughout the urban area the algorithm see sections 2 3 and 2 4 for validating and reconstructing the water consumption data has been applied to the time series 02 12 2008 03 03 2013 of smart meter readings available for the district meter area shown in fig 3 the first years 2006 2008 are characterized by long periods in which the amr system did not operate due to data transmission problems in the present study when validating the entire data set a manual validation was not too onerous since the data refer to only one district and a limited number of meters if a larger number of meters had been available a manual validation would not have been convenient and we would have devised an automatic procedure in the present case the number of meters is not sufficient to represent a statistically significant sample to propose a reliable automatic procedure for deciding if the outliers are to be considered correct measurement errors or internal losses fig 4 shows the comparison between the water consumption per housing unit m3 day hu before and after the data validation and reconstruction for single houses and apartment blocks the difference between the measured and reconstructed lines for apartment blocks is lower compared to that of single houses this indicates how the smart meters installed in the apartment blocks are more reliable over time than those installed in single houses fig 5 shows the variability in the different seasons of average daily consumption for apartment blocks and single house housing unit after the validation reconstruction procedure cr the boxplots clearly highlight the higher scattering that characterizes the water consumption of single houses in comparison to the housing units of the apartment blocks monthly seasonality of average daily consumption per housing unit m3 day hu in single house blue and apartment block red is exhibited in fig 6 in terms of ratio between the monthly consumption and the yearly average its variation is between 0 409 and 0 547 m3 day hu with an average value of 0 456 m3 day hu in single houses and between 0 353 and 0 385 m3 day hu with an average value of 0 367 m3 day hu in apartment blocks for the observation period the monthly consumption of single houses is higher in comparison to apartment blocks and is characterized by greater seasonal variability this is coherent with the presence of gardens for single houses as it is possible to observe in fig 3 which increases the outdoor water use during the summer for apartment blocks the green area is normally scarce and thus the outdoor consumption is much lower in comparison to the indoor water use for the case study the total private green area belongs for 20 to the apartment blocks and for 80 to the single houses this confirms the need in the analysis to separate apartments blocks from single houses the investigation of the influence of social economic and climatic parameters on the water consumption requires a specific analysis like in toth et al 2018 it was not investigated here because it is not the focus of the present work but it may be a future development 3 2 assessment of water losses by swb the methodology see section 2 5 for the assessment of the water losses has been tested on the district meter area fig 3 metered consumption mc is obtained by a group of n 57 smart meters as indicated in table 1 the assessment of the daily water losses wl is obtained applying the daily synchronous water balance eq 1 over the considered time series d 1553 days from 02 12 2008 to 03 03 2013 fig 7 shows the water loss in percentage and absolute value m3 day the assessment of water loss is considered unreliable when water loss m3 day is negative or water loss percentage is lower than 10 of the system input volume dotted line in fig 7a such relative high value for the threshold chosen in order to identify unbehavioural losses was suggested by the water utility for the case study of fano in fig 7b it is possible to observe the time periods when the information on water losses is considered unreliable such period relatively long in respect to the total length of the available time series 39 may be attributed to the problem to the malfunctioning of the meters on the inlet points to prevent such problems an asset maintenance plan for smart meters and a robust warning system signaling the presence of temporary lack of measurements or anomalies in the data transmission should be foreseen if possible already in the smart meters system design phase the error that may have been introduced over the validated water losses data by the reconstruction process ranges between 0 and 9 1 of the total consumption with an average of 4 7 the average water loss for the reliable data a total of d 947 days shown in blue line in fig 7b is equal to 92 21 m3 day st dev 86 08 m3 day min 20 18 m3 day max 340 97 m3 day in terms of percentage of the lost water volume over the systems input volume the average water loss corresponds to 30 06 st dev 17 36 min 10 08 max 68 15 the unreliable time periods are removed from the data used in the following analyses on the assessment of the water losses thus considering only the days corresponding to the blue line in fig 7b 3 3 comparison with minimum night flow method in the assessment of water losses from the bottom up minimum night flow mnf approach puust et al 2010 the water losses are estimated as the part of mnf that exceeds a predetermined threshold corresponding to an assessment of the customer night use cnu covas et al 2006 as above said mnf is characterized by a high temporal resolution of network inflow data but the estimate of the cnu is affected by a very large uncertainty wrc 1994 proposes a refer value for the cnu equal to 1 7 l hu h recent literature models have tried to improve the efficiency of the mnf method mainly trying to estimate water losses in district meter areas based only on inlet flow data mazzolani et al 2017 or using a multiple linear regression to determine the main factors that contribute to mnf alkasseh et al 2013 but there have not yet been significant improvements in the estimate of cnu with reference to the fano case study the utility provided additional smart water readings referring to night flows and corresponding to a set of two night readings in the same dates at 2 00 a m and 5 00 a m respectively these measures are not available every day but in a discontinuous set of days along the observation period such data have not been used in the herein proposed swb method where the water balance is not limited to the night period when the uncertainties in the measurements are the largest due to the small values of the flows but have allowed a reliable cnu estimate for the present case study according to the night measurements we found an average cnu between 2 00am 5 00am equal to 2 69 l hu h this value even if not too different from the literature reference 1 7 l hu h shows the importance of having actual consumption measurements also for the application of the classic mnf method the water losses identified by the mnf method were reported to daily values and are compared in fig 8 with the assessment obtained with the swb approach the mnf measures are generally available once per month for the same night of the monthly reading used in the swb approach the mnf estimates refer to a single daily flow whereas the swb balance is circa monthly it may thus occur that the daily mnf values dots are in some cases much higher than the corresponding monthly averaged swb volumes solid line overall leaving aside the peaks due to the different time samplings the results show that the two estimates are in good agreement and allow a validation of the proposed procedure through the comparison with a traditional approach 3 4 impact on water loss assessment due to the temporary loss of smart meter data reading this section investigates the influence on the water loss estimate due to the lack of one or more smart meters this situation that can often occur in the real contexts with high number of smart meters and transmitting devices determines shortcoming in the data and thus it does not allow to directly use them to assess water balance in near real time for the water losses monitoring the basic idea is to overtake this gap estimating the expected water consumption data by relying on the seasonality that characterizes water consumption unavailable data are reconstructed as unmetered consumption umc as described in section 2 4 3 5 impact assessment metric statistical sampling fuller 2009 is used to obtain subsets samples of the working smart meters to estimate the impact in the assessment of water losses due to the temporary lack of k smart meters belonging to the population n of smart meters k n a version of the stratified random sampling is herein used fortunato et al 2015 fuller 2009 in which the population is partitioned into non overlapping groups called strata when measurements within strata are sufficiently homogeneous stratification improves the representativeness of the sample by reducing sampling error a simple random sampling in which all members of the population have an equal and independent chance of being selected is applied the most important dissimilarity in the users is here considered as the only stratification and it concerns the grouping of smart meters installed in apartment blocks n ab and in single houses n sh due to the differences in consumption for the presence of green outdoor areas for a given k a set of possible configurations with k missing smart meters is identified for each configuration x the model proposed in section 2 4 is used to reconstruct the unmeasured consumption umc of the k unavailable smart meters and the daily water loss wl k x j with j 1 d is evaluated applying the synchronous water balance wb eq 5 the impact of the lack of k smart meter for each x is then estimated with a comparison between wl k x j and the daily water loss wl 0 j obtained in fact assuming that all smart meters are operating k 0 computing the corresponding rmse k x over the d days eq 6 6 r m s e k x j 1 d w l k x j w l 0 j 2 d the metacode of the section is shown in appendix 4 and the procedure is exemplified in fig 9 3 6 impact of the lack of an increasing number of smart meters for the case study of fano initially the impact of the two strata apartment block and single house is analyzed independently in the assessment of the impact due to the lack of smart meters installed in apartment blocks it is assumed that all the smart meters belonging to the strata single house with n sh 35 are properly working and thus all the water information on their consumption is available it was then supposed that inside the strata apartment block with n ab 22 a number k of smart meters is in turn unavailable with 0 k n ab for each k 10000 random samples x of n ab k measuring smart meter were generated and the corresponding rmse k x calculated according to eq 6 when considering the lack of smart meters installed in single houses the procedure is the same carried out with respect to the apartment blocks in this case it is assumed that all the smart meters belonging to the strata apartment blocks are properly working inside the strata single houses with n sh 35 a given number of smart meters k is in turn considered unavailable with 0 k n sh for each k 10000 random samples of n sh k measuring smart meter are generated and rmse k x calculated according to eq 6 subsequently for each k the average rmse k of the rmse k x random samples is evaluated and the result is shown in fig 10 the maximum value of the standard deviation of the set of rmse k is equal to 1 90 m3 day for the apartment blocks k 1 22 and for the single houses 0 33 m3 day k 1 35 the last points of the two lines represent the rmse of the wl estimate when all the 22 smart meters of the apartment blocks are unavailable k 22 for the red line with a rmse equal to 7 06 m3 day and when all the 35 smart meters of the single houses are unavailable k 35 for the block line with an rmse equal to 0 88 m3 day considering a condition with 50 of smart meters working average rmse becomes equal to 0 95 m3 day for single houses k 18 and 5 11 m3 day for apartment blocks k 11 when apartment blocks meters are missing red line in fig 10 there is a strong deterioration of the estimate whereas the impact of the not availability of single houses on the assessment of water losses is much lower this is due to the fact that each missing smart meter in apartment blocks corresponds to a set of 8 42 housing units and its impact is therefore much larger 3 7 discontinuity or absence of an automatic meter reading system the analysis reported in section 4 2 allows also a comparison towards a baseline representing the lack of all smart meters i e all the meters of the apartments blocks and of the single houses are lacking in this case the rmse of the wl results equal to 7 34 m3 day this value would represent the average error resulting when the amr system is discontinued in a district that was previously metered for a period of time sufficient to characterize its consumptions such period of fine metering allows for the period following the discontinuity a data reconstruction based on past fine time scale readings urc conditions such a situation may occur if the utility decides not to invest any more on the amr technology for example due to the obsolescence of the devices or to the costs for replacing the batteries on the other hand the large majority of the water distribution networks districts have never been equipped with an amr system not even for a limited period of time in such conditions only annual readings of water consumption are available for each meter daily consumption is generally estimated assuming a value constant over the year and equal to the average of the annual reading in the meter itself applying eq 6 under this assumption for the case study of fano the rmse of the water loss estimate amounts to 9 39 m3 day comparing such value with the one obtainable in the case where the existing amr system is discontinued 7 34 m3 day it is shown that even in the case in which the amr system is not working anymore a more reliable characterization of the demand is allowed by its historical implementation such result also suggests that if an utility has availability of only a limited number of portable smart meters not enough for covering all the costumers it may be useful to monitor in turn the different districts in this case even if a real time swb based on actual demand would not be possible after the metering period the collected data would allow to characterize the expected consumption providing information for a more reliable estimate of the water loss 4 conclusion and future research smart meters or automatic meter reading amr systems open to a new near real time knowledge of water consumption the high resolution in its characterization can improve the assessment and reduction of water losses in water distribution networks the attention is herein focalized on the water losses assessment in water distribution networks in which amr systems allows to set readings at the same time synchronism for all the meters the proposed synchronous water balance swb methodology evaluates the water loss on suitable time intervals whose temporal extension can vary adapting to operational requirements differently from the minimum night flow procedure the swb approach does not require night readings that is the period when low water consumption values increase the possibility of measurement errors for a near real time control of water losses the day or multiples of days are sufficient for the swb approach changing the reading acquisition from the night hours to the entire day the probability of reading errors associated to the minimum consumption that in case of residential users is indeed in the night hours is reduced the proposed methodology takes into account the fact that a part of users may not be covered by the measurements and the presence of missing information in the readings of the smart water meters and or in the gathering and transmission of the data a simple data reconstruction algorithm is developed inside the swb methodology assimilating the seasonality that characterizes water consumption the impact on water loss monitoring due to the unavailability of an increasing number of smart meters is then investigated applying a random sampling and evaluating the corresponding error the advantages allowed by a period of fine readings even in the case when the amr system is then discontinued are also presented the length of the measurement period 4 25 years in total as well as the presence of irregular intervals between the readings and the fact that their frequency is circa one per month is a limitation of the case study for near real time water loss assessment a daily or at least weekly reading would be much preferable nevertheless the application to a district of the city of fano italy shows that even a low resolution reading of water consumption can substantially improve the assessment of water losses in addition the analysis highlights that in real world systems the time periods with not reliable information in raw data can be long if compared to the total length of the available time series for the case study of fano the percentage of days in which the water loss is lower than the behavioral threshold of the inlet volume is 39 such situation is far from optimal for real time water loss monitoring and may severely compromise the ability to use smart meters for this purpose furthermore the size of the study area with an overall limited number of smart meters affects the possibility of grouping the set into more homogeneous clusters for improving both the water reconstruction model for unmetered consumption and the sampling procedure for the case study of fano only the most important dissimilarity in the users typology i e apartment blocks versus single houses is used as unique stratification for water supply networks with a larger numbers of smart meters other criteria can be introduced to define the sampling strata i e the number of housing units per apartment block the average water consumption the extension of green areas but also other information might be collected such as for example the number of family members that is not available in the present case study due to privacy reasons provided that more information on the users is available future developments of the proposed methodology might also focus on the improvement in the characterization of user s water consumption which could take advantage from the extensive recent research findings cominola et al 2016 cheifetz et al 2017 software and data availability the analyses have been carried out with scripts developed within the r project free software environmental for statistical computing using the packages xts author jeffrey a ryan joshua m ulrich and hydrogof author mauricio zambrano bigiarini available from the cran comprehensive r archive network platform https cran r project org mirrors html and the scripts are available upon request to the first author the data on inlet flow and water consumptions are not public and have been provided by aset spa acknowledgments the authors are grateful to aset spa for the availability in providing the data in particular to dr danilo galeri for the technical support appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 10 010 appendix metacode appendix 1 data validation image 1 appendix 2 water consumption reconstruction model image 2 appendix 3 actual water losses evaluation image 3 appendix 4 impact on water loss assessment due to the temporary lack of smart meters image 4 
26273,in this study we introduce a data stream method for dynamic runoff simulation which allows capturing the evolving relationship between runoff and its impact factors e g temperature rainfall the basic idea is to view continuously arriving data of runoff and its impact factors as a data stream and dynamically learn its relationship to validate the effectiveness of the proposed method we compare its performance with that of three data driven models ann svr random forest and six representative hydrological models swat awbm simhyd smar sacramento and tank in simulating monthly runoff the proposed method performs well with the best nse of 0 88 being superior to comparable models furthermore the data stream model also shows its advantage in the flexibility of combing various impact factors of runoff into the model the findings demonstrate that the data stream method provides a promising way to dynamically simulate runoff in a changing environment graphical abstract image keywords runoff climate change land cover change data stream mining software availability name of software data stream model for dynamic runoff simulation darus developers junming shao qinli yang contact address no 2006 xiyuan avenue chengdu 611731 china email junmshao uestc edu cn qinli yang uestc edu cn year first available 2018 required hardware and software darus works in java on windows linux based computers cost free software availability http dm uestc edu cn wp content uploads code darus zip 1 introduction 1 1 challenges of runoff response simulation in the changing environment there have been extensive studies on runoff simulation yet more accurate runoff simulation still remains a challenge in the context of a changing environment montanari et al 2013 in recent decades the formation and variation of runoff is suggested to have been considerably impacted by climate change and increasingly intensive human activities wang et al 2016 yang et al 2017 the runoff series and the rainfall runoff relationship have exhibited non stationary features milly et al 2005 which result in poor performances of many existing runoff simulation models so for non stationary runoff sequences and non stationary rainfall runoff relationships how to establish and apply effective models to simulate runoff response to environment changes has been one of the forefront and challenging topics in water resources research xu et al 2013 at present the methods for runoff simulation can be broadly categorized into two groups data driven models and hydrologic models a data driven model simulates runoff by constructing a mathematical relationship between runoff and affecting factors based on data mining or machine learning techniques like artificial neural network ann lee et al 2008 genetic programming mehr and nourani 2017 neuro fuzzy inference system bartoletti et al 2018 and support vector regression svr granata et al 2016 a hydrologic model is a simplification of a real world water system in a given basin it quantitatively simulates runoff by establishing relationship between rainfall and runoff based on physical mechanisms wang et al 2016 to date with the development of hydrology and computer science numerous hydrologic models ranging from catchment scale to global scale have been developed li et al 2015 they include lumped models e g awbm boughton 1995 simhyd chiew et al 2002 along with semi distributed models e g swat arnold et al 1998 and distributed models e g vic liang et al 1994 mike she refsgaard et al 2010 in spite of different structures mechanisms scale and platforms of the hydrological models a procedure for runoff simulation can be summarized into two phases firstly calibrate parameters in a given hydrologic model based on a certain period of historical data secondly simulate runoff response over the validation period future period by using the calibrated model however in reality the relationship between climate human activities and runoff may vary over time i e become non stationary milly et al 2005 wang et al 2016 this can be seen in the changes of dominant factors attributed to runoff variation chang et al 2015 yin et al 2017 for example chang et al 2015 applied the vic model to investigate the impacts of climate change and human activities on runoff change in weihe river basin they reported that climate change accounted for 36 28 53 and 10 of runoff change in the weihe river in the 1970s 1980s 1990s and 2000s respectively thereby in the non stationary context the calibrated model may no longer suit for the validation periods to adapt hydrological models to the instability of rainfall runoff relationships one of feasible strategies is to calibrate models at regular intervals but the work is time consuming and in particularly the interval length is difficult to decide for instance merz et al 2011 calibrated the hbv model for every 5 years based on historical data of 41 years in 273 basins in austria they found that both parameters of hbv and nse varied obviously with time moreover the selection of calibration periods is also important for hydrological model performance young 2000 gibbs et al 2018 to the best of our knowledge the structures and parameters of hydrologic models are rarely updated over constituent time periods xu et al 2013 ejigu et al 2013 more recently how to simulate runoff more accurately under the changing environment conditions has attracted much attention marshall et al 2006 meng et al 2016 chang and yeh 2018 gibbs et al 2018 qi and liu 2018 data assimilation is a new emerging way to explore the dynamic properties of runoff simulation the basic idea is to combine recent observation s with background estimates of a system variable evensen 2006 for instance meng et al 2016 proposed to trace parameter changes in hydrological models by using an ensemble kalman filter technique with a constrained parameter evolution scheme but the findings were based on synthetic experiments not real practice pathiraja et al 2016a simultaneously estimated model parameters and states by using the ensemble kalman filter and further improved runoff prediction for two pairs of experimental catchments in western australia pathiraja et al 2016b in the context of changing environment dynamic runoff simulation by taking non stationary rainfall runoff relationship into account still remains a challenge beyond most existing hydrological models are based on physical mechanisms and require a great deal of designated data in many cases this required data is hard to collect or unavailable in addition data indirectly related to runoff such as population water withdrawals or enhanced vegetation index evi one of commonly used vegetation indices are difficult to integrate into hydrological models therefore current hydrological models are themselves limited in their practical extensions currently for non stationary rainfall runoff relationships in a changing environment a new method is required to simulate runoff response to multiple driving factors in a dynamic manner 1 2 introduction to data stream and concept drift regarding the challenge of dynamic runoff simulation in a changing environment data stream model provides a promising way and a new perspective data stream different from traditional data is characterized by its continuity time sequence huge volume and time evolution shao et al 2017a data stream model aims at handling data streams especially focusing on dynamic evolution cabrera and sànchez marrè 2018 in this study the concurrent time series of driving factors e g climatic variables and runoff can be regarded as a data stream in addition the change of relationship between runoff and driving factors can be viewed as abrupt or gradual changes concept drift in a data stream from the new perspective the question of runoff simulation is transferred to a data stream mining task during the past decades hundreds of techniques have been proposed for data mining gaber et al 2009 shao et al 2016 2017b gomes et al 2017 and have been widely used in water resources research yang et al 2011 2015 yaseen et al 2015 tan et al 2018 and environment science gibert et al 2018 here only the relevant concept drift detection methods and data stream learning methods are presented concept drift is defined as the change of conditional distribution of the target variable given the input data widmer and kubat 1996 the objective of concept drift detection is to capture the changes of data patterns in a data stream according to the speed of change concept drift can be categorized as gradual concept drift or abrupt concept drift to detect the gradual concept drift the most used methods are the sliding window model and the decay function however the selection of window size has a decisive effect on the performance of the model to identify the abrupt concept drift two strategies are adopted the distribution based method kuncheva 2008 and the error rate based method ross et al 2012 the distribution based methods detect concept drifts by dynamically monitoring the change of distributions between two fixed or adaptive windows of data for instance adwin bifet and gavaldà 2007 maintains a time window of the data stream separating the window into two sub windows and finally decides whether to shrink the window by comparing the difference of expected values of the two sub windows due to the evolving nature of data streams it is often difficult to determine the appropriate window size moreover window dependent approaches tend to detect and identify concept drifts in a relatively slow way the error rate based methods identify concept drifts by dynamically monitoring the performance of data stream prediction and deciding concept drift happens when the performance gets worse one typical representative algorithm is ddm gama et al 2004 however since the prediction performance also heavily depends on the presence of noisy instances and the learning model itself so the error rate based method is not always a good option regarding data stream learning two main strategies have been proposed model based and instance based strategies a model based strategy is learning from fixed or flexible recent data to update predictive models ikonomovska et al 2011 almeida et al 2013 for example ikonomovska et al 2011 proposed a decision tree based data stream regression algorithm which can incrementally update the decision tree to adapt to the concept drift in the data stream almeida et al 2013 developed a rule based regression algorithm named amrules which can adaptively update models according to the concept of drift detection based on the page hinkley test the results are easier for interpretation alternatively instance based strategy mines data stream patterns by treating the sampling data as instances typically shaker and huellermeier 2012 proposed a lazy learning based algorithm iblstreams for data stream regression and classification based on the spatial and temporal correlation and consistency of sampling data iblstreams deletes or adds samples so as to maintain data representing the current data pattern cabrera and sànchez marrè 2018 proposed a case based stochastic learning approach for environmental data stream mining via a case based reasoning system 1 3 aims and objectives this study aims to develop a new method for dynamic runoff simulation based on data stream mining which allows learning the non stationary relationship between runoff and its impact factors the objectives are as follows 1 simulate runoff response to climate and land cover changes in a dynamic manner 2 introduce a new flexible way to integrate various impact factors in runoff simulation 3 taking the qingliu river catchment as a case study to verify the superiority of the data stream model over several representative data driven models and hydrological models 2 study area and data acquisition 2 1 study area to demonstrate the procedure of our approach in runoff simulation qingliu river catchment fig 1 is chosen as a representative case study the reasons behind the catchment selection include 1 the authors are familiar with the catchment 2 located in the rapidly developing south east of china the study area is experiencing hydro climatic change and land use variation liu et al 2010 zhang and pu 2008 and 3 high quality long term and full data have been collected for the selected catchment the qingliu river is a secondary order tributary of the lower reaches of the yangtze river the qingliu river catchment 32 13 32 40 n 117 59 118 25 e fig 1 covers a drainage area of 1070 km2 receives an average annual precipitation of about 1000 mm and is subject to a mean annual temperature of about 16 0 c seven rain gauges and one hydrological station are located in the catchment according to the recorded hydrological data the annual average runoff in the qingliu river catchment is about 0 298 billion m3 the land use in the catchment is dominated by farmland and forest only a small proportion of grassland has been identified 2 2 data acquisition 2 2 1 hydro climatic data daily precipitation temperature pan evaporation wind speed relative humidity and solar radiation at seven rain gauges recorded from 1989 to 2010 are acquired from the china meteorological administration monthly runoff data over the period of 1989 2010 and daily runoff data with 20 of missing at the chuzhou station are provided by nanjing hydraulic research institute based on the available data the missing data of daily runoff are interpolated and supplemented which can be further used for hydrological models fig 2 illustrates the annual runoff coefficient runoff depth rainfall over the study period of 1989 2010 which also reflects the changes of the rainfall runoff relationship 2 2 2 landsat data and evi data all available level 1 terrain l1t landsat 5 7 and 8 images of the study area with cloud cover less than 90 and corresponding evi data files from 1989 to 2010 are acquired from usgs united states geological survey briefly the authors classify the land cover into five categories namely farmland forest water body residential area and others based on landsat imagery continuous change detection and classification of land cover ccdc algorithm zhu et al 2014 is applied to yield the continuous classification of land use in the qingliu river catchment building up on the continuous classification results clear pixels without cloud cover covered by farm or forest are identified in each image and evi of the identified pixels are averaged to represent the vegetation coverage level of the study area since the time step of the averaged evi data is 16 days monthly evi time series of the qingliu river catchment is constructed by integration and interpolation it worth noting that the evi data not only represent vegetation cover level of the study area but also partially reflects land cover change in the catchment the basic statistical information of evi can be identified 3 methodology 3 1 existing hydrologic models for runoff simulation 3 1 1 swat model the swat model is a well established distributed hydrologic model for assessing water resource and nonpoint source pollution problems for a wide range of scales and environmental conditions across the globe gassman et al 2007 in this study only the components relevant to runoff simulation in swat will be introduced swat operates by dividing a given basin into multiple sub basins and further delineating sub basins into hydrologic response units hrus the hrus exhibit homogeneous combinations of land use soil properties and slope swat model requires a great deal of input data mainly including meteorological data topography soils and land use land cover data the meteorological variables commonly refer to precipitation and temperature depending on the potential evapotranspiration pet calculation method selected in swat so wind speed solar radiation and relative humidity may also be required more details can be found in the swat theoretical documentation http swatmodel tamu edu 3 1 2 lumped hydrologic models five lumped conceptual rainfall runoff models are selected namely awbm sacramento simhyd smar and tank model they are integrated in the rainfall runoff library rrl https toolkit ewater org au tools rrl and are suited for catchments from 10 km2 to 10 000 km2 the awbm model australian water balance model is a rainfall runoff model based on the principle of water balance boughton 1995 the model divides a basin into three parts with different surface water storage capacities the structure of the model is relatively simple and there are 8 parameters that need to be determined awbw requires evapotranspiration et as an input in contrast the sacramento model finnerty et al 1997 gan and burges 2006 is relatively complex using a total of 16 parameters to simulate the water balance in the simulation of hydrological process the sacramento model divides the basin into 3 parts permeable area impervious area and variable impervious area the soil is vertically divided into two layers and five stores in the model runoff generated within the sacramento model consists of three flow components surface runoff interflow and base flow simhyd model is a simplified lumped conceptual rainfall runoff model on daily time step with 7 parameters chiew et al 2002 it has been widely used in simulating runoff across australia and worldwide chiew et al 2009 zhan et al 2014 one advantage of the model is that it takes into account two types of flow generation mechanisms namely saturation excess runoff and infiltration excess runoff the model estimates river runoff generation from three contributions infiltration runoff interflow and saturation excess runoff and base flow the soil moisture accounting and routing smar model was proposed by professor o connell and nash of the national university of ireland in the 1970s tan and o connor 1996 after continuous improvement it has been widely used around the world the smar model has 9 parameters and consists of two parts water balance and sink calculation the mechanism of runoff generation in the model is based on saturation excess the tank model sugawara 1974 treats the catchment as one or a few tanks the advantage of the model is its simple principle but the relatively large number of parameters is the major drawback of the model in total there are 18 parameters needing to be determined in the model 3 2 selected data driven models support vector regression svr proposed by vapnik et al 1997 is a regression technique based on support vector machine svm the performance of svr depends on kernel functions such as linear kernel polynomial kernel and radial basis kernel the basic ideas underlying svm for regression and function estimation can be found in smola and schölkopf 1998 random forest proposed by breiman 2002 is an ensemble learning based on bagging it constructs many diverse regression trees via random sampling and determines output for predictions by majority voting artificial neural network ann lawrence 1994 is a structure of interconnected units or nodes of large number of artificial neurons it can learn about the nonlinear relationships between the inputs and outputs without a detailed understanding of its physical processes ann has been extensively used in runoff simulation and prediction in recent years nourani et al 2014 3 3 dynamic runoff response simulation with a data stream method in the following sections we start with the detection of evolving relationship and afterwards introduce an instance based data stream regression to model the dynamic runoff response to a changing environment fig 3 presents the framework of the instance based data stream model 3 3 1 evolving relationship detection and adaptation in the context of changing environment the relationship between runoff and its influencing factors may change smoothly or abruptly which corresponds to gradual concept drift and sudden concept drift in data stream mining respectively in handling gradual concept drift instead of using recent data in a sliding fixed size window to re train the model we employ an instance based learning model called iblstreams shaker and huellermeier 2012 to optimize the composition and size of the relevant instances autonomously in instance based learning instead of taking time to build a global model the target function is approximated locally by means of selected instances the inherent incremental nature of instance based learning algorithms and their simple and flexible adaptation mechanism makes this type of algorithm suitable for learning in complex dynamic environments shao et al 2014 specifically for a time step i a new incoming example x i y i where x i r i t i e i y i q i and r i t i e i q i represent the rainfall temperature evaporation and runoff respectively is first added into a relevant base set during the learning process the relevant base set is dynamically updated where some redundant data or outliers will be removed to make it better characterize the current relationship between runoff and its influencing factors to this end a set c of examples within a neighborhood of x i are considered as candidates afterwards the kc youngest examples in the neighborhood set c are used to determine a confidence interval as follows 1 ci y z α 2 σ k c y z α 2 σ k c where y is the average target value i e y i for the youngest kc examples and σ is the standard deviation α is the significance level and α is set as 0 001 in this study a candidate example in c is removed if it falls outside this confidence interval and is not one of the youngest kc instances for abrupt concept drift old instances in the relevant base set should be removed instantaneously since the relationship between runoff and its affecting factors has changed significantly to detect sudden concept drift a statistical test gama et al 2004 is applied specifically we maintain the mean absolute error e and standard deviation s for the last 50 training instances let e min denote the smallest among these errors and s min be the associated standard deviation a change is detected if the current value of e is significantly higher than e min with standard z test once such an abrupt change is detected the relative increase of error is used to determine the percentage of old examples to be removed 3 3 2 runoff simulation with evolving relationship learning building upon the evolving relationship detection and adaptation the instance based learning in the form of the nearest neighbor classifier is employed for runoff simulation the most simple and typical way is to use the weighted mean of the neighbor s outputs as a prediction formally given an input vector x i x i 1 x i m where m is the number of impacting factors that will be considered in this study and i is the number of observations its output y i can be estimated as follows 2 y i e s t x j n k x i w x j y j to gain a better prediction performance here we assume that the relationship between runoff and its impacting factors can at least be approximated sufficiently well with a locally weighted linear regression function as follows 3 y i f x i 1 x i m β 0 j 1 m β j x i j β t 1 x i where x i j is the j th dimension of an example x i and β t β 0 β 1 β m is the corresponding coefficients of x i which can be estimated as 4 β x t w x 1 x t w y here w is a diagonal weight matrix d i a g w 1 w k where w i is defined as follows 5 w x j 1 d x j x i x j n k x i 1 d x j x i where d x j x i is a metric function and euclidean distance is used in this study once β is computed with eq 4 the estimated runoff y i e s t can be directly obtained based on eq 3 in case x t w x is singular and its inverse does not exist the weighted average in eq 2 is used for prediction instead in addition the performance of a nearest neighbor classifier is affected by the number of neighbors with an initial value of k k is then automatically updated by checking whether the parameter benefits by increasing or decreasing the current value by 1 to this end the mean error in a window formed by the last 100 instances with k 1 and k 1 neighbors is considered whenever one of these two variants performs better in terms of the mean error the current k is adapted correspondingly shaker and hüllermeier 2012 3 4 evaluation metrics to evaluate the performance of different environmenta models please refer to literature published by bennett et al 2013 in this study the nash sutcliffe model efficiency coefficient nse mean absolute error mae root mean square error rmse relative volume error re akaike information criterion aic akaike 1974 and bayesian information criterion bic schwarz 1978 are selected as the evaluation criteria equations 6 11 6 nse 1 i 1 n y i e s t y i 2 i 1 n y i y i 2 7 mae i 1 n y i e s t y i n 8 rmse i 1 n y i e s t y i 2 n 9 re i 1 n y i e s t y i i 1 n y i 10 aic 2 k 2 ln l 11 bic kln n 2 ln l where y i e s t is the simulated runoff by the model at time i y i represents the observed runoff at time i y i indicates for the average value of observed runoff n means the number of observations k is the number of parameters of the model and l is the maximum of the likelihood function 4 experiments 4 1 synthetic data for better illustration a synthetic data stream is generated which consists of both stationary part and non stationary part for non stationary part it further includes the gradual concept drift and sudden concept drift specifically a linear function y w 1 x 1 w 2 x 2 w 3 is considered for illustration x is only two dimensional input data y is the output for stationary part one thousand 2 dimensional points are randomly generated and w is fixed as 0 5 0 2 2 5 namely h1 y 0 5 x 1 0 2 x 2 2 5 n 0 0 001 where n u sigma 2 is the gaussian noise to generate a data stream with gradual concept drift w is gradually changed as follows w 1 w 1 0 1 i 1000 n 0 0 001 w 2 w 2 0 1 i 1000 n 0 0 001 where i 1 1000 in this way the relationship between x and y is slowly changed over time finally h1 is slowly changed to h2 y 0 6 x 1 0 3 x 2 2 5 n 0 0 001 for sudden concept drift w 1 and w 2 are changed w 1 0 5 and w 2 0 4 making the relationship vary significantly i e h3 y 0 5 x 1 0 4 x 2 2 5 n 0 0 001 the corresponding synthetic data stream is plot in fig 4 here 3000 examples are given and the first 1000 example are in a stationary environment h1 from the point of 1001 2000 the relationship gradually changed from h1 to h2 at the point of 2001 a sudden concept drift occurs and the relationship is changed to h3 from fig 4 d it can be observed that there is no abrupt change found for both stationary period and the period with gradual concept drift and a sudden concept drift is detected at the 2008th point for stationary period it is relatively easy to model the relationship among y and x x 1 x 2 for the period with gradual concept drift according to the oldness of instances and the prediction performance only important instances representing current concept are kept making the data stream approach being able to model the evolving relationship effectively for sudden concept drift the proposed approach allows quickly detecting the change with seven instances delay and then the model is quickly updated to learn the new relationship and support high simulation performance later 4 2 real data to verify the efficiency of the data stream model we conduct experiments on real data the qingliu river catchment to compare its performance with that of other data driven and hydrological models the statistical information of the data set on monthly scale is illustrated in table 1 different models have different requirements for input data with respect of amount temporal scale spatial resolution category and format considering the data availability and to keep the consistency of time scale of all available input variables the comparisons with the hydrologic models are on monthly scale finally to make the comparison more distinct and straightforward we provide fig 5 to illustrate the experimental setup the swat model requires a digital elevation model dem with 30 m resolution from geospatial data cloud of china land use classification data e g forest farm land water body bare land and residential area in 1989 derived from landsat imagery it also requires soil data from harmonized world soil database hwsd v1 1 daily meteorological data rainfall temperature wind speed relative humidity and sun radiation from the chinese meteorology administration data is used to set up and run swat in arcswat 2012 the fao penman monteith method allen et al 1998 is selected to calculate pet data from the period 1989 1999 is used for calibration with one year for warm up while data from 2000 to 2010 is selected for validation the sufi2 algorithm in swat cup is used for model calibration and nse is taken as the objective function the statistics of the input and output data over the two periods are presented in table 1 five lumped models are implemented on the platform of the rainfall runoff library rrl model parameters are optimized by the shuffled complex evolution sce ua algorithm against the objective function of nse data segmentation used for calibration warm up and validation is the same as that used in swat the input data for the data driven models are monthly rainfall temperature pan evapotranspiration and evi data between january 1989 and december 1999 are used for training and the rest of data jan 2000 dec 2010 are used for testing for the data stream method the required input data and the training data are same with those in the data driven models while the rest of the monthly data enter the model one by one for runoff simulation in each month the selected data driven models and the data stream model are implemented with java on a pc with windows 10 operating system to evaluate the runoff simulation under different conditions e g climate change or climate and land cover changes the data stream model is run with or without evi data respectively as shown in table 2 considering that the important parameter k i e the number of initial neighbors in data stream model may affect the performance of the model we run the model with different k values ranging from 30 to 60 with an interval of 5 5 results and discussion 5 1 runoff simulation result with the data stream model with different inputs data and k values in the data stream model the corresponding runoff simulation results are summarized in table 2 these suggest that the model shows a stable and good performance with high nse and low errors specifically nse ranges from 0 85 to 0 88 or from 0 83 to 0 85 for the data stream model with or without evi respectively the insensitivity of parameter k suggests the data stream model may suit many cases where only a limited length of data is available noticeably on average the results demonstrate that the data stream model with evi achieved better runoff simulation results than that without evi with respect to all evaluation metrics this finding implies evi is an important contributor to runoff change and should be included during runoff simulation this can be explained from two aspects 1 evi representing for vegetation cover may impact runoff via evapotranspiration and interception processes marques et al 2007 2 evi variation partially representing land use change as mentioned in section 2 2 2 may influence runoff generation via changing soil storage capacity rogger et al 2017 in addition relative error re of the data stream model with evi is negative while that without evi is positive this means that runoff in the former model is under estimated and runoff in the latter model is over estimated taking the best result of runoff simulation i e nse 0 88 mae 4 03 rmse 6 7 re 0 04 as an example where k 30 and evi is taken into consideration fig 6 illustrates the modelled and observed monthly runoff at chuzhou station during 2000 and 2010 it is intuitively noted that the data stream model simulates runoff response well being able to capture the trend of runoff variation and evolution of the rainfall runoff relationship 5 2 comparison with the existing models to further test the data stream model three data driven models and six hydrological models are selected for comparison table 3 presents the best performances of different models for runoff simulation in terms of nse mae rmse re aic and bic in general data stream model is superior compared to other comparable models with respect of all evaluation metrics except for mae specifically no matter whether evi is included in the model the data stream model gains the best results in term of nse rmse and re compared with other comparable models swat model also achieved relatively good results for runoff simulation with nse of 0 82 and with the smallest mae value of 3 97 in contrast the five lumped hydrological models and svr produced relatively worse results with low nse and or high errors 5 3 advantages limitations and future work to present hydrological models have been widely used in runoff simulation however more and more data driven models have been proposed in recent years montanari et al 2013 gibert et al 2018 to the best of our knowledge both kinds of methods come with advantages and disadvantages for hydrological models since they are based on the physical mechanism the pros are thus obvious they are easy to understand and can be used to interpret hydrologic processes however the factors affecting runoff are diverse and the relationships between these factors are complicated hydrological models may not be easy to consider all affecting factors direct or indirect based on physical mechanisms thereby the flexibility to incorporate new affecting factors may be limited and the performance of runoff simulation or prediction may suffer regarding data driven model its main desirable property is high simulation or prediction performance via mining all kinds of available data however for most existing data driven models the results are hard to interpret traditional hydrological models are good at simulating the stationary relationship between runoff and its affecting factors xu et al 2013 but fail to capture the dynamic relationship the advantages of the data stream model are highlighted by its dynamics and flexibility regarding the dynamics taking non stationary rainfall runoff relationships into account the data stream model can dynamically simulate runoff response to environmental changes by detecting the relationship changes and updating the model although the data stream model is designed for non stationary conditions it also works for stationary scenarios in terms of flexibility when more impact factors related to runoff are included namely land cover or population they can be easily coupled into the data stream model once their data series are identified as described in equation 3 m represents the number of impact factors the high flexibility allows for a wide range of applications of the data stream method however the data stream model lacks interpretation of physical mechanisms of driving changes in the hydrological process as a data driven model the data stream method provides a good supplementary tool for runoff simulation on monthly scale in a changing environment nevertheless if the daily scaled data of runoff and its impact factors are available the proposed approach can also be used for runoff simulation on daily scale via adjusting different k values therefore the proposed data stream approach is useable for catchments in which long observational datasets exist although the data stream can include more driving factors in this study only evi data is extended and tested to better understand such interactions quantitative attribution of factors leading to runoff change needs to be determined in the future more impact factors such as population water withdrawals total and by sector and reservoir capacities could be integrated into the model for more comprehensive study in addition the data stream model has the potential to be used for environment related applications such as water quality or air quality simulation 6 conclusions in this study a data stream method is introduced to simulate runoff response to environmental changes where the data series of runoff and its impact factors e g rainfall temperature are regarded as a data stream taking non stationary rainfall runoff relationships into account the data stream method can dynamically simulate runoff response to historical climate and land cover changes by detecting relationship changes and updating the model the qingliu river catchment is used as a case study to verify the effectiveness of the data stream method model performance is compared with that of three data driven models svr ann random forest and six internationally used hydrological models swat awbm simhyd smar sacramento and tank the results demonstrate that the data stream model achieves a stable and good performance with mean nse over 0 84 being superior to all the comparable models additionally the data stream model with evi nse ranging from 0 85 to 0 88 outperforms that without evi nse ranging from 0 83 to 0 85 the data stream approach provides a promising way for dynamic runoff simulation in the context of a changing environment furthermore the findings will be beneficial to local water resources management and planning author contributions q y and j s designed the research g w provided hydrological data and made valuable suggestions and comments on the research design q y and j s analysed the data h z processed the landsat data s l participated in swat model analysis d c and w p did data pre process q y wrote the first manuscript draft all authors read and commented on the manuscript acknowledgments this work has been financially supported by national key research and development program of china grant number 2016yfa0601501 2016yfb0502303 national natural science foundation of china grant numbers 41601025 61403062 41830863 state key laboratory of hydrology water resources and hydraulic engineering grant number 2017490211 science technology foundation for young scientist of sichuan province grant number 2016jq0007 sichuan provincial soft science research program grant number 2017zr0208 and fok ying tong education foundation for young teachers in the higher education institutions of china grant number 161062 we are grateful to dr martin e parkes for constructive comments that improved the quality of the work we are also grateful to the two editors and the three anonymous reviewers for their valuable advices on the earlier version of this manuscript 
26273,in this study we introduce a data stream method for dynamic runoff simulation which allows capturing the evolving relationship between runoff and its impact factors e g temperature rainfall the basic idea is to view continuously arriving data of runoff and its impact factors as a data stream and dynamically learn its relationship to validate the effectiveness of the proposed method we compare its performance with that of three data driven models ann svr random forest and six representative hydrological models swat awbm simhyd smar sacramento and tank in simulating monthly runoff the proposed method performs well with the best nse of 0 88 being superior to comparable models furthermore the data stream model also shows its advantage in the flexibility of combing various impact factors of runoff into the model the findings demonstrate that the data stream method provides a promising way to dynamically simulate runoff in a changing environment graphical abstract image keywords runoff climate change land cover change data stream mining software availability name of software data stream model for dynamic runoff simulation darus developers junming shao qinli yang contact address no 2006 xiyuan avenue chengdu 611731 china email junmshao uestc edu cn qinli yang uestc edu cn year first available 2018 required hardware and software darus works in java on windows linux based computers cost free software availability http dm uestc edu cn wp content uploads code darus zip 1 introduction 1 1 challenges of runoff response simulation in the changing environment there have been extensive studies on runoff simulation yet more accurate runoff simulation still remains a challenge in the context of a changing environment montanari et al 2013 in recent decades the formation and variation of runoff is suggested to have been considerably impacted by climate change and increasingly intensive human activities wang et al 2016 yang et al 2017 the runoff series and the rainfall runoff relationship have exhibited non stationary features milly et al 2005 which result in poor performances of many existing runoff simulation models so for non stationary runoff sequences and non stationary rainfall runoff relationships how to establish and apply effective models to simulate runoff response to environment changes has been one of the forefront and challenging topics in water resources research xu et al 2013 at present the methods for runoff simulation can be broadly categorized into two groups data driven models and hydrologic models a data driven model simulates runoff by constructing a mathematical relationship between runoff and affecting factors based on data mining or machine learning techniques like artificial neural network ann lee et al 2008 genetic programming mehr and nourani 2017 neuro fuzzy inference system bartoletti et al 2018 and support vector regression svr granata et al 2016 a hydrologic model is a simplification of a real world water system in a given basin it quantitatively simulates runoff by establishing relationship between rainfall and runoff based on physical mechanisms wang et al 2016 to date with the development of hydrology and computer science numerous hydrologic models ranging from catchment scale to global scale have been developed li et al 2015 they include lumped models e g awbm boughton 1995 simhyd chiew et al 2002 along with semi distributed models e g swat arnold et al 1998 and distributed models e g vic liang et al 1994 mike she refsgaard et al 2010 in spite of different structures mechanisms scale and platforms of the hydrological models a procedure for runoff simulation can be summarized into two phases firstly calibrate parameters in a given hydrologic model based on a certain period of historical data secondly simulate runoff response over the validation period future period by using the calibrated model however in reality the relationship between climate human activities and runoff may vary over time i e become non stationary milly et al 2005 wang et al 2016 this can be seen in the changes of dominant factors attributed to runoff variation chang et al 2015 yin et al 2017 for example chang et al 2015 applied the vic model to investigate the impacts of climate change and human activities on runoff change in weihe river basin they reported that climate change accounted for 36 28 53 and 10 of runoff change in the weihe river in the 1970s 1980s 1990s and 2000s respectively thereby in the non stationary context the calibrated model may no longer suit for the validation periods to adapt hydrological models to the instability of rainfall runoff relationships one of feasible strategies is to calibrate models at regular intervals but the work is time consuming and in particularly the interval length is difficult to decide for instance merz et al 2011 calibrated the hbv model for every 5 years based on historical data of 41 years in 273 basins in austria they found that both parameters of hbv and nse varied obviously with time moreover the selection of calibration periods is also important for hydrological model performance young 2000 gibbs et al 2018 to the best of our knowledge the structures and parameters of hydrologic models are rarely updated over constituent time periods xu et al 2013 ejigu et al 2013 more recently how to simulate runoff more accurately under the changing environment conditions has attracted much attention marshall et al 2006 meng et al 2016 chang and yeh 2018 gibbs et al 2018 qi and liu 2018 data assimilation is a new emerging way to explore the dynamic properties of runoff simulation the basic idea is to combine recent observation s with background estimates of a system variable evensen 2006 for instance meng et al 2016 proposed to trace parameter changes in hydrological models by using an ensemble kalman filter technique with a constrained parameter evolution scheme but the findings were based on synthetic experiments not real practice pathiraja et al 2016a simultaneously estimated model parameters and states by using the ensemble kalman filter and further improved runoff prediction for two pairs of experimental catchments in western australia pathiraja et al 2016b in the context of changing environment dynamic runoff simulation by taking non stationary rainfall runoff relationship into account still remains a challenge beyond most existing hydrological models are based on physical mechanisms and require a great deal of designated data in many cases this required data is hard to collect or unavailable in addition data indirectly related to runoff such as population water withdrawals or enhanced vegetation index evi one of commonly used vegetation indices are difficult to integrate into hydrological models therefore current hydrological models are themselves limited in their practical extensions currently for non stationary rainfall runoff relationships in a changing environment a new method is required to simulate runoff response to multiple driving factors in a dynamic manner 1 2 introduction to data stream and concept drift regarding the challenge of dynamic runoff simulation in a changing environment data stream model provides a promising way and a new perspective data stream different from traditional data is characterized by its continuity time sequence huge volume and time evolution shao et al 2017a data stream model aims at handling data streams especially focusing on dynamic evolution cabrera and sànchez marrè 2018 in this study the concurrent time series of driving factors e g climatic variables and runoff can be regarded as a data stream in addition the change of relationship between runoff and driving factors can be viewed as abrupt or gradual changes concept drift in a data stream from the new perspective the question of runoff simulation is transferred to a data stream mining task during the past decades hundreds of techniques have been proposed for data mining gaber et al 2009 shao et al 2016 2017b gomes et al 2017 and have been widely used in water resources research yang et al 2011 2015 yaseen et al 2015 tan et al 2018 and environment science gibert et al 2018 here only the relevant concept drift detection methods and data stream learning methods are presented concept drift is defined as the change of conditional distribution of the target variable given the input data widmer and kubat 1996 the objective of concept drift detection is to capture the changes of data patterns in a data stream according to the speed of change concept drift can be categorized as gradual concept drift or abrupt concept drift to detect the gradual concept drift the most used methods are the sliding window model and the decay function however the selection of window size has a decisive effect on the performance of the model to identify the abrupt concept drift two strategies are adopted the distribution based method kuncheva 2008 and the error rate based method ross et al 2012 the distribution based methods detect concept drifts by dynamically monitoring the change of distributions between two fixed or adaptive windows of data for instance adwin bifet and gavaldà 2007 maintains a time window of the data stream separating the window into two sub windows and finally decides whether to shrink the window by comparing the difference of expected values of the two sub windows due to the evolving nature of data streams it is often difficult to determine the appropriate window size moreover window dependent approaches tend to detect and identify concept drifts in a relatively slow way the error rate based methods identify concept drifts by dynamically monitoring the performance of data stream prediction and deciding concept drift happens when the performance gets worse one typical representative algorithm is ddm gama et al 2004 however since the prediction performance also heavily depends on the presence of noisy instances and the learning model itself so the error rate based method is not always a good option regarding data stream learning two main strategies have been proposed model based and instance based strategies a model based strategy is learning from fixed or flexible recent data to update predictive models ikonomovska et al 2011 almeida et al 2013 for example ikonomovska et al 2011 proposed a decision tree based data stream regression algorithm which can incrementally update the decision tree to adapt to the concept drift in the data stream almeida et al 2013 developed a rule based regression algorithm named amrules which can adaptively update models according to the concept of drift detection based on the page hinkley test the results are easier for interpretation alternatively instance based strategy mines data stream patterns by treating the sampling data as instances typically shaker and huellermeier 2012 proposed a lazy learning based algorithm iblstreams for data stream regression and classification based on the spatial and temporal correlation and consistency of sampling data iblstreams deletes or adds samples so as to maintain data representing the current data pattern cabrera and sànchez marrè 2018 proposed a case based stochastic learning approach for environmental data stream mining via a case based reasoning system 1 3 aims and objectives this study aims to develop a new method for dynamic runoff simulation based on data stream mining which allows learning the non stationary relationship between runoff and its impact factors the objectives are as follows 1 simulate runoff response to climate and land cover changes in a dynamic manner 2 introduce a new flexible way to integrate various impact factors in runoff simulation 3 taking the qingliu river catchment as a case study to verify the superiority of the data stream model over several representative data driven models and hydrological models 2 study area and data acquisition 2 1 study area to demonstrate the procedure of our approach in runoff simulation qingliu river catchment fig 1 is chosen as a representative case study the reasons behind the catchment selection include 1 the authors are familiar with the catchment 2 located in the rapidly developing south east of china the study area is experiencing hydro climatic change and land use variation liu et al 2010 zhang and pu 2008 and 3 high quality long term and full data have been collected for the selected catchment the qingliu river is a secondary order tributary of the lower reaches of the yangtze river the qingliu river catchment 32 13 32 40 n 117 59 118 25 e fig 1 covers a drainage area of 1070 km2 receives an average annual precipitation of about 1000 mm and is subject to a mean annual temperature of about 16 0 c seven rain gauges and one hydrological station are located in the catchment according to the recorded hydrological data the annual average runoff in the qingliu river catchment is about 0 298 billion m3 the land use in the catchment is dominated by farmland and forest only a small proportion of grassland has been identified 2 2 data acquisition 2 2 1 hydro climatic data daily precipitation temperature pan evaporation wind speed relative humidity and solar radiation at seven rain gauges recorded from 1989 to 2010 are acquired from the china meteorological administration monthly runoff data over the period of 1989 2010 and daily runoff data with 20 of missing at the chuzhou station are provided by nanjing hydraulic research institute based on the available data the missing data of daily runoff are interpolated and supplemented which can be further used for hydrological models fig 2 illustrates the annual runoff coefficient runoff depth rainfall over the study period of 1989 2010 which also reflects the changes of the rainfall runoff relationship 2 2 2 landsat data and evi data all available level 1 terrain l1t landsat 5 7 and 8 images of the study area with cloud cover less than 90 and corresponding evi data files from 1989 to 2010 are acquired from usgs united states geological survey briefly the authors classify the land cover into five categories namely farmland forest water body residential area and others based on landsat imagery continuous change detection and classification of land cover ccdc algorithm zhu et al 2014 is applied to yield the continuous classification of land use in the qingliu river catchment building up on the continuous classification results clear pixels without cloud cover covered by farm or forest are identified in each image and evi of the identified pixels are averaged to represent the vegetation coverage level of the study area since the time step of the averaged evi data is 16 days monthly evi time series of the qingliu river catchment is constructed by integration and interpolation it worth noting that the evi data not only represent vegetation cover level of the study area but also partially reflects land cover change in the catchment the basic statistical information of evi can be identified 3 methodology 3 1 existing hydrologic models for runoff simulation 3 1 1 swat model the swat model is a well established distributed hydrologic model for assessing water resource and nonpoint source pollution problems for a wide range of scales and environmental conditions across the globe gassman et al 2007 in this study only the components relevant to runoff simulation in swat will be introduced swat operates by dividing a given basin into multiple sub basins and further delineating sub basins into hydrologic response units hrus the hrus exhibit homogeneous combinations of land use soil properties and slope swat model requires a great deal of input data mainly including meteorological data topography soils and land use land cover data the meteorological variables commonly refer to precipitation and temperature depending on the potential evapotranspiration pet calculation method selected in swat so wind speed solar radiation and relative humidity may also be required more details can be found in the swat theoretical documentation http swatmodel tamu edu 3 1 2 lumped hydrologic models five lumped conceptual rainfall runoff models are selected namely awbm sacramento simhyd smar and tank model they are integrated in the rainfall runoff library rrl https toolkit ewater org au tools rrl and are suited for catchments from 10 km2 to 10 000 km2 the awbm model australian water balance model is a rainfall runoff model based on the principle of water balance boughton 1995 the model divides a basin into three parts with different surface water storage capacities the structure of the model is relatively simple and there are 8 parameters that need to be determined awbw requires evapotranspiration et as an input in contrast the sacramento model finnerty et al 1997 gan and burges 2006 is relatively complex using a total of 16 parameters to simulate the water balance in the simulation of hydrological process the sacramento model divides the basin into 3 parts permeable area impervious area and variable impervious area the soil is vertically divided into two layers and five stores in the model runoff generated within the sacramento model consists of three flow components surface runoff interflow and base flow simhyd model is a simplified lumped conceptual rainfall runoff model on daily time step with 7 parameters chiew et al 2002 it has been widely used in simulating runoff across australia and worldwide chiew et al 2009 zhan et al 2014 one advantage of the model is that it takes into account two types of flow generation mechanisms namely saturation excess runoff and infiltration excess runoff the model estimates river runoff generation from three contributions infiltration runoff interflow and saturation excess runoff and base flow the soil moisture accounting and routing smar model was proposed by professor o connell and nash of the national university of ireland in the 1970s tan and o connor 1996 after continuous improvement it has been widely used around the world the smar model has 9 parameters and consists of two parts water balance and sink calculation the mechanism of runoff generation in the model is based on saturation excess the tank model sugawara 1974 treats the catchment as one or a few tanks the advantage of the model is its simple principle but the relatively large number of parameters is the major drawback of the model in total there are 18 parameters needing to be determined in the model 3 2 selected data driven models support vector regression svr proposed by vapnik et al 1997 is a regression technique based on support vector machine svm the performance of svr depends on kernel functions such as linear kernel polynomial kernel and radial basis kernel the basic ideas underlying svm for regression and function estimation can be found in smola and schölkopf 1998 random forest proposed by breiman 2002 is an ensemble learning based on bagging it constructs many diverse regression trees via random sampling and determines output for predictions by majority voting artificial neural network ann lawrence 1994 is a structure of interconnected units or nodes of large number of artificial neurons it can learn about the nonlinear relationships between the inputs and outputs without a detailed understanding of its physical processes ann has been extensively used in runoff simulation and prediction in recent years nourani et al 2014 3 3 dynamic runoff response simulation with a data stream method in the following sections we start with the detection of evolving relationship and afterwards introduce an instance based data stream regression to model the dynamic runoff response to a changing environment fig 3 presents the framework of the instance based data stream model 3 3 1 evolving relationship detection and adaptation in the context of changing environment the relationship between runoff and its influencing factors may change smoothly or abruptly which corresponds to gradual concept drift and sudden concept drift in data stream mining respectively in handling gradual concept drift instead of using recent data in a sliding fixed size window to re train the model we employ an instance based learning model called iblstreams shaker and huellermeier 2012 to optimize the composition and size of the relevant instances autonomously in instance based learning instead of taking time to build a global model the target function is approximated locally by means of selected instances the inherent incremental nature of instance based learning algorithms and their simple and flexible adaptation mechanism makes this type of algorithm suitable for learning in complex dynamic environments shao et al 2014 specifically for a time step i a new incoming example x i y i where x i r i t i e i y i q i and r i t i e i q i represent the rainfall temperature evaporation and runoff respectively is first added into a relevant base set during the learning process the relevant base set is dynamically updated where some redundant data or outliers will be removed to make it better characterize the current relationship between runoff and its influencing factors to this end a set c of examples within a neighborhood of x i are considered as candidates afterwards the kc youngest examples in the neighborhood set c are used to determine a confidence interval as follows 1 ci y z α 2 σ k c y z α 2 σ k c where y is the average target value i e y i for the youngest kc examples and σ is the standard deviation α is the significance level and α is set as 0 001 in this study a candidate example in c is removed if it falls outside this confidence interval and is not one of the youngest kc instances for abrupt concept drift old instances in the relevant base set should be removed instantaneously since the relationship between runoff and its affecting factors has changed significantly to detect sudden concept drift a statistical test gama et al 2004 is applied specifically we maintain the mean absolute error e and standard deviation s for the last 50 training instances let e min denote the smallest among these errors and s min be the associated standard deviation a change is detected if the current value of e is significantly higher than e min with standard z test once such an abrupt change is detected the relative increase of error is used to determine the percentage of old examples to be removed 3 3 2 runoff simulation with evolving relationship learning building upon the evolving relationship detection and adaptation the instance based learning in the form of the nearest neighbor classifier is employed for runoff simulation the most simple and typical way is to use the weighted mean of the neighbor s outputs as a prediction formally given an input vector x i x i 1 x i m where m is the number of impacting factors that will be considered in this study and i is the number of observations its output y i can be estimated as follows 2 y i e s t x j n k x i w x j y j to gain a better prediction performance here we assume that the relationship between runoff and its impacting factors can at least be approximated sufficiently well with a locally weighted linear regression function as follows 3 y i f x i 1 x i m β 0 j 1 m β j x i j β t 1 x i where x i j is the j th dimension of an example x i and β t β 0 β 1 β m is the corresponding coefficients of x i which can be estimated as 4 β x t w x 1 x t w y here w is a diagonal weight matrix d i a g w 1 w k where w i is defined as follows 5 w x j 1 d x j x i x j n k x i 1 d x j x i where d x j x i is a metric function and euclidean distance is used in this study once β is computed with eq 4 the estimated runoff y i e s t can be directly obtained based on eq 3 in case x t w x is singular and its inverse does not exist the weighted average in eq 2 is used for prediction instead in addition the performance of a nearest neighbor classifier is affected by the number of neighbors with an initial value of k k is then automatically updated by checking whether the parameter benefits by increasing or decreasing the current value by 1 to this end the mean error in a window formed by the last 100 instances with k 1 and k 1 neighbors is considered whenever one of these two variants performs better in terms of the mean error the current k is adapted correspondingly shaker and hüllermeier 2012 3 4 evaluation metrics to evaluate the performance of different environmenta models please refer to literature published by bennett et al 2013 in this study the nash sutcliffe model efficiency coefficient nse mean absolute error mae root mean square error rmse relative volume error re akaike information criterion aic akaike 1974 and bayesian information criterion bic schwarz 1978 are selected as the evaluation criteria equations 6 11 6 nse 1 i 1 n y i e s t y i 2 i 1 n y i y i 2 7 mae i 1 n y i e s t y i n 8 rmse i 1 n y i e s t y i 2 n 9 re i 1 n y i e s t y i i 1 n y i 10 aic 2 k 2 ln l 11 bic kln n 2 ln l where y i e s t is the simulated runoff by the model at time i y i represents the observed runoff at time i y i indicates for the average value of observed runoff n means the number of observations k is the number of parameters of the model and l is the maximum of the likelihood function 4 experiments 4 1 synthetic data for better illustration a synthetic data stream is generated which consists of both stationary part and non stationary part for non stationary part it further includes the gradual concept drift and sudden concept drift specifically a linear function y w 1 x 1 w 2 x 2 w 3 is considered for illustration x is only two dimensional input data y is the output for stationary part one thousand 2 dimensional points are randomly generated and w is fixed as 0 5 0 2 2 5 namely h1 y 0 5 x 1 0 2 x 2 2 5 n 0 0 001 where n u sigma 2 is the gaussian noise to generate a data stream with gradual concept drift w is gradually changed as follows w 1 w 1 0 1 i 1000 n 0 0 001 w 2 w 2 0 1 i 1000 n 0 0 001 where i 1 1000 in this way the relationship between x and y is slowly changed over time finally h1 is slowly changed to h2 y 0 6 x 1 0 3 x 2 2 5 n 0 0 001 for sudden concept drift w 1 and w 2 are changed w 1 0 5 and w 2 0 4 making the relationship vary significantly i e h3 y 0 5 x 1 0 4 x 2 2 5 n 0 0 001 the corresponding synthetic data stream is plot in fig 4 here 3000 examples are given and the first 1000 example are in a stationary environment h1 from the point of 1001 2000 the relationship gradually changed from h1 to h2 at the point of 2001 a sudden concept drift occurs and the relationship is changed to h3 from fig 4 d it can be observed that there is no abrupt change found for both stationary period and the period with gradual concept drift and a sudden concept drift is detected at the 2008th point for stationary period it is relatively easy to model the relationship among y and x x 1 x 2 for the period with gradual concept drift according to the oldness of instances and the prediction performance only important instances representing current concept are kept making the data stream approach being able to model the evolving relationship effectively for sudden concept drift the proposed approach allows quickly detecting the change with seven instances delay and then the model is quickly updated to learn the new relationship and support high simulation performance later 4 2 real data to verify the efficiency of the data stream model we conduct experiments on real data the qingliu river catchment to compare its performance with that of other data driven and hydrological models the statistical information of the data set on monthly scale is illustrated in table 1 different models have different requirements for input data with respect of amount temporal scale spatial resolution category and format considering the data availability and to keep the consistency of time scale of all available input variables the comparisons with the hydrologic models are on monthly scale finally to make the comparison more distinct and straightforward we provide fig 5 to illustrate the experimental setup the swat model requires a digital elevation model dem with 30 m resolution from geospatial data cloud of china land use classification data e g forest farm land water body bare land and residential area in 1989 derived from landsat imagery it also requires soil data from harmonized world soil database hwsd v1 1 daily meteorological data rainfall temperature wind speed relative humidity and sun radiation from the chinese meteorology administration data is used to set up and run swat in arcswat 2012 the fao penman monteith method allen et al 1998 is selected to calculate pet data from the period 1989 1999 is used for calibration with one year for warm up while data from 2000 to 2010 is selected for validation the sufi2 algorithm in swat cup is used for model calibration and nse is taken as the objective function the statistics of the input and output data over the two periods are presented in table 1 five lumped models are implemented on the platform of the rainfall runoff library rrl model parameters are optimized by the shuffled complex evolution sce ua algorithm against the objective function of nse data segmentation used for calibration warm up and validation is the same as that used in swat the input data for the data driven models are monthly rainfall temperature pan evapotranspiration and evi data between january 1989 and december 1999 are used for training and the rest of data jan 2000 dec 2010 are used for testing for the data stream method the required input data and the training data are same with those in the data driven models while the rest of the monthly data enter the model one by one for runoff simulation in each month the selected data driven models and the data stream model are implemented with java on a pc with windows 10 operating system to evaluate the runoff simulation under different conditions e g climate change or climate and land cover changes the data stream model is run with or without evi data respectively as shown in table 2 considering that the important parameter k i e the number of initial neighbors in data stream model may affect the performance of the model we run the model with different k values ranging from 30 to 60 with an interval of 5 5 results and discussion 5 1 runoff simulation result with the data stream model with different inputs data and k values in the data stream model the corresponding runoff simulation results are summarized in table 2 these suggest that the model shows a stable and good performance with high nse and low errors specifically nse ranges from 0 85 to 0 88 or from 0 83 to 0 85 for the data stream model with or without evi respectively the insensitivity of parameter k suggests the data stream model may suit many cases where only a limited length of data is available noticeably on average the results demonstrate that the data stream model with evi achieved better runoff simulation results than that without evi with respect to all evaluation metrics this finding implies evi is an important contributor to runoff change and should be included during runoff simulation this can be explained from two aspects 1 evi representing for vegetation cover may impact runoff via evapotranspiration and interception processes marques et al 2007 2 evi variation partially representing land use change as mentioned in section 2 2 2 may influence runoff generation via changing soil storage capacity rogger et al 2017 in addition relative error re of the data stream model with evi is negative while that without evi is positive this means that runoff in the former model is under estimated and runoff in the latter model is over estimated taking the best result of runoff simulation i e nse 0 88 mae 4 03 rmse 6 7 re 0 04 as an example where k 30 and evi is taken into consideration fig 6 illustrates the modelled and observed monthly runoff at chuzhou station during 2000 and 2010 it is intuitively noted that the data stream model simulates runoff response well being able to capture the trend of runoff variation and evolution of the rainfall runoff relationship 5 2 comparison with the existing models to further test the data stream model three data driven models and six hydrological models are selected for comparison table 3 presents the best performances of different models for runoff simulation in terms of nse mae rmse re aic and bic in general data stream model is superior compared to other comparable models with respect of all evaluation metrics except for mae specifically no matter whether evi is included in the model the data stream model gains the best results in term of nse rmse and re compared with other comparable models swat model also achieved relatively good results for runoff simulation with nse of 0 82 and with the smallest mae value of 3 97 in contrast the five lumped hydrological models and svr produced relatively worse results with low nse and or high errors 5 3 advantages limitations and future work to present hydrological models have been widely used in runoff simulation however more and more data driven models have been proposed in recent years montanari et al 2013 gibert et al 2018 to the best of our knowledge both kinds of methods come with advantages and disadvantages for hydrological models since they are based on the physical mechanism the pros are thus obvious they are easy to understand and can be used to interpret hydrologic processes however the factors affecting runoff are diverse and the relationships between these factors are complicated hydrological models may not be easy to consider all affecting factors direct or indirect based on physical mechanisms thereby the flexibility to incorporate new affecting factors may be limited and the performance of runoff simulation or prediction may suffer regarding data driven model its main desirable property is high simulation or prediction performance via mining all kinds of available data however for most existing data driven models the results are hard to interpret traditional hydrological models are good at simulating the stationary relationship between runoff and its affecting factors xu et al 2013 but fail to capture the dynamic relationship the advantages of the data stream model are highlighted by its dynamics and flexibility regarding the dynamics taking non stationary rainfall runoff relationships into account the data stream model can dynamically simulate runoff response to environmental changes by detecting the relationship changes and updating the model although the data stream model is designed for non stationary conditions it also works for stationary scenarios in terms of flexibility when more impact factors related to runoff are included namely land cover or population they can be easily coupled into the data stream model once their data series are identified as described in equation 3 m represents the number of impact factors the high flexibility allows for a wide range of applications of the data stream method however the data stream model lacks interpretation of physical mechanisms of driving changes in the hydrological process as a data driven model the data stream method provides a good supplementary tool for runoff simulation on monthly scale in a changing environment nevertheless if the daily scaled data of runoff and its impact factors are available the proposed approach can also be used for runoff simulation on daily scale via adjusting different k values therefore the proposed data stream approach is useable for catchments in which long observational datasets exist although the data stream can include more driving factors in this study only evi data is extended and tested to better understand such interactions quantitative attribution of factors leading to runoff change needs to be determined in the future more impact factors such as population water withdrawals total and by sector and reservoir capacities could be integrated into the model for more comprehensive study in addition the data stream model has the potential to be used for environment related applications such as water quality or air quality simulation 6 conclusions in this study a data stream method is introduced to simulate runoff response to environmental changes where the data series of runoff and its impact factors e g rainfall temperature are regarded as a data stream taking non stationary rainfall runoff relationships into account the data stream method can dynamically simulate runoff response to historical climate and land cover changes by detecting relationship changes and updating the model the qingliu river catchment is used as a case study to verify the effectiveness of the data stream method model performance is compared with that of three data driven models svr ann random forest and six internationally used hydrological models swat awbm simhyd smar sacramento and tank the results demonstrate that the data stream model achieves a stable and good performance with mean nse over 0 84 being superior to all the comparable models additionally the data stream model with evi nse ranging from 0 85 to 0 88 outperforms that without evi nse ranging from 0 83 to 0 85 the data stream approach provides a promising way for dynamic runoff simulation in the context of a changing environment furthermore the findings will be beneficial to local water resources management and planning author contributions q y and j s designed the research g w provided hydrological data and made valuable suggestions and comments on the research design q y and j s analysed the data h z processed the landsat data s l participated in swat model analysis d c and w p did data pre process q y wrote the first manuscript draft all authors read and commented on the manuscript acknowledgments this work has been financially supported by national key research and development program of china grant number 2016yfa0601501 2016yfb0502303 national natural science foundation of china grant numbers 41601025 61403062 41830863 state key laboratory of hydrology water resources and hydraulic engineering grant number 2017490211 science technology foundation for young scientist of sichuan province grant number 2016jq0007 sichuan provincial soft science research program grant number 2017zr0208 and fok ying tong education foundation for young teachers in the higher education institutions of china grant number 161062 we are grateful to dr martin e parkes for constructive comments that improved the quality of the work we are also grateful to the two editors and the three anonymous reviewers for their valuable advices on the earlier version of this manuscript 
26274,light detection and ranging lidar digital elevation models dems are frequently applied in modeling coastal environments we present an object based correction approach for accurate and precise dems by integrating lidar point data aerial imagery and real time kinematic global positioning systems four machine learning techniques random forest support vector machine k nearest neighbor and artificial neural network were compared with the commonly used bias correction method the random forest object based model produced best predictions for two study areas nine mile mean bias error mbe reduced 0 18 to 0 02 m root mean square error rmse reduced 0 22 to 0 08 m and flamingo mbe reduced 0 17 to 0 02 m rmse reduced 0 24 to 0 10 m a monte carlo model was developed to combine errors into the object based machine learning corrected dems and uncertainty maps spatially revealed the likelihood of error the object based correction approach provides an attractive alternative to the bias correction method keywords lidar object based image analysis machine learning monte carlo dems coastal wetlands 1 introduction 1 1 significance of lidar digital elevation models dems in the coastal everglades a unique federal state partnership in the usa is making efforts to restore the original everglades ecosystem which has been largely modified in the past century by human activities nrc 2014 everglades restoration requires better digital elevation models dems with a vertical root mean square error rmse less than 0 15 m to monitor and simulate water levels water depths and hydroperiods jones et al 2012 for the coastal everglades dems are also recommended to have a fine spatial resolution e g 5 m to identify local areas vulnerable to coastal hazards such as sea level rise and hurricanes zhang 2011 cooper et al 2015 current available elevation datasets in the coastal everglades include the national elevation dataset ned shuttle radar topography mission srtm high accuracy elevation database haed and light detection and ranging lidar the vertical error of the 30 m ned and srtm datasets are 0 48 1 89 m and 4 01 m in terms of the rmse gesch et al 2014 respectively which are insufficient for restoration requirements jones et al 2012 the everglades hydrologic community has agreed upon the vertical elevation error threshold of 0 15 m for restoration projects desmond 2003 jones et al 2012 which is interpreted here as the rmse to meet this strict error specification the u s geological survey usgs conducted a region scale survey from 1995 to 2007 using an airborne height finder ahf with differential global positioning system gps to measure sub water and terrain surface elevation elevation data collected in this airborne survey were combined with the ground survey data to generate a dem dataset known as haed in the everglades the vertical error of haed meets the restoration requirements however its coarse spatial resolution 400 m limits its applications in coastal areas to generate fine spatial resolution dems in the coastal everglades the florida division of emergency management collected lidar data over florida s coastal areas lidar has been recognized as a standard to generate fine spatial resolution dems for various applications jensen 2015 however it is a challenge for lidar to obtain accurate dems in coastal marsh environments due to several combined factors that include the complexity of coastal marshes instrument and software coastal marshes in the everglades are typically inundated with murky water containing dark peat soils which make the automated ground elevation measures difficult lidar often fails to penetrate through dense marsh plants preventing the system instrument from discerning true ground returns from non ground returns i e vegetation in addition the common software used by vendors are commonly not effective at filtering lidar mass points into ground returns for marsh vegetated areas morris et al 2005 rosso et al 2006 schmid et al 2011 1 2 coastal marsh lidar dem correction lidar dems have shown to overestimate marsh ground elevations as much as 0 65 m medeiros et al 2015 and errors tend to grow with both increasing vegetation density and marsh height rosso et al 2006 sadro et al 2007 schmid et al 2011 to address this issue a binning procedure was utilized to generate binning dems because the bias and error can be reduced considerably rosso et al 2003 schmid et al 2011 medieros et al 2015 buffington et al 2016 researchers typically apply corrections to an already generated lidar dem using field surveyed real time kinematic rtk gps data schmid et al 2011 hlaldik and alber 2012 medieros et al 2015 mcclure et al 2016 buffington et al 2016 rtk gps is an alternative technique for collecting accurate elevation data through field surveys the application of rtk gps data has been recognized as a standard to assess and calibrate lidar dems the uncertainty of lidar dems in marsh environments is mainly attributable to vegetation and tends to be species dependent therefore one method in marsh lidar dem correction using rtk gps data focuses on a solution to derive a bias specific to each species and then correct the lidar dem cell by cell using the species dependent bias e g hlaldik and alber 2012 mcclure et al 2016 this is referred to as the grid based bias correction method in this study this type of correction assumes that errors in the lidar dem are spatially consistent within a species and the heterogeneous structure of this species has not been considered lidar uncertainty in marsh environments is influenced by the characteristics of plants which present a varying elevation within a species rather than a constant rogers et al 2016 2018 a second method in lidar dem correction uses above ground biomass and vegetation indices with linear modeling techniques medeiros et al 2015 buffington et al 2016 while this type of correction considers the heterogenous structure within a species it assumes that the underlying data follow a normal distribution which may not always the case therefore a non parametric approach that incorporates the spatial heterogeneity of lidar uncertainty and has the potential to enhance the integrity of lidar dems in marsh environments has been developed in this study 1 3 object based nonparametric modeling techniques in lidar correction and dem generation one strategy to include the heterogeneity of lidar uncertainty in the correction is to identify the relationship between marsh characteristics and lidar uncertainty lidar statistical metrics have proven useful to characterize vegetation in the florida everglades zhang 2014 a quantification of the relationship between lidar measurements and rtk gps data might be an effective alternative to the commonly used bias correction method lidar statistical metrics can be derived at the grid level with a regular size and shape or the object level with varying sizes and shapes object based modeling and mapping is more valuable than grid based methods in the everglades because coastal managers and planners are more interested in a region patch rather than an individual grid raster zhang et al 2018 object based image analysis obia provides the unique opportunity to develop an object based lidar correction approach to be reported and tested in this study obia has been well developed and widely used in image classification as reviewed by blaschke 2010 however obia has never been applied in correcting lidar dems or lidar point data this study is the first to explore the potential of obia for lidar correction and generating lidar dems by developing an object based correction approach a recent study from rogers et al 2018 shows that lidar derived measurements tend to be nonlinearly related to lidar uncertainty suggesting nonparametric machine learning modeling algorithms might be more effective than traditional linear modeling techniques in lidar correction in this study we examined four nonparametric machine learning modeling algorithms for lidar correction and dem generation random forest rf support vector machine svm k nearest neighbor k nn and artificial neural network ann these algorithms have shown to perform well in marsh characterization and biomass modeling in the everglades zhang 2014 zhang et al 2018 1 4 monte carlo uncertainty simulation since corrected dems approximate the true elevation they too contain vertical inaccuracies and errors that need considering the vertical accuracy of a dem refers to how close the modeled elevations are to the true elevations of an independent data source of better accuracy i e rtk gps vertical error or uncertainty in dems may be expressed numerically by standard deviation σ which is a measure of precision that represents a range of errors around the mean accuracy between the dem and rtk gps common sources of lidar dem vertical accuracy and error and the standards used to assess them are provided in a review by cooper et al 2013 adjusting for the uncertainty in the underlying data respective to the dems is especially important for low slope marsh environments where a difference of a few centimeters in elevation can impact the results of rigorous ecological and hydrological modeling unfortunately the effect of uncertainty in dems is often neglected and consequently not adjusted for by dem providers and users wechsler and kroll 2006 monte carlo simulation techniques provide the unique opportunity where the effect of the uncertainty related to the underlying data in lidar corrected dems can be observed and adjusted monte carlo is a statistical approach useful for modeling dynamic geographic phenomena with significant uncertainty such as dems e g wechsler and kroll 2006 it has been widely used in mapping and modeling of coastal marsh environments such as the probabilistic estimation of sea level rise marine inundation cooper and chen 2013 clough et al 2016 and groundwater inundation cooper et al 2015 carbon emissions henman and poulter 2008 and intertidal habitats on barrier islands enwright et al 2018 however it has not been applied in adjusting corrected dems by considering the underlying error estimates in the correction procedure in principle monte carlo simulation follows the law of large numbers theorem where averaging the results over many trials provides a more reliable result of the expected value graham and talay 2013 in this study we extended the monte carlo dem uncertainty approach by wechsler and kroll 2006 to consider the vertical accuracy and errors in the corrected dems rtk gps data and benchmarks and vertical datums to which the data are vertically referenced it is expected that by combining monte carlo simulation with the corrected dems and error estimates the approximations of the true elevations being modeled are more reliable for end users needs 1 5 objectives the main objective of this study is to develop an object based lidar correction approach for generating corrected dems to complement the existing bias correction methods by combining obia and machine learning modeling techniques the specific objectives are to 1 explore the pros and cons of object based machine learning lidar correction method compared with the current grid based bias correction methods 2 examine the potential benefits of object based elevation data over the grid based raster dems and 3 adjust for the effect of uncertainty in the final corrected dem deliverables and produce maps where the likelihood of object based corrected dem uncertainty can be observed 2 study area and data 2 1 study area the study area is in the southern coastal region of everglades national park enp fig 1 we surveyed two sites of this area for collecting rtk gps data in this study the first site is located just below the nine mile pond turnoff from main park road hereinafter referred to as nine mile 1 km2 in area this site is historically a freshwater marsh dominated by sawgrass cladium jamaicense the second site is in the southernmost headquarters of enp and includes the flamingo visitor center hereinafter referred to as flamingo 3 km2 in area this site is dominated by high density tall stands of trees taller than 5 m including red mangrove rhizophora mangle black mangrove avicennia germinans white mangrove laguncularia racemosa and buttonwood conocarpus erectus during the survey the ecological impact of hurricanes could be seen within the plant communities at each study site in nine mile low density trees with heights less than 5 m are pervasive including red mangrove and buttonwood this is because winds from hurricanes donna 1960 and betsy 1965 resulted in the establishment of red mangrove rhizophora mangle seedlings in nine mile thus the red mangrove does not require its intertidal habitat to survive lodge 2010 in flamingo the inland movement of mud during hurricanes resulted in a patchwork of coastal prairies where dominant species include saltwort batis maritima glasswort salicornia spp and saltgrass distichlis spicata all generally less than 1 m tall the soils for nine mile are dominated by sawgrass peat deposits while the soils for flamingo vary spatially and are dominated by mangrove peat deposits and hurricane deposited marl both study sites are underlain by highly permeable miami limestone which makes up part of the biscayne aquifer water flows towards the southwest where extensive freshwater diversions due to current water management practices coupled with rising sea levels are threatening the stability of the coastal peat marshes 2 2 data data sources used in this study include rtk gps ground elevation data for correcting lidar elevations lidar for building object based corrected dems and aerial imagery for generating image objects for our rtk gps survey data collection a reconnaissance was performed to identify the survey sites and existing survey controls survey sites were selected based on roadside access to different vegetation communities after the reconnaissance we collected rtk gps elevation data in february and march 2016 the dry season of south florida november may to be seasonally consistent with the acquisition of lidar and aerial imagery in the data collection existing survey controls were chosen based on their vicinity to our study sites unobstructed view of the sky accessibility and horizontal and vertical orders that affect the overall error of the survey three national geodetic survey ngs benchmarks were used in the field ngs tidal benchmark pid ab2404 ngs benchmarks pid ac4648 and pid ac4659 which were reported to have a horizontal and vertical classification of first order class ii with a maximum elevation difference error of 0 07 m 1 standard deviation or σ https www ngs noaa gov datasheets the leica 1200 system was utilized with reported real time errors of 0 01 m in the horizontal and 0 02 m in the vertical rmse at 68 confidence interval leica geosystems 2008 the leica 1200 base receiver was set up over the ngs and tidal benchmarks a topo shoe was fitted to the survey rods of the moving receivers or rovers to prevent sinking into the peat soils elevations were surveyed near low tide with nominal precisions 0 01 m in horizontal and 0 03 m in vertical positions rmse at 68 confidence interval survey data were collected for each dominant vegetation community to provide a representation of the overall population and with a separation radius greater than or equal to 2 m in total we collected 483 rtk gps ground elevations 132 for nine mile 351 for flamingo which were vertically referenced to north american vertical datum of 1988 navd88 using geoid 12a the current best available lidar data for the study area were collected by the florida division of emergency management in february 2008 using the leica als50 airborne laser scanner system which was reported to have a horizontal error of 0 07 0 64 m 1 standard deviation or 1 σ and vertical error of 0 08 0 24 m 1 σ after post processing leica geosystems 2007 the data are available at national oceanic and atmospheric administration noaa s data access viewer https coast noaa gov dataviewer although intensity images are also available they were not used in this study for model simplicity the average lidar point density reported is 2 points m2 for unobscured areas no trees or buildings the vendor filtered the lidar point cloud into classified ground bare earth and non ground returns vegetation structures and buildings rtk gps trimble 4700 and trimble 4000 series and total station surveys were also conducted by the vendor for lidar accuracy assessment in july 2007 for nine mile and in december 2007 for flamingo which were also considered in this study since both the lidar and vendor s survey data were vertically referenced to the navd88 using geoid 03 we transformed their lidar and rtk gps data to be consistent with our rtk gps data using a vertical datum transformation tool from noaa https vdatum noaa gov fine spatial resolution aerial photography with three spectral channels red green and blue is available for both sites the nine mile aerial imagery was collected in january 2016 with a spatial resolution of 0 25 m while the flamingo imagery was collected in january 2012 with a spatial resolution of 0 30 m we used the aerial imagery to produce image objects for conducting object based lidar correction and dem modeling 3 methodology 3 1 framework of the object based machine learning lidar correction and dem generation we designed a framework for conducting object based machine learning lidar correction and dem generation as shown in fig 2 in the framework image objects were produced first using an image segmentation approach then lidar statistical metrics were extracted for each object to be used for ground elevation modeling and prediction field rtk gps elevation data were spatially matched at the object level to the lidar measurements to be used for model development a training dataset was used to fine tune the parameters of four machine learning regression algorithms rf svm k nn and ann before making predictions on the new lidar measurements to generate the object based lidar corrected dems an independent test dataset was then used to quantitatively compare the object based lidar corrected dems that were then combined with monte carlo simulation to account for the uncertainty in the underlying data the major steps in the framework include image segmentation data matching machine learning modeling lidar correction and dem generation accuracy assessment and monte carlo uncertainty simulation these steps are provided in the following subsections 3 1 1 image segmentation we generated objects from the aerial imagery using the multiresolution segmentation algorithm in ecognition developer 9 3 benz et al 2004 trimble 2017 the algorithm first segments individual pixels of an image before merging neighboring segments together until a heterogeneity threshold is reached benz et al 2004 the heterogeneity threshold is determined by user defined parameters including scale color shape and smoothness compactness weights the scale is an arbitrary value allowing for a relative comparison between different scales the scale determines the size of objects where a smaller scale value produces smaller homogeneous objects and a larger scale value produces larger heterogenous objects in this study a smaller scale is reasonable for restoration planning and sea level rise mapping which requires highly detailed elevation products to help identify an optimal scale for generating the best prediction model we tested three segmentations using scale parameters ranging from 20 to 40 at an interval of 10 the color shape parameter was set to 0 9 1 0 for all three bands so that spectral information was weighted most heavily for segmentation the smoothness compactness parameter was set to 0 5 0 5 for all three bands so that compact and non compact segments were favored equally after segmentation the lidar elevation statistical metrics minimum mean maximum standard deviation range and count or number of lidar points were extracted for each image object to be used as independent variables for ground elevation modeling and mapping objects without lidar measurements were dropped for further analysis the lidar statistical metrics were derived using the original lidar point elevations which have proven more valuable than the application of a raster layer generated from lidar data for extracting lidar measurements zhang et al 2011 2014 3 1 2 data matching it is common for researchers to match the observed rtk gps measures with individual lidar dem grid cells to calculate correction factors in this study we spatially matched our rtk gps data 132 for nine mile 351 for flamingo with the lidar measurements at the object level we expected two advantages from the object based matching scheme first it can reduce the uncertainty of positional discrepancy between lidar measures and field rtk gps data second a pure object region is more representative for the plant structure and or dem than any individual grid cell within this object region obia offers the capability to match the field rtk gps data to relatively homogeneous objects with a varying shape and size rather than a grid cell in which plants might be heterogeneous in structure 3 1 3 object based nonparametric machine learning modeling and lidar correction and dem generation in this study we examined four nonparametric machine learning algorithms rf svm k nn and ann for ground elevation modeling with rtk gps data as the dependent variable and lidar measurements as the independent variables rf is an ensemble learning technique developed by breiman 2001 to improve the classification and regression tree method advantages of rf include its speed and capability to deal with complex relationships between predictors but it requires many training samples that should be representative belgiu and drãguţ 2016 svm is a statistical learning approach that looks for an optimal hyperplane to minimize training errors an advantage to svm is its capability to produce improved estimations based on a small number of training samples but parameter issues may arrive with this algorithm mountrakis et al 2011 k nn is an instance based learning method that searches for the best match to denote inputs an advantage of k nn is its capability to make local approximations while simultaneously solving multiple problems and dealing with changes in the problem domain however it requires the selection of an appropriate distance metric to combine neighbors for predictions chirici et al 2016 ann models data based on the structure of neurons and the synapses of human brains ann can generalize in a noisy environment but it may be difficult to interpret because the inner workings are like a black box mas and flores 2008 since each model has its own pros and cons the potential of each for lidar correction has been explored in this study the 132 matched samples for nine mile and 351 matched samples for flamingo were randomly split into two datasets with one to train the machine learning models used to correct the lidar measurements before generating dems and the other to test the predictive performance of all finalized models used to generate corrected dems the national digital elevation program ndep 2004 recommends a minimum of 30 checkpoints to access the vertical accuracy of elevation data we chose to randomly select a total of 30 samples as test data samples used to assess the vertical accuracy of the corrected dems for the nine mile and flamingo study areas respectively the remaining samples 102 for nine mile 321 for flamingo were used to train and fine tune the models parameters to help reduce overfitting training time and improve model accuracy automatic attribute selection is useful for determining the most relevant explanatory variables the supervised attribute selection filter in weka version 3 8 hall et al 2009 was used to select relevant explanatory variables for use in model construction the filter evaluates the value of a subset of explanatory variables by considering the individual predictive ability of each variable along with the degree of redundancy between them hall 1998 the relevant explanatory variables for the nine mile object based dataset were all lidar measurements except standard deviation and range and the relevant explanatory variables for the flamingo object based dataset were lidar minimum and count we employed weka version 3 8 hall et al 2009 where the training datasets were used to fit the four machine learning regression algorithms over different tuning parameters using resampling techniques the rf algorithm breiman 2001 was utilized where the following tuning parameters were defined 1 the number of decision trees in the forest and 2 the number of randomly selected variables subsetted from the total number of variables that are used for each node split in a tree the kernel precision and penalty parameters were adjusted when using the svm algorithm shevade et al 1999 the k nn algorithm aha and kibler 1991 was evaluated by fine tuning distance measures weighted functions and k value parameters the parameters fit for the ann multilayer perceptron algorithm mas and flores 2008 were the learning rate and number of hidden layers and training cycles repeatability was achieved by using the same sequence of random numbers where the seed was set to 123 for all applicable model runs for each of the four machine learning techniques the tuning parameters were chosen based on trial and error using the lowest rmse provided by 10 fold cross validation the finalized models were then utilized to make predictions on new lidar measurements using arcgis version 10 4 http www esri com data management tools the prediction attribute features were spatially joined to the object based features based on their spatial relationship to generate the object based lidar corrected dems 3 1 4 accuracy assessment the test data were used to produce quantitative assessments of the final corrected dems to compare between models choose which models produced the best results and perform monte carlo simulations the association between each corrected dem s model predicted p values and observed o rtk gps measures were evaluated using several correlation difference and summary measures one type of correlation measure was calculated the correlation or degree of association between p and o was expressed as pearson s coefficient of correlation r three types of difference measures were calculated estimates of the average error were described as the mean absolute error mae and rmse any systematic error that makes all estimates off by a certain amount or bias was described as the mean bias error mbe several summary measures were then calculated these include the mean μ and standard deviation σ these indices were calculated by 1 r i 1 n p i p i o i o i i 1 n p i p i 2 i 1 n o i o i 2 2 m a e i 1 n p i o i n 3 r m s e i 1 n p i o i 2 n 4 m b e i 1 n p i o i n where p i is the model predicted value p i is the mean of the model predicted values o i is the observed value o i is the mean of the observed values n is the number of matched test data samples and i is an integer from 1 to n 3 1 5 monte carlo uncertainty simulation on corrected dems before we can adjust for the effect of uncertainty in the final corrected dem deliverables and produce maps where the likelihood of corrected dem uncertainty can be illustrated the vertical accuracy and error in the underlying data along with their distributions needs considering these uncertainties included the following 1 vertical datums and any transformations made between them 2 rtk gps measurements 3 vertical benchmarks used in survey and 4 the model predictions for each object in the object based corrected dems first the errors associated with the vertical datum to which the data are referenced were obtained from noaa 2013 the σ is used to quantify uncertainties for vertical datums where noaa considers a constant σ of 5 cm for navd88 nationwide no transformations between datasets were needed for this study to include transformation errors second the rtk gps measurements contained a vertical rmse 0 03 m rmse 1 σ while the vertical benchmarks to which the surveys were based is 0 07 m 1 σ it is assumed that the individual uncertainties are independent where the value of one measurement does not affect the value of the other measurement and are randomly distributed following a normal distribution this allows for the total uncertainty of the rtk gps measures and vertical benchmarks to be calculated by the root sum of squares as the maximum cumulative uncertainty mcu σ 5 m c u σ σ 1 2 σ 2 2 σ n 2 finally the accuracy and error in the corrected dems were calculated in the previous section which are needed for the monte carlo simulation to adjust for the uncertainty related to the underlying data monte carlo simulation is applied to our corrected dems having a single output quantity more reliable elevation where the input quantities survey vertical datums and model predictions are characterized by any specific probability distribution pd we did not apply filtered error fields in the procedure e g wechsler and kroll 2006 enwright et al 2018 instead unfiltered error fields were considered as a worst case scenario of the effects of corrected dem uncertainty wechsler and kroll 2006 we reduce the equation by cooper and chen 2013 to consider the uncertainty related to our corrected dems 6 d e m r e l i a b l e d e m s u r v e y d e m d a t u m d e m e r r o r d e m x y n where dem reliable is a final object based corrected dem value that is more reliable provided the uncertainty in the underlying data d e m s u r v e y is a random variable sampled proportional to the rtk gps measures and vertical benchmarks survey data distribution d e m d a t u m is a random variable sampled proportional to the vertical datum distribution d e m e r r o r is a random variable sampled proportional to the model predictions and dem x y is a constant elevation z value of an object at an x y location the sampling procedure is repeated a total of 1000 cases for each object an illustration of the monte carlo based elevation uncertainty model is shown in fig 3 to produce maps that spatially illustrate the likelihood that a corrected dem is in error it is assumed that the true elevation is best represented by the corrected dem derived from the most accurate model in this way the likelihood that a corrected dem value is in error is based on the probability that the corrected dem value deviates from the true elevation monte carlo simulation is applied to our object based corrected dems having a single output quantity likelihood of error where the input quantities survey vertical datums and model predictions are characterized by any specific probability distribution pd using the following equation 7 l i k e l i h o o d e r r o r d e m s u r v e y d e m d a t u m d e m e r r o r d e m x y d e m s u r v e y d e m d a t u m t r u e e r r o r t r u e x y n where the difference from equation 6 is that likelihood error is the probability an object based corrected value deviates from the t r u e elevation t r u e e r r o r is a random variable sampled from the most accurate object based corrected dem and true x y is a constant elevation z value at an x y location derived from the most accurate object based corrected dem although any ranking scheme can be utilized we chose to generalize the likelihood of error values using the following ranking scheme where object based probability values ranging from 0 to 0 39 are assigned equal to low object based probability values ranging from 0 4 to 0 59 are assigned equal to medium and object based probability values ranging from 0 6 to 1 are assigned equal to high likelihood of error the generalized object based likelihood layers are then used to calculate total land area susceptible to error for each ranking scheme low medium high 3 2 grid based nonparametric machine learning modeling and lidar correction and dem generation for comparison purposes we also conducted the grid based lidar correction and dem generation the process was first implemented in python version 2 7 10 https www python org using arcgis arcpy python site package to create an easy workflow first three grids with resolutions ranging from 1 to 3 m at an interval of 1 m were generated the minimum resolution was chosen based on the reported lidar average point density of 2 pts m2 the binning technique was then utilized rosso et al 2003 schmid et al 2011 medieros et al 2015 buffington et al 2016 to assign a grid cell s center point the respective lidar statistic minimum mean maximum standard deviation range count when more than one lidar attribute falls within that cell if only one lidar attribute falls within a cell that attribute is assigned to that cell null assigned to no data due to the nature of the landscape consisting of dense coastal vegetation and surface water with dark peat soils below making lidar measures sometimes difficult certain cells contained null values the results were a total of six raster datasets for nine mile and six raster datasets for flamingo each containing lidar minimum mean maximum standard deviation range and count per respective grid cell the relevant explanatory variables for the nine mile grid based dataset were all lidar measurements except standard deviation range and count and the relevant explanatory variables for the flamingo grid based dataset were simply the lidar mean and minimum the process presented in section 3 1 3 above was then repeated to generate grid based corrected dems 3 3 bin and bin bias correction dem generation since lidar ground returns are known to overestimate coastal marsh elevations the minimum bin technique hereinafter simply referred to as bin seems more reasonable than interpolation methods for our two study sites when comparing the bias correction procedure with the four machine learning regression algorithms here the grid based and object based datasets containing the minimum lidar value are referred to as the grid based and object based bin it is common for researchers to calibrate grid based lidar dems with bias correction factors schmid et al 2011 hladik and alber 2012 mcclure et al 2016 for comparison purposes the grid based and object based bin datasets were also calibrated by the mean bias error mbe calculated from the training datasets 102 for nine mile 321 for flamingo these datasets are referred to as the grid based and object based bin bias correction 4 results 4 1 grid based vs object based corrected dems to compare the bias correction method with machine learning modeling descriptive statistics are displayed in table 1 in reviewing the summary position parameter or average of predictions p the bin and bin bias correction techniques tend to systematically overestimate the corresponding observed average of observations o to a higher degree than the machine learning techniques with one exception the flamingo object based scale 30 bin dataset suggests that p and o are the same table 1 therefore the bias correction technique cannot be applied to the flamingo object based scale 30 bin because the difference parameter mbe is zero machine learning techniques provide a valuable alternative to the bias correction approach when lidar data contains no systematic bias i e mbe 0 0 and is less unprecise e g σ 0 23 m the difference measures mae rmse and mbe help to better determine which modeled grid based 3 m resolution and modeled object based scale 30 datasets are preferred table 1 in terms of the mae and rmse for nine mile a meaningful distinction can be made between the bin bias based and machine learning experiments the mae and rmse are lowest best for the object based and grid based machine learning experiments at the nine mile study site in reviewing the mbe the machine learning experiments also reduce the bias when compared to the bias correction procedure further examination of r indicates that the nine mile grid based 3 m rf has a slightly higher degree of association between p and o r 0 76 when compared to the object based scale 30 rf r 0 70 overall the rf algorithm performed best for the nine mile grid based and object based datasets when compared to the bias correction technique in terms of the mae and rmse for flamingo a slightly different distinction can be made between the bin bias correction and machine learning experiments the mae and rmse are lowest best for the object based machine learning experiments where rf performs best overall at the flamingo study site however the grid based machine learning experiments did not perform as well as the grid based bin bias correction experiment at the flamingo study site the examination of r indicates that the flamingo object based scale 30 rf has a slightly higher degree of association r 0 95 when compared to the grid based bin bias correction r 0 88 overall the object based rf algorithm performed best compared to all other experiments for the flamingo study site we also compared the rtk gps measurements with the uncorrected lidar ground return elevations within an object based or grid based dataset table 1 the lidar systematically overestimates the ground for both study sites for nine mile the uncorrected lidar within an object demonstrated a lower mbe of 0 18 m and rmse of 0 22 m when compared to the uncorrected lidar within a grid mbe 0 24 m rmse 0 27 m although the standard deviation of 0 13 cm is the same for both the object based and grid based datasets the rmse may be more useful because it demonstrates that large errors are present in the mae this is because the rmse calculation assigns higher weights to larger errors by first squaring the errors before taking their average for flamingo no meaningful distinction can be made between the uncorrected lidar within an object mbe 0 17 m rmse of 0 24 m when compared to the uncorrected lidar within a grid mbe 0 17 m rmse 0 22 m 4 2 grid based and object based dem mapping the corrected dem maps were derived by applying the machine learning models to the object based scale 30 and grid based 3 m data for each of the two study sites this provided the opportunity to incorporate the uncertainty in the underlying data into the final corrected dem products using equation 6 with monte carlo simulations see figs 4 and 5 in the maps red represents higher elevations above navd88 and blue represents lower elevations below navd88 all in meters for both study areas the object based corrected dem maps qualitatively do a better job at estimating unknown locations when compared to the grid based corrected dem maps this is illustrated by the missing data values shown in white that are more prevalent in the grid based corrected dem maps the flamingo study site also has more topographic variation than the nine mile study site allowing for better visual examination of the object based and grid based experimental results the corrected dem maps show that the object based approach provides a more detailed account of the ground elevation values when compared to the grid based corrected dem maps see fig 5 for more variation in the graduated colors the corrected dem maps also demonstrate that the object based corrected dems are more representative of the landscape 4 3 uncertainty mapping relative to the best model the final uncertainty maps that illustrate the probability that an object based corrected elevation value is in error from the true elevation are shown for the nine mile and flamingo study areas in figs 6 and 7 respectively although the nine mile and flamingo object based rf corrected dems do not represent an unbiased result they were considered as the true elevation because they achieved the best result when compared to all other object based machine learning models the nine mile and flamingo object based rf corrected dems were used to calculate the likelihood of corrected dem uncertainty using equation 7 in the maps the likelihoods that svm k nn and ann object based corrected elevations are in error from the true elevations i e rf object based corrected elevations are represented by a ranking scheme of blue for low green for medium and red for high chance of error although the nine mile maps in fig 6 illustrate that the likelihood of error varies from each experiment it is difficult to qualitatively assess the preferred approach from these maps alone on the other hand the flamingo maps shown in fig 7 clearly illustrate the ann object based corrected dem has the lowest likelihood of error while the k nn object based corrected dem has the highest likelihood of error table 2 compliments the uncertainty maps by providing a quantitative assessment of the total area that is in likelihood of error for nine mile the ann object based corrected dem contains the largest area with a low likelihood of uncertainty 20 of total land area and smallest area with a high likelihood of uncertainty 12 of total land area making it the preferred approach overall the uncertainty maps and complimentary table help dem users to qualitatively and quantitatively assess the uncertainty of an object based corrected elevation 5 discussion and conclusions 5 1 machine learning regression for lidar correction after conducting a thorough literature review we found one recent study that used machine learning modeling in marshes to develop a lidar correction approach rogers et al 2018 these authors recommended that this type of modeling be extended to other regions with different vegetation types to examine its robustness in this study the potential of four machine learning regression algorithms rf svm k nn and ann were examined to correct lidar elevation data in the coastal everglades for two study sites 1 nine mile dominated by sawgrass marsh and mangrove swamp and 2 flamingo dominated by mangrove swamp and coastal prairies all algorithms produced an acceptable degree of association between the predicted and observed values where the coefficient of correlation r is over 0 55 using object based modeling rf performed best for the nine mile object based modeling r 0 70 where the uncorrected lidar mbe was reduced from 0 18 to 0 02 m mae was reduced from 0 19 to 0 05 m standard deviation σ was reduced from 0 13 to 0 07 m and rmse was reduced from 0 22 to 0 08 m rf also performed best for the flamingo object based modeling r 0 95 where the uncorrected lidar mbe was reduced from 0 17 to 0 02 m mae was reduced from 0 17 to 0 06 m standard deviation σ was reduced from 0 17 to 0 10 m and rmse was reduced from 0 24 to 0 10 m although the vegetation communities explanatory variables of importance and number of rtk gps are not the same our results are comparable with the study by rogers et al 2018 where rf performed best for discrete lidar where all marsh vegetation mbe 0 01 cm standard deviation and rmse 0 11 cm based on this study and the study by rogers et al 2018 the rf algorithm is robust when correcting discrete lidar elevation data for the two regions with their own unique vegetation communities the rf algorithm seems to handle well datasets that do not follow a gaussian distribution contain a bias and are highly spatially autocorrelated which is typical of marsh environments this is likely due to the nature of the rf algorithm injecting randomness in the variables and averaging over many predictions to get an unbiased model result with low variance see criminisi et al 2011 for a review on rf however rf is not without weaknesses when using rf for regression the model does not do a good job at predicting beyond the range of the training data which may have an impact on our results depending on how well the lidar and rtk gps measurements represent the overall population since it is known that dems generated using interpolation results in additional error and due to the nature of the lidar overestimating the true ground due to vegetation and open water we decided against using interpolation instead a binning procedure to correcting lidar elevations known as the minimum binning bin approach was utilized because it has shown to reduce bias and error in marsh environments when compared to interpolation rosso et al 2003 schmid et al 2011 medeiros et al 2015 buffington et al 2016 a few case studies have also demonstrated that the bias correction technique does a satisfactory job at improving the bias e g hladik and alber 2012 mcclure et al 2016 it was anticipated that by combining the bin and bias correction techniques into a bin bias correction object based procedure the bias and error could be further improved for comparison with the machine learning object based modeling however this was not the case for our nine mile study site where the bin bias correction object based dataset mbe was slightly increased from 0 03 to 0 04 cm and the change in the σ and rmse was negligible 0 17 0 16 cm when compared to the bin object based dataset rogers et al 2016 pointed out that while the bias correction technique typically does well in correcting for bias machine learning techniques provide a better option for error removal in this study the rf object based modeling improved bias and significantly lowered error when compared to the bin and bin bias correction techniques 5 2 grid based vs object based lidar correction and dem generation researchers have focused on correcting lidar dems at the grid level e g schmid et al 2011 hlaldik and alber 2012 mcclure et al 2016 buffington et al 2016 rogers et al 2016 a thorough literature review revealed that no other efforts have been made for applying obia in object based lidar correction nor applying obia in modeling lidar dems at the object level the object based lidar correction approach developed and tested in this study successfully combines the spatial features of fine spatial resolution aerial imagery 0 30 m with low posting density 2 pts m2 lidar elevation measurements making it an attractive alternative to the commonly used grid based method the object based technique s ability to take advantage of spatial features within an image to define an object patch and predict more area where lidar is sparse is attractive see figs 4 and 5 this reduces the need for dem void filling procedures the object based approach can also reduce positional discrepancy between the image and rtk gps measurements because an object represents a vegetation patch better than a single grid cell fig 8 the object based mapping and modeling reduces local noise in heterogeneous wetland environments by averaging the spectra of all pixels within an object dronova 2015 in the everglades vegetation has a high spatial and spectral heterogeneity the heterogeneity threshold is determined by a user defined scale parameter a smaller scale value produces smaller homogeneous patches of vegetation while a larger scale value produces larger heterogenous patches additionally a species typically presents a range of elevation uncertainty opposed to a constant rogers et al 2016 so it is unlikely that an entire vegetation species would need a constant correction using the bias correction technique rogers et al 2018 the capability of the object based lidar correction technique to reduce the spatial bias and uncertainty within each individual object patch successfully addresses the issue where the assumption is made that the accuracy and error is the same for an entire vegetation community the inherent structure of vector data makes the object based corrected dems attractive and convenient when manipulating and performing operations on elevation data the vector object based mapping in our case produced smoother elevation gradients and was geographically more accurate because an object is represented by the line features of a vegetation patch whereas the raster grid based mapping is represented by an artificial uniform grid size fig 8 the mixed pixel problem is most apparent in the 2 m grid because it is assumed that a grid cell covers a homogeneous vegetation patch since the aerial imagery have a fine spatial resolution 0 25 0 3 m it is easier to interpret homogenous vegetation patches as objects from the fine resolution images fig 8 additionally object based elevations are stored as attribute tables allowing for flexibility in data manipulation such as joining tables since both data management and the integrity of object based corrected elevations is maintained through topology rules network and proximity operations are performed more efficiently for example network based analytical tools used for solving complex routing issues in a gis can easily be applied to object based corrected dems to identify thresholds of dynamical marsh system response to water and nutrient fluxes proximity based analytical tools can also be easily applied to an object based corrected dem to select by attributes and identify proximity to other features such as the shoreline these vector based operations are especially useful for assessing the impacts of sea level rise and storm related events on natural and human coastal systems e g cooper et al 2015 although the object based technique provides some benefits in elevation mapping and modeling it is not without its weaknesses here we identify three limitations related to the scale parameter vector data structure and processing time first the scale parameter used for generating the size of objects can impact the object based modeling results a small scale produces homogenous objects while a large scale produces more heterogenous objects in this study we tested several scale parameters by trial and error although methods exist for optimizing the scale parameters in image segmentation e g johnson and xie 2011 we did not apply them in this study these techniques may help improve the results second spatial analysis tools and filtering methods require raster data to perform these operations the nine mile object based corrected dem would need to be converted to a raster using the original 0 25 m resolution aerial imagery as its grid size and processing extent to maintain the integrity of the object based corrected elevations although this requires an additional step converting an object based corrected dem to a grid based corrected dem that matches the fine resolution imagery used to generate the objects may be useful for fine scale mapping of habitat and elevation loss due to the conversion of coastal peat marshes to inland open water from sea level rise i e peat collapse third the object based machine learning technique has a high computational intensity when compared to the mb bias correction method although the object based corrected dems provided the best results the bin bias correction method may be more attractive to researchers that require a technique with a lower computation intensity finally it is interesting to re note that although the mae and rmse are lowest best for the object based machine learning experiments at the flamingo study site the grid based machine learning experiments did not perform as well as the grid based bin bias correction we speculate that this may be due to the vegetation the flamingo study site is dominated by high density tall stands of mangroves taller than 5 m making it difficult for the laser to penetrate to the ground there are a limited number lidar classified ground returns within each 3 m grid cell in this study the machine learning techniques may produce better predictions for an object because there are generally more lidar returns within an object when compared to a grid cell 5 3 monte carlo corrected dem uncertainty mapping past research focuses on lidar dem correction without including the effects of uncertainty in the corrected dems e g schmid et al 2011 hladik and alber 2012 medieros et al 2015 mcclure et al 2016 buffington et al 2016 since vertical inaccuracies and or errors remain in our corrected dems we used monte carlo simulation to adjust for these effects in the final corrected dem products and produced maps where the likelihood of error can be observed on an object by object basis researchers utilizing a sea level rise bathtub approach that does not consider uncertainty in the underlying data e g zhang 2011 would benefit from more reliable delineations of the inundation zones based on a corrected dem that has been further adjusted by any underlying errors the uncertainty maps and complimentary table allow dem users to effectively evaluate which regions are prone to error for their intended application for example a researcher investigating freshwater marsh elevation loss due to saltwater intrusion may find the uncertainty maps useful in guiding additional field campaigns where it is more challenging for the remotely sensed data and machine algorithms to accurately predict elevations monte carlo simulation has been widely used in mapping and modeling of coastal marsh environments however there has been limited application of this technique in adjusting for the inaccuracies and or errors in corrected dems 5 4 error sources in lidar correction and dem generation many sources of error impact the elevation estimation such as upscaling rtk gps data to objects the geoid models and transformations relative the vertical datum to which the data are referenced and time gaps between rtk gps lidar and aerial imagery acquisition in our study the rtk gps were upscaled from a single x y point to an entire object s area the rtk gps measurements were matched to the image objects of varying shapes and sizes the spatial resolution of the aerial imagery for nine mile was 0 25 m and the minimum size of each object was 0 25 m2 while the spatial resolution of the aerial imagery for flamingo was 0 30 m and the minimum size of each object was 0 09 m2 therefore it was assumed that the rtk gps measures represented the elevation of the entire object also the lidar data were transformed from navd88 using geoid 03 to navd88 using geoid 12a we considered a constant error for navd88 nationwide noaa 2013 in the modeling however it is not clear how transforming between geoids may impact this error this may lead to more uncertainties in the corrected dems finally there were significant time gaps between the acquisition years of the current best available lidar aerial imagery and rtk gps field measurements variability in the vegetation and topography is largely controlled by season hurricanes and water management practices south florida s dry and rainy seasons will have an impact on vegetation growth and sediment shrinkage and swelling all data utilized in this study were collected between january march during the rainy season sediment accumulation will also have a minimal impact on elevation change between 2008 and 2016 because the elevation change is within the vertical error of 0 08 0 24 m of the lidar system leica geosystems 2007 for example elevation change for everglades mangroves was measured as 0 37 0 12 mm yr cahoon and lynch 1997 the maximum elevation change 0 37 0 12 0 49 mm multiplied by the maximum number of differences in years 2016 2008 8 yrs or 0 49 mm 8 yrs 3 92 mm this 3 92 mm or 0 392 cm elevation change is within the lidar system s vertical error a similar calculation can be made for everglades sawgrass where elevation change was measured as 0 32 0 04 mm yr craft and richardson 1993 it should also be noted that no hurricanes hit the study area between 2008 and 2016 which would likely have had an impact on soil deposition and erosion and vegetation structure 5 5 orientation of future works although the developed object based lidar correction approach was successful at estimating accurate and precise elevations in this study the authors identify several suggestions for improvement of future works the developed object based lidar correction approach was tested on two sites consisting of coastal marsh swamp and prairie additional research is needed in other regions with coastal marsh and woody environments to examine the robustness of this technique we tested several independent variables in the predictive modeling including six lidar elevation statistical metrics minimum mean maximum standard deviation range and count future works should explore data fusion techniques that integrate multiple data sources in elevation correction for example additional object based spatial features such as spectral features texture and inclusion of more lidar features such as intensity statistics and optical image features can be extracted for each object which may have potential to improve the object based lidar correction we only tested the aerial imagery in the segmentation process when generating objects however the multiresolution segmentation algorithm in ecognition can also generate objects by integrating the lidar point cloud with aerial imagery this may be advantageous when generating larger image objects by accounting for elevation gradients that may be observed in other areas but this needs exploring additionally we determined the rf object based model performed best for this study however ensemble analysis of comparable models may make the predictions more reliable than using an individual model this should be explored in future works in elevation correction it should be noted that we did not consider the spatial structure of error e g holmes et al 2000 in the monte carlo modeling which has potential to improve the reliability of the final corrected dem products this requires additional rtk gps measurements used in testing and needs exploring we only compared errors spatially for each object based machine learning approach to the best model as a reference restoration managers and planners may find useful the spatial distribution of errors for the best model this also requires additional rtk gps measurements and should be explored in future works finally with the advancement of unmanned aerial vehicles uavs data acquisition in coastal environments klemas 2015 and special approval by the national park service simultaneous collection of all data sources during the same year and season is more achievable and may improve the results development of an object based uav derived measurements approach has potential in elevation correction 5 6 conclusions in this study we developed an object based lidar correction approach to modeling sawgrass marsh mangrove swamp and coastal prairie elevations by combining object based image analysis obia machine learning and monte carlo techniques the results suggested that this new approach was promising for elevation modeling and mapping compared to the bias correction and grid based modeling techniques the authors draw the following conclusions from this study machine learning regression techniques are effective at generating accurate and precise object based digital elevation models dems four machine learning regression techniques were evaluated with the bias correction technique random forest rf support vector machine svm k nearest neighbor k nn and artificial neural network ann rf achieved the best result for the nine mile object based modeling where the mean bias error mbe was reduced from 0 18 to 0 02 m standard deviation σ was reduced from 0 13 to 0 07 m and root mean square error rmse was reduced from 0 22 to 0 08 m rf also performed best for the flamingo object based modeling where the mbe was reduced from 0 17 to 0 02 m σ was reduced from 0 17 to 0 10 m and rmse was reduced from 0 24 to 0 10 m the object based lidar correction approach provides an attractive alternative to the commonly used grid based modeling the object based approach has potential to reduce positional discrepancy between an image object and rtk gps measurement because an object better represents a vegetation patch than a single grid cell the capability of the object based modeling to reduce the spatial bias and uncertainty on an object by object basis successfully addresses the issue where the bias correction technique assumes spatial bias and uncertainty is the same for an entire vegetation community monte carlo simulation is valuable for modeling and mapping the uncertainty in corrected dems the monte carlo modeling technique can make the estimations of the true elevations more reliable for an end user s needs because it combines the bias and uncertainty related to the underlying data into the final corrected dem products additionally monte carlo can provide error maps and supportive tables to compliment the model accuracy assessment the object based lidar correction approach has potential to better assist future research in generating more accurate and precise dems the robustness of these techniques should be explored in other applications e g water table elevation model wtem generation and ecosystems and urban areas with the advancement of uav data acquisition and derived measurements simultaneous collection of all data sources is more achievable and may improve the object based corrected dem results and applications future research will explore the potential of object based machine learning techniques to investigate coastal peat marsh loss by monitoring changes in small homogenous vegetation patches and elevations due to sea level rise it is also anticipated that the object based correction technique will become more popular in dem generation declaration of interest none acknowledgements this research was funded by the everglades foundation foreverglades fellowship and the american association of geographers mel marcus fund for physical geography the opinions is this research do not necessarily reflect those of the everglades foundation or aag we also thank the helpful and friendly staff at everglades national park this material was developed in collaboration with the florida coastal everglades long term ecological research program under national science foundation grant no deb 1237517 appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 11 003 
26274,light detection and ranging lidar digital elevation models dems are frequently applied in modeling coastal environments we present an object based correction approach for accurate and precise dems by integrating lidar point data aerial imagery and real time kinematic global positioning systems four machine learning techniques random forest support vector machine k nearest neighbor and artificial neural network were compared with the commonly used bias correction method the random forest object based model produced best predictions for two study areas nine mile mean bias error mbe reduced 0 18 to 0 02 m root mean square error rmse reduced 0 22 to 0 08 m and flamingo mbe reduced 0 17 to 0 02 m rmse reduced 0 24 to 0 10 m a monte carlo model was developed to combine errors into the object based machine learning corrected dems and uncertainty maps spatially revealed the likelihood of error the object based correction approach provides an attractive alternative to the bias correction method keywords lidar object based image analysis machine learning monte carlo dems coastal wetlands 1 introduction 1 1 significance of lidar digital elevation models dems in the coastal everglades a unique federal state partnership in the usa is making efforts to restore the original everglades ecosystem which has been largely modified in the past century by human activities nrc 2014 everglades restoration requires better digital elevation models dems with a vertical root mean square error rmse less than 0 15 m to monitor and simulate water levels water depths and hydroperiods jones et al 2012 for the coastal everglades dems are also recommended to have a fine spatial resolution e g 5 m to identify local areas vulnerable to coastal hazards such as sea level rise and hurricanes zhang 2011 cooper et al 2015 current available elevation datasets in the coastal everglades include the national elevation dataset ned shuttle radar topography mission srtm high accuracy elevation database haed and light detection and ranging lidar the vertical error of the 30 m ned and srtm datasets are 0 48 1 89 m and 4 01 m in terms of the rmse gesch et al 2014 respectively which are insufficient for restoration requirements jones et al 2012 the everglades hydrologic community has agreed upon the vertical elevation error threshold of 0 15 m for restoration projects desmond 2003 jones et al 2012 which is interpreted here as the rmse to meet this strict error specification the u s geological survey usgs conducted a region scale survey from 1995 to 2007 using an airborne height finder ahf with differential global positioning system gps to measure sub water and terrain surface elevation elevation data collected in this airborne survey were combined with the ground survey data to generate a dem dataset known as haed in the everglades the vertical error of haed meets the restoration requirements however its coarse spatial resolution 400 m limits its applications in coastal areas to generate fine spatial resolution dems in the coastal everglades the florida division of emergency management collected lidar data over florida s coastal areas lidar has been recognized as a standard to generate fine spatial resolution dems for various applications jensen 2015 however it is a challenge for lidar to obtain accurate dems in coastal marsh environments due to several combined factors that include the complexity of coastal marshes instrument and software coastal marshes in the everglades are typically inundated with murky water containing dark peat soils which make the automated ground elevation measures difficult lidar often fails to penetrate through dense marsh plants preventing the system instrument from discerning true ground returns from non ground returns i e vegetation in addition the common software used by vendors are commonly not effective at filtering lidar mass points into ground returns for marsh vegetated areas morris et al 2005 rosso et al 2006 schmid et al 2011 1 2 coastal marsh lidar dem correction lidar dems have shown to overestimate marsh ground elevations as much as 0 65 m medeiros et al 2015 and errors tend to grow with both increasing vegetation density and marsh height rosso et al 2006 sadro et al 2007 schmid et al 2011 to address this issue a binning procedure was utilized to generate binning dems because the bias and error can be reduced considerably rosso et al 2003 schmid et al 2011 medieros et al 2015 buffington et al 2016 researchers typically apply corrections to an already generated lidar dem using field surveyed real time kinematic rtk gps data schmid et al 2011 hlaldik and alber 2012 medieros et al 2015 mcclure et al 2016 buffington et al 2016 rtk gps is an alternative technique for collecting accurate elevation data through field surveys the application of rtk gps data has been recognized as a standard to assess and calibrate lidar dems the uncertainty of lidar dems in marsh environments is mainly attributable to vegetation and tends to be species dependent therefore one method in marsh lidar dem correction using rtk gps data focuses on a solution to derive a bias specific to each species and then correct the lidar dem cell by cell using the species dependent bias e g hlaldik and alber 2012 mcclure et al 2016 this is referred to as the grid based bias correction method in this study this type of correction assumes that errors in the lidar dem are spatially consistent within a species and the heterogeneous structure of this species has not been considered lidar uncertainty in marsh environments is influenced by the characteristics of plants which present a varying elevation within a species rather than a constant rogers et al 2016 2018 a second method in lidar dem correction uses above ground biomass and vegetation indices with linear modeling techniques medeiros et al 2015 buffington et al 2016 while this type of correction considers the heterogenous structure within a species it assumes that the underlying data follow a normal distribution which may not always the case therefore a non parametric approach that incorporates the spatial heterogeneity of lidar uncertainty and has the potential to enhance the integrity of lidar dems in marsh environments has been developed in this study 1 3 object based nonparametric modeling techniques in lidar correction and dem generation one strategy to include the heterogeneity of lidar uncertainty in the correction is to identify the relationship between marsh characteristics and lidar uncertainty lidar statistical metrics have proven useful to characterize vegetation in the florida everglades zhang 2014 a quantification of the relationship between lidar measurements and rtk gps data might be an effective alternative to the commonly used bias correction method lidar statistical metrics can be derived at the grid level with a regular size and shape or the object level with varying sizes and shapes object based modeling and mapping is more valuable than grid based methods in the everglades because coastal managers and planners are more interested in a region patch rather than an individual grid raster zhang et al 2018 object based image analysis obia provides the unique opportunity to develop an object based lidar correction approach to be reported and tested in this study obia has been well developed and widely used in image classification as reviewed by blaschke 2010 however obia has never been applied in correcting lidar dems or lidar point data this study is the first to explore the potential of obia for lidar correction and generating lidar dems by developing an object based correction approach a recent study from rogers et al 2018 shows that lidar derived measurements tend to be nonlinearly related to lidar uncertainty suggesting nonparametric machine learning modeling algorithms might be more effective than traditional linear modeling techniques in lidar correction in this study we examined four nonparametric machine learning modeling algorithms for lidar correction and dem generation random forest rf support vector machine svm k nearest neighbor k nn and artificial neural network ann these algorithms have shown to perform well in marsh characterization and biomass modeling in the everglades zhang 2014 zhang et al 2018 1 4 monte carlo uncertainty simulation since corrected dems approximate the true elevation they too contain vertical inaccuracies and errors that need considering the vertical accuracy of a dem refers to how close the modeled elevations are to the true elevations of an independent data source of better accuracy i e rtk gps vertical error or uncertainty in dems may be expressed numerically by standard deviation σ which is a measure of precision that represents a range of errors around the mean accuracy between the dem and rtk gps common sources of lidar dem vertical accuracy and error and the standards used to assess them are provided in a review by cooper et al 2013 adjusting for the uncertainty in the underlying data respective to the dems is especially important for low slope marsh environments where a difference of a few centimeters in elevation can impact the results of rigorous ecological and hydrological modeling unfortunately the effect of uncertainty in dems is often neglected and consequently not adjusted for by dem providers and users wechsler and kroll 2006 monte carlo simulation techniques provide the unique opportunity where the effect of the uncertainty related to the underlying data in lidar corrected dems can be observed and adjusted monte carlo is a statistical approach useful for modeling dynamic geographic phenomena with significant uncertainty such as dems e g wechsler and kroll 2006 it has been widely used in mapping and modeling of coastal marsh environments such as the probabilistic estimation of sea level rise marine inundation cooper and chen 2013 clough et al 2016 and groundwater inundation cooper et al 2015 carbon emissions henman and poulter 2008 and intertidal habitats on barrier islands enwright et al 2018 however it has not been applied in adjusting corrected dems by considering the underlying error estimates in the correction procedure in principle monte carlo simulation follows the law of large numbers theorem where averaging the results over many trials provides a more reliable result of the expected value graham and talay 2013 in this study we extended the monte carlo dem uncertainty approach by wechsler and kroll 2006 to consider the vertical accuracy and errors in the corrected dems rtk gps data and benchmarks and vertical datums to which the data are vertically referenced it is expected that by combining monte carlo simulation with the corrected dems and error estimates the approximations of the true elevations being modeled are more reliable for end users needs 1 5 objectives the main objective of this study is to develop an object based lidar correction approach for generating corrected dems to complement the existing bias correction methods by combining obia and machine learning modeling techniques the specific objectives are to 1 explore the pros and cons of object based machine learning lidar correction method compared with the current grid based bias correction methods 2 examine the potential benefits of object based elevation data over the grid based raster dems and 3 adjust for the effect of uncertainty in the final corrected dem deliverables and produce maps where the likelihood of object based corrected dem uncertainty can be observed 2 study area and data 2 1 study area the study area is in the southern coastal region of everglades national park enp fig 1 we surveyed two sites of this area for collecting rtk gps data in this study the first site is located just below the nine mile pond turnoff from main park road hereinafter referred to as nine mile 1 km2 in area this site is historically a freshwater marsh dominated by sawgrass cladium jamaicense the second site is in the southernmost headquarters of enp and includes the flamingo visitor center hereinafter referred to as flamingo 3 km2 in area this site is dominated by high density tall stands of trees taller than 5 m including red mangrove rhizophora mangle black mangrove avicennia germinans white mangrove laguncularia racemosa and buttonwood conocarpus erectus during the survey the ecological impact of hurricanes could be seen within the plant communities at each study site in nine mile low density trees with heights less than 5 m are pervasive including red mangrove and buttonwood this is because winds from hurricanes donna 1960 and betsy 1965 resulted in the establishment of red mangrove rhizophora mangle seedlings in nine mile thus the red mangrove does not require its intertidal habitat to survive lodge 2010 in flamingo the inland movement of mud during hurricanes resulted in a patchwork of coastal prairies where dominant species include saltwort batis maritima glasswort salicornia spp and saltgrass distichlis spicata all generally less than 1 m tall the soils for nine mile are dominated by sawgrass peat deposits while the soils for flamingo vary spatially and are dominated by mangrove peat deposits and hurricane deposited marl both study sites are underlain by highly permeable miami limestone which makes up part of the biscayne aquifer water flows towards the southwest where extensive freshwater diversions due to current water management practices coupled with rising sea levels are threatening the stability of the coastal peat marshes 2 2 data data sources used in this study include rtk gps ground elevation data for correcting lidar elevations lidar for building object based corrected dems and aerial imagery for generating image objects for our rtk gps survey data collection a reconnaissance was performed to identify the survey sites and existing survey controls survey sites were selected based on roadside access to different vegetation communities after the reconnaissance we collected rtk gps elevation data in february and march 2016 the dry season of south florida november may to be seasonally consistent with the acquisition of lidar and aerial imagery in the data collection existing survey controls were chosen based on their vicinity to our study sites unobstructed view of the sky accessibility and horizontal and vertical orders that affect the overall error of the survey three national geodetic survey ngs benchmarks were used in the field ngs tidal benchmark pid ab2404 ngs benchmarks pid ac4648 and pid ac4659 which were reported to have a horizontal and vertical classification of first order class ii with a maximum elevation difference error of 0 07 m 1 standard deviation or σ https www ngs noaa gov datasheets the leica 1200 system was utilized with reported real time errors of 0 01 m in the horizontal and 0 02 m in the vertical rmse at 68 confidence interval leica geosystems 2008 the leica 1200 base receiver was set up over the ngs and tidal benchmarks a topo shoe was fitted to the survey rods of the moving receivers or rovers to prevent sinking into the peat soils elevations were surveyed near low tide with nominal precisions 0 01 m in horizontal and 0 03 m in vertical positions rmse at 68 confidence interval survey data were collected for each dominant vegetation community to provide a representation of the overall population and with a separation radius greater than or equal to 2 m in total we collected 483 rtk gps ground elevations 132 for nine mile 351 for flamingo which were vertically referenced to north american vertical datum of 1988 navd88 using geoid 12a the current best available lidar data for the study area were collected by the florida division of emergency management in february 2008 using the leica als50 airborne laser scanner system which was reported to have a horizontal error of 0 07 0 64 m 1 standard deviation or 1 σ and vertical error of 0 08 0 24 m 1 σ after post processing leica geosystems 2007 the data are available at national oceanic and atmospheric administration noaa s data access viewer https coast noaa gov dataviewer although intensity images are also available they were not used in this study for model simplicity the average lidar point density reported is 2 points m2 for unobscured areas no trees or buildings the vendor filtered the lidar point cloud into classified ground bare earth and non ground returns vegetation structures and buildings rtk gps trimble 4700 and trimble 4000 series and total station surveys were also conducted by the vendor for lidar accuracy assessment in july 2007 for nine mile and in december 2007 for flamingo which were also considered in this study since both the lidar and vendor s survey data were vertically referenced to the navd88 using geoid 03 we transformed their lidar and rtk gps data to be consistent with our rtk gps data using a vertical datum transformation tool from noaa https vdatum noaa gov fine spatial resolution aerial photography with three spectral channels red green and blue is available for both sites the nine mile aerial imagery was collected in january 2016 with a spatial resolution of 0 25 m while the flamingo imagery was collected in january 2012 with a spatial resolution of 0 30 m we used the aerial imagery to produce image objects for conducting object based lidar correction and dem modeling 3 methodology 3 1 framework of the object based machine learning lidar correction and dem generation we designed a framework for conducting object based machine learning lidar correction and dem generation as shown in fig 2 in the framework image objects were produced first using an image segmentation approach then lidar statistical metrics were extracted for each object to be used for ground elevation modeling and prediction field rtk gps elevation data were spatially matched at the object level to the lidar measurements to be used for model development a training dataset was used to fine tune the parameters of four machine learning regression algorithms rf svm k nn and ann before making predictions on the new lidar measurements to generate the object based lidar corrected dems an independent test dataset was then used to quantitatively compare the object based lidar corrected dems that were then combined with monte carlo simulation to account for the uncertainty in the underlying data the major steps in the framework include image segmentation data matching machine learning modeling lidar correction and dem generation accuracy assessment and monte carlo uncertainty simulation these steps are provided in the following subsections 3 1 1 image segmentation we generated objects from the aerial imagery using the multiresolution segmentation algorithm in ecognition developer 9 3 benz et al 2004 trimble 2017 the algorithm first segments individual pixels of an image before merging neighboring segments together until a heterogeneity threshold is reached benz et al 2004 the heterogeneity threshold is determined by user defined parameters including scale color shape and smoothness compactness weights the scale is an arbitrary value allowing for a relative comparison between different scales the scale determines the size of objects where a smaller scale value produces smaller homogeneous objects and a larger scale value produces larger heterogenous objects in this study a smaller scale is reasonable for restoration planning and sea level rise mapping which requires highly detailed elevation products to help identify an optimal scale for generating the best prediction model we tested three segmentations using scale parameters ranging from 20 to 40 at an interval of 10 the color shape parameter was set to 0 9 1 0 for all three bands so that spectral information was weighted most heavily for segmentation the smoothness compactness parameter was set to 0 5 0 5 for all three bands so that compact and non compact segments were favored equally after segmentation the lidar elevation statistical metrics minimum mean maximum standard deviation range and count or number of lidar points were extracted for each image object to be used as independent variables for ground elevation modeling and mapping objects without lidar measurements were dropped for further analysis the lidar statistical metrics were derived using the original lidar point elevations which have proven more valuable than the application of a raster layer generated from lidar data for extracting lidar measurements zhang et al 2011 2014 3 1 2 data matching it is common for researchers to match the observed rtk gps measures with individual lidar dem grid cells to calculate correction factors in this study we spatially matched our rtk gps data 132 for nine mile 351 for flamingo with the lidar measurements at the object level we expected two advantages from the object based matching scheme first it can reduce the uncertainty of positional discrepancy between lidar measures and field rtk gps data second a pure object region is more representative for the plant structure and or dem than any individual grid cell within this object region obia offers the capability to match the field rtk gps data to relatively homogeneous objects with a varying shape and size rather than a grid cell in which plants might be heterogeneous in structure 3 1 3 object based nonparametric machine learning modeling and lidar correction and dem generation in this study we examined four nonparametric machine learning algorithms rf svm k nn and ann for ground elevation modeling with rtk gps data as the dependent variable and lidar measurements as the independent variables rf is an ensemble learning technique developed by breiman 2001 to improve the classification and regression tree method advantages of rf include its speed and capability to deal with complex relationships between predictors but it requires many training samples that should be representative belgiu and drãguţ 2016 svm is a statistical learning approach that looks for an optimal hyperplane to minimize training errors an advantage to svm is its capability to produce improved estimations based on a small number of training samples but parameter issues may arrive with this algorithm mountrakis et al 2011 k nn is an instance based learning method that searches for the best match to denote inputs an advantage of k nn is its capability to make local approximations while simultaneously solving multiple problems and dealing with changes in the problem domain however it requires the selection of an appropriate distance metric to combine neighbors for predictions chirici et al 2016 ann models data based on the structure of neurons and the synapses of human brains ann can generalize in a noisy environment but it may be difficult to interpret because the inner workings are like a black box mas and flores 2008 since each model has its own pros and cons the potential of each for lidar correction has been explored in this study the 132 matched samples for nine mile and 351 matched samples for flamingo were randomly split into two datasets with one to train the machine learning models used to correct the lidar measurements before generating dems and the other to test the predictive performance of all finalized models used to generate corrected dems the national digital elevation program ndep 2004 recommends a minimum of 30 checkpoints to access the vertical accuracy of elevation data we chose to randomly select a total of 30 samples as test data samples used to assess the vertical accuracy of the corrected dems for the nine mile and flamingo study areas respectively the remaining samples 102 for nine mile 321 for flamingo were used to train and fine tune the models parameters to help reduce overfitting training time and improve model accuracy automatic attribute selection is useful for determining the most relevant explanatory variables the supervised attribute selection filter in weka version 3 8 hall et al 2009 was used to select relevant explanatory variables for use in model construction the filter evaluates the value of a subset of explanatory variables by considering the individual predictive ability of each variable along with the degree of redundancy between them hall 1998 the relevant explanatory variables for the nine mile object based dataset were all lidar measurements except standard deviation and range and the relevant explanatory variables for the flamingo object based dataset were lidar minimum and count we employed weka version 3 8 hall et al 2009 where the training datasets were used to fit the four machine learning regression algorithms over different tuning parameters using resampling techniques the rf algorithm breiman 2001 was utilized where the following tuning parameters were defined 1 the number of decision trees in the forest and 2 the number of randomly selected variables subsetted from the total number of variables that are used for each node split in a tree the kernel precision and penalty parameters were adjusted when using the svm algorithm shevade et al 1999 the k nn algorithm aha and kibler 1991 was evaluated by fine tuning distance measures weighted functions and k value parameters the parameters fit for the ann multilayer perceptron algorithm mas and flores 2008 were the learning rate and number of hidden layers and training cycles repeatability was achieved by using the same sequence of random numbers where the seed was set to 123 for all applicable model runs for each of the four machine learning techniques the tuning parameters were chosen based on trial and error using the lowest rmse provided by 10 fold cross validation the finalized models were then utilized to make predictions on new lidar measurements using arcgis version 10 4 http www esri com data management tools the prediction attribute features were spatially joined to the object based features based on their spatial relationship to generate the object based lidar corrected dems 3 1 4 accuracy assessment the test data were used to produce quantitative assessments of the final corrected dems to compare between models choose which models produced the best results and perform monte carlo simulations the association between each corrected dem s model predicted p values and observed o rtk gps measures were evaluated using several correlation difference and summary measures one type of correlation measure was calculated the correlation or degree of association between p and o was expressed as pearson s coefficient of correlation r three types of difference measures were calculated estimates of the average error were described as the mean absolute error mae and rmse any systematic error that makes all estimates off by a certain amount or bias was described as the mean bias error mbe several summary measures were then calculated these include the mean μ and standard deviation σ these indices were calculated by 1 r i 1 n p i p i o i o i i 1 n p i p i 2 i 1 n o i o i 2 2 m a e i 1 n p i o i n 3 r m s e i 1 n p i o i 2 n 4 m b e i 1 n p i o i n where p i is the model predicted value p i is the mean of the model predicted values o i is the observed value o i is the mean of the observed values n is the number of matched test data samples and i is an integer from 1 to n 3 1 5 monte carlo uncertainty simulation on corrected dems before we can adjust for the effect of uncertainty in the final corrected dem deliverables and produce maps where the likelihood of corrected dem uncertainty can be illustrated the vertical accuracy and error in the underlying data along with their distributions needs considering these uncertainties included the following 1 vertical datums and any transformations made between them 2 rtk gps measurements 3 vertical benchmarks used in survey and 4 the model predictions for each object in the object based corrected dems first the errors associated with the vertical datum to which the data are referenced were obtained from noaa 2013 the σ is used to quantify uncertainties for vertical datums where noaa considers a constant σ of 5 cm for navd88 nationwide no transformations between datasets were needed for this study to include transformation errors second the rtk gps measurements contained a vertical rmse 0 03 m rmse 1 σ while the vertical benchmarks to which the surveys were based is 0 07 m 1 σ it is assumed that the individual uncertainties are independent where the value of one measurement does not affect the value of the other measurement and are randomly distributed following a normal distribution this allows for the total uncertainty of the rtk gps measures and vertical benchmarks to be calculated by the root sum of squares as the maximum cumulative uncertainty mcu σ 5 m c u σ σ 1 2 σ 2 2 σ n 2 finally the accuracy and error in the corrected dems were calculated in the previous section which are needed for the monte carlo simulation to adjust for the uncertainty related to the underlying data monte carlo simulation is applied to our corrected dems having a single output quantity more reliable elevation where the input quantities survey vertical datums and model predictions are characterized by any specific probability distribution pd we did not apply filtered error fields in the procedure e g wechsler and kroll 2006 enwright et al 2018 instead unfiltered error fields were considered as a worst case scenario of the effects of corrected dem uncertainty wechsler and kroll 2006 we reduce the equation by cooper and chen 2013 to consider the uncertainty related to our corrected dems 6 d e m r e l i a b l e d e m s u r v e y d e m d a t u m d e m e r r o r d e m x y n where dem reliable is a final object based corrected dem value that is more reliable provided the uncertainty in the underlying data d e m s u r v e y is a random variable sampled proportional to the rtk gps measures and vertical benchmarks survey data distribution d e m d a t u m is a random variable sampled proportional to the vertical datum distribution d e m e r r o r is a random variable sampled proportional to the model predictions and dem x y is a constant elevation z value of an object at an x y location the sampling procedure is repeated a total of 1000 cases for each object an illustration of the monte carlo based elevation uncertainty model is shown in fig 3 to produce maps that spatially illustrate the likelihood that a corrected dem is in error it is assumed that the true elevation is best represented by the corrected dem derived from the most accurate model in this way the likelihood that a corrected dem value is in error is based on the probability that the corrected dem value deviates from the true elevation monte carlo simulation is applied to our object based corrected dems having a single output quantity likelihood of error where the input quantities survey vertical datums and model predictions are characterized by any specific probability distribution pd using the following equation 7 l i k e l i h o o d e r r o r d e m s u r v e y d e m d a t u m d e m e r r o r d e m x y d e m s u r v e y d e m d a t u m t r u e e r r o r t r u e x y n where the difference from equation 6 is that likelihood error is the probability an object based corrected value deviates from the t r u e elevation t r u e e r r o r is a random variable sampled from the most accurate object based corrected dem and true x y is a constant elevation z value at an x y location derived from the most accurate object based corrected dem although any ranking scheme can be utilized we chose to generalize the likelihood of error values using the following ranking scheme where object based probability values ranging from 0 to 0 39 are assigned equal to low object based probability values ranging from 0 4 to 0 59 are assigned equal to medium and object based probability values ranging from 0 6 to 1 are assigned equal to high likelihood of error the generalized object based likelihood layers are then used to calculate total land area susceptible to error for each ranking scheme low medium high 3 2 grid based nonparametric machine learning modeling and lidar correction and dem generation for comparison purposes we also conducted the grid based lidar correction and dem generation the process was first implemented in python version 2 7 10 https www python org using arcgis arcpy python site package to create an easy workflow first three grids with resolutions ranging from 1 to 3 m at an interval of 1 m were generated the minimum resolution was chosen based on the reported lidar average point density of 2 pts m2 the binning technique was then utilized rosso et al 2003 schmid et al 2011 medieros et al 2015 buffington et al 2016 to assign a grid cell s center point the respective lidar statistic minimum mean maximum standard deviation range count when more than one lidar attribute falls within that cell if only one lidar attribute falls within a cell that attribute is assigned to that cell null assigned to no data due to the nature of the landscape consisting of dense coastal vegetation and surface water with dark peat soils below making lidar measures sometimes difficult certain cells contained null values the results were a total of six raster datasets for nine mile and six raster datasets for flamingo each containing lidar minimum mean maximum standard deviation range and count per respective grid cell the relevant explanatory variables for the nine mile grid based dataset were all lidar measurements except standard deviation range and count and the relevant explanatory variables for the flamingo grid based dataset were simply the lidar mean and minimum the process presented in section 3 1 3 above was then repeated to generate grid based corrected dems 3 3 bin and bin bias correction dem generation since lidar ground returns are known to overestimate coastal marsh elevations the minimum bin technique hereinafter simply referred to as bin seems more reasonable than interpolation methods for our two study sites when comparing the bias correction procedure with the four machine learning regression algorithms here the grid based and object based datasets containing the minimum lidar value are referred to as the grid based and object based bin it is common for researchers to calibrate grid based lidar dems with bias correction factors schmid et al 2011 hladik and alber 2012 mcclure et al 2016 for comparison purposes the grid based and object based bin datasets were also calibrated by the mean bias error mbe calculated from the training datasets 102 for nine mile 321 for flamingo these datasets are referred to as the grid based and object based bin bias correction 4 results 4 1 grid based vs object based corrected dems to compare the bias correction method with machine learning modeling descriptive statistics are displayed in table 1 in reviewing the summary position parameter or average of predictions p the bin and bin bias correction techniques tend to systematically overestimate the corresponding observed average of observations o to a higher degree than the machine learning techniques with one exception the flamingo object based scale 30 bin dataset suggests that p and o are the same table 1 therefore the bias correction technique cannot be applied to the flamingo object based scale 30 bin because the difference parameter mbe is zero machine learning techniques provide a valuable alternative to the bias correction approach when lidar data contains no systematic bias i e mbe 0 0 and is less unprecise e g σ 0 23 m the difference measures mae rmse and mbe help to better determine which modeled grid based 3 m resolution and modeled object based scale 30 datasets are preferred table 1 in terms of the mae and rmse for nine mile a meaningful distinction can be made between the bin bias based and machine learning experiments the mae and rmse are lowest best for the object based and grid based machine learning experiments at the nine mile study site in reviewing the mbe the machine learning experiments also reduce the bias when compared to the bias correction procedure further examination of r indicates that the nine mile grid based 3 m rf has a slightly higher degree of association between p and o r 0 76 when compared to the object based scale 30 rf r 0 70 overall the rf algorithm performed best for the nine mile grid based and object based datasets when compared to the bias correction technique in terms of the mae and rmse for flamingo a slightly different distinction can be made between the bin bias correction and machine learning experiments the mae and rmse are lowest best for the object based machine learning experiments where rf performs best overall at the flamingo study site however the grid based machine learning experiments did not perform as well as the grid based bin bias correction experiment at the flamingo study site the examination of r indicates that the flamingo object based scale 30 rf has a slightly higher degree of association r 0 95 when compared to the grid based bin bias correction r 0 88 overall the object based rf algorithm performed best compared to all other experiments for the flamingo study site we also compared the rtk gps measurements with the uncorrected lidar ground return elevations within an object based or grid based dataset table 1 the lidar systematically overestimates the ground for both study sites for nine mile the uncorrected lidar within an object demonstrated a lower mbe of 0 18 m and rmse of 0 22 m when compared to the uncorrected lidar within a grid mbe 0 24 m rmse 0 27 m although the standard deviation of 0 13 cm is the same for both the object based and grid based datasets the rmse may be more useful because it demonstrates that large errors are present in the mae this is because the rmse calculation assigns higher weights to larger errors by first squaring the errors before taking their average for flamingo no meaningful distinction can be made between the uncorrected lidar within an object mbe 0 17 m rmse of 0 24 m when compared to the uncorrected lidar within a grid mbe 0 17 m rmse 0 22 m 4 2 grid based and object based dem mapping the corrected dem maps were derived by applying the machine learning models to the object based scale 30 and grid based 3 m data for each of the two study sites this provided the opportunity to incorporate the uncertainty in the underlying data into the final corrected dem products using equation 6 with monte carlo simulations see figs 4 and 5 in the maps red represents higher elevations above navd88 and blue represents lower elevations below navd88 all in meters for both study areas the object based corrected dem maps qualitatively do a better job at estimating unknown locations when compared to the grid based corrected dem maps this is illustrated by the missing data values shown in white that are more prevalent in the grid based corrected dem maps the flamingo study site also has more topographic variation than the nine mile study site allowing for better visual examination of the object based and grid based experimental results the corrected dem maps show that the object based approach provides a more detailed account of the ground elevation values when compared to the grid based corrected dem maps see fig 5 for more variation in the graduated colors the corrected dem maps also demonstrate that the object based corrected dems are more representative of the landscape 4 3 uncertainty mapping relative to the best model the final uncertainty maps that illustrate the probability that an object based corrected elevation value is in error from the true elevation are shown for the nine mile and flamingo study areas in figs 6 and 7 respectively although the nine mile and flamingo object based rf corrected dems do not represent an unbiased result they were considered as the true elevation because they achieved the best result when compared to all other object based machine learning models the nine mile and flamingo object based rf corrected dems were used to calculate the likelihood of corrected dem uncertainty using equation 7 in the maps the likelihoods that svm k nn and ann object based corrected elevations are in error from the true elevations i e rf object based corrected elevations are represented by a ranking scheme of blue for low green for medium and red for high chance of error although the nine mile maps in fig 6 illustrate that the likelihood of error varies from each experiment it is difficult to qualitatively assess the preferred approach from these maps alone on the other hand the flamingo maps shown in fig 7 clearly illustrate the ann object based corrected dem has the lowest likelihood of error while the k nn object based corrected dem has the highest likelihood of error table 2 compliments the uncertainty maps by providing a quantitative assessment of the total area that is in likelihood of error for nine mile the ann object based corrected dem contains the largest area with a low likelihood of uncertainty 20 of total land area and smallest area with a high likelihood of uncertainty 12 of total land area making it the preferred approach overall the uncertainty maps and complimentary table help dem users to qualitatively and quantitatively assess the uncertainty of an object based corrected elevation 5 discussion and conclusions 5 1 machine learning regression for lidar correction after conducting a thorough literature review we found one recent study that used machine learning modeling in marshes to develop a lidar correction approach rogers et al 2018 these authors recommended that this type of modeling be extended to other regions with different vegetation types to examine its robustness in this study the potential of four machine learning regression algorithms rf svm k nn and ann were examined to correct lidar elevation data in the coastal everglades for two study sites 1 nine mile dominated by sawgrass marsh and mangrove swamp and 2 flamingo dominated by mangrove swamp and coastal prairies all algorithms produced an acceptable degree of association between the predicted and observed values where the coefficient of correlation r is over 0 55 using object based modeling rf performed best for the nine mile object based modeling r 0 70 where the uncorrected lidar mbe was reduced from 0 18 to 0 02 m mae was reduced from 0 19 to 0 05 m standard deviation σ was reduced from 0 13 to 0 07 m and rmse was reduced from 0 22 to 0 08 m rf also performed best for the flamingo object based modeling r 0 95 where the uncorrected lidar mbe was reduced from 0 17 to 0 02 m mae was reduced from 0 17 to 0 06 m standard deviation σ was reduced from 0 17 to 0 10 m and rmse was reduced from 0 24 to 0 10 m although the vegetation communities explanatory variables of importance and number of rtk gps are not the same our results are comparable with the study by rogers et al 2018 where rf performed best for discrete lidar where all marsh vegetation mbe 0 01 cm standard deviation and rmse 0 11 cm based on this study and the study by rogers et al 2018 the rf algorithm is robust when correcting discrete lidar elevation data for the two regions with their own unique vegetation communities the rf algorithm seems to handle well datasets that do not follow a gaussian distribution contain a bias and are highly spatially autocorrelated which is typical of marsh environments this is likely due to the nature of the rf algorithm injecting randomness in the variables and averaging over many predictions to get an unbiased model result with low variance see criminisi et al 2011 for a review on rf however rf is not without weaknesses when using rf for regression the model does not do a good job at predicting beyond the range of the training data which may have an impact on our results depending on how well the lidar and rtk gps measurements represent the overall population since it is known that dems generated using interpolation results in additional error and due to the nature of the lidar overestimating the true ground due to vegetation and open water we decided against using interpolation instead a binning procedure to correcting lidar elevations known as the minimum binning bin approach was utilized because it has shown to reduce bias and error in marsh environments when compared to interpolation rosso et al 2003 schmid et al 2011 medeiros et al 2015 buffington et al 2016 a few case studies have also demonstrated that the bias correction technique does a satisfactory job at improving the bias e g hladik and alber 2012 mcclure et al 2016 it was anticipated that by combining the bin and bias correction techniques into a bin bias correction object based procedure the bias and error could be further improved for comparison with the machine learning object based modeling however this was not the case for our nine mile study site where the bin bias correction object based dataset mbe was slightly increased from 0 03 to 0 04 cm and the change in the σ and rmse was negligible 0 17 0 16 cm when compared to the bin object based dataset rogers et al 2016 pointed out that while the bias correction technique typically does well in correcting for bias machine learning techniques provide a better option for error removal in this study the rf object based modeling improved bias and significantly lowered error when compared to the bin and bin bias correction techniques 5 2 grid based vs object based lidar correction and dem generation researchers have focused on correcting lidar dems at the grid level e g schmid et al 2011 hlaldik and alber 2012 mcclure et al 2016 buffington et al 2016 rogers et al 2016 a thorough literature review revealed that no other efforts have been made for applying obia in object based lidar correction nor applying obia in modeling lidar dems at the object level the object based lidar correction approach developed and tested in this study successfully combines the spatial features of fine spatial resolution aerial imagery 0 30 m with low posting density 2 pts m2 lidar elevation measurements making it an attractive alternative to the commonly used grid based method the object based technique s ability to take advantage of spatial features within an image to define an object patch and predict more area where lidar is sparse is attractive see figs 4 and 5 this reduces the need for dem void filling procedures the object based approach can also reduce positional discrepancy between the image and rtk gps measurements because an object represents a vegetation patch better than a single grid cell fig 8 the object based mapping and modeling reduces local noise in heterogeneous wetland environments by averaging the spectra of all pixels within an object dronova 2015 in the everglades vegetation has a high spatial and spectral heterogeneity the heterogeneity threshold is determined by a user defined scale parameter a smaller scale value produces smaller homogeneous patches of vegetation while a larger scale value produces larger heterogenous patches additionally a species typically presents a range of elevation uncertainty opposed to a constant rogers et al 2016 so it is unlikely that an entire vegetation species would need a constant correction using the bias correction technique rogers et al 2018 the capability of the object based lidar correction technique to reduce the spatial bias and uncertainty within each individual object patch successfully addresses the issue where the assumption is made that the accuracy and error is the same for an entire vegetation community the inherent structure of vector data makes the object based corrected dems attractive and convenient when manipulating and performing operations on elevation data the vector object based mapping in our case produced smoother elevation gradients and was geographically more accurate because an object is represented by the line features of a vegetation patch whereas the raster grid based mapping is represented by an artificial uniform grid size fig 8 the mixed pixel problem is most apparent in the 2 m grid because it is assumed that a grid cell covers a homogeneous vegetation patch since the aerial imagery have a fine spatial resolution 0 25 0 3 m it is easier to interpret homogenous vegetation patches as objects from the fine resolution images fig 8 additionally object based elevations are stored as attribute tables allowing for flexibility in data manipulation such as joining tables since both data management and the integrity of object based corrected elevations is maintained through topology rules network and proximity operations are performed more efficiently for example network based analytical tools used for solving complex routing issues in a gis can easily be applied to object based corrected dems to identify thresholds of dynamical marsh system response to water and nutrient fluxes proximity based analytical tools can also be easily applied to an object based corrected dem to select by attributes and identify proximity to other features such as the shoreline these vector based operations are especially useful for assessing the impacts of sea level rise and storm related events on natural and human coastal systems e g cooper et al 2015 although the object based technique provides some benefits in elevation mapping and modeling it is not without its weaknesses here we identify three limitations related to the scale parameter vector data structure and processing time first the scale parameter used for generating the size of objects can impact the object based modeling results a small scale produces homogenous objects while a large scale produces more heterogenous objects in this study we tested several scale parameters by trial and error although methods exist for optimizing the scale parameters in image segmentation e g johnson and xie 2011 we did not apply them in this study these techniques may help improve the results second spatial analysis tools and filtering methods require raster data to perform these operations the nine mile object based corrected dem would need to be converted to a raster using the original 0 25 m resolution aerial imagery as its grid size and processing extent to maintain the integrity of the object based corrected elevations although this requires an additional step converting an object based corrected dem to a grid based corrected dem that matches the fine resolution imagery used to generate the objects may be useful for fine scale mapping of habitat and elevation loss due to the conversion of coastal peat marshes to inland open water from sea level rise i e peat collapse third the object based machine learning technique has a high computational intensity when compared to the mb bias correction method although the object based corrected dems provided the best results the bin bias correction method may be more attractive to researchers that require a technique with a lower computation intensity finally it is interesting to re note that although the mae and rmse are lowest best for the object based machine learning experiments at the flamingo study site the grid based machine learning experiments did not perform as well as the grid based bin bias correction we speculate that this may be due to the vegetation the flamingo study site is dominated by high density tall stands of mangroves taller than 5 m making it difficult for the laser to penetrate to the ground there are a limited number lidar classified ground returns within each 3 m grid cell in this study the machine learning techniques may produce better predictions for an object because there are generally more lidar returns within an object when compared to a grid cell 5 3 monte carlo corrected dem uncertainty mapping past research focuses on lidar dem correction without including the effects of uncertainty in the corrected dems e g schmid et al 2011 hladik and alber 2012 medieros et al 2015 mcclure et al 2016 buffington et al 2016 since vertical inaccuracies and or errors remain in our corrected dems we used monte carlo simulation to adjust for these effects in the final corrected dem products and produced maps where the likelihood of error can be observed on an object by object basis researchers utilizing a sea level rise bathtub approach that does not consider uncertainty in the underlying data e g zhang 2011 would benefit from more reliable delineations of the inundation zones based on a corrected dem that has been further adjusted by any underlying errors the uncertainty maps and complimentary table allow dem users to effectively evaluate which regions are prone to error for their intended application for example a researcher investigating freshwater marsh elevation loss due to saltwater intrusion may find the uncertainty maps useful in guiding additional field campaigns where it is more challenging for the remotely sensed data and machine algorithms to accurately predict elevations monte carlo simulation has been widely used in mapping and modeling of coastal marsh environments however there has been limited application of this technique in adjusting for the inaccuracies and or errors in corrected dems 5 4 error sources in lidar correction and dem generation many sources of error impact the elevation estimation such as upscaling rtk gps data to objects the geoid models and transformations relative the vertical datum to which the data are referenced and time gaps between rtk gps lidar and aerial imagery acquisition in our study the rtk gps were upscaled from a single x y point to an entire object s area the rtk gps measurements were matched to the image objects of varying shapes and sizes the spatial resolution of the aerial imagery for nine mile was 0 25 m and the minimum size of each object was 0 25 m2 while the spatial resolution of the aerial imagery for flamingo was 0 30 m and the minimum size of each object was 0 09 m2 therefore it was assumed that the rtk gps measures represented the elevation of the entire object also the lidar data were transformed from navd88 using geoid 03 to navd88 using geoid 12a we considered a constant error for navd88 nationwide noaa 2013 in the modeling however it is not clear how transforming between geoids may impact this error this may lead to more uncertainties in the corrected dems finally there were significant time gaps between the acquisition years of the current best available lidar aerial imagery and rtk gps field measurements variability in the vegetation and topography is largely controlled by season hurricanes and water management practices south florida s dry and rainy seasons will have an impact on vegetation growth and sediment shrinkage and swelling all data utilized in this study were collected between january march during the rainy season sediment accumulation will also have a minimal impact on elevation change between 2008 and 2016 because the elevation change is within the vertical error of 0 08 0 24 m of the lidar system leica geosystems 2007 for example elevation change for everglades mangroves was measured as 0 37 0 12 mm yr cahoon and lynch 1997 the maximum elevation change 0 37 0 12 0 49 mm multiplied by the maximum number of differences in years 2016 2008 8 yrs or 0 49 mm 8 yrs 3 92 mm this 3 92 mm or 0 392 cm elevation change is within the lidar system s vertical error a similar calculation can be made for everglades sawgrass where elevation change was measured as 0 32 0 04 mm yr craft and richardson 1993 it should also be noted that no hurricanes hit the study area between 2008 and 2016 which would likely have had an impact on soil deposition and erosion and vegetation structure 5 5 orientation of future works although the developed object based lidar correction approach was successful at estimating accurate and precise elevations in this study the authors identify several suggestions for improvement of future works the developed object based lidar correction approach was tested on two sites consisting of coastal marsh swamp and prairie additional research is needed in other regions with coastal marsh and woody environments to examine the robustness of this technique we tested several independent variables in the predictive modeling including six lidar elevation statistical metrics minimum mean maximum standard deviation range and count future works should explore data fusion techniques that integrate multiple data sources in elevation correction for example additional object based spatial features such as spectral features texture and inclusion of more lidar features such as intensity statistics and optical image features can be extracted for each object which may have potential to improve the object based lidar correction we only tested the aerial imagery in the segmentation process when generating objects however the multiresolution segmentation algorithm in ecognition can also generate objects by integrating the lidar point cloud with aerial imagery this may be advantageous when generating larger image objects by accounting for elevation gradients that may be observed in other areas but this needs exploring additionally we determined the rf object based model performed best for this study however ensemble analysis of comparable models may make the predictions more reliable than using an individual model this should be explored in future works in elevation correction it should be noted that we did not consider the spatial structure of error e g holmes et al 2000 in the monte carlo modeling which has potential to improve the reliability of the final corrected dem products this requires additional rtk gps measurements used in testing and needs exploring we only compared errors spatially for each object based machine learning approach to the best model as a reference restoration managers and planners may find useful the spatial distribution of errors for the best model this also requires additional rtk gps measurements and should be explored in future works finally with the advancement of unmanned aerial vehicles uavs data acquisition in coastal environments klemas 2015 and special approval by the national park service simultaneous collection of all data sources during the same year and season is more achievable and may improve the results development of an object based uav derived measurements approach has potential in elevation correction 5 6 conclusions in this study we developed an object based lidar correction approach to modeling sawgrass marsh mangrove swamp and coastal prairie elevations by combining object based image analysis obia machine learning and monte carlo techniques the results suggested that this new approach was promising for elevation modeling and mapping compared to the bias correction and grid based modeling techniques the authors draw the following conclusions from this study machine learning regression techniques are effective at generating accurate and precise object based digital elevation models dems four machine learning regression techniques were evaluated with the bias correction technique random forest rf support vector machine svm k nearest neighbor k nn and artificial neural network ann rf achieved the best result for the nine mile object based modeling where the mean bias error mbe was reduced from 0 18 to 0 02 m standard deviation σ was reduced from 0 13 to 0 07 m and root mean square error rmse was reduced from 0 22 to 0 08 m rf also performed best for the flamingo object based modeling where the mbe was reduced from 0 17 to 0 02 m σ was reduced from 0 17 to 0 10 m and rmse was reduced from 0 24 to 0 10 m the object based lidar correction approach provides an attractive alternative to the commonly used grid based modeling the object based approach has potential to reduce positional discrepancy between an image object and rtk gps measurement because an object better represents a vegetation patch than a single grid cell the capability of the object based modeling to reduce the spatial bias and uncertainty on an object by object basis successfully addresses the issue where the bias correction technique assumes spatial bias and uncertainty is the same for an entire vegetation community monte carlo simulation is valuable for modeling and mapping the uncertainty in corrected dems the monte carlo modeling technique can make the estimations of the true elevations more reliable for an end user s needs because it combines the bias and uncertainty related to the underlying data into the final corrected dem products additionally monte carlo can provide error maps and supportive tables to compliment the model accuracy assessment the object based lidar correction approach has potential to better assist future research in generating more accurate and precise dems the robustness of these techniques should be explored in other applications e g water table elevation model wtem generation and ecosystems and urban areas with the advancement of uav data acquisition and derived measurements simultaneous collection of all data sources is more achievable and may improve the object based corrected dem results and applications future research will explore the potential of object based machine learning techniques to investigate coastal peat marsh loss by monitoring changes in small homogenous vegetation patches and elevations due to sea level rise it is also anticipated that the object based correction technique will become more popular in dem generation declaration of interest none acknowledgements this research was funded by the everglades foundation foreverglades fellowship and the american association of geographers mel marcus fund for physical geography the opinions is this research do not necessarily reflect those of the everglades foundation or aag we also thank the helpful and friendly staff at everglades national park this material was developed in collaboration with the florida coastal everglades long term ecological research program under national science foundation grant no deb 1237517 appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 11 003 
