index,text
26215,watershed models are essential tools to understand quantify and predict hydrologic processes and water quality responses from scales ranging from field to large river basins however the reliability of watershed models in a management context depends largely on inherent uncertainties in model predictions the objective of this study is to quantify prediction uncertainty for flow sediment total nitrogen tn and total phosphorus tp resulting from model structure we do this using using three process based models the soil and water assessment tool variable source area model swat vsa the standard soil and water assessment tool swat st and the chesapeake bay program s phase 6 watershed model cbp model we initialize each of the models using meteorological soil and land use data and analyze outputs of flow sediment tn and tp fluxes at the u s geological survey stream gauge at the downstream end of the susquehanna river basin in conowingo maryland using these three models we develop and compare two types of bayesian models a bayesian generalized non linear multilevel model bgmm and a bayesian model averaging bma for flow sediment tn and tp and 95 credible intervals we compare the bayesian models results against the individual model results and straight model averaging sma using a split time period analysis to assess their predictive strengths both bayesian models provided substantially better predictions than the individual process based models and estimates of prediction uncertainty which can enhance decision making and improve watershed management by providing a risk based assessment of outcomes keywords swat vsa swat standard chesapeake bay watershed model bayesian model ensemble multilevel models 1 introduction ecosystems operate under dynamic and complex interactions of physical chemical and biological processes the complexity of the interactions leads to difficulties in conceptualizing the processes mathematically which makes accurately modeling of ecosystems challenging uncertainty can be classified from two primary sources epistemic and aleatory uncertainty epistemic uncertainty arises from a lack of knowledge or data necessary to estimate probabilities aleatory uncertainty in contrast derives from the inherent randomness in the system e g weather related variation beven 2002 although complex process based models have been built to approximate real world conditions and to assist in effective decision making quantifying structural uncertainties i e uncertainty related to the configuration of a system of watershed models remains a challenge most previous research into model uncertainty has focused on model parameters or input data uncertainties renard et al 2010 and less on structural model uncertainty which defined by breuer et al 2009 and as used in this article is the uncertainty from unknown simplified incomplete or incorrect process descriptions within a model or simplified mathematical forms or expressions of ecosystem processes in the model this type of uncertainty can occur even if the true input values for all model parameters are known sources of structural model uncertainty include exclusion of important controlling variables processes either missing or incorrectly represented or misaligned boundary conditions ascough et al 2008 which can propagate to prediction uncertainty vrugt et al 2005 mäntyniemi et al 2013 yen et al 2014 shen et al 2015 quantifying uncertainties that arise from model formulation and the associated uncertainty bounds would help to make more informed and effective management decisions numerous approaches for quantifying uncertainties in watershed models have been proposed including bayesian model averaging raftery et al 1997 ajami et al 2007 vrugt et al 2008 tasdighi et al 2018 generalized likelihood uncertainty estimation glue vrugt et al 2005 blasone et al 2008 multiple objective function criteria blasone et al 2008 sequential data assimilation moradkhani et al 2005 bayesian recursive estimation thiemann et al 2001 and the ensemble kalman filter abaza et al 2014 however only a few methods such as multi model ensembles mme stoica et al 2004 duan et al 2007 have been proposed for quantifying structural model uncertainties this is due to the complexity and difficulty of separating structural uncertainties from either model parameters or input data uncertainty zhang et al 2011 sharifi et al 2017 evaluated the performance of different watershed models to assess water quality impacts on queenstown drainage stow and scavia 2009 demonstrated improved performance in estimating hypoxia in the chesapeake bay using a hierarchal bayesian ensemble approach but the study was limited to processes within the bay and does not describe upland watershed processes boomer et al 2013 and exbrayat et al 2010 also suggested use of multi model ensembles to improve predictions multi model ensembles mmes combine two or more models in an effort to improve model prediction and objectively evaluate model uncertainty boomer et al 2013 mmes utilize the diversity of skillful predictions from different models and can improve the estimation of uncertainties associated with model outputs duan et al 2007 mmes are commonly used in weather forecasting krishnamurti et al 2000 and climate change analysis christensen and lettenmaier 2006 there are different ways to implement mme approaches ensembles can be used to sample uncertainties in forcing assumptions initial conditions process conceptualization input data and training calibration data generally mmes use multiple models to simulate responses of identical driving forces e g climate inputs where model predictions are aggregated and compared to estimate uncertainty in model predictions for example an mme could be used to evaluate the ability of proposed management practices to achieve a water quality standard distinct solutions would be obtained from multiple models and then be aggregated to obtain an estimated mean nutrient reduction achieved and the probability that their implementation will achieve the desired water quality improvements mme projections have been found to reproduce historical observations more accurately than individual models in a number of fields reichler and kim 2008 and historically the variability in model projections has been used as a measure of uncertainty little research has been carried out to quantify the structural uncertainties in models using mme duan et al 2007 and the application and analysis of ensemble methods remaina challenge in this field raftery et al 2005 proposed bayesian model averaging bma post processing methods to ensemble weather prediction models requiring accurate estimates of the weights and variances of the individual competing models in the ensemble vrugt et al 2008 additionally the bma algorithm does not always provide global convergence vrugt et al 2006 for instance in the bma approach model averaging refers to estimating some quantity from each model and then averaging the estimates based on how likely each model predicts the data by assigning weights for each model wasserman 2000 the bma approach sometimes underperformed compared to simple unweighted averages straight model averaging because it does not assign two equally performing models with the same weights graefe et al 2015 another approach the ensemble kalman filter assumes all probability distributions are gaussian in shape which is suitable for large state variables but problematic for constituents that exhibit non gaussian behavior evensen 2003 mandel 2009 thus for this research we use a new approach a bayesian generalized non linear multilevel models bgmm bürkner 2017 which integrates different probability distributions explicitly incorporates prior knowledge about parameter distribution shape allows predictor variables to be linear or nonlinear and incorporates information from all potential predictors this method offers a more complete quantification of uncertainty rather than primarily attempting to improve prediction skill we develop a bgmm as an ensemble capable of quantifying structural uncertainty of multiple models used to predict watershed responses we employ the hamiltonian monte carlo method using the nou turn sampler nuts extension which has been shown to converge to optimal bayesian coefficients faster and more reliably than other markov chain monte carlo mcmc algorithms betancourt et al 2017 bürkner 2017 structural uncertainties and credible intervals of model predictions for flow sediment total nitrogen tn and total phosphorus tp fluxes at the u s geological survey stream gauge at the downstream end of the susquehanna river basin in conowingo maryland were developed using the bgmm method specifically we 1 initialized and evaluated two watershed models the soil and water assessment tool variable source area swat vsa model easton et al 2008 the swat standard swat st model arnold et al 2012 and obtained outputs from the chesapeake bay program s phase 6 watershed model cbp model 2 developed two bayesian mmes a bgmm and a bma using outputs from the three models as predictors and observed data as the response variable and 3 developed a straight model average sma mme per the recommendation of boomer et al 2013 we evaluate all models performance based on two statistical metrics root mean square error rmse and nash sutcliffe efficiency nse and for the bgmm we developed and evaluated the marginal effects of each model in the mme 2 materials and methods 2 1 study area description the susquehanna river the largest contributor of fresh water to the chesapeake bay drains approximately 42 of the bay watershed and contributes more than 50 of the fresh water flows to the bay ko and baker 2004 and largely controls salinity in the bay gibson and najjar 2000 the basin is approximately 71 000 km2 with elevation ranging from 10 to 960 m and covers parts of three states new york pennsylvania and maryland fig 1 the susquehanna river basin has six major subbasins upper susquehanna chemung middle susquehanna west branch susquehanna juniata and lower susquehanna fig 1 the climate varies along a north south gradient with the northern portion receiving more precipitation 1245 mm than the lower basin 838 mm which experiences hotter and longer summers dephilip and moberg 2010 the land cover of the susquehanna river basin is primarily forest 70 agriculture 22 and developed 7 the major soil texture type is silt loam 70 and the soil depth in most parts of the basin ranges from 0 5 to 2 m ray et al 2016 2 2 watershed models 2 2 1 swat and swat vsa model descriptions the soil and water assessment tool swat model is a process based semi distributed watershed model developed to simulate landscape processes and predict the impact of land management practices on water availability and water quality arnold et al 1998 swat requires meteorological soil land cover and land management data to simulate surface and subsurface hydrology and various chemical nutrient and sediment fluxes in swat the watershed is divided into sub watersheds and then further into hydrologic response units hrus which are unique combinations of soil type and land use soil and water assessment tool variable source area swat vsa re conceptualizes the standard swat to account for areas of the landscape subject to variable saturation dynamics easton et al 2008 in swta vsa the area of each hru is defined by the coincidence of land use and wetness index class determined from a topographic index ti to differentiate areas of the landscape with respect to their moisture storage and saturation index easton et al 2008 swat vsa has been shown to provide better predictions of soil moisture and runoff generation than the standard swat model in watersheds with similar physical characteristics and climate to the study watershed easton et al 2008 2 2 1 1 swat model inputs the susquehanna river swat st and swat vsa models were initialized with 10 m digital elevation models dem obtained from the united states geologic survey usgs national elevation dataset ned guenther and maune 2007 using arcswat 2012 swat tamu edu software arcswat and toposwat fuka and easton 2016 toposwat automates the swat vsa initialization process by assimilating soil data creating the ti map overlaying the soil and ti maps and developing the required database for model initialization soils data included in toposwat are based on the food and agriculture organization fao soils database fao 2007 the land cover of the watershed was obtained from the national land cover database 2011 the most recent national land cover product created by the multi resolution land characteristics mrlc consortium homer et al 2015 the model was forced with daily meteorological data from the climate forecast system reanalysis cfsr including precipitation temperature min and max relative humidity wind speed and solar radiation at the centroids of the six major sub basins the dataset consists of hourly weather forecasts generated by the national weather service s ncep global forecast system and has been successfully used for watershed modeling in different hydrologic settings fuka et al 2014 2 2 1 2 swat model calibration and evaluation both swat vsa and swat st calibration followed a cascading procedure first calibrating all upper basins upper susquehanna chemung west branch susquehanna and juanita sub basins to observed data for their respective monitoring stations next the middle susquehanna was calibrated leaving all parameters values for the upper basins unchanged finally the lower susquehanna subbasin was calibrated leaving all parameter values for the previously calibrated basins unchanged the model was calibrated and evaluated using the swat calibration and uncertainty procedure swat cup arnold et al 2012 and the sufi2 sequential uncertainty fitting optimization algorithm with the objective function set to use the nse the model was calibrated for daily flow sediment tn and tp fluxes from 1985 to 1994 and evaluated from 1995 to 2004 at each of the six sub basins in the watershed wagena and easton 2018 2 2 2 chesapeake bay program s watershed model the chesapeake bay program s phase 6 watershed model cbp model was developed for the 2017 mid point assessment of the chesapeake bay total maximum daily load tmdl it simulates hydrology and fate of nutrients and sediment that contribute to chesapeake bay water quality degradation the hydrological component of the cbp model is based on the hspf model hydrologic simulation program fortran which was derived from the stanford watershed model shenk et al 2012 shenk and linker 2013 boomer et al 2013 hspf uses a mass balance approach to solve a linked set of equations representing natural and anthropogenic mechanisms and lumped parameters and uses hourly metrological data sharifi et al 2017 the cbp model consists of 1063 modeling segments subbasins with the average size of approximately 170 km2 shenk and linker 2013 the model input includes land use meteorology elevation features fertilizer applications wastewater plant discharges septic systems air deposition farm animal populations and other variables to estimate the amount of nutrients and sediment reaching the chesapeake bay epa 2010 for the purpose of this research daily flow sediment tn and tp fluxes data were obtained for the susquehanna river at the conowingo dam from the outputs of chesapeake bay program s phase 6 watershed model the susquehanna model domain was calibrated for flow temperature and water quality variables sediment dissolved oxygen nitrate ammonia nitrogen dissolved orthophosphate phosphorus and chlorophyll from 1985 to 2014 at 101 streamflow monitoring stations and about 56 water quality monitoring stations an automated calibration process was used for the hydrology calibration that iteratively adjusts hydrologic model parameters based on several statistics summarizing the degree of agreement in simulated and observed streamflow at downstream monitoring stations a total of 12 statistics for each monitoring stations were used to quantify distinct aspects of the hydrograph agreement between cumulative frequency distributions of observed and simulated paired concentrations were used for the riverine water quality calibration the agreement of the cumulative frequency distributions was measured in terms of differences in the average concentrations of the five quantile partitions or quantile biases these differences in quantile average concentrations were used for adjusting model parameters based on a number of calibration rules in the automated water quality calibration the calibration rules establish formal relationships between calibration performance i e quantile biases and model parameters these relationships ascertain which quantile biases should be used for targeting specific model parameters or processes and estimating changes necessary in the model parameters for improving agreement of the simulated responses with the observations these calibration rules use model parameters of the last two iterations and corresponding concentration biases in selected quintiles at downstream monitoring stations for estimating parameter values for the next calibration iteration separate calibration rules for a number of important model parameters were defined based on numerous trial and error sensitivity runs in this application the statistical metrics for the cbp model were calculated to align with the other models training and testing periods 2 2 3 observed data daily observed flow data for the six subbasins were obtained from the usgs waterdata website available from https waterdata usgs gov nwis sw and monthly sediment tn and tp fluxes data for the six subbasins were obtained from hirsch et al 2010 https cbrim er usgs gov methods html using the watershed regression on time discharge season wrtds method wrtds is the usgs approved method to develop time series data from discrete measurements chanat et al 2016 the number of data points in the calibration and evaluation periods for daily flow was 3650 each period from 1985 to 1994 for calibration and 1995 to 2004 for evaluation for nutrients and sediment wrtds uses all available usgs data which varies by the constituent but is usually 2 15 samples per month during the calibration and evaluation periods there were 1362 total water quality measurements made wagena and easton 2018 2 3 bayesian ensemble models 2 3 1 bayesian generalized non linear multilevel model the bgmm procedure and a statistical post processing method were fitted using the stan package in r carpenter et al 2016 and developed with the equations bürkner 2016 1 y i d f η i θ 2 η β x z υ where y i is response y through the linear combination of η of predictors transformed by inverse functions f assuming a certain distribution d β and u are the coefficients at the population level and group level respectively and x and z are the corresponding design matrices β u and θ are the model parameters being estimated using the hamiltonian monte carlo nuts algorithm bürkner 2016 by defining the prior distribution for each parameter fig 2 the uniform function is used for prior distribution for the parameters β u and θ following the recommendations of bürkner 2017 stan is a probabilistic programming language for full bayesian inference and optimization carpenter et al 2016 incorporates the hamiltonian monte carlo nuts algorithm bürkner 2017 and provides flexibility for modeling and fitting different complex functions and distributions bürkner 2017 developed the brms bayesian regression models using stan r package using the stan programming language by deploying the hamiltonian monte carlo nuts algorithm for optimization of high dimensional multi level models stan is integrated with the r software environment through an interface called rstan a wide range of distributions and linkage functions are supported in the package allowing users to fit linear and nonlinear models incorporate prior knowledge of all parameters in the response distribution and also incorporate predictor variable distributions to model with different priors model fit can easily be evaluated and compared against observed data with a posterior predictive check and or leave one out cross validation bürkner 2017 we trained flow sediment tn and tp fluxes from 1985 to 1994 and then tested the model predictions from 1995 to 2004 using bgmm in stan by defining family functions for response variables normal distributions for all of the constituents based on the rhat convergence statistic bürkner 2017 the bgmm model performance was evaluated using two metrics rmse and nse for training and testing periods kharin and zwiers 2002 the weights for each model for each constituent were obtained by summing up the absolute values of the fitted coefficients and then dividing each coefficient by the sum of the absolute values of the coefficients obtained during the training period 3 w e i g h t s i β i i β where the sum of all weights across i predictors is equal to one and the coefficient β is the fitted absolute coefficient value of each predictor the convergence of the weights coefficients were evaluated based on rhat values rhat is the scale reduction factor for samples drawn using nuts and at convergence rhat is 1 2 3 1 1 marginal effects after fitting the bgmm for each prediction the effect of each model as a predictor for each constituent was quantified using a marginal effects method the marginal effect is the rate at which the dependent variable changes with respect to one predictor while holding other model variables constant leeper 2017 or the change in the probability of the observed given a one unit change in the model prediction 2 3 2 bayesian model averaging bayesian model averaging bma accounts for model uncertainty by using a posterior model probability and averaging over the best models raftery et al 2005 raftery and painter 2005 bma assumes all model priors follow a uniform distribution and thus differs from bgmm in that manner bma employs bayesian information criterion approximation to develop the prior probabilities on the regression coefficients model weights bma differs from the bgmm approach in that it focuses on which models to include in a mme using a likelihood approach in the bma approach each model receives a weight and the final estimates for each prediction is calculated by weighted averages of the variable estimates from each model amini and parmeter 2011 the bas r package was used to fit the bma model the mcmc convergence was checked using the diagnostic approach by evaluating whether the plots of relative monte carlo frequencies of the sampled models and renormalized marginal likelihood of sampled models follow a 1 1 relationship clyde et al 2011 2 3 3 estimating structural model uncertainty in the bayesian methods using either the monte carlo nuts for bgmm or the monte carlo markov chain for bma posterior estimation methods model input parameter uncertainty is imbedded directly into the estimates of model prediction but does not vary between models as they are forced with the same data structural model uncertainty is accounted for via plausible choices between model structures incorporated by averaging the posterior distributions from the competing models using derived weights from the pseudo marginal likelihood and the deviance information criterion measures of expected model predictive utility jackson et al 2010 thus the difference in the posterior estimates represents the contribution of structural model uncertainty in the bgmm or bma prediction essentially input parameters are treated as deterministic and the resultant uncertainty is due to model structure the results from the bgmm and bma methods are compared against the performance of each individual model and the straight model averaging sma and evaluated both graphically and based on statistical metrics 3 results 3 1 individual model performance evaluation table 1 shows a comparison of the three individual models prediction performance for daily flow sediment tn and tp fluxes with respect to observed data wrtds during the training and testing periods all models predicted flow reasonably well with daily nse values ranging from 0 49 to 0 85 during both periods table 1 all models predicted slightly less inter annual variability than the observed flow for sediment and tn the cbp model provided the best predictions with nse values 0 59 0 79 while both swat models performed considerable worse nse 0 01 0 37 total p was well predicted by all three models during the training period nse 0 49 0 66 but declined during the testing period precipitously so for the cbp model nse of 0 72 with the swat models in the nse range of 0 12 0 14 all three models tended to underpredict tn and overpredict tp values of rmse table 1 tended to scale inversely proportional with nse with greater rmse values occurring where model performance as indicated by the nse was poorer 3 2 ensemble model performance evaluation table 1 shows the performance of the bgmm bma and sma models for daily flow sediment tn and tp fluxes in general the bgmm and bma models performed very well with nse values ranging from 0 67 to 0 95 both bayesian modelsperformed equivalently for predicting flow nse 0 86 0 89 sediment nse 0 68 0 72 and tn nse 0 78 0 95 with the sma model performing slightly worse although still very good for flow nse 0 78 0 80 adequate for sediment nse 0 40 0 55 and good for tn nse 0 68 0 79 all three mmes predicted tp with good performance nse 0 67 0 79 rmse values for the two bayesian models were lower than the corresponding rmse values for the individual models except for sediment in the cbp model table 1 table 2 shows the bayesian model bgmm and bma weights of each model during the training and testing periods the cbp model had more influence in both the bgmm and bma models compared to the other individual watershed models as indicated by its higher weights the main reason for variability among model predictions and performance is likely the scale of model inputs and outputs the cbp model was developed at a fine temporal scale hourly but a coarse spatial scale land river segments approximately 170 km2 and was developed as a decision support management model e g to be used by decision makers in tmdl planning both swat vsa and swat st were initialized at a coarser daily time step but with finer spatial resolution approximately field scale and were developed to address fine spatial scale watershed processes furthermore swat vsa has the most highly constrained calibration of the three models easton et al 2008 2011 these differences are reflected in the bgmm and bma model coefficients table 2 and are an example of how uncertainty resulting from the different model initializations and formulations are captured in the bgmm and bma prediction daily observed data and bgmm model predictions with the 95 credible intervals are shown in fig 3 and bma model predictions with the 95 credible intervals are shown in fig 4 the bgmm captured the variability in flow very well during both periods fig 3a with 3 7 of observed flow falling outside the bgmm 95 credible interval for the training period and only 2 during the testing period table 3 the bgmm captured the sediment flux dynamics and peaks well during the training period with 1 0 of observed data falling outside of the 95 ci during the training period and 0 8 during testing fig 3c and d table 3 for tn the bgmm captured both the pattern and peaks of tn very well with 3 3 of observed data falling outside of the 95 ci during the training and 1 5 during testing fig 3e and f table 3 for tp only 2 1 of the observed data fell outside of the 95 ci during the training and 2 4 during the testing periods fig 3g and h table 3 fig 4 shows the same predictions as in fig 3 but for the bma model in general it is very difficult to distinguish the bma model predictions in fig 4 from the bgmm model predictions in fig 3 like the bgmm the bma captured variability in flow sediment and nutrients during both periods fig 3 the bma captured flow variability well with 3 of observed data falling outside of the 95 ci during the training period and only 1 5 during the testing period slightly better than the bgmm values fig 4a and b table 3 for sediment 0 9 and 0 7 of the observations were outside of the 95 ci during training and testing respectively although the largest sediment export events were under predicted fig 4c and d both tn and tp were also well captured fig 4e f g h with less than 3 of the observed falling outside the 95 ci table 3 generally both bayesian models captured the pattern of all constituents but tended to miss the peak values of nutrients and sediment fig 5 shows the marginal effects of each individual model for the flow and water quality constituents in the bgmm model the marginal effect is the rate at which the dependent variable changes with respect to one predictor while holding other predictor variables constant leeper 2017 or the change in the probability of the observed given a one unit change in the model prediction for flow the marginal effects of the cbp model and swat vsa were similar as the observed flow increased the marginal effect increased for each model and the 95 ci is relatively narrow and constant fig 5a and b which indicates that both models captured both low and high flow values fairly well with low uncertainty however for swat vsa high flows were associated with wider 95 credible intervals of the marginal effects relative to low flows indicating greater model uncertainty the swat st results fig 5c show that the marginal effect had a much wider 95 ci compared to other models equating to greater model uncertainty particularly at high flows for sediment fig 5d e f and tn fig 5g h i the marginal effects for each model increased at higher values however the marginal effect 95 ci was somewhat different for each of the three models narrower across the marginal effects for the cbp model and wider for swat vsa and swat st for sediment for tn the cbp model and swat st had slightly narrower 95 ci than swat vsa for tp the cbp model and swat st models both have relatively narrow and positive 95 ci marginal effects while the marginal effects for swat vsa decreased as the observed values increase and had a substantially wider 95 ci fig 5j k l it also had the smallest weights in both models table 2 4 discussion using the bgmm and bma methods presented here quantifies the structural uncertainty in the flow and water quality predictions of three process based watershed models for the susquehanna river basin our results show that both bayesian models performed equivalently and better than the sma procedure particularly for sediment and substantially better than the individual models in most cases these results are similar to other studies that concluded that mmes generally outperformed single models stoica et al 2004 ajami et al 2006 weisheimer et al 2009 for instance boomer et al 2013 found that sma outperformed individual model predictions of flow sediment tn and tp fluxes in the patuxent estuary the usa similar to our sma results table 1 although they report no estimates of sma model uncertainty the cbp model had higher weights in the bgmm and bma models indicating that on average it had better skill however bothbayesian methods consistently outperformed any individual model indicating that inclusion of models with lower skill still adds predictive power to the ensemble as an added benefit the bayesian methods allow for uncertainty quantification indeed the strength of the bayesian models is not necessarily in the absolute predictive skill but rather in providing the uncertainty associated with those predictions for instance the larger 95 credible intervals for sediment and nutrients suggest that more model development is warranted or there is need to gain a better understanding of the physical watershed processes governing nutrient and sediment transport boomer et al 2013 the 95 ci for nutrients and sediments being considerably wider than the 95 ci for flow further indicate more uncertainty in these predictions the marginal effects fig 5 generally show all models captured smaller more frequent values for all constituents with more precision e g tighter 95 marginal ci while the 95 marginal ci at higher values perhaps reflects the fact that those larger flows or nutrient and sediment fluxes are less frequent and thus have less effect on the bgmm weights use of the marginal effects decomposes the bgmm into the individual model components provides some diagnostic measures to evaluate individual model performance and the associated uncertainty bounds for each model fig 5 while the cbp model individually tended to exhibit the best skill of all three models both of the bayesian methods consistently outperformed the cbp model and the other individual models clearly being able to incorporate the best aspects of each model more than makes up for the deficiencies in individual models doblas reyes et al 2005 hagedorn et al 2005 the use of mme to quantify uncertainty in model projections has been explored for general circulation model climate projections through the coupled model intercomparison project ensembles taylor et al 2012 with increasing research focused on crop models through the agricultural model intercomparison project agmip rosenzweig et al 2013 and to some extent for hydrologic models duan et al 2007 najafi et al 2011 krysanova and hattermann 2017 this approach relies on using multi model ensembles mmes where individual model projections are used to develop a probability density function pdf for the outcome of interest mme s provide more complete representations of the uncertainty of future impacts than any of the individual models and averaging predictions from independent simulations reduces model variance and predictive error relative to single models trevor et al 2009 for example the bayesian methods developed here could be used to evaluate management practices needed to achieve a water quality standard where results from each model are aggregated to obtain an estimated mean nutrient reduction achieved and the probability that the proposed management practices will achieve the desired water quality improvements under these assumptions solutions for different management alternatives that generate smaller uncertainty ranges theoretically represent more conservative management approaches i e less chance of failure the bayesian methods results can also be used in other decision making frameworks such as robust decision making shortridge et al 2017 robust decision making uses the output from bayesian models to compare the robustness of different management strategies and identify strategy vulnerabilities defined as the conditions that cause these strategies to fail this process has been used to identify policies that are most robust to future uncertainties and critical thresholds beyond which the system does not perform satisfactorily as well as to understand the relative contribution of different sources of uncertainty bryant and lempert 2010 5 conclusions this study describes and evaluates two bayesian mme approaches the bgmm and bma approache capable of quantifying structural uncertainty and generally improving predictions over individual models while both bayesian approaches performed well in terms of prediction skill the quantification of uncertainty provides additional justification for undertaking the development of a bayesian model understanding the uncertainty associated with model predictions can assist policy makers and managers with the ecological and environmental decision making process allowing a risk based approach to watershed management acknowledgements we would like to acknowledge high performance computing support from yellowstone http n2t net ark 85065 d7wd3xhc provided by ncar s computational and information systems laboratory and support from the national science foundation under award numbers 1360415 1740704 and 1343802 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 013 
26215,watershed models are essential tools to understand quantify and predict hydrologic processes and water quality responses from scales ranging from field to large river basins however the reliability of watershed models in a management context depends largely on inherent uncertainties in model predictions the objective of this study is to quantify prediction uncertainty for flow sediment total nitrogen tn and total phosphorus tp resulting from model structure we do this using using three process based models the soil and water assessment tool variable source area model swat vsa the standard soil and water assessment tool swat st and the chesapeake bay program s phase 6 watershed model cbp model we initialize each of the models using meteorological soil and land use data and analyze outputs of flow sediment tn and tp fluxes at the u s geological survey stream gauge at the downstream end of the susquehanna river basin in conowingo maryland using these three models we develop and compare two types of bayesian models a bayesian generalized non linear multilevel model bgmm and a bayesian model averaging bma for flow sediment tn and tp and 95 credible intervals we compare the bayesian models results against the individual model results and straight model averaging sma using a split time period analysis to assess their predictive strengths both bayesian models provided substantially better predictions than the individual process based models and estimates of prediction uncertainty which can enhance decision making and improve watershed management by providing a risk based assessment of outcomes keywords swat vsa swat standard chesapeake bay watershed model bayesian model ensemble multilevel models 1 introduction ecosystems operate under dynamic and complex interactions of physical chemical and biological processes the complexity of the interactions leads to difficulties in conceptualizing the processes mathematically which makes accurately modeling of ecosystems challenging uncertainty can be classified from two primary sources epistemic and aleatory uncertainty epistemic uncertainty arises from a lack of knowledge or data necessary to estimate probabilities aleatory uncertainty in contrast derives from the inherent randomness in the system e g weather related variation beven 2002 although complex process based models have been built to approximate real world conditions and to assist in effective decision making quantifying structural uncertainties i e uncertainty related to the configuration of a system of watershed models remains a challenge most previous research into model uncertainty has focused on model parameters or input data uncertainties renard et al 2010 and less on structural model uncertainty which defined by breuer et al 2009 and as used in this article is the uncertainty from unknown simplified incomplete or incorrect process descriptions within a model or simplified mathematical forms or expressions of ecosystem processes in the model this type of uncertainty can occur even if the true input values for all model parameters are known sources of structural model uncertainty include exclusion of important controlling variables processes either missing or incorrectly represented or misaligned boundary conditions ascough et al 2008 which can propagate to prediction uncertainty vrugt et al 2005 mäntyniemi et al 2013 yen et al 2014 shen et al 2015 quantifying uncertainties that arise from model formulation and the associated uncertainty bounds would help to make more informed and effective management decisions numerous approaches for quantifying uncertainties in watershed models have been proposed including bayesian model averaging raftery et al 1997 ajami et al 2007 vrugt et al 2008 tasdighi et al 2018 generalized likelihood uncertainty estimation glue vrugt et al 2005 blasone et al 2008 multiple objective function criteria blasone et al 2008 sequential data assimilation moradkhani et al 2005 bayesian recursive estimation thiemann et al 2001 and the ensemble kalman filter abaza et al 2014 however only a few methods such as multi model ensembles mme stoica et al 2004 duan et al 2007 have been proposed for quantifying structural model uncertainties this is due to the complexity and difficulty of separating structural uncertainties from either model parameters or input data uncertainty zhang et al 2011 sharifi et al 2017 evaluated the performance of different watershed models to assess water quality impacts on queenstown drainage stow and scavia 2009 demonstrated improved performance in estimating hypoxia in the chesapeake bay using a hierarchal bayesian ensemble approach but the study was limited to processes within the bay and does not describe upland watershed processes boomer et al 2013 and exbrayat et al 2010 also suggested use of multi model ensembles to improve predictions multi model ensembles mmes combine two or more models in an effort to improve model prediction and objectively evaluate model uncertainty boomer et al 2013 mmes utilize the diversity of skillful predictions from different models and can improve the estimation of uncertainties associated with model outputs duan et al 2007 mmes are commonly used in weather forecasting krishnamurti et al 2000 and climate change analysis christensen and lettenmaier 2006 there are different ways to implement mme approaches ensembles can be used to sample uncertainties in forcing assumptions initial conditions process conceptualization input data and training calibration data generally mmes use multiple models to simulate responses of identical driving forces e g climate inputs where model predictions are aggregated and compared to estimate uncertainty in model predictions for example an mme could be used to evaluate the ability of proposed management practices to achieve a water quality standard distinct solutions would be obtained from multiple models and then be aggregated to obtain an estimated mean nutrient reduction achieved and the probability that their implementation will achieve the desired water quality improvements mme projections have been found to reproduce historical observations more accurately than individual models in a number of fields reichler and kim 2008 and historically the variability in model projections has been used as a measure of uncertainty little research has been carried out to quantify the structural uncertainties in models using mme duan et al 2007 and the application and analysis of ensemble methods remaina challenge in this field raftery et al 2005 proposed bayesian model averaging bma post processing methods to ensemble weather prediction models requiring accurate estimates of the weights and variances of the individual competing models in the ensemble vrugt et al 2008 additionally the bma algorithm does not always provide global convergence vrugt et al 2006 for instance in the bma approach model averaging refers to estimating some quantity from each model and then averaging the estimates based on how likely each model predicts the data by assigning weights for each model wasserman 2000 the bma approach sometimes underperformed compared to simple unweighted averages straight model averaging because it does not assign two equally performing models with the same weights graefe et al 2015 another approach the ensemble kalman filter assumes all probability distributions are gaussian in shape which is suitable for large state variables but problematic for constituents that exhibit non gaussian behavior evensen 2003 mandel 2009 thus for this research we use a new approach a bayesian generalized non linear multilevel models bgmm bürkner 2017 which integrates different probability distributions explicitly incorporates prior knowledge about parameter distribution shape allows predictor variables to be linear or nonlinear and incorporates information from all potential predictors this method offers a more complete quantification of uncertainty rather than primarily attempting to improve prediction skill we develop a bgmm as an ensemble capable of quantifying structural uncertainty of multiple models used to predict watershed responses we employ the hamiltonian monte carlo method using the nou turn sampler nuts extension which has been shown to converge to optimal bayesian coefficients faster and more reliably than other markov chain monte carlo mcmc algorithms betancourt et al 2017 bürkner 2017 structural uncertainties and credible intervals of model predictions for flow sediment total nitrogen tn and total phosphorus tp fluxes at the u s geological survey stream gauge at the downstream end of the susquehanna river basin in conowingo maryland were developed using the bgmm method specifically we 1 initialized and evaluated two watershed models the soil and water assessment tool variable source area swat vsa model easton et al 2008 the swat standard swat st model arnold et al 2012 and obtained outputs from the chesapeake bay program s phase 6 watershed model cbp model 2 developed two bayesian mmes a bgmm and a bma using outputs from the three models as predictors and observed data as the response variable and 3 developed a straight model average sma mme per the recommendation of boomer et al 2013 we evaluate all models performance based on two statistical metrics root mean square error rmse and nash sutcliffe efficiency nse and for the bgmm we developed and evaluated the marginal effects of each model in the mme 2 materials and methods 2 1 study area description the susquehanna river the largest contributor of fresh water to the chesapeake bay drains approximately 42 of the bay watershed and contributes more than 50 of the fresh water flows to the bay ko and baker 2004 and largely controls salinity in the bay gibson and najjar 2000 the basin is approximately 71 000 km2 with elevation ranging from 10 to 960 m and covers parts of three states new york pennsylvania and maryland fig 1 the susquehanna river basin has six major subbasins upper susquehanna chemung middle susquehanna west branch susquehanna juniata and lower susquehanna fig 1 the climate varies along a north south gradient with the northern portion receiving more precipitation 1245 mm than the lower basin 838 mm which experiences hotter and longer summers dephilip and moberg 2010 the land cover of the susquehanna river basin is primarily forest 70 agriculture 22 and developed 7 the major soil texture type is silt loam 70 and the soil depth in most parts of the basin ranges from 0 5 to 2 m ray et al 2016 2 2 watershed models 2 2 1 swat and swat vsa model descriptions the soil and water assessment tool swat model is a process based semi distributed watershed model developed to simulate landscape processes and predict the impact of land management practices on water availability and water quality arnold et al 1998 swat requires meteorological soil land cover and land management data to simulate surface and subsurface hydrology and various chemical nutrient and sediment fluxes in swat the watershed is divided into sub watersheds and then further into hydrologic response units hrus which are unique combinations of soil type and land use soil and water assessment tool variable source area swat vsa re conceptualizes the standard swat to account for areas of the landscape subject to variable saturation dynamics easton et al 2008 in swta vsa the area of each hru is defined by the coincidence of land use and wetness index class determined from a topographic index ti to differentiate areas of the landscape with respect to their moisture storage and saturation index easton et al 2008 swat vsa has been shown to provide better predictions of soil moisture and runoff generation than the standard swat model in watersheds with similar physical characteristics and climate to the study watershed easton et al 2008 2 2 1 1 swat model inputs the susquehanna river swat st and swat vsa models were initialized with 10 m digital elevation models dem obtained from the united states geologic survey usgs national elevation dataset ned guenther and maune 2007 using arcswat 2012 swat tamu edu software arcswat and toposwat fuka and easton 2016 toposwat automates the swat vsa initialization process by assimilating soil data creating the ti map overlaying the soil and ti maps and developing the required database for model initialization soils data included in toposwat are based on the food and agriculture organization fao soils database fao 2007 the land cover of the watershed was obtained from the national land cover database 2011 the most recent national land cover product created by the multi resolution land characteristics mrlc consortium homer et al 2015 the model was forced with daily meteorological data from the climate forecast system reanalysis cfsr including precipitation temperature min and max relative humidity wind speed and solar radiation at the centroids of the six major sub basins the dataset consists of hourly weather forecasts generated by the national weather service s ncep global forecast system and has been successfully used for watershed modeling in different hydrologic settings fuka et al 2014 2 2 1 2 swat model calibration and evaluation both swat vsa and swat st calibration followed a cascading procedure first calibrating all upper basins upper susquehanna chemung west branch susquehanna and juanita sub basins to observed data for their respective monitoring stations next the middle susquehanna was calibrated leaving all parameters values for the upper basins unchanged finally the lower susquehanna subbasin was calibrated leaving all parameter values for the previously calibrated basins unchanged the model was calibrated and evaluated using the swat calibration and uncertainty procedure swat cup arnold et al 2012 and the sufi2 sequential uncertainty fitting optimization algorithm with the objective function set to use the nse the model was calibrated for daily flow sediment tn and tp fluxes from 1985 to 1994 and evaluated from 1995 to 2004 at each of the six sub basins in the watershed wagena and easton 2018 2 2 2 chesapeake bay program s watershed model the chesapeake bay program s phase 6 watershed model cbp model was developed for the 2017 mid point assessment of the chesapeake bay total maximum daily load tmdl it simulates hydrology and fate of nutrients and sediment that contribute to chesapeake bay water quality degradation the hydrological component of the cbp model is based on the hspf model hydrologic simulation program fortran which was derived from the stanford watershed model shenk et al 2012 shenk and linker 2013 boomer et al 2013 hspf uses a mass balance approach to solve a linked set of equations representing natural and anthropogenic mechanisms and lumped parameters and uses hourly metrological data sharifi et al 2017 the cbp model consists of 1063 modeling segments subbasins with the average size of approximately 170 km2 shenk and linker 2013 the model input includes land use meteorology elevation features fertilizer applications wastewater plant discharges septic systems air deposition farm animal populations and other variables to estimate the amount of nutrients and sediment reaching the chesapeake bay epa 2010 for the purpose of this research daily flow sediment tn and tp fluxes data were obtained for the susquehanna river at the conowingo dam from the outputs of chesapeake bay program s phase 6 watershed model the susquehanna model domain was calibrated for flow temperature and water quality variables sediment dissolved oxygen nitrate ammonia nitrogen dissolved orthophosphate phosphorus and chlorophyll from 1985 to 2014 at 101 streamflow monitoring stations and about 56 water quality monitoring stations an automated calibration process was used for the hydrology calibration that iteratively adjusts hydrologic model parameters based on several statistics summarizing the degree of agreement in simulated and observed streamflow at downstream monitoring stations a total of 12 statistics for each monitoring stations were used to quantify distinct aspects of the hydrograph agreement between cumulative frequency distributions of observed and simulated paired concentrations were used for the riverine water quality calibration the agreement of the cumulative frequency distributions was measured in terms of differences in the average concentrations of the five quantile partitions or quantile biases these differences in quantile average concentrations were used for adjusting model parameters based on a number of calibration rules in the automated water quality calibration the calibration rules establish formal relationships between calibration performance i e quantile biases and model parameters these relationships ascertain which quantile biases should be used for targeting specific model parameters or processes and estimating changes necessary in the model parameters for improving agreement of the simulated responses with the observations these calibration rules use model parameters of the last two iterations and corresponding concentration biases in selected quintiles at downstream monitoring stations for estimating parameter values for the next calibration iteration separate calibration rules for a number of important model parameters were defined based on numerous trial and error sensitivity runs in this application the statistical metrics for the cbp model were calculated to align with the other models training and testing periods 2 2 3 observed data daily observed flow data for the six subbasins were obtained from the usgs waterdata website available from https waterdata usgs gov nwis sw and monthly sediment tn and tp fluxes data for the six subbasins were obtained from hirsch et al 2010 https cbrim er usgs gov methods html using the watershed regression on time discharge season wrtds method wrtds is the usgs approved method to develop time series data from discrete measurements chanat et al 2016 the number of data points in the calibration and evaluation periods for daily flow was 3650 each period from 1985 to 1994 for calibration and 1995 to 2004 for evaluation for nutrients and sediment wrtds uses all available usgs data which varies by the constituent but is usually 2 15 samples per month during the calibration and evaluation periods there were 1362 total water quality measurements made wagena and easton 2018 2 3 bayesian ensemble models 2 3 1 bayesian generalized non linear multilevel model the bgmm procedure and a statistical post processing method were fitted using the stan package in r carpenter et al 2016 and developed with the equations bürkner 2016 1 y i d f η i θ 2 η β x z υ where y i is response y through the linear combination of η of predictors transformed by inverse functions f assuming a certain distribution d β and u are the coefficients at the population level and group level respectively and x and z are the corresponding design matrices β u and θ are the model parameters being estimated using the hamiltonian monte carlo nuts algorithm bürkner 2016 by defining the prior distribution for each parameter fig 2 the uniform function is used for prior distribution for the parameters β u and θ following the recommendations of bürkner 2017 stan is a probabilistic programming language for full bayesian inference and optimization carpenter et al 2016 incorporates the hamiltonian monte carlo nuts algorithm bürkner 2017 and provides flexibility for modeling and fitting different complex functions and distributions bürkner 2017 developed the brms bayesian regression models using stan r package using the stan programming language by deploying the hamiltonian monte carlo nuts algorithm for optimization of high dimensional multi level models stan is integrated with the r software environment through an interface called rstan a wide range of distributions and linkage functions are supported in the package allowing users to fit linear and nonlinear models incorporate prior knowledge of all parameters in the response distribution and also incorporate predictor variable distributions to model with different priors model fit can easily be evaluated and compared against observed data with a posterior predictive check and or leave one out cross validation bürkner 2017 we trained flow sediment tn and tp fluxes from 1985 to 1994 and then tested the model predictions from 1995 to 2004 using bgmm in stan by defining family functions for response variables normal distributions for all of the constituents based on the rhat convergence statistic bürkner 2017 the bgmm model performance was evaluated using two metrics rmse and nse for training and testing periods kharin and zwiers 2002 the weights for each model for each constituent were obtained by summing up the absolute values of the fitted coefficients and then dividing each coefficient by the sum of the absolute values of the coefficients obtained during the training period 3 w e i g h t s i β i i β where the sum of all weights across i predictors is equal to one and the coefficient β is the fitted absolute coefficient value of each predictor the convergence of the weights coefficients were evaluated based on rhat values rhat is the scale reduction factor for samples drawn using nuts and at convergence rhat is 1 2 3 1 1 marginal effects after fitting the bgmm for each prediction the effect of each model as a predictor for each constituent was quantified using a marginal effects method the marginal effect is the rate at which the dependent variable changes with respect to one predictor while holding other model variables constant leeper 2017 or the change in the probability of the observed given a one unit change in the model prediction 2 3 2 bayesian model averaging bayesian model averaging bma accounts for model uncertainty by using a posterior model probability and averaging over the best models raftery et al 2005 raftery and painter 2005 bma assumes all model priors follow a uniform distribution and thus differs from bgmm in that manner bma employs bayesian information criterion approximation to develop the prior probabilities on the regression coefficients model weights bma differs from the bgmm approach in that it focuses on which models to include in a mme using a likelihood approach in the bma approach each model receives a weight and the final estimates for each prediction is calculated by weighted averages of the variable estimates from each model amini and parmeter 2011 the bas r package was used to fit the bma model the mcmc convergence was checked using the diagnostic approach by evaluating whether the plots of relative monte carlo frequencies of the sampled models and renormalized marginal likelihood of sampled models follow a 1 1 relationship clyde et al 2011 2 3 3 estimating structural model uncertainty in the bayesian methods using either the monte carlo nuts for bgmm or the monte carlo markov chain for bma posterior estimation methods model input parameter uncertainty is imbedded directly into the estimates of model prediction but does not vary between models as they are forced with the same data structural model uncertainty is accounted for via plausible choices between model structures incorporated by averaging the posterior distributions from the competing models using derived weights from the pseudo marginal likelihood and the deviance information criterion measures of expected model predictive utility jackson et al 2010 thus the difference in the posterior estimates represents the contribution of structural model uncertainty in the bgmm or bma prediction essentially input parameters are treated as deterministic and the resultant uncertainty is due to model structure the results from the bgmm and bma methods are compared against the performance of each individual model and the straight model averaging sma and evaluated both graphically and based on statistical metrics 3 results 3 1 individual model performance evaluation table 1 shows a comparison of the three individual models prediction performance for daily flow sediment tn and tp fluxes with respect to observed data wrtds during the training and testing periods all models predicted flow reasonably well with daily nse values ranging from 0 49 to 0 85 during both periods table 1 all models predicted slightly less inter annual variability than the observed flow for sediment and tn the cbp model provided the best predictions with nse values 0 59 0 79 while both swat models performed considerable worse nse 0 01 0 37 total p was well predicted by all three models during the training period nse 0 49 0 66 but declined during the testing period precipitously so for the cbp model nse of 0 72 with the swat models in the nse range of 0 12 0 14 all three models tended to underpredict tn and overpredict tp values of rmse table 1 tended to scale inversely proportional with nse with greater rmse values occurring where model performance as indicated by the nse was poorer 3 2 ensemble model performance evaluation table 1 shows the performance of the bgmm bma and sma models for daily flow sediment tn and tp fluxes in general the bgmm and bma models performed very well with nse values ranging from 0 67 to 0 95 both bayesian modelsperformed equivalently for predicting flow nse 0 86 0 89 sediment nse 0 68 0 72 and tn nse 0 78 0 95 with the sma model performing slightly worse although still very good for flow nse 0 78 0 80 adequate for sediment nse 0 40 0 55 and good for tn nse 0 68 0 79 all three mmes predicted tp with good performance nse 0 67 0 79 rmse values for the two bayesian models were lower than the corresponding rmse values for the individual models except for sediment in the cbp model table 1 table 2 shows the bayesian model bgmm and bma weights of each model during the training and testing periods the cbp model had more influence in both the bgmm and bma models compared to the other individual watershed models as indicated by its higher weights the main reason for variability among model predictions and performance is likely the scale of model inputs and outputs the cbp model was developed at a fine temporal scale hourly but a coarse spatial scale land river segments approximately 170 km2 and was developed as a decision support management model e g to be used by decision makers in tmdl planning both swat vsa and swat st were initialized at a coarser daily time step but with finer spatial resolution approximately field scale and were developed to address fine spatial scale watershed processes furthermore swat vsa has the most highly constrained calibration of the three models easton et al 2008 2011 these differences are reflected in the bgmm and bma model coefficients table 2 and are an example of how uncertainty resulting from the different model initializations and formulations are captured in the bgmm and bma prediction daily observed data and bgmm model predictions with the 95 credible intervals are shown in fig 3 and bma model predictions with the 95 credible intervals are shown in fig 4 the bgmm captured the variability in flow very well during both periods fig 3a with 3 7 of observed flow falling outside the bgmm 95 credible interval for the training period and only 2 during the testing period table 3 the bgmm captured the sediment flux dynamics and peaks well during the training period with 1 0 of observed data falling outside of the 95 ci during the training period and 0 8 during testing fig 3c and d table 3 for tn the bgmm captured both the pattern and peaks of tn very well with 3 3 of observed data falling outside of the 95 ci during the training and 1 5 during testing fig 3e and f table 3 for tp only 2 1 of the observed data fell outside of the 95 ci during the training and 2 4 during the testing periods fig 3g and h table 3 fig 4 shows the same predictions as in fig 3 but for the bma model in general it is very difficult to distinguish the bma model predictions in fig 4 from the bgmm model predictions in fig 3 like the bgmm the bma captured variability in flow sediment and nutrients during both periods fig 3 the bma captured flow variability well with 3 of observed data falling outside of the 95 ci during the training period and only 1 5 during the testing period slightly better than the bgmm values fig 4a and b table 3 for sediment 0 9 and 0 7 of the observations were outside of the 95 ci during training and testing respectively although the largest sediment export events were under predicted fig 4c and d both tn and tp were also well captured fig 4e f g h with less than 3 of the observed falling outside the 95 ci table 3 generally both bayesian models captured the pattern of all constituents but tended to miss the peak values of nutrients and sediment fig 5 shows the marginal effects of each individual model for the flow and water quality constituents in the bgmm model the marginal effect is the rate at which the dependent variable changes with respect to one predictor while holding other predictor variables constant leeper 2017 or the change in the probability of the observed given a one unit change in the model prediction for flow the marginal effects of the cbp model and swat vsa were similar as the observed flow increased the marginal effect increased for each model and the 95 ci is relatively narrow and constant fig 5a and b which indicates that both models captured both low and high flow values fairly well with low uncertainty however for swat vsa high flows were associated with wider 95 credible intervals of the marginal effects relative to low flows indicating greater model uncertainty the swat st results fig 5c show that the marginal effect had a much wider 95 ci compared to other models equating to greater model uncertainty particularly at high flows for sediment fig 5d e f and tn fig 5g h i the marginal effects for each model increased at higher values however the marginal effect 95 ci was somewhat different for each of the three models narrower across the marginal effects for the cbp model and wider for swat vsa and swat st for sediment for tn the cbp model and swat st had slightly narrower 95 ci than swat vsa for tp the cbp model and swat st models both have relatively narrow and positive 95 ci marginal effects while the marginal effects for swat vsa decreased as the observed values increase and had a substantially wider 95 ci fig 5j k l it also had the smallest weights in both models table 2 4 discussion using the bgmm and bma methods presented here quantifies the structural uncertainty in the flow and water quality predictions of three process based watershed models for the susquehanna river basin our results show that both bayesian models performed equivalently and better than the sma procedure particularly for sediment and substantially better than the individual models in most cases these results are similar to other studies that concluded that mmes generally outperformed single models stoica et al 2004 ajami et al 2006 weisheimer et al 2009 for instance boomer et al 2013 found that sma outperformed individual model predictions of flow sediment tn and tp fluxes in the patuxent estuary the usa similar to our sma results table 1 although they report no estimates of sma model uncertainty the cbp model had higher weights in the bgmm and bma models indicating that on average it had better skill however bothbayesian methods consistently outperformed any individual model indicating that inclusion of models with lower skill still adds predictive power to the ensemble as an added benefit the bayesian methods allow for uncertainty quantification indeed the strength of the bayesian models is not necessarily in the absolute predictive skill but rather in providing the uncertainty associated with those predictions for instance the larger 95 credible intervals for sediment and nutrients suggest that more model development is warranted or there is need to gain a better understanding of the physical watershed processes governing nutrient and sediment transport boomer et al 2013 the 95 ci for nutrients and sediments being considerably wider than the 95 ci for flow further indicate more uncertainty in these predictions the marginal effects fig 5 generally show all models captured smaller more frequent values for all constituents with more precision e g tighter 95 marginal ci while the 95 marginal ci at higher values perhaps reflects the fact that those larger flows or nutrient and sediment fluxes are less frequent and thus have less effect on the bgmm weights use of the marginal effects decomposes the bgmm into the individual model components provides some diagnostic measures to evaluate individual model performance and the associated uncertainty bounds for each model fig 5 while the cbp model individually tended to exhibit the best skill of all three models both of the bayesian methods consistently outperformed the cbp model and the other individual models clearly being able to incorporate the best aspects of each model more than makes up for the deficiencies in individual models doblas reyes et al 2005 hagedorn et al 2005 the use of mme to quantify uncertainty in model projections has been explored for general circulation model climate projections through the coupled model intercomparison project ensembles taylor et al 2012 with increasing research focused on crop models through the agricultural model intercomparison project agmip rosenzweig et al 2013 and to some extent for hydrologic models duan et al 2007 najafi et al 2011 krysanova and hattermann 2017 this approach relies on using multi model ensembles mmes where individual model projections are used to develop a probability density function pdf for the outcome of interest mme s provide more complete representations of the uncertainty of future impacts than any of the individual models and averaging predictions from independent simulations reduces model variance and predictive error relative to single models trevor et al 2009 for example the bayesian methods developed here could be used to evaluate management practices needed to achieve a water quality standard where results from each model are aggregated to obtain an estimated mean nutrient reduction achieved and the probability that the proposed management practices will achieve the desired water quality improvements under these assumptions solutions for different management alternatives that generate smaller uncertainty ranges theoretically represent more conservative management approaches i e less chance of failure the bayesian methods results can also be used in other decision making frameworks such as robust decision making shortridge et al 2017 robust decision making uses the output from bayesian models to compare the robustness of different management strategies and identify strategy vulnerabilities defined as the conditions that cause these strategies to fail this process has been used to identify policies that are most robust to future uncertainties and critical thresholds beyond which the system does not perform satisfactorily as well as to understand the relative contribution of different sources of uncertainty bryant and lempert 2010 5 conclusions this study describes and evaluates two bayesian mme approaches the bgmm and bma approache capable of quantifying structural uncertainty and generally improving predictions over individual models while both bayesian approaches performed well in terms of prediction skill the quantification of uncertainty provides additional justification for undertaking the development of a bayesian model understanding the uncertainty associated with model predictions can assist policy makers and managers with the ecological and environmental decision making process allowing a risk based approach to watershed management acknowledgements we would like to acknowledge high performance computing support from yellowstone http n2t net ark 85065 d7wd3xhc provided by ncar s computational and information systems laboratory and support from the national science foundation under award numbers 1360415 1740704 and 1343802 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 013 
26216,watershed scale simulation technology allows quantification of the impact of individual and or integrated management practices throughout the watershed in the annagnps watershed scale model the watershed is subdivided into basic modeling units fields in which all spatially varying physical parameters are assumed to be homogeneous conversely rusle2 model can estimate sediment yield at sub field scale with enhanced characterization capabilities in this study an integrated approach was developed and evaluated that combines both models for enhanced representation of individual fields while accounting for the watershed wide integrated intra fields effect when routing sediment loads through the watershed critical sediment producing areas identified through integrated rusle2 erosion and annagnps sediment transport models can be used to support the development and evaluation of conservation management plans specific to fields but impacting the entire watershed keywords watershed simulation soil erosion annagnps rusle2 integrated field watershed modeling 1 introduction watershed scale modeling technology supports efforts to improve agriculture sustainability through quantitative spatio temporal evaluation of farming and conservation practice alternatives the impact of implementing a particular practice needs to be assessed at the field scale field where the practice was implemented and more importantly at the watershed scale through determination of the integrated effects of an applied practice and corresponding physical process impact to downstream watershed systems at the watershed scale the basin is commonly divided into cells also referred to as sub catchments sub watersheds or hydrological response units representing uplands and reaches denoting concentrated flow bingner and theurer 2001a b fitzhugh and mackay 2001 momm et al 2017 the entire basin is represented as a hierarchical structure in which cells are connected to reaches based on estimated surface flow momm et al 2017 basic modeling units are characterized by a large number of parameters depicting soils climate farming management topography and hydrology and each parameter is assumed to be spatially invariant within cells and reaches fitzhugh and mackay 2000 this homogeneity assumption within basic modeling units restricts description of parameters variation at scales smaller than cell sizes and can affect how physical processes are described however it provides conservation practice assessment at the watershed scale through supporting timely input database preparation simplified analysis of results and reasonable computer programming execution time at the field scale models are conceptualized using a one dimensional representative profile that can be further divided into segments to characterize variation within field sized areas foster et al 2003 williams and izaurralde 2006 flanagan and nearing 1995 these models were designed to predict soil losses at field scale to inform users on development of targeted conservation practices one clear advantage is the enhanced characterization of features at scales smaller than field size areas smaller than cells however application of field scale models to entire watersheds requires significant effort for database input generation and profile characterization as well as high computational cost moreover this approach only depicts local field impacts as there is no mechanism to evaluate the integrated effect of individual practices and or physical processes throughout the watershed while both watershed and field scale models have strengths and limitations an integrated alternative combining these technologies has the potential to provide improved field characterization while also estimating complex relationships between practices and physical processes by integrating the routing of overland flow and suspended material throughout the watershed comparisons of field scale sediment load estimates from soil and water assessment tool swat watershed scale model arnold et al 1993 with estimates from revised universal soil loss equation version 2 rusle2 field scale model usda ars 2008 indicated discrepancies between field and watershed scale models in their capability to select critical sediment producing areas sommerlot et al 2013a sommerlot et al 2013a attributed these findings to the enhanced description of physical processes and farming management practices at scales smaller than field area by the field scale model and therefore this study recognized the need for a watershed field integrated solution similarly the swat model has been integrated with the agricultural policy environmental extender apex field level model williams and izaurralde 2006 for enhanced characterization of farming operations crop growth and conservation practices such as filter strips terraces and waterways wang et al 2011 however this study was focused on a regional scale u s 8 digit watershed level with no channel routing components used limiting this way the routing of overland flow and suspended material throughout the watershed wang et al 2011 another common characteristic of regional scale watershed models such as swat is the assumption that farming management assigned to sub catchments are either static one year management repeated for the duration of the simulation or pseudo static few years of crop rotation repeated for the duration of the simulation this assumption is required for simplified representation of reality within models simulating physical processes at these large scales even when swat is integrated with field scale models such as apex saleh et al 2000 better integrated methods capable of 1 capturing processes at scales within and not just at the edge of fields cells 2 including routing field estimates to downstream waterbodies while at the same time 3 integrating these estimates from other fields and with channel processes are critically needed for watershed conservation management planning sommerlot et al 2013a b in this study an integrated watershed field scales modeling framework is proposed and evaluated the annualized agricultural non point source annagnps watershed pollution model bingner et al 2015 was selected at the watershed scale and the revised universal soil loss equation version 2 rusle2 soil erosion model usda ars 2008 at the field scale by integrating the annagnps model with rusle2 at the watershed scale this combined approach can be used as an enhanced screening tool to identify non point source producing critical cells sub catchments next those selected cells are further characterized and simulated using the rusle2 model finally soil erosion estimates from rusle2 are supplied to another annagnps model simulation for evaluation of potential physical processes interactions and flow sediment routing throughout the watershed specifically the objectives of this study are three fold 1 development and evaluation of methods for integrating annagnps and rusle2 models 2 evaluation of sediment loads estimates at the field scale generated by annagnps and rusle2 models 3 assessment of the integrated multi step multi scale annagnps rusle2 modeling system sediment yield estimations at the watershed outlet 2 methods 2 1 study site 2 1 1 goodwin creek watershed goodwin creek watershed gcw is an experimental watershed located southeast of batesville mississippi in the loess covered bluff hills of the yazoo river basin fig 1 a gcw is monitored by the usda ars national sedimentation laboratory nsl that supports research involving upland erosion stream erosion and sedimentation and watershed hydrology alonso and bingner 2000 elevation of the watershed ranges from 71 m to 128 m above sea level soils in the watershed comprise two major associations on the cultivated flood plains and terrace areas the soil belongs to the falaya grenada calloway association which area are poorly to moderately well drained pasture and wooded areas on the watershed s loess ridges and hillsides are characterized by the loring grenada memphis soil association which are silty in texture and easily eroded in the absence of vegetative cover average daily maximum temperature is 30 c in the summer and 10 c in the winter with high air humidity throughout the year the majority of large runoff events occurs during the winter and spring months table 1 streamflow is sensitive to rainfall patters and individual storm events can generate flows capable of transporting large particle sizes kuhnle et al 1996 binger 1996 the average annual rainfall is 1440 mm yr 1 the land use land cover contains row crops pasture and forest in the early 1980s row crop was the dominant land use but after 1990s agriculture was replaced by native forest and pasture kuhnle et al 2008 corn soybeans and cotton were the primary crops produced on cultivated land the selected study area was a subset of the gcw defined by the outlet selected at stream gauge 14 green triangle in fig 1 stream gauge 14 was monitored for discharge and suspended sediment loads using constructed in stream flume structures these flumes were built with v shape profile and a moderate slope of 0 04 m m to prevent sediment deposition kuhnle et al 2008 additionally daily precipitation information and rusle storm erosivity ei was monitored at the stream gauge 14 outlet and four other precipitation gauge sites black triangle in fig 1b daily minimum and maximum air temperatures were obtained from a nearby weather station within gcw a complete revised dataset containing weather and flow and suspended sediment information between 1982 and 1994 were available and used to develop and validate the watershed simulations 2 2 watershed scale modeling the agricultural non point source agnps pollution modeling system contains a set of integrated tools for distributed event based long term modeling and simulation of surface runoff sediment nutrient nitrogen phosphorus and organic carbon and pesticide transport primarily from agricultural watersheds young et al 1989 the agnps model was originally designed to simulate un gauged watersheds through enhanced processes characterization such as dynamic management description one component in the agnps modeling system is the annualized agricultural non point source annagnps pollution model containing capabilities for continuous simulation watershed scale evaluation mixed land use and surface runoff modeling and simulation usda ars 2015 surface hydrological fluxes are estimated daily using weather information such as precipitation maximum and minimum temperatures dew point temperatures solar radiation or sky cover and wind speed zema et al 2012 the watershed is subdivided into cells and reaches based on user provided input either manually or using topographic attributes surface runoff sediment and pollutant estimates generated in cells are routed into reaches allowing for interpretation evaluation of results spatially and for investigation of inter relation between practices at cells surface runoff is determined based on the scs curve number method usda scs 1972 coupled with daily surface hydrological fluxes and farming management practices crop seasonal rotations and crop specific farming operations estimates of sediment load from sheet and rill erosion processes in each cell are generated daily using the revised universal soil loss equation rusle method renard et al 1997 and their delivery to the edge of fields determined using the hydro geomorphic universal soil loss equation husle theurer and clarke 1991 sediment detachment transport and deposition processes for five classes of particle sizes clay silt sand and small and large aggregates are considered in annagnps cells are defined as basic modeling units and are assumed homogeneous i e no spatial variation in physical parameters within cells are considered this information can be obtained manually or through gis based spatial analysis such as averaging or majority allocation methods momm et al 2017 therefore when estimating sediment loads from sheet and rill erosional processes cells are depicted by a representative one segment rusle profile which is described by parameters from weather topography farming management and soil databases fig 2 the annagnps model has been successfully applied to a multitude of agricultural watersheds to quantify the effect of farming and or conservation practices and to support the development of plans for best management practices implementation yuan et al 2008 licciardello et al 2007 parajuli et al 2009 polyakov et al 2007 sarangia et al 2007 chahor et al 2013 2 3 field scale modeling the revised universal soil loss equation version 2 rusle2 erosion model was devised to support decision making on the selection and design of conservation practices with the ultimate objective of reducing long term soil erosion at field scales the rusle2 model is the result of revisions and improvements to previous versions of the universal soil loss equation usle wischmeier and smith 1978 and rusle renard et al 1997 like its previous versions rusle2 is used to estimate average annual soil loss from sheet and rill erosion processes based on user provided parameters describing weather soil properties topography and farming management foster et al 2003 in rusle2 basic modeling units agricultural fields can be internally modeled by a representative one dimensional rill slope profile however rusle2 offers the capability of modeling complex rill slopes using multi segment profiles in which each segment can be characterized by different parameters from multiple databases soil weather farming management conservation practices topography and others the rusle2 model estimates sheet and rill erosion using empirical equations and uses process based equations to calculate sediment transport capacity and deposition dabney et al 2011 additionally improvements in the rusle technology incorporated into to rusle2 model include an approach to calculate sediment deposition as result of slope steepness change temporally variable soil erodibility factor enhanced determination of the ratio to interrill erosion based on soil erodibility consolidation and residue cover adjustment of particle size deposition daily calculations are summed to generate average annual erosion and others for complete discussion of rusle2 improvements see foster et al 2003 kinnell 2010 and usda ars 2008 another important enhancement was the introduction of daily runoff driven calculations dabney et al 2011 this option allows for using daily precipitation and erosivity values either observed or from climate generators to calculate daily erosion and sediment delivery values rusle2 technology is available as a computer program with comprehensive graphical user interface with accompanying country wide database of input parameters for the continental usa an application programming interface api is also available for easy integration with other computer programs 2 4 input database development detailed yearly land use land cover information at individual field scales were recorded from 1982 to the present individual land use land cover layers were converted into farming management schedules and crop operations using standard rusle2 templates usa country wide database developed and maintained by u s department of agriculture as a starting point these databases were further modified for improved representation of local practices temporal sequences of land cover from 1982 to 1994 were converted into individual and unique management schedules see two examples in table 2 each management schedule was further described by a typical sequence of operations attributable to the land cover grown each year table 3 a management schedule is assembled by integrating yearly land use table 1 with representative sequences of operations table 2 for example for management 122 the first two years will repeat operations in the first column of table 2 and years 3 and 4 contains operations listed in third column a total of 234 unique management schedules and respective operations were developed and assigned to the 438 sub catchments the subdivision of the watershed into 438 sub catchments was based on topographic analysis using critical source area of 0 25 ha and minimum channel length of 50 m fig 1b a 3 m spatial resolution dem generated from lidar survey was preprocessed and hydrologically corrected to assure surface flow through constructed field drainage channels and culverts bridges the topagnps module also part of the agnps toolset was used to perform topographic parametrization and to generate the annagnps cell and reach input data sections momm et al 2012 cells were characterized for climate soil and management through a gis based overlay and majority allocation analysis between the sub catchment layer and layers containing climate soil and management momm et al 2017 a zone of influence for each precipitation gauging station was generated based on thiessen polygon analysis fig 3 a precipitation data within gcw was collected by usda ars from 30 gauges located in and around the watershed black triangles in fig 1 soil polygons from the ssurgo database were utilized to determine the spatial patterns of soil types with the associated soil databases generated using information from the us department of agriculture national conservation service nrcs national soil information system nasis fig 3b field boundaries were used as the basis for farm management descriptions fig 3c scs curve number values were defined based on farm management practices and the hydrologic soil group associated with each soil type 2 5 simulation approach the simulation approach for this study required three steps 1 simulation of the gcw station 14 watershed with annagnps to validate sediment loads and to identify the five highest producing erosion cells for use with rusle2 2 apply rusle2 to the selected five cells while applying various slope profile characterizations 3 perform integrated annagnps watershed simulations with each of the rusle2 slope profile characterizations for comparisons with 1 step 1 watershed scale simulation in the initial step annagnps was used to simulate gcw station 14 watershed conditions from 1982 to 1994 and to identify high sediment load producing cells which primarily consisted of row crops the simulation period from 1982 to 1994 was selected due to the availability of observed flow and sediment loads at the outlet and also to capture the land use change from intensive row crops to mostly pasture and forest 2 6 watershed scale model validation using monthly time scales simulated values of runoff and suspended sediment loads were quantitatively compared to observed values fig 4 the following statistical measurements were used nash sutcliffe efficiency nse pearson s correlation coefficient r percent bias pbias and root mean square error of observations standard deviation ratio rsr moriasi et al 2007 2015 1 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 2 r i 1 n o i o p i p i 1 n o i o 2 i 1 n p i p 2 3 p b i a s i 1 n o i p i i 1 n o i 100 4 r s r i 1 n o i p i 2 i 1 n o i p 2 in these equations n is the number of observations o is observed and p is predicted the nash sutcliffe equation 1 quantifies the discrepancies between the linear relationship of observed versus predicted values in relation to the one to one line moriasi et al 2007 the nse has been extensively used to describe the overall fit of hydrographs the pearson s correlation coefficient equation 2 describes the linear correlation between observed and predicted values the pbias equation 3 measures the average tendency of the simulated data to be larger or smaller than observed data moriasi et al 2007 and the rsr is the ratio between root mean squared error and the standard deviation of observations the lower the rsr value the better the model performance the model validation was performed in two steps stream flow and suspended sediment estimates observed runoff values were processed to filter out base flow as annagnps does not simulate base flow binger 1996 fig 4b input parameters were selected based on expert knowledge of this watershed reported in previous studies kuhnle et al 1996 2008 yasarer et al 2018 in the most recent study yasarer et al 2018 the entire goodwin creek watershed was simulated using the annagnps model with high agreement between observed and predicted at four stream flow gauges stations 5 11 and 14 and the watershed outlet without calibration of flow parameters at monthly time scales nse values at these four locations ranged from 0 91 to 0 94 r from 0 96 to 0 97 pbias from 0 32 to 16 and rsr scores were 0 24 0 30 table 1 in yasarer et al 2018 no changes from previous studies in input runoff curve number values controlling infiltration overland flow ratios for each land cover were needed fig 4b in the suspended sediment validation upland erosional processes were described using observed rusle2 rainfall runoff erosivity factor and values of rusle2 cover management and support practice factors associated with typical farming practices based on actual land use land cover measurements the main source of uncertainty in estimate suspended sediment at the outlet was how much suspended sediment was generated by channel processes versus how much was generated by upland processes minor adjustments were iteratively performed to improve validation statistic measures of suspended sediments loads agreement between observed and simulated total suspended sediment estimations were improved by changing input parameters controlling channel and upland erosion site visits to the watershed combined with previous studies kuhnle 1996 documenting median bed size material supported the selection of stream segments impacted by channel erosion fig 2 in kuhnle 1996 in the annagnps input database those segments were flagged and simulated by channel erosion algorithms fig 4c soil erodibility values for each soil were determined based on nomographs using the soil properties of percent silt percent sand percent organic matter soil structure class and permeability wischmeier et al 1971 fig 4d final validation statistical values are reported in table 4 based on threshold values proposed by moriasi et al 2015 the simulation results are within very good category for runoff nse 0 8 and pbias 5 within the upper bonds of the good category for total suspended sediment 0 70 nse 0 80 and within the good category for fine sediments 0 70 nse 0 80 and 10 pbias 15 these statistics give confidence the model is accurately representing current conditions and physical processes within the watershed 2 7 identification of high sediment producing cells the sediment contribution of each annagnps cell to the outlet was used as the criteria for top sediment producing cells selection based on the unit area ranked ratio analysis there are 10 cells that cover approximately 5 of the watershed that contribute to approximately 24 of all sediment delivered to the outlet fig 5 a out of these ten five were selected based on their management in which preference was given to cells with row crop management which were cells 83 312 522 552 and 1091 fig 5b estimates of flow and sediment from these five cells and their respective contribution to the outlet are herein referred to as scenario a i e estimates using standard rusle1 technology within the annagnps model 2 8 databases synchronization the five selected annagnps cells were simulated using the rusle2 erosion model through the rusle2 graphical user interface an important step was the synchronization of databases between the two models daily precipitation and ei values assigned to each annagnps cell were entered into the user provided climate section within rusle2 additionally modifications made to farming management and crop operations during annagnps input preparation were carefully replicated in the rusle2 input data sections soil type and properties individual managements farming operations curve numbers crop data and crop growth the objective was to have the annagnps and rusle2 databases matching as close to each other as possible so differences in estimates could be attributed to differences in rusle technology rather than input parameters the final step was to use the rusle2 built in crop rotation tools to develop the needed management schedules for the 12 year simulation period 2 9 slope profile definition and characterization each of the five selected annagnps cells required characterization using a representative slope profile multiple scenarios were considered as referred to later as scenarios bf for simplicity annagnps cell 83 was used to provide a detailed description of the approaches employed however the same methods apply to the other four cells a topagnps gis analyses was performed to generate raster grid files of flow direction flow accumulation local terrain slope and distance to channel fig 6 the longest flow path within the annagnps cell was selected as the representative slope profile for use in rusle2 blue line in fig 6 the slope length was determined as the total length of the longest flow path and the slope steepness as the average local terrain slope for all raster grid cells along the longest flow path dashed black line in fig 6 the slope profile was assigned the same management climate and soil as the annagnps cell this slope profile representation with one segment constituted scenario b in scenario c the slope profile was segmented based on farming management information specifically for annagnps cell 83 the rusle2 slope profile was represented by four segments fig 7 each segment was described by individual slope length slope steepness and management all segments were assigned the same soil type and precipitation gauge station slope profile was also subdivided based on landscape classification forming scenario d and intersection of management and landscape classification defining scenario e the landscape classification procedure groups individual raster grid cells into five slope profile features of summit shoulder backslope footslope and toeslope based on topographic attributes such as profile curvature slope gradient and relative elevation using different raster grid cell sizes miller and schetzl 2015 similar to scenario c each segment in d and e scenarios was described by slope length slope steepness and management while all segments were assigned the same soil type and weather station the last scenario considered f used the ls factor generated by topagnps to determine the rusle2 slope profile dimensions topagnps computes the ls factor values for each raster grid cell fig 8 and a single value for each annagnps cell is obtained by a raster weighted average method bingner and theurer 2001a b the ls factor for each of the selected annagnps cells was used to determine the slope length and slope steepness for rusle2 profiles a complete description of all scenarios considered for the five annagnps cells selected is presented in table 5 slope length and slope steepness values for scenario a annagnps internal calculations using rulse1 technology are the same as values for scenario f individual annagnps simulations for scenarios b to f were performed to route rusle2 generated erosion estimates for the five cells to downstream waterbodies and to account for interactions between sediment sources this was accomplished by implementing modifications to the annagnps model the cell data section was modified by adding a new rusle2 identification field if for example within an annagnps cell sub catchment a rusle2 input field is not designated then annagnps proceeds with standard rusle1 internal calculations however if a rusle2 identification code ids is found annagnps replaces the internal daily calculations of sheet and rill erosion for the selected cell by values stored in an external file in this study only the five selected annagnps cells received rusle2 ids for erosion estimates from step 2 3 results and discussion 3 1 field scale at the field scale observational data of sediment load output by each field is unavailable in the absence of edge of field measurement stations therefore a relative comparison between the six considered scenarios for the five selected cells was performed scenario a which represents standard annagnps rusle1 calculations was considered as the reference to which all other scenarios were compared although this relative comparison cannot define the predictive accuracy of any of the individual scenarios considered it still provides a quantitative measure of how different technologies perform and the impact of process characterization at scales smaller than cell area only sediment load estimates were compared because daily runoff in the selected annagnps cells did not vary between scenarios runoff discharge values from these annagnps cells in all scenarios were internally calculated by the annagnps model using the same topographic information annagnps cell overall topography thus the storm event runoff volumes and peak runoff rates determined by annagnps remain the same for each rusle2 slope profile in each alternative scenario different slope profile configurations were considered when simulating suspended sediment loads in the rusle2 model however in the annagnps simulations the only difference between scenarios were the sediment load produced by each of the selected cells quantitative comparisons using monthly and yearly temporal scales were considered fig 9 and table 6 the measures used were the coefficient of determination r2 nash sutcliffe efficiency nse percent bias pbias and standard deviation of measured data rsr highest agreement was obtained between scenarios a and f geter and theurer 1998 the slope profile is characterized with similar input parameters in both scenarios and these estimates differ only in the erosion estimation technology used annagnps rusle1 and rusle2 for scenarios a and f respectively fig 9 for total erosion amounts by sheet and rill processes for cells 83 522 552 and 1091 there was a good agreement at monthly scale r2 0 73 and nse 0 36 and very good to excellent agreement at yearly scale r2 0 8 and nse 0 66 annagnps cell 312 yielded a smaller level of agreement with values of r2 0 59 and nse 1 75 for monthly and r2 0 58 and nse 1 19 for yearly table 6 studies performing direct comparison between rusle1 and rusle2 estimations have indicated the potential for differences within 20 for annual averages foster et al 2003 however further discrepancies in this study are attributed to custom implementation of rusle1 technology within the annagnps model to efficiently perform analysis at watershed scale for example while a daily time step is used within the annagnps model a 15 day increment is used for determining the rusle1 cover management c factor and soil erodibility k factor while the rusle2 uses a daily time step bingner et al 2018 usda ars 2008 still statistic values comparing scenarios a and f for cell 312 produced higher levels of agreement than other scenarios absolute differences between scenarios a and f in annual average sediment loads for clay size particles range from 2 3 mg yr largest to 0 3 mg yr smallest and for silt size particles from 11 6 mg yr largest to 0 4 mg yr smallest for cell 83 and 1091 respectively comparisons between monthly averages for silt and clay particle sizes for these cells indicate analogous timing of when sediment erosion occurs fig 10 early in the growing season when soils are exposed and in the winter raining season the overall agreement between the two scenarios indicates proper synchronization of input databases and more importantly similar estimation of total sediment erosion from both annagnps rusle1 and rusle2 erosion technologies when a single segment slope profile is considered comparisons between estimates from scenarios a and b show small agreement between them as represented by small nse and high pbias values fig 9 and table 6 high pbias suggests results from scenario b are larger than from scenario a see green circle line and black line in fig 9 in addition to the different erosion estimation technology used these two scenarios also differ in how the slope profile was defined in a it was defined by a weighted average of raster grid cells and in b by gis analysis of the longest flow path in each annagnps cell sub catchment scenarios a and b used a single segment slope profile but in scenario b with higher slope length for all five annagnps cells considered table 5 percent difference between scenarios a and b for clay and silt particle sizes on annual average loads range from 160 max cell 552 to 70 min cell 1091 for clay and from 147 max cell 552 to 40 min cell 1091 for silt at monthly time scale for all cells and all particle sizes the highest difference is in april and may for example 10 mg for scenario b versus 0 26 mg for scenario a for may the longer slope profile explains the higher sediment load estimation by scenario b than a this demonstrates the importance of appropriate slope length definition it has also been suggested that slope profiles with slope length greater than 100 m should be avoided as they may not be representative of the rusle empirical relationships developed using plots with standard dimensions kinnell 2010 similar to scenario b scenario d differs from scenario a in the erosion technology used and how the slope profile was defined however in scenario d the slope profile still has the slope length defined by the longest flow path in each cell but it is subdivided into multiple segments defined based on topography landform classification statistical and graphical results between scenarios a and d indicate a higher agreement than scenarios a and b same slope length but single slope steepness the highest differences were observed for cell 552 nse 140 and pbias 1000 the significant difference in soil loads estimates when compared to scenario a can be attributed to topographic characteristics of slope length of 10 m for scenario a and 292 m for scenario b and d scenario b has one segment of 292 m and slope steepness of 11 while scenario d has total slope length of 292 m but with multiple segments three of them with slope steepness less than 8 and one with 15 slope steepness the difference sediment loads in cell 552 from scenario d monthly average for may of 241 mg for clay and 1110 mg for silt when compared to scenario b monthly average for may of 205 mg for clay and 950 mg for silt can be attributed to enhanced representation of topographic features combined with rusle2 s capability of estimating sediment transport deposition between segments based on slope steepness analysis of results for cell 83 between scenarios a and c and a and e illustrates the influence of management in sediment load estimates pbias of 79 and 85 for c and e respectively graphical illustration for annagnps cell 83 depicting subdivision of slope profile based on management for scenario c is provided in fig 7 specifically for annagnps cell 83 under scenarios b and c the same slope length in both alternatives is considered but in scenario c multiple segments are simulated based on management table 5 in scenario b the entire slope profile is assigned management id 130 while in scenario c the second segment is assigned management id 153 which describes idle field management of either conversion to pasture or to conservation reserve program crp land this change in management yields an annual average total sediment load reduction from 250 mg yr b to 15 mg yr c evaluation of total monthly averages indicate change for both clay and silt particle sizes fig 11 the biomass in this management is composed primarily of grass like vegetation with small or no periods of exposed soil the above ground vegetation can reduce surface flow velocity and promote deposition yuan et al 2009 as simulated by rusle2 see scenarios c and e for annagnps cells 83 in fig 9 conversely for annagnps cell 312 there is a peak in sediment load estimation for the first two years blue line in fig 9 the second segment in the slope profile for scenario c is assigned to management 134 soybeans in first two years than idle after that the management combined with the high slope length and slope steepness contributed to high estimates of sediment load in the first two years however after the first two years estimated sediment load diminishes despite the high slope steepness as management becomes the dominant factor in controlling sediment erosion 3 2 watershed scale routing sediments originated in cells fields throughout the watershed generated information on the impact of local practices to other watershed systems located downstream of the cell being investigated and the overall watershed s sediment load output additionally it can aid in the selection process of areas to be targeted for mitigation a useful evaluation procedure is to sort cells from the highest to the lowest total average annual erosion per unit of area then calculate the accumulated drainage area and the accumulated total erosion mass associated with the sorted cells each cell s values can then be divided by the entire watershed accumulated drainage area and erosion values expressed as percentage of total drainage area and erosion fig 12 this graphical representation is helpful to identify critical sediment producing cells for example in fig 12a a vertical line denotes 20 of total drainage area x axis however for each scenario the same drainage area corresponds to a different contributing load percentage for scenario a black line it corresponds to approximately 60 of the total sediment load and for scenario b it corresponds to approximately 80 of total sediment load reaching the outlet additionally this analysis is based on sorting cells using their total annual average erosion per unit area values therefore varying how sediment sources are modeled may lead to differences in the cell s sediment load estimation and consequently in the selection of critical sediment producing cells fig 12b the five selected cells in this study vary significantly in this analysis reflecting their differences in erosion output and consequently their potential selection for mitigation similar analysis can be performed spatially in the form of map analysis of the highest cell average annual erosion per unit of area fig 13 likewise how the ls factor and farming management is represented affects the cell s sediment production and how each cell is compared to others cells although the six scenarios evaluated only differ in how five out of 438 annagnps cells were simulated these selected cells impact the overall watershed output as demonstrated by comparison with reference scenario a for fine sediments table 7 similar to results at the field scale watershed scale analysis considering contribution of all annagnps cells to the watershed outlet sheet and rill sources only scenarios a and f generated comparable average annual unit area estimates of fine sediment clay and silt at the outlet 0 77 mg ha and 1 93 mg ha for scenario a and 0 85 mg ha and 2 00 mg ha for scenario f however other scenarios produced over estimation of fine sediment loading for example scenarios b and d yielded pbias of 27 6 and 23 8 respectively when comparing total contribution of suspended sediment load to the watershed outlet with reference similar pattern was obtained for annual averages of clay and silt 1 41 mg ha and 2 73 mg ha b and 1 35 mg ha and 2 57 mg ha d these findings signify the importance of characterizing field scale activities and the improved representation of field slope profiles by technologies with capabilities for multi segment representation and detachment and deposition estimations 4 conclusions this study developed and evaluated a methodology for watershed scale simulation using a multi step approach based on enhanced description of erosional processes at field scales two models annagnps and rusle2 were evaluated through the identification of critical sediment producing cells using the annagnps model simulation of selected critical annagnps cells using the rusle2 model and routing of the sediment load from these annagnps cells through the watershed using the integrated annagnps rusle2 modeling methodology this approach utilizes annagnps as a screening tool to identify highly erosive areas that rusle2 would then be applied for advanced conservation practice planning evaluation and implementation significant efforts were devoted to input database integration although these models are both developed and supported by the u s department of agriculture and despite the fact they share conceptual similarities for sheet and rill erosion calculations both are rooted in the usle and rusle predecessors they differ on how information is stored computer file format and overall data organization and even in the terminology used to describe input parameters these dissimilarities increased the learning time needed for input databases integration the development of automated semi automated methods to facilitate exchange of information between the two technologies will support future multi scale watershed field integrated application of these models observed edge of the field measurements were not available and therefore a relative comparison between alternative scenarios were performed in which the standard annagnps rulse1 alternative scenario a was selected as the reference since in this configuration the annagnps model was validated using watershed scale observed values watershed outlet results at field and watershed scale comparing single segment slope profiles described by the same input parameters but estimated using different technologies annagnps rusle1 scenario a and rusle2 scenario f yielded comparable results at yearly and monthly time scales these findings signify the similarities between the two technologies when a single segment is simulated and the appropriate synchronization of input databases within each model the option of defining slope profiles composed of multi segments exploits rusle2 s capabilities of characterization of individual segment based on topography slope length and slope steepness soil and management and availability of algorithms for sediment transport capacity and or deposition balance the challenge is in the definition of the representative slope profile slope profiles based on gis analysis can be discretized and characterized in different ways the same slope profile can be partitioned based on topography soil land use and other field parameters in here gis analysis was used to define the longest flow path within sub catchments annagnps cells flow paths were subdivided based on topography management and a combination of both results indicate that the multi segment approach is more sensitive to farming management land use than topography as demonstrated by cell 83 under scenarios b and c in which in scenario c one of the grassed managements performed as a riparian buffer strip reducing sediment loads reaching the bottom of the slope percent change of 177 on annual average conversely enhanced characterization at scales smaller than basic modeling units have the potential to describe field based conservation practices such as edge of field buffer strips grassed waterways and contouring each unique field scale conservation practice could be represented by multi segmented slope profiles the challenge resides in the appropriate definition of the representative profile for each cell slope steepness and slope length as observed by the results ls factors definition significantly affects erosion estimate results including estimates of clay and silt sized particles differently the definition of representative slope profiles for selected cells should be the subject of future studies likewise future enhancements to the annagnps model should include continued development of modules that describe physical parameters at scales smaller than basic modeling units reaches and cells examples of existing modules with such characteristics include the wetland component agwet momm et al 2016 at scales smaller than reaches the ephemeral gully component peg momm et al 2012 at scales smaller than cells and the riparian buffer component agbuf momm et al 2014 at scales smaller than reaches and cells the rusle2 model has been recognized as an important tool for agriculture conservation and farming practices design and evaluation not only in the united states but also around the world this is due to its reliability in estimating soil erosion at the field scale simplicity and accompanying ready to use input database kinnell 2010 integrating rusle2 with annagnps has the potential for improved characterization of field parameters controlling sediment erosion at scales smaller than field while at the same time routing field estimates to downstream waterbodies through channel physical processes however future enhancements should be focused on distributed and or computationally efficiency rusle2 implementations that allow for easier integration with other models and or web based deployments this could facilitate estimation of sheet and rill erosion at scales smaller than basic modeling units used by watershed models the proposed framework can serve as template for integrating hydrological watershed scale or even basin scale models with erosion field scale models in addition to allowing for enhanced characterization of conservation practices it offers a platform for routing edge of field observed values through the watershed as computing science and technology improves there is opportunity to re visit how watershed physical processes are described and modeled and how modeling technology is accessed by end users these improved modeling efforts support the evaluation of existing conditions and assessment of proposed conservation practices software availability both models annagnps and rusle2 are distributed by the u s department of agriculture natural resources conservation service nrcs agency annagnps https www nrcs usda gov wps portal nrcs detailfull null cid stelprdb1042468 and rusle2 https www nrcs usda gov wps portal nrcs detail national technical tools rusle2 cid stelprdb1247278 for further inquiries on these software packages please contact ronald bingner ron bingner ars usda gov leading scientist and project manager for the annagnps model and robert wells robert wells ars usda gov leading scientist and project manager for the rusle2 model acknowledgements the authors would like to acknowledge the support provided by glenn herring in performing modifications to the annagnps source code and to evan snap for programming the annagnps rusle2 computer interface this work was supported by funds provided by the u s department of agriculture agricultural research service 
26216,watershed scale simulation technology allows quantification of the impact of individual and or integrated management practices throughout the watershed in the annagnps watershed scale model the watershed is subdivided into basic modeling units fields in which all spatially varying physical parameters are assumed to be homogeneous conversely rusle2 model can estimate sediment yield at sub field scale with enhanced characterization capabilities in this study an integrated approach was developed and evaluated that combines both models for enhanced representation of individual fields while accounting for the watershed wide integrated intra fields effect when routing sediment loads through the watershed critical sediment producing areas identified through integrated rusle2 erosion and annagnps sediment transport models can be used to support the development and evaluation of conservation management plans specific to fields but impacting the entire watershed keywords watershed simulation soil erosion annagnps rusle2 integrated field watershed modeling 1 introduction watershed scale modeling technology supports efforts to improve agriculture sustainability through quantitative spatio temporal evaluation of farming and conservation practice alternatives the impact of implementing a particular practice needs to be assessed at the field scale field where the practice was implemented and more importantly at the watershed scale through determination of the integrated effects of an applied practice and corresponding physical process impact to downstream watershed systems at the watershed scale the basin is commonly divided into cells also referred to as sub catchments sub watersheds or hydrological response units representing uplands and reaches denoting concentrated flow bingner and theurer 2001a b fitzhugh and mackay 2001 momm et al 2017 the entire basin is represented as a hierarchical structure in which cells are connected to reaches based on estimated surface flow momm et al 2017 basic modeling units are characterized by a large number of parameters depicting soils climate farming management topography and hydrology and each parameter is assumed to be spatially invariant within cells and reaches fitzhugh and mackay 2000 this homogeneity assumption within basic modeling units restricts description of parameters variation at scales smaller than cell sizes and can affect how physical processes are described however it provides conservation practice assessment at the watershed scale through supporting timely input database preparation simplified analysis of results and reasonable computer programming execution time at the field scale models are conceptualized using a one dimensional representative profile that can be further divided into segments to characterize variation within field sized areas foster et al 2003 williams and izaurralde 2006 flanagan and nearing 1995 these models were designed to predict soil losses at field scale to inform users on development of targeted conservation practices one clear advantage is the enhanced characterization of features at scales smaller than field size areas smaller than cells however application of field scale models to entire watersheds requires significant effort for database input generation and profile characterization as well as high computational cost moreover this approach only depicts local field impacts as there is no mechanism to evaluate the integrated effect of individual practices and or physical processes throughout the watershed while both watershed and field scale models have strengths and limitations an integrated alternative combining these technologies has the potential to provide improved field characterization while also estimating complex relationships between practices and physical processes by integrating the routing of overland flow and suspended material throughout the watershed comparisons of field scale sediment load estimates from soil and water assessment tool swat watershed scale model arnold et al 1993 with estimates from revised universal soil loss equation version 2 rusle2 field scale model usda ars 2008 indicated discrepancies between field and watershed scale models in their capability to select critical sediment producing areas sommerlot et al 2013a sommerlot et al 2013a attributed these findings to the enhanced description of physical processes and farming management practices at scales smaller than field area by the field scale model and therefore this study recognized the need for a watershed field integrated solution similarly the swat model has been integrated with the agricultural policy environmental extender apex field level model williams and izaurralde 2006 for enhanced characterization of farming operations crop growth and conservation practices such as filter strips terraces and waterways wang et al 2011 however this study was focused on a regional scale u s 8 digit watershed level with no channel routing components used limiting this way the routing of overland flow and suspended material throughout the watershed wang et al 2011 another common characteristic of regional scale watershed models such as swat is the assumption that farming management assigned to sub catchments are either static one year management repeated for the duration of the simulation or pseudo static few years of crop rotation repeated for the duration of the simulation this assumption is required for simplified representation of reality within models simulating physical processes at these large scales even when swat is integrated with field scale models such as apex saleh et al 2000 better integrated methods capable of 1 capturing processes at scales within and not just at the edge of fields cells 2 including routing field estimates to downstream waterbodies while at the same time 3 integrating these estimates from other fields and with channel processes are critically needed for watershed conservation management planning sommerlot et al 2013a b in this study an integrated watershed field scales modeling framework is proposed and evaluated the annualized agricultural non point source annagnps watershed pollution model bingner et al 2015 was selected at the watershed scale and the revised universal soil loss equation version 2 rusle2 soil erosion model usda ars 2008 at the field scale by integrating the annagnps model with rusle2 at the watershed scale this combined approach can be used as an enhanced screening tool to identify non point source producing critical cells sub catchments next those selected cells are further characterized and simulated using the rusle2 model finally soil erosion estimates from rusle2 are supplied to another annagnps model simulation for evaluation of potential physical processes interactions and flow sediment routing throughout the watershed specifically the objectives of this study are three fold 1 development and evaluation of methods for integrating annagnps and rusle2 models 2 evaluation of sediment loads estimates at the field scale generated by annagnps and rusle2 models 3 assessment of the integrated multi step multi scale annagnps rusle2 modeling system sediment yield estimations at the watershed outlet 2 methods 2 1 study site 2 1 1 goodwin creek watershed goodwin creek watershed gcw is an experimental watershed located southeast of batesville mississippi in the loess covered bluff hills of the yazoo river basin fig 1 a gcw is monitored by the usda ars national sedimentation laboratory nsl that supports research involving upland erosion stream erosion and sedimentation and watershed hydrology alonso and bingner 2000 elevation of the watershed ranges from 71 m to 128 m above sea level soils in the watershed comprise two major associations on the cultivated flood plains and terrace areas the soil belongs to the falaya grenada calloway association which area are poorly to moderately well drained pasture and wooded areas on the watershed s loess ridges and hillsides are characterized by the loring grenada memphis soil association which are silty in texture and easily eroded in the absence of vegetative cover average daily maximum temperature is 30 c in the summer and 10 c in the winter with high air humidity throughout the year the majority of large runoff events occurs during the winter and spring months table 1 streamflow is sensitive to rainfall patters and individual storm events can generate flows capable of transporting large particle sizes kuhnle et al 1996 binger 1996 the average annual rainfall is 1440 mm yr 1 the land use land cover contains row crops pasture and forest in the early 1980s row crop was the dominant land use but after 1990s agriculture was replaced by native forest and pasture kuhnle et al 2008 corn soybeans and cotton were the primary crops produced on cultivated land the selected study area was a subset of the gcw defined by the outlet selected at stream gauge 14 green triangle in fig 1 stream gauge 14 was monitored for discharge and suspended sediment loads using constructed in stream flume structures these flumes were built with v shape profile and a moderate slope of 0 04 m m to prevent sediment deposition kuhnle et al 2008 additionally daily precipitation information and rusle storm erosivity ei was monitored at the stream gauge 14 outlet and four other precipitation gauge sites black triangle in fig 1b daily minimum and maximum air temperatures were obtained from a nearby weather station within gcw a complete revised dataset containing weather and flow and suspended sediment information between 1982 and 1994 were available and used to develop and validate the watershed simulations 2 2 watershed scale modeling the agricultural non point source agnps pollution modeling system contains a set of integrated tools for distributed event based long term modeling and simulation of surface runoff sediment nutrient nitrogen phosphorus and organic carbon and pesticide transport primarily from agricultural watersheds young et al 1989 the agnps model was originally designed to simulate un gauged watersheds through enhanced processes characterization such as dynamic management description one component in the agnps modeling system is the annualized agricultural non point source annagnps pollution model containing capabilities for continuous simulation watershed scale evaluation mixed land use and surface runoff modeling and simulation usda ars 2015 surface hydrological fluxes are estimated daily using weather information such as precipitation maximum and minimum temperatures dew point temperatures solar radiation or sky cover and wind speed zema et al 2012 the watershed is subdivided into cells and reaches based on user provided input either manually or using topographic attributes surface runoff sediment and pollutant estimates generated in cells are routed into reaches allowing for interpretation evaluation of results spatially and for investigation of inter relation between practices at cells surface runoff is determined based on the scs curve number method usda scs 1972 coupled with daily surface hydrological fluxes and farming management practices crop seasonal rotations and crop specific farming operations estimates of sediment load from sheet and rill erosion processes in each cell are generated daily using the revised universal soil loss equation rusle method renard et al 1997 and their delivery to the edge of fields determined using the hydro geomorphic universal soil loss equation husle theurer and clarke 1991 sediment detachment transport and deposition processes for five classes of particle sizes clay silt sand and small and large aggregates are considered in annagnps cells are defined as basic modeling units and are assumed homogeneous i e no spatial variation in physical parameters within cells are considered this information can be obtained manually or through gis based spatial analysis such as averaging or majority allocation methods momm et al 2017 therefore when estimating sediment loads from sheet and rill erosional processes cells are depicted by a representative one segment rusle profile which is described by parameters from weather topography farming management and soil databases fig 2 the annagnps model has been successfully applied to a multitude of agricultural watersheds to quantify the effect of farming and or conservation practices and to support the development of plans for best management practices implementation yuan et al 2008 licciardello et al 2007 parajuli et al 2009 polyakov et al 2007 sarangia et al 2007 chahor et al 2013 2 3 field scale modeling the revised universal soil loss equation version 2 rusle2 erosion model was devised to support decision making on the selection and design of conservation practices with the ultimate objective of reducing long term soil erosion at field scales the rusle2 model is the result of revisions and improvements to previous versions of the universal soil loss equation usle wischmeier and smith 1978 and rusle renard et al 1997 like its previous versions rusle2 is used to estimate average annual soil loss from sheet and rill erosion processes based on user provided parameters describing weather soil properties topography and farming management foster et al 2003 in rusle2 basic modeling units agricultural fields can be internally modeled by a representative one dimensional rill slope profile however rusle2 offers the capability of modeling complex rill slopes using multi segment profiles in which each segment can be characterized by different parameters from multiple databases soil weather farming management conservation practices topography and others the rusle2 model estimates sheet and rill erosion using empirical equations and uses process based equations to calculate sediment transport capacity and deposition dabney et al 2011 additionally improvements in the rusle technology incorporated into to rusle2 model include an approach to calculate sediment deposition as result of slope steepness change temporally variable soil erodibility factor enhanced determination of the ratio to interrill erosion based on soil erodibility consolidation and residue cover adjustment of particle size deposition daily calculations are summed to generate average annual erosion and others for complete discussion of rusle2 improvements see foster et al 2003 kinnell 2010 and usda ars 2008 another important enhancement was the introduction of daily runoff driven calculations dabney et al 2011 this option allows for using daily precipitation and erosivity values either observed or from climate generators to calculate daily erosion and sediment delivery values rusle2 technology is available as a computer program with comprehensive graphical user interface with accompanying country wide database of input parameters for the continental usa an application programming interface api is also available for easy integration with other computer programs 2 4 input database development detailed yearly land use land cover information at individual field scales were recorded from 1982 to the present individual land use land cover layers were converted into farming management schedules and crop operations using standard rusle2 templates usa country wide database developed and maintained by u s department of agriculture as a starting point these databases were further modified for improved representation of local practices temporal sequences of land cover from 1982 to 1994 were converted into individual and unique management schedules see two examples in table 2 each management schedule was further described by a typical sequence of operations attributable to the land cover grown each year table 3 a management schedule is assembled by integrating yearly land use table 1 with representative sequences of operations table 2 for example for management 122 the first two years will repeat operations in the first column of table 2 and years 3 and 4 contains operations listed in third column a total of 234 unique management schedules and respective operations were developed and assigned to the 438 sub catchments the subdivision of the watershed into 438 sub catchments was based on topographic analysis using critical source area of 0 25 ha and minimum channel length of 50 m fig 1b a 3 m spatial resolution dem generated from lidar survey was preprocessed and hydrologically corrected to assure surface flow through constructed field drainage channels and culverts bridges the topagnps module also part of the agnps toolset was used to perform topographic parametrization and to generate the annagnps cell and reach input data sections momm et al 2012 cells were characterized for climate soil and management through a gis based overlay and majority allocation analysis between the sub catchment layer and layers containing climate soil and management momm et al 2017 a zone of influence for each precipitation gauging station was generated based on thiessen polygon analysis fig 3 a precipitation data within gcw was collected by usda ars from 30 gauges located in and around the watershed black triangles in fig 1 soil polygons from the ssurgo database were utilized to determine the spatial patterns of soil types with the associated soil databases generated using information from the us department of agriculture national conservation service nrcs national soil information system nasis fig 3b field boundaries were used as the basis for farm management descriptions fig 3c scs curve number values were defined based on farm management practices and the hydrologic soil group associated with each soil type 2 5 simulation approach the simulation approach for this study required three steps 1 simulation of the gcw station 14 watershed with annagnps to validate sediment loads and to identify the five highest producing erosion cells for use with rusle2 2 apply rusle2 to the selected five cells while applying various slope profile characterizations 3 perform integrated annagnps watershed simulations with each of the rusle2 slope profile characterizations for comparisons with 1 step 1 watershed scale simulation in the initial step annagnps was used to simulate gcw station 14 watershed conditions from 1982 to 1994 and to identify high sediment load producing cells which primarily consisted of row crops the simulation period from 1982 to 1994 was selected due to the availability of observed flow and sediment loads at the outlet and also to capture the land use change from intensive row crops to mostly pasture and forest 2 6 watershed scale model validation using monthly time scales simulated values of runoff and suspended sediment loads were quantitatively compared to observed values fig 4 the following statistical measurements were used nash sutcliffe efficiency nse pearson s correlation coefficient r percent bias pbias and root mean square error of observations standard deviation ratio rsr moriasi et al 2007 2015 1 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 2 r i 1 n o i o p i p i 1 n o i o 2 i 1 n p i p 2 3 p b i a s i 1 n o i p i i 1 n o i 100 4 r s r i 1 n o i p i 2 i 1 n o i p 2 in these equations n is the number of observations o is observed and p is predicted the nash sutcliffe equation 1 quantifies the discrepancies between the linear relationship of observed versus predicted values in relation to the one to one line moriasi et al 2007 the nse has been extensively used to describe the overall fit of hydrographs the pearson s correlation coefficient equation 2 describes the linear correlation between observed and predicted values the pbias equation 3 measures the average tendency of the simulated data to be larger or smaller than observed data moriasi et al 2007 and the rsr is the ratio between root mean squared error and the standard deviation of observations the lower the rsr value the better the model performance the model validation was performed in two steps stream flow and suspended sediment estimates observed runoff values were processed to filter out base flow as annagnps does not simulate base flow binger 1996 fig 4b input parameters were selected based on expert knowledge of this watershed reported in previous studies kuhnle et al 1996 2008 yasarer et al 2018 in the most recent study yasarer et al 2018 the entire goodwin creek watershed was simulated using the annagnps model with high agreement between observed and predicted at four stream flow gauges stations 5 11 and 14 and the watershed outlet without calibration of flow parameters at monthly time scales nse values at these four locations ranged from 0 91 to 0 94 r from 0 96 to 0 97 pbias from 0 32 to 16 and rsr scores were 0 24 0 30 table 1 in yasarer et al 2018 no changes from previous studies in input runoff curve number values controlling infiltration overland flow ratios for each land cover were needed fig 4b in the suspended sediment validation upland erosional processes were described using observed rusle2 rainfall runoff erosivity factor and values of rusle2 cover management and support practice factors associated with typical farming practices based on actual land use land cover measurements the main source of uncertainty in estimate suspended sediment at the outlet was how much suspended sediment was generated by channel processes versus how much was generated by upland processes minor adjustments were iteratively performed to improve validation statistic measures of suspended sediments loads agreement between observed and simulated total suspended sediment estimations were improved by changing input parameters controlling channel and upland erosion site visits to the watershed combined with previous studies kuhnle 1996 documenting median bed size material supported the selection of stream segments impacted by channel erosion fig 2 in kuhnle 1996 in the annagnps input database those segments were flagged and simulated by channel erosion algorithms fig 4c soil erodibility values for each soil were determined based on nomographs using the soil properties of percent silt percent sand percent organic matter soil structure class and permeability wischmeier et al 1971 fig 4d final validation statistical values are reported in table 4 based on threshold values proposed by moriasi et al 2015 the simulation results are within very good category for runoff nse 0 8 and pbias 5 within the upper bonds of the good category for total suspended sediment 0 70 nse 0 80 and within the good category for fine sediments 0 70 nse 0 80 and 10 pbias 15 these statistics give confidence the model is accurately representing current conditions and physical processes within the watershed 2 7 identification of high sediment producing cells the sediment contribution of each annagnps cell to the outlet was used as the criteria for top sediment producing cells selection based on the unit area ranked ratio analysis there are 10 cells that cover approximately 5 of the watershed that contribute to approximately 24 of all sediment delivered to the outlet fig 5 a out of these ten five were selected based on their management in which preference was given to cells with row crop management which were cells 83 312 522 552 and 1091 fig 5b estimates of flow and sediment from these five cells and their respective contribution to the outlet are herein referred to as scenario a i e estimates using standard rusle1 technology within the annagnps model 2 8 databases synchronization the five selected annagnps cells were simulated using the rusle2 erosion model through the rusle2 graphical user interface an important step was the synchronization of databases between the two models daily precipitation and ei values assigned to each annagnps cell were entered into the user provided climate section within rusle2 additionally modifications made to farming management and crop operations during annagnps input preparation were carefully replicated in the rusle2 input data sections soil type and properties individual managements farming operations curve numbers crop data and crop growth the objective was to have the annagnps and rusle2 databases matching as close to each other as possible so differences in estimates could be attributed to differences in rusle technology rather than input parameters the final step was to use the rusle2 built in crop rotation tools to develop the needed management schedules for the 12 year simulation period 2 9 slope profile definition and characterization each of the five selected annagnps cells required characterization using a representative slope profile multiple scenarios were considered as referred to later as scenarios bf for simplicity annagnps cell 83 was used to provide a detailed description of the approaches employed however the same methods apply to the other four cells a topagnps gis analyses was performed to generate raster grid files of flow direction flow accumulation local terrain slope and distance to channel fig 6 the longest flow path within the annagnps cell was selected as the representative slope profile for use in rusle2 blue line in fig 6 the slope length was determined as the total length of the longest flow path and the slope steepness as the average local terrain slope for all raster grid cells along the longest flow path dashed black line in fig 6 the slope profile was assigned the same management climate and soil as the annagnps cell this slope profile representation with one segment constituted scenario b in scenario c the slope profile was segmented based on farming management information specifically for annagnps cell 83 the rusle2 slope profile was represented by four segments fig 7 each segment was described by individual slope length slope steepness and management all segments were assigned the same soil type and precipitation gauge station slope profile was also subdivided based on landscape classification forming scenario d and intersection of management and landscape classification defining scenario e the landscape classification procedure groups individual raster grid cells into five slope profile features of summit shoulder backslope footslope and toeslope based on topographic attributes such as profile curvature slope gradient and relative elevation using different raster grid cell sizes miller and schetzl 2015 similar to scenario c each segment in d and e scenarios was described by slope length slope steepness and management while all segments were assigned the same soil type and weather station the last scenario considered f used the ls factor generated by topagnps to determine the rusle2 slope profile dimensions topagnps computes the ls factor values for each raster grid cell fig 8 and a single value for each annagnps cell is obtained by a raster weighted average method bingner and theurer 2001a b the ls factor for each of the selected annagnps cells was used to determine the slope length and slope steepness for rusle2 profiles a complete description of all scenarios considered for the five annagnps cells selected is presented in table 5 slope length and slope steepness values for scenario a annagnps internal calculations using rulse1 technology are the same as values for scenario f individual annagnps simulations for scenarios b to f were performed to route rusle2 generated erosion estimates for the five cells to downstream waterbodies and to account for interactions between sediment sources this was accomplished by implementing modifications to the annagnps model the cell data section was modified by adding a new rusle2 identification field if for example within an annagnps cell sub catchment a rusle2 input field is not designated then annagnps proceeds with standard rusle1 internal calculations however if a rusle2 identification code ids is found annagnps replaces the internal daily calculations of sheet and rill erosion for the selected cell by values stored in an external file in this study only the five selected annagnps cells received rusle2 ids for erosion estimates from step 2 3 results and discussion 3 1 field scale at the field scale observational data of sediment load output by each field is unavailable in the absence of edge of field measurement stations therefore a relative comparison between the six considered scenarios for the five selected cells was performed scenario a which represents standard annagnps rusle1 calculations was considered as the reference to which all other scenarios were compared although this relative comparison cannot define the predictive accuracy of any of the individual scenarios considered it still provides a quantitative measure of how different technologies perform and the impact of process characterization at scales smaller than cell area only sediment load estimates were compared because daily runoff in the selected annagnps cells did not vary between scenarios runoff discharge values from these annagnps cells in all scenarios were internally calculated by the annagnps model using the same topographic information annagnps cell overall topography thus the storm event runoff volumes and peak runoff rates determined by annagnps remain the same for each rusle2 slope profile in each alternative scenario different slope profile configurations were considered when simulating suspended sediment loads in the rusle2 model however in the annagnps simulations the only difference between scenarios were the sediment load produced by each of the selected cells quantitative comparisons using monthly and yearly temporal scales were considered fig 9 and table 6 the measures used were the coefficient of determination r2 nash sutcliffe efficiency nse percent bias pbias and standard deviation of measured data rsr highest agreement was obtained between scenarios a and f geter and theurer 1998 the slope profile is characterized with similar input parameters in both scenarios and these estimates differ only in the erosion estimation technology used annagnps rusle1 and rusle2 for scenarios a and f respectively fig 9 for total erosion amounts by sheet and rill processes for cells 83 522 552 and 1091 there was a good agreement at monthly scale r2 0 73 and nse 0 36 and very good to excellent agreement at yearly scale r2 0 8 and nse 0 66 annagnps cell 312 yielded a smaller level of agreement with values of r2 0 59 and nse 1 75 for monthly and r2 0 58 and nse 1 19 for yearly table 6 studies performing direct comparison between rusle1 and rusle2 estimations have indicated the potential for differences within 20 for annual averages foster et al 2003 however further discrepancies in this study are attributed to custom implementation of rusle1 technology within the annagnps model to efficiently perform analysis at watershed scale for example while a daily time step is used within the annagnps model a 15 day increment is used for determining the rusle1 cover management c factor and soil erodibility k factor while the rusle2 uses a daily time step bingner et al 2018 usda ars 2008 still statistic values comparing scenarios a and f for cell 312 produced higher levels of agreement than other scenarios absolute differences between scenarios a and f in annual average sediment loads for clay size particles range from 2 3 mg yr largest to 0 3 mg yr smallest and for silt size particles from 11 6 mg yr largest to 0 4 mg yr smallest for cell 83 and 1091 respectively comparisons between monthly averages for silt and clay particle sizes for these cells indicate analogous timing of when sediment erosion occurs fig 10 early in the growing season when soils are exposed and in the winter raining season the overall agreement between the two scenarios indicates proper synchronization of input databases and more importantly similar estimation of total sediment erosion from both annagnps rusle1 and rusle2 erosion technologies when a single segment slope profile is considered comparisons between estimates from scenarios a and b show small agreement between them as represented by small nse and high pbias values fig 9 and table 6 high pbias suggests results from scenario b are larger than from scenario a see green circle line and black line in fig 9 in addition to the different erosion estimation technology used these two scenarios also differ in how the slope profile was defined in a it was defined by a weighted average of raster grid cells and in b by gis analysis of the longest flow path in each annagnps cell sub catchment scenarios a and b used a single segment slope profile but in scenario b with higher slope length for all five annagnps cells considered table 5 percent difference between scenarios a and b for clay and silt particle sizes on annual average loads range from 160 max cell 552 to 70 min cell 1091 for clay and from 147 max cell 552 to 40 min cell 1091 for silt at monthly time scale for all cells and all particle sizes the highest difference is in april and may for example 10 mg for scenario b versus 0 26 mg for scenario a for may the longer slope profile explains the higher sediment load estimation by scenario b than a this demonstrates the importance of appropriate slope length definition it has also been suggested that slope profiles with slope length greater than 100 m should be avoided as they may not be representative of the rusle empirical relationships developed using plots with standard dimensions kinnell 2010 similar to scenario b scenario d differs from scenario a in the erosion technology used and how the slope profile was defined however in scenario d the slope profile still has the slope length defined by the longest flow path in each cell but it is subdivided into multiple segments defined based on topography landform classification statistical and graphical results between scenarios a and d indicate a higher agreement than scenarios a and b same slope length but single slope steepness the highest differences were observed for cell 552 nse 140 and pbias 1000 the significant difference in soil loads estimates when compared to scenario a can be attributed to topographic characteristics of slope length of 10 m for scenario a and 292 m for scenario b and d scenario b has one segment of 292 m and slope steepness of 11 while scenario d has total slope length of 292 m but with multiple segments three of them with slope steepness less than 8 and one with 15 slope steepness the difference sediment loads in cell 552 from scenario d monthly average for may of 241 mg for clay and 1110 mg for silt when compared to scenario b monthly average for may of 205 mg for clay and 950 mg for silt can be attributed to enhanced representation of topographic features combined with rusle2 s capability of estimating sediment transport deposition between segments based on slope steepness analysis of results for cell 83 between scenarios a and c and a and e illustrates the influence of management in sediment load estimates pbias of 79 and 85 for c and e respectively graphical illustration for annagnps cell 83 depicting subdivision of slope profile based on management for scenario c is provided in fig 7 specifically for annagnps cell 83 under scenarios b and c the same slope length in both alternatives is considered but in scenario c multiple segments are simulated based on management table 5 in scenario b the entire slope profile is assigned management id 130 while in scenario c the second segment is assigned management id 153 which describes idle field management of either conversion to pasture or to conservation reserve program crp land this change in management yields an annual average total sediment load reduction from 250 mg yr b to 15 mg yr c evaluation of total monthly averages indicate change for both clay and silt particle sizes fig 11 the biomass in this management is composed primarily of grass like vegetation with small or no periods of exposed soil the above ground vegetation can reduce surface flow velocity and promote deposition yuan et al 2009 as simulated by rusle2 see scenarios c and e for annagnps cells 83 in fig 9 conversely for annagnps cell 312 there is a peak in sediment load estimation for the first two years blue line in fig 9 the second segment in the slope profile for scenario c is assigned to management 134 soybeans in first two years than idle after that the management combined with the high slope length and slope steepness contributed to high estimates of sediment load in the first two years however after the first two years estimated sediment load diminishes despite the high slope steepness as management becomes the dominant factor in controlling sediment erosion 3 2 watershed scale routing sediments originated in cells fields throughout the watershed generated information on the impact of local practices to other watershed systems located downstream of the cell being investigated and the overall watershed s sediment load output additionally it can aid in the selection process of areas to be targeted for mitigation a useful evaluation procedure is to sort cells from the highest to the lowest total average annual erosion per unit of area then calculate the accumulated drainage area and the accumulated total erosion mass associated with the sorted cells each cell s values can then be divided by the entire watershed accumulated drainage area and erosion values expressed as percentage of total drainage area and erosion fig 12 this graphical representation is helpful to identify critical sediment producing cells for example in fig 12a a vertical line denotes 20 of total drainage area x axis however for each scenario the same drainage area corresponds to a different contributing load percentage for scenario a black line it corresponds to approximately 60 of the total sediment load and for scenario b it corresponds to approximately 80 of total sediment load reaching the outlet additionally this analysis is based on sorting cells using their total annual average erosion per unit area values therefore varying how sediment sources are modeled may lead to differences in the cell s sediment load estimation and consequently in the selection of critical sediment producing cells fig 12b the five selected cells in this study vary significantly in this analysis reflecting their differences in erosion output and consequently their potential selection for mitigation similar analysis can be performed spatially in the form of map analysis of the highest cell average annual erosion per unit of area fig 13 likewise how the ls factor and farming management is represented affects the cell s sediment production and how each cell is compared to others cells although the six scenarios evaluated only differ in how five out of 438 annagnps cells were simulated these selected cells impact the overall watershed output as demonstrated by comparison with reference scenario a for fine sediments table 7 similar to results at the field scale watershed scale analysis considering contribution of all annagnps cells to the watershed outlet sheet and rill sources only scenarios a and f generated comparable average annual unit area estimates of fine sediment clay and silt at the outlet 0 77 mg ha and 1 93 mg ha for scenario a and 0 85 mg ha and 2 00 mg ha for scenario f however other scenarios produced over estimation of fine sediment loading for example scenarios b and d yielded pbias of 27 6 and 23 8 respectively when comparing total contribution of suspended sediment load to the watershed outlet with reference similar pattern was obtained for annual averages of clay and silt 1 41 mg ha and 2 73 mg ha b and 1 35 mg ha and 2 57 mg ha d these findings signify the importance of characterizing field scale activities and the improved representation of field slope profiles by technologies with capabilities for multi segment representation and detachment and deposition estimations 4 conclusions this study developed and evaluated a methodology for watershed scale simulation using a multi step approach based on enhanced description of erosional processes at field scales two models annagnps and rusle2 were evaluated through the identification of critical sediment producing cells using the annagnps model simulation of selected critical annagnps cells using the rusle2 model and routing of the sediment load from these annagnps cells through the watershed using the integrated annagnps rusle2 modeling methodology this approach utilizes annagnps as a screening tool to identify highly erosive areas that rusle2 would then be applied for advanced conservation practice planning evaluation and implementation significant efforts were devoted to input database integration although these models are both developed and supported by the u s department of agriculture and despite the fact they share conceptual similarities for sheet and rill erosion calculations both are rooted in the usle and rusle predecessors they differ on how information is stored computer file format and overall data organization and even in the terminology used to describe input parameters these dissimilarities increased the learning time needed for input databases integration the development of automated semi automated methods to facilitate exchange of information between the two technologies will support future multi scale watershed field integrated application of these models observed edge of the field measurements were not available and therefore a relative comparison between alternative scenarios were performed in which the standard annagnps rulse1 alternative scenario a was selected as the reference since in this configuration the annagnps model was validated using watershed scale observed values watershed outlet results at field and watershed scale comparing single segment slope profiles described by the same input parameters but estimated using different technologies annagnps rusle1 scenario a and rusle2 scenario f yielded comparable results at yearly and monthly time scales these findings signify the similarities between the two technologies when a single segment is simulated and the appropriate synchronization of input databases within each model the option of defining slope profiles composed of multi segments exploits rusle2 s capabilities of characterization of individual segment based on topography slope length and slope steepness soil and management and availability of algorithms for sediment transport capacity and or deposition balance the challenge is in the definition of the representative slope profile slope profiles based on gis analysis can be discretized and characterized in different ways the same slope profile can be partitioned based on topography soil land use and other field parameters in here gis analysis was used to define the longest flow path within sub catchments annagnps cells flow paths were subdivided based on topography management and a combination of both results indicate that the multi segment approach is more sensitive to farming management land use than topography as demonstrated by cell 83 under scenarios b and c in which in scenario c one of the grassed managements performed as a riparian buffer strip reducing sediment loads reaching the bottom of the slope percent change of 177 on annual average conversely enhanced characterization at scales smaller than basic modeling units have the potential to describe field based conservation practices such as edge of field buffer strips grassed waterways and contouring each unique field scale conservation practice could be represented by multi segmented slope profiles the challenge resides in the appropriate definition of the representative profile for each cell slope steepness and slope length as observed by the results ls factors definition significantly affects erosion estimate results including estimates of clay and silt sized particles differently the definition of representative slope profiles for selected cells should be the subject of future studies likewise future enhancements to the annagnps model should include continued development of modules that describe physical parameters at scales smaller than basic modeling units reaches and cells examples of existing modules with such characteristics include the wetland component agwet momm et al 2016 at scales smaller than reaches the ephemeral gully component peg momm et al 2012 at scales smaller than cells and the riparian buffer component agbuf momm et al 2014 at scales smaller than reaches and cells the rusle2 model has been recognized as an important tool for agriculture conservation and farming practices design and evaluation not only in the united states but also around the world this is due to its reliability in estimating soil erosion at the field scale simplicity and accompanying ready to use input database kinnell 2010 integrating rusle2 with annagnps has the potential for improved characterization of field parameters controlling sediment erosion at scales smaller than field while at the same time routing field estimates to downstream waterbodies through channel physical processes however future enhancements should be focused on distributed and or computationally efficiency rusle2 implementations that allow for easier integration with other models and or web based deployments this could facilitate estimation of sheet and rill erosion at scales smaller than basic modeling units used by watershed models the proposed framework can serve as template for integrating hydrological watershed scale or even basin scale models with erosion field scale models in addition to allowing for enhanced characterization of conservation practices it offers a platform for routing edge of field observed values through the watershed as computing science and technology improves there is opportunity to re visit how watershed physical processes are described and modeled and how modeling technology is accessed by end users these improved modeling efforts support the evaluation of existing conditions and assessment of proposed conservation practices software availability both models annagnps and rusle2 are distributed by the u s department of agriculture natural resources conservation service nrcs agency annagnps https www nrcs usda gov wps portal nrcs detailfull null cid stelprdb1042468 and rusle2 https www nrcs usda gov wps portal nrcs detail national technical tools rusle2 cid stelprdb1247278 for further inquiries on these software packages please contact ronald bingner ron bingner ars usda gov leading scientist and project manager for the annagnps model and robert wells robert wells ars usda gov leading scientist and project manager for the rusle2 model acknowledgements the authors would like to acknowledge the support provided by glenn herring in performing modifications to the annagnps source code and to evan snap for programming the annagnps rusle2 computer interface this work was supported by funds provided by the u s department of agriculture agricultural research service 
26217,multiobjective evolutionary algorithms moeas with colorado water managers rebecca smith a joseph kasprzyk b lisa dilling c d a bureau of reclamation 1777 exposition dr ste 113 boulder co 80301 usa bureau of reclamation 1777 exposition dr ste 113 boulder co 80301 usa b department of civil environmental and architectural engineering university of colorado 607 ucb boulder co 80309 usa department of civil environmental and architectural engineering university of colorado 607 ucb boulder co 80309 usa c environmental studies program university of colorado 397 ucb boulder co 80309 usa environmental studies program university of colorado 397 ucb boulder co 80309 usa d western water assessment university of colorado cires 216 ucb boulder co 80309 usa western water assessment university of colorado cires 216 ucb boulder co 80309 usa corresponding author multiobjective evolutionary algorithms moeas generate quantitative information about performance relationships between a system s potentially conflicting objectives termed tradeoffs research applications have suggested that evaluating tradeoffs can enhance long term water utility planning but no studies have formally engaged with practitioners to assess their perceptions of tradeoffs generated by moeas this article examines how practitioners interact with moea tradeoffs and reports their ideas for how their agencies could use moea results we hosted a group of colorado water managers at a charrette or structured investigatory workshop where they directly interacted with tradeoffs discussed how they used the information and linked their workshop experiences to opportunities for moeas to enhance their agencies planning processes among other interesting results we found that managers portfolio preferences diverged as tradeoff information increased and that structured information about the relationships between decision levers and performance would be beneficial for interpreting tradeoffs keywords participatory modeling workshop multiobjective evolutionary algorithm moea decision making long term planning tradeoffs 1 introduction decision making is a process when a choice is available to be made deliberation must occur if an agent desires an outcome and is able to take action aristotle 1920 in most decision making processes preferences are constructed based on problem framing previous experience and available information time and resources payne et al 1992 roy 1999 slovic 1995 tsoukias 2008 in combination these factors help decision makers develop what montgomery 1983 terms a dominance structure a dominance structure is a set of cognitive rules that serve to create advantages for certain alternatives or neutralize disadvantages of others such a framework is necessary when there is no strictly optimal option the dominance structure is iteratively built up in stages using mechanisms that help decision makers assess relative merits of alternatives and or alter their internal representations of situations until one alternative becomes dominant this process of creating arguments for and against alternatives develops a justification or basis for reasoning that can be conveyed to others justifiability is a cornerstone of deliberate human decision making connolly and reb 2012 payne et al 1992 slovic 1975 tversky 1972 and studying this in technology based decision support is warranted multiobjective evolutionary algorithms moeas have been researched and applied as tools to aid decision making processes concerning complex systems for which there are multiple conflicting performance measures moeas seek to optimize system performance in multiple performance objectives efficiently searching through thousands of alternatives to develop a set that quantitatively characterizes the approximate best tradeoffs between those objectives these quantified tradeoffs reveal how much performance in one objective must be forfeited to get better performance in another in the context of developing a long term water resources plan moeas test thousands of alternative portfolios of new sources new infrastructure and new operations in order to balance between performance objectives such as maximizing supply reliability and minimizing environmental impact several studies have applied moeas to long term water resources planning problems long term plans are essentially overarching decisions about pursuing a set of actions over an extended time horizon three recent academic examples are matrosov et al s use of an moea to develop long term planning portfolios for london balancing cost energy use resilience and environmental objectives 2015 zeff et al 2016 optimization of long and short term risk triggers to develop adaptation strategies and support regional cooperation between utilities in north carolina and wu et al s application of multiobjective optimization to identify portfolios of traditional and alternative water sources for adelaide in consideration of cost emissions reliability and the environmental impacts of water and wastewater reuse 2017 these studies demonstrate that moeas can produce informative tradeoffs for multiple aspects of planning in a variety of geographic contexts which could inform agencies planning decisions however none of these examples have undertaken a structured exploration of how a practitioner or agency employing an moea would interact with or perceive tradeoffs and thus have not determined whether or how they actually aid decision making to study whether the quantitative information found in moea tradeoffs contributes to the creation of defensible dominance structures that help water managers construct preferences and justify decisions researchers need to be able to observe interrogate and analyze practitioners usage of tradeoffs accomplishing this necessitates an interface between practitioners and researchers designed specifically around the type of information that results from moea assisted optimization here we can draw on an approach called a charrette which is used in non academic settings to achieve a high level of public awareness and input on the design or vision of a community project or plan us epa 2014 charrettes are also used by researchers in the fields of construction management and safety research charrettes are structured workshops that bring together industry professionals and academics in a relatively short but intensely productive session in order to generate discussion and feedback about newly created products or practices intended for industry use gibson and whittington 2010 charrettes combine the advantages of surveys interviews and focus groups in an accelerated time frame overcoming the difficulties of undertaking these methods individually e g low response rates time commitments from both researchers and practitioners access to data etc results from applying these mixed methods to technical research topics have shown that charrettes can offer both short and long term benefits to participating industry professionals and improved validity and reliability of research outcomes abowitz and toole 2010 green et al 2010 this paper presents the content methods and results of a research charrette through which our transdisciplinary research team engaged with front range colorado water managers over the use of moea tradeoff information for long term water utility planning the workshop was designed to discover how practitioners used tradeoff information to make decisions and whether and how the managers perceived the information to be useful in their agencies planning processes the goals of the workshop were to expose practitioners to an emerging tool and use the collected data to hone future moea research agendas and target new applications the charrette that we focus on in this paper is the culmination of a larger study that introduced and applied the participatory framework for assessment and improvement of tools parfait smith et al 2017 the following section briefly introduces moeas and presents work from the previous phases of our parfait efforts that pertain to this final step in the framework in section 3 we describe the methods and content from our workshop next we describe the results and in section 5 offer concluding remarks 2 background 2 1 moea assisted optimization for long term water utility planning for water utilities planning for long term sustainable water security is a critical task and a major undertaking technical staff review alternative planning portfolios and iteratively discuss goals needs and strategies with board or council level decision makers csu 2017a mwd 2015 and increasingly the public as well wuca 2015 they generally do not find a perfect plan due to the conflicts between the financial social and environmental factors that utilities must navigate elkington 2004 but utilities strive to make smart responsible and justifiable decisions that allow their systems to meet the communities chosen demand reliability policies in combination with community values multiobjective evolutionary algorithm moea assisted optimization has been studied matrosov et al 2015 mortazavi et al 2012 smith et al 2018 wu et al 2016 and applied basdekas 2014 csu 2017a as a method to help utilities develop long term plans while a traditional planning process compares the performance of a handful of planning portfolios moea assisted optimization efficiently designs and tests thousands of potential portfolios this extensive search and evaluation produces quantitative information about the system s performance in multiple objectives and the tradeoff relationships between those objectives performing moea assisted optimization requires a simulation model already developed by most utilities a problem formulation an moea and tradeoff visualizations the problem formulation is a set of decision levers objectives and constraints that the moea uses to construct and compare planning portfolios decision levers are a utility s options to modify its system e g building a reservoir or enacting conservation the set of chosen decision levers makes up a portfolio objectives are measures of system performance that are quantified representations of a system s goals or purposes e g minimizing frequency of lawn watering restrictions or maximizing water in storage constraints are numeric limits to acceptable performance e g if a portfolio cannot meet 100 of indoor demand at all times it is not considered a valid planning approach moea assisted optimization is carried out through many cycles of a computational loop the moea generates an initial population of portfolios and feeds each one to the simulation model which tests the portfolio over one or more future scenarios at the end of the simulation values for objectives and constraints are reported back to the moea this loop iterates thousands of times during which the moea intelligently evolves new generations of portfolios through both systematic and random recombination and mutation of the high performing portfolios of previous generations this results in a set of nondominated portfolios in which performance improvement in one objective is only achieved by sacrificing performance in another thus the portfolios trade off levels of performance analyzing the tradeoffs requires careful analysis including visualization techniques and these are the final component of moea assisted optimization more information about tradeoff visualization is presented in section 3 1 water utility planning is a complex process which may benefit from new technologies increased public scrutiny greater mandates to protect social and environmental interests and heightened awareness of future uncertainty all suggest that extensive portfolio search and explicit performance tradeoff information would be useful to the agencies 2 2 participatory framework for assessment and improvement of tools parfait many research applications of moea assisted optimization have established the ability of moeas to generate tradeoff information about water supply systems and produce innovative portfolios that can outperform plans developed with human expertise or previously established operational approaches maier et al 2014 nicklow et al 2010 while colorado springs utilities and melbourne water are two notable examples csu 2017b kularathna et al 2015 instances of this promising tool being applied in real world planning studies are rare to understand and potentially overcome the limited uptake of moea assisted optimization researchers must consider the factors that lead industries to adopt tools and consciously seek to create useable science that is researchers must undertake intentional iterative interaction with practitioners to understand their needs transmit research and co produce relevant future research directions díez and mcintosh 2009 dilling and lemos 2011 sarewitz and pielke 2007 smits 2002 the participatory framework for assessment and improvement of tools parfait is a research process designed to bring academics and practitioners together in a structured way smith et al 2017 parfait is a four phase research sequence that can be summarized as follows step 1 choose a promising research tool and a practical use for it that is supported by academic literature and knowledge of the proposed industry step 2 hold workshop 1 to solicit input from practitioners that will inform development of a tool testbed a testbed is a platform on which the tool can be demonstrated to practitioners step 3 build the tool testbed iterating with practitioners as necessary to ensure relatability and relevance to real world tool application context step 4 hold workshop 2 a research charrette to solicit practitioner feedback on the testbed results i e results representative of what they could expect if their agencies adopted the tool smith et al 2017 introduced parfait including the detailed steps and methodology the supporting theory behind the process and the results of workshop 1 briefly summarized below the parfait purpose and process distinguish the study presented in this paper from previous moea research studies which either applied the tool to a stylized system without input from practitioners or worked with water managers to inform its application to a real system this study instead seeks to create a context and platform through which practitioners from many agencies can gain experience using moea tradeoffs and provide their feedback to researchers here we present results from our application of parfait including a charrette with utility managers to understand their engagement with tradeoffs and the overall usability of moeas for long term decision making 2 2 1 parfait workshop 1 workshop 1 of our parfait process took place in february 2015 it brought together water managers from six front range colorado utilities 1 1 city of aurora city of boulder colorado springs utilities denver water city of fort collins and northern water and our research team which was made up of engineering social science and climate science researchers as well as water utility practitioners through targeted but free form group discussions managers shared their experiences of front range management challenges and provided feedback and suggestions to inform the elements needed to create an moea assisted optimization testbed supply and demand decision levers performance objectives and constraints future supply and demand scenarios and important features for a generic but relevant hypothetical water supply simulation model creating a relatable testbed is crucial for the successful application of parfait because it is the basis for generating representative results and also because its components must be recognizable to participants in the second parfait workshop this enables them to quickly grasp the testbed and focus on engaging with the results based on the information we generated through workshop 1 and iteration with practitioners on our research team we developed the problem formulation decision levers objectives and constraints and water supply simulation model that make up the eldorado utility planning model testbed 2 2 2 parfait testbed the eldorado utility planning model and case study to demonstrate moea assisted optimization we used the context of a hypothetical water utility called eldorado utility undertaking a long term planning process the eldorado utility planning model and case study generically capture management context relevant to utilities on the front range of colorado as well as other regions in the western u s the rest of this section will briefly describe eldorado utility s supply system and problem formulation the model and minimal pertinent front range context technical details about the optimization problem are included in the appendix for more front range context refer to smith et al 2017 and for in depth discussion about the model and case study results refer to smith et al 2018 much of the western u s is severely water limited and tightly regulated by the prior appropriation legal doctrine or first in time first in right hobbs 2004 one practical outcome of these factors is that as cities grow they obtain a variety of types of water rights e g storage rights and streamflow diversion rights each with different temporal priorities and which may be sourced from multiple geographic locations to represent this eldorado s hypothetical system includes two reservoirs on two different rivers with junior priority dates three direct diversion streamflow rights on a nearby river one senior one mid seniority and one junior one junior diversion right on a distant river that requires the diverted water to be conveyed under a mountain range in order to be stored closer to the utility and 10 000 shares of a water wholesale company that eldorado takes directly from a reservoir owned and operated by the wholesaler in many years junior right holders do not all get their full allotments caulfield jr et al 1987 p o abbott 1985 e g a reservoir does not necessarily fill or a streamflow right does not always get to divert streamflow and competition for water on different rivers varies however and this means that utilities water supplies strategically span entire regions the eldorado utility planning model encompasses 5 basins and 12 water users besides eldorado the other users with senior water rights often limit the yields from most of eldorado s sources but some also provide opportunities for the utility to acquire more reliable supplies the eldorado utility is a relatively small water provider and like much of the western u s is expecting rapid population growth state of colorado 2017 the utility has a set of 13 decision levers it can use to modify its system to meet growing demands the levers fall into three general categories pursuing new water building new storage and altering management of reusable water the third category includes decisions about leasing strategic space in other agencies reservoirs and obtaining the right to move reusable water around the region for more efficient access pursuing new water refers to decisions to acquire rights from regional agricultural or industrial users or buying shares from water wholesalers conservation is also considered new water because it frees up water that would have otherwise not been available to meet growing demands building new storage includes decisions about whether and how much to expand an existing reservoir and whether to build a new one either upstream downstream or both eldorado has defined five 2 2 the full optimization problem had two additional restrictions based objectives that were not presented in the workshop but which are described in the appendix performance objectives on which to evaluate potential portfolios they are qualitatively described here and also summarized in table 1 the first objective years in restriction 1 seeks to minimize how frequently eldorado goes into level 1 restrictions which occurs when the utility s storage drops below 75 of average annual demand 3 3 when storage drops below 50 of annual demand more severe restrictions are triggered but those were not presented in the workshop see the previous footnote eldorado s reliability policy dictates that the utility should not enact these restrictions more than 5 times in 25 years the next objective captures the utility s desire to minimize missed opportunities i e inability to use available water missed op water this measures how much of certain types of water that eldorado had access to but could not use due to incompatible demand timing lack of storage etc next eldorado seeks to minimize new supply this means minimizing the average annual volume of water over the course of the simulation that eldorado acquires through decisions such as buying rights or shares or conserving water i e freeing up water to meet new demands while eldorado does need more water for a growing population this objective is minimized because drawing more water than necessary away from other users creates social and economic disruption in their communities the new storage objective minimizes the volume of newly built storage within a portfolio because adding infrastructure is expensive uncertain and environmentally problematic finally measuring april 1 storage to demand is another way for eldorado to evaluate the reliability of their system this objective seeks to maximize the lowest april 1st storage volume over the course of the simulation i e it measures how much water is left in storage at the end of the winter drawdown season the single constraint included in the problem formulation was that all portfolios had to meet 100 of indoor demand demand remaining after outdoor water use is prohibited by level 3 restrictions the eldorado utility and regional system are modeled using the riverware platform zagona et al 2001 the optimization was performed on a 25 year simulation horizon with a monthly timestep using the borg moea hadka and reed 2013 optimizations were performed in three different hydrologic scenarios historic streamflow resulting from a 1 c perturbed future qualitatively named very warm for workshop purposes and streamflow resulting from a 4 c perturbed future referred to as very hot in the workshop these temperatures were based on a previous climate change study in which all of our front range utilities participated more information about the choice of these scenarios can be found in woodbury et al 2012 and a description of their generation is in smith et al 2018 3 methods 3 1 interactive tradeoff visualization workbooks to explore and understand the quantitative tradeoffs contained within a set of nondominated portfolios produced by moea assisted optimization users need to be able to see the complex relationships between the portfolios this is facilitated by visualizing multiple portfolios at a time in several objectives or dimensions being able to see relationships across all dimensions simultaneously provides the greatest opportunity to see tradeoffs since only seeing a subset of the objectives can obscure higher dimensional relationships kollat and reed 2007 understanding and exploring a large dataset in many dimensions requires advanced visualization techniques called visual analytics keim et al 2006 liu et al 2017 thomas and cook 2006 woodruff et al 2013 this study uses parallel axis plotting the plots use a series of vertical axes to represent as many dimensions as desired fleming et al 2005 herman et al 2014 inselberg 1985 watson and kasprzyk 2017 studies have shown that if parallel plots are interactive first time users can learn to use them effectively with 5 10 min of training johansson and forsell 2016 siirtola and räihä 2006 previous research has assessed whether users can evaluate multiple dimensions to complete a closed form task with the plots e g which one of the cars manufactured in 1982 has the slowest acceleration siirtola and räihä 2006 our workshop differs in that we asked participants to use the information from the plots to make their own choices so our results will reflect how practitioners used parallel plots to weigh tradeoffs and make judgements to enable the water managers to use parallel plots for subjective analyses we created plots that supported extensive browsing multiple selections and comparisons between portfolios and across workshop activities we used tableau a commercially available business analytics program jones 2014 to create a series of interactive worksheets on which participants could hover over portfolios to get full decision and performance information select one or more portfolios to highlight them and enter portfolio ids that changed the colors of those portfolios to register their choices for the activities described below critically the workbooks allowed us to save their choices which both recorded them for later research analysis as well as allowed us to show managers how their choices changed or did not change over the course of the workshop example results from optimizing the eldorado utility case study are presented in fig 1 briefly discussing the example results will facilitate readers understanding of the information that water managers used during the charrette described in the next section as demonstrated below we showed charrette participants the objectives and decisions together to provide all information about the portfolios and enable them to evaluate tradeoffs between different objectives while simultaneously exploring decision preferences the plots in fig 1 show 20 portfolios 4 4 the full tradeoff sets produced by the eldorado utility optimizations included approximately 1000 portfolios each smith et al 2018 in order to make the most of limited workshop activity time we only showed participants 20 predetermined alternatives that were hand selected by researchers such that the subset captured a wide range of performance for each objective and clearly presented the system s performance tradeoffs that resulted from optimizing the eldorado utility case study using hydrology generated for a 4 c warmer or very hot future the top plot has five vertical axes one for each performance objective each of the lines connecting the axes is a portfolio the vertical position at which a portfolio line crosses an objective axis denotes its performance where lower intersection is better note that the objectives and decision levers all have different numerical scales and we have normalized the values so that each dimension fully spans its axis the portfolios are colored based on how many years of level 1 restrictions they produced i e the performance on the rightmost axis blue lines all have five years in restriction red lines all have nine years two portfolios are highlighted to demonstrate the tradeoffs presented in the plot the blue portfolio has the best possible performance in april 1 storage to demand and years in restriction 1 has medium poor performance in new storage and missed op water and the worst possible performance in new supply these levels indicate the tradeoffs between reliability measures on the right two axes and other system performance considerations conversely the red portfolio performs the worst in april 1 storage to demand and years in restriction 1 but better sometimes much better than the blue portfolio in the other three objectives depending on eldorado s preferences and priorities they might choose portfolios with different performance characteristics the bottom plot shows decision lever attributes using a vertical axis for each of the 13 levers as in the objectives plot the lines connecting across axes are portfolios and the position at which they intersect an axis denotes how much of a decision is included in the portfolio the lower a portfolio line crosses the less of that lever is present each portfolio line in the objectives plot has a corresponding line in the decision levers plot so we can compare a few of the decisions led to the contrasting performance of the two highlighted alternatives described above 3 2 moea research charrette june 2016 step four of our application of parfait a research charrette provided water managers with hands on experience with moea assisted optimization results our goals for the workshop were to 1 provide exposure for the emerging tool 2 observe managers analyses of tradeoff information 3 understand how managers relate the tradeoff information to their current needs and practices 4 get feedback about what potential uses and barriers managers see in the tool 5 learn about the general process of utilities adopting a new tool and 6 report any opportunities for future research to meet the needs of practice nine total participants from six front range utilities attended the workshop the utilities represented a wide range of system sizes and the individuals themselves also spanned a range of experience levels 4 managers had over 16 years of experience in front range water management 1 had between 11 and 15 years 1 had between 6 and 10 years and 3 had 0 5 years of experience we also had participants with different roles within their respective agencies four were at a management level and five were technical staff this variety was helpful in getting different perspectives and the presence of both technical and managerial practitioners was especially encouraging since having advocates at multiple levels of administration increases the likelihood of innovation uptake daniell et al 2014 3 2 1 charrette development intense preparation and attention to charrette form function and sequencing made it possible for both participants and our team to approach the actual experience as a fun day of learning once we produced tradeoffs for the eldorado system in multiple future scenarios completed approximately three months prior to the charrette we began the process of developing content and activities that could effectively introduce new concepts and provide an engaging hands on experience to accomplish this we undertook trials of content with unaffiliated water professionals and water researchers to learn about how moea novices reacted to various levels of information and visualization complexity these dry runs helped shape the presentation of moea and testbed information choices of activities timing to complete them and design of the tableau workbooks the nine practitioner attendees were divided into three groups of three and seated at different tables each table had a facilitator and the facilitators were chosen based on their familiarity with workshops and water management so that they could prompt and guide discussions in a neutral and knowledgeable way members of the core research team floated between groups to clarify technical or procedural questions each manager was given a laptop with the tableau workbooks pre loaded so that they could complete charrette activities independently they then reflected on their individual efforts in small group discussions which are described in the results section data from this workshop includes the portfolio choices that managers made as well as discussions about the moea testbed tradeoffs managers analytical processes utilities planning approaches tool adoption potential for moeas overall and workshop content as such we made sure to capture participants portfolio choices but also took audio recordings and notes of each small group of managers having three types of information allowed us to ensure accuracy and produce results that synthesized both qualitative and quantitative responses additionally post workshop surveys recorded participants overall perceptions of the usefulness of moea assisted optimization the charrette used a detailed format custom computer workbooks and concrete tasks associated with the activities all of which guided information flows between participants and researchers compared to our first parfait workshop which relied on free form discussions about targeted topics to inform the direction of the overall project this workshop was a relatively formal participation mechanism newig et al 2008 smith et al 2017 however the facilitated small group discussion sessions built into each activity captured open discussion and impressions from participants and allowed us to access subtleties of how utilities plan and operate and how managers relate to their systems after the workshop we electronically surveyed participants about their perceptions of moea usefulness this mixture of methods is fundamental to the success of charrettes gibson and whittington 2010 the incorporation of focus group type activities and discussions was particularly useful for bridging the gap between researchers and practitioners because these interactions provide a clear view of how others think and talk morgan 1993 3 2 2 training and support materials in order for participants to fully engage in the workshop and provide researchers with thoughtful relevant feedback about using the tool they needed to be able to 1 understand why moea assisted optimization has been proposed as a useful tool for water planning 2 understand the concept of performance tradeoff sets 3 have sufficient understanding and acceptance of the hypothetical utility its supply and demand context and its policies to be able to focus on tradeoffs 4 understand and relate to the problem formulation and planning scenarios and 5 effectively operate the tableau workbooks and interact with parallel plots we covered these topics in a 90 min introductory presentation after explaining and taking questions about moeas and the testbed content similar to that found in the background section of this article we held an interactive parallel plot training session in order to introduce parallel plots and tradeoff analysis we created a simple multiobjective grocery shopping problem each participant used a tableau worksheet set up identically to those that they would see in later activities that showed plots of performance and decision levers we defined three conflicting objectives minimize cost maximize nutrition and maximize pleasure through which to optimize a set of eleven potential shopping items such as apples ice cream eggs etc as a group we went through incremental closed form exercises finding the least expensive shopping list the most nutritious list etc the exercises required participants to analyze both the decision and objective plots and learn their interactive functions the total training time was approximately 10 min questions were encouraged throughout and no participants expressed any prolonged difficulty in interpreting the worksheets to support the managers in the day s activities we gave them printed packets that included a diagram of the eldorado utility planning model current and future utility demands utility policies descriptions of the decision levers and objectives and descriptions of the different hydrologic scenarios the diagram reproduced in fig 2 conveys the spatial and temporal complexity of the system using icons colors dates and arrows 3 2 3 tradeoff activities the core of the charrette was organized into four main activities to test behavior with the tool in four different situations of increasing complexity during each activity managers were given 10 15 min to independently explore tradeoffs presented in tableau workbooks and apply their own logic or dominance structure to choose two portfolios the activities are summarized in table 2 the purpose of activity 1 was to establish initial preferences and create a basis for managers to compare decision making with and without tradeoff information the participants chose one of three portfolios developed heuristically by an expert consultant researcher familiar with the model and case study each portfolio was characterized by its constituent decisions and its firm yield in historical hydrology but no performance tradeoff information was offered the chosen portfolios from this activity were brought back in activity 4 the activity 2 sequence was designed to ease the managers into evaluating tradeoffs in complex plots to create space for analyzing tradeoffs without the dominant influence of reliability smith et al 2017 to have managers be able to explicitly compare their use of different amounts of information and to do all of this without considering the likelihood or implications of climate change on front range supplies in activity 2 exercise 1 participants were shown performance of 20 algorithm optimized portfolios in a two objective tradeoff along with a plot of all of the portfolios decisions and asked to select two portfolios of interest the portfolios resulted from optimizing for historical hydrology and were constrained to meet eldorado s restrictions based reliability policy this was made clear to managers so they knew they did not have to worry about reliability in this first activity in activity 2 exercise 2 managers were shown the same set of 20 portfolios as in exercise 1 but now were given performance information in a four objective tradeoff plot along with the decisions plot in activity 2 exercise 3 participants were shown the choices they made from exercises 1 and 2 in one plot to compare the preferences they expressed with different amounts of tradeoff information activity 3 introduced the frequency of level 1 restrictions objective and perturbed hydrology the exercises allowed researchers to probe how the presence of the level 1 restrictions objective influenced participants perceptions of other tradeoffs and added hydrologic challenges to their decision calculations in exercise 1 participants were shown 20 algorithm optimized portfolios that resulted from optimizing in a 1 c warmer very warm future to understand the implications of the different hydrology managers referred to the informational packets where plots showing a slightly lower magnitude of peak runoff slightly earlier peak timing and similar regional flow variability they were again asked to choose two portfolios and had to directly trade off reliability policy performance with the other four objectives from activity 2 exercise 2 was identical to exercise 1 except that the 20 portfolios were from a set produced by optimizing for a 4 c warmer very hot future this scenario had a much lower peak runoff magnitude much earlier peak timing and lower variability due to lower magnitude high flow years activity 4 was designed to emphasize to participants that portfolios developed for or optimized under specific futures may not be acceptable if the future is different than they planned for managers saw the exact same set of portfolios from activity 3 exercise 2 but now their performance in a set of varied hydrologic traces was shown i e in a supply scenario that they were not optimized for the set of 10 traces were drawn from all other scenarios so performance reflected the portfolios average performances in a wide set of futures they were again asked to make two choices from this set and while making the choices they could see how each portfolio performed in varied as well as 4 c hydrology so they had two parallel plots of objectives and one plot of decision levers managers were also shown how their hand crafted solution from activity 1 performed in both scenarios and asked to reflect on how they felt about those portfolios which were developed using historical hydrology at the workshop the managers played the roles of engineers at the hypothetical eldorado utility who were evaluating a new tool for its potential to enhance their upcoming long term planning process asking them to play a fictional role and use hypothetical but realistic tradeoff results helped participants to engage more candidly by distancing them from physical social and political pressures of their own systems similarly for each activity we asked the managers to choose two portfolios to subject to further analysis to avoid comparisons with the real world complex process that a utility undertakes to actually decide on one plan it was important however to ask them to make individual choices this forced them to really grapple with tradeoffs and to use some logical process and thus created a more defined experience for them to discuss with researchers and each other for each exercise except for activity 2 exercise 3 during which participants just compared two sets of portfolio choices the group facilitators asked three main questions to prompt discussion a what objective performances or tradeoffs made the two portfolios you chose interesting to you b what decision lever attributes made the solutions interesting to you c based on the objectives performance as a manager at your utility do you think you would have chosen the same solutions to investigate further why or why not questions a and b were designed to separate the ways that performance and decision levers impacted choices and question c was designed to emphasize that we wanted the managers to choose freely but also provide as much real world decision making context as they could 4 results throughout the day managers engaged with tradeoffs facilitators and each other they took the purposes of the workshop seriously and combined openness to the activities with reflections about their own agencies planning contexts as we prompted them with specific concepts they each interpreted and applied them differently a result of this was that across nine managers the portfolio selections often varied widely and sometimes the processes they used to make them also varied significantly rather than report each individual s choices and processes below is a description of common themes and examples of how logic changed over the course of the day 4 1 managers use of tradeoffs within this section about how managers used tradeoff information there are four subsections the first discusses findings from activity 2 which presented managers with first two then four objectives to analyze brief synopses of two managers decision processes and how they relate to dominance structuring are included the second subsection of results is based on activity 3 which introduced a fifth objective level 1 restrictions and two new more hydrologically challenging scenarios subsections three and four present findings that emerged throughout all four activities 4 1 1 tradeoffs in two objectives vs in four objectives in activity 2 exercise 1 where participants saw tradeoffs in two objectives three general strategies emerged for choosing portfolios of interest five managers weighted performance in the objectives equally two performed cost benefit analyses between the two objectives and two managers prioritized performance in one objective over the other fig 3 shows the results of manager b4 s cost benefit analysis the manager started by picking the portfolio with the least storage then worked incrementally up the new storage axis to find out how much better the performance in missed opportunity could get the manager ultimately tried to find the portfolios where the tradeoff was reasonable where the sacrifice in one objective came with a worthwhile gain in the other this process is an example of the creation of a dominance structure the manager initially screened alternatives based on the lexicographic decision rule where there is a most important attribute and then iteratively applied the addition of utilities rule to select portfolios montgomery 1983 in activity 2 exercise 2 managers were asked to make two selections from the same set of portfolios that they saw in exercise 1 but they did so with performance tradeoffs in four objectives instead of just two so it was possible to choose the same portfolios in both exercises indeed two managers chose the exact same portfolios as they did in exercise 1 three managers chose one matching portfolio and the other four participants chose two new solutions when presented with more tradeoff information in activity 2 exercise 3 where managers were shown the two sets of choices they made the managers who had identical sets of choices said that they used the same criteria in the second exercise as they did first this is a good reminder that new tools and new information do not necessarily result in changed preferences or different choices moea tradeoffs may also reinforce existing cognitive heuristics or increase confidence in decisions depending on the nature of the tradeoffs and how managers use a priori judgements to create dominance structures for the seven managers who chose at least one new portfolio in exercise 2 they tended to balance their two choices against each other for example if they chose one portfolio that was middle of the road across the objectives they allowed themselves to choose a second portfolio that prioritized one objective regardless of whether it performed poorly in another fig 4 shows manager b2 s two selections for the first choice blue the manager balanced across the objectives i e he collapsed performance attributes using the addition of utilities rule the second choice maroon resulted from screening lexicographically on new storage performance and then deemphasizing missed op to support the dominance of the chosen portfolio when discussing the process used to make choices with two tradeoffs versus four this manager said more objectives is better in terms of understanding the system and its performance i assume that at some point it gets too noisy but i definitely see value in going from two to four even if i end up prioritizing one or two objectives it helps to see the implications that has on the others 4 1 2 use of the level 1 restrictions objective the tradeoff analyses from activity 2 included only portfolios that complied with eldorado s restrictions based reliability criteria defined as not exceeding 5 years in level 1 restrictions over the 25 year simulation this condition was explicitly conveyed to participants and they were not presented with an objective that measured performance with respect to restrictions by omitting an objective about restriction performance the participants were able to consider their performance and decision preferences without directly grappling with level of service or policy consequences once frequency of restrictions was introduced as the fifth objective in activity 3 all participants used it as their initial screening criterion additionally both exercises in this activity used a climate change perturbed hydrology the portfolios in exercise 1 resulted from a 1 c warmer scenario and portfolios in exercise 2 resulted from a 4 c warmer scenario though they had many options that resulted in only two years of restrictions five of nine managers considered portfolios that exhibited from three to five years in level 1 restriction in activity 3 exercise 1 however only two managers ended up choosing portfolios with three or more years in restriction while none of the other seven managers found that the performance gains in other objectives warranted the extra years seven of nine participants expressed satisfaction with the performance and decision lever balances they were able to strike and two expressed concerns that their decision preferences seemed less effective in the warmer scenario while all managers chose to perform better than eldorado s reliability policy criteria in activity 2 exercise 1 it was difficult to meet the criteria in exercise 2 because the portfolios were evaluated in a hydrologically challenging 4 c warmer future there were no portfolios that had fewer than five years in level 1 restrictions the maximum allowed by eldorado s policy all portfolios that met the criteria required great sacrifices in at least two other objectives for the three participants who stayed within the criteria two focused on performance in one other objective and one tried to balance three other objectives within the compliant portfolios of the six other managers four determined that one or two extra years in restrictions was worth the gains in other objectives noting that this thinking would trigger policy discussions with their decision making boards this tool would be really useful in demonstrating just how much service we would have to give up in order to avoid unpopular storage or supply decisions two managers felt that once the climate had warmed by 4 c norms would have changed lawns would have disappeared and people wouldn t expect the same levels of service that they had seen in the past so they chose portfolios with nine years in restrictions and were able to avoid big storage projects so given difficult tradeoffs three managers made painful concessions to comply with restrictions policy four bargained on relatively small policy deviations thinking that the trade could become the focus of broader negotiations and two managers reframed the problem in order to justify alternative s that they considered superior 4 1 3 more information divergent choices in analyzing all managers specific portfolio choices over the course of the day we found that as more information was added and tradeoff experience increased the group s choices started to diverge three participants made identical choices to one another in activity 2 exercise 1 the same was true in activity 2 exercise 2 the sets of participants and the choices were different though and there were no correlations with experience level of the participants or size of utility overall seven different portfolios were chosen in exercise 1 and five different portfolios were chosen in exercise 2 out of 18 total choices made per exercise 2 choices for each of 9 participants there were no sets of identical choices in activities 3 and 4 in activity 3 exercises 1 and 2 10 and 12 different portfolios were chosen respectively out of 18 total choices that were made for each exercise finally 12 different portfolios were chosen in activity 4 when participants were prompted to select portfolios while evaluating their performance in two different futures simultaneously adding the level 1 restrictions objective and using more challenging hydrology resulted in more divergent choices than those with historical hydrology and only two or four objectives in play the finding that more information can lead to a wider variety of choices is perhaps not surprising because there are more avenues for creating dominance structure however it is worth considering how increasing information and greater divergence would affect a real world planning process that involves several levels of scrutiny by many employees decision makers and the public while this workshop focused on how individuals perceived and worked with high dimensional tradeoffs more formal preference elicitation and application via multicriteria decision making mcdm could be helpful when employing an moea in formal planning settings triantaphyllou 2000 mcdm encompasses a variety of techniques that are designed to systematically incorporate different perspectives and values into the process of selecting among alternatives when there is no clear optimal solution here a posteriori mcdm method such as the analytical hierarchical process ahp saaty 1980 would be appropriate because its method of pairwise comparison yields calculations of weights after the alternatives have already been generated i e post optimization 4 1 4 using objectives vs decisions to make choices for all participants objective performance was the main focus when choosing portfolios of interest whether they tried to balance across all objectives or prioritized a subset of them managers tended to structure preferences primarily around performance from the subset of portfolios that had satisfactory performance they would sometimes try to find the ones that had decisions they preferred this secondary screening based on decision levers almost always centered on avoidance or pursuit of certain types of storage and or moderate or aggressive use of soft path options e g interruptible shares or conservation one manager who was focusing on portfolios that minimized new storage was also concerned that some portfolios that performed relatively well in this objective could actually be hard to pursue because they had small amounts of storage in multiple locations e g a medium expansion of the south reservoir and building a small west slope reservoir another manager noted that they were all starting with performance and then looking at the levers we want to see the results first and then work backwards that s not how a lot of things are done in reality normally we look at sets of levers and then model outcomes another reflected that if you pick totally on performance and ignore decision levers you pick solutions that you wouldn t have chosen just based on decisions conversely if you pick based on decisions first you ll probably be surprised about their poor performance use of decision levers to make choices varied across the activities in the first exercise with two objectives four of nine participants considered decision levers while only three of nine did so in the four objective exercise and the 1 c warming exercise this slight drop off may at least partially be due to the fact that complexity was added via number of tradeoffs and in the case of the 1 c exercise new hydrology these changes may have taken up some extra cognitive bandwidth for participants as suggested by one manager how many objectives is too many what can we handle versus what do you miss if you don t include all of the objectives in the 4 c scenario seven of nine participants looked at decision levers while making choices one reason for this could be the fact that only hydrology changed between 1 c and 4 c besides participants greater comfort with the complex data visualizations the increased consideration of decision levers in 4 c was related to the fact that limiting the number of years in restrictions required large sacrifices in new storage and or new supply in coming to terms with this tradeoff managers tried to reduce reliance on storage or permanent agriculture dry up but ended up having to weigh this tradeoff against meeting reliability criteria whether or not they used decision lever characteristics to choose portfolios of interest all participants expressed surprise and curiosity about the relationships between decisions and performance comments like the ones below came up frequently throughout the day why do the decisions change so much but give me similar performance why do very similar portfolios perform so differently why didn t this certain lever ever get turned on in the portfolios i was focusing on why can t i have this performance but with more conservation what are the differences in decisions with these two extremes in performance why aren t these levers ever turned on are they not effective one participant wondered is the impact that subtle changes in decisions can have on performance something that utilities miss in the way we currently do things 4 2 opportunities and challenges to using moea assisted optimization 4 2 1 opportunities over the course of the exercises as well as during the large group discussion participants noted how the type of information produced by moea assisted optimization could be used to enhance their utilities long term planning processes managers proposed two uses that support the technical foundations of planning to help staff understand complex dynamics of their supply sources and infrastructure interactions and to use surprising dynamics to interrogate the accuracy of their planning models one manager focused on the public participation aspect of planning suggesting the results could be used to show community members how much service they would have to give up via more frequent restrictions in order to avoid unpopular and expensive infrastructure projects another manager brought up the council level component of planning when considering how the tradeoffs could help make the case for changing reliability policy to decision makers or board members one participant had ideas to tie the tool to two common water utility planning concepts triple bottom line assessments and robust decisions regarding the triple bottom line the manager wondered whether each lever could be scored by knowledgeable utility staff based on its economic social and environmental costs these scores could then be used as objectives and minimized by the moea although this scoring would be somewhat qualitative subjective and may possibly under or overestimate costs of specific projects that have not been thoroughly studied connecting tradeoffs directly to fundamental utility planning concepts may prove useful utilities are also concerned with elucidating robust decisions or those that support good system performance in a wide range of climatic futures the manager suggested that finding specific decisions that featured prominently in portfolios that achieved desired performance balance in multiple planning scenarios could mean that they are robust one participant thought it might be useful to give each portfolio an area under the curve score based on the objectives as a way to objectively compare portfolios another reflected that with the current trend of relying on algorithms make choices based on a priori weighting this application of optimization was appealing because it still focuses on human decision making but with extra information our post workshop survey asked participants two questions via email 1 how useful do you think the quantitative tradeoff information produced by the moea would be for learning about your utility s system 2 how useful do you think the quantitative tradeoff information produced by the moea would be for enhancing your utility s approach to long term planning the scale for responses was from 1 not useful to 5 very useful all responses for being useful for system learning and planning were 3 or above out of 5 with an average rating of 3 7 for useful for system learning and 3 9 for planning 4 2 2 challenges the challenges brought up by participants fell into three general categories modeling personnel and conveying process and results a manager from a utility that had just used moea assisted optimization in their planning asked a simple question how much do you trust your model when the participant s agency was confronted with surprising tradeoff results in some cases the results provided verifiable novel system understanding in other cases they were the product of model errors this issue was exacerbated when existing models were run in extreme hydrologic and portfolio combinations and underscored the importance of having system and project experts review portfolios on the topic of modeling other managers noted that a utility has to have the right kind of planning model a model that provides a useful timestep resolution an appropriate level of internal system detail and external context and that can run reasonably quickly managers from the two utilities that had moea experience agreed that training staff to understand the tradeoff results and maintaining those skills was a struggle interacting with tradeoff visualizations and using the information to reason about a problem requires a particular cognitive approach and the managers reported that for many members of their teams seeing results from their consultants once a month required a review at each meeting the managers did not specify how the tradeoffs were visualized during their previous planning studies furthermore to continue to see benefits from using the tool one or more staff members would need to maintain proficiency with the results and potentially be able to produce new optimization runs the managers also said that when technical staff who had often spent years developing certain projects were confronted with portfolios that performed well but did not incorporate their projects discussions and negotiations could become difficult while this workshop was not designed to analyze the details of how these two utilities used moeas conducting in depth interviews with their staff would likely result in valuable insights for both technical and social science research communities the inner workings of the moea the process of employing one and the results it produces are complex in order for a utility to use one one or more staff would have to make considerable effort to build understanding about the tool within the agency because the results are so complex technical staff also must simplify the message to decision makers without sacrificing confidence in the tool or alienating their audience managers with and without moea experience agreed that this is a difficult task a critical component of understanding simplifying and conveying the results is having visualizations that are as easy as possible to understand a manager suggested that this is so important that it may be worth consulting someone with training in data visualization for input 4 3 adoption of new tools researchers asked participants to discuss their utilities experiences with the process of adopting new tools the process starts when technical staff become aware of new tools according to these managers the water management industry is bombarded with great ideas and they have to sift through them and think about what they can apply sometimes the ideas come from consultants e g through integrated water resources plan iwrp requests for proposals and sometimes the ideas come from conferences workshops and or co production with researchers they agreed that case studies of real world applications are helpful for opening utilities up to new tools and the cases are especially influential when they involve neighboring utilities managers from one of the utilities that had used an moea said it took sustained effort to convince upper management to become that case study when a tool is being considered by a utility many conditions must be met before it can be adopted managers reported that getting broad acceptance is very challenging it can take years and requires at least one champion within the agency but preferably two because this lends credibility and force to the proposal however once technical staff have boiled down the details and shown need for and potential benefit from the tool upper managers and boards trust them to perform the innovative analyses however in order for staff to buy in they need to understand the background and guts of using the new tool they need to be heavily involved in developing and integrating it and they need to have access to technical assistance once they are using it one manager offered a distilled version of the above we need proof that the tool works trust in the people proposing it and we need evidence that it is useful and useable in other words the tool needs to be credible legitimate and salient cash et al 2003 4 4 real world planning and decision making context during small group discussions participants answered the specific questions posed by facilitators but also had discussions among themselves that provided insight into how utilities think about difficult planning questions one such conversation was around level of service versus customer billing rates if utilities try to avoid costly infrastructure as much as possible rates don t have to go up to pay for it but in the long term future when colorado utilities will potentially be even more dependent on wet years to recover system storage the only way to take advantage of that is to have adequate storage capacity if avoiding infrastructure means greater frequency of restrictions that can lead to long term reduced utility revenues prompting rates to slowly creep up anyway once rates increase they will never go back down so how do customers want to experience this a manager summarized the tradeoff by asking where do you want the pain to be in your system low reliability or high rates another group discussed the drawbacks of conservation and demand hardening howe and goemans 2007 for smaller service areas conservation has very little impact compared to the yield of new supply and may only displace the need for new supply in the short term conservation is really most effective at saving water used on lawns in wetter years in dry years watering is already reduced via restrictions once conservation sets off demand hardening or years of restrictions creates a drought shadow persistence of lower demand than pre drought levels restrictions don t produce as much savings and deeper more invasive restrictions become necessary this effect was recently further documented in dilling et al 2018 we also learned about some practical realities of how utilities make decisions managers noted that opportunism plays a big role in determining which projects go forward and when e g if a cable company is ripping up a road a utility will go ahead and fix leaks in the nearby pipes another major factor for whether utilities take on a project is whether it involves federal permitting agencies strongly prefer not to undertake this process which commonly takes more than 10 years millions of dollars and relies on highly uncertain outcomes sunk costs also motivate utility decision making any projects that have already seen some investment may be pursued regardless of optimization results in the large group discussion at the end of the day several managers lamented the realities of planning at five year intervals long term planning has become such an undertaking that the preparation involved in creating a plan can take many months after which the staff need time to recuperate as the planning process itself is so arduous once normal staff functioning resumes it may be time to start thinking about the next plan researchers should consider how their current expertise or future research can contribute tools or processes that support sustained planning 5 conclusions in the charrette tradeoffs often but not always influenced managers construction of preferences it was clear that the ability to directly compare alternatives across several dimensions helped managers reason out a dominance structure sometimes they iterated until they found a satisfying alternative and sometimes they worked backward to justify a choice in a few responses though managers simply applied their preferences to the set of options they were given and chose e g the portfolio with the least new storage this suggests that the other objectives were not compelling enough to warrant compromises and or that additional information does not always affect core priorities on another level managers often used the opportunity to make two selections to balance their indecision and actually seemed to trade performance between their two choices beyond using tradeoffs to justify their own selections managers came up with ways that tradeoff sets could bring justification to broader aspects of the utility planning process they suggested that the tradeoffs could support policy negotiations with boards or councils as well as communications with the public if the tradeoffs revealed that a minor relaxation of reliability criteria could drastically reduce reliance on new storage that information could be a valuable point of discussion similarly if a community opposed a specific project tradeoffs could explicitly show sacrifices that would be necessary to avoid it several managers noted that the ability to choose performance priorities and then learn what portfolios and decisions contribute to the performance offers a new potentially valuable way of approaching the planning process this feedback from the participants clearly points to how moeas can enhance many aspects and phases of planning though there are challenges to incorporating moeas e g appropriate and trustworthy modeling maintaining tradeoff fluency and securing technical support many managers found their distinct capabilities appealing they appreciated the ability to see relationships between objectives we heard that it was refreshing to be able to combine an optimization tool with human reasoning that the tradeoffs empowered managers instead of diminishing their input the results from this charrette suggest to us at least two promising avenues for further technical research to support moeas for long term water utility planning we draw the first from what we heard about the relationships between decision levers and objectives and how each traditionally influence planning generally utilities devise portfolios to see how they perform in the workshop a manager pointed out that they were choosing performance and then seeing which levers that entailed this shift prompted many questions about what influence one or more levers had on performance future research that quantifies relationships between levers and performance could increase the value of tradeoff sets to the agencies that use moeas the other avenue is to begin exploring the role that moea tradeoff sets can play in sustained planning can the system information attained through the tradeoffs and the large set of potential portfolios form the basis of adaptation as supply or demand information solidifies or infrastructure projects do or do not come to fruition can future actions be informed by tradeoffs and portfolios that have already been generated thus reducing the burden of planning cycles the moea research charrette was an effective approach to engaging with water managers about the potential for moeas to enhance long term water utility planning through the workshop we exchanged and created new knowledge with our participants this success was possible through the application of the participatory framework for assessment and improvement of tools parfait which created a roadmap for research activities and structured the relationships with our practitioner partners this transdisciplinary participatory venture resulted in deeper understanding of water management context inspired future research directions and forged new links between academia and practice acknowledgements this work was funded by the national oceanographic and atmospheric administration noaa through their sectoral applications research program sarp grant na14oar4310251 we would like to thank the following water supply agencies for contributing their time and expertise to this study city of aurora city of boulder colorado springs utilities denver water city of fort collins and northern water we thank western water assessment a noaa funded regional integrated sciences and assessment project for assistance in designing and facilitating the process for the workshop discussed finally we thank our reviewers for their insights and suggestions which strengthened this paper appendix this appendix provides technical details about the optimization problem that produced the tradeoffs used in the charrette for further discussion about the model the problem formulation and the tradeoff results that underpin this workshop refer to smith et al 2018 the eldorado utility planning model was created for this study using in the riverware modeling platform zagona et al 2001 riverware has features such as built in water rights accounting and a customizable policy language that allowed us to incorporate the level of complexity that is common among real world colorado water supply systems the model has five streamflow input sites in the headwaters of two major subbasins 11 water users with spatially distributed water storage diversions and return flows and 19 individual water rights ranging from 1888 most senior to 1940 most junior the hypothetical utility at the center of the optimization problem is surrounded by water users whose more senior rights limit the yields of eldorado s rights but also offer opportunities to acquire more water for example ag3 is an agricultural user downstream of eldorado s south res that has senior rights that can prevent the utility from filling the reservoir and one of the decision levers allows the utility the utility to purchase a portion of this users water rights fig 2 presents the layout of the network and the water rights the 13 decision levers their value ranges and brief descriptions are presented in table 3 the names and order given here match the workshop screenshots in figs 3 and 4 but are slightly different than how the levers were presented in smith et al 2018 the workshop names and order were tailored for managers practical understanding the different naming schemes are easily translatable between articles table 3 list of decision levers table 3 decision lever description units range ag3 purchase percentage of ag3 s senior diversion right to purchase 0 20 industrial purchase percentage of industrial user s mid seniority diversion right to purchase 0 20 wholesaler shares number of additional shares of wholesaler water to purchase shares 0 6000 ag2 irr co shares number of shares of ag2 irrigation co water to purchase shares 0 10 000 interruptible shares number of shares to include in an agreement with ag2 irrigation co for optional supply leases shares 0 10 000 conservation level level of conservation through which to reduce starting per capita demand 0 no change 1 10 reduction 2 20 reduction 0 2 dist efficiency percentage of distribution efficiency to pursue by reducing unaccounted for water baseline efficiency is 90 90 93 south res volume volume by which to expand the south reservoir mcm af 0 2 47 0 2000 west slope res volume size of a potential new west slope reservoir mcm af 0 12 3 0 10 000 gravel pit develop gravel pits to store reusable return flows downstream of the city 0 not developed 1 developed 0 1 exchange to north res acquire right to exchange reusable return flows to northres 0 1 external res space volume of space to rent in an external reservoir that can facilitate exchange efficiency mcm af 0 3 7 0 3000 ag2 res if when pay ag2 irrigation co to store water in any available unused space 0 off 1 on 0 1 moea assisted optimization evaluates performance based on an objective function vector f x where x is the portfolio defined by decision lever values each value in the vector results from calculating a separate objective f o b j e c t i v e 1 f x f r e s t l e v 1 f r e s t l e v 2 f r e s t l e v 3 f m i s s e d o p p f n e w s u p p l y f a p r i l 1 s t o r a g e f n e w s t o r a g e x ω f r e s t l e v 1 f r e s t l e v 2 and f r e s t l e v 3 are restrictions based reliability measures 5 5 we only showed managers five objectives total during the charrette because limited time with the visualizations precluded showing the full suite of objectives objectives minimizing restrictions levels 2 and 3 were omitted restriction levels are triggered based on april 1 storage levels which are used by front range colorado utilities to assess their system status for the upcoming year in the model restrictions are represented by reductions in outdoor water use while indoor use is never curtailed table 4 summarizes the restriction triggers and impacts table 4storage based triggers and water use impacts of restriction levels current storage to long term avg annual demand restriction level resulting indoor use resulting outdoor use 75 0 100 100 75 1 100 80 50 2 100 50 25 3 100 0 where current storage to long term avg annual demand is defined as 2 r e s t l e v t o t a l w a t e r i n s t o r a g e o n a p r i l 1 l o n g t e r m u n r e s t r i c t e d a n n u a l u t i l i t y d e m a n d 100 the three restrictions objectives are calculated as follows minimize the number of years that eldorado spends in level 1 restrictions 3 f r e s t l e v 1 x e i 1 y y r e s t l e v i 1 t minimize the number of years that eldorado spends in level 2 restrictions 4 f r e s t l e v 2 x e i 1 y y r e s t l e v i 2 t minimize the number of years that eldorado spends in level 3 restrictions 5 f r e s t l e v 3 x e i 1 y y r e s t l e v i 3 t where y is the number of years simulated per t traces in the hydrologic ensemble expectation notation e denotes that the average across the traces was used the optimization seeks to minimize the fourth objective f m i s s e d o p p which measures how efficiently eldorado uses its supplies and system components to meet demands it is affected by whether the utility can capitalize on reusable water and also whether eldorado acquires an overabundance of wholesaler or ag2 shares f m i s s e d o p p x 6 e 1 y i 1 y u n u s e d s h a r e s w h o l e s a l e r i u n u s e d s h a r e s i n t e r r u p t i b l e i l o s t r e u s a b l e r e t u r n f l o w s i t objective five f n e w s u p p l y is also minimized and quantifies the amount of new water that the utility acquires form shares and other water users or creates through conservation 7 f n e w s u p p l y x e 1 y y i e l d f r o m r i g h t s a g 3 r i g h t s i n d u s t r i a l s h a r e s w h o l e s a l e r s h a r e s a g 2 c o n s f a c t o r d i s t e f f i 1 y i t the sixth objective f a p r i l 1 s t o r a g e seeks to maximize the amount of water eldorado has in carryover storage on april 1 of every year 8 f a p r i l 1 s t o r a g e x e y m i n t o t a l e l d o r a d o a p r i l 1 s t o r a g e v o l a v g l o n g t e r m a n n u a l d e m a n d 100 t where y m i n denotes that the objective is calculated using the minimum annual value over the course of the simulation the final objective f n e w s t o r a g e minimizes the total volume of new storage that eldorado builds 8 f n e w s t o r a g e x e x p a n d v o l s o u t h r e s b u i l d v o l w e s t s l o p e r e s g p 0 99 m c m note that gp is multiplied by 0 99 million cubic meters mcm 800 af because the gp lever is on off or 1 0 but the volume added is 0 99 mcm 800 af the riverware model was designed to run for 25 years at a monthly timestep portfolios were considered fully implemented at the beginning of the simulation and evaluated over the 25 years each simulation took 20 s to complete we embedded the model within the borg moea hadka and reed 2013 and ran the optimization for approximately 5000 function evaluations a separate optimization was run for each hydrologic scenario the performance of each portfolio was averaged across ten hydrologic traces which were distributed to 10 computing cores using riverware s concurrent multiple run management functionality each optimization took approximately 36 h to complete and used the default borg settings except for the initial population size parameter which was changed from 100 to 50 so that evolutionary search would commence more quickly hadka et al 2012 reed et al 2013 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 011 
26217,multiobjective evolutionary algorithms moeas with colorado water managers rebecca smith a joseph kasprzyk b lisa dilling c d a bureau of reclamation 1777 exposition dr ste 113 boulder co 80301 usa bureau of reclamation 1777 exposition dr ste 113 boulder co 80301 usa b department of civil environmental and architectural engineering university of colorado 607 ucb boulder co 80309 usa department of civil environmental and architectural engineering university of colorado 607 ucb boulder co 80309 usa c environmental studies program university of colorado 397 ucb boulder co 80309 usa environmental studies program university of colorado 397 ucb boulder co 80309 usa d western water assessment university of colorado cires 216 ucb boulder co 80309 usa western water assessment university of colorado cires 216 ucb boulder co 80309 usa corresponding author multiobjective evolutionary algorithms moeas generate quantitative information about performance relationships between a system s potentially conflicting objectives termed tradeoffs research applications have suggested that evaluating tradeoffs can enhance long term water utility planning but no studies have formally engaged with practitioners to assess their perceptions of tradeoffs generated by moeas this article examines how practitioners interact with moea tradeoffs and reports their ideas for how their agencies could use moea results we hosted a group of colorado water managers at a charrette or structured investigatory workshop where they directly interacted with tradeoffs discussed how they used the information and linked their workshop experiences to opportunities for moeas to enhance their agencies planning processes among other interesting results we found that managers portfolio preferences diverged as tradeoff information increased and that structured information about the relationships between decision levers and performance would be beneficial for interpreting tradeoffs keywords participatory modeling workshop multiobjective evolutionary algorithm moea decision making long term planning tradeoffs 1 introduction decision making is a process when a choice is available to be made deliberation must occur if an agent desires an outcome and is able to take action aristotle 1920 in most decision making processes preferences are constructed based on problem framing previous experience and available information time and resources payne et al 1992 roy 1999 slovic 1995 tsoukias 2008 in combination these factors help decision makers develop what montgomery 1983 terms a dominance structure a dominance structure is a set of cognitive rules that serve to create advantages for certain alternatives or neutralize disadvantages of others such a framework is necessary when there is no strictly optimal option the dominance structure is iteratively built up in stages using mechanisms that help decision makers assess relative merits of alternatives and or alter their internal representations of situations until one alternative becomes dominant this process of creating arguments for and against alternatives develops a justification or basis for reasoning that can be conveyed to others justifiability is a cornerstone of deliberate human decision making connolly and reb 2012 payne et al 1992 slovic 1975 tversky 1972 and studying this in technology based decision support is warranted multiobjective evolutionary algorithms moeas have been researched and applied as tools to aid decision making processes concerning complex systems for which there are multiple conflicting performance measures moeas seek to optimize system performance in multiple performance objectives efficiently searching through thousands of alternatives to develop a set that quantitatively characterizes the approximate best tradeoffs between those objectives these quantified tradeoffs reveal how much performance in one objective must be forfeited to get better performance in another in the context of developing a long term water resources plan moeas test thousands of alternative portfolios of new sources new infrastructure and new operations in order to balance between performance objectives such as maximizing supply reliability and minimizing environmental impact several studies have applied moeas to long term water resources planning problems long term plans are essentially overarching decisions about pursuing a set of actions over an extended time horizon three recent academic examples are matrosov et al s use of an moea to develop long term planning portfolios for london balancing cost energy use resilience and environmental objectives 2015 zeff et al 2016 optimization of long and short term risk triggers to develop adaptation strategies and support regional cooperation between utilities in north carolina and wu et al s application of multiobjective optimization to identify portfolios of traditional and alternative water sources for adelaide in consideration of cost emissions reliability and the environmental impacts of water and wastewater reuse 2017 these studies demonstrate that moeas can produce informative tradeoffs for multiple aspects of planning in a variety of geographic contexts which could inform agencies planning decisions however none of these examples have undertaken a structured exploration of how a practitioner or agency employing an moea would interact with or perceive tradeoffs and thus have not determined whether or how they actually aid decision making to study whether the quantitative information found in moea tradeoffs contributes to the creation of defensible dominance structures that help water managers construct preferences and justify decisions researchers need to be able to observe interrogate and analyze practitioners usage of tradeoffs accomplishing this necessitates an interface between practitioners and researchers designed specifically around the type of information that results from moea assisted optimization here we can draw on an approach called a charrette which is used in non academic settings to achieve a high level of public awareness and input on the design or vision of a community project or plan us epa 2014 charrettes are also used by researchers in the fields of construction management and safety research charrettes are structured workshops that bring together industry professionals and academics in a relatively short but intensely productive session in order to generate discussion and feedback about newly created products or practices intended for industry use gibson and whittington 2010 charrettes combine the advantages of surveys interviews and focus groups in an accelerated time frame overcoming the difficulties of undertaking these methods individually e g low response rates time commitments from both researchers and practitioners access to data etc results from applying these mixed methods to technical research topics have shown that charrettes can offer both short and long term benefits to participating industry professionals and improved validity and reliability of research outcomes abowitz and toole 2010 green et al 2010 this paper presents the content methods and results of a research charrette through which our transdisciplinary research team engaged with front range colorado water managers over the use of moea tradeoff information for long term water utility planning the workshop was designed to discover how practitioners used tradeoff information to make decisions and whether and how the managers perceived the information to be useful in their agencies planning processes the goals of the workshop were to expose practitioners to an emerging tool and use the collected data to hone future moea research agendas and target new applications the charrette that we focus on in this paper is the culmination of a larger study that introduced and applied the participatory framework for assessment and improvement of tools parfait smith et al 2017 the following section briefly introduces moeas and presents work from the previous phases of our parfait efforts that pertain to this final step in the framework in section 3 we describe the methods and content from our workshop next we describe the results and in section 5 offer concluding remarks 2 background 2 1 moea assisted optimization for long term water utility planning for water utilities planning for long term sustainable water security is a critical task and a major undertaking technical staff review alternative planning portfolios and iteratively discuss goals needs and strategies with board or council level decision makers csu 2017a mwd 2015 and increasingly the public as well wuca 2015 they generally do not find a perfect plan due to the conflicts between the financial social and environmental factors that utilities must navigate elkington 2004 but utilities strive to make smart responsible and justifiable decisions that allow their systems to meet the communities chosen demand reliability policies in combination with community values multiobjective evolutionary algorithm moea assisted optimization has been studied matrosov et al 2015 mortazavi et al 2012 smith et al 2018 wu et al 2016 and applied basdekas 2014 csu 2017a as a method to help utilities develop long term plans while a traditional planning process compares the performance of a handful of planning portfolios moea assisted optimization efficiently designs and tests thousands of potential portfolios this extensive search and evaluation produces quantitative information about the system s performance in multiple objectives and the tradeoff relationships between those objectives performing moea assisted optimization requires a simulation model already developed by most utilities a problem formulation an moea and tradeoff visualizations the problem formulation is a set of decision levers objectives and constraints that the moea uses to construct and compare planning portfolios decision levers are a utility s options to modify its system e g building a reservoir or enacting conservation the set of chosen decision levers makes up a portfolio objectives are measures of system performance that are quantified representations of a system s goals or purposes e g minimizing frequency of lawn watering restrictions or maximizing water in storage constraints are numeric limits to acceptable performance e g if a portfolio cannot meet 100 of indoor demand at all times it is not considered a valid planning approach moea assisted optimization is carried out through many cycles of a computational loop the moea generates an initial population of portfolios and feeds each one to the simulation model which tests the portfolio over one or more future scenarios at the end of the simulation values for objectives and constraints are reported back to the moea this loop iterates thousands of times during which the moea intelligently evolves new generations of portfolios through both systematic and random recombination and mutation of the high performing portfolios of previous generations this results in a set of nondominated portfolios in which performance improvement in one objective is only achieved by sacrificing performance in another thus the portfolios trade off levels of performance analyzing the tradeoffs requires careful analysis including visualization techniques and these are the final component of moea assisted optimization more information about tradeoff visualization is presented in section 3 1 water utility planning is a complex process which may benefit from new technologies increased public scrutiny greater mandates to protect social and environmental interests and heightened awareness of future uncertainty all suggest that extensive portfolio search and explicit performance tradeoff information would be useful to the agencies 2 2 participatory framework for assessment and improvement of tools parfait many research applications of moea assisted optimization have established the ability of moeas to generate tradeoff information about water supply systems and produce innovative portfolios that can outperform plans developed with human expertise or previously established operational approaches maier et al 2014 nicklow et al 2010 while colorado springs utilities and melbourne water are two notable examples csu 2017b kularathna et al 2015 instances of this promising tool being applied in real world planning studies are rare to understand and potentially overcome the limited uptake of moea assisted optimization researchers must consider the factors that lead industries to adopt tools and consciously seek to create useable science that is researchers must undertake intentional iterative interaction with practitioners to understand their needs transmit research and co produce relevant future research directions díez and mcintosh 2009 dilling and lemos 2011 sarewitz and pielke 2007 smits 2002 the participatory framework for assessment and improvement of tools parfait is a research process designed to bring academics and practitioners together in a structured way smith et al 2017 parfait is a four phase research sequence that can be summarized as follows step 1 choose a promising research tool and a practical use for it that is supported by academic literature and knowledge of the proposed industry step 2 hold workshop 1 to solicit input from practitioners that will inform development of a tool testbed a testbed is a platform on which the tool can be demonstrated to practitioners step 3 build the tool testbed iterating with practitioners as necessary to ensure relatability and relevance to real world tool application context step 4 hold workshop 2 a research charrette to solicit practitioner feedback on the testbed results i e results representative of what they could expect if their agencies adopted the tool smith et al 2017 introduced parfait including the detailed steps and methodology the supporting theory behind the process and the results of workshop 1 briefly summarized below the parfait purpose and process distinguish the study presented in this paper from previous moea research studies which either applied the tool to a stylized system without input from practitioners or worked with water managers to inform its application to a real system this study instead seeks to create a context and platform through which practitioners from many agencies can gain experience using moea tradeoffs and provide their feedback to researchers here we present results from our application of parfait including a charrette with utility managers to understand their engagement with tradeoffs and the overall usability of moeas for long term decision making 2 2 1 parfait workshop 1 workshop 1 of our parfait process took place in february 2015 it brought together water managers from six front range colorado utilities 1 1 city of aurora city of boulder colorado springs utilities denver water city of fort collins and northern water and our research team which was made up of engineering social science and climate science researchers as well as water utility practitioners through targeted but free form group discussions managers shared their experiences of front range management challenges and provided feedback and suggestions to inform the elements needed to create an moea assisted optimization testbed supply and demand decision levers performance objectives and constraints future supply and demand scenarios and important features for a generic but relevant hypothetical water supply simulation model creating a relatable testbed is crucial for the successful application of parfait because it is the basis for generating representative results and also because its components must be recognizable to participants in the second parfait workshop this enables them to quickly grasp the testbed and focus on engaging with the results based on the information we generated through workshop 1 and iteration with practitioners on our research team we developed the problem formulation decision levers objectives and constraints and water supply simulation model that make up the eldorado utility planning model testbed 2 2 2 parfait testbed the eldorado utility planning model and case study to demonstrate moea assisted optimization we used the context of a hypothetical water utility called eldorado utility undertaking a long term planning process the eldorado utility planning model and case study generically capture management context relevant to utilities on the front range of colorado as well as other regions in the western u s the rest of this section will briefly describe eldorado utility s supply system and problem formulation the model and minimal pertinent front range context technical details about the optimization problem are included in the appendix for more front range context refer to smith et al 2017 and for in depth discussion about the model and case study results refer to smith et al 2018 much of the western u s is severely water limited and tightly regulated by the prior appropriation legal doctrine or first in time first in right hobbs 2004 one practical outcome of these factors is that as cities grow they obtain a variety of types of water rights e g storage rights and streamflow diversion rights each with different temporal priorities and which may be sourced from multiple geographic locations to represent this eldorado s hypothetical system includes two reservoirs on two different rivers with junior priority dates three direct diversion streamflow rights on a nearby river one senior one mid seniority and one junior one junior diversion right on a distant river that requires the diverted water to be conveyed under a mountain range in order to be stored closer to the utility and 10 000 shares of a water wholesale company that eldorado takes directly from a reservoir owned and operated by the wholesaler in many years junior right holders do not all get their full allotments caulfield jr et al 1987 p o abbott 1985 e g a reservoir does not necessarily fill or a streamflow right does not always get to divert streamflow and competition for water on different rivers varies however and this means that utilities water supplies strategically span entire regions the eldorado utility planning model encompasses 5 basins and 12 water users besides eldorado the other users with senior water rights often limit the yields from most of eldorado s sources but some also provide opportunities for the utility to acquire more reliable supplies the eldorado utility is a relatively small water provider and like much of the western u s is expecting rapid population growth state of colorado 2017 the utility has a set of 13 decision levers it can use to modify its system to meet growing demands the levers fall into three general categories pursuing new water building new storage and altering management of reusable water the third category includes decisions about leasing strategic space in other agencies reservoirs and obtaining the right to move reusable water around the region for more efficient access pursuing new water refers to decisions to acquire rights from regional agricultural or industrial users or buying shares from water wholesalers conservation is also considered new water because it frees up water that would have otherwise not been available to meet growing demands building new storage includes decisions about whether and how much to expand an existing reservoir and whether to build a new one either upstream downstream or both eldorado has defined five 2 2 the full optimization problem had two additional restrictions based objectives that were not presented in the workshop but which are described in the appendix performance objectives on which to evaluate potential portfolios they are qualitatively described here and also summarized in table 1 the first objective years in restriction 1 seeks to minimize how frequently eldorado goes into level 1 restrictions which occurs when the utility s storage drops below 75 of average annual demand 3 3 when storage drops below 50 of annual demand more severe restrictions are triggered but those were not presented in the workshop see the previous footnote eldorado s reliability policy dictates that the utility should not enact these restrictions more than 5 times in 25 years the next objective captures the utility s desire to minimize missed opportunities i e inability to use available water missed op water this measures how much of certain types of water that eldorado had access to but could not use due to incompatible demand timing lack of storage etc next eldorado seeks to minimize new supply this means minimizing the average annual volume of water over the course of the simulation that eldorado acquires through decisions such as buying rights or shares or conserving water i e freeing up water to meet new demands while eldorado does need more water for a growing population this objective is minimized because drawing more water than necessary away from other users creates social and economic disruption in their communities the new storage objective minimizes the volume of newly built storage within a portfolio because adding infrastructure is expensive uncertain and environmentally problematic finally measuring april 1 storage to demand is another way for eldorado to evaluate the reliability of their system this objective seeks to maximize the lowest april 1st storage volume over the course of the simulation i e it measures how much water is left in storage at the end of the winter drawdown season the single constraint included in the problem formulation was that all portfolios had to meet 100 of indoor demand demand remaining after outdoor water use is prohibited by level 3 restrictions the eldorado utility and regional system are modeled using the riverware platform zagona et al 2001 the optimization was performed on a 25 year simulation horizon with a monthly timestep using the borg moea hadka and reed 2013 optimizations were performed in three different hydrologic scenarios historic streamflow resulting from a 1 c perturbed future qualitatively named very warm for workshop purposes and streamflow resulting from a 4 c perturbed future referred to as very hot in the workshop these temperatures were based on a previous climate change study in which all of our front range utilities participated more information about the choice of these scenarios can be found in woodbury et al 2012 and a description of their generation is in smith et al 2018 3 methods 3 1 interactive tradeoff visualization workbooks to explore and understand the quantitative tradeoffs contained within a set of nondominated portfolios produced by moea assisted optimization users need to be able to see the complex relationships between the portfolios this is facilitated by visualizing multiple portfolios at a time in several objectives or dimensions being able to see relationships across all dimensions simultaneously provides the greatest opportunity to see tradeoffs since only seeing a subset of the objectives can obscure higher dimensional relationships kollat and reed 2007 understanding and exploring a large dataset in many dimensions requires advanced visualization techniques called visual analytics keim et al 2006 liu et al 2017 thomas and cook 2006 woodruff et al 2013 this study uses parallel axis plotting the plots use a series of vertical axes to represent as many dimensions as desired fleming et al 2005 herman et al 2014 inselberg 1985 watson and kasprzyk 2017 studies have shown that if parallel plots are interactive first time users can learn to use them effectively with 5 10 min of training johansson and forsell 2016 siirtola and räihä 2006 previous research has assessed whether users can evaluate multiple dimensions to complete a closed form task with the plots e g which one of the cars manufactured in 1982 has the slowest acceleration siirtola and räihä 2006 our workshop differs in that we asked participants to use the information from the plots to make their own choices so our results will reflect how practitioners used parallel plots to weigh tradeoffs and make judgements to enable the water managers to use parallel plots for subjective analyses we created plots that supported extensive browsing multiple selections and comparisons between portfolios and across workshop activities we used tableau a commercially available business analytics program jones 2014 to create a series of interactive worksheets on which participants could hover over portfolios to get full decision and performance information select one or more portfolios to highlight them and enter portfolio ids that changed the colors of those portfolios to register their choices for the activities described below critically the workbooks allowed us to save their choices which both recorded them for later research analysis as well as allowed us to show managers how their choices changed or did not change over the course of the workshop example results from optimizing the eldorado utility case study are presented in fig 1 briefly discussing the example results will facilitate readers understanding of the information that water managers used during the charrette described in the next section as demonstrated below we showed charrette participants the objectives and decisions together to provide all information about the portfolios and enable them to evaluate tradeoffs between different objectives while simultaneously exploring decision preferences the plots in fig 1 show 20 portfolios 4 4 the full tradeoff sets produced by the eldorado utility optimizations included approximately 1000 portfolios each smith et al 2018 in order to make the most of limited workshop activity time we only showed participants 20 predetermined alternatives that were hand selected by researchers such that the subset captured a wide range of performance for each objective and clearly presented the system s performance tradeoffs that resulted from optimizing the eldorado utility case study using hydrology generated for a 4 c warmer or very hot future the top plot has five vertical axes one for each performance objective each of the lines connecting the axes is a portfolio the vertical position at which a portfolio line crosses an objective axis denotes its performance where lower intersection is better note that the objectives and decision levers all have different numerical scales and we have normalized the values so that each dimension fully spans its axis the portfolios are colored based on how many years of level 1 restrictions they produced i e the performance on the rightmost axis blue lines all have five years in restriction red lines all have nine years two portfolios are highlighted to demonstrate the tradeoffs presented in the plot the blue portfolio has the best possible performance in april 1 storage to demand and years in restriction 1 has medium poor performance in new storage and missed op water and the worst possible performance in new supply these levels indicate the tradeoffs between reliability measures on the right two axes and other system performance considerations conversely the red portfolio performs the worst in april 1 storage to demand and years in restriction 1 but better sometimes much better than the blue portfolio in the other three objectives depending on eldorado s preferences and priorities they might choose portfolios with different performance characteristics the bottom plot shows decision lever attributes using a vertical axis for each of the 13 levers as in the objectives plot the lines connecting across axes are portfolios and the position at which they intersect an axis denotes how much of a decision is included in the portfolio the lower a portfolio line crosses the less of that lever is present each portfolio line in the objectives plot has a corresponding line in the decision levers plot so we can compare a few of the decisions led to the contrasting performance of the two highlighted alternatives described above 3 2 moea research charrette june 2016 step four of our application of parfait a research charrette provided water managers with hands on experience with moea assisted optimization results our goals for the workshop were to 1 provide exposure for the emerging tool 2 observe managers analyses of tradeoff information 3 understand how managers relate the tradeoff information to their current needs and practices 4 get feedback about what potential uses and barriers managers see in the tool 5 learn about the general process of utilities adopting a new tool and 6 report any opportunities for future research to meet the needs of practice nine total participants from six front range utilities attended the workshop the utilities represented a wide range of system sizes and the individuals themselves also spanned a range of experience levels 4 managers had over 16 years of experience in front range water management 1 had between 11 and 15 years 1 had between 6 and 10 years and 3 had 0 5 years of experience we also had participants with different roles within their respective agencies four were at a management level and five were technical staff this variety was helpful in getting different perspectives and the presence of both technical and managerial practitioners was especially encouraging since having advocates at multiple levels of administration increases the likelihood of innovation uptake daniell et al 2014 3 2 1 charrette development intense preparation and attention to charrette form function and sequencing made it possible for both participants and our team to approach the actual experience as a fun day of learning once we produced tradeoffs for the eldorado system in multiple future scenarios completed approximately three months prior to the charrette we began the process of developing content and activities that could effectively introduce new concepts and provide an engaging hands on experience to accomplish this we undertook trials of content with unaffiliated water professionals and water researchers to learn about how moea novices reacted to various levels of information and visualization complexity these dry runs helped shape the presentation of moea and testbed information choices of activities timing to complete them and design of the tableau workbooks the nine practitioner attendees were divided into three groups of three and seated at different tables each table had a facilitator and the facilitators were chosen based on their familiarity with workshops and water management so that they could prompt and guide discussions in a neutral and knowledgeable way members of the core research team floated between groups to clarify technical or procedural questions each manager was given a laptop with the tableau workbooks pre loaded so that they could complete charrette activities independently they then reflected on their individual efforts in small group discussions which are described in the results section data from this workshop includes the portfolio choices that managers made as well as discussions about the moea testbed tradeoffs managers analytical processes utilities planning approaches tool adoption potential for moeas overall and workshop content as such we made sure to capture participants portfolio choices but also took audio recordings and notes of each small group of managers having three types of information allowed us to ensure accuracy and produce results that synthesized both qualitative and quantitative responses additionally post workshop surveys recorded participants overall perceptions of the usefulness of moea assisted optimization the charrette used a detailed format custom computer workbooks and concrete tasks associated with the activities all of which guided information flows between participants and researchers compared to our first parfait workshop which relied on free form discussions about targeted topics to inform the direction of the overall project this workshop was a relatively formal participation mechanism newig et al 2008 smith et al 2017 however the facilitated small group discussion sessions built into each activity captured open discussion and impressions from participants and allowed us to access subtleties of how utilities plan and operate and how managers relate to their systems after the workshop we electronically surveyed participants about their perceptions of moea usefulness this mixture of methods is fundamental to the success of charrettes gibson and whittington 2010 the incorporation of focus group type activities and discussions was particularly useful for bridging the gap between researchers and practitioners because these interactions provide a clear view of how others think and talk morgan 1993 3 2 2 training and support materials in order for participants to fully engage in the workshop and provide researchers with thoughtful relevant feedback about using the tool they needed to be able to 1 understand why moea assisted optimization has been proposed as a useful tool for water planning 2 understand the concept of performance tradeoff sets 3 have sufficient understanding and acceptance of the hypothetical utility its supply and demand context and its policies to be able to focus on tradeoffs 4 understand and relate to the problem formulation and planning scenarios and 5 effectively operate the tableau workbooks and interact with parallel plots we covered these topics in a 90 min introductory presentation after explaining and taking questions about moeas and the testbed content similar to that found in the background section of this article we held an interactive parallel plot training session in order to introduce parallel plots and tradeoff analysis we created a simple multiobjective grocery shopping problem each participant used a tableau worksheet set up identically to those that they would see in later activities that showed plots of performance and decision levers we defined three conflicting objectives minimize cost maximize nutrition and maximize pleasure through which to optimize a set of eleven potential shopping items such as apples ice cream eggs etc as a group we went through incremental closed form exercises finding the least expensive shopping list the most nutritious list etc the exercises required participants to analyze both the decision and objective plots and learn their interactive functions the total training time was approximately 10 min questions were encouraged throughout and no participants expressed any prolonged difficulty in interpreting the worksheets to support the managers in the day s activities we gave them printed packets that included a diagram of the eldorado utility planning model current and future utility demands utility policies descriptions of the decision levers and objectives and descriptions of the different hydrologic scenarios the diagram reproduced in fig 2 conveys the spatial and temporal complexity of the system using icons colors dates and arrows 3 2 3 tradeoff activities the core of the charrette was organized into four main activities to test behavior with the tool in four different situations of increasing complexity during each activity managers were given 10 15 min to independently explore tradeoffs presented in tableau workbooks and apply their own logic or dominance structure to choose two portfolios the activities are summarized in table 2 the purpose of activity 1 was to establish initial preferences and create a basis for managers to compare decision making with and without tradeoff information the participants chose one of three portfolios developed heuristically by an expert consultant researcher familiar with the model and case study each portfolio was characterized by its constituent decisions and its firm yield in historical hydrology but no performance tradeoff information was offered the chosen portfolios from this activity were brought back in activity 4 the activity 2 sequence was designed to ease the managers into evaluating tradeoffs in complex plots to create space for analyzing tradeoffs without the dominant influence of reliability smith et al 2017 to have managers be able to explicitly compare their use of different amounts of information and to do all of this without considering the likelihood or implications of climate change on front range supplies in activity 2 exercise 1 participants were shown performance of 20 algorithm optimized portfolios in a two objective tradeoff along with a plot of all of the portfolios decisions and asked to select two portfolios of interest the portfolios resulted from optimizing for historical hydrology and were constrained to meet eldorado s restrictions based reliability policy this was made clear to managers so they knew they did not have to worry about reliability in this first activity in activity 2 exercise 2 managers were shown the same set of 20 portfolios as in exercise 1 but now were given performance information in a four objective tradeoff plot along with the decisions plot in activity 2 exercise 3 participants were shown the choices they made from exercises 1 and 2 in one plot to compare the preferences they expressed with different amounts of tradeoff information activity 3 introduced the frequency of level 1 restrictions objective and perturbed hydrology the exercises allowed researchers to probe how the presence of the level 1 restrictions objective influenced participants perceptions of other tradeoffs and added hydrologic challenges to their decision calculations in exercise 1 participants were shown 20 algorithm optimized portfolios that resulted from optimizing in a 1 c warmer very warm future to understand the implications of the different hydrology managers referred to the informational packets where plots showing a slightly lower magnitude of peak runoff slightly earlier peak timing and similar regional flow variability they were again asked to choose two portfolios and had to directly trade off reliability policy performance with the other four objectives from activity 2 exercise 2 was identical to exercise 1 except that the 20 portfolios were from a set produced by optimizing for a 4 c warmer very hot future this scenario had a much lower peak runoff magnitude much earlier peak timing and lower variability due to lower magnitude high flow years activity 4 was designed to emphasize to participants that portfolios developed for or optimized under specific futures may not be acceptable if the future is different than they planned for managers saw the exact same set of portfolios from activity 3 exercise 2 but now their performance in a set of varied hydrologic traces was shown i e in a supply scenario that they were not optimized for the set of 10 traces were drawn from all other scenarios so performance reflected the portfolios average performances in a wide set of futures they were again asked to make two choices from this set and while making the choices they could see how each portfolio performed in varied as well as 4 c hydrology so they had two parallel plots of objectives and one plot of decision levers managers were also shown how their hand crafted solution from activity 1 performed in both scenarios and asked to reflect on how they felt about those portfolios which were developed using historical hydrology at the workshop the managers played the roles of engineers at the hypothetical eldorado utility who were evaluating a new tool for its potential to enhance their upcoming long term planning process asking them to play a fictional role and use hypothetical but realistic tradeoff results helped participants to engage more candidly by distancing them from physical social and political pressures of their own systems similarly for each activity we asked the managers to choose two portfolios to subject to further analysis to avoid comparisons with the real world complex process that a utility undertakes to actually decide on one plan it was important however to ask them to make individual choices this forced them to really grapple with tradeoffs and to use some logical process and thus created a more defined experience for them to discuss with researchers and each other for each exercise except for activity 2 exercise 3 during which participants just compared two sets of portfolio choices the group facilitators asked three main questions to prompt discussion a what objective performances or tradeoffs made the two portfolios you chose interesting to you b what decision lever attributes made the solutions interesting to you c based on the objectives performance as a manager at your utility do you think you would have chosen the same solutions to investigate further why or why not questions a and b were designed to separate the ways that performance and decision levers impacted choices and question c was designed to emphasize that we wanted the managers to choose freely but also provide as much real world decision making context as they could 4 results throughout the day managers engaged with tradeoffs facilitators and each other they took the purposes of the workshop seriously and combined openness to the activities with reflections about their own agencies planning contexts as we prompted them with specific concepts they each interpreted and applied them differently a result of this was that across nine managers the portfolio selections often varied widely and sometimes the processes they used to make them also varied significantly rather than report each individual s choices and processes below is a description of common themes and examples of how logic changed over the course of the day 4 1 managers use of tradeoffs within this section about how managers used tradeoff information there are four subsections the first discusses findings from activity 2 which presented managers with first two then four objectives to analyze brief synopses of two managers decision processes and how they relate to dominance structuring are included the second subsection of results is based on activity 3 which introduced a fifth objective level 1 restrictions and two new more hydrologically challenging scenarios subsections three and four present findings that emerged throughout all four activities 4 1 1 tradeoffs in two objectives vs in four objectives in activity 2 exercise 1 where participants saw tradeoffs in two objectives three general strategies emerged for choosing portfolios of interest five managers weighted performance in the objectives equally two performed cost benefit analyses between the two objectives and two managers prioritized performance in one objective over the other fig 3 shows the results of manager b4 s cost benefit analysis the manager started by picking the portfolio with the least storage then worked incrementally up the new storage axis to find out how much better the performance in missed opportunity could get the manager ultimately tried to find the portfolios where the tradeoff was reasonable where the sacrifice in one objective came with a worthwhile gain in the other this process is an example of the creation of a dominance structure the manager initially screened alternatives based on the lexicographic decision rule where there is a most important attribute and then iteratively applied the addition of utilities rule to select portfolios montgomery 1983 in activity 2 exercise 2 managers were asked to make two selections from the same set of portfolios that they saw in exercise 1 but they did so with performance tradeoffs in four objectives instead of just two so it was possible to choose the same portfolios in both exercises indeed two managers chose the exact same portfolios as they did in exercise 1 three managers chose one matching portfolio and the other four participants chose two new solutions when presented with more tradeoff information in activity 2 exercise 3 where managers were shown the two sets of choices they made the managers who had identical sets of choices said that they used the same criteria in the second exercise as they did first this is a good reminder that new tools and new information do not necessarily result in changed preferences or different choices moea tradeoffs may also reinforce existing cognitive heuristics or increase confidence in decisions depending on the nature of the tradeoffs and how managers use a priori judgements to create dominance structures for the seven managers who chose at least one new portfolio in exercise 2 they tended to balance their two choices against each other for example if they chose one portfolio that was middle of the road across the objectives they allowed themselves to choose a second portfolio that prioritized one objective regardless of whether it performed poorly in another fig 4 shows manager b2 s two selections for the first choice blue the manager balanced across the objectives i e he collapsed performance attributes using the addition of utilities rule the second choice maroon resulted from screening lexicographically on new storage performance and then deemphasizing missed op to support the dominance of the chosen portfolio when discussing the process used to make choices with two tradeoffs versus four this manager said more objectives is better in terms of understanding the system and its performance i assume that at some point it gets too noisy but i definitely see value in going from two to four even if i end up prioritizing one or two objectives it helps to see the implications that has on the others 4 1 2 use of the level 1 restrictions objective the tradeoff analyses from activity 2 included only portfolios that complied with eldorado s restrictions based reliability criteria defined as not exceeding 5 years in level 1 restrictions over the 25 year simulation this condition was explicitly conveyed to participants and they were not presented with an objective that measured performance with respect to restrictions by omitting an objective about restriction performance the participants were able to consider their performance and decision preferences without directly grappling with level of service or policy consequences once frequency of restrictions was introduced as the fifth objective in activity 3 all participants used it as their initial screening criterion additionally both exercises in this activity used a climate change perturbed hydrology the portfolios in exercise 1 resulted from a 1 c warmer scenario and portfolios in exercise 2 resulted from a 4 c warmer scenario though they had many options that resulted in only two years of restrictions five of nine managers considered portfolios that exhibited from three to five years in level 1 restriction in activity 3 exercise 1 however only two managers ended up choosing portfolios with three or more years in restriction while none of the other seven managers found that the performance gains in other objectives warranted the extra years seven of nine participants expressed satisfaction with the performance and decision lever balances they were able to strike and two expressed concerns that their decision preferences seemed less effective in the warmer scenario while all managers chose to perform better than eldorado s reliability policy criteria in activity 2 exercise 1 it was difficult to meet the criteria in exercise 2 because the portfolios were evaluated in a hydrologically challenging 4 c warmer future there were no portfolios that had fewer than five years in level 1 restrictions the maximum allowed by eldorado s policy all portfolios that met the criteria required great sacrifices in at least two other objectives for the three participants who stayed within the criteria two focused on performance in one other objective and one tried to balance three other objectives within the compliant portfolios of the six other managers four determined that one or two extra years in restrictions was worth the gains in other objectives noting that this thinking would trigger policy discussions with their decision making boards this tool would be really useful in demonstrating just how much service we would have to give up in order to avoid unpopular storage or supply decisions two managers felt that once the climate had warmed by 4 c norms would have changed lawns would have disappeared and people wouldn t expect the same levels of service that they had seen in the past so they chose portfolios with nine years in restrictions and were able to avoid big storage projects so given difficult tradeoffs three managers made painful concessions to comply with restrictions policy four bargained on relatively small policy deviations thinking that the trade could become the focus of broader negotiations and two managers reframed the problem in order to justify alternative s that they considered superior 4 1 3 more information divergent choices in analyzing all managers specific portfolio choices over the course of the day we found that as more information was added and tradeoff experience increased the group s choices started to diverge three participants made identical choices to one another in activity 2 exercise 1 the same was true in activity 2 exercise 2 the sets of participants and the choices were different though and there were no correlations with experience level of the participants or size of utility overall seven different portfolios were chosen in exercise 1 and five different portfolios were chosen in exercise 2 out of 18 total choices made per exercise 2 choices for each of 9 participants there were no sets of identical choices in activities 3 and 4 in activity 3 exercises 1 and 2 10 and 12 different portfolios were chosen respectively out of 18 total choices that were made for each exercise finally 12 different portfolios were chosen in activity 4 when participants were prompted to select portfolios while evaluating their performance in two different futures simultaneously adding the level 1 restrictions objective and using more challenging hydrology resulted in more divergent choices than those with historical hydrology and only two or four objectives in play the finding that more information can lead to a wider variety of choices is perhaps not surprising because there are more avenues for creating dominance structure however it is worth considering how increasing information and greater divergence would affect a real world planning process that involves several levels of scrutiny by many employees decision makers and the public while this workshop focused on how individuals perceived and worked with high dimensional tradeoffs more formal preference elicitation and application via multicriteria decision making mcdm could be helpful when employing an moea in formal planning settings triantaphyllou 2000 mcdm encompasses a variety of techniques that are designed to systematically incorporate different perspectives and values into the process of selecting among alternatives when there is no clear optimal solution here a posteriori mcdm method such as the analytical hierarchical process ahp saaty 1980 would be appropriate because its method of pairwise comparison yields calculations of weights after the alternatives have already been generated i e post optimization 4 1 4 using objectives vs decisions to make choices for all participants objective performance was the main focus when choosing portfolios of interest whether they tried to balance across all objectives or prioritized a subset of them managers tended to structure preferences primarily around performance from the subset of portfolios that had satisfactory performance they would sometimes try to find the ones that had decisions they preferred this secondary screening based on decision levers almost always centered on avoidance or pursuit of certain types of storage and or moderate or aggressive use of soft path options e g interruptible shares or conservation one manager who was focusing on portfolios that minimized new storage was also concerned that some portfolios that performed relatively well in this objective could actually be hard to pursue because they had small amounts of storage in multiple locations e g a medium expansion of the south reservoir and building a small west slope reservoir another manager noted that they were all starting with performance and then looking at the levers we want to see the results first and then work backwards that s not how a lot of things are done in reality normally we look at sets of levers and then model outcomes another reflected that if you pick totally on performance and ignore decision levers you pick solutions that you wouldn t have chosen just based on decisions conversely if you pick based on decisions first you ll probably be surprised about their poor performance use of decision levers to make choices varied across the activities in the first exercise with two objectives four of nine participants considered decision levers while only three of nine did so in the four objective exercise and the 1 c warming exercise this slight drop off may at least partially be due to the fact that complexity was added via number of tradeoffs and in the case of the 1 c exercise new hydrology these changes may have taken up some extra cognitive bandwidth for participants as suggested by one manager how many objectives is too many what can we handle versus what do you miss if you don t include all of the objectives in the 4 c scenario seven of nine participants looked at decision levers while making choices one reason for this could be the fact that only hydrology changed between 1 c and 4 c besides participants greater comfort with the complex data visualizations the increased consideration of decision levers in 4 c was related to the fact that limiting the number of years in restrictions required large sacrifices in new storage and or new supply in coming to terms with this tradeoff managers tried to reduce reliance on storage or permanent agriculture dry up but ended up having to weigh this tradeoff against meeting reliability criteria whether or not they used decision lever characteristics to choose portfolios of interest all participants expressed surprise and curiosity about the relationships between decisions and performance comments like the ones below came up frequently throughout the day why do the decisions change so much but give me similar performance why do very similar portfolios perform so differently why didn t this certain lever ever get turned on in the portfolios i was focusing on why can t i have this performance but with more conservation what are the differences in decisions with these two extremes in performance why aren t these levers ever turned on are they not effective one participant wondered is the impact that subtle changes in decisions can have on performance something that utilities miss in the way we currently do things 4 2 opportunities and challenges to using moea assisted optimization 4 2 1 opportunities over the course of the exercises as well as during the large group discussion participants noted how the type of information produced by moea assisted optimization could be used to enhance their utilities long term planning processes managers proposed two uses that support the technical foundations of planning to help staff understand complex dynamics of their supply sources and infrastructure interactions and to use surprising dynamics to interrogate the accuracy of their planning models one manager focused on the public participation aspect of planning suggesting the results could be used to show community members how much service they would have to give up via more frequent restrictions in order to avoid unpopular and expensive infrastructure projects another manager brought up the council level component of planning when considering how the tradeoffs could help make the case for changing reliability policy to decision makers or board members one participant had ideas to tie the tool to two common water utility planning concepts triple bottom line assessments and robust decisions regarding the triple bottom line the manager wondered whether each lever could be scored by knowledgeable utility staff based on its economic social and environmental costs these scores could then be used as objectives and minimized by the moea although this scoring would be somewhat qualitative subjective and may possibly under or overestimate costs of specific projects that have not been thoroughly studied connecting tradeoffs directly to fundamental utility planning concepts may prove useful utilities are also concerned with elucidating robust decisions or those that support good system performance in a wide range of climatic futures the manager suggested that finding specific decisions that featured prominently in portfolios that achieved desired performance balance in multiple planning scenarios could mean that they are robust one participant thought it might be useful to give each portfolio an area under the curve score based on the objectives as a way to objectively compare portfolios another reflected that with the current trend of relying on algorithms make choices based on a priori weighting this application of optimization was appealing because it still focuses on human decision making but with extra information our post workshop survey asked participants two questions via email 1 how useful do you think the quantitative tradeoff information produced by the moea would be for learning about your utility s system 2 how useful do you think the quantitative tradeoff information produced by the moea would be for enhancing your utility s approach to long term planning the scale for responses was from 1 not useful to 5 very useful all responses for being useful for system learning and planning were 3 or above out of 5 with an average rating of 3 7 for useful for system learning and 3 9 for planning 4 2 2 challenges the challenges brought up by participants fell into three general categories modeling personnel and conveying process and results a manager from a utility that had just used moea assisted optimization in their planning asked a simple question how much do you trust your model when the participant s agency was confronted with surprising tradeoff results in some cases the results provided verifiable novel system understanding in other cases they were the product of model errors this issue was exacerbated when existing models were run in extreme hydrologic and portfolio combinations and underscored the importance of having system and project experts review portfolios on the topic of modeling other managers noted that a utility has to have the right kind of planning model a model that provides a useful timestep resolution an appropriate level of internal system detail and external context and that can run reasonably quickly managers from the two utilities that had moea experience agreed that training staff to understand the tradeoff results and maintaining those skills was a struggle interacting with tradeoff visualizations and using the information to reason about a problem requires a particular cognitive approach and the managers reported that for many members of their teams seeing results from their consultants once a month required a review at each meeting the managers did not specify how the tradeoffs were visualized during their previous planning studies furthermore to continue to see benefits from using the tool one or more staff members would need to maintain proficiency with the results and potentially be able to produce new optimization runs the managers also said that when technical staff who had often spent years developing certain projects were confronted with portfolios that performed well but did not incorporate their projects discussions and negotiations could become difficult while this workshop was not designed to analyze the details of how these two utilities used moeas conducting in depth interviews with their staff would likely result in valuable insights for both technical and social science research communities the inner workings of the moea the process of employing one and the results it produces are complex in order for a utility to use one one or more staff would have to make considerable effort to build understanding about the tool within the agency because the results are so complex technical staff also must simplify the message to decision makers without sacrificing confidence in the tool or alienating their audience managers with and without moea experience agreed that this is a difficult task a critical component of understanding simplifying and conveying the results is having visualizations that are as easy as possible to understand a manager suggested that this is so important that it may be worth consulting someone with training in data visualization for input 4 3 adoption of new tools researchers asked participants to discuss their utilities experiences with the process of adopting new tools the process starts when technical staff become aware of new tools according to these managers the water management industry is bombarded with great ideas and they have to sift through them and think about what they can apply sometimes the ideas come from consultants e g through integrated water resources plan iwrp requests for proposals and sometimes the ideas come from conferences workshops and or co production with researchers they agreed that case studies of real world applications are helpful for opening utilities up to new tools and the cases are especially influential when they involve neighboring utilities managers from one of the utilities that had used an moea said it took sustained effort to convince upper management to become that case study when a tool is being considered by a utility many conditions must be met before it can be adopted managers reported that getting broad acceptance is very challenging it can take years and requires at least one champion within the agency but preferably two because this lends credibility and force to the proposal however once technical staff have boiled down the details and shown need for and potential benefit from the tool upper managers and boards trust them to perform the innovative analyses however in order for staff to buy in they need to understand the background and guts of using the new tool they need to be heavily involved in developing and integrating it and they need to have access to technical assistance once they are using it one manager offered a distilled version of the above we need proof that the tool works trust in the people proposing it and we need evidence that it is useful and useable in other words the tool needs to be credible legitimate and salient cash et al 2003 4 4 real world planning and decision making context during small group discussions participants answered the specific questions posed by facilitators but also had discussions among themselves that provided insight into how utilities think about difficult planning questions one such conversation was around level of service versus customer billing rates if utilities try to avoid costly infrastructure as much as possible rates don t have to go up to pay for it but in the long term future when colorado utilities will potentially be even more dependent on wet years to recover system storage the only way to take advantage of that is to have adequate storage capacity if avoiding infrastructure means greater frequency of restrictions that can lead to long term reduced utility revenues prompting rates to slowly creep up anyway once rates increase they will never go back down so how do customers want to experience this a manager summarized the tradeoff by asking where do you want the pain to be in your system low reliability or high rates another group discussed the drawbacks of conservation and demand hardening howe and goemans 2007 for smaller service areas conservation has very little impact compared to the yield of new supply and may only displace the need for new supply in the short term conservation is really most effective at saving water used on lawns in wetter years in dry years watering is already reduced via restrictions once conservation sets off demand hardening or years of restrictions creates a drought shadow persistence of lower demand than pre drought levels restrictions don t produce as much savings and deeper more invasive restrictions become necessary this effect was recently further documented in dilling et al 2018 we also learned about some practical realities of how utilities make decisions managers noted that opportunism plays a big role in determining which projects go forward and when e g if a cable company is ripping up a road a utility will go ahead and fix leaks in the nearby pipes another major factor for whether utilities take on a project is whether it involves federal permitting agencies strongly prefer not to undertake this process which commonly takes more than 10 years millions of dollars and relies on highly uncertain outcomes sunk costs also motivate utility decision making any projects that have already seen some investment may be pursued regardless of optimization results in the large group discussion at the end of the day several managers lamented the realities of planning at five year intervals long term planning has become such an undertaking that the preparation involved in creating a plan can take many months after which the staff need time to recuperate as the planning process itself is so arduous once normal staff functioning resumes it may be time to start thinking about the next plan researchers should consider how their current expertise or future research can contribute tools or processes that support sustained planning 5 conclusions in the charrette tradeoffs often but not always influenced managers construction of preferences it was clear that the ability to directly compare alternatives across several dimensions helped managers reason out a dominance structure sometimes they iterated until they found a satisfying alternative and sometimes they worked backward to justify a choice in a few responses though managers simply applied their preferences to the set of options they were given and chose e g the portfolio with the least new storage this suggests that the other objectives were not compelling enough to warrant compromises and or that additional information does not always affect core priorities on another level managers often used the opportunity to make two selections to balance their indecision and actually seemed to trade performance between their two choices beyond using tradeoffs to justify their own selections managers came up with ways that tradeoff sets could bring justification to broader aspects of the utility planning process they suggested that the tradeoffs could support policy negotiations with boards or councils as well as communications with the public if the tradeoffs revealed that a minor relaxation of reliability criteria could drastically reduce reliance on new storage that information could be a valuable point of discussion similarly if a community opposed a specific project tradeoffs could explicitly show sacrifices that would be necessary to avoid it several managers noted that the ability to choose performance priorities and then learn what portfolios and decisions contribute to the performance offers a new potentially valuable way of approaching the planning process this feedback from the participants clearly points to how moeas can enhance many aspects and phases of planning though there are challenges to incorporating moeas e g appropriate and trustworthy modeling maintaining tradeoff fluency and securing technical support many managers found their distinct capabilities appealing they appreciated the ability to see relationships between objectives we heard that it was refreshing to be able to combine an optimization tool with human reasoning that the tradeoffs empowered managers instead of diminishing their input the results from this charrette suggest to us at least two promising avenues for further technical research to support moeas for long term water utility planning we draw the first from what we heard about the relationships between decision levers and objectives and how each traditionally influence planning generally utilities devise portfolios to see how they perform in the workshop a manager pointed out that they were choosing performance and then seeing which levers that entailed this shift prompted many questions about what influence one or more levers had on performance future research that quantifies relationships between levers and performance could increase the value of tradeoff sets to the agencies that use moeas the other avenue is to begin exploring the role that moea tradeoff sets can play in sustained planning can the system information attained through the tradeoffs and the large set of potential portfolios form the basis of adaptation as supply or demand information solidifies or infrastructure projects do or do not come to fruition can future actions be informed by tradeoffs and portfolios that have already been generated thus reducing the burden of planning cycles the moea research charrette was an effective approach to engaging with water managers about the potential for moeas to enhance long term water utility planning through the workshop we exchanged and created new knowledge with our participants this success was possible through the application of the participatory framework for assessment and improvement of tools parfait which created a roadmap for research activities and structured the relationships with our practitioner partners this transdisciplinary participatory venture resulted in deeper understanding of water management context inspired future research directions and forged new links between academia and practice acknowledgements this work was funded by the national oceanographic and atmospheric administration noaa through their sectoral applications research program sarp grant na14oar4310251 we would like to thank the following water supply agencies for contributing their time and expertise to this study city of aurora city of boulder colorado springs utilities denver water city of fort collins and northern water we thank western water assessment a noaa funded regional integrated sciences and assessment project for assistance in designing and facilitating the process for the workshop discussed finally we thank our reviewers for their insights and suggestions which strengthened this paper appendix this appendix provides technical details about the optimization problem that produced the tradeoffs used in the charrette for further discussion about the model the problem formulation and the tradeoff results that underpin this workshop refer to smith et al 2018 the eldorado utility planning model was created for this study using in the riverware modeling platform zagona et al 2001 riverware has features such as built in water rights accounting and a customizable policy language that allowed us to incorporate the level of complexity that is common among real world colorado water supply systems the model has five streamflow input sites in the headwaters of two major subbasins 11 water users with spatially distributed water storage diversions and return flows and 19 individual water rights ranging from 1888 most senior to 1940 most junior the hypothetical utility at the center of the optimization problem is surrounded by water users whose more senior rights limit the yields of eldorado s rights but also offer opportunities to acquire more water for example ag3 is an agricultural user downstream of eldorado s south res that has senior rights that can prevent the utility from filling the reservoir and one of the decision levers allows the utility the utility to purchase a portion of this users water rights fig 2 presents the layout of the network and the water rights the 13 decision levers their value ranges and brief descriptions are presented in table 3 the names and order given here match the workshop screenshots in figs 3 and 4 but are slightly different than how the levers were presented in smith et al 2018 the workshop names and order were tailored for managers practical understanding the different naming schemes are easily translatable between articles table 3 list of decision levers table 3 decision lever description units range ag3 purchase percentage of ag3 s senior diversion right to purchase 0 20 industrial purchase percentage of industrial user s mid seniority diversion right to purchase 0 20 wholesaler shares number of additional shares of wholesaler water to purchase shares 0 6000 ag2 irr co shares number of shares of ag2 irrigation co water to purchase shares 0 10 000 interruptible shares number of shares to include in an agreement with ag2 irrigation co for optional supply leases shares 0 10 000 conservation level level of conservation through which to reduce starting per capita demand 0 no change 1 10 reduction 2 20 reduction 0 2 dist efficiency percentage of distribution efficiency to pursue by reducing unaccounted for water baseline efficiency is 90 90 93 south res volume volume by which to expand the south reservoir mcm af 0 2 47 0 2000 west slope res volume size of a potential new west slope reservoir mcm af 0 12 3 0 10 000 gravel pit develop gravel pits to store reusable return flows downstream of the city 0 not developed 1 developed 0 1 exchange to north res acquire right to exchange reusable return flows to northres 0 1 external res space volume of space to rent in an external reservoir that can facilitate exchange efficiency mcm af 0 3 7 0 3000 ag2 res if when pay ag2 irrigation co to store water in any available unused space 0 off 1 on 0 1 moea assisted optimization evaluates performance based on an objective function vector f x where x is the portfolio defined by decision lever values each value in the vector results from calculating a separate objective f o b j e c t i v e 1 f x f r e s t l e v 1 f r e s t l e v 2 f r e s t l e v 3 f m i s s e d o p p f n e w s u p p l y f a p r i l 1 s t o r a g e f n e w s t o r a g e x ω f r e s t l e v 1 f r e s t l e v 2 and f r e s t l e v 3 are restrictions based reliability measures 5 5 we only showed managers five objectives total during the charrette because limited time with the visualizations precluded showing the full suite of objectives objectives minimizing restrictions levels 2 and 3 were omitted restriction levels are triggered based on april 1 storage levels which are used by front range colorado utilities to assess their system status for the upcoming year in the model restrictions are represented by reductions in outdoor water use while indoor use is never curtailed table 4 summarizes the restriction triggers and impacts table 4storage based triggers and water use impacts of restriction levels current storage to long term avg annual demand restriction level resulting indoor use resulting outdoor use 75 0 100 100 75 1 100 80 50 2 100 50 25 3 100 0 where current storage to long term avg annual demand is defined as 2 r e s t l e v t o t a l w a t e r i n s t o r a g e o n a p r i l 1 l o n g t e r m u n r e s t r i c t e d a n n u a l u t i l i t y d e m a n d 100 the three restrictions objectives are calculated as follows minimize the number of years that eldorado spends in level 1 restrictions 3 f r e s t l e v 1 x e i 1 y y r e s t l e v i 1 t minimize the number of years that eldorado spends in level 2 restrictions 4 f r e s t l e v 2 x e i 1 y y r e s t l e v i 2 t minimize the number of years that eldorado spends in level 3 restrictions 5 f r e s t l e v 3 x e i 1 y y r e s t l e v i 3 t where y is the number of years simulated per t traces in the hydrologic ensemble expectation notation e denotes that the average across the traces was used the optimization seeks to minimize the fourth objective f m i s s e d o p p which measures how efficiently eldorado uses its supplies and system components to meet demands it is affected by whether the utility can capitalize on reusable water and also whether eldorado acquires an overabundance of wholesaler or ag2 shares f m i s s e d o p p x 6 e 1 y i 1 y u n u s e d s h a r e s w h o l e s a l e r i u n u s e d s h a r e s i n t e r r u p t i b l e i l o s t r e u s a b l e r e t u r n f l o w s i t objective five f n e w s u p p l y is also minimized and quantifies the amount of new water that the utility acquires form shares and other water users or creates through conservation 7 f n e w s u p p l y x e 1 y y i e l d f r o m r i g h t s a g 3 r i g h t s i n d u s t r i a l s h a r e s w h o l e s a l e r s h a r e s a g 2 c o n s f a c t o r d i s t e f f i 1 y i t the sixth objective f a p r i l 1 s t o r a g e seeks to maximize the amount of water eldorado has in carryover storage on april 1 of every year 8 f a p r i l 1 s t o r a g e x e y m i n t o t a l e l d o r a d o a p r i l 1 s t o r a g e v o l a v g l o n g t e r m a n n u a l d e m a n d 100 t where y m i n denotes that the objective is calculated using the minimum annual value over the course of the simulation the final objective f n e w s t o r a g e minimizes the total volume of new storage that eldorado builds 8 f n e w s t o r a g e x e x p a n d v o l s o u t h r e s b u i l d v o l w e s t s l o p e r e s g p 0 99 m c m note that gp is multiplied by 0 99 million cubic meters mcm 800 af because the gp lever is on off or 1 0 but the volume added is 0 99 mcm 800 af the riverware model was designed to run for 25 years at a monthly timestep portfolios were considered fully implemented at the beginning of the simulation and evaluated over the 25 years each simulation took 20 s to complete we embedded the model within the borg moea hadka and reed 2013 and ran the optimization for approximately 5000 function evaluations a separate optimization was run for each hydrologic scenario the performance of each portfolio was averaged across ten hydrologic traces which were distributed to 10 computing cores using riverware s concurrent multiple run management functionality each optimization took approximately 36 h to complete and used the default borg settings except for the initial population size parameter which was changed from 100 to 50 so that evolutionary search would commence more quickly hadka et al 2012 reed et al 2013 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 011 
26218,understanding the spatial and temporal distribution of hydrologic variables such as streamflow is important for sustainable development especially with global population growth and climate variations typical monitoring of streamflow is conducted using in situ gauging stations however stations are costly to setup and maintain leading to data gaps in regions that cannot afford gauges satellite data including altimetry data are used to supplement in situ observations and in some cases supply information where they are lacking this study introduces an open source web application to access and explore altimetry datasets for use in water level monitoring named the altimetry explorer altex this web application along with its relevant rest api facilitates access to altimetry data for analysis visualization and impact the data provided through altex is validated using thirteen gauges in the amazon basin from 2008 to 2018 with an average nash sutcliffe coefficient and root mean square error of 0 78 and 1 2 m respectively access to global water level data should be particularly helpful for water resource practitioners and researchers seeking to understand the long term trends and dynamics of global water level and availability this work provides an initial framework for a more robust and comprehensive platform to access future altimetry datasets and support research related to global water resources keywords satellite altimetry tethys platform online web application water level global applications 1 introduction global population growth human activities climate change and uneven distribution of water supplies whether being water excess or scarcity are leading to future water insecurity vörösmarty et al 2010 tao et al 2015 understanding the spatial and temporal distribution of water resources is critical to support sustainable development policies and activities addressing water security vörösmarty et al 2000 streamflow and river heights in developed nations are actively being monitored since the late nineteenth century for better management and allocation of water resources barrow 1998 while these important hydrologic variables are being monitored in developed countries in situ streamflow monitoring is largely lacking in developing regions predominantly due to the impractical cost of installation and maintenance of these instruments alsdorf et al 2001 streamflow and river height information collected by means of in situ monitoring can be used to study and understand the impact of climate change on water resources and help understand trends of natural hazards e g flood and drought conditions haritashya et al 2006 huntington 2006 chen et al 2014 given the utility of such data for understanding the hydrologic cycle these data gaps provide opportunities for innovative solutions to deliver data in data sparse regions particularly for transboundary river basins lakshmi et al 2018 satellite based remote sensing and modeling techniques have been used to fill data gaps and provide vital information for international and transboundary water resources monitoring and management e g mohammed et al 2018a mohammed et al 2018b the results of many studies support the use of remotely sensed data for a wide range of water resources applications across the globe for example maswood and hossain 2016 used satellite data including altimetry data as inputs into a hydrologic model in the ganges brahmaputra and meghna gbm river basins providing improved river monitoring for flood forecasting applications across the three basins additionally chang et al 2019 used the variable infiltration capacity vic model with altimetry data to develop a forecasting model in the mekong basin resulting in an improved five day forecast along the mainstem river a similar approach to flood forecasting using altimetry data was implemented by hossain and bhuiyan hossain et al 2016 for the gbm basins increasing the flood forecast time to five days moreover bonnema and hossain 2017 demonstrated the use of only satellite remote sensing datasets altimetry and landsat data to infer streamflow response to reservoir operations in the mekong basin other studies have focused solely on using altimetry datasets for monitoring purposes along the in the yangtze river basin chu et al 2008 the zambezi river basin michailovsky et al 2012 the congo river basin kim et al 2019 lakes in indonesia sulistioadi et al 2015 and more broadly across north america africa and southeast asia ricko et al 2012 these studies highlight how a growing number of users are interested in applying remotely sensed water level estimates and more specifically satellite altimetry to a wide variety of studies and water resources management challenges related to the hydrologic cycle and climate change with the advent of satellite and radar altimetry data the capability to monitor sea river and lake level changes with acceptable accuracy has significantly improved since the early 1990s satellite radar altimetry sends radar pulses to the earth and measures the return time from the reflecting surface to calculate the water level relative to a reference datum along with the satellites altitude the first major oceanographic research altimeter topex poseidon fu et al 1994 collected the data from 1992 to 2005 paving the way for subsequent satellites such as the jason series jason 1 2 and 3 provided data from 2002 to present creating a long series of available water level measurements additional sensors such as envisat operated from 2002 to 2010 saral altika operated from 2013 to 2016 in exact repeat orbit and sentinel 3 currently operating since 2016 have been launched to supplement and continue the water level measurements from the jason series although altimeters were designed primarily for ocean and ice studies they have been applied to the monitoring of inland water bodies lee et al 2009 in particular the ability to remotely detect water surface level changes in lakes and inland seas has been demonstrated studies have shown that results derived from altimetry data demonstrate how sub monthly seasonal and inter annual variations in height can be monitored and provide invaluable information in data sparse regions bonnema et al 2016 despite the use of altimetry data in scientific studies gao et al 2012 concluded that access to water level data has been a major challenge in the global study of reservoirs applications exist to lower the barrier to access and use water level information from altimetry sensors for example a gui based jason 2 3 and sentinel 3a data processing toolbox to generate time series of water level changes over user defined inland water bodies has been developed based on the automation algorithm by okeowo et al 2017 the toolbox has been delivered to stakeholders of developing countries to monitor water level changes over locations of their interest furthermore web applications exist to provide water level information for preprocessed locations such as the database for hydrological time series of inland waters https dahiti dgfi tum de en lakes rivers and wetlands water levels from satellite altimetry http www legos obs mip fr soa hydrologie hydroweb page 2 html and global reservoirs lakes g realm https ipad fas usda gov cropexplorer global reservoir although these desktop and web applications have significantly helped to provide water level measurements with reduced barriers to entry downloading and processing the altimetry data for multiple sites takes significant time moreover the current web applications are limited to several fixed locations to date no service is available that allows users to query the large global altimetry database dynamically to explore data availability and access water level time series information on the fly here we present a web application named altimetry explorer altex that allows users to dynamically access historical global water level data derived from satellite altimeters without expert knowledge the goal of this web application is to provide a free and open source platform that allows users to access historical and future altimetry data records of river height decreasing the time spent handling and processing data allowing more time for analysis interpretation and impact in this paper we provide a detailed explanation of the altimetry data processing algorithm and web applications used to serve global river height information furthermore we validate the results from the application using observed river height data from in situ gauges building on okeowo et al 2017 discuss implications of such an application and outline future additions to the web application 2 materials and methods 2 1 altimetry data the jason satellite mission series a successor to the topex poseidon mission launched three satellites equipped with altimeters to provide a long time series of surface water level observations the ocean surface topography mission ostm jason 2 satellite mission was launched in 2008 and has a revisit period of 10 days the jason 3 satellite mission was launched in 2016 to further extend the long term time series of surface water level measurements from jason 1 and 2 thus has the same orbit revisit and sensor characteristics as jason 2 the jason 2 3 data are available in 1 hz and 20 hz sampling rate the along track ground distance on the equator between two measurements in the 20 hz data corresponds to about 330 m on the equator and is suitable for inland water applications dumont et al 2017a 2017b hence we used the 20 hz geophysical data record gdr height measurements on this web application to generate a water level time series allowing for the monitoring of water bodies with at least 350 m wide along the satellite ground track on this web application we use the jason altimetry data provided by noaas national centres for environmental information ncei the gdr product is of scientific quality and has been validated for a range of measurements picot et al 2018 however the gdr product has latency of 60 days making it unavailable to near real time applications the interim geophysical data record igdr data product is also available in near real time however with lower accuracy to facilitate quick distribution in this study the ostm jason 2 level 2 gdr data dumont et al 2017a was used for jason 2 due to its data availability and higher quality whereas the jason 3 level 2 igdr product dumont et al 2017b is used initially for jason 3 to provide near real time observations and switched to the gdr product upon availability this allows for delivering near real time product through the developed web applications while keeping scientific quality data the jason 2 3 altimetry data is downloaded from the ncei site compressed and stored on the server running altex 2 2 outlier removal algorithm satellite altimetry data inherently contain errors which are due to contamination from the surrounding topography berry et al 2005 when used to monitor inland water bodies often altimetry data are screened for poor quality measurements using the retracked range quality flags as recommended by birkett and beckley 2010 additional manual removal of these outliers has been implemented in studies birkett and beckley 2010 but this can be challenging and time consuming further studies have used automated approaches to remove outliers from the altimetry data using thresholds huang et al 2013 cauchy and gaussian distributions to represent observations nielsen et al 2015 and kalman filters schwatke et al 2015 these outlier removal algorithms require additional datasets or are computationally expensive making them infeasible for global on the fly processing hence we implemented an outlier detection algorithm that does not require user intervention or ancillary datasets with a short runtime developed by okeowo et al 2017 to generate a filtered water level time series from the altimetry datasets the outlier removal algorithm developed by okeowo et al 2017 is an iterative approach based on a combination of k means unsupervised clustering and statistical analyses of the height measurements to detect outliers without prior knowledge of the water body of interest first the interquartile range iqr is calculated and any data outside of the iqr is thrown out next the iqr filtered data is classified into two classes using the k means algorithm arthur and vassilvitskii 2007 and the cluster with class with the least number of observations is thrown out the k means clustering is repeated until the range of values in the resulting good cluster is within a certain threshold in this case 5 m the mean value of the cluster is computed and data points furthest away from the mean are removed until the threshold of inter class standard deviation is met in this case 0 3 m finally the resulting data are filtered using the new iqr this approach is applied to all of the 20 hz observations from each cycle for the specified distance along the satellites flight path the algorithm was validated on envisat and jason 2 water level retrievals using in situ observations over 37 lakes and reservoirs with rmse values ranging from 0 09 to 1 20 m according to okeowo et al 2017 the algorithm can be extended to process jason 3 satellite data and also has a potential to be used over rivers and wetlands for a more detailed explanation of the algorithm and justification for the thresholds used readers are referred to okeowo et al 2017 the outlier removal algorithm described in okeowo et al 2017 for the river height time series is a vital component of the web application and provides users with accurate information without having to manually filter poor data from the altimetry dataset additionally when a user wants to acquire water level information for a select geographic point location this approach reduces the margin of error in the event a selected point falls outside of a water body it is suggested that at least 50 of the data points used in the outlier removal algorithm be water retrievals not noise or else the algorithm will fail schwatke et al 2015 okeowo et al 2017 the altex web application attempts to mitigate this problem by constraining users to select river segments at least 350 m wide along satellite ground track to ensure at least one altimetry data point falls within the waterbody in question it should be noted that the outlier removal algorithm employed in altex does not verify whether users have selected points absolutely over water employing such a filter based on dynamic user queries is beyond the scope of this study and current application 2 3 web application tethys swain et al 2015 2016 is a free and open source web development framework developed at brigham young university provo utah the unique features that tethys offers helps lower the barrier for developing hydrologic web applications the tethys platform was selected for developing altex given it already comes with several built in software packages that can be used for the application enabling a streamlined development process the tethys platform architecture is separated into three major components tethys software development kit sdk tethys portal and tethys software suite tethys web applications are developed with the python programming language and an sdk the sdk provides python module links to each software component of the tethys platform making the functionality of each component easy to incorporate in web applications in addition users can access all of the python modules that they are accustomed to using in their scientific python scripts to power their web applications the tethys portal is where developed applications can be published it provides an application library page for users to access installed applications and includes several tools and functionalities enabling custom features such as user permissions portal design and portal settings the tethys application structure follows the model view controller mvc software architecture allowing for a simple readable and reusable code the mvc structure has three components 1 model the model is responsible for initializing the database and managing the database structure 2 view the views represent the html pages that are rendered for the user to see 3 controller the controllers handle the logic in the web application and connect the database model to the front end view any data retrieval and presentation is done through the controllers the software suite contains several tools and packages such as geoserver postgresql openlayers etc that are used commonly when developing geospatial web applications readers are directed to the tethys online documentation http docs tethysplatform org en stable for a more in depth description of tethys and web application development using tethys the tethys application folder structure is set up so that it can be easily deployed and replicated one component within the folder the application package contains the source code for executing the tethys application within the application package there are several different files that come together to create the application the application directory structure is also shown in fig 1 the model py file initializes the database and the controllers py file contains the controller functions that are used throughout the application the utilities py file contains common functions that are used throughout the application and the persistentstore py file is the database itself the templates directory contains the html pages that are rendered to the front end the public directory contains resources that are responsible for rendering the html content such as javascript cascading style sheets css and images it also contains any external libraries the original django application structure has several moving parts and it is not straightforward for novice developers however the tethys project application structure has the mvc components in one central location making it easier for first time web developers to leverage the mvc structure the application does not have a database of its own rather it relies on the raw satellite data as the database or the model in the mvc paradigm the altimetry data is set to download automatically using a cron job on download ancillary data fields are removed from the netcdf datasets reducing the storage size of global data the data is stored in a flat file system where datasets from individual satellites i e jason 2 or jason 3 are stored in their own subdirectory the views render information from the controllers to the front end for the users to see the data from the controllers is passed in the form of context variables meaning variables that are created every time the application is initiated thus ensuring that the data returned are dynamic these variables can be rendered directly through the django html template users interact with the web application by selecting a sensor the sensors track path on a web map and dates to process the altex application user interface is designed to be intuitive and extensible the altimetry track paths of each sensor are published as a layer on the local tethys geoserver once they are published they are accessible as a web mapping service wms layer and displayed in the altex web map view to help the users create a cross section over trackpath with water data from the joint research centres global surface water dataset pekel et al 2016 from google earth engine gorelick et al 2017 is displayed on the web map this dataset provides surface water occurrence frequency globally from 1984 to 2015 at 30 m spatial resolution this layer is meant to be a visual aid when users select a track path helping ensure that the majority of the cross section falls within an area with water but as mentioned in the previous section this does not inhibit the user from wrongfully selecting points along tracks that fall over solid terrain any request submitted by the user through the interface is mapped through app py and is ultimately executed through either controllers py ajax controllers py or api py the controller returns data through the html page or as a json object which can be rendered as an html object a representational state transfer rest api is enabled for altex which allows experienced researchers and application developers to use the data in their own applications and scripts a http request can be sent to the rest api with the desired parameters lower latitude upper latitude start date end date track path and the sensor and is returned the time series water level data as a json object 2 4 validation of water level retrievals to ensure high quality data are produced through the altex application the results of the outlier removal algorithm server processing were compared with in situ water level data the in situ data was collected from the geodynamical hydrological and biogeochemical control of erosion alteration and material transport in the amazon orinoco and congo basins so hybam http www ore hybam org observation service which aims to provide the researchers with high quality scientific data used to understand and model earth systems and long term dynamics for the purpose of this validation the in situ station data from the amazon basin were used as they provide a wide range of riverine environments including the amazon main channel tributaries and floodplains each station was manually examined for a corresponding jason satellite ground track to analyse out of the seventeen original in situ stations only thirteen were selected given that availability of data corresponding with satellite overpasses in proximity of the station fig 2 the altex application was used to extract the time series water level data from jason 2 3 data to compare with the in situ data temporal filtering was applied to ensure altimeter and in situ measurements from the same day were compared the instantaneous measurements from the altimeters were considered to be the mean value for the entire day to compare with the daily values from the in situ stations error statistics were calculated for the gauge stations to understand the accuracy of the altimetry data provided through altex the error statistics used include nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 root mean square error rmse and correlation coefficient r 3 results and discussion the source code for altex is publicly available on github at https github com servir altex the application is currently developed to work with tethys 2 0 and later it is licensed under the mit licensing convention thus any individual or organization that wishes to use it can download the source code and deploy it on to their own portals without any restrictions detailed instructions on deploying the application are available at http altex readthedocs io the altex application is currently deployed on the tethys portal at https tethys servirglobal net the apps on the portal are hosted on the applications page on this website a user can access the application by clicking on the altex application icon on the applications homepage 3 1 a case study for using altex this section presents a brief case study describing the use of altex web application for accessing and visualizing water level near the tonle sap lake in cambodia the goal of the case study is to identify trends in water storage changes in tonle sap the case study assumes that the user is using the web application available at https tethys servirglobal net apps altex the acquisition and visualization of the data using altex is highlighted in this case study when the user enters the website there is a global map with a layer showing the geographic boundaries jason 2 3 track path and jrc surface water occurrence the user can zoom and pan to the area of interest once the user is at the area of interest they can select an upper and lower point on the track path with water the jrc surface water layer allows the user to ensure that the cross section that they have selected is within a track path with water the user can specify the date range of the data in this case study the user selects a cross section near tonle sap lake in cambodia approximately 110 km upstream from phnom penh on track path 1 as seen in fig 3 a the user then clicks on the submit button to retrieve the time series data the retrieved data are rendered as a time series through the highcharts javascript charting library as seen in fig 3b the user can hover the time series to see the value at the given point the user can also zoom into a specific data range by clicking and dragging on the chart the user notices that the water level changes follow a seasonal pattern from 2008 to 2016 the water level reaches the peak around october november which are the monsoon months in the summer months from march to may there is a significant decrease in the water levels these values are consistent with historic water levels and seasonality of the tonle sap kite 2001 moreover the lower than normal peak water levels observed by jason 2 in 2015 and 2016 correspond to some of the regions worst droughts guo et al 2017 to export the data the user clicks on the chart context menu icon at the top right corner of the chart the user can then export the data into one of the following formats png jpeg pdf csv svg xls the user can then manipulate or visualize the data in other applications using the api programmers and researchers can use the time series data in their own statistical packages to further analyse the data the api currently consists of one method to retrieve time series the timeseries method in the api requires a lower latitude upper latitude start date end date track path and the sensor the full api documentation can be found with the rest of the documentation of the application the following is an example of a sample rest api call made to the altex application https tethys servirglobal net apps altex api timeseries lat1 12 508647 lat2 12 523467 start date 2008 1 1 end date 2018 12 31 track 001 sensor jason2 https tethys servirglobal net apps altex api timeseries lat1 12 508647 lat2 12 523467 start date 2008 1 1 end date 2018 12 31 track 001 sensor jason2 the above example returns a json object with time series values from the jason 2 satellite for the above use case the time series json is formatted in utc milliseconds and the corresponding value at that time as follows 1485523207706 1 2400541055837568 the readers are referred to appendix a for a full example using python to programmatically request the time series data from both jason 2 and jason 3 for this use case 3 2 validation results error statistics for the generated time series extracted from the altex application were calculated for the thirteen gauge stations across the amazon basin fig 2 table 1 shows the statistics for each station where the reported error statistics are well within acceptable ranges for validation according to moriasi et al 2015 the nse for the water level time series ranges from 0 3 to 0 97 with a mean nse of 0 78 showing that the altimetry data from altex can be used with reasonable accuracy for water level time series furthermore the range of rmse retrieved from altex data is from 0 23 to 3 13 m with a mean of 1 2 m further adding that the water level time series derived from altimetry data can adequately depict in situ water levels the majority of the stations show good agreement with the altimetry water level data time series fig 4 however three stations with marginally satisfactory error statistics show either erroneous values or noise within the time series for example stations fazenda vista alegre and porto velho fig 4e and cont k respectively have high rmse errors where the altimetry data do not fit with the observed data in late 2017 during low flow conditions at the fazenda vista alegre station fig 4e the jason 2 data show much higher values than in situ measurements there is an inverse case for porto velho fig 4 cont k around early mid 2017 when the jason 3 data missed a rise peak and fall in water levels these discrepancies occur when the outlier removal algorithm fails to remove erroneous values cause by excess noise in the retrievals for that day in the case of the francisco de orellana station fig 4f the nse is approximately 0 3 where it can be seen that the altimetry water level retrievals are within about 2 m 0 5 rmse of the observed data but do not align temporally this effect is likely due to francisco de orellanas being a small tributary at the headwaters of the amazon and where a small cross section for the satellite overpass and rapid changes in water level add noise to the altimeter retrievals although the data quality flags and outlier removal algorithm are meant to alleviate such errors in the altimetry water level data these cases present themselves and highlight the need for users to understand and review the systems for which they are applying altimetry data the accuracy of water level data provided through altex is comparable to other studies that have aimed to provide altimetry datasets to users for example bogning et al 2018 used the jason 2 3 ers 2 envisat cryosat 2 saral and sentinel 3a data to provide water level information in the ogoou river basin of gabon where an rmse of 0 219 1 05 m was reported at different in situ stations another application which provided altimetry data across global reservoirs reports an accuracy of a few centimeters to several tens of centimeters birkett et al 2011 while the altex system reports errors as high as 3 14 m the system performs well in most cases and is meant to be used for global applications thus will have some errors at the local scale 3 3 future work due to the modular design of the application the workflow for adding a new altimetry sensors into the system is as simple as creating a new controller or a processing script followed by the creation of the relevant html elements and javascript to interact with that controller this approach keeps the barrier low for developing new functions and algorithms since the existing structure can seamlessly integrate with any new datasets currently altex has jason 2 and jason 3 processing available and it is planned to include additional altimetry datasets i e saral envisat sentinel 3 and potentially the forthcoming surface water ocean topography swot mission in the application many scientists and practitioners representing a broad community have expressed demand for more satellite altimetry data that are timely easy to access and interpret and relevant to a plethora of applications for societal benefit hossain et al 2017 additional studies will be completed to highlight use cases of altex for research and water resources management applications when including additional datasets it is important to validate the results further validation will be performed in data sparse regions such as the mekong basin to further highlight the application of such a tool in such regions application use cases could be conducted using altex api for data assimilation climatology trends estimating water availability flooding and impacts of dams on hydrologic systems e g arias et al 2012 arias et al 2014 kummu and sarkkula 2008 any additional features needed to assist researchers and practitioners in using altimetry data at a global scale will be considered to improve availability and use of altimetry data 4 conclusion this article describes the design development and implementation of a web based application for extracting and visualizing time series data from altimetry datasets this work builds on existing research efforts to provide accurate water level time series from altimetry datasets the valuable contributions of this study are 1 altex provides a modular framework for accessing and visualizing time series values of water level heights from various altimetry datasets 2 the altex web application provides an intuitive user interface and an api access to researchers students and decision makers working with hydrologic data 3 the use of open source technologies for developing the application help promote a potential user and developer community for further improvements and enhancements other studies have provided altimetry data for particular use cases altex provides access to global water level data where water resource managers can monitor water levels at rivers and reservoirs without expensive in situ monitoring stations building on prior validation work this study shows that altex performs well when compared to in situ observations across the amazon basin the results from altex can be used to fill in gaps and monitor water levels in areas with no physical infrastructure this is a novel contribution to the field of hydrology by providing open source resources to access accurate water level retrievals anywhere across the globe without expert knowledge ultimately this service aims to reduce time needed to acquire process and visualize alimtery data thus availing users to spend more time to developing meaningful use cases and applications the altex web app is under continuous development and new features will be added to the source code repository and push to the development server after testing feature requests comments and contributions are welcome at the public software repository software availability name of software altex altimetry explorer developers kel n markert and sarva t pulla contact km0033 uah edu software required linux os python 3 tethys platform 2 0 numpy scipy netcdf4 program language python 3 software availability source code at https github com servir altex documentation detailed documentation for required software and application installation can be found at https altex readthedocs io en latest data required for local installation and use of altex users will need to have the jason 2 and jason 3 data locally available altimetry data can be accessed at ftp ftp nodc noaa gov pub data nodc jason2 or ftp ftp nodc noaa gov pub data nodc jason3 acknowledgement the authors would like to thank the so hybam for making the in situ water level data used in this study available a special thanks is given to all those who have provided comments on the web application and paper especially susantha jayasinghe biplov bhandari chinaporn meechiya ate poortinga senaka basnayake farrukh chishtie and david s saah thanks to the google earth engine team for their support and allowing access and use of the earth engine platform used in this application support for this work was provided through the joint u s agency for international development usaid and national aeronautics and space administration nasa initiative servir particularly through the nasa applied sciences capacity building program nasa cooperative agreement nnm11aa01a and the servir applied sciences team appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 021 appendix b example api request workflow using python here python code is provided to illustrate how the altex api can be used programatically the requests highlighted are for the jason 2 and jason 3 time series data for the tonle sap lake case the code is provided as in a jupyter notebook using ipython syntax users may need to modify the code slightly to create functional python scripts image 2 fig a1 plot of example water level data as requested from the altex web application for the tonle sap case study fig a1 
26218,understanding the spatial and temporal distribution of hydrologic variables such as streamflow is important for sustainable development especially with global population growth and climate variations typical monitoring of streamflow is conducted using in situ gauging stations however stations are costly to setup and maintain leading to data gaps in regions that cannot afford gauges satellite data including altimetry data are used to supplement in situ observations and in some cases supply information where they are lacking this study introduces an open source web application to access and explore altimetry datasets for use in water level monitoring named the altimetry explorer altex this web application along with its relevant rest api facilitates access to altimetry data for analysis visualization and impact the data provided through altex is validated using thirteen gauges in the amazon basin from 2008 to 2018 with an average nash sutcliffe coefficient and root mean square error of 0 78 and 1 2 m respectively access to global water level data should be particularly helpful for water resource practitioners and researchers seeking to understand the long term trends and dynamics of global water level and availability this work provides an initial framework for a more robust and comprehensive platform to access future altimetry datasets and support research related to global water resources keywords satellite altimetry tethys platform online web application water level global applications 1 introduction global population growth human activities climate change and uneven distribution of water supplies whether being water excess or scarcity are leading to future water insecurity vörösmarty et al 2010 tao et al 2015 understanding the spatial and temporal distribution of water resources is critical to support sustainable development policies and activities addressing water security vörösmarty et al 2000 streamflow and river heights in developed nations are actively being monitored since the late nineteenth century for better management and allocation of water resources barrow 1998 while these important hydrologic variables are being monitored in developed countries in situ streamflow monitoring is largely lacking in developing regions predominantly due to the impractical cost of installation and maintenance of these instruments alsdorf et al 2001 streamflow and river height information collected by means of in situ monitoring can be used to study and understand the impact of climate change on water resources and help understand trends of natural hazards e g flood and drought conditions haritashya et al 2006 huntington 2006 chen et al 2014 given the utility of such data for understanding the hydrologic cycle these data gaps provide opportunities for innovative solutions to deliver data in data sparse regions particularly for transboundary river basins lakshmi et al 2018 satellite based remote sensing and modeling techniques have been used to fill data gaps and provide vital information for international and transboundary water resources monitoring and management e g mohammed et al 2018a mohammed et al 2018b the results of many studies support the use of remotely sensed data for a wide range of water resources applications across the globe for example maswood and hossain 2016 used satellite data including altimetry data as inputs into a hydrologic model in the ganges brahmaputra and meghna gbm river basins providing improved river monitoring for flood forecasting applications across the three basins additionally chang et al 2019 used the variable infiltration capacity vic model with altimetry data to develop a forecasting model in the mekong basin resulting in an improved five day forecast along the mainstem river a similar approach to flood forecasting using altimetry data was implemented by hossain and bhuiyan hossain et al 2016 for the gbm basins increasing the flood forecast time to five days moreover bonnema and hossain 2017 demonstrated the use of only satellite remote sensing datasets altimetry and landsat data to infer streamflow response to reservoir operations in the mekong basin other studies have focused solely on using altimetry datasets for monitoring purposes along the in the yangtze river basin chu et al 2008 the zambezi river basin michailovsky et al 2012 the congo river basin kim et al 2019 lakes in indonesia sulistioadi et al 2015 and more broadly across north america africa and southeast asia ricko et al 2012 these studies highlight how a growing number of users are interested in applying remotely sensed water level estimates and more specifically satellite altimetry to a wide variety of studies and water resources management challenges related to the hydrologic cycle and climate change with the advent of satellite and radar altimetry data the capability to monitor sea river and lake level changes with acceptable accuracy has significantly improved since the early 1990s satellite radar altimetry sends radar pulses to the earth and measures the return time from the reflecting surface to calculate the water level relative to a reference datum along with the satellites altitude the first major oceanographic research altimeter topex poseidon fu et al 1994 collected the data from 1992 to 2005 paving the way for subsequent satellites such as the jason series jason 1 2 and 3 provided data from 2002 to present creating a long series of available water level measurements additional sensors such as envisat operated from 2002 to 2010 saral altika operated from 2013 to 2016 in exact repeat orbit and sentinel 3 currently operating since 2016 have been launched to supplement and continue the water level measurements from the jason series although altimeters were designed primarily for ocean and ice studies they have been applied to the monitoring of inland water bodies lee et al 2009 in particular the ability to remotely detect water surface level changes in lakes and inland seas has been demonstrated studies have shown that results derived from altimetry data demonstrate how sub monthly seasonal and inter annual variations in height can be monitored and provide invaluable information in data sparse regions bonnema et al 2016 despite the use of altimetry data in scientific studies gao et al 2012 concluded that access to water level data has been a major challenge in the global study of reservoirs applications exist to lower the barrier to access and use water level information from altimetry sensors for example a gui based jason 2 3 and sentinel 3a data processing toolbox to generate time series of water level changes over user defined inland water bodies has been developed based on the automation algorithm by okeowo et al 2017 the toolbox has been delivered to stakeholders of developing countries to monitor water level changes over locations of their interest furthermore web applications exist to provide water level information for preprocessed locations such as the database for hydrological time series of inland waters https dahiti dgfi tum de en lakes rivers and wetlands water levels from satellite altimetry http www legos obs mip fr soa hydrologie hydroweb page 2 html and global reservoirs lakes g realm https ipad fas usda gov cropexplorer global reservoir although these desktop and web applications have significantly helped to provide water level measurements with reduced barriers to entry downloading and processing the altimetry data for multiple sites takes significant time moreover the current web applications are limited to several fixed locations to date no service is available that allows users to query the large global altimetry database dynamically to explore data availability and access water level time series information on the fly here we present a web application named altimetry explorer altex that allows users to dynamically access historical global water level data derived from satellite altimeters without expert knowledge the goal of this web application is to provide a free and open source platform that allows users to access historical and future altimetry data records of river height decreasing the time spent handling and processing data allowing more time for analysis interpretation and impact in this paper we provide a detailed explanation of the altimetry data processing algorithm and web applications used to serve global river height information furthermore we validate the results from the application using observed river height data from in situ gauges building on okeowo et al 2017 discuss implications of such an application and outline future additions to the web application 2 materials and methods 2 1 altimetry data the jason satellite mission series a successor to the topex poseidon mission launched three satellites equipped with altimeters to provide a long time series of surface water level observations the ocean surface topography mission ostm jason 2 satellite mission was launched in 2008 and has a revisit period of 10 days the jason 3 satellite mission was launched in 2016 to further extend the long term time series of surface water level measurements from jason 1 and 2 thus has the same orbit revisit and sensor characteristics as jason 2 the jason 2 3 data are available in 1 hz and 20 hz sampling rate the along track ground distance on the equator between two measurements in the 20 hz data corresponds to about 330 m on the equator and is suitable for inland water applications dumont et al 2017a 2017b hence we used the 20 hz geophysical data record gdr height measurements on this web application to generate a water level time series allowing for the monitoring of water bodies with at least 350 m wide along the satellite ground track on this web application we use the jason altimetry data provided by noaas national centres for environmental information ncei the gdr product is of scientific quality and has been validated for a range of measurements picot et al 2018 however the gdr product has latency of 60 days making it unavailable to near real time applications the interim geophysical data record igdr data product is also available in near real time however with lower accuracy to facilitate quick distribution in this study the ostm jason 2 level 2 gdr data dumont et al 2017a was used for jason 2 due to its data availability and higher quality whereas the jason 3 level 2 igdr product dumont et al 2017b is used initially for jason 3 to provide near real time observations and switched to the gdr product upon availability this allows for delivering near real time product through the developed web applications while keeping scientific quality data the jason 2 3 altimetry data is downloaded from the ncei site compressed and stored on the server running altex 2 2 outlier removal algorithm satellite altimetry data inherently contain errors which are due to contamination from the surrounding topography berry et al 2005 when used to monitor inland water bodies often altimetry data are screened for poor quality measurements using the retracked range quality flags as recommended by birkett and beckley 2010 additional manual removal of these outliers has been implemented in studies birkett and beckley 2010 but this can be challenging and time consuming further studies have used automated approaches to remove outliers from the altimetry data using thresholds huang et al 2013 cauchy and gaussian distributions to represent observations nielsen et al 2015 and kalman filters schwatke et al 2015 these outlier removal algorithms require additional datasets or are computationally expensive making them infeasible for global on the fly processing hence we implemented an outlier detection algorithm that does not require user intervention or ancillary datasets with a short runtime developed by okeowo et al 2017 to generate a filtered water level time series from the altimetry datasets the outlier removal algorithm developed by okeowo et al 2017 is an iterative approach based on a combination of k means unsupervised clustering and statistical analyses of the height measurements to detect outliers without prior knowledge of the water body of interest first the interquartile range iqr is calculated and any data outside of the iqr is thrown out next the iqr filtered data is classified into two classes using the k means algorithm arthur and vassilvitskii 2007 and the cluster with class with the least number of observations is thrown out the k means clustering is repeated until the range of values in the resulting good cluster is within a certain threshold in this case 5 m the mean value of the cluster is computed and data points furthest away from the mean are removed until the threshold of inter class standard deviation is met in this case 0 3 m finally the resulting data are filtered using the new iqr this approach is applied to all of the 20 hz observations from each cycle for the specified distance along the satellites flight path the algorithm was validated on envisat and jason 2 water level retrievals using in situ observations over 37 lakes and reservoirs with rmse values ranging from 0 09 to 1 20 m according to okeowo et al 2017 the algorithm can be extended to process jason 3 satellite data and also has a potential to be used over rivers and wetlands for a more detailed explanation of the algorithm and justification for the thresholds used readers are referred to okeowo et al 2017 the outlier removal algorithm described in okeowo et al 2017 for the river height time series is a vital component of the web application and provides users with accurate information without having to manually filter poor data from the altimetry dataset additionally when a user wants to acquire water level information for a select geographic point location this approach reduces the margin of error in the event a selected point falls outside of a water body it is suggested that at least 50 of the data points used in the outlier removal algorithm be water retrievals not noise or else the algorithm will fail schwatke et al 2015 okeowo et al 2017 the altex web application attempts to mitigate this problem by constraining users to select river segments at least 350 m wide along satellite ground track to ensure at least one altimetry data point falls within the waterbody in question it should be noted that the outlier removal algorithm employed in altex does not verify whether users have selected points absolutely over water employing such a filter based on dynamic user queries is beyond the scope of this study and current application 2 3 web application tethys swain et al 2015 2016 is a free and open source web development framework developed at brigham young university provo utah the unique features that tethys offers helps lower the barrier for developing hydrologic web applications the tethys platform was selected for developing altex given it already comes with several built in software packages that can be used for the application enabling a streamlined development process the tethys platform architecture is separated into three major components tethys software development kit sdk tethys portal and tethys software suite tethys web applications are developed with the python programming language and an sdk the sdk provides python module links to each software component of the tethys platform making the functionality of each component easy to incorporate in web applications in addition users can access all of the python modules that they are accustomed to using in their scientific python scripts to power their web applications the tethys portal is where developed applications can be published it provides an application library page for users to access installed applications and includes several tools and functionalities enabling custom features such as user permissions portal design and portal settings the tethys application structure follows the model view controller mvc software architecture allowing for a simple readable and reusable code the mvc structure has three components 1 model the model is responsible for initializing the database and managing the database structure 2 view the views represent the html pages that are rendered for the user to see 3 controller the controllers handle the logic in the web application and connect the database model to the front end view any data retrieval and presentation is done through the controllers the software suite contains several tools and packages such as geoserver postgresql openlayers etc that are used commonly when developing geospatial web applications readers are directed to the tethys online documentation http docs tethysplatform org en stable for a more in depth description of tethys and web application development using tethys the tethys application folder structure is set up so that it can be easily deployed and replicated one component within the folder the application package contains the source code for executing the tethys application within the application package there are several different files that come together to create the application the application directory structure is also shown in fig 1 the model py file initializes the database and the controllers py file contains the controller functions that are used throughout the application the utilities py file contains common functions that are used throughout the application and the persistentstore py file is the database itself the templates directory contains the html pages that are rendered to the front end the public directory contains resources that are responsible for rendering the html content such as javascript cascading style sheets css and images it also contains any external libraries the original django application structure has several moving parts and it is not straightforward for novice developers however the tethys project application structure has the mvc components in one central location making it easier for first time web developers to leverage the mvc structure the application does not have a database of its own rather it relies on the raw satellite data as the database or the model in the mvc paradigm the altimetry data is set to download automatically using a cron job on download ancillary data fields are removed from the netcdf datasets reducing the storage size of global data the data is stored in a flat file system where datasets from individual satellites i e jason 2 or jason 3 are stored in their own subdirectory the views render information from the controllers to the front end for the users to see the data from the controllers is passed in the form of context variables meaning variables that are created every time the application is initiated thus ensuring that the data returned are dynamic these variables can be rendered directly through the django html template users interact with the web application by selecting a sensor the sensors track path on a web map and dates to process the altex application user interface is designed to be intuitive and extensible the altimetry track paths of each sensor are published as a layer on the local tethys geoserver once they are published they are accessible as a web mapping service wms layer and displayed in the altex web map view to help the users create a cross section over trackpath with water data from the joint research centres global surface water dataset pekel et al 2016 from google earth engine gorelick et al 2017 is displayed on the web map this dataset provides surface water occurrence frequency globally from 1984 to 2015 at 30 m spatial resolution this layer is meant to be a visual aid when users select a track path helping ensure that the majority of the cross section falls within an area with water but as mentioned in the previous section this does not inhibit the user from wrongfully selecting points along tracks that fall over solid terrain any request submitted by the user through the interface is mapped through app py and is ultimately executed through either controllers py ajax controllers py or api py the controller returns data through the html page or as a json object which can be rendered as an html object a representational state transfer rest api is enabled for altex which allows experienced researchers and application developers to use the data in their own applications and scripts a http request can be sent to the rest api with the desired parameters lower latitude upper latitude start date end date track path and the sensor and is returned the time series water level data as a json object 2 4 validation of water level retrievals to ensure high quality data are produced through the altex application the results of the outlier removal algorithm server processing were compared with in situ water level data the in situ data was collected from the geodynamical hydrological and biogeochemical control of erosion alteration and material transport in the amazon orinoco and congo basins so hybam http www ore hybam org observation service which aims to provide the researchers with high quality scientific data used to understand and model earth systems and long term dynamics for the purpose of this validation the in situ station data from the amazon basin were used as they provide a wide range of riverine environments including the amazon main channel tributaries and floodplains each station was manually examined for a corresponding jason satellite ground track to analyse out of the seventeen original in situ stations only thirteen were selected given that availability of data corresponding with satellite overpasses in proximity of the station fig 2 the altex application was used to extract the time series water level data from jason 2 3 data to compare with the in situ data temporal filtering was applied to ensure altimeter and in situ measurements from the same day were compared the instantaneous measurements from the altimeters were considered to be the mean value for the entire day to compare with the daily values from the in situ stations error statistics were calculated for the gauge stations to understand the accuracy of the altimetry data provided through altex the error statistics used include nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 root mean square error rmse and correlation coefficient r 3 results and discussion the source code for altex is publicly available on github at https github com servir altex the application is currently developed to work with tethys 2 0 and later it is licensed under the mit licensing convention thus any individual or organization that wishes to use it can download the source code and deploy it on to their own portals without any restrictions detailed instructions on deploying the application are available at http altex readthedocs io the altex application is currently deployed on the tethys portal at https tethys servirglobal net the apps on the portal are hosted on the applications page on this website a user can access the application by clicking on the altex application icon on the applications homepage 3 1 a case study for using altex this section presents a brief case study describing the use of altex web application for accessing and visualizing water level near the tonle sap lake in cambodia the goal of the case study is to identify trends in water storage changes in tonle sap the case study assumes that the user is using the web application available at https tethys servirglobal net apps altex the acquisition and visualization of the data using altex is highlighted in this case study when the user enters the website there is a global map with a layer showing the geographic boundaries jason 2 3 track path and jrc surface water occurrence the user can zoom and pan to the area of interest once the user is at the area of interest they can select an upper and lower point on the track path with water the jrc surface water layer allows the user to ensure that the cross section that they have selected is within a track path with water the user can specify the date range of the data in this case study the user selects a cross section near tonle sap lake in cambodia approximately 110 km upstream from phnom penh on track path 1 as seen in fig 3 a the user then clicks on the submit button to retrieve the time series data the retrieved data are rendered as a time series through the highcharts javascript charting library as seen in fig 3b the user can hover the time series to see the value at the given point the user can also zoom into a specific data range by clicking and dragging on the chart the user notices that the water level changes follow a seasonal pattern from 2008 to 2016 the water level reaches the peak around october november which are the monsoon months in the summer months from march to may there is a significant decrease in the water levels these values are consistent with historic water levels and seasonality of the tonle sap kite 2001 moreover the lower than normal peak water levels observed by jason 2 in 2015 and 2016 correspond to some of the regions worst droughts guo et al 2017 to export the data the user clicks on the chart context menu icon at the top right corner of the chart the user can then export the data into one of the following formats png jpeg pdf csv svg xls the user can then manipulate or visualize the data in other applications using the api programmers and researchers can use the time series data in their own statistical packages to further analyse the data the api currently consists of one method to retrieve time series the timeseries method in the api requires a lower latitude upper latitude start date end date track path and the sensor the full api documentation can be found with the rest of the documentation of the application the following is an example of a sample rest api call made to the altex application https tethys servirglobal net apps altex api timeseries lat1 12 508647 lat2 12 523467 start date 2008 1 1 end date 2018 12 31 track 001 sensor jason2 https tethys servirglobal net apps altex api timeseries lat1 12 508647 lat2 12 523467 start date 2008 1 1 end date 2018 12 31 track 001 sensor jason2 the above example returns a json object with time series values from the jason 2 satellite for the above use case the time series json is formatted in utc milliseconds and the corresponding value at that time as follows 1485523207706 1 2400541055837568 the readers are referred to appendix a for a full example using python to programmatically request the time series data from both jason 2 and jason 3 for this use case 3 2 validation results error statistics for the generated time series extracted from the altex application were calculated for the thirteen gauge stations across the amazon basin fig 2 table 1 shows the statistics for each station where the reported error statistics are well within acceptable ranges for validation according to moriasi et al 2015 the nse for the water level time series ranges from 0 3 to 0 97 with a mean nse of 0 78 showing that the altimetry data from altex can be used with reasonable accuracy for water level time series furthermore the range of rmse retrieved from altex data is from 0 23 to 3 13 m with a mean of 1 2 m further adding that the water level time series derived from altimetry data can adequately depict in situ water levels the majority of the stations show good agreement with the altimetry water level data time series fig 4 however three stations with marginally satisfactory error statistics show either erroneous values or noise within the time series for example stations fazenda vista alegre and porto velho fig 4e and cont k respectively have high rmse errors where the altimetry data do not fit with the observed data in late 2017 during low flow conditions at the fazenda vista alegre station fig 4e the jason 2 data show much higher values than in situ measurements there is an inverse case for porto velho fig 4 cont k around early mid 2017 when the jason 3 data missed a rise peak and fall in water levels these discrepancies occur when the outlier removal algorithm fails to remove erroneous values cause by excess noise in the retrievals for that day in the case of the francisco de orellana station fig 4f the nse is approximately 0 3 where it can be seen that the altimetry water level retrievals are within about 2 m 0 5 rmse of the observed data but do not align temporally this effect is likely due to francisco de orellanas being a small tributary at the headwaters of the amazon and where a small cross section for the satellite overpass and rapid changes in water level add noise to the altimeter retrievals although the data quality flags and outlier removal algorithm are meant to alleviate such errors in the altimetry water level data these cases present themselves and highlight the need for users to understand and review the systems for which they are applying altimetry data the accuracy of water level data provided through altex is comparable to other studies that have aimed to provide altimetry datasets to users for example bogning et al 2018 used the jason 2 3 ers 2 envisat cryosat 2 saral and sentinel 3a data to provide water level information in the ogoou river basin of gabon where an rmse of 0 219 1 05 m was reported at different in situ stations another application which provided altimetry data across global reservoirs reports an accuracy of a few centimeters to several tens of centimeters birkett et al 2011 while the altex system reports errors as high as 3 14 m the system performs well in most cases and is meant to be used for global applications thus will have some errors at the local scale 3 3 future work due to the modular design of the application the workflow for adding a new altimetry sensors into the system is as simple as creating a new controller or a processing script followed by the creation of the relevant html elements and javascript to interact with that controller this approach keeps the barrier low for developing new functions and algorithms since the existing structure can seamlessly integrate with any new datasets currently altex has jason 2 and jason 3 processing available and it is planned to include additional altimetry datasets i e saral envisat sentinel 3 and potentially the forthcoming surface water ocean topography swot mission in the application many scientists and practitioners representing a broad community have expressed demand for more satellite altimetry data that are timely easy to access and interpret and relevant to a plethora of applications for societal benefit hossain et al 2017 additional studies will be completed to highlight use cases of altex for research and water resources management applications when including additional datasets it is important to validate the results further validation will be performed in data sparse regions such as the mekong basin to further highlight the application of such a tool in such regions application use cases could be conducted using altex api for data assimilation climatology trends estimating water availability flooding and impacts of dams on hydrologic systems e g arias et al 2012 arias et al 2014 kummu and sarkkula 2008 any additional features needed to assist researchers and practitioners in using altimetry data at a global scale will be considered to improve availability and use of altimetry data 4 conclusion this article describes the design development and implementation of a web based application for extracting and visualizing time series data from altimetry datasets this work builds on existing research efforts to provide accurate water level time series from altimetry datasets the valuable contributions of this study are 1 altex provides a modular framework for accessing and visualizing time series values of water level heights from various altimetry datasets 2 the altex web application provides an intuitive user interface and an api access to researchers students and decision makers working with hydrologic data 3 the use of open source technologies for developing the application help promote a potential user and developer community for further improvements and enhancements other studies have provided altimetry data for particular use cases altex provides access to global water level data where water resource managers can monitor water levels at rivers and reservoirs without expensive in situ monitoring stations building on prior validation work this study shows that altex performs well when compared to in situ observations across the amazon basin the results from altex can be used to fill in gaps and monitor water levels in areas with no physical infrastructure this is a novel contribution to the field of hydrology by providing open source resources to access accurate water level retrievals anywhere across the globe without expert knowledge ultimately this service aims to reduce time needed to acquire process and visualize alimtery data thus availing users to spend more time to developing meaningful use cases and applications the altex web app is under continuous development and new features will be added to the source code repository and push to the development server after testing feature requests comments and contributions are welcome at the public software repository software availability name of software altex altimetry explorer developers kel n markert and sarva t pulla contact km0033 uah edu software required linux os python 3 tethys platform 2 0 numpy scipy netcdf4 program language python 3 software availability source code at https github com servir altex documentation detailed documentation for required software and application installation can be found at https altex readthedocs io en latest data required for local installation and use of altex users will need to have the jason 2 and jason 3 data locally available altimetry data can be accessed at ftp ftp nodc noaa gov pub data nodc jason2 or ftp ftp nodc noaa gov pub data nodc jason3 acknowledgement the authors would like to thank the so hybam for making the in situ water level data used in this study available a special thanks is given to all those who have provided comments on the web application and paper especially susantha jayasinghe biplov bhandari chinaporn meechiya ate poortinga senaka basnayake farrukh chishtie and david s saah thanks to the google earth engine team for their support and allowing access and use of the earth engine platform used in this application support for this work was provided through the joint u s agency for international development usaid and national aeronautics and space administration nasa initiative servir particularly through the nasa applied sciences capacity building program nasa cooperative agreement nnm11aa01a and the servir applied sciences team appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 021 appendix b example api request workflow using python here python code is provided to illustrate how the altex api can be used programatically the requests highlighted are for the jason 2 and jason 3 time series data for the tonle sap lake case the code is provided as in a jupyter notebook using ipython syntax users may need to modify the code slightly to create functional python scripts image 2 fig a1 plot of example water level data as requested from the altex web application for the tonle sap case study fig a1 
26219,because effects of climate change and an increase in elements at risk mountain hazard loss increased throughout europe yet factors influencing loss i e vulnerability have gained less attention to date vulnerability is defined as the degree of loss resulting from the hazard impact on buildings recent studies have focused on evaluating vulnerability to dynamic flooding using proxies from case studies and based on empirical ex post approaches however the transferability to other case studies and therefore the ability of such models to actually predict future losses is limited to overcome this gap we present a beta model based on loss data from the european alps which clearly shows that a single vulnerability function is sufficient to predict losses resulting from different types of torrential hazards and to provide probabilities of destruction under specific scenarios as a result the curves are transferable and may significantly increase the predictive power of risk analyses keywords vulnerability elements at risk torrential hazards loss zero and one inflated beta regression austria 1 introduction throughout europe increasing losses due to flood hazards have been reported kreibich et al 2014 especially for inundation along large rivers barredo 2007 di baldassarre et al 2018 in contrast there is a lack of studies on the effects of mountain river flooding papathoma köhle 2016 papathoma köhle et al 2017 zhang et al 2018 increases in overall losses are mainly attributed to the effects of climate change mountain research initiative edw working group 2015 which affect the magnitude and frequency of events and to population dynamics in mountain areas fuchs et al 2015 2017 however the vulnerability of communities experiencing the impact of such hazards is less well known jakob et al 2012 zimmermann and keiler 2015 which is also articulated by international frameworks for disaster risk reduction such as the sendai framework un isdr 2015 klein et al 2019 calling for the need for improved understanding of disaster risk in all its dimensions of exposure vulnerability and hazard characteristics mountain rivers are characterized by dynamic flooding with variable amounts of sediment erosion deposition and remobilisation sturm et al 2018 typical hazard processes include fluvial sediment transport debris flows and related phenomena slaymaker 2010 mazzorana et al 2014 karagiorgos et al 2016 milanesi et al 2018 in europe such processes repeatedly result in considerable damage to infrastructure and buildings on a local and regional level guzzetti et al 2005 hilker et al 2009 fuchs et al 2015 zhang et al 2018 zischg et al 2018 zou et al 2018 schlögl et al 2019 several studies used empirical data from past events to assess damage monetarily by using vulnerability functions fuchs et al 2007 akbas et al 2009 quan luna et al 2011 papathoma köhle et al 2012 sterlacchini et al 2013 a recent overview is provided in papathoma köhle et al 2017 vulnerability functions are based on the empirical assessment of observed damage and describe underlying process magnitudes and related loss patterns consequently vulnerability ranges from 0 no damage to 1 complete destruction the use of such vulnerability functions is common in the case of hazards e g river flooding and earthquakes that affect larger areas and a considerable amount of elements at risk nevertheless despite numerous attempts and models to empirically describe the vulnerability of static flooding along european rivers kreibich et al 2015 such methods and tools are hardly available for assessing the effects of dynamic flooding in mountain watersheds moreover available data about the empirical assessment of affected buildings located on torrential fans are limited papathoma köhle et al 2011 2017 and were often not found to be transferable to other case studies cammerer et al 2013 on the contrary there is an emphasis on hazard assessment in specific catchments the magnitude of these hazard types pose case specific threats to elements at risk depending on the geology catchment and channel morphology as well as meteorological triggers and interactions among these factors during an individual event recent attempts to derive vulnerability functions using loss data included curve fitting to a series of data that was recorded after major incidents with the aim to predict the probability of occurrence of a certain degree of loss with respect to different flood levels papathoma köhle et al 2012 totschnig and fuchs 2013 carisi et al 2018 to determine the flood level deposition height was repeatedly used as a proxy since data on flow velocities were not available jakob et al 2012 papathoma köhle et al 2015 chow et al 2018 the function with the best fit should minimize the squared differences in data which is consistent with the classical approach of curve fitting this was repeatedly computed with a weibull distribution function totschnig and fuchs 2013 papathoma köhle et al 2015 as a result the overall relationship between hazard magnitude and the observed degree of loss can be mirrored uncertainties can be expressed by confidence intervals which depend on the distribution of errors these uncertainties are of aleatory type and are based on the assumption of symmetrically distributed errors around the mean degree of loss which is rarely observed in reality the data spread of weibull functions results in theoretical loss values below zero and above one which is inconsistent with the definition of vulnerability moreover the observed loss pattern totschnig and fuchs 2013 papathoma köhle et al 2015 is characterized by more data with small values lower degree of loss than with high values larger degree of loss until complete destruction and the data showed a right skewed distribution the larger loss values and therefore the larger degree of loss tended to be farther away from the mean degree of loss than the smaller values hence a suitable and stochastically valid probability model must be able to model the skewness in the degree of loss this model requires a parametric assumption and the selection of a suitable probability distribution which enables the statistical treatment of uncertainties a lack of predictive power of the degree of loss for future events is evident since the current approaches were based on spurious error assumptions hastie and tibshirani 1990 fox 2016 to overcome this gap we compiled an integrative dataset that was used to evaluate a model and which is able to better explain the observed parameter values and the associated skewness the dataset comprises available data from individual studies conducted in the european alps i e incidents from austria and italy fuchs et al 2007 fuchs 2008 totschnig et al 2011 papathoma köhle et al 2012 totschnig and fuchs 2013 papathoma köhle et al 2015 2 methods we chose a beta distribution as the underlying model for assessing the vulnerability of buildings the proposed model assumes that the variable of interest degree of loss is continuous restricted to the interval between zero and one and is related to the process magnitude through a regression structure ferrari and cribari neto 2004 the final dataset consists of 214 data points that link observed process magnitudes and the degree of loss for individual buildings damaged during torrent hazards in the european alps additional information about the process subtype e g fluvial sediment transport hyperconcentrated flow debris flow and the type of building was collected for 60 of the 214 cases investigated damages occurred due to debris flow hazards while the remaining 154 damages were caused by fluvial sediment transport and hyperconcentrated flows the observations were equally distributed over austria and italy with 107 cases from each country only 10 samples represented commercial buildings and the remaining 204 samples represented residential buildings the degree of loss was treated as a random variable with values between the interval zero no loss and one complete destruction hence including the endpoints 0 1 therefore we modelled the influence of the process magnitude ω on the degree of loss with a zero and one inflated beta distribution which is a mixture of bernoulli and beta distributions the resultant density function is given as 1 f ω μ σ ν τ π 0 i f ω 0 1 π 0 π 1 1 b α β ω α 1 1 ω β 1 i f 0 ω 1 π 1 i f ω 1 where μ α α β σ α β 1 1 2 ν π 0 π 1 τ π 0 1 π 0 π 1 and b α β is the beta function the expected value of the distribution is given by e ω τ μ 1 ν τ see rigby and stasinopoulos 2005 and stasinopoulos and rigby 2007 to model the dependency of the distribution parameters on the process magnitude we used a logit link for μ a log link for ν and τ while σ was assumed to be constant the model parameters were estimated by maximizing the log likelihood using the gamlss package stasinopoulos and rigby 2007 for the statistical software r r core team 2018 to decide if process magnitude process type as well as geographical location and building types influence the behavior of the degree of loss we applied two procedures 1 a stepwise model selection procedure based on the generalized akaike information criterion akaike 1974 as the selection criterion and 2 bootstrapping i e resampling with replacement for the first predictive variables for the regression model were selected by applying bidirectional elimination i e a combination of forward selection and backward elimination in an iterative process until the optimal model i e the model with minimal information loss was found for the latter eqn 1 was estimated on 5000 bootstrap samples from the original dataset for each bootstrap sample the p value of the explanatory variable was estimated the number of times variable was significant hence the p value was below 0 05 and was used as a measure of parameter importance gilenko and mironova 2017 uncertainty for the fitted zero and one inflated beta distribution was assessed by means of non parametric bootstrapping by applying the same analysis steps used for obtaining the final vulnerability function to 5000 new samples generated from the original dataset from these bootstrap samples confidence intervals for the regression model were calculated 3 results bootstrap based variable selection revealed that there was no need to distinguish between observations regarding geographical location and building type as the p values were below 0 05 only in 17 and 13 of all models fig 1 upper left and right in contrast the process type was a significant predictor in 58 and the process magnitude was significant in 100 of all models fig 1 lower left and right the clear rejection of the geographical location and the building type as potential predictors the indifferent importance of process type as predictor and the clear significance of process magnitude supports the hypothesis that a single vulnerability function depending on the process magnitude is sufficient to model the degree of loss this is also mirrored by the large overlap of the boxplots given in the inlet of fig 2 these findings are consistent with the stepwise model selection procedure which resulted in a model featuring process magnitude as the most relevant predictor the final model for the mean degree of loss l d as a function of process magnitude is expressed as 2 l d ω 0 if ω 0 e 7 40 2 56 ω e 3 27 1 67 ω 1 e 3 27 1 67 ω 1 e 9 49 ω e 7 40 2 56 ω if ω 0 the results are presented in fig 2 in the main panel the process magnitude is shown on the x axis and the degree of loss on the y axis boxplots were used to indicate the spread in the degree of loss for different process magnitudes observed the data were summarized into classes of different widths of process magnitudes to ensure a minimum number of five data points for boxplot computation secondary x axis both the median degree of loss and the spread increase with increasing process magnitude as a result three functional relationships can be established 1 the dashed line represents the probability of an undamaged building degree of loss 0 and rapidly decreases with increasing process magnitude and 2 the dotted line represents the probability of a completely damaged building this probability remains below 0 1 until a process magnitude of around 2 m and increases significantly thereafter this is clearly more advantageous when compared to an empirical assessment of vulnerability based on curve fitting finally the black line 3 shows the evolution of the degree of loss for buildings that are neither fully intact nor totally damaged the mean degree of loss as a result of the respective process magnitudes is obtained by combining these three components into one mixture model eqn 2 in fig 3 the expected value of the modelled distribution c f eqn 2 is shown for the entire data range including no loss and complete destruction this model includes the process magnitude as the explaining variable and specifically supports the computation of mean losses for buildings affected by torrent events by accounting for data skewness consequently the model shows that once a building is only marginally affected by dynamic flooding a small degree of loss occurs therefore the model is an extension of an empirical approach that relies on a normal distribution of the degree of loss and allows for an ex ante assessment of damage once potential process magnitudes and the values of elements at risk are known e g from modelling exercises uncertainty of the fitted zero and one inflated beta distribution is assessed by means of non parametric bootstrapping i e resampling with replacement by applying the same analysis steps used for obtaining the final vulnerability function to 5000 new samples generated from the original data set bootstrap confidence intervals can be calculated for the regression model apart from the uncertainty assessment based on bootstrapping model quality is assessed by comparing modelled versus observed values of degree of loss fig 4 as readily seen no significant bias towards over or underestimation is observed as the 95 confidence area of the regression blue shaded area includes the 1 1 line grey dashed line hence neither the intercept is significantly different from 0 nor the slope is significantly different from 1 even if considerable variability remains this is also reflected in corresponding error statistics concerning mean squared deviation and its components kobayashi and salam 2000 gauch et al 2003 based on 5 fold cross validation translation squared bias 0 0004 rotation non unity slope 0 0011 and scatter lack of correlation 0 0106 between model based predictions and observations result in a rmse of 0 1095 and a mae of 0 0737 4 discussion and conclusion we extended available studies on the physical vulnerability of buildings to dynamic flooding due to torrential processes to achieve a robust prediction of losses an improved vulnerability function is presented derived from a considerable volume of empirical data in comparison to available studies it is capable of predicting torrential hazard losses in different countries and for various building types while earlier attempts were only focused on representing the mean degree of loss for different process magnitudes observed our approach allows risk managers to model future losses as a function of possible impacts due to the deposition of material and water on the building envelope additionally the model considers the probability of no loss and the complete destruction of affected buildings due to its predictive power the approach may be applied to operational risk management and also in areas were empirical data is currently not available the latter is of particular importance due to climate and socioeconomic changes the impacts of mountain floods are expected in areas with limited to no records of such previous events knowledge of probabilities supports decision making and the prioritization of resources for the construction and implementation of protection measures thus the inclusion of such predictions into future planning processes will further enhance the reliability of vulnerability assessments future research may focus on 1 increasing the overall amount of data that can improve and update vulnerability functions and 2 considering other explaining variables besides deposition height e g flow velocity author contributions s f initiated this work and contributed with loss data from the austrian alps m h and m s performed the statistical analysis and programmed the loss model with support from s f and a z and m k m p k and a z contributed with loss data from the italian alps all authors were jointly involved with writing the manuscript raw data may be obtained from the corresponding author s f the model is available as electronic supplement competing interests the authors declare no competing interests acknowledgements the work received funding from the austrian science fund p 27400 s f v 519 n29 m p k as well as from the swiss national science foundation snf 200021 159899 m k appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 026 
26219,because effects of climate change and an increase in elements at risk mountain hazard loss increased throughout europe yet factors influencing loss i e vulnerability have gained less attention to date vulnerability is defined as the degree of loss resulting from the hazard impact on buildings recent studies have focused on evaluating vulnerability to dynamic flooding using proxies from case studies and based on empirical ex post approaches however the transferability to other case studies and therefore the ability of such models to actually predict future losses is limited to overcome this gap we present a beta model based on loss data from the european alps which clearly shows that a single vulnerability function is sufficient to predict losses resulting from different types of torrential hazards and to provide probabilities of destruction under specific scenarios as a result the curves are transferable and may significantly increase the predictive power of risk analyses keywords vulnerability elements at risk torrential hazards loss zero and one inflated beta regression austria 1 introduction throughout europe increasing losses due to flood hazards have been reported kreibich et al 2014 especially for inundation along large rivers barredo 2007 di baldassarre et al 2018 in contrast there is a lack of studies on the effects of mountain river flooding papathoma köhle 2016 papathoma köhle et al 2017 zhang et al 2018 increases in overall losses are mainly attributed to the effects of climate change mountain research initiative edw working group 2015 which affect the magnitude and frequency of events and to population dynamics in mountain areas fuchs et al 2015 2017 however the vulnerability of communities experiencing the impact of such hazards is less well known jakob et al 2012 zimmermann and keiler 2015 which is also articulated by international frameworks for disaster risk reduction such as the sendai framework un isdr 2015 klein et al 2019 calling for the need for improved understanding of disaster risk in all its dimensions of exposure vulnerability and hazard characteristics mountain rivers are characterized by dynamic flooding with variable amounts of sediment erosion deposition and remobilisation sturm et al 2018 typical hazard processes include fluvial sediment transport debris flows and related phenomena slaymaker 2010 mazzorana et al 2014 karagiorgos et al 2016 milanesi et al 2018 in europe such processes repeatedly result in considerable damage to infrastructure and buildings on a local and regional level guzzetti et al 2005 hilker et al 2009 fuchs et al 2015 zhang et al 2018 zischg et al 2018 zou et al 2018 schlögl et al 2019 several studies used empirical data from past events to assess damage monetarily by using vulnerability functions fuchs et al 2007 akbas et al 2009 quan luna et al 2011 papathoma köhle et al 2012 sterlacchini et al 2013 a recent overview is provided in papathoma köhle et al 2017 vulnerability functions are based on the empirical assessment of observed damage and describe underlying process magnitudes and related loss patterns consequently vulnerability ranges from 0 no damage to 1 complete destruction the use of such vulnerability functions is common in the case of hazards e g river flooding and earthquakes that affect larger areas and a considerable amount of elements at risk nevertheless despite numerous attempts and models to empirically describe the vulnerability of static flooding along european rivers kreibich et al 2015 such methods and tools are hardly available for assessing the effects of dynamic flooding in mountain watersheds moreover available data about the empirical assessment of affected buildings located on torrential fans are limited papathoma köhle et al 2011 2017 and were often not found to be transferable to other case studies cammerer et al 2013 on the contrary there is an emphasis on hazard assessment in specific catchments the magnitude of these hazard types pose case specific threats to elements at risk depending on the geology catchment and channel morphology as well as meteorological triggers and interactions among these factors during an individual event recent attempts to derive vulnerability functions using loss data included curve fitting to a series of data that was recorded after major incidents with the aim to predict the probability of occurrence of a certain degree of loss with respect to different flood levels papathoma köhle et al 2012 totschnig and fuchs 2013 carisi et al 2018 to determine the flood level deposition height was repeatedly used as a proxy since data on flow velocities were not available jakob et al 2012 papathoma köhle et al 2015 chow et al 2018 the function with the best fit should minimize the squared differences in data which is consistent with the classical approach of curve fitting this was repeatedly computed with a weibull distribution function totschnig and fuchs 2013 papathoma köhle et al 2015 as a result the overall relationship between hazard magnitude and the observed degree of loss can be mirrored uncertainties can be expressed by confidence intervals which depend on the distribution of errors these uncertainties are of aleatory type and are based on the assumption of symmetrically distributed errors around the mean degree of loss which is rarely observed in reality the data spread of weibull functions results in theoretical loss values below zero and above one which is inconsistent with the definition of vulnerability moreover the observed loss pattern totschnig and fuchs 2013 papathoma köhle et al 2015 is characterized by more data with small values lower degree of loss than with high values larger degree of loss until complete destruction and the data showed a right skewed distribution the larger loss values and therefore the larger degree of loss tended to be farther away from the mean degree of loss than the smaller values hence a suitable and stochastically valid probability model must be able to model the skewness in the degree of loss this model requires a parametric assumption and the selection of a suitable probability distribution which enables the statistical treatment of uncertainties a lack of predictive power of the degree of loss for future events is evident since the current approaches were based on spurious error assumptions hastie and tibshirani 1990 fox 2016 to overcome this gap we compiled an integrative dataset that was used to evaluate a model and which is able to better explain the observed parameter values and the associated skewness the dataset comprises available data from individual studies conducted in the european alps i e incidents from austria and italy fuchs et al 2007 fuchs 2008 totschnig et al 2011 papathoma köhle et al 2012 totschnig and fuchs 2013 papathoma köhle et al 2015 2 methods we chose a beta distribution as the underlying model for assessing the vulnerability of buildings the proposed model assumes that the variable of interest degree of loss is continuous restricted to the interval between zero and one and is related to the process magnitude through a regression structure ferrari and cribari neto 2004 the final dataset consists of 214 data points that link observed process magnitudes and the degree of loss for individual buildings damaged during torrent hazards in the european alps additional information about the process subtype e g fluvial sediment transport hyperconcentrated flow debris flow and the type of building was collected for 60 of the 214 cases investigated damages occurred due to debris flow hazards while the remaining 154 damages were caused by fluvial sediment transport and hyperconcentrated flows the observations were equally distributed over austria and italy with 107 cases from each country only 10 samples represented commercial buildings and the remaining 204 samples represented residential buildings the degree of loss was treated as a random variable with values between the interval zero no loss and one complete destruction hence including the endpoints 0 1 therefore we modelled the influence of the process magnitude ω on the degree of loss with a zero and one inflated beta distribution which is a mixture of bernoulli and beta distributions the resultant density function is given as 1 f ω μ σ ν τ π 0 i f ω 0 1 π 0 π 1 1 b α β ω α 1 1 ω β 1 i f 0 ω 1 π 1 i f ω 1 where μ α α β σ α β 1 1 2 ν π 0 π 1 τ π 0 1 π 0 π 1 and b α β is the beta function the expected value of the distribution is given by e ω τ μ 1 ν τ see rigby and stasinopoulos 2005 and stasinopoulos and rigby 2007 to model the dependency of the distribution parameters on the process magnitude we used a logit link for μ a log link for ν and τ while σ was assumed to be constant the model parameters were estimated by maximizing the log likelihood using the gamlss package stasinopoulos and rigby 2007 for the statistical software r r core team 2018 to decide if process magnitude process type as well as geographical location and building types influence the behavior of the degree of loss we applied two procedures 1 a stepwise model selection procedure based on the generalized akaike information criterion akaike 1974 as the selection criterion and 2 bootstrapping i e resampling with replacement for the first predictive variables for the regression model were selected by applying bidirectional elimination i e a combination of forward selection and backward elimination in an iterative process until the optimal model i e the model with minimal information loss was found for the latter eqn 1 was estimated on 5000 bootstrap samples from the original dataset for each bootstrap sample the p value of the explanatory variable was estimated the number of times variable was significant hence the p value was below 0 05 and was used as a measure of parameter importance gilenko and mironova 2017 uncertainty for the fitted zero and one inflated beta distribution was assessed by means of non parametric bootstrapping by applying the same analysis steps used for obtaining the final vulnerability function to 5000 new samples generated from the original dataset from these bootstrap samples confidence intervals for the regression model were calculated 3 results bootstrap based variable selection revealed that there was no need to distinguish between observations regarding geographical location and building type as the p values were below 0 05 only in 17 and 13 of all models fig 1 upper left and right in contrast the process type was a significant predictor in 58 and the process magnitude was significant in 100 of all models fig 1 lower left and right the clear rejection of the geographical location and the building type as potential predictors the indifferent importance of process type as predictor and the clear significance of process magnitude supports the hypothesis that a single vulnerability function depending on the process magnitude is sufficient to model the degree of loss this is also mirrored by the large overlap of the boxplots given in the inlet of fig 2 these findings are consistent with the stepwise model selection procedure which resulted in a model featuring process magnitude as the most relevant predictor the final model for the mean degree of loss l d as a function of process magnitude is expressed as 2 l d ω 0 if ω 0 e 7 40 2 56 ω e 3 27 1 67 ω 1 e 3 27 1 67 ω 1 e 9 49 ω e 7 40 2 56 ω if ω 0 the results are presented in fig 2 in the main panel the process magnitude is shown on the x axis and the degree of loss on the y axis boxplots were used to indicate the spread in the degree of loss for different process magnitudes observed the data were summarized into classes of different widths of process magnitudes to ensure a minimum number of five data points for boxplot computation secondary x axis both the median degree of loss and the spread increase with increasing process magnitude as a result three functional relationships can be established 1 the dashed line represents the probability of an undamaged building degree of loss 0 and rapidly decreases with increasing process magnitude and 2 the dotted line represents the probability of a completely damaged building this probability remains below 0 1 until a process magnitude of around 2 m and increases significantly thereafter this is clearly more advantageous when compared to an empirical assessment of vulnerability based on curve fitting finally the black line 3 shows the evolution of the degree of loss for buildings that are neither fully intact nor totally damaged the mean degree of loss as a result of the respective process magnitudes is obtained by combining these three components into one mixture model eqn 2 in fig 3 the expected value of the modelled distribution c f eqn 2 is shown for the entire data range including no loss and complete destruction this model includes the process magnitude as the explaining variable and specifically supports the computation of mean losses for buildings affected by torrent events by accounting for data skewness consequently the model shows that once a building is only marginally affected by dynamic flooding a small degree of loss occurs therefore the model is an extension of an empirical approach that relies on a normal distribution of the degree of loss and allows for an ex ante assessment of damage once potential process magnitudes and the values of elements at risk are known e g from modelling exercises uncertainty of the fitted zero and one inflated beta distribution is assessed by means of non parametric bootstrapping i e resampling with replacement by applying the same analysis steps used for obtaining the final vulnerability function to 5000 new samples generated from the original data set bootstrap confidence intervals can be calculated for the regression model apart from the uncertainty assessment based on bootstrapping model quality is assessed by comparing modelled versus observed values of degree of loss fig 4 as readily seen no significant bias towards over or underestimation is observed as the 95 confidence area of the regression blue shaded area includes the 1 1 line grey dashed line hence neither the intercept is significantly different from 0 nor the slope is significantly different from 1 even if considerable variability remains this is also reflected in corresponding error statistics concerning mean squared deviation and its components kobayashi and salam 2000 gauch et al 2003 based on 5 fold cross validation translation squared bias 0 0004 rotation non unity slope 0 0011 and scatter lack of correlation 0 0106 between model based predictions and observations result in a rmse of 0 1095 and a mae of 0 0737 4 discussion and conclusion we extended available studies on the physical vulnerability of buildings to dynamic flooding due to torrential processes to achieve a robust prediction of losses an improved vulnerability function is presented derived from a considerable volume of empirical data in comparison to available studies it is capable of predicting torrential hazard losses in different countries and for various building types while earlier attempts were only focused on representing the mean degree of loss for different process magnitudes observed our approach allows risk managers to model future losses as a function of possible impacts due to the deposition of material and water on the building envelope additionally the model considers the probability of no loss and the complete destruction of affected buildings due to its predictive power the approach may be applied to operational risk management and also in areas were empirical data is currently not available the latter is of particular importance due to climate and socioeconomic changes the impacts of mountain floods are expected in areas with limited to no records of such previous events knowledge of probabilities supports decision making and the prioritization of resources for the construction and implementation of protection measures thus the inclusion of such predictions into future planning processes will further enhance the reliability of vulnerability assessments future research may focus on 1 increasing the overall amount of data that can improve and update vulnerability functions and 2 considering other explaining variables besides deposition height e g flow velocity author contributions s f initiated this work and contributed with loss data from the austrian alps m h and m s performed the statistical analysis and programmed the loss model with support from s f and a z and m k m p k and a z contributed with loss data from the italian alps all authors were jointly involved with writing the manuscript raw data may be obtained from the corresponding author s f the model is available as electronic supplement competing interests the authors declare no competing interests acknowledgements the work received funding from the austrian science fund p 27400 s f v 519 n29 m p k as well as from the swiss national science foundation snf 200021 159899 m k appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 026 
