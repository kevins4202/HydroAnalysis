index,text
26405,forecast verification is necessary to determine the skill and quality of a forecasting system and whether it shows improvement with pre or post processing s2dverification v2 8 0 is an open source r package for the quality assessment of climate forecasts using state of the art verification scores the package provides tools for each step of the forecast verification process data retrieval processing calculation of verification measures and visualisation of the results examples are provided and explained for each of these stages using climate model output keywords forecast verification climate forecast skill score r package 1 introduction from their modest beginning in the 1950s numerical weather forecasts have evolved in scale and complexity to become an integral part of countless decision making processes worldwide although not as widely disseminated as weather forecasts many operational research centres also produce seasonal climate forecasts doblas reyes et al 2013 these forecasts estimate the monthly and seasonal mean values of climate variables e g temperature and precipitation 1 12 months in advance to anticipate for example drought or flooding events related to large scale atmospheric flow patterns such as the el niño southern oscillation enso rasmusson and wallace 1983 kousky et al 1984 or the north atlantic oscillation nao hurrell 1996 with increases in computing power combined with advances in climate science and in the quality and quantity of observational data there is growing interest in the feasibility of decadal climate forecasts commonly referred to as climate forecasts i e forecasts providing estimates one year to several decades in advance smith et al 2007 the accuracy of these forecasts has progressed substantially over the last 20 years e g see doblas reyes et al 2013 and meehl et al 2014 for more information on seasonal and decadal forecasts respectively regardless of the timescale in consideration it is essential to assess the forecast s quality through forecast verification forecast verification is conducted by comparing forecasts of past events also known as retrospective forecasts or hindcasts with their corresponding observations or observational products referred to as references hereafter the verification involves quantifying the accuracy the correspondence between the forecasts and references and the association the strength of the relationship between the forecasts and the references potts 2003 of the forecast system ideally a number of different metrics should be considered as a single measure cannot fully characterise the forecast quality bennett et al 2013 this manuscript introduces s2dverification v2 8 0 where s2d stands for seasonal to decadal a software package for the r statistical programming language r core team 2015 the key advantage of using s2dverification is that it enables the user to conduct the entire workflow of the verification process with a single software package it provides an end to end unified framework including a powerful data loading and homogenization function section 2 1 functions that transparently implement well known methods for pre processing and verifying climate data novel verification and synthetic forecast generation methods and functions to generate frequently required visual products the package stems from a collection of verification tools developed over several years by scientists from the earth sciences department of the barcelona supercomputing center centro nacional de supercomputación and their collaborators the group specializes in the production and analysis of climate forecasts derived from dynamical climate models or statistical forecast systems with forecast periods ranging from weeks to years the package has also been developed in conjunction with external partners to address the needs of forecast users from the private sector although the package is designed mainly for the verification of forecasts of any climate variable it can also be used for forecast verification in other fields or on different timescales the groups of functions and modules in s2dverification and their potential interactions are depicted in fig 1 several software packages exist for weather and climate forecast verification for a variety of platforms and programming languages these range greatly in the breadth of their contents from solely calculating verification statistics to providing a complete framework including tools for data management visualisation and other analyses typical of the field the package specializes in outside of r some of the most relevant open source packages that provide a full framework for the evaluation of climate model output are the model evaluation tools met brown et al 2009 developed at the national center for atmospheric research u s ncar the ensemble verification system evs brown et al 2010 developed at the national oceanic and atmospheric administration u s noaa and the earth system model validation tool esmvaltool eyring et al 2015 which provides community reviewed tools for calculating metrics for simulations produced in the context of the coupled model intercomparison project cmip a summary of some of the available r packages containing functions relevant to forecast verification is provided in table 1 the added value of s2dverification is its data management utilities designed for the efficient computation of forecast quality its range of state of the art forecast quality evaluation scores and visualisation tools tailored to forecast quality inspection the rest of this paper is organized as follows section 2 describes the main modules the package comprises data retrieval processing verification measures and visualization functions for generating synthetic data and verifying hurricane forecasts are also described in this section section 3 illustrates the use of the package with a case study utilizing climate model output section 4 concludes this paper and discusses some future developments for the package 2 methods the s2dverification package provides tools to assist with each of the four stages of the verification process data retrieval processing calculation of verification measures and visualization the package is constantly evolving with new verification measures and functionality being added in this section we present examples of some of the available functions for each stage of the verification process some of the core s2dverification functions are listed in table 2 2 1 data retrieval data retrieval refers to the loading and homogenizing of the data sets so that they are in a commensurable format which is compatible with the processing and analysis functions the data retrieval stage can be relatively time consuming due to the following reasons i it may involve several software packages ii the data sets may come from different sources and follow different conventions and finally iii data may need to be interpolated onto a common grid the load function in s2dverification streamlines this process automating many of the steps provided that the data are stored according to some simple guidelines these guidelines are as follows the data is in netcdf format in a local file system or on remote servers that are able to communicate using the open source project for a network data access protocol opendap such as thematic real time environmental distributed data services 1 1 http www unidata ucar edu software thredds current tds thredds servers or earth system grid federation 2 2 http esgf llnl gov esgf nodes there is one file for each simulation initialized at a different time referred to here as start date with an identifier for the given start date appearing in the file path or filename the reference data contains either one file per month with the corresponding year and month included in its path or filename or a single file for the entire reference data set all files under consideration contain daily or monthly area averages or two dimensional fields on either regular rectangular or gaussian grids the load function can be used to retrieve data from various forecast systems and reference data sets for multiple start dates model members and forecast time steps for a user defined region and variable the user can interpolate the data on a common grid choosing between the interpolation techniques supported by the climate data operators software cdo schulzweida 2015 the load function also allows the user to apply separate masks to both the forecasts and references either when requesting two dimensional fields or before averaging data over a specific region the data are loaded into two multi dimensional arrays of similar structure one containing the forecasts and the other containing the references the data pairing i e searching for the references matching the dates of the forecasts is done automatically the resulting arrays are the working units of s2dverification that can then be passed directly to other functions in the package this powerful feature significantly reduces the amount of time spent manipulating files since any modifications to the desired output e g the inclusion of a new reference data set are handled by load and it is straightforward to modify the parameters in the function call load is designed to work on a single workstation connected to the file systems or servers containing the relevant data usually in an infrastructure with this topology the connection between the workstation and the data servers becomes a bottleneck and it is hence recommended to minimize the amount of time the connection is unused however by default the load function is alternating data retrieval processes where the connection is in use with corresponding interpolation and arrangement processes where the connection is unused in a way that the connection is not optimally used in order to address this the function can be adjusted via a parameter to work on multiple cores this way when the connection to the sources of data is slow load runs multiple retrieval processing couples simultaneously to constantly exploit the connection bandwidth when the connection is fast running load in parallel will ensure the available computing resources in the workstation are used efficiently to interpolate and arrange the large flow of retrieved data as it is loaded the use of the cdo libraries in s2dverification makes it a useful and unique package for handling data on regular gaussian grids which are widely used in climate modelling and for interpolating to or from regular longitude latitude grids with a variety of methods 2 2 processing one of the purposes of s2dverification is to provide a versatile post processing adjustment of seasonal to decadal climate predictions through the application of different bias correction methods due to inherent model approximations as well as errors in the initial conditions boundary conditions and forcing fields after initialization a climate model drifts towards its preferred climate state or attractor dijkstra 2013 the model bias is an average model error over the validation period the bias in climate forecasts can change in time exhibiting non stationary drifts that can be dependent on the forecast time and start dates the clim function allows the user to account for the drift when calculating the climatology with one of three available empirical adjustment methods i the mean per pair bias correction method e g garcía serrano and doblas reyes 2012 ii the linear trend bias correction method kharin et al 2012 and iii the initial condition bias correction method fučkar et al 2014 as well as bias correction filtering and smoothing of the data is often necessary the variability in a climate field is exhibited by a number of processes occurring at different time scales for example the persistence of an atmospheric anomaly is not longer than a few days while the persistence of a sea surface temperature anomaly can last for months or seasons internal e g deep oceanic circulation and external e g solar activity or greenhouse gases agents can drive variability at different frequencies via dynamical mechanisms depending on the type of analysis being conducted some of these processes may be of greater interest than others and it may be necessary to filter out some of that variability before analysing the data several procedures can be employed for this purpose the fourier transform filter which is based on a discrete fourier transform and converts a finite series of samples equally spaced in time from its original time dimension into the frequency domain in order to ease the isolation of a specified frequency e g von storch and zwiers 2001 s2dverification provides the functions spectrum and filter which convert data to the frequency domain and filter out specified frequencies respectively the moving average filter also known as the running mean which computes the mean within a sliding time window thereby representing a low pass filter e g in decadal prediction a 4 year forecast moving average is often performed in order to retain interannual to decadal variability and to reduce unpredictable higher frequency variability garcía serrano and doblas reyes 2012 the package includes the smoothing function to perform this operation on multi dimensional arrays the piecewise filter which is applied to raw values and computes differences between consecutive samples thus largely attenuating low frequency signals e g using yearly data it emphasizes inter annual anomalies the departure from the long term average see e g stephenson et al 2000 and using daily data it retains synoptic activity 24h filter e g wallace et al 1988 for example one could assess the characteristic transient activity of the extra tropical storm tracks although there is no function in s2dverification for piecewise filtering this simple procedure can be handled with the base r functions the forecast and reference data sets are typically very large making it difficult to analyse individual processes and extract meaningful information from the data however a number of techniques are available for dimensionality reduction and data compression as well as pattern extraction to facilitate this purpose widely used methods include the calculation of the empirical orthogonal functions eof which identify the leading modes of variability in a particular field e g von storch and zwiers 2001 and the calculation of extended eofs venegas 2001 which apply eofs to a time evolving field these can be computed with the eof function in s2dverification climate data fields can then be regressed onto the leading modes of variability identified by eof using the projectfield function the svd function allows the user to investigate modes of covariability between two fields using maximum covariance analysis mca and canonical correlation analysis cca bretherton et al 1992 as well as extended mca for the analysis of time evolving coupled patterns garcía serrano et al 2008 the existence of groups or clusters within groups whose identities or patterns are not known in advance e g fučkar et al 2016 caron et al 2015 can be investigated with the clusters function 2 3 verification after processing the use of the s2dverification package to calculate relevant statistics and scores to assist in the verification of the forecast is straightforward all of the functions in the package are designed to take multidimensional arrays as input the dimensions of the array can include multiple runs ensemble members from several climate models as well as forecast time steps initialization dates geographical locations longitudes and latitudes and the scores can be computed along any of these dimensions usually in forecast verification the scores are computed along the start date dimension here we consider the verification of deterministic and probabilistic forecasts separately in the context of ensemble forecasts an example of a deterministic forecast is the ensemble mean e g nino 3 4 region sea surface temperature sst will be 0 6 c above the climatological average in winter and an example of a probabilistic forecast could be the probability density function of the sst distribution taking into account all ensemble members as possible outcomes 2 3 1 deterministic forecasts the correlation coefficient is used to measure the linear dependence between deterministic forecasts and the corresponding references it can be calculated for large arrays with the corr function in s2dverification in addition to pearson s correlation the non parametric spearman and kendall rank correlation coefficients are also supported as the correlation is a measure of association and not of accuracy it is not sensitive to the forecast bias the difference between the mean forecast and the mean reference or to systematic errors consequently a forecast system providing anomalies of the same sign as the references will result in forecasts with very high correlations even if the amplitude of the anomalies is systematically lower or higher than the reference amplitude déqué 2003 the accuracy of a forecast system can be assessed with the root mean square error rmse the rms function calculates the rmse between the forecast and reference anomalies in addition to rms ratiorms computes the ratio of rmse scores of two different forecast data sets see appendix a a ratio lower than one indicates that the forecast system used to generate the first set of forecasts performs better than the one used to generate the second set of forecasts while the ratio between rmses is used to compare two forecast systems the rmse skill score rmsss computed with the rmsss function can be used to assess the skill of the forecast with respect to a climatological forecast a forecast based only on the climatological statistics this score ranges from minus infinity no skill to 1 perfect forecast a score larger than 0 indicates an improvement with respect to the climatological forecast as well as being a measure of the forecast accuracy the rmse also provides information about the reliability of the ensemble spread a forecast ensemble is considered reliable when it samples the full range of possible outcomes that can arise from a given initial state if the forecast system has known imperfections which are reflected in the rmse the ensemble spread should sample this range of model errors in order to represent the full range of possible outcomes in practice the ensemble spread is often smaller than the rmse and the forecasts are therefore overconfident the diagnostic ratiosdrms known as the spread versus error relationship slingo and palmer 2011 measures this relationship by computing the fraction of the ensemble spread standard deviation of the ensemble across all start dates with the rmse of the ensemble mean a forecast is considered reliable if this measure is close to 1 the ability of a forecast to reproduce spatial patterns can be investigated with the anomaly correlation coefficient which is calculated with the acc function krishnamurti et al 2003 this function calculates the anomaly correlation coefficient to measure the linear association between the longitude latitude forecast anomalies at a single time step and the corresponding observational reference anomalies this is somewhat different from the function corr which evaluates the ability of a forecast system to capture temporal variability spatial correlation like time correlation is not sensitive to the forecast bias or to systematic errors in the forecast system variance 2 3 2 probabilistic forecasts probabilistic verification measures can be broadly divided into dichotomous binary or continuous measures dichotomous scores measure the accuracy of a forecast predicting the probability of a binary event e g is it going to rain more than 300 mm this summer whereas continuous scores measure the accuracy of the estimated probability at all possible thresholds hence evaluating the entire continuous probability distribution function in s2dverification the dichotomous brier score can be calculated to measure the mean distance for a given binary event between the forecasts and references in probability space since the brier score is dichotomous it cannot be applied directly to a deterministic forecast first the probability of the deterministic ensemble forecast of exceeding a certain threshold needs to be computed the function probbins allows the computation of such probabilities which can then be provided as inputs to the function brierscore to compute the brier score the function ultimatebrier allows one to use different versions of the brier score that take into account corrections for limited ensemble size ferro 2014 and limited hindcast size ferro and fricker 2012 or to compute the decomposition of the brier score as described in stephenson et al 2008 2 3 3 confidence intervals when conducting hypothesis tests or calculating confidence intervals serial dependence in the data needs to be taken into account because for highly autocorrelated data the number of independent measurements might be significantly lower than the actual number of data points trenberth 1984 failing to take this feature of the data into consideration will lead to a systematic overestimation of the significance of the skill scores in s2dverification the eno function computes the number of independent observations in a time series also called effective sample size and enonew performs the same computation but offers the option to filter out the trend or to exclude some frequency peaks before estimating the equivalent number of independent data guemas et al 2014 eno is transparently used in the calculation of the confidence intervals by the following functions acc corr ratiorms ratiosdrms rms and rmsss this approach allows the accurate computation of the statistical significance of skill scores when serial dependence is present in the data 2 4 visualization due to the large amount of data and variables that are usually involved in the verification process visualizing the data in a comprehensible way is non trivial for example about 10 9 values have to be stored and organized to encode the 2 m air temperature on a 512 256 global grid for two forecast systems each running with 10 members initialized twice a year for 30 years and integrated over 6 months visualization tools are thus required in all of the stages of the verification process to quickly inspect the results of a newly produced set of simulations i e to check the physical consistency of the results to assess the model output after processing and to display verification statistics and confidence intervals in a user friendly way s2dverification includes a set of tools to plot indices scores and maps of user defined regions examples of usage and outputs of the most relevant plotting functions are shown in the next section time series plots for multiple models on the same axis can be created with the plotvsltime function or of two different scores for multiple forecast systems against multiple experiments observational references can be compared using the function plot2varsvsltime the function plotacc plots the spatial anomaly correlation coefficients as a function of the start date and if requested also the forecast time including the 95 confidence interval for the correlation in order to determine the statistical significance plotclim displays climatologies of a variable or index from multiple forecasts and references along the forecast time plotano can display time series of either raw data or their deviation with respect to the climatology for indices of teleconnection patterns such as the nao information on the ensemble spread and correlation between the ensemble mean index and reference data index can be summarized visually using the plotboxwhisker function this function plots for each start date the box and whisker diagram of the interquartile range ensemble standard deviation and outlier members of the ensemble forecast as well as the corresponding reference time series based on outputs from the eof function spatial fields can be displayed on maps with cylindrical equidistant and stereographic projections with the plotequimap and plotstereomap functions respectively the boundaries of the region to be plotted can be customized the colourbar limits adjusted continents shaded and arrows displayed along with other customizable features it is also possible to create animations with animvsltime a function that makes the composite of different maps in a gif object in order to represent the time evolution of the provided feature for instance the change in correlation indices with an observational reference as the forecast time increases finally plotsection plots a 2 dimensional subset latitude depth or longitude depth of a three dimensional variable for example the temperature in the atmosphere the functions for plotting time series are based on r base s plot legend and boxplot the map plots consist of raster plots from r base s image function superimposed to map projections drawn with the maps brownrigg 2013 and mapproj deckmyn 2015 or geomap and geomapdata lees 2012 packages the software imagemagik is required as a system dependency for generating map animations in animvsltime several other packages have functions for plotting time series e g ggplot2 or maps with superimposed raster data e g ggplot2 lattice sp tmap and rastervis the advantage of the visualisation functions provided in s2dverification are their ease of use as they have been designed for application to data in the common s2dverification format or in raw r arrays 2 5 synthetic forecast generation as well as functions for loading climate model output s2dverification contains two functions for generating synthetic data from existing fields or by random sampling of variables from distributions defined by the user the toymodel function is a statistical toy model based on the model presented in weigel et al 2008 with an extension to allow simulation from non stationary distributions containing a linear trend this toy model allows the exploration of forecast features for which little information is available usually due to the reduced length of the hindcasts or ensemble members of real forecast systems for example how the correlation of the ensemble mean with the reference changes with increasing ensemble size up to thousands of members further applications of this model can be found in weigel et al 2008 siegert et al 2015 and bellprat and doblas reyes 2016 the toy model imitates the typical components of a forecast i predictability ii forecast error iii non stationarity and iv ensemble generation the predictability is defined by the fraction of the observed outcome which is explained hence the model does not serve as a probabilistic prediction but merely mimics different forecast components it allows the generation of an artificial forecast based on some inputted references obtained with load or otherwise or synthetic references based on the input parameters standard deviation of interannual variability and a trend term the model allows the user to vary the predictability of the system with a linear term from the reference anomalies and the magnitude of the forecast error the magnitude of these terms is constrained by imposing the condition that the model has the same total variability as the references see weigel et al 2008 for details imposing this condition allows the user to explore verification aspects which are free from systematic biases in the mean climate state and variability the toymodel function also includes parameters for defining the number of predictions the ensemble size the forecast length and the long term trend 2 6 hurricane forecasting the function statseasatlhurr can be used to estimate the mean seasonal atlantic hurricane activity in climate simulations hurricane activity is estimated using a statistical downscaling technique which relies on seasonal averages of the sea surface temperature anomalies over the tropical atlantic and over the tropics in general there are dynamical and thermodynamical arguments linking sea surface temperature over these regions and atlantic hurricane activity vecchi et al 2011 more information on that link on the downscaling technique itself and how that function can be used in hurricane forecast studies can be found in villarini et al 2010 villarini and vecchi 2012 villarini et al 2012 and caron et al 2014 statseasatlhurr can be used to estimate either the mean number of hurricanes the mean number of tropical cyclones with lifetime greater than 48 h or the mean power dissipation index emanuel 2005 which is a measure which incorporates the number intensity and duration of the storms over an entire hurricane season the function also provides information on the distribution around the seasonal mean 3 case study this section presents a case study which illustrates the use of some of the key functions for each of the four stages of the verification process forecasts from two different climate forecast systems are compared with an observational reference data set in this example near surface temperature tas forecasts have been selected although the package would handle any other user selected variable both forecast systems use the ec earth v2 3 0 climate model initialized every november 1st between 1991 and 2000 for the first forecast system ocean initial conditions are taken from the oras4 reanalysis balmaseda et al 2013 interpolated to the model grid while for the second forecast system the ocean initial conditions are taken from an assimilation run which has been nudged to the oras4 reanalysis both forecast systems use atmosphere and land initial conditions from the era interim reanalysis dee et al 2011 and sea ice initial conditions from an ocean and sea ice coupled simulation forced by the drakkar forcing set v4 3 brodeau et al 2010 five members are generated for each start date from perturbations of the initial ocean and atmosphere conditions du et al 2012 the forecast duration is one year and the observational references are taken from the drakkar forcing set v5 2 reanalysis dussin et al 2014 this study considers the north pacific region 10s 60 e 100e 250 e 3 3 all the monthly mean files used in the example can be downloaded from www bsc es projects earthscience s2dverification s2dv example files tar the verification case study consists of the following steps loading and visualizing area averaged surface temperatures over the north pacific from a set of forecasts and references computing and visualizing climatologies and anomalies computing and visualizing the correlation and rmse between the forecasts and references loading two dimensional subsets of the data over the north pacific computing and visualising the modes of variability of the target data sets computing and visualising the spatial correlations and brier scores 3 1 data retrieval first we construct lists containing the location of the data sets to be considered the directory structure of the considered data files is represented in appendix b the path component must contain a character string with the path patterns to the files of the corresponding data set the names surrounded by symbols are wildcards that load will replace automatically with the appropriate value additional details can be found in load image 1 the load command can now be called by specifying the variable of interest the desired data sets the region of interest the start dates and the forecast time steps of interest here the area averages are requested with the output option image 2 the object returned by load contains two arrays one for the forecast data d a t a m o d and the other for the corresponding reference data d a t a o b s with a dimension structure specific to s2dverification 4 4 this object can be downloaded directly from www bsc es projects earthscience s2dverification s2dv example data rdata image 3 before processing the data we can first visually inspect the time series of the raw data using plotano image 4 from fig 2 the forecast anomalies appear to be in broad agreement with the references from which we can infer the data both experimental and reference have been correctly retrieved and we can now proceed with the processing and verification 3 2 processing climatologies can be computed and plotted with clim and plotclim respectively clim computes per pair climatologies by default since by default plotclim assumes that the forecasts start in january the actual initial month in our experiments november needs to be specified with the parameter monini image 5 each line in fig 3 shows the climatology of one member a single climatology of the ensemble mean could be obtained by setting the parameter m e m b f a l s e in clim we can then obtain the anomalies with the function ano which subtracts the climatologies from each start date and member of the raw data the anomalies can be plotted with plotano see fig 4 image 6 by default all the forecasts started at different dates contribute to the per pair climatology and when computing the anomaly of one of the start dates its contribution to the climatology is subtracted in order to avoid taking into account this contribution thus mimicking as closely as possible an operational context in which the forecast data is not used for the estimation of the climatology the function a n o c r o s s v a l i d automatically computes a climatology for each start date without taking that start date into account and subtracts it from the original data image 7 3 3 verification to measure the skill of the two forecast systems pearsons correlation coefficient and the rmse of the ensemble mean are computed and the results are plotted using plotvsltime figs 5 and 6 as in plotclim the initial month can be specified with monini image 8 in order to compute modes of variability and or spatial correlations we use two dimensional fields using the load function the data sets are interpolated onto a common t 106 gaussian grid the coarsest grid among the considered data sets with cdo s distance weighted method image 9 the returned forecast and observational reference data now has two additional spatial dimensions 5 5 this r object can be downloaded directly from www bsc es projects earthscience s2dverification s2dv example map data rdata image 10 the modes of variability can be computed with the eof function which automatically applies area weighting to the input data by default the first 10 modes are provided they can be plotted with plotequimap for example the second mode of the reference shows a dipole between alaska and northern japan fig 7 image 11 the index of this mode can be computed for each ensemble member by projecting the anomaly map on the mode of variability with projectfield and the results can be displayed with plotboxwhisker as in fig 8 reference indices are available in the eof outputs image 12 spatial correlations and confidence intervals at all forecast times and start dates for all experiments are computed via acc the results are plotted for experiment a with plotacc as shown in fig 9 image 13 as well as the spatial correlation maps of the temporal correlation can be plotted with the plotequimap function as follows image 14 the results are shown in fig 10 plotstereomap could also have been used to produce the same plots but with a stereographic projection the ensemble forecast can be converted into probabilistic forecasts by binning and counting the forecast values this conversion is done automatically in the ultimatebrier function which can compute the standard brier score and its decomposition after binning the forecasts into terciles image 15 we can see from figs 10 and 11 that both forecasts show similar spatial variability however in general experiment a outperforms experiment b with the former having higher correlations and lower briers scores for most regions the region with the most skill in both experiments is 5 s 5 n 190 240 e also known as the niño 3 4 region 4 conclusions this paper has introduced s2dverification an r package for streamlining and simplifying the forecast verification process in climate research and other disciplines its key features have been described including a powerful data loading and homogenization function functions that transparently implement well known methods for pre processing and verifying climate data novel verification and synthetic forecast generation methods and functions to generate frequently required visual products the stages of data retrieval pre processing computation of scores and visualisation of data and results have been briefly described and put into context together with an explanation of the typical challenges encountered and the corresponding mechanisms and functions the package provides to deal with these challenges and a short review of the relevant technical aspects given the proliferation of verification packages it seems likely and desirable that their developers will have to make a concerted effort to converge in the near future either by agreeing on a set of conventions and good practices to make the various tools compatible with all forecast data structures or by creating a single scalable and sustainable verification framework there are several further developments planned for s2dverification the data retrieval module is currently being extended to support data sets stored in additional file schemes and additional file formats and with temporal resolutions other than daily or monthly as well as data sets with start dates at the sub daily level also the verification scores and pre processing functions are being adapted to work on multi core platforms for the mid term future the package is being extended to trace the steps followed from the beginning to the end of the verification process to ensure the reproducibility of results in terms of visualisation the time series and map plotting functions are being unified and extended with additional options finally the overall usability of the framework is being improved to make some technical details more transparent to the user and to efficiently handle large forecast data sets distributed on cluster platforms taking into consideration existing solutions in raster sp zoo ff and other packages acknowledgements the research leading to these results has received funding from the eu seventh framework programme fp7 2007 2014 under grant agreements 308378 specs 607085 eucleia 603521 preface and 308291 euporias and from the copernicus climate change services c3s contract 2016 c3s 51 lot3 bsc sc1 qa4seas o bellprat s contract has been financed by the european space agency under the living planet fellowship as part of the project veritas cci software availability software s2dverification description the s2dverification software is freely available as an r package a comprehensive framework for forecast verification functions are provided for loading and processing the forecast and reference data calculating verification statistics and visualizing the results main developers v guemas and n manubens source language r availability https cran r project org web packages s2dverification index html appendix a verification statistics throughout this section x i represents the forecast value for reference y i for i 1 n total references root mean square error rmse the square root of the mean of the squared differences the rmse is a measure of the distance between the forecast and the references i e the forecast accuracy 1 r m s e i 1 n x i y i 2 n root mean square skill score rmsss a skill score based on the rmse the rmsss compares the rmse of the forecasts with that of the climatology at a fixed location 2 r m s s s 100 n i 1 n 1 x ˆ i x i 2 x x i 2 brier score the mean square differences between the forecast probability x i and reference probability y i the bs takes values between 0 and 1 with lower values corresponding to more accurate forecasts for further information and a decomposition of the brier score see ferro and fricker 2012 3 b s i 1 n x i y i 2 appendix b use case directory tree in the case study the data files are distributed as depicted in the following directory tree image 16 these files need to be netcdf 3 4 compliant and contain the variable tas fulfilling the guidelines detailed in load 
26405,forecast verification is necessary to determine the skill and quality of a forecasting system and whether it shows improvement with pre or post processing s2dverification v2 8 0 is an open source r package for the quality assessment of climate forecasts using state of the art verification scores the package provides tools for each step of the forecast verification process data retrieval processing calculation of verification measures and visualisation of the results examples are provided and explained for each of these stages using climate model output keywords forecast verification climate forecast skill score r package 1 introduction from their modest beginning in the 1950s numerical weather forecasts have evolved in scale and complexity to become an integral part of countless decision making processes worldwide although not as widely disseminated as weather forecasts many operational research centres also produce seasonal climate forecasts doblas reyes et al 2013 these forecasts estimate the monthly and seasonal mean values of climate variables e g temperature and precipitation 1 12 months in advance to anticipate for example drought or flooding events related to large scale atmospheric flow patterns such as the el niño southern oscillation enso rasmusson and wallace 1983 kousky et al 1984 or the north atlantic oscillation nao hurrell 1996 with increases in computing power combined with advances in climate science and in the quality and quantity of observational data there is growing interest in the feasibility of decadal climate forecasts commonly referred to as climate forecasts i e forecasts providing estimates one year to several decades in advance smith et al 2007 the accuracy of these forecasts has progressed substantially over the last 20 years e g see doblas reyes et al 2013 and meehl et al 2014 for more information on seasonal and decadal forecasts respectively regardless of the timescale in consideration it is essential to assess the forecast s quality through forecast verification forecast verification is conducted by comparing forecasts of past events also known as retrospective forecasts or hindcasts with their corresponding observations or observational products referred to as references hereafter the verification involves quantifying the accuracy the correspondence between the forecasts and references and the association the strength of the relationship between the forecasts and the references potts 2003 of the forecast system ideally a number of different metrics should be considered as a single measure cannot fully characterise the forecast quality bennett et al 2013 this manuscript introduces s2dverification v2 8 0 where s2d stands for seasonal to decadal a software package for the r statistical programming language r core team 2015 the key advantage of using s2dverification is that it enables the user to conduct the entire workflow of the verification process with a single software package it provides an end to end unified framework including a powerful data loading and homogenization function section 2 1 functions that transparently implement well known methods for pre processing and verifying climate data novel verification and synthetic forecast generation methods and functions to generate frequently required visual products the package stems from a collection of verification tools developed over several years by scientists from the earth sciences department of the barcelona supercomputing center centro nacional de supercomputación and their collaborators the group specializes in the production and analysis of climate forecasts derived from dynamical climate models or statistical forecast systems with forecast periods ranging from weeks to years the package has also been developed in conjunction with external partners to address the needs of forecast users from the private sector although the package is designed mainly for the verification of forecasts of any climate variable it can also be used for forecast verification in other fields or on different timescales the groups of functions and modules in s2dverification and their potential interactions are depicted in fig 1 several software packages exist for weather and climate forecast verification for a variety of platforms and programming languages these range greatly in the breadth of their contents from solely calculating verification statistics to providing a complete framework including tools for data management visualisation and other analyses typical of the field the package specializes in outside of r some of the most relevant open source packages that provide a full framework for the evaluation of climate model output are the model evaluation tools met brown et al 2009 developed at the national center for atmospheric research u s ncar the ensemble verification system evs brown et al 2010 developed at the national oceanic and atmospheric administration u s noaa and the earth system model validation tool esmvaltool eyring et al 2015 which provides community reviewed tools for calculating metrics for simulations produced in the context of the coupled model intercomparison project cmip a summary of some of the available r packages containing functions relevant to forecast verification is provided in table 1 the added value of s2dverification is its data management utilities designed for the efficient computation of forecast quality its range of state of the art forecast quality evaluation scores and visualisation tools tailored to forecast quality inspection the rest of this paper is organized as follows section 2 describes the main modules the package comprises data retrieval processing verification measures and visualization functions for generating synthetic data and verifying hurricane forecasts are also described in this section section 3 illustrates the use of the package with a case study utilizing climate model output section 4 concludes this paper and discusses some future developments for the package 2 methods the s2dverification package provides tools to assist with each of the four stages of the verification process data retrieval processing calculation of verification measures and visualization the package is constantly evolving with new verification measures and functionality being added in this section we present examples of some of the available functions for each stage of the verification process some of the core s2dverification functions are listed in table 2 2 1 data retrieval data retrieval refers to the loading and homogenizing of the data sets so that they are in a commensurable format which is compatible with the processing and analysis functions the data retrieval stage can be relatively time consuming due to the following reasons i it may involve several software packages ii the data sets may come from different sources and follow different conventions and finally iii data may need to be interpolated onto a common grid the load function in s2dverification streamlines this process automating many of the steps provided that the data are stored according to some simple guidelines these guidelines are as follows the data is in netcdf format in a local file system or on remote servers that are able to communicate using the open source project for a network data access protocol opendap such as thematic real time environmental distributed data services 1 1 http www unidata ucar edu software thredds current tds thredds servers or earth system grid federation 2 2 http esgf llnl gov esgf nodes there is one file for each simulation initialized at a different time referred to here as start date with an identifier for the given start date appearing in the file path or filename the reference data contains either one file per month with the corresponding year and month included in its path or filename or a single file for the entire reference data set all files under consideration contain daily or monthly area averages or two dimensional fields on either regular rectangular or gaussian grids the load function can be used to retrieve data from various forecast systems and reference data sets for multiple start dates model members and forecast time steps for a user defined region and variable the user can interpolate the data on a common grid choosing between the interpolation techniques supported by the climate data operators software cdo schulzweida 2015 the load function also allows the user to apply separate masks to both the forecasts and references either when requesting two dimensional fields or before averaging data over a specific region the data are loaded into two multi dimensional arrays of similar structure one containing the forecasts and the other containing the references the data pairing i e searching for the references matching the dates of the forecasts is done automatically the resulting arrays are the working units of s2dverification that can then be passed directly to other functions in the package this powerful feature significantly reduces the amount of time spent manipulating files since any modifications to the desired output e g the inclusion of a new reference data set are handled by load and it is straightforward to modify the parameters in the function call load is designed to work on a single workstation connected to the file systems or servers containing the relevant data usually in an infrastructure with this topology the connection between the workstation and the data servers becomes a bottleneck and it is hence recommended to minimize the amount of time the connection is unused however by default the load function is alternating data retrieval processes where the connection is in use with corresponding interpolation and arrangement processes where the connection is unused in a way that the connection is not optimally used in order to address this the function can be adjusted via a parameter to work on multiple cores this way when the connection to the sources of data is slow load runs multiple retrieval processing couples simultaneously to constantly exploit the connection bandwidth when the connection is fast running load in parallel will ensure the available computing resources in the workstation are used efficiently to interpolate and arrange the large flow of retrieved data as it is loaded the use of the cdo libraries in s2dverification makes it a useful and unique package for handling data on regular gaussian grids which are widely used in climate modelling and for interpolating to or from regular longitude latitude grids with a variety of methods 2 2 processing one of the purposes of s2dverification is to provide a versatile post processing adjustment of seasonal to decadal climate predictions through the application of different bias correction methods due to inherent model approximations as well as errors in the initial conditions boundary conditions and forcing fields after initialization a climate model drifts towards its preferred climate state or attractor dijkstra 2013 the model bias is an average model error over the validation period the bias in climate forecasts can change in time exhibiting non stationary drifts that can be dependent on the forecast time and start dates the clim function allows the user to account for the drift when calculating the climatology with one of three available empirical adjustment methods i the mean per pair bias correction method e g garcía serrano and doblas reyes 2012 ii the linear trend bias correction method kharin et al 2012 and iii the initial condition bias correction method fučkar et al 2014 as well as bias correction filtering and smoothing of the data is often necessary the variability in a climate field is exhibited by a number of processes occurring at different time scales for example the persistence of an atmospheric anomaly is not longer than a few days while the persistence of a sea surface temperature anomaly can last for months or seasons internal e g deep oceanic circulation and external e g solar activity or greenhouse gases agents can drive variability at different frequencies via dynamical mechanisms depending on the type of analysis being conducted some of these processes may be of greater interest than others and it may be necessary to filter out some of that variability before analysing the data several procedures can be employed for this purpose the fourier transform filter which is based on a discrete fourier transform and converts a finite series of samples equally spaced in time from its original time dimension into the frequency domain in order to ease the isolation of a specified frequency e g von storch and zwiers 2001 s2dverification provides the functions spectrum and filter which convert data to the frequency domain and filter out specified frequencies respectively the moving average filter also known as the running mean which computes the mean within a sliding time window thereby representing a low pass filter e g in decadal prediction a 4 year forecast moving average is often performed in order to retain interannual to decadal variability and to reduce unpredictable higher frequency variability garcía serrano and doblas reyes 2012 the package includes the smoothing function to perform this operation on multi dimensional arrays the piecewise filter which is applied to raw values and computes differences between consecutive samples thus largely attenuating low frequency signals e g using yearly data it emphasizes inter annual anomalies the departure from the long term average see e g stephenson et al 2000 and using daily data it retains synoptic activity 24h filter e g wallace et al 1988 for example one could assess the characteristic transient activity of the extra tropical storm tracks although there is no function in s2dverification for piecewise filtering this simple procedure can be handled with the base r functions the forecast and reference data sets are typically very large making it difficult to analyse individual processes and extract meaningful information from the data however a number of techniques are available for dimensionality reduction and data compression as well as pattern extraction to facilitate this purpose widely used methods include the calculation of the empirical orthogonal functions eof which identify the leading modes of variability in a particular field e g von storch and zwiers 2001 and the calculation of extended eofs venegas 2001 which apply eofs to a time evolving field these can be computed with the eof function in s2dverification climate data fields can then be regressed onto the leading modes of variability identified by eof using the projectfield function the svd function allows the user to investigate modes of covariability between two fields using maximum covariance analysis mca and canonical correlation analysis cca bretherton et al 1992 as well as extended mca for the analysis of time evolving coupled patterns garcía serrano et al 2008 the existence of groups or clusters within groups whose identities or patterns are not known in advance e g fučkar et al 2016 caron et al 2015 can be investigated with the clusters function 2 3 verification after processing the use of the s2dverification package to calculate relevant statistics and scores to assist in the verification of the forecast is straightforward all of the functions in the package are designed to take multidimensional arrays as input the dimensions of the array can include multiple runs ensemble members from several climate models as well as forecast time steps initialization dates geographical locations longitudes and latitudes and the scores can be computed along any of these dimensions usually in forecast verification the scores are computed along the start date dimension here we consider the verification of deterministic and probabilistic forecasts separately in the context of ensemble forecasts an example of a deterministic forecast is the ensemble mean e g nino 3 4 region sea surface temperature sst will be 0 6 c above the climatological average in winter and an example of a probabilistic forecast could be the probability density function of the sst distribution taking into account all ensemble members as possible outcomes 2 3 1 deterministic forecasts the correlation coefficient is used to measure the linear dependence between deterministic forecasts and the corresponding references it can be calculated for large arrays with the corr function in s2dverification in addition to pearson s correlation the non parametric spearman and kendall rank correlation coefficients are also supported as the correlation is a measure of association and not of accuracy it is not sensitive to the forecast bias the difference between the mean forecast and the mean reference or to systematic errors consequently a forecast system providing anomalies of the same sign as the references will result in forecasts with very high correlations even if the amplitude of the anomalies is systematically lower or higher than the reference amplitude déqué 2003 the accuracy of a forecast system can be assessed with the root mean square error rmse the rms function calculates the rmse between the forecast and reference anomalies in addition to rms ratiorms computes the ratio of rmse scores of two different forecast data sets see appendix a a ratio lower than one indicates that the forecast system used to generate the first set of forecasts performs better than the one used to generate the second set of forecasts while the ratio between rmses is used to compare two forecast systems the rmse skill score rmsss computed with the rmsss function can be used to assess the skill of the forecast with respect to a climatological forecast a forecast based only on the climatological statistics this score ranges from minus infinity no skill to 1 perfect forecast a score larger than 0 indicates an improvement with respect to the climatological forecast as well as being a measure of the forecast accuracy the rmse also provides information about the reliability of the ensemble spread a forecast ensemble is considered reliable when it samples the full range of possible outcomes that can arise from a given initial state if the forecast system has known imperfections which are reflected in the rmse the ensemble spread should sample this range of model errors in order to represent the full range of possible outcomes in practice the ensemble spread is often smaller than the rmse and the forecasts are therefore overconfident the diagnostic ratiosdrms known as the spread versus error relationship slingo and palmer 2011 measures this relationship by computing the fraction of the ensemble spread standard deviation of the ensemble across all start dates with the rmse of the ensemble mean a forecast is considered reliable if this measure is close to 1 the ability of a forecast to reproduce spatial patterns can be investigated with the anomaly correlation coefficient which is calculated with the acc function krishnamurti et al 2003 this function calculates the anomaly correlation coefficient to measure the linear association between the longitude latitude forecast anomalies at a single time step and the corresponding observational reference anomalies this is somewhat different from the function corr which evaluates the ability of a forecast system to capture temporal variability spatial correlation like time correlation is not sensitive to the forecast bias or to systematic errors in the forecast system variance 2 3 2 probabilistic forecasts probabilistic verification measures can be broadly divided into dichotomous binary or continuous measures dichotomous scores measure the accuracy of a forecast predicting the probability of a binary event e g is it going to rain more than 300 mm this summer whereas continuous scores measure the accuracy of the estimated probability at all possible thresholds hence evaluating the entire continuous probability distribution function in s2dverification the dichotomous brier score can be calculated to measure the mean distance for a given binary event between the forecasts and references in probability space since the brier score is dichotomous it cannot be applied directly to a deterministic forecast first the probability of the deterministic ensemble forecast of exceeding a certain threshold needs to be computed the function probbins allows the computation of such probabilities which can then be provided as inputs to the function brierscore to compute the brier score the function ultimatebrier allows one to use different versions of the brier score that take into account corrections for limited ensemble size ferro 2014 and limited hindcast size ferro and fricker 2012 or to compute the decomposition of the brier score as described in stephenson et al 2008 2 3 3 confidence intervals when conducting hypothesis tests or calculating confidence intervals serial dependence in the data needs to be taken into account because for highly autocorrelated data the number of independent measurements might be significantly lower than the actual number of data points trenberth 1984 failing to take this feature of the data into consideration will lead to a systematic overestimation of the significance of the skill scores in s2dverification the eno function computes the number of independent observations in a time series also called effective sample size and enonew performs the same computation but offers the option to filter out the trend or to exclude some frequency peaks before estimating the equivalent number of independent data guemas et al 2014 eno is transparently used in the calculation of the confidence intervals by the following functions acc corr ratiorms ratiosdrms rms and rmsss this approach allows the accurate computation of the statistical significance of skill scores when serial dependence is present in the data 2 4 visualization due to the large amount of data and variables that are usually involved in the verification process visualizing the data in a comprehensible way is non trivial for example about 10 9 values have to be stored and organized to encode the 2 m air temperature on a 512 256 global grid for two forecast systems each running with 10 members initialized twice a year for 30 years and integrated over 6 months visualization tools are thus required in all of the stages of the verification process to quickly inspect the results of a newly produced set of simulations i e to check the physical consistency of the results to assess the model output after processing and to display verification statistics and confidence intervals in a user friendly way s2dverification includes a set of tools to plot indices scores and maps of user defined regions examples of usage and outputs of the most relevant plotting functions are shown in the next section time series plots for multiple models on the same axis can be created with the plotvsltime function or of two different scores for multiple forecast systems against multiple experiments observational references can be compared using the function plot2varsvsltime the function plotacc plots the spatial anomaly correlation coefficients as a function of the start date and if requested also the forecast time including the 95 confidence interval for the correlation in order to determine the statistical significance plotclim displays climatologies of a variable or index from multiple forecasts and references along the forecast time plotano can display time series of either raw data or their deviation with respect to the climatology for indices of teleconnection patterns such as the nao information on the ensemble spread and correlation between the ensemble mean index and reference data index can be summarized visually using the plotboxwhisker function this function plots for each start date the box and whisker diagram of the interquartile range ensemble standard deviation and outlier members of the ensemble forecast as well as the corresponding reference time series based on outputs from the eof function spatial fields can be displayed on maps with cylindrical equidistant and stereographic projections with the plotequimap and plotstereomap functions respectively the boundaries of the region to be plotted can be customized the colourbar limits adjusted continents shaded and arrows displayed along with other customizable features it is also possible to create animations with animvsltime a function that makes the composite of different maps in a gif object in order to represent the time evolution of the provided feature for instance the change in correlation indices with an observational reference as the forecast time increases finally plotsection plots a 2 dimensional subset latitude depth or longitude depth of a three dimensional variable for example the temperature in the atmosphere the functions for plotting time series are based on r base s plot legend and boxplot the map plots consist of raster plots from r base s image function superimposed to map projections drawn with the maps brownrigg 2013 and mapproj deckmyn 2015 or geomap and geomapdata lees 2012 packages the software imagemagik is required as a system dependency for generating map animations in animvsltime several other packages have functions for plotting time series e g ggplot2 or maps with superimposed raster data e g ggplot2 lattice sp tmap and rastervis the advantage of the visualisation functions provided in s2dverification are their ease of use as they have been designed for application to data in the common s2dverification format or in raw r arrays 2 5 synthetic forecast generation as well as functions for loading climate model output s2dverification contains two functions for generating synthetic data from existing fields or by random sampling of variables from distributions defined by the user the toymodel function is a statistical toy model based on the model presented in weigel et al 2008 with an extension to allow simulation from non stationary distributions containing a linear trend this toy model allows the exploration of forecast features for which little information is available usually due to the reduced length of the hindcasts or ensemble members of real forecast systems for example how the correlation of the ensemble mean with the reference changes with increasing ensemble size up to thousands of members further applications of this model can be found in weigel et al 2008 siegert et al 2015 and bellprat and doblas reyes 2016 the toy model imitates the typical components of a forecast i predictability ii forecast error iii non stationarity and iv ensemble generation the predictability is defined by the fraction of the observed outcome which is explained hence the model does not serve as a probabilistic prediction but merely mimics different forecast components it allows the generation of an artificial forecast based on some inputted references obtained with load or otherwise or synthetic references based on the input parameters standard deviation of interannual variability and a trend term the model allows the user to vary the predictability of the system with a linear term from the reference anomalies and the magnitude of the forecast error the magnitude of these terms is constrained by imposing the condition that the model has the same total variability as the references see weigel et al 2008 for details imposing this condition allows the user to explore verification aspects which are free from systematic biases in the mean climate state and variability the toymodel function also includes parameters for defining the number of predictions the ensemble size the forecast length and the long term trend 2 6 hurricane forecasting the function statseasatlhurr can be used to estimate the mean seasonal atlantic hurricane activity in climate simulations hurricane activity is estimated using a statistical downscaling technique which relies on seasonal averages of the sea surface temperature anomalies over the tropical atlantic and over the tropics in general there are dynamical and thermodynamical arguments linking sea surface temperature over these regions and atlantic hurricane activity vecchi et al 2011 more information on that link on the downscaling technique itself and how that function can be used in hurricane forecast studies can be found in villarini et al 2010 villarini and vecchi 2012 villarini et al 2012 and caron et al 2014 statseasatlhurr can be used to estimate either the mean number of hurricanes the mean number of tropical cyclones with lifetime greater than 48 h or the mean power dissipation index emanuel 2005 which is a measure which incorporates the number intensity and duration of the storms over an entire hurricane season the function also provides information on the distribution around the seasonal mean 3 case study this section presents a case study which illustrates the use of some of the key functions for each of the four stages of the verification process forecasts from two different climate forecast systems are compared with an observational reference data set in this example near surface temperature tas forecasts have been selected although the package would handle any other user selected variable both forecast systems use the ec earth v2 3 0 climate model initialized every november 1st between 1991 and 2000 for the first forecast system ocean initial conditions are taken from the oras4 reanalysis balmaseda et al 2013 interpolated to the model grid while for the second forecast system the ocean initial conditions are taken from an assimilation run which has been nudged to the oras4 reanalysis both forecast systems use atmosphere and land initial conditions from the era interim reanalysis dee et al 2011 and sea ice initial conditions from an ocean and sea ice coupled simulation forced by the drakkar forcing set v4 3 brodeau et al 2010 five members are generated for each start date from perturbations of the initial ocean and atmosphere conditions du et al 2012 the forecast duration is one year and the observational references are taken from the drakkar forcing set v5 2 reanalysis dussin et al 2014 this study considers the north pacific region 10s 60 e 100e 250 e 3 3 all the monthly mean files used in the example can be downloaded from www bsc es projects earthscience s2dverification s2dv example files tar the verification case study consists of the following steps loading and visualizing area averaged surface temperatures over the north pacific from a set of forecasts and references computing and visualizing climatologies and anomalies computing and visualizing the correlation and rmse between the forecasts and references loading two dimensional subsets of the data over the north pacific computing and visualising the modes of variability of the target data sets computing and visualising the spatial correlations and brier scores 3 1 data retrieval first we construct lists containing the location of the data sets to be considered the directory structure of the considered data files is represented in appendix b the path component must contain a character string with the path patterns to the files of the corresponding data set the names surrounded by symbols are wildcards that load will replace automatically with the appropriate value additional details can be found in load image 1 the load command can now be called by specifying the variable of interest the desired data sets the region of interest the start dates and the forecast time steps of interest here the area averages are requested with the output option image 2 the object returned by load contains two arrays one for the forecast data d a t a m o d and the other for the corresponding reference data d a t a o b s with a dimension structure specific to s2dverification 4 4 this object can be downloaded directly from www bsc es projects earthscience s2dverification s2dv example data rdata image 3 before processing the data we can first visually inspect the time series of the raw data using plotano image 4 from fig 2 the forecast anomalies appear to be in broad agreement with the references from which we can infer the data both experimental and reference have been correctly retrieved and we can now proceed with the processing and verification 3 2 processing climatologies can be computed and plotted with clim and plotclim respectively clim computes per pair climatologies by default since by default plotclim assumes that the forecasts start in january the actual initial month in our experiments november needs to be specified with the parameter monini image 5 each line in fig 3 shows the climatology of one member a single climatology of the ensemble mean could be obtained by setting the parameter m e m b f a l s e in clim we can then obtain the anomalies with the function ano which subtracts the climatologies from each start date and member of the raw data the anomalies can be plotted with plotano see fig 4 image 6 by default all the forecasts started at different dates contribute to the per pair climatology and when computing the anomaly of one of the start dates its contribution to the climatology is subtracted in order to avoid taking into account this contribution thus mimicking as closely as possible an operational context in which the forecast data is not used for the estimation of the climatology the function a n o c r o s s v a l i d automatically computes a climatology for each start date without taking that start date into account and subtracts it from the original data image 7 3 3 verification to measure the skill of the two forecast systems pearsons correlation coefficient and the rmse of the ensemble mean are computed and the results are plotted using plotvsltime figs 5 and 6 as in plotclim the initial month can be specified with monini image 8 in order to compute modes of variability and or spatial correlations we use two dimensional fields using the load function the data sets are interpolated onto a common t 106 gaussian grid the coarsest grid among the considered data sets with cdo s distance weighted method image 9 the returned forecast and observational reference data now has two additional spatial dimensions 5 5 this r object can be downloaded directly from www bsc es projects earthscience s2dverification s2dv example map data rdata image 10 the modes of variability can be computed with the eof function which automatically applies area weighting to the input data by default the first 10 modes are provided they can be plotted with plotequimap for example the second mode of the reference shows a dipole between alaska and northern japan fig 7 image 11 the index of this mode can be computed for each ensemble member by projecting the anomaly map on the mode of variability with projectfield and the results can be displayed with plotboxwhisker as in fig 8 reference indices are available in the eof outputs image 12 spatial correlations and confidence intervals at all forecast times and start dates for all experiments are computed via acc the results are plotted for experiment a with plotacc as shown in fig 9 image 13 as well as the spatial correlation maps of the temporal correlation can be plotted with the plotequimap function as follows image 14 the results are shown in fig 10 plotstereomap could also have been used to produce the same plots but with a stereographic projection the ensemble forecast can be converted into probabilistic forecasts by binning and counting the forecast values this conversion is done automatically in the ultimatebrier function which can compute the standard brier score and its decomposition after binning the forecasts into terciles image 15 we can see from figs 10 and 11 that both forecasts show similar spatial variability however in general experiment a outperforms experiment b with the former having higher correlations and lower briers scores for most regions the region with the most skill in both experiments is 5 s 5 n 190 240 e also known as the niño 3 4 region 4 conclusions this paper has introduced s2dverification an r package for streamlining and simplifying the forecast verification process in climate research and other disciplines its key features have been described including a powerful data loading and homogenization function functions that transparently implement well known methods for pre processing and verifying climate data novel verification and synthetic forecast generation methods and functions to generate frequently required visual products the stages of data retrieval pre processing computation of scores and visualisation of data and results have been briefly described and put into context together with an explanation of the typical challenges encountered and the corresponding mechanisms and functions the package provides to deal with these challenges and a short review of the relevant technical aspects given the proliferation of verification packages it seems likely and desirable that their developers will have to make a concerted effort to converge in the near future either by agreeing on a set of conventions and good practices to make the various tools compatible with all forecast data structures or by creating a single scalable and sustainable verification framework there are several further developments planned for s2dverification the data retrieval module is currently being extended to support data sets stored in additional file schemes and additional file formats and with temporal resolutions other than daily or monthly as well as data sets with start dates at the sub daily level also the verification scores and pre processing functions are being adapted to work on multi core platforms for the mid term future the package is being extended to trace the steps followed from the beginning to the end of the verification process to ensure the reproducibility of results in terms of visualisation the time series and map plotting functions are being unified and extended with additional options finally the overall usability of the framework is being improved to make some technical details more transparent to the user and to efficiently handle large forecast data sets distributed on cluster platforms taking into consideration existing solutions in raster sp zoo ff and other packages acknowledgements the research leading to these results has received funding from the eu seventh framework programme fp7 2007 2014 under grant agreements 308378 specs 607085 eucleia 603521 preface and 308291 euporias and from the copernicus climate change services c3s contract 2016 c3s 51 lot3 bsc sc1 qa4seas o bellprat s contract has been financed by the european space agency under the living planet fellowship as part of the project veritas cci software availability software s2dverification description the s2dverification software is freely available as an r package a comprehensive framework for forecast verification functions are provided for loading and processing the forecast and reference data calculating verification statistics and visualizing the results main developers v guemas and n manubens source language r availability https cran r project org web packages s2dverification index html appendix a verification statistics throughout this section x i represents the forecast value for reference y i for i 1 n total references root mean square error rmse the square root of the mean of the squared differences the rmse is a measure of the distance between the forecast and the references i e the forecast accuracy 1 r m s e i 1 n x i y i 2 n root mean square skill score rmsss a skill score based on the rmse the rmsss compares the rmse of the forecasts with that of the climatology at a fixed location 2 r m s s s 100 n i 1 n 1 x ˆ i x i 2 x x i 2 brier score the mean square differences between the forecast probability x i and reference probability y i the bs takes values between 0 and 1 with lower values corresponding to more accurate forecasts for further information and a decomposition of the brier score see ferro and fricker 2012 3 b s i 1 n x i y i 2 appendix b use case directory tree in the case study the data files are distributed as depicted in the following directory tree image 16 these files need to be netcdf 3 4 compliant and contain the variable tas fulfilling the guidelines detailed in load 
26406,from 1990 the agricultural production systems simulator apsim has grown from a field focused farming systems framework used by a small number of people into a large collection of models used by thousands of modellers internationally the software grew to consist of several hundred thousand lines of code in multiple programming languages this has led to a large complex software ecosystem that is difficult to maintain in addition systems modellers increasingly require software systems that integrate multiple disciplines can represent evermore complex farming systems can run on multiple operating systems desktop web mobile can operate at or be adjusted to multiple temporal and spatial scales field farm region continent global and run faster for larger simulation analyses this is difficult to achieve in an aging framework for these reasons the apsim initiative is building the next generation of apsim this manuscript outlines the approach taken and lessons learnt keywords model framework agile software apsim agricultural systems model software availability name agricultural production systems simulator apsim developer apsim initiative email apsim daf qld gov au year first available 1994 hardware required any recent pc with a minimum of 2 gb of ram 32 or 64 bit operating systems supported windows 7 or higher linux based systems with mono 4 8 or higher osx with mono 4 8 or higher availability www apsim info cost free for non commercial use 1 introduction apsim holzworth et al 2014 is a suite of models used by researchers to simulate a wide range of complex agricultural systems it contains interconnected biophysical and management models to simulate systems comprising soil crop tree pasture and livestock processes and has the flexibility to integrate non biological farm resources such as water storages and farm machinery agricultural modelling has expanded in scope over the last decade holzworth et al 2015 at its inception in the early 1990s apsim was a point based model with a limited range of soil and crop models that were used primarily for improving land management decisions at a field level mccown et al 1996 1995 over the intervening years apsim has evolved into a framework containing over 80 models of soil and crop processes that are used together in simulation analyses that go far beyond the original envisaged problem domain today modellers are using apsim for investigating agricultural soil sustainability processes luo et al 2014 2011 modelling horticultural cropping systems brown et al 2011 huth et al 2009 modelling agroforestry systems huth et al 2014 luedeling et al 2016 evaluating resource use and efficiency hochman et al 2009a hunt et al 2013 hunt and kirkegaard 2011 kirkegaard and hunt 2010 environmental characterisation chauhan et al 2013 chenu et al 2011 zheng et al 2012 providing farmer advice hochman et al 2009b mccown et al 2009 yield gap assessments carberry et al 2013 hochman et al 2016 2012 van rees et al 2014 informing plant breeding programs hammer et al 2010 messina et al 2011 hypothetical trait modification lilley and kirkegaard 2016 2011 manschadi et al 2006 zheng et al 2013 climate change and adaptation analyses teixeira et al 2012 thorburn et al 2012 wang et al 2011 understanding drivers of production and environmental effects of grazed pastures cichota et al 2013 snow and white 2013 snow et al 2018 understanding interactions between livestock and mixed crop livestock enterprises bell et al 2009 lilley et al 2015 lilley and moore 2009 moore et al 2009 whole farm modelling approaches brennan et al 2008 rodriguez et al 2014 snow et al 2014 investigating biotic and abiotic system constraints whish et al 2015 continental and sub continental scale analyses elliott et al 2014 zhao et al 2013 in addition to the expanding scope the computing landscape has also changed significantly initially apsim was built using fortran 77 with scientists designing and coding much of the models themselves and interfacing with the model via control and output files apsim has evolved since then with additional programming languages being supported software engineers doing most of the development work under guidance of scientific domain experts a graphical user interface created for configuring running and visualising simulation results and other new capabilities being continually added and new models incorporated so that they run natively in apsim changes have been occurring consistently over the life of apsim but resources to support this development have been limited and the gradual evolution has resulted in a source code base in various combinations of fortran c c vb tcl and pascal that is complex and difficult to maintain difficult to achieve operating system independence particularly the user interface slow execution time documentation that is out of date some models that lack formal test and validation simulations a software engineering process that has produced inadequate testing systems and untidy code overcoming these problems with limited resources is difficult over the last decade the team has deliberately diverted resources from developing new models into fixing the above list of challenges with some successes ultimately though the rapid change in computing hardware and software technologies and a move to more distributed environments combined with user s demands has led the team to consider an alternative to the iterative evolutionary style of development a predominately fortran c code base was not well suited to running on mobile devices web and cloud based portals or behind web services newer programming languages are better suited to a mobile and web based environment while the team could have continued to evolve apsim to a new language over time it was decided to begin development of a replacement system that addressed all the challenges outlined above by porting the science not the software of the existing models to a new redesigned framework in addition to this the team have learnt many lessons about developing robust and efficient models from several decades of model development in rapidly changing environments the team has relearnt many of these several times and have seen other agricultural modelling teams face similar issues this paper discusses these lessons in a general way in the next section and follows that with two sections that discuss how these lessons have been applied to the development of a next generation of apsim an open source agricultural modelling platform freely available for non commercial use 2 lessons learnt from many years of apsim development the apsim software team has maintained a whiteboard of lessons learnt over many years of apsim development when a lesson was relearnt many times it was added to the whiteboard this section outlines some of them most of these have been adapted from agile software development processes and have formed the basis for design aspects of the next generation of apsim 2 1 don t reinvent the wheel modellers and software developers often make the mistake of building a completely new model function widget rather than adapting an existing one there is a tendency in the modelling community to build yet another crop model of a particular type e g wheat even though the better strategy is usually to adapt an existing model sometimes this is because phd postdoctoral students and scientists new to model development need to build their own model to become a model developer and be part of the model development club being the owner of a model also brings recognition in the industry these are valid reasons to build a new model but it leads to a multitude of models and tools which are often not supported beyond the life of the project our preferred strategy is to use an iterative style of development and evolve code to new capability 2 2 you can t teach old code new tricks once a code base ages and evolves there comes a point where it becomes increasingly difficult to refactor the code to do new things this is particularly the case when insufficient attention and resources have been invested in refactoring and cleaning the code during this evolutionary process sometimes new requirements on the code are sufficient to force a rewrite for example trying to have fortran code run behind a web service or on a mobile device would require some form of wrapping which would be difficult if not impossible while we prefer an iterative style of development sometimes it becomes necessary to rewrite the code knowing when this point is reached can be difficult to determine 2 3 modern languages and frameworks were developed for a reason newer languages and their frameworks have capabilities that help solve many of the issues outlined in this paper for example java and the net languages are capable of reflection the ability for executable code to examine many aspects of its own structure at runtime this can be used to produce models that can document themselves and removes the need to have hard coded links to other models the net framework can also compile and execute additional code at run time which can be used instead of the in house built apsim manager scripting model having features like a runtime compiler built into the development environment negates the need to reproduce this functionality by building custom precompiled code 2 4 if writing documentation was easy people would have done it already writing documentation that is clear concise and correctly reflects the model or system that it is documenting is difficult keeping that documentation current with modifications or improvements is also difficult for example there is a long history of poor documentation in many agricultural models traditionally it requires a concerted effort by a team of people to keep the documentation up to date and most fail at this this of course takes valuable resources away from development of new models and features being able to automatically document models as much as possible helps to alleviate many of these issues 2 5 testing is important it is critical that tests are automatically executed whenever a change is made to the source code it is easy for a developer to inadvertently introduce defects into a system writing tests early in the development lifecycle helps reduce the time spent fixing defects later beck 2003 2000 and having them automatically executed improves the chances that defects will be detected early duvall et al 2007 many models in apsim were validated while the associated journal paper was being prepared but these data sets were not added to the apsim test suite without automated testing it is difficult to determine the validity of the model after many years of iterative development it isn t possible to be sure that a model is performing as expected without regularly executing tests being smarter about the way tests are written is also important tests should range in scale from testing individual functions through to inter model communications when a model changes it is critical that its performance measured against observed data is compared against the previously accepted performance to do this the new predicted observed statistics should be compared against the previous accepted statistics within allowable tolerances a change in performance is then easily detected 2 6 keep it simple modellers and developers find simplicity difficult to achieve the tendency can be to construct over engineered systems in the expectation that the new functionality will be required at some point in the future in a large percentage of cases the functionality will never be needed and the effort to build it will have been wasted kanat alexander 2012 as an example of this a complex automated build and test system was designed and created for apsim where the need for distributing the build and test process across multiple computers was envisaged the need never eventuated and the team is left with the legacy of having to maintain a complex piece of code as another example porting the fortran generic plant model code base to c was initially straight forward however altering it to incorporate a complex inheritance hierarchy introduced unneeded complexity over time more and more complexity was added resulting in a large code base that greatly hindered the team s ability to understand the code 2 7 smaller simpler code usually executes faster to make code execute more quickly make it do less usually a piece of software will run more quickly when the processor executes less code the apsim common modelling protocol moore et al 2007 is a good example of a lot of code being executed to facilitate inter model communication the protocol defines how models send and receive values of variables packing and unpacking values as needed while being useful at the time by allowing models written in different programming languages to communicate removing this overhead in the next generation of apsim has significantly improved the speed of apsim 3 description of the next generation of apsim 3 1 software platforms faster execution speed and cross platform development were key requirements from the outset as well as the ability to run the models and user interface on windows linux and osx desktop and cluster architectures in addition the advent of mobile platforms and web based development required the running of models not the user interface behind web pages and on mobile devices this is enabled by clearly separating the user interface from the models the net skills of the software team was the foundation for using c and the mono framework for cross platform development initially the team attempted to use windows forms for the user interface development but the user interface didn t perform well on linux or osx machines the decision was made to migrate to gtk for cross platform user interface development producing a user experience that is almost identical across the three operating systems the initial decision to use a model view presenter pattern for constructing the user interface significantly helped to migrate from windows forms to gtk adopting a single language framework net removes the need for complex coupling between code written in different languages java was also considered as an obvious approach but this would have required reskilling the development team ultimately the decision was made to continue using familiar tools and languages the team knows the technology and has discovered its limitations the decision was also made to support multiple development ide s visual studio community edition sharp develop and mono develop allowing developers to work on their operating system of choice significant work has gone into cross platform support with the apsim next generation models and user interface now working on windows linux and mac osx 3 2 improved execution speed execution speed of apsim has always been an issue for users with large analyses apsim is increasingly being used for large continental and global analyses where a large number of points are executed in parallel on high performance clusters elliott et al 2014 zhao et al 2013 and multi field implementations snow et al 2018 much of the existing runtime of apsim version 7 x and earlier is spent on the inter model protocol for passing variables and events between models moore et al 2007 each individual variable value passed between models of which there are many hundreds each daily time step is packed to and unpacked from a binary structure this strategy is necessary for managing the multi language nature of the existing apsim where the sub models are loosely coupled and where they were built by several different developers in a range of different languages however this isn t necessary when a single programming language is used and the whole codebase is contained in a single build environment models in the new framework declare their data input dependencies using a link reflection metadata tag this dependency is then resolved by the infrastructure at runtime by looking for field declarations with this tag finding the matching data elsewhere in the simulation and setting the field to point to the matching data this is often called dependency injection seemann 2011 this decouples models because the models don t know nor care where the data originates from it is the responsibility of the infrastructure to connect models with their input data this allows models of the same type e g two water balance models to be easily interchanged holzworth et al 2010 once the infrastructure has resolved the link dependency a one time operation at simulation start up the models can call each other directly when needed with no infrastructure overhead this has resulted in a significantly improved execution speed seven times quicker than the older apsim version 3 3 model construction and visualisation to improve the transparency of model development and reduce the development time a mechanism was needed to allow scientists to construct and configure plant and soil models in a visual way icon based tools like vensim smith et al 2005 and simile muetzelfeldt and massheder 2003 have been explored as a possible option for constructing models for non programmers these tools however have proved cumbersome when describing large complex models moving to a finer model granularity partially alleviated this problem but debugging and determining the flow of execution proved difficult what was needed was a way to construct and parameterise a model visually by aggregating smaller processes and functions into larger models while maintaining the ability to debug execution the plant modelling framework pmf was conceived to provide this functionality brown et al 2014 the pmf allows scientists to construct plant models from smaller building blocks often without coding this is undertaken graphically in the user interface using drag and drop to assemble a model from smaller processes and functions and then entering scalar values to parameterise the functions as an example fig 1 shows development of two models maize and wheat in the user interface parameterising the extinction coefficient k that is used in the calculation of radiation interception monsi and saeki 2005 in the leaf organ for maize a multiply function is employed to multiply a potential extinction coefficient potentialextinctioncoeff by a water stress value waterstressimpact the figure shows a graph for the potential extinction coefficient as a function of maize row spacing to provide a hedge row effect for the wheat model the extinction coefficient is expressed using a constant function with a value of 0 5 this demonstrates how very different relationships can be described by the modeller without the need of any source code changes processes can be swapped in and out to construct very different types of plant models with this approach a crop model can be constructed more quickly with a significant reduction in the amount of source code that needs to be maintained by the software team while we have made it trivial for software developers to try different approaches we only advocate approaches that are physiologically justified and scientifically sound this approach is currently being extended for other types of models for example soil and tree models the existing user interface in apsim is generally accepted as being easy to use intuitive and sufficiently flexible for a wide range of scenario analyses the same look and feel was adopted in the next generation of apsim with a hierarchical view of models and simulations with a right hand panel showing the properties of the selected model this decision was intentional to minimise retraining and maintain continuity with the existing versions however the user interface has been enhanced in many ways for example to allow a user to describe field experiments in terms of their experimental design fig 2 shows an experiment configuration of two factors population 3 5 7 9 plants m2 and irrigation rate wet and dry this experiment facility is sufficiently flexible to allow a large permutation of simulations to be described which can also be used to gauge model sensitivity or model performance 3 4 increased manager script flexibility another strength of apsim was the ability to write scripts to mimic on farm management practices moore et al 2014 this has been retained and extended users can write scripts in c or vb net compiled at run time by the net framework that sow and harvest crops perform tillage actions fertilise and irrigate etc once the script has been written a user interface view is automatically produced allowing the user to parameterise the script using simple edit boxes this by default hides the scripts complexity from users while still providing access to the script should they want to see understand or amend it scripts can also be used to control the user interface load files click on models etc which is important for automated testing of the user interface externalising the description of farm management to a scripting language provides a great deal of flexibility for the modeller allowing them to very precisely describe field level management decisions and create scenario analyses with complete freedom using a modern language for scripting provides many new capabilities over the existing apsim implementation e g classes looping methods array handling these scripts can then easily be shared amongst users greatly extending the capability of apsim the scripting facility can also allow a user to prototype simple models or perform specific actions within the simulation 3 5 automatic documentation to overcome the problems of maintaining documentation all science model documentation in the next generation of apsim is automatically generated from the validation and test simulations and by using reflection to extract comments from the source code by having models constructed from smaller units each unit can be documented individually by inspecting the source code and parameterisation the result is a formatted pdf that describes the components of the model including graphs of many of the individual functions and the validation datasets and graphs fig 3 shows two excerpts from a maize pdf that was generated the first part of all documents is a description of the model this contains descriptive text with references equations and graphs the second part of each document is a suite of graphs and associated statistics showing model performance apsim next generation uses databases to store outputs from simulations and collates these alongside observed data this allows queries and filters using sql to be quickly run enabling the development of more robust and flexible graphing tools within the user interface this in turn allows developers to produce graphs of simulated output with observed data and associated statistics thereby facilitating quicker model validation 4 the software development process the apsim development team is distributed geographically so a process was needed that coordinated the team s efforts the team has adopted a forking github feature branch workflow and applied an off the shelf product jenkins https jenkins ci org as the build test and continuous integration system when a developer wishes to modify or create new functionality in apsim they fork make a copy of the apsim repository master branch into their own remote github repository they then clone this repository to their computer doing this allows them to work independently of other developers when they begin work on a new feature or defect fix they create a branch derived from the master branch give it a name that is appropriate to their work and implement their changes fig 4 describes the process for a model developer to incorporate their changes back into apsim developers commit to the repository on their computer as often as required when they are ready to share their changes with others or incorporate their changes into apsim they push their changed files to their repository on github and raise a pull request this is a signal that they would like their changes to be reviewed and automatically tested by jenkins a software developer will examine the changes ensuring that it meets style guidelines and that it won t have unintended consequences for users if the change is a major science change then it will also be reviewed by a scientist in a similar way to a peer reviewed journal article to ensure that the changes are scientifically valid if the jenkins testing passes and the peer reviews indicate that the changes are acceptable an administrator of the repository merges the change github will then instruct the continuous release system to make an installation available to users as an upgrade option each installation has a unique version number that can be cited in publications and users are free to choose what versions they want to use for given projects github is also a collaboration tool developers can pull changes from another developer and push any additional modifications back to the developer this facilitates peer review of code and models without cumbersome exchanges of files and associated metadata all models released in apsim next generation must include a validation set containing test simulations graphs and statistics that show the model performance against the observed data the jenkins automated test procedure runs these validations re generates the statistics and compares these against accepted statistics if there is a reduction in the performance of any aspect of a model jenkins registers a fail and the changes will not be merged into a release of apsim the developer must then review their code changes to ensure they are not causing the changed behaviour before their changes will be merged into the master branch and made available in a release through the continuous integration system there are also pass fail unit tests and sensibility tests when there is a lack of observed data for a particular model holzworth et al 2011 this level of testing helps to detect unintended changes to a model s performance unexpected impacts of one model on another and tests for stability issues when jenkins compiles and tests apsim it also constructs a software installation package this makes new versions of apsim immediately available to users who no longer need to wait for periodic releases this greatly shortens the time for the delivery of bug fixes to users statistics on apsim usage are also managed as all user upgrades are recorded in a database much of the apsim development process has been significantly influenced by agile software principles the agile software manifesto http agilemanifesto org values individuals and interactions over processes and tools working software over comprehensive documentation customer collaboration over contract negotiation and responding to change over following a plan we have adopted various practices from the scrum and extreme programming frameworks including sprints 1 4 week efforts to implement specific features models pair programming two developers sitting at a single computer to develop a feature continuous integration and delivery test first development and refactoring these have all helped to improve the robustness stability and quality of apsim next generation 5 discussion conclusion a new generation of apsim was released in october 2014 following construction of a proof of concept this initial version had two crop models and was a long way from being able to replace the existing version of apsim since this early version new models have been added and the infrastructure has been refined the development team are continually looking for ways to improve various parts of the infrastructure to increase run time speed and make the auto generated documentation more useful and readable real gains have also been made in the way models are built there is now an expectation that more comprehensive validation data sets are provided before a model is incorporated into a release and there are many tools graphs maps and statistics to help create these validation datasets this helps to ensure that only high quality models with a good validation are included in the release gathering the necessary experimental data and building the necessary graphs and associated statistics for a validation is time consuming often taking months years there is now a lot more scrutiny by the apsim community on the performance and documentation of the apsim models as compared with apsim 7 x looking at multiple variables across many experiments and in doing so finding issues than need resolving before incorporation in a release ultimately though the additional time spent building testing and documenting models will generate models that are of a higher standard that are more transparent to users in the way they work multiple point simulations present another challenge currently multiple simulations are executed concurrently due to their independent nature but multiple points within a simulation are executed sequentially this is due to dependencies between the points for example the multi point capability may be used to represent fields of a farm where a field is dependent on an adjacent field in terms of water movement or management decisions with an increase in whole farm modelling livestock and agroforestry systems analysis there is an increasing need for better ways of representing and executing multi point simulations much of the poor runtime in apsim 7 x is related to the way models communicate currently this happens at a sub daily level with models retrieving their dependent variables many times during a time step and multiple methods are called to perform part of their calculations for the time step doing this means that models can t be run asynchronously due to the multiple dependency points if instead a model is given values of dependent variables by the infrastructure and a model s calculations are executed in a single method without relying on other models independent design then asynchronous execution is possible this is the approach taken in the design of the environmental modelling framework object modelling system david et al 2013 an alternative to changing the inter model communication is to make all models stateless that is the classes that make up the models contain no state variables instead a model is given the values of its dependant variables an execute function is called and the results of the calculations are returned to be stored elsewhere this is the approach taken in the design of bioma donatelli et al 2014 2012 indeed stateless programming is widely used in web programming it is a requirement for restful web services and is used in parts of the java and net frameworks this approach is being investigated for adoption within apsim to improve the design of models in apsim the software process that we have adopted is an improvement over the apsim 7 x process and while it came with a steep learning curve for some is effective at coordinating a distributed development team the process is working well and leads to quicker release of defect fixes documentation that is more accurate and includes up to date model performance graphs and statistics using the new process has been a challenge for all model developers particularly those who aren t software developers so we are looking for ways of making it simpler for them an important lesson that we have learnt is that the time between making changes to source code and the time it takes to be merged into the master branch on github should be minimised as much as possible poor practice is to take many months to build a model and merge it into the master once at the end of the development by this time there is a good chance that the infrastructure and supporting libraries will have been modified from when the code was branched from the repository this would now require a large merge effort to bring the model up to date if large model development tasks are broken into small units and merged regularly at least daily the task of merging is greatly simplified this can be a challenge for some developers though who aren t used to small development iterations we need to engage the apsim development community to look for ways of overcoming this challenge moving to an open source platform in 2007 had a profound positive impact on the size of the apsim user community while some modellers were initially concerned about a perceived lack of control over what is incorporated into apsim the peer review system has meant that the apsim initiative maintains full control having the ability to manage changes being incorporated into the release the move to open source has encouraged developers to modify and enhance apsim and has made it attractive for organisations outside the founding agencies to collaborate with the apsim initiative as evidence of this table 1 provides some github development metrics for the next generation of apsim one of the success stories is how the apsim community of users and developers continues to grow ensuring apsim s longevity even after many of the foundation members have moved on to other things or have retired we did not throw away many years of development when we commenced working on the new framework instead we captured the ideas of apsim 7 x that worked well and re implemented the infrastructure for today s world of web and mobile development as it can be seen from this manuscript there is still much to do and there are several challenges that we are still working on 
26406,from 1990 the agricultural production systems simulator apsim has grown from a field focused farming systems framework used by a small number of people into a large collection of models used by thousands of modellers internationally the software grew to consist of several hundred thousand lines of code in multiple programming languages this has led to a large complex software ecosystem that is difficult to maintain in addition systems modellers increasingly require software systems that integrate multiple disciplines can represent evermore complex farming systems can run on multiple operating systems desktop web mobile can operate at or be adjusted to multiple temporal and spatial scales field farm region continent global and run faster for larger simulation analyses this is difficult to achieve in an aging framework for these reasons the apsim initiative is building the next generation of apsim this manuscript outlines the approach taken and lessons learnt keywords model framework agile software apsim agricultural systems model software availability name agricultural production systems simulator apsim developer apsim initiative email apsim daf qld gov au year first available 1994 hardware required any recent pc with a minimum of 2 gb of ram 32 or 64 bit operating systems supported windows 7 or higher linux based systems with mono 4 8 or higher osx with mono 4 8 or higher availability www apsim info cost free for non commercial use 1 introduction apsim holzworth et al 2014 is a suite of models used by researchers to simulate a wide range of complex agricultural systems it contains interconnected biophysical and management models to simulate systems comprising soil crop tree pasture and livestock processes and has the flexibility to integrate non biological farm resources such as water storages and farm machinery agricultural modelling has expanded in scope over the last decade holzworth et al 2015 at its inception in the early 1990s apsim was a point based model with a limited range of soil and crop models that were used primarily for improving land management decisions at a field level mccown et al 1996 1995 over the intervening years apsim has evolved into a framework containing over 80 models of soil and crop processes that are used together in simulation analyses that go far beyond the original envisaged problem domain today modellers are using apsim for investigating agricultural soil sustainability processes luo et al 2014 2011 modelling horticultural cropping systems brown et al 2011 huth et al 2009 modelling agroforestry systems huth et al 2014 luedeling et al 2016 evaluating resource use and efficiency hochman et al 2009a hunt et al 2013 hunt and kirkegaard 2011 kirkegaard and hunt 2010 environmental characterisation chauhan et al 2013 chenu et al 2011 zheng et al 2012 providing farmer advice hochman et al 2009b mccown et al 2009 yield gap assessments carberry et al 2013 hochman et al 2016 2012 van rees et al 2014 informing plant breeding programs hammer et al 2010 messina et al 2011 hypothetical trait modification lilley and kirkegaard 2016 2011 manschadi et al 2006 zheng et al 2013 climate change and adaptation analyses teixeira et al 2012 thorburn et al 2012 wang et al 2011 understanding drivers of production and environmental effects of grazed pastures cichota et al 2013 snow and white 2013 snow et al 2018 understanding interactions between livestock and mixed crop livestock enterprises bell et al 2009 lilley et al 2015 lilley and moore 2009 moore et al 2009 whole farm modelling approaches brennan et al 2008 rodriguez et al 2014 snow et al 2014 investigating biotic and abiotic system constraints whish et al 2015 continental and sub continental scale analyses elliott et al 2014 zhao et al 2013 in addition to the expanding scope the computing landscape has also changed significantly initially apsim was built using fortran 77 with scientists designing and coding much of the models themselves and interfacing with the model via control and output files apsim has evolved since then with additional programming languages being supported software engineers doing most of the development work under guidance of scientific domain experts a graphical user interface created for configuring running and visualising simulation results and other new capabilities being continually added and new models incorporated so that they run natively in apsim changes have been occurring consistently over the life of apsim but resources to support this development have been limited and the gradual evolution has resulted in a source code base in various combinations of fortran c c vb tcl and pascal that is complex and difficult to maintain difficult to achieve operating system independence particularly the user interface slow execution time documentation that is out of date some models that lack formal test and validation simulations a software engineering process that has produced inadequate testing systems and untidy code overcoming these problems with limited resources is difficult over the last decade the team has deliberately diverted resources from developing new models into fixing the above list of challenges with some successes ultimately though the rapid change in computing hardware and software technologies and a move to more distributed environments combined with user s demands has led the team to consider an alternative to the iterative evolutionary style of development a predominately fortran c code base was not well suited to running on mobile devices web and cloud based portals or behind web services newer programming languages are better suited to a mobile and web based environment while the team could have continued to evolve apsim to a new language over time it was decided to begin development of a replacement system that addressed all the challenges outlined above by porting the science not the software of the existing models to a new redesigned framework in addition to this the team have learnt many lessons about developing robust and efficient models from several decades of model development in rapidly changing environments the team has relearnt many of these several times and have seen other agricultural modelling teams face similar issues this paper discusses these lessons in a general way in the next section and follows that with two sections that discuss how these lessons have been applied to the development of a next generation of apsim an open source agricultural modelling platform freely available for non commercial use 2 lessons learnt from many years of apsim development the apsim software team has maintained a whiteboard of lessons learnt over many years of apsim development when a lesson was relearnt many times it was added to the whiteboard this section outlines some of them most of these have been adapted from agile software development processes and have formed the basis for design aspects of the next generation of apsim 2 1 don t reinvent the wheel modellers and software developers often make the mistake of building a completely new model function widget rather than adapting an existing one there is a tendency in the modelling community to build yet another crop model of a particular type e g wheat even though the better strategy is usually to adapt an existing model sometimes this is because phd postdoctoral students and scientists new to model development need to build their own model to become a model developer and be part of the model development club being the owner of a model also brings recognition in the industry these are valid reasons to build a new model but it leads to a multitude of models and tools which are often not supported beyond the life of the project our preferred strategy is to use an iterative style of development and evolve code to new capability 2 2 you can t teach old code new tricks once a code base ages and evolves there comes a point where it becomes increasingly difficult to refactor the code to do new things this is particularly the case when insufficient attention and resources have been invested in refactoring and cleaning the code during this evolutionary process sometimes new requirements on the code are sufficient to force a rewrite for example trying to have fortran code run behind a web service or on a mobile device would require some form of wrapping which would be difficult if not impossible while we prefer an iterative style of development sometimes it becomes necessary to rewrite the code knowing when this point is reached can be difficult to determine 2 3 modern languages and frameworks were developed for a reason newer languages and their frameworks have capabilities that help solve many of the issues outlined in this paper for example java and the net languages are capable of reflection the ability for executable code to examine many aspects of its own structure at runtime this can be used to produce models that can document themselves and removes the need to have hard coded links to other models the net framework can also compile and execute additional code at run time which can be used instead of the in house built apsim manager scripting model having features like a runtime compiler built into the development environment negates the need to reproduce this functionality by building custom precompiled code 2 4 if writing documentation was easy people would have done it already writing documentation that is clear concise and correctly reflects the model or system that it is documenting is difficult keeping that documentation current with modifications or improvements is also difficult for example there is a long history of poor documentation in many agricultural models traditionally it requires a concerted effort by a team of people to keep the documentation up to date and most fail at this this of course takes valuable resources away from development of new models and features being able to automatically document models as much as possible helps to alleviate many of these issues 2 5 testing is important it is critical that tests are automatically executed whenever a change is made to the source code it is easy for a developer to inadvertently introduce defects into a system writing tests early in the development lifecycle helps reduce the time spent fixing defects later beck 2003 2000 and having them automatically executed improves the chances that defects will be detected early duvall et al 2007 many models in apsim were validated while the associated journal paper was being prepared but these data sets were not added to the apsim test suite without automated testing it is difficult to determine the validity of the model after many years of iterative development it isn t possible to be sure that a model is performing as expected without regularly executing tests being smarter about the way tests are written is also important tests should range in scale from testing individual functions through to inter model communications when a model changes it is critical that its performance measured against observed data is compared against the previously accepted performance to do this the new predicted observed statistics should be compared against the previous accepted statistics within allowable tolerances a change in performance is then easily detected 2 6 keep it simple modellers and developers find simplicity difficult to achieve the tendency can be to construct over engineered systems in the expectation that the new functionality will be required at some point in the future in a large percentage of cases the functionality will never be needed and the effort to build it will have been wasted kanat alexander 2012 as an example of this a complex automated build and test system was designed and created for apsim where the need for distributing the build and test process across multiple computers was envisaged the need never eventuated and the team is left with the legacy of having to maintain a complex piece of code as another example porting the fortran generic plant model code base to c was initially straight forward however altering it to incorporate a complex inheritance hierarchy introduced unneeded complexity over time more and more complexity was added resulting in a large code base that greatly hindered the team s ability to understand the code 2 7 smaller simpler code usually executes faster to make code execute more quickly make it do less usually a piece of software will run more quickly when the processor executes less code the apsim common modelling protocol moore et al 2007 is a good example of a lot of code being executed to facilitate inter model communication the protocol defines how models send and receive values of variables packing and unpacking values as needed while being useful at the time by allowing models written in different programming languages to communicate removing this overhead in the next generation of apsim has significantly improved the speed of apsim 3 description of the next generation of apsim 3 1 software platforms faster execution speed and cross platform development were key requirements from the outset as well as the ability to run the models and user interface on windows linux and osx desktop and cluster architectures in addition the advent of mobile platforms and web based development required the running of models not the user interface behind web pages and on mobile devices this is enabled by clearly separating the user interface from the models the net skills of the software team was the foundation for using c and the mono framework for cross platform development initially the team attempted to use windows forms for the user interface development but the user interface didn t perform well on linux or osx machines the decision was made to migrate to gtk for cross platform user interface development producing a user experience that is almost identical across the three operating systems the initial decision to use a model view presenter pattern for constructing the user interface significantly helped to migrate from windows forms to gtk adopting a single language framework net removes the need for complex coupling between code written in different languages java was also considered as an obvious approach but this would have required reskilling the development team ultimately the decision was made to continue using familiar tools and languages the team knows the technology and has discovered its limitations the decision was also made to support multiple development ide s visual studio community edition sharp develop and mono develop allowing developers to work on their operating system of choice significant work has gone into cross platform support with the apsim next generation models and user interface now working on windows linux and mac osx 3 2 improved execution speed execution speed of apsim has always been an issue for users with large analyses apsim is increasingly being used for large continental and global analyses where a large number of points are executed in parallel on high performance clusters elliott et al 2014 zhao et al 2013 and multi field implementations snow et al 2018 much of the existing runtime of apsim version 7 x and earlier is spent on the inter model protocol for passing variables and events between models moore et al 2007 each individual variable value passed between models of which there are many hundreds each daily time step is packed to and unpacked from a binary structure this strategy is necessary for managing the multi language nature of the existing apsim where the sub models are loosely coupled and where they were built by several different developers in a range of different languages however this isn t necessary when a single programming language is used and the whole codebase is contained in a single build environment models in the new framework declare their data input dependencies using a link reflection metadata tag this dependency is then resolved by the infrastructure at runtime by looking for field declarations with this tag finding the matching data elsewhere in the simulation and setting the field to point to the matching data this is often called dependency injection seemann 2011 this decouples models because the models don t know nor care where the data originates from it is the responsibility of the infrastructure to connect models with their input data this allows models of the same type e g two water balance models to be easily interchanged holzworth et al 2010 once the infrastructure has resolved the link dependency a one time operation at simulation start up the models can call each other directly when needed with no infrastructure overhead this has resulted in a significantly improved execution speed seven times quicker than the older apsim version 3 3 model construction and visualisation to improve the transparency of model development and reduce the development time a mechanism was needed to allow scientists to construct and configure plant and soil models in a visual way icon based tools like vensim smith et al 2005 and simile muetzelfeldt and massheder 2003 have been explored as a possible option for constructing models for non programmers these tools however have proved cumbersome when describing large complex models moving to a finer model granularity partially alleviated this problem but debugging and determining the flow of execution proved difficult what was needed was a way to construct and parameterise a model visually by aggregating smaller processes and functions into larger models while maintaining the ability to debug execution the plant modelling framework pmf was conceived to provide this functionality brown et al 2014 the pmf allows scientists to construct plant models from smaller building blocks often without coding this is undertaken graphically in the user interface using drag and drop to assemble a model from smaller processes and functions and then entering scalar values to parameterise the functions as an example fig 1 shows development of two models maize and wheat in the user interface parameterising the extinction coefficient k that is used in the calculation of radiation interception monsi and saeki 2005 in the leaf organ for maize a multiply function is employed to multiply a potential extinction coefficient potentialextinctioncoeff by a water stress value waterstressimpact the figure shows a graph for the potential extinction coefficient as a function of maize row spacing to provide a hedge row effect for the wheat model the extinction coefficient is expressed using a constant function with a value of 0 5 this demonstrates how very different relationships can be described by the modeller without the need of any source code changes processes can be swapped in and out to construct very different types of plant models with this approach a crop model can be constructed more quickly with a significant reduction in the amount of source code that needs to be maintained by the software team while we have made it trivial for software developers to try different approaches we only advocate approaches that are physiologically justified and scientifically sound this approach is currently being extended for other types of models for example soil and tree models the existing user interface in apsim is generally accepted as being easy to use intuitive and sufficiently flexible for a wide range of scenario analyses the same look and feel was adopted in the next generation of apsim with a hierarchical view of models and simulations with a right hand panel showing the properties of the selected model this decision was intentional to minimise retraining and maintain continuity with the existing versions however the user interface has been enhanced in many ways for example to allow a user to describe field experiments in terms of their experimental design fig 2 shows an experiment configuration of two factors population 3 5 7 9 plants m2 and irrigation rate wet and dry this experiment facility is sufficiently flexible to allow a large permutation of simulations to be described which can also be used to gauge model sensitivity or model performance 3 4 increased manager script flexibility another strength of apsim was the ability to write scripts to mimic on farm management practices moore et al 2014 this has been retained and extended users can write scripts in c or vb net compiled at run time by the net framework that sow and harvest crops perform tillage actions fertilise and irrigate etc once the script has been written a user interface view is automatically produced allowing the user to parameterise the script using simple edit boxes this by default hides the scripts complexity from users while still providing access to the script should they want to see understand or amend it scripts can also be used to control the user interface load files click on models etc which is important for automated testing of the user interface externalising the description of farm management to a scripting language provides a great deal of flexibility for the modeller allowing them to very precisely describe field level management decisions and create scenario analyses with complete freedom using a modern language for scripting provides many new capabilities over the existing apsim implementation e g classes looping methods array handling these scripts can then easily be shared amongst users greatly extending the capability of apsim the scripting facility can also allow a user to prototype simple models or perform specific actions within the simulation 3 5 automatic documentation to overcome the problems of maintaining documentation all science model documentation in the next generation of apsim is automatically generated from the validation and test simulations and by using reflection to extract comments from the source code by having models constructed from smaller units each unit can be documented individually by inspecting the source code and parameterisation the result is a formatted pdf that describes the components of the model including graphs of many of the individual functions and the validation datasets and graphs fig 3 shows two excerpts from a maize pdf that was generated the first part of all documents is a description of the model this contains descriptive text with references equations and graphs the second part of each document is a suite of graphs and associated statistics showing model performance apsim next generation uses databases to store outputs from simulations and collates these alongside observed data this allows queries and filters using sql to be quickly run enabling the development of more robust and flexible graphing tools within the user interface this in turn allows developers to produce graphs of simulated output with observed data and associated statistics thereby facilitating quicker model validation 4 the software development process the apsim development team is distributed geographically so a process was needed that coordinated the team s efforts the team has adopted a forking github feature branch workflow and applied an off the shelf product jenkins https jenkins ci org as the build test and continuous integration system when a developer wishes to modify or create new functionality in apsim they fork make a copy of the apsim repository master branch into their own remote github repository they then clone this repository to their computer doing this allows them to work independently of other developers when they begin work on a new feature or defect fix they create a branch derived from the master branch give it a name that is appropriate to their work and implement their changes fig 4 describes the process for a model developer to incorporate their changes back into apsim developers commit to the repository on their computer as often as required when they are ready to share their changes with others or incorporate their changes into apsim they push their changed files to their repository on github and raise a pull request this is a signal that they would like their changes to be reviewed and automatically tested by jenkins a software developer will examine the changes ensuring that it meets style guidelines and that it won t have unintended consequences for users if the change is a major science change then it will also be reviewed by a scientist in a similar way to a peer reviewed journal article to ensure that the changes are scientifically valid if the jenkins testing passes and the peer reviews indicate that the changes are acceptable an administrator of the repository merges the change github will then instruct the continuous release system to make an installation available to users as an upgrade option each installation has a unique version number that can be cited in publications and users are free to choose what versions they want to use for given projects github is also a collaboration tool developers can pull changes from another developer and push any additional modifications back to the developer this facilitates peer review of code and models without cumbersome exchanges of files and associated metadata all models released in apsim next generation must include a validation set containing test simulations graphs and statistics that show the model performance against the observed data the jenkins automated test procedure runs these validations re generates the statistics and compares these against accepted statistics if there is a reduction in the performance of any aspect of a model jenkins registers a fail and the changes will not be merged into a release of apsim the developer must then review their code changes to ensure they are not causing the changed behaviour before their changes will be merged into the master branch and made available in a release through the continuous integration system there are also pass fail unit tests and sensibility tests when there is a lack of observed data for a particular model holzworth et al 2011 this level of testing helps to detect unintended changes to a model s performance unexpected impacts of one model on another and tests for stability issues when jenkins compiles and tests apsim it also constructs a software installation package this makes new versions of apsim immediately available to users who no longer need to wait for periodic releases this greatly shortens the time for the delivery of bug fixes to users statistics on apsim usage are also managed as all user upgrades are recorded in a database much of the apsim development process has been significantly influenced by agile software principles the agile software manifesto http agilemanifesto org values individuals and interactions over processes and tools working software over comprehensive documentation customer collaboration over contract negotiation and responding to change over following a plan we have adopted various practices from the scrum and extreme programming frameworks including sprints 1 4 week efforts to implement specific features models pair programming two developers sitting at a single computer to develop a feature continuous integration and delivery test first development and refactoring these have all helped to improve the robustness stability and quality of apsim next generation 5 discussion conclusion a new generation of apsim was released in october 2014 following construction of a proof of concept this initial version had two crop models and was a long way from being able to replace the existing version of apsim since this early version new models have been added and the infrastructure has been refined the development team are continually looking for ways to improve various parts of the infrastructure to increase run time speed and make the auto generated documentation more useful and readable real gains have also been made in the way models are built there is now an expectation that more comprehensive validation data sets are provided before a model is incorporated into a release and there are many tools graphs maps and statistics to help create these validation datasets this helps to ensure that only high quality models with a good validation are included in the release gathering the necessary experimental data and building the necessary graphs and associated statistics for a validation is time consuming often taking months years there is now a lot more scrutiny by the apsim community on the performance and documentation of the apsim models as compared with apsim 7 x looking at multiple variables across many experiments and in doing so finding issues than need resolving before incorporation in a release ultimately though the additional time spent building testing and documenting models will generate models that are of a higher standard that are more transparent to users in the way they work multiple point simulations present another challenge currently multiple simulations are executed concurrently due to their independent nature but multiple points within a simulation are executed sequentially this is due to dependencies between the points for example the multi point capability may be used to represent fields of a farm where a field is dependent on an adjacent field in terms of water movement or management decisions with an increase in whole farm modelling livestock and agroforestry systems analysis there is an increasing need for better ways of representing and executing multi point simulations much of the poor runtime in apsim 7 x is related to the way models communicate currently this happens at a sub daily level with models retrieving their dependent variables many times during a time step and multiple methods are called to perform part of their calculations for the time step doing this means that models can t be run asynchronously due to the multiple dependency points if instead a model is given values of dependent variables by the infrastructure and a model s calculations are executed in a single method without relying on other models independent design then asynchronous execution is possible this is the approach taken in the design of the environmental modelling framework object modelling system david et al 2013 an alternative to changing the inter model communication is to make all models stateless that is the classes that make up the models contain no state variables instead a model is given the values of its dependant variables an execute function is called and the results of the calculations are returned to be stored elsewhere this is the approach taken in the design of bioma donatelli et al 2014 2012 indeed stateless programming is widely used in web programming it is a requirement for restful web services and is used in parts of the java and net frameworks this approach is being investigated for adoption within apsim to improve the design of models in apsim the software process that we have adopted is an improvement over the apsim 7 x process and while it came with a steep learning curve for some is effective at coordinating a distributed development team the process is working well and leads to quicker release of defect fixes documentation that is more accurate and includes up to date model performance graphs and statistics using the new process has been a challenge for all model developers particularly those who aren t software developers so we are looking for ways of making it simpler for them an important lesson that we have learnt is that the time between making changes to source code and the time it takes to be merged into the master branch on github should be minimised as much as possible poor practice is to take many months to build a model and merge it into the master once at the end of the development by this time there is a good chance that the infrastructure and supporting libraries will have been modified from when the code was branched from the repository this would now require a large merge effort to bring the model up to date if large model development tasks are broken into small units and merged regularly at least daily the task of merging is greatly simplified this can be a challenge for some developers though who aren t used to small development iterations we need to engage the apsim development community to look for ways of overcoming this challenge moving to an open source platform in 2007 had a profound positive impact on the size of the apsim user community while some modellers were initially concerned about a perceived lack of control over what is incorporated into apsim the peer review system has meant that the apsim initiative maintains full control having the ability to manage changes being incorporated into the release the move to open source has encouraged developers to modify and enhance apsim and has made it attractive for organisations outside the founding agencies to collaborate with the apsim initiative as evidence of this table 1 provides some github development metrics for the next generation of apsim one of the success stories is how the apsim community of users and developers continues to grow ensuring apsim s longevity even after many of the foundation members have moved on to other things or have retired we did not throw away many years of development when we commenced working on the new framework instead we captured the ideas of apsim 7 x that worked well and re implemented the infrastructure for today s world of web and mobile development as it can be seen from this manuscript there is still much to do and there are several challenges that we are still working on 
26407,we systematically evaluated the effect of model complexity and calibration strategy on estimated recharge using four varyingly complex models and a unique long term recharge data set a differential split sample test was carried out by using six calibration periods with climatically contrasting conditions in a constrained monte carlo approach all models performed better during calibration than during validation due to differences in model structures and climatic conditions the two more complex physically based models predicted the observed recharge with relatively small uncertainties even when calibration and prediction periods had different climatic conditions in contrast the more simplistic soil water balance model significantly underestimated the recharge rates the fourth semi mechanistic model captured the observed recharge rates but with a larger uncertainty range than the physically based models our results may have relevant implications for a broad range of applications when recharge models are used as decision making tools keywords groundwater recharge model complexity calibration period split sample test contrasted climate droughts 1 introduction groundwater recharge is one of the main drivers in hydrogeological and hydrological systems bakker et al 2013 understanding the relationship between groundwater recharge rates and climatic conditions is essential for sustainable water resource management typically hydrogeological models are applied to gain insight into this relationship and to generate recharge predictions to inform decision makers meixner et al 2016 the selection of a recharge model that is suitable for robust recharge predictions under site specific climatic conditions is however often subjective and might be biased by the common practice of the modeller kurylyk and macquarrie 2013 vansteenkiste et al 2014 this selection can be critical as the model structure and parametrization can strongly affect the quality of the simulations breuer et al 2009 li et al 2015 velazquez et al 2013 for instance moeck et al 2016 evaluated a variety of different recharge models in a climate change impact study and concluded that the selected model complexity can lead to significant model bias in the predictions for hydrological models butts et al 2004 indicated that model bias due to variation in model complexity is an issue for both lumped and physically based models consequently utilizing only one model for hydrograph predictions ignores the possible uncertainty associated with the model structure doll and fiedler 2008 although sophisticated calibration tools can result in an optimal fit between model simulations and observations for the calibration period doherty 2003 moeck et al 2015 zambrano bigiarini and rojas 2013 it is indispensable to have solid understanding of the reliability of model predictions beyond the calibration period apart from the model structure and parametrization implicit assumptions in the calibration approach are additional sources of uncertainty for instance many hydrological predictions are based on the assumption that the model calibration based on historical time periods is similarly valid for the prediction period seibert 2003 the assumption of stationarity however is not always true especially under changing climatic conditions non stationarity of model parameters can occur suggesting that certain historic time periods might be more useful for the identification of the parameter space while others might be less informative li et al 2012 vaze et al 2010 wagener et al 2003 as it was shown for rainfall runoff modelling studies extreme climatic periods such as heat waves or heavy precipitation events can have a strong impact on the predictions and increase the uncertainties of the predictions coron et al 2012 merz et al 2011 seiller et al 2012 cuthbert and tindimugaya 2010 demonstrated that different groundwater recharge models simulated comparable historic recharge rates but the models responded very differently to changes in precipitation intensity in a climate change impact study crosbie et al 2011 concluded that the recharge model uncertainty is the smallest source of uncertainty compared to different global climate models or the downscaling methods however the effects of the calibration strategy and observation period as potential sources of uncertainty were not considered in the study of crosbie et al 2011 it can be speculated that model recharge predictions were not always consistent due to the chosen calibration strategy and period even though the models performed similarly for the calibration period with regard to hydrological models for impact studies brigode et al 2013 emphasized that the model reliability should be enhanced through developing calibration strategies that increase model performance and robustness under dissimilar climatic conditions in that regard kirchner 2006 proposed that prediction models should be tested with a more comprehensive and incisive validation method such as the differential split sample test with such a test the model is calibrated and validated on time periods with very contrasting climatic and or hydrological conditions klemes 1986 for instance vaze et al 2010 performed differential split sample tests on four different rainfall runoff models and found that models calibrated over wet periods generally tended to predict runoff incorrectly over a dry period brigode et al 2013 also compared two rainfall runoff models in a split sample test to evaluate the model robustness and parameter uncertainty the authors found that the major source of uncertainty and lacking robustness of the models occurred for climatic conditions that were very different from the calibration period coron et al 2012 developed a generalized split sample test methodology to verify all possible combinations during the calibration validation period the authors indicated that the transferability of model parameters can introduce significant errors in the model predictions which has strong implications in all model applications the differential split sample test is still rarely applied for model testing andréassian et al 2009 and if so it is typically used for rainfall runoff models for the systematic evaluation of groundwater recharge models this method has to the best of our knowledge not yet been applied we speculate that such an analysis might be precluded by the lack of long term measurements of recharge rates required for model calibration at the catchment scale groundwater recharge cannot be measured experimentally scanlon et al 2002 scanlon et al 2006 von freyberg et al 2015 so that the only direct measurements of vertical groundwater recharge at the plot scale can be obtained from large lysimeters groh et al 2016 however long time series of vertical groundwater recharge that cover a wide range of climatic conditions are generally rare in this study we aim to address this research gap by utilizing a unique long term data set from the large lysimeter in the rietholzbach research catchment in switzerland gurtz et al 2003 seneviratne et al 2012 lysimeter seepage measurements from 1976 until today allow us to systematically evaluate how groundwater recharge predictions are affected by i the type of model structure ii the parameterization and iii the used calibration periods in the first part of our study we systematically compare different model structures through five different model performance criteria and methods such as taylor plots and post calibrated uncertainty analysis for the sum of annual recharge for the entire simulation period we investigate how different parameterizations of models due to parameter non identifiability influence the predictions by applying a monte carlo approach subsequently we perform differential split sample tests to investigate the relationship between the model performance and robustness and the choice of the calibration period 2 study area and groundwater recharge models 2 1 rietholzbach lysimeter the large free draining weighting lysimeter 2 5 m deep 2 m diameter is located in the rietholzbach research catchment a pre alpine head watershed of the thur river basin in north eastern switzerland fig 1 a the lysimeter was constructed in 1975 and is mainly filled with gley brown soil from the same location gurtz et al 2003 the lysimeter surface is covered with grass to imitate the surrounding conditions at the bottom of the lysimeter column outflow is measured with a tipping bucket fig 1b as groundwater table depths are generally shallow at the site typically less than 5 m beneath surface but below plant root depth lysimeter seepage is assumed to be a reliable indicator of actual vertical groundwater recharge ghasemizade et al 2015 von freyberg et al 2015 observed lysimeter seepage also correlates well with the streamflow signal of the rietholzbach river indicating that recharge processes at the plot scale are representative for the 3 14 km2 large catchment seneviratne et al 2012 in the present study we use daily lysimeter seepage as a surrogate for vertical groundwater recharge with this we follow vereecken et al 2015 who argue that an improved description of soil hydrological fluxes at the local scale are fundamental to reduce the large uncertainties which are still present in large scale models used to predict these fluxes 2 2 groundwater recharge models we compared four groundwater recharge models that differ in terms of model complexity all four models have already been applied successfully in recharge studies worldwide these models are a linear soil water balance model finch finch 1998 2001 moeck et al 2016 von freyberg et al 2015 a lumped semi mechanistic model lumprem meeks et al 2017 moeck et al 2016 watson et al 2013 and a physically based model hydrus 1d simunek et al 2005 the hydrus 1d model was applied with two different complexities homogenous single porosity model hydrushom van genuchten 1980 and dual porosity model hydrusmac durner 1994 the number of parameters of each model depends on the complexity of recharge process description table 1 and si table s1 all models were run at daily time steps and recharge simulations were aggregated to weekly sums forcing functions and model dimensions were based on the data from 1992 to 2012 of the rietholzbach lysimeter before 1992 measurement gaps at the meteorological station occurred and therefore calculating potential evapotranspiration using the penman montheith equation after allen et al 2005 was not possible because snowpack drainage is assumed to affect groundwater recharge patterns meeks et al 2017 precipitation inputs in winter and spring were transferred into snow water equivalent and melt water volumes using temperature index modelling based on the procedure described in walter et al 2005 the seasonal and annual patterns of precipitation potential evapotranspiration and lysimeter seepage are provided in fig s1 in the electronic supplementary material esm the following sections provide a brief overview of the recharge models from simple to complex model structure 2 2 1 soil water balance model finch in the soil water balance model referred to as finch model direct recharge from a homogeneous soil column was calculated using a simple daily water balance equation where only matrix flow was considered 1 r i p i e ai i i b i δs i where r i is groundwater recharge l t p i is precipitation l t e ai is actual evaporation l t i i is canopy interception loss l t b i is flow bypassing the soil water storage l t and δs i is change in soil water storage l t recharge occurs when the soil moisture content exceeds field capacity von freyberg et al 2015 upward movement of water e g capillary flow is neglected in the finch model water uptake from plant roots occurs at a time varying rate that is limited by water stress the root zone for the finch model was divided into four layers following the standard approach for grass covered soil presented in finch 1998 for a more comprehensive description of the model see finch 2001 1998 2 2 2 lumped parameter bucket model lumprem this lumped parameter bucket model referred to as lumprem model provides a basic representation of the unsaturated zone including matrix and macropore flow matrix and macropore flow are activated after specified delay times macro pore recharge takes place when the soil becomes saturated lumprem calculates the actual evapotranspiration taking soil moisture storage and plant parameters into account water is lost from the soil column as unsaturated vertical flow the soil moisture content in the soil column controls the recharge rate as well as the hydraulic conductivity a decrease in soil moisture lowers the hydraulic conductivity following the concept of the water retention curve the governing equation of the lumprem model reads 2 r k s υ l 1 1 υ 1 m m 2 where r l t is the recharge rate k s l t is the saturated hydraulic conductivity υ the volume of the bucket storage l is tortuosity and m determines the shape of relationship between the recharge rate and soil moisture storage note that the model parameters have no physical meaning more detailed information about the lumprem model can be found in i e watson et al 2013 and meeks et al 2017 2 2 3 physically based model hydrus 1d homogenous a 1d physically based vertical flow model referred to as hydrushom model of a 250 cm deep soil column with 202 layers was created with the software suite hydrus simunek et al 2005 the discretization was 0 7 cm at the ground surface and increased linearly to 1 5 cm in 100 cm depth for the remaining depth a constant value of 1 5 cm was used this fine discretization of the upper part of the soil column allowed for a numerical solution with reasonable accuracy for recharge processes vogel and ippisch 2008 it further enabled to incorporate vertically variable distribution of evapotranspiration rates saturated hydraulic conductivities were estimated with rosetta a program for pedo transfer functions schaap et al 2001 based on soil surveys in close vicinity of the rietholzbach lysimeter von freyberg et al 2015 these local estimates were used as initial conditions for the model parameter calibration the functions of soil water retention and hydraulic conductivity are described in mualem 1976 and van genuchten 1980 the hydraulic boundary condition of the bottom of the soil column was represented by a seepage face with no flow when the boundary was unsaturated and a fixed atmospheric pressure head once saturation was reached grass rooting depth was set at 25 cm seneviratne et al 2012 root water uptake was simulated for grass cover following the model presented in feddes et al 1974 2 2 4 physically based model hydrus 1d dual porosity the most complex model referred to as hydrusmac model in this study follows the same conceptualization of the underlying physical processes described for the hydrushom model apart from a dual porosity formulation this approach assumes that the porous medium can be divided into two overlapping soil domains durner 1994 where both domains are based on the van genuchten 1980 function of soil hydraulic properties 3 s e w 1 1 α 1 h n 1 m 1 w 2 1 α 2 h n 2 m 2 where s e is the effective water content h is the pressure head l w i are the weighting factors and α i 1 l n i and m i are the empirical van genuchten parameters for the two soil domains combining this retention curve model with the mualem 1976 and van genuchten 1980 model leads to 4 k s e k s w 1 s e 1 w 2 s e 2 l w 1 α 1 1 1 s e 1 l m 1 m 1 w 2 α 2 1 1 s e 2 l m 2 m 2 2 w 1 α 1 w 2 α 2 2 where w i are the weighting factors for the sub curves of the overlapping subregions k s is the saturated hydraulic conductivity l t and k the hydraulic conductivity l t as a function of s e 3 methodology 3 1 step 1 identification of climatically contrasting calibration periods the systematic evaluation of the four groundwater recharge models was carried out in four steps which are visualized in fig 2 and explained in detail below as a first step climatically contrasting calibration periods were identified we applied two different metrics to identify dissimilar calibration periods the palmer drought severity index pdsi and lysimeter seepage measurements meteorological data and lysimeter data from january 1992 to december 2012 were used during which climatic conditions from very wet to very dry periods occurred the pdsi which is commonly used for characterizing meteorological droughts palmer 1968 was calculated in this study with the matlab code provided by jacobi et al 2013 required input variables were mean monthly precipitation air temperature latitude of the study area and the local available water capacity of the soil ficklin et al 2015 precipitation inputs were adjusted for snow water equivalent following the procedure described in walter et al 2005 pdsi values between 0 5 and 0 5 represent conditions near normal whereas values below 4 and above 4 indicate severe drought or very wet conditions respectively alley 1984 as a second metric for wet and dry conditions monthly sums of observed lysimeter seepage were used there is no general guideline for the minimum length of the required calibration period perrin et al 2007 according to singh and bárdossy 2012 it is very difficult to precisely define what length of observation data is sufficient to identify model parameters in the calibration brigode et al 2013 found that calibration periods longer than three years do not generate more robust simulations and predictions for rainfall runoff models based on modelling results for different flow characteristics based on the pdsi and observed lysimeter recharge in our study we found two years an appropriate length for the calibration period because the contrasts in the different calibration periods generally decrease with increasing length of the time periods the chosen six two year calibration periods are characterized by high hydro climatic variability sorted from wet to dry conditions 1994 1995 wet wet 2002 2003 wet dry 2005 2006 dry avg 2010 2011 avg dry 2003 2004 dry dry and 2003 2011 dry dry fig 2 and table 2 the years 2003 and 2011 were chosen for the dry dry calibration period because an extreme summer drought occurred in 2003 while 2011 was affected by a spring drought staudinger et al 2015 thus the information content and climatic variability from the calibration period is likely to be higher than by using years where dry conditions only occurred during the summer season wet periods with several high intensity precipitation events or a significant accumulation of snow occurred e g in 1995 and in 2002 at the rietholzbach site average annual values of precipitation actual evapotranspiration and lysimeter seepage between 1992 and 2012 were 1473 mm 560 mm and 1029 mm respectively table 2 the deviations of the six calibration periods from the mean annual precipitation and potential evapotranspiration range from 25 to 17 and from 13 to 20 respectively the relative differences in lysimeter seepage range between 35 and 44 3 2 step 2 model calibration within null space monte carlo analysis each groundwater recharge model was calibrated with the parameter estimation software pest doherty 2016 against cumulative weekly lysimeter seepage for each of the six calibration periods a 365 days warm up phase i e year prior of calibration period was included in every calibration to provide valid initial conditions for all simulations our calibration workflow included several steps the model was calibrated once against the weekly recharge rates from the different calibration periods the initial model parameter values were based on the prior expert knowledge see also si table s1 subsequently parameters belonging to the solution and null space were identified based on singular value decomposition decomposition of parameters took place on the basis of the jacobian matrix which is the matrix of sensitivities of all model outputs corresponding to observations from the calibration dataset to all adjustable parameters the solution space dimensionality was calculated based on the ratio of highest to lowest squared singular value ratio which is defined to be not larger than 5 107 doherty 2016 thus due to the different calibration periods with different information content the number of parameters in the solution space can vary si table s2 then a calibration constrained monte carlo analysis tonkin and doherty 2009 was performed to provide important information about model parameter uncertainty and consequently in recharge simulation here 300 different random parameter sets ps were generated for each model and calibration period in total 7200 simulations from 300 ps six calibration periods and four recharge models random log normal parameters were generated based on the provided upper and lower parameter range as well as on computed means based on existing parameter values obtained through the first calibration subsequently random parameter sets were modified through null space projection first the difference was taken between each random parameter set and the calibration parameter set this difference was then projected onto the calibration null space and the projected difference was re added to the calibration parameter set typically due to the non linearity of the models parameter sets did not result in perfectly calibrated models and re calibration is required here the calibration took place through alterations of parameters in solution space whereas the null space was unadjusted the calibration efficiency with the different ps was evaluated with the target function 6 m i n i 1 n w i r o i r m i 2 where w is the weight assigned to lysimeter recharge r and the indices o and m represent observed and simulated weekly values at the time step i respectively in our study equal weights were used for all observations for each model although we are fully aware that the calibration is more strongly affected by high recharge rates however this weighting strategy is most commonly applied in modelling exercises foglia et al 2009 moeck et al 2015 poeter and hill 1997 it further avoids user subjectivity in the optimization approach rosolem et al 2012 the constrained monte carlo analysis method increases the efficiency of the calibration procedure by reducing the computational burden that is often related to monte carlo approaches and thus typically restricts such an approach especially for physically based models in our study only solution space parameters were adjusted to lower the objective function moreover the first iteration of each calibration process was based on the computed parameter sensitivities using the first calibration parameter set the first iteration of each calibration therefore required only a single model run a maximum of two iterations was applied in the calibration exercise to reduce the computational demand the calibrated model parameters and standard deviation for each groundwater recharge model can be found in table s3 a more comprehensive description of the method can be found in tonkin and doherty 2009 optimal ps were chosen based on a defined target function threshold t h r e s h o l d that was used for all models and all calibration periods we assumed that the accuracy of the lysimeter recharge measurements ε is constant and equal to 5 of the recharge amount ghasemizade et al 2015 only if the calculated target function eq 6 is smaller or equal than t h r e s h o l d equation 7 the ps was accepted 7 t h r e s h o l d i 1 n r o i ε 2 8 p s a c c p e t e d y e s t h r e s h o l d n o n t h r e s h o l d this procedure ensured a reasonable comparison between the different recharge models it is important to note that the selected optimal ps for each model can lead to different numbers of accepted model ps note that the number of acceptable ps for each model might increase if more than two iterations are applied 3 3 step 3 model simulation with different parameter sets ps and calibration periods simulations were carried out for the time period 1993 2012 similar to the calibration methodology a one year warm up phase year 1992 365 days for recharge simulations was used 3 4 step 4 evaluation of model performances based on different recharge characteristics the effects of model complexity and calibration periods on model performance were analysed through various performance criteria and methods namely i taylor plot taylor 2001 ii annual distribution of residuals iii model bias against the climate differences between calibration and validation period iv a comparison of observed and simulated recharge deficit volumes for each year of the simulation period and v post calibrated uncertainty analysis for the sum of annual recharge for the entire simulation period the application of multiple performance criteria is generally recommended because a single criterion might only evaluate specific aspects of model robustness and performance bennett et al 2013 the taylor plot i provides a graphical summary of how closely simulated recharge rate patterns match the observations orth et al 2015 the similarity between temporal patterns was quantified by using their correlation their normalized root mean square differences and the amplitude of their variations represented by their standard deviations a ranking of recharge residuals ii was carried out on an annual time scale subsequently the sensitivity of recharge predictions to the calibration period and transferability to dissimilar conditions iii were conducted in order to evaluate model performance under very dry climatic conditions iv we calculated the deficit volume of recharge v x following the framework of drought characteristics in hydrological systems tallaksen and van lanen 2004 we used the 20th percentile of the weekly recharge time series as a static threshold x 0 and calculated the difference between x 0 and simulated recharge note that x 0 is different for each model as it is based on the simulated weekly recharge rates finally v post calibration uncertainty analysis was performed for the sum of annual recharge for the entire simulation period using the genlinpred program from the pest suite doherty 2016 we computed post calibrated predictive uncertainties as well as uncertainty reductions in our predictions between pre and post calibrated parameters the calibrated uncertainty variance σ s 2 and standard deviation σ s σ s 2 were calculated based on the following equation christensen and doherty 2008 9 σ s 2 y t c p y y t c p x t x c p x t c ε 1 x c p y where the model parameters span the vector p and the variability of p is described by the covariance matrix c p the sensitivity of our prediction s to parameters p is represented by the vector y the covariance matrix c p of innate parameter variability which can also be seen as the pre calibrated uncertainty is supplied through an uncertainty file the file contains information about prior knowledge the covariance matrix c ε comprise the measurement noise expressed by the measurement weights measurements are assumed to be proportional to the standard deviation of noise however in most cases model to measurement misfit is an outcome of structural noise i e the imperfection of the model itself rather than measurement noise alone this effect can never be identified exactly and therefore an underestimation of predictive uncertainty will commonly occur the first term in eq 9 is the pre calibration uncertainty of the prediction james et al 2009 it is solely a function of innate parameter variability c p and the dependence of our prediction on model parameter sensitivity y the second term expresses the reduction in predictive uncertainty through the conditioning effect i e calibration of historical observation christensen and doherty 2008 this can be used to determine the relative efficacy of various existing observations or periods in reducing the predictive uncertainty the reduction of predictive uncertainty was demonstrated shortly for the six different calibration periods for the applied models note that eq 9 does not require actual parameter or observation values solely parameter and observation sensitivities are used to calculate pre and post calibrated uncertainty james et al 2009 moeck et al 2015 moore and doherty 2005 although the method depends on linear assumptions some studies demonstrated that the outcome from linear and non linear uncertainty methodologies were very similar e g brunner et al 2012 james et al 2009 a detailed description and derivation of the method is given elsewhere doherty 2016 doherty and hunt 2009 moore and doherty 2005 4 results 4 1 performance analysis calibration the following section describes model simulation results for the calibration period 2003 2004 dry dry as an example for the other five calibration periods which are presented in the supplementary material si figs s2 s6 and table s5 all calibrated recharge models reproduced the observed weekly time series well despite considerably different model structures fig 3 the best agreement between ensemble mean and measured lysimeter recharge in terms of the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 and r2 was found for the models hydrusmac nse 0 88 r2 0 88 and hydrushom 0 88 0 88 as well as for lumprem 0 83 0 83 the finch soil water balance model 0 68 0 71 simulated highly variable and generally too small recharge rates because of its simplified representation of flow processes in contrast to the finch model recharge rates simulated by the other three models seem more realistic as the retention curve behaviour is implicitly implemented in the model structures different numbers of accepted model realization were obtained through our approach finch 35 ps out of 300 12 lumprem 189 ps out of 300 63 hydrushom 243 ps out of 300 81 and hydrusmac 261 ps out of 300 87 overall the amount of acceptable model ps increased with the degree of model complexity fig 4 depicts the taylor plots for the six calibration periods and the four recharge models for which the number of optimal ps is shown in brackets eq 7 in a taylor plot a perfect fit between observed and simulated weekly recharge values would lead to a value of 1 black point on the x axis in fig 4 all models performed well i e absolute differences in recharge were small for all models except for the finch model correlations between observed and mean ensemble recharge were generally between 0 85 and 0 95 depending on the model structures stochastic realizations and calibration periods the normalized standard deviations were generally close to 1 perfect fit in terms of standard deviation for hydrushom and hydrusmac and around 0 8 for finch and lumprem the normalized root mean square differences were smallest and ranged between 0 4 and 0 6 except for the finch model the hydrushom and hydrusmac performed similarly well due to their similar physical representation of recharge processes with respect to the calibration periods there was no clear tendency of what calibration period resulted in the best model performance during calibration since all calibration periods led to comparable results 4 2 performance analysis validation for recharge simulations over the whole observation period 1993 2012 1992 as warm up phase the taylor plots in fig 5 illustrate a worsening of performance for all models compared to the calibration period although the model performances decreased for all models distinct differences between the models and the calibration periods could be identified a wide range of normalized standard deviations and normalized root mean square differences occurred for the more simplistic finch and lumprem models from all models the finch model showed the poorest performance for most calibration periods with correlations smaller than 0 6 and normalized standard deviations between 0 2 and 1 3 in contrast to the data presented for the model calibration fig 4 the data of the validation period fig 5 further indicate a link between the chosen calibration period and the performance of the models thus a more detailed analysis of the role of the calibration period on the recharge predictions was carried out sections 4 3 and 4 4 4 3 robustness of annual recharge predictions with respect to the climatic conditions of the calibration period patterns of annual residuals ensemble mean of annual simulated recharge minus observed annual recharge were compared through a ranking fig 6 smallest residuals between observed and simulated values highest ranks light blue colour indicate a better model performance under the specific calibration period larger residuals lowest ranks red colour were obtained when the calibration period led to a poorer model performance uncertainties in terms of variability in the annual recharge rates was expressed as the standard deviation for all acceptable ps point size annual residuals for the six different calibration periods were continuously ranked between large residuals rank 1 red colour to smaller residuals rank 6 blue colour annual residuals were generally large for all models with the driest calibration period 2003 2011 dry dry whereas the residuals were smallest with the calibration period 2002 2003 wet dry apart from this trend there is no consistent pattern indicating better or worse model performance over the entire period i e high ranking small standard deviations with rather wet or rather dry calibration periods respectively for instance the wettest calibration period 1994 1995 wet wet resulted in a higher ranking of simulated annual recharge when the model finch was used while much lower ranks were obtained for the model hydrushom similar inconsistent patterns were apparent between the models for the calibration periods 2010 2011 avg dry interestingly rankings high and low were more consistent among all models for those calibration periods that contained the drought year 2003 i e 2002 2003 wet dry 2003 2004 dry dry and 2003 2011 dry dry the previous analysis indicated a relationship between the climatic conditions during the calibration period and the model performance for simulating the whole observation period 1993 2012 1992 as warm up phase therefore we investigated this relationship in more detail fig 7 displays the differences between annual potential et and annual precipitation from the climatic conditions during the calibration the colouring of the circles represents the percentage of bias pbias simulated minus observed value and the size of the circles represents the variability in the results in terms of standard deviations considering solely mean pbias values a diagonal pattern could be observed for all models indeed overestimation of annual recharge occurred when the validation period was wetter than the calibration period whereas underestimation occurred when the validation period was dryer than the calibration period only if the climatic conditions during the calibration and validation periods were similar pbias became small it further seemed that changes in precipitation influenced the pbias more than changes in et which is in line with findings of other studies coron et al 2012 oudin et al 2006 this can be seen when a fixed value for et i e 0 lumprem calibration period 1994 1995 is chosen and the pbias along the x axis is recorded changes in pbias from red colours underestimation to blue colours overestimation occur in contrast fixing a precipitation value i e 0 lumprem calibration period 1994 1995 does not lead to a persistent pattern of pbias on the y axis the largest pbias occurred when precipitation changed more than 20 compared to the calibration period for all models a similar effect of precipitation changes on model performance from a calibration to validation period was also observed by merz et al 2011 for rainfall runoff models 4 4 post calibrated uncertainty analysis simulated total recharge for the entire time period 1993 2012 for the four different models and under the six calibration periods is shown in table 3 the models finch and lumprem underestimated total observed recharge by 20713 mm whereas the ranges of simulated recharge from the two hydrus models comprised the observed value the post calibrated uncertainty standard deviations obtained from the linear uncertainty analysis table 3 were generally larger for the finch and lumprem models compared to the hydrushom and hydrusmac models the hydrushom model showed the smallest ranges of total recharge and respective standard deviations for all models the standard deviations were largest when the calibration period 2005 2006 dry avg was used moreover the calibration period 2002 2003 wet dry led to the largest uncertainty reduction between pre and post calibrated predictive uncertainty for all models the smallest uncertainty reduction occurred for the two hydrus models for the calibration period 2005 2006 dry avg whereas for the finch and lumprem models and the calibration periods 1994 1995 wet wet and 2003 2011 dry dry the uncertainty reductions were smallest for the remaining calibration periods there was no consistent trend indicating better or worse uncertainty reduction which corroborates the observations from section 4 3 moreover the climatic conditions during the calibration period resulted in more variable post calibrated standard deviations and uncertainty reductions for the lumprem and finch models compared to the two hydrus models in addition to the linear uncertainty analysis results from the non linear uncertainty analysis were also available thanks to the calibration constrained monte carlo analysis due to the generated ps for each model under each calibration period a range of total recharge can be observed fig 8 for the finch model the majority of simulations underestimated the total observed recharge of 20713 mm and only a few ps under all calibration periods captured the observed value the underestimation was most pronounced for the dry calibration periods 2010 2011 and 2003 2011 for the lumprem model the observed recharge value was captured under all calibration periods except for the driest 2003 2011 in most cases the observed recharge value was within the 50 range of the simulated recharge similar results were obtained with the two physically based hydrus models that also performed well under the direst calibration period 2003 2011 for all models the calibration period 2002 2003 wet dry led to the best results in terms of simulated recharge and small standard deviation for all models as observed already from the linear uncertainty analysis the standard deviations were generally larger for the finch and lumprem models compared to the two hydrus models our analysis also showed that the standard deviations from both linear and non linear uncertainty analyses were of the same order although small differences had to be expected as a result of the more approximate assumption of the linear uncertainty analysis the fact that both methods led to similar results is in line with findings of previous studies e g brunner et al 2012 james et al 2009 4 5 simulation of recharge during dry climatic conditions meteorological droughts and the associated recharge deficits are of great concern with regard to water resource management lehner et al 2006 thus our model comparison study was also carried out for dry periods for which weekly recharge deficits relative to the 20th percentile of the long term average were calculated fig 2 we aggregated these simulated deficit volumes to annual sums and compared them to the observed deficit volumes based on lysimeter seepage measurements fig 9 uncertainties in terms of variability in the residual recharge deficit volumes are expressed as the standard deviation for all accepted ps i e point size under all calibration periods a consistent pattern emerged where the models finch and lumprem tended to underestimate annual recharge deficit volumes hereby the deviations from the observations were larger for the finch model than for the lumprem model except for the calibration period 2010 2011 avg dry where the lumprem model performed less well both hydrus models showed comparably small deviations from the observed recharge deficit volumes for deficits less than 20 mm for all models the smallest residuals were obtained under the driest calibration periods whereas with the wettest calibration periods e g 1994 1995 residuals became larger this potentially indicates that dry dry calibration periods are most efficient to simulate recharge during drought conditions simulated recharge deficit volumes and thus model performance differentiated more with increasingly dry conditions while the hydrus models still showed an acceptable agreement during drier periods annual recharge deficits more than 20 mm the lumped and soil water balance models were not able to reproduce the most extreme recharge deficit volumes the largest deviations occurred for the drought year 2003 5 discussion in this section we discuss how the structures and parameterizations of the four recharge models influence recharge simulations subsequently we consider the effect of dissimilar climatic conditions in the calibration on the performance of the four models we discuss our findings with respect to previous comparison studies which have focused merely on hydrological models during calibration all models performed acceptably in terms of the selected model performance criteria for all calibration periods however distinct differences during the validation period 1993 2012 among the models suggest that model complexity is a source of bias for recharge predictions our results illustrate that the two more complex physically based models hydrushom and hydrusmac successfully reproduced observed recharge during the validation even when climatic conditions were highly dissimilar during the calibration periods sections 4 3 and 4 4 due to the fact that both models performed similarly well we do not see an advantage by using the more complex model hydrusmac where a larger number of parameters would result in a higher computational demand during the calibration and validation periods compared to the hydrushom model and larger uncertainties however at field sites where macropore flow plays a dominant role for groundwater recharge the hydrusmac model may be the better choice our analysis also showed that the soil water balance finch and lumped lumprem models performed differently for the validation period in comparison to the physically based models especially under very contrasting climatic conditions figs 4 and 5 this behaviour of the simplified models is in line with the findings of dams et al 2015 and moeck et al 2016 both studies demonstrated that the model structure becomes more important for robust recharge predictions under extreme or very contrasting climatic conditions the majority of simulations with the finch model underestimated groundwater recharge the model performed poorest during the dry calibration periods 2010 2011 and 2003 2011 table 3 and fig 8 in contrast the uncertainties were slightly smaller and the sensitivity to the chosen calibration period was lower for the lumprem model overall recharge simulated with lumprem compared well with the observed rates taken into account the faster running times and non convergence difficulties of the lumprem model compared to the physical based hydrus models the lumprem model might be attractive when fast simulations are required at the expense of higher uncertainties for our recharge models we found that the variability of the simulation results was generally smaller during the validation period for the physically based models than for the more simplistic recharge models this suggests that the physical representation of the recharge processes allows for more constrained parameter estimation during the calibration with a physically based model the parameter behaviour follows physical rules and incorporates non linear interrelationships between model parameters and recharge also under extreme climatic conditions moeck et al 2016 our analysis showed that it is still difficult to provide general guidelines on how to identify the optimal calibration period for recharge simulations this is a general problem for modelling applications as shown for streamflow predictions by brigode et al 2013 overall the highest predictive uncertainty reductions and best fits between simulated and observed recharge were obtained for the calibration period 2002 2003 wet dry e g table 3 and fig 8 apart from this calibration period our study suggests that there is no optimal calibration period that allows for equally good simulations with all four recharge models e g fig 6 similar to orth et al 2015 our findings indicate that model performance varies with respect to the hydrological conditions especially for the simplified models finch and lumprem an overestimation in recharge occurs when the validation period is wetter than the calibration period whereas underestimation occurs when the validation period is dryer than the calibration period similar conclusions for hydrological models were drawn based on studies by brigode et al 2013 and coron et al 2012 the recharge models generally performed best when a combined wet and dry year was used for calibration or the validation period was very similar to the calibration period only if the calibration and validation period were quite similar the deviations between observed and simulated values were small moreover the model performance was more influenced by changes in precipitation than by changes in et which is in line with other studies coron et al 2012 hartmann et al 2017 oudin et al 2006 the observed dependency of the recharge simulations on the climatology of the calibration period was however less pronounced for the physically based models we found that this dependency had an effect on the simulation of extremely dry periods the residuals of simulated and observed recharge deficit volumes became smaller when dry periods were used as a calibration period especially for the simplified models this is in line with a hydrological study by vaze et al 2010 who stated that non stationarity of model parameters occurred meaning that some periods in the calibration are more useful for the identification of the parameter space while others might be less informative also li et al 2012 noted that some parameters of hydrological models are particularly sensitive to the climatic conditions of the calibration period the results of our comparison study of recharge models are consistent with those of previous studies focusing on hydrological models we found that physically based models have a greater potential to obtain predictions beyond the range of conditions during calibration and thus are more robust with regard to the chosen calibration period at the expense of higher computational times for calibration and prediction compared to the simplified models the higher computational demands of the physically based models limit however their application at the catchment or global scale thus simplified models often provide the only means to estimate recharge at these spatial scales whichever model is used the predictive uncertainty has to be assessed through uncertainty analysis e g post calibrated uncertainty analysis to clearly communicate the uncertainty limits to stakeholders and decision makers in addition to the chosen model complexity and calibration period model performance can also be influenced by the applied objective target function formulation however this potential effect was not included in our analysis wang and brubaker 2015 demonstrated that a multi objective auto optimization improved the model performance however they also stated that the model structure itself is more important than the set up of the calibration 6 summary and conclusions this study systematically evaluated the effect of model complexity and calibration strategy on simulated recharge rates by utilizing a long term recharge data set a differential split sample test was carried out with four recharge models that vary in terms of model complexity the models were calibrated with six climatically contrasting calibration periods in a constrained monte carlo approach despite the climatically contrasting conditions all models performed similarly well during calibration during validation the more complex physically based models hydrushom hydrusmac predicted recharge with relatively small uncertainty limits even when calibration and prediction periods had very different climatic conditions in contrast the more simplistic soil water balance model finch performed poorer under such conditions during validation especially when the calibration periods contained an extremely dry year the semi mechanistic model lumprem also showed large standard deviations compared to the physically based models the quantified post calibration uncertainty limits did however capture the observed recharge rates taken into account the faster running times and non convergence difficulties compared to the physical based model the lumprem model might be attractive when fast simulations are required overall we found a strong sensitivity of model performance on the chosen calibration period for the more simplified models it is still difficult to provide general guidelines on how to choose an optimal calibration period the smallest standard deviations highest predictive uncertainty reductions and generally best fits between simulated and observed recharge was obtained for the most contrasting calibration period 2002 2003 wet dry for the remaining calibration periods our analysis suggests that there is no optimal calibration period that allows for equally good simulations with all four models however some general tendencies could be observed for all models an overestimation in recharge occurred when the validation period was wetter than the calibration period whereas underestimation occurred when the validation period was dryer than the calibration period the recharge models generally performed best when a combined wet and dry year was used for calibration of if the validation period was very similar to the calibration period the observed dependency of the recharge simulations on the climatology of the calibration period was however less pronounced for the physically based models in this numerical experiment differences between the models and calibration periods could be observed that are expected to occur for various soil types and study areas the results obtained here can have important implications when using recharge models as decision making tools in a broad range of applications e g water availability climate change impact studies water resource management etc our systematic analysis is a preliminary testing of model robustness and parameter transferability and provides a more realistic representation of uncertainty in model based recharge estimations and predictions the approach should be extended and is just one way to provide more insight into model behaviour under dissimilar climatic conditions conflicts of interest none acknowledgments the authors gratefully acknowledge financial assistance provided by the swiss national science foundation snsf projects 200021 129735 1 and 200020 143688 additional funding was provided by the competence center environment and sustainability cces of the eth domain in the framework of the record catchment coupled ecological hydrological and social dynamics in restored and channelized corridors of a river at the catchment scale project we thank the group of s seneviratne land climate dynamics institute for atmospheric and climate science iac swiss federal institute of technology zurich ethz who provided meteorological data and lysimeter seepage time series the from the rietholzbach research catchment appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 005 
26407,we systematically evaluated the effect of model complexity and calibration strategy on estimated recharge using four varyingly complex models and a unique long term recharge data set a differential split sample test was carried out by using six calibration periods with climatically contrasting conditions in a constrained monte carlo approach all models performed better during calibration than during validation due to differences in model structures and climatic conditions the two more complex physically based models predicted the observed recharge with relatively small uncertainties even when calibration and prediction periods had different climatic conditions in contrast the more simplistic soil water balance model significantly underestimated the recharge rates the fourth semi mechanistic model captured the observed recharge rates but with a larger uncertainty range than the physically based models our results may have relevant implications for a broad range of applications when recharge models are used as decision making tools keywords groundwater recharge model complexity calibration period split sample test contrasted climate droughts 1 introduction groundwater recharge is one of the main drivers in hydrogeological and hydrological systems bakker et al 2013 understanding the relationship between groundwater recharge rates and climatic conditions is essential for sustainable water resource management typically hydrogeological models are applied to gain insight into this relationship and to generate recharge predictions to inform decision makers meixner et al 2016 the selection of a recharge model that is suitable for robust recharge predictions under site specific climatic conditions is however often subjective and might be biased by the common practice of the modeller kurylyk and macquarrie 2013 vansteenkiste et al 2014 this selection can be critical as the model structure and parametrization can strongly affect the quality of the simulations breuer et al 2009 li et al 2015 velazquez et al 2013 for instance moeck et al 2016 evaluated a variety of different recharge models in a climate change impact study and concluded that the selected model complexity can lead to significant model bias in the predictions for hydrological models butts et al 2004 indicated that model bias due to variation in model complexity is an issue for both lumped and physically based models consequently utilizing only one model for hydrograph predictions ignores the possible uncertainty associated with the model structure doll and fiedler 2008 although sophisticated calibration tools can result in an optimal fit between model simulations and observations for the calibration period doherty 2003 moeck et al 2015 zambrano bigiarini and rojas 2013 it is indispensable to have solid understanding of the reliability of model predictions beyond the calibration period apart from the model structure and parametrization implicit assumptions in the calibration approach are additional sources of uncertainty for instance many hydrological predictions are based on the assumption that the model calibration based on historical time periods is similarly valid for the prediction period seibert 2003 the assumption of stationarity however is not always true especially under changing climatic conditions non stationarity of model parameters can occur suggesting that certain historic time periods might be more useful for the identification of the parameter space while others might be less informative li et al 2012 vaze et al 2010 wagener et al 2003 as it was shown for rainfall runoff modelling studies extreme climatic periods such as heat waves or heavy precipitation events can have a strong impact on the predictions and increase the uncertainties of the predictions coron et al 2012 merz et al 2011 seiller et al 2012 cuthbert and tindimugaya 2010 demonstrated that different groundwater recharge models simulated comparable historic recharge rates but the models responded very differently to changes in precipitation intensity in a climate change impact study crosbie et al 2011 concluded that the recharge model uncertainty is the smallest source of uncertainty compared to different global climate models or the downscaling methods however the effects of the calibration strategy and observation period as potential sources of uncertainty were not considered in the study of crosbie et al 2011 it can be speculated that model recharge predictions were not always consistent due to the chosen calibration strategy and period even though the models performed similarly for the calibration period with regard to hydrological models for impact studies brigode et al 2013 emphasized that the model reliability should be enhanced through developing calibration strategies that increase model performance and robustness under dissimilar climatic conditions in that regard kirchner 2006 proposed that prediction models should be tested with a more comprehensive and incisive validation method such as the differential split sample test with such a test the model is calibrated and validated on time periods with very contrasting climatic and or hydrological conditions klemes 1986 for instance vaze et al 2010 performed differential split sample tests on four different rainfall runoff models and found that models calibrated over wet periods generally tended to predict runoff incorrectly over a dry period brigode et al 2013 also compared two rainfall runoff models in a split sample test to evaluate the model robustness and parameter uncertainty the authors found that the major source of uncertainty and lacking robustness of the models occurred for climatic conditions that were very different from the calibration period coron et al 2012 developed a generalized split sample test methodology to verify all possible combinations during the calibration validation period the authors indicated that the transferability of model parameters can introduce significant errors in the model predictions which has strong implications in all model applications the differential split sample test is still rarely applied for model testing andréassian et al 2009 and if so it is typically used for rainfall runoff models for the systematic evaluation of groundwater recharge models this method has to the best of our knowledge not yet been applied we speculate that such an analysis might be precluded by the lack of long term measurements of recharge rates required for model calibration at the catchment scale groundwater recharge cannot be measured experimentally scanlon et al 2002 scanlon et al 2006 von freyberg et al 2015 so that the only direct measurements of vertical groundwater recharge at the plot scale can be obtained from large lysimeters groh et al 2016 however long time series of vertical groundwater recharge that cover a wide range of climatic conditions are generally rare in this study we aim to address this research gap by utilizing a unique long term data set from the large lysimeter in the rietholzbach research catchment in switzerland gurtz et al 2003 seneviratne et al 2012 lysimeter seepage measurements from 1976 until today allow us to systematically evaluate how groundwater recharge predictions are affected by i the type of model structure ii the parameterization and iii the used calibration periods in the first part of our study we systematically compare different model structures through five different model performance criteria and methods such as taylor plots and post calibrated uncertainty analysis for the sum of annual recharge for the entire simulation period we investigate how different parameterizations of models due to parameter non identifiability influence the predictions by applying a monte carlo approach subsequently we perform differential split sample tests to investigate the relationship between the model performance and robustness and the choice of the calibration period 2 study area and groundwater recharge models 2 1 rietholzbach lysimeter the large free draining weighting lysimeter 2 5 m deep 2 m diameter is located in the rietholzbach research catchment a pre alpine head watershed of the thur river basin in north eastern switzerland fig 1 a the lysimeter was constructed in 1975 and is mainly filled with gley brown soil from the same location gurtz et al 2003 the lysimeter surface is covered with grass to imitate the surrounding conditions at the bottom of the lysimeter column outflow is measured with a tipping bucket fig 1b as groundwater table depths are generally shallow at the site typically less than 5 m beneath surface but below plant root depth lysimeter seepage is assumed to be a reliable indicator of actual vertical groundwater recharge ghasemizade et al 2015 von freyberg et al 2015 observed lysimeter seepage also correlates well with the streamflow signal of the rietholzbach river indicating that recharge processes at the plot scale are representative for the 3 14 km2 large catchment seneviratne et al 2012 in the present study we use daily lysimeter seepage as a surrogate for vertical groundwater recharge with this we follow vereecken et al 2015 who argue that an improved description of soil hydrological fluxes at the local scale are fundamental to reduce the large uncertainties which are still present in large scale models used to predict these fluxes 2 2 groundwater recharge models we compared four groundwater recharge models that differ in terms of model complexity all four models have already been applied successfully in recharge studies worldwide these models are a linear soil water balance model finch finch 1998 2001 moeck et al 2016 von freyberg et al 2015 a lumped semi mechanistic model lumprem meeks et al 2017 moeck et al 2016 watson et al 2013 and a physically based model hydrus 1d simunek et al 2005 the hydrus 1d model was applied with two different complexities homogenous single porosity model hydrushom van genuchten 1980 and dual porosity model hydrusmac durner 1994 the number of parameters of each model depends on the complexity of recharge process description table 1 and si table s1 all models were run at daily time steps and recharge simulations were aggregated to weekly sums forcing functions and model dimensions were based on the data from 1992 to 2012 of the rietholzbach lysimeter before 1992 measurement gaps at the meteorological station occurred and therefore calculating potential evapotranspiration using the penman montheith equation after allen et al 2005 was not possible because snowpack drainage is assumed to affect groundwater recharge patterns meeks et al 2017 precipitation inputs in winter and spring were transferred into snow water equivalent and melt water volumes using temperature index modelling based on the procedure described in walter et al 2005 the seasonal and annual patterns of precipitation potential evapotranspiration and lysimeter seepage are provided in fig s1 in the electronic supplementary material esm the following sections provide a brief overview of the recharge models from simple to complex model structure 2 2 1 soil water balance model finch in the soil water balance model referred to as finch model direct recharge from a homogeneous soil column was calculated using a simple daily water balance equation where only matrix flow was considered 1 r i p i e ai i i b i δs i where r i is groundwater recharge l t p i is precipitation l t e ai is actual evaporation l t i i is canopy interception loss l t b i is flow bypassing the soil water storage l t and δs i is change in soil water storage l t recharge occurs when the soil moisture content exceeds field capacity von freyberg et al 2015 upward movement of water e g capillary flow is neglected in the finch model water uptake from plant roots occurs at a time varying rate that is limited by water stress the root zone for the finch model was divided into four layers following the standard approach for grass covered soil presented in finch 1998 for a more comprehensive description of the model see finch 2001 1998 2 2 2 lumped parameter bucket model lumprem this lumped parameter bucket model referred to as lumprem model provides a basic representation of the unsaturated zone including matrix and macropore flow matrix and macropore flow are activated after specified delay times macro pore recharge takes place when the soil becomes saturated lumprem calculates the actual evapotranspiration taking soil moisture storage and plant parameters into account water is lost from the soil column as unsaturated vertical flow the soil moisture content in the soil column controls the recharge rate as well as the hydraulic conductivity a decrease in soil moisture lowers the hydraulic conductivity following the concept of the water retention curve the governing equation of the lumprem model reads 2 r k s υ l 1 1 υ 1 m m 2 where r l t is the recharge rate k s l t is the saturated hydraulic conductivity υ the volume of the bucket storage l is tortuosity and m determines the shape of relationship between the recharge rate and soil moisture storage note that the model parameters have no physical meaning more detailed information about the lumprem model can be found in i e watson et al 2013 and meeks et al 2017 2 2 3 physically based model hydrus 1d homogenous a 1d physically based vertical flow model referred to as hydrushom model of a 250 cm deep soil column with 202 layers was created with the software suite hydrus simunek et al 2005 the discretization was 0 7 cm at the ground surface and increased linearly to 1 5 cm in 100 cm depth for the remaining depth a constant value of 1 5 cm was used this fine discretization of the upper part of the soil column allowed for a numerical solution with reasonable accuracy for recharge processes vogel and ippisch 2008 it further enabled to incorporate vertically variable distribution of evapotranspiration rates saturated hydraulic conductivities were estimated with rosetta a program for pedo transfer functions schaap et al 2001 based on soil surveys in close vicinity of the rietholzbach lysimeter von freyberg et al 2015 these local estimates were used as initial conditions for the model parameter calibration the functions of soil water retention and hydraulic conductivity are described in mualem 1976 and van genuchten 1980 the hydraulic boundary condition of the bottom of the soil column was represented by a seepage face with no flow when the boundary was unsaturated and a fixed atmospheric pressure head once saturation was reached grass rooting depth was set at 25 cm seneviratne et al 2012 root water uptake was simulated for grass cover following the model presented in feddes et al 1974 2 2 4 physically based model hydrus 1d dual porosity the most complex model referred to as hydrusmac model in this study follows the same conceptualization of the underlying physical processes described for the hydrushom model apart from a dual porosity formulation this approach assumes that the porous medium can be divided into two overlapping soil domains durner 1994 where both domains are based on the van genuchten 1980 function of soil hydraulic properties 3 s e w 1 1 α 1 h n 1 m 1 w 2 1 α 2 h n 2 m 2 where s e is the effective water content h is the pressure head l w i are the weighting factors and α i 1 l n i and m i are the empirical van genuchten parameters for the two soil domains combining this retention curve model with the mualem 1976 and van genuchten 1980 model leads to 4 k s e k s w 1 s e 1 w 2 s e 2 l w 1 α 1 1 1 s e 1 l m 1 m 1 w 2 α 2 1 1 s e 2 l m 2 m 2 2 w 1 α 1 w 2 α 2 2 where w i are the weighting factors for the sub curves of the overlapping subregions k s is the saturated hydraulic conductivity l t and k the hydraulic conductivity l t as a function of s e 3 methodology 3 1 step 1 identification of climatically contrasting calibration periods the systematic evaluation of the four groundwater recharge models was carried out in four steps which are visualized in fig 2 and explained in detail below as a first step climatically contrasting calibration periods were identified we applied two different metrics to identify dissimilar calibration periods the palmer drought severity index pdsi and lysimeter seepage measurements meteorological data and lysimeter data from january 1992 to december 2012 were used during which climatic conditions from very wet to very dry periods occurred the pdsi which is commonly used for characterizing meteorological droughts palmer 1968 was calculated in this study with the matlab code provided by jacobi et al 2013 required input variables were mean monthly precipitation air temperature latitude of the study area and the local available water capacity of the soil ficklin et al 2015 precipitation inputs were adjusted for snow water equivalent following the procedure described in walter et al 2005 pdsi values between 0 5 and 0 5 represent conditions near normal whereas values below 4 and above 4 indicate severe drought or very wet conditions respectively alley 1984 as a second metric for wet and dry conditions monthly sums of observed lysimeter seepage were used there is no general guideline for the minimum length of the required calibration period perrin et al 2007 according to singh and bárdossy 2012 it is very difficult to precisely define what length of observation data is sufficient to identify model parameters in the calibration brigode et al 2013 found that calibration periods longer than three years do not generate more robust simulations and predictions for rainfall runoff models based on modelling results for different flow characteristics based on the pdsi and observed lysimeter recharge in our study we found two years an appropriate length for the calibration period because the contrasts in the different calibration periods generally decrease with increasing length of the time periods the chosen six two year calibration periods are characterized by high hydro climatic variability sorted from wet to dry conditions 1994 1995 wet wet 2002 2003 wet dry 2005 2006 dry avg 2010 2011 avg dry 2003 2004 dry dry and 2003 2011 dry dry fig 2 and table 2 the years 2003 and 2011 were chosen for the dry dry calibration period because an extreme summer drought occurred in 2003 while 2011 was affected by a spring drought staudinger et al 2015 thus the information content and climatic variability from the calibration period is likely to be higher than by using years where dry conditions only occurred during the summer season wet periods with several high intensity precipitation events or a significant accumulation of snow occurred e g in 1995 and in 2002 at the rietholzbach site average annual values of precipitation actual evapotranspiration and lysimeter seepage between 1992 and 2012 were 1473 mm 560 mm and 1029 mm respectively table 2 the deviations of the six calibration periods from the mean annual precipitation and potential evapotranspiration range from 25 to 17 and from 13 to 20 respectively the relative differences in lysimeter seepage range between 35 and 44 3 2 step 2 model calibration within null space monte carlo analysis each groundwater recharge model was calibrated with the parameter estimation software pest doherty 2016 against cumulative weekly lysimeter seepage for each of the six calibration periods a 365 days warm up phase i e year prior of calibration period was included in every calibration to provide valid initial conditions for all simulations our calibration workflow included several steps the model was calibrated once against the weekly recharge rates from the different calibration periods the initial model parameter values were based on the prior expert knowledge see also si table s1 subsequently parameters belonging to the solution and null space were identified based on singular value decomposition decomposition of parameters took place on the basis of the jacobian matrix which is the matrix of sensitivities of all model outputs corresponding to observations from the calibration dataset to all adjustable parameters the solution space dimensionality was calculated based on the ratio of highest to lowest squared singular value ratio which is defined to be not larger than 5 107 doherty 2016 thus due to the different calibration periods with different information content the number of parameters in the solution space can vary si table s2 then a calibration constrained monte carlo analysis tonkin and doherty 2009 was performed to provide important information about model parameter uncertainty and consequently in recharge simulation here 300 different random parameter sets ps were generated for each model and calibration period in total 7200 simulations from 300 ps six calibration periods and four recharge models random log normal parameters were generated based on the provided upper and lower parameter range as well as on computed means based on existing parameter values obtained through the first calibration subsequently random parameter sets were modified through null space projection first the difference was taken between each random parameter set and the calibration parameter set this difference was then projected onto the calibration null space and the projected difference was re added to the calibration parameter set typically due to the non linearity of the models parameter sets did not result in perfectly calibrated models and re calibration is required here the calibration took place through alterations of parameters in solution space whereas the null space was unadjusted the calibration efficiency with the different ps was evaluated with the target function 6 m i n i 1 n w i r o i r m i 2 where w is the weight assigned to lysimeter recharge r and the indices o and m represent observed and simulated weekly values at the time step i respectively in our study equal weights were used for all observations for each model although we are fully aware that the calibration is more strongly affected by high recharge rates however this weighting strategy is most commonly applied in modelling exercises foglia et al 2009 moeck et al 2015 poeter and hill 1997 it further avoids user subjectivity in the optimization approach rosolem et al 2012 the constrained monte carlo analysis method increases the efficiency of the calibration procedure by reducing the computational burden that is often related to monte carlo approaches and thus typically restricts such an approach especially for physically based models in our study only solution space parameters were adjusted to lower the objective function moreover the first iteration of each calibration process was based on the computed parameter sensitivities using the first calibration parameter set the first iteration of each calibration therefore required only a single model run a maximum of two iterations was applied in the calibration exercise to reduce the computational demand the calibrated model parameters and standard deviation for each groundwater recharge model can be found in table s3 a more comprehensive description of the method can be found in tonkin and doherty 2009 optimal ps were chosen based on a defined target function threshold t h r e s h o l d that was used for all models and all calibration periods we assumed that the accuracy of the lysimeter recharge measurements ε is constant and equal to 5 of the recharge amount ghasemizade et al 2015 only if the calculated target function eq 6 is smaller or equal than t h r e s h o l d equation 7 the ps was accepted 7 t h r e s h o l d i 1 n r o i ε 2 8 p s a c c p e t e d y e s t h r e s h o l d n o n t h r e s h o l d this procedure ensured a reasonable comparison between the different recharge models it is important to note that the selected optimal ps for each model can lead to different numbers of accepted model ps note that the number of acceptable ps for each model might increase if more than two iterations are applied 3 3 step 3 model simulation with different parameter sets ps and calibration periods simulations were carried out for the time period 1993 2012 similar to the calibration methodology a one year warm up phase year 1992 365 days for recharge simulations was used 3 4 step 4 evaluation of model performances based on different recharge characteristics the effects of model complexity and calibration periods on model performance were analysed through various performance criteria and methods namely i taylor plot taylor 2001 ii annual distribution of residuals iii model bias against the climate differences between calibration and validation period iv a comparison of observed and simulated recharge deficit volumes for each year of the simulation period and v post calibrated uncertainty analysis for the sum of annual recharge for the entire simulation period the application of multiple performance criteria is generally recommended because a single criterion might only evaluate specific aspects of model robustness and performance bennett et al 2013 the taylor plot i provides a graphical summary of how closely simulated recharge rate patterns match the observations orth et al 2015 the similarity between temporal patterns was quantified by using their correlation their normalized root mean square differences and the amplitude of their variations represented by their standard deviations a ranking of recharge residuals ii was carried out on an annual time scale subsequently the sensitivity of recharge predictions to the calibration period and transferability to dissimilar conditions iii were conducted in order to evaluate model performance under very dry climatic conditions iv we calculated the deficit volume of recharge v x following the framework of drought characteristics in hydrological systems tallaksen and van lanen 2004 we used the 20th percentile of the weekly recharge time series as a static threshold x 0 and calculated the difference between x 0 and simulated recharge note that x 0 is different for each model as it is based on the simulated weekly recharge rates finally v post calibration uncertainty analysis was performed for the sum of annual recharge for the entire simulation period using the genlinpred program from the pest suite doherty 2016 we computed post calibrated predictive uncertainties as well as uncertainty reductions in our predictions between pre and post calibrated parameters the calibrated uncertainty variance σ s 2 and standard deviation σ s σ s 2 were calculated based on the following equation christensen and doherty 2008 9 σ s 2 y t c p y y t c p x t x c p x t c ε 1 x c p y where the model parameters span the vector p and the variability of p is described by the covariance matrix c p the sensitivity of our prediction s to parameters p is represented by the vector y the covariance matrix c p of innate parameter variability which can also be seen as the pre calibrated uncertainty is supplied through an uncertainty file the file contains information about prior knowledge the covariance matrix c ε comprise the measurement noise expressed by the measurement weights measurements are assumed to be proportional to the standard deviation of noise however in most cases model to measurement misfit is an outcome of structural noise i e the imperfection of the model itself rather than measurement noise alone this effect can never be identified exactly and therefore an underestimation of predictive uncertainty will commonly occur the first term in eq 9 is the pre calibration uncertainty of the prediction james et al 2009 it is solely a function of innate parameter variability c p and the dependence of our prediction on model parameter sensitivity y the second term expresses the reduction in predictive uncertainty through the conditioning effect i e calibration of historical observation christensen and doherty 2008 this can be used to determine the relative efficacy of various existing observations or periods in reducing the predictive uncertainty the reduction of predictive uncertainty was demonstrated shortly for the six different calibration periods for the applied models note that eq 9 does not require actual parameter or observation values solely parameter and observation sensitivities are used to calculate pre and post calibrated uncertainty james et al 2009 moeck et al 2015 moore and doherty 2005 although the method depends on linear assumptions some studies demonstrated that the outcome from linear and non linear uncertainty methodologies were very similar e g brunner et al 2012 james et al 2009 a detailed description and derivation of the method is given elsewhere doherty 2016 doherty and hunt 2009 moore and doherty 2005 4 results 4 1 performance analysis calibration the following section describes model simulation results for the calibration period 2003 2004 dry dry as an example for the other five calibration periods which are presented in the supplementary material si figs s2 s6 and table s5 all calibrated recharge models reproduced the observed weekly time series well despite considerably different model structures fig 3 the best agreement between ensemble mean and measured lysimeter recharge in terms of the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 and r2 was found for the models hydrusmac nse 0 88 r2 0 88 and hydrushom 0 88 0 88 as well as for lumprem 0 83 0 83 the finch soil water balance model 0 68 0 71 simulated highly variable and generally too small recharge rates because of its simplified representation of flow processes in contrast to the finch model recharge rates simulated by the other three models seem more realistic as the retention curve behaviour is implicitly implemented in the model structures different numbers of accepted model realization were obtained through our approach finch 35 ps out of 300 12 lumprem 189 ps out of 300 63 hydrushom 243 ps out of 300 81 and hydrusmac 261 ps out of 300 87 overall the amount of acceptable model ps increased with the degree of model complexity fig 4 depicts the taylor plots for the six calibration periods and the four recharge models for which the number of optimal ps is shown in brackets eq 7 in a taylor plot a perfect fit between observed and simulated weekly recharge values would lead to a value of 1 black point on the x axis in fig 4 all models performed well i e absolute differences in recharge were small for all models except for the finch model correlations between observed and mean ensemble recharge were generally between 0 85 and 0 95 depending on the model structures stochastic realizations and calibration periods the normalized standard deviations were generally close to 1 perfect fit in terms of standard deviation for hydrushom and hydrusmac and around 0 8 for finch and lumprem the normalized root mean square differences were smallest and ranged between 0 4 and 0 6 except for the finch model the hydrushom and hydrusmac performed similarly well due to their similar physical representation of recharge processes with respect to the calibration periods there was no clear tendency of what calibration period resulted in the best model performance during calibration since all calibration periods led to comparable results 4 2 performance analysis validation for recharge simulations over the whole observation period 1993 2012 1992 as warm up phase the taylor plots in fig 5 illustrate a worsening of performance for all models compared to the calibration period although the model performances decreased for all models distinct differences between the models and the calibration periods could be identified a wide range of normalized standard deviations and normalized root mean square differences occurred for the more simplistic finch and lumprem models from all models the finch model showed the poorest performance for most calibration periods with correlations smaller than 0 6 and normalized standard deviations between 0 2 and 1 3 in contrast to the data presented for the model calibration fig 4 the data of the validation period fig 5 further indicate a link between the chosen calibration period and the performance of the models thus a more detailed analysis of the role of the calibration period on the recharge predictions was carried out sections 4 3 and 4 4 4 3 robustness of annual recharge predictions with respect to the climatic conditions of the calibration period patterns of annual residuals ensemble mean of annual simulated recharge minus observed annual recharge were compared through a ranking fig 6 smallest residuals between observed and simulated values highest ranks light blue colour indicate a better model performance under the specific calibration period larger residuals lowest ranks red colour were obtained when the calibration period led to a poorer model performance uncertainties in terms of variability in the annual recharge rates was expressed as the standard deviation for all acceptable ps point size annual residuals for the six different calibration periods were continuously ranked between large residuals rank 1 red colour to smaller residuals rank 6 blue colour annual residuals were generally large for all models with the driest calibration period 2003 2011 dry dry whereas the residuals were smallest with the calibration period 2002 2003 wet dry apart from this trend there is no consistent pattern indicating better or worse model performance over the entire period i e high ranking small standard deviations with rather wet or rather dry calibration periods respectively for instance the wettest calibration period 1994 1995 wet wet resulted in a higher ranking of simulated annual recharge when the model finch was used while much lower ranks were obtained for the model hydrushom similar inconsistent patterns were apparent between the models for the calibration periods 2010 2011 avg dry interestingly rankings high and low were more consistent among all models for those calibration periods that contained the drought year 2003 i e 2002 2003 wet dry 2003 2004 dry dry and 2003 2011 dry dry the previous analysis indicated a relationship between the climatic conditions during the calibration period and the model performance for simulating the whole observation period 1993 2012 1992 as warm up phase therefore we investigated this relationship in more detail fig 7 displays the differences between annual potential et and annual precipitation from the climatic conditions during the calibration the colouring of the circles represents the percentage of bias pbias simulated minus observed value and the size of the circles represents the variability in the results in terms of standard deviations considering solely mean pbias values a diagonal pattern could be observed for all models indeed overestimation of annual recharge occurred when the validation period was wetter than the calibration period whereas underestimation occurred when the validation period was dryer than the calibration period only if the climatic conditions during the calibration and validation periods were similar pbias became small it further seemed that changes in precipitation influenced the pbias more than changes in et which is in line with findings of other studies coron et al 2012 oudin et al 2006 this can be seen when a fixed value for et i e 0 lumprem calibration period 1994 1995 is chosen and the pbias along the x axis is recorded changes in pbias from red colours underestimation to blue colours overestimation occur in contrast fixing a precipitation value i e 0 lumprem calibration period 1994 1995 does not lead to a persistent pattern of pbias on the y axis the largest pbias occurred when precipitation changed more than 20 compared to the calibration period for all models a similar effect of precipitation changes on model performance from a calibration to validation period was also observed by merz et al 2011 for rainfall runoff models 4 4 post calibrated uncertainty analysis simulated total recharge for the entire time period 1993 2012 for the four different models and under the six calibration periods is shown in table 3 the models finch and lumprem underestimated total observed recharge by 20713 mm whereas the ranges of simulated recharge from the two hydrus models comprised the observed value the post calibrated uncertainty standard deviations obtained from the linear uncertainty analysis table 3 were generally larger for the finch and lumprem models compared to the hydrushom and hydrusmac models the hydrushom model showed the smallest ranges of total recharge and respective standard deviations for all models the standard deviations were largest when the calibration period 2005 2006 dry avg was used moreover the calibration period 2002 2003 wet dry led to the largest uncertainty reduction between pre and post calibrated predictive uncertainty for all models the smallest uncertainty reduction occurred for the two hydrus models for the calibration period 2005 2006 dry avg whereas for the finch and lumprem models and the calibration periods 1994 1995 wet wet and 2003 2011 dry dry the uncertainty reductions were smallest for the remaining calibration periods there was no consistent trend indicating better or worse uncertainty reduction which corroborates the observations from section 4 3 moreover the climatic conditions during the calibration period resulted in more variable post calibrated standard deviations and uncertainty reductions for the lumprem and finch models compared to the two hydrus models in addition to the linear uncertainty analysis results from the non linear uncertainty analysis were also available thanks to the calibration constrained monte carlo analysis due to the generated ps for each model under each calibration period a range of total recharge can be observed fig 8 for the finch model the majority of simulations underestimated the total observed recharge of 20713 mm and only a few ps under all calibration periods captured the observed value the underestimation was most pronounced for the dry calibration periods 2010 2011 and 2003 2011 for the lumprem model the observed recharge value was captured under all calibration periods except for the driest 2003 2011 in most cases the observed recharge value was within the 50 range of the simulated recharge similar results were obtained with the two physically based hydrus models that also performed well under the direst calibration period 2003 2011 for all models the calibration period 2002 2003 wet dry led to the best results in terms of simulated recharge and small standard deviation for all models as observed already from the linear uncertainty analysis the standard deviations were generally larger for the finch and lumprem models compared to the two hydrus models our analysis also showed that the standard deviations from both linear and non linear uncertainty analyses were of the same order although small differences had to be expected as a result of the more approximate assumption of the linear uncertainty analysis the fact that both methods led to similar results is in line with findings of previous studies e g brunner et al 2012 james et al 2009 4 5 simulation of recharge during dry climatic conditions meteorological droughts and the associated recharge deficits are of great concern with regard to water resource management lehner et al 2006 thus our model comparison study was also carried out for dry periods for which weekly recharge deficits relative to the 20th percentile of the long term average were calculated fig 2 we aggregated these simulated deficit volumes to annual sums and compared them to the observed deficit volumes based on lysimeter seepage measurements fig 9 uncertainties in terms of variability in the residual recharge deficit volumes are expressed as the standard deviation for all accepted ps i e point size under all calibration periods a consistent pattern emerged where the models finch and lumprem tended to underestimate annual recharge deficit volumes hereby the deviations from the observations were larger for the finch model than for the lumprem model except for the calibration period 2010 2011 avg dry where the lumprem model performed less well both hydrus models showed comparably small deviations from the observed recharge deficit volumes for deficits less than 20 mm for all models the smallest residuals were obtained under the driest calibration periods whereas with the wettest calibration periods e g 1994 1995 residuals became larger this potentially indicates that dry dry calibration periods are most efficient to simulate recharge during drought conditions simulated recharge deficit volumes and thus model performance differentiated more with increasingly dry conditions while the hydrus models still showed an acceptable agreement during drier periods annual recharge deficits more than 20 mm the lumped and soil water balance models were not able to reproduce the most extreme recharge deficit volumes the largest deviations occurred for the drought year 2003 5 discussion in this section we discuss how the structures and parameterizations of the four recharge models influence recharge simulations subsequently we consider the effect of dissimilar climatic conditions in the calibration on the performance of the four models we discuss our findings with respect to previous comparison studies which have focused merely on hydrological models during calibration all models performed acceptably in terms of the selected model performance criteria for all calibration periods however distinct differences during the validation period 1993 2012 among the models suggest that model complexity is a source of bias for recharge predictions our results illustrate that the two more complex physically based models hydrushom and hydrusmac successfully reproduced observed recharge during the validation even when climatic conditions were highly dissimilar during the calibration periods sections 4 3 and 4 4 due to the fact that both models performed similarly well we do not see an advantage by using the more complex model hydrusmac where a larger number of parameters would result in a higher computational demand during the calibration and validation periods compared to the hydrushom model and larger uncertainties however at field sites where macropore flow plays a dominant role for groundwater recharge the hydrusmac model may be the better choice our analysis also showed that the soil water balance finch and lumped lumprem models performed differently for the validation period in comparison to the physically based models especially under very contrasting climatic conditions figs 4 and 5 this behaviour of the simplified models is in line with the findings of dams et al 2015 and moeck et al 2016 both studies demonstrated that the model structure becomes more important for robust recharge predictions under extreme or very contrasting climatic conditions the majority of simulations with the finch model underestimated groundwater recharge the model performed poorest during the dry calibration periods 2010 2011 and 2003 2011 table 3 and fig 8 in contrast the uncertainties were slightly smaller and the sensitivity to the chosen calibration period was lower for the lumprem model overall recharge simulated with lumprem compared well with the observed rates taken into account the faster running times and non convergence difficulties of the lumprem model compared to the physical based hydrus models the lumprem model might be attractive when fast simulations are required at the expense of higher uncertainties for our recharge models we found that the variability of the simulation results was generally smaller during the validation period for the physically based models than for the more simplistic recharge models this suggests that the physical representation of the recharge processes allows for more constrained parameter estimation during the calibration with a physically based model the parameter behaviour follows physical rules and incorporates non linear interrelationships between model parameters and recharge also under extreme climatic conditions moeck et al 2016 our analysis showed that it is still difficult to provide general guidelines on how to identify the optimal calibration period for recharge simulations this is a general problem for modelling applications as shown for streamflow predictions by brigode et al 2013 overall the highest predictive uncertainty reductions and best fits between simulated and observed recharge were obtained for the calibration period 2002 2003 wet dry e g table 3 and fig 8 apart from this calibration period our study suggests that there is no optimal calibration period that allows for equally good simulations with all four recharge models e g fig 6 similar to orth et al 2015 our findings indicate that model performance varies with respect to the hydrological conditions especially for the simplified models finch and lumprem an overestimation in recharge occurs when the validation period is wetter than the calibration period whereas underestimation occurs when the validation period is dryer than the calibration period similar conclusions for hydrological models were drawn based on studies by brigode et al 2013 and coron et al 2012 the recharge models generally performed best when a combined wet and dry year was used for calibration or the validation period was very similar to the calibration period only if the calibration and validation period were quite similar the deviations between observed and simulated values were small moreover the model performance was more influenced by changes in precipitation than by changes in et which is in line with other studies coron et al 2012 hartmann et al 2017 oudin et al 2006 the observed dependency of the recharge simulations on the climatology of the calibration period was however less pronounced for the physically based models we found that this dependency had an effect on the simulation of extremely dry periods the residuals of simulated and observed recharge deficit volumes became smaller when dry periods were used as a calibration period especially for the simplified models this is in line with a hydrological study by vaze et al 2010 who stated that non stationarity of model parameters occurred meaning that some periods in the calibration are more useful for the identification of the parameter space while others might be less informative also li et al 2012 noted that some parameters of hydrological models are particularly sensitive to the climatic conditions of the calibration period the results of our comparison study of recharge models are consistent with those of previous studies focusing on hydrological models we found that physically based models have a greater potential to obtain predictions beyond the range of conditions during calibration and thus are more robust with regard to the chosen calibration period at the expense of higher computational times for calibration and prediction compared to the simplified models the higher computational demands of the physically based models limit however their application at the catchment or global scale thus simplified models often provide the only means to estimate recharge at these spatial scales whichever model is used the predictive uncertainty has to be assessed through uncertainty analysis e g post calibrated uncertainty analysis to clearly communicate the uncertainty limits to stakeholders and decision makers in addition to the chosen model complexity and calibration period model performance can also be influenced by the applied objective target function formulation however this potential effect was not included in our analysis wang and brubaker 2015 demonstrated that a multi objective auto optimization improved the model performance however they also stated that the model structure itself is more important than the set up of the calibration 6 summary and conclusions this study systematically evaluated the effect of model complexity and calibration strategy on simulated recharge rates by utilizing a long term recharge data set a differential split sample test was carried out with four recharge models that vary in terms of model complexity the models were calibrated with six climatically contrasting calibration periods in a constrained monte carlo approach despite the climatically contrasting conditions all models performed similarly well during calibration during validation the more complex physically based models hydrushom hydrusmac predicted recharge with relatively small uncertainty limits even when calibration and prediction periods had very different climatic conditions in contrast the more simplistic soil water balance model finch performed poorer under such conditions during validation especially when the calibration periods contained an extremely dry year the semi mechanistic model lumprem also showed large standard deviations compared to the physically based models the quantified post calibration uncertainty limits did however capture the observed recharge rates taken into account the faster running times and non convergence difficulties compared to the physical based model the lumprem model might be attractive when fast simulations are required overall we found a strong sensitivity of model performance on the chosen calibration period for the more simplified models it is still difficult to provide general guidelines on how to choose an optimal calibration period the smallest standard deviations highest predictive uncertainty reductions and generally best fits between simulated and observed recharge was obtained for the most contrasting calibration period 2002 2003 wet dry for the remaining calibration periods our analysis suggests that there is no optimal calibration period that allows for equally good simulations with all four models however some general tendencies could be observed for all models an overestimation in recharge occurred when the validation period was wetter than the calibration period whereas underestimation occurred when the validation period was dryer than the calibration period the recharge models generally performed best when a combined wet and dry year was used for calibration of if the validation period was very similar to the calibration period the observed dependency of the recharge simulations on the climatology of the calibration period was however less pronounced for the physically based models in this numerical experiment differences between the models and calibration periods could be observed that are expected to occur for various soil types and study areas the results obtained here can have important implications when using recharge models as decision making tools in a broad range of applications e g water availability climate change impact studies water resource management etc our systematic analysis is a preliminary testing of model robustness and parameter transferability and provides a more realistic representation of uncertainty in model based recharge estimations and predictions the approach should be extended and is just one way to provide more insight into model behaviour under dissimilar climatic conditions conflicts of interest none acknowledgments the authors gratefully acknowledge financial assistance provided by the swiss national science foundation snsf projects 200021 129735 1 and 200020 143688 additional funding was provided by the competence center environment and sustainability cces of the eth domain in the framework of the record catchment coupled ecological hydrological and social dynamics in restored and channelized corridors of a river at the catchment scale project we thank the group of s seneviratne land climate dynamics institute for atmospheric and climate science iac swiss federal institute of technology zurich ethz who provided meteorological data and lysimeter seepage time series the from the rietholzbach research catchment appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 005 
26408,a new open source software tool the generic environmental model gem is summarized and illustrated herein the gem is a numerical approach to solving the classical advective dispersive partial differential equations describing chemical fate and transport in environmental media its generic capabilities are due to the flexible compartment approach for spatial discretizations as well as an architecture that makes it amenable to a wide variety of environmental problems theoretical underpinnings and capabilities are described in this paper and are illustrated with a multi media example using a holistic approach to multi media modeling that includes both feedforward and feedback processes among media the holistic approach is compared to results that might arise from a more conventional approach using a set of single media models coupled together in a feedforward only fashion the gem is open source and was developed to facilitate rapid model building for a wide variety of environmental contamination problems the authors encourage users to download and use the tool keywords environmental modeling chemical fate and transport analysis multi media modeling 1 introduction this paper introduces a new software tool designed to facilitate fate and transport f t modeling of toxic chemicals in a wide variety of environmental settings the software named the generic environmental model gem is flexible mathematically rigorous based on fully numerical solutions to the underlying partial differential equations pdes and available on line as open source link https github com keithwlittle gem it is written in the java language the gem is intended as a tool that allows users to quickly develop accurate solutions to pdes that are unique to their chemicals environmental settings and spatial temporal resolution of interest the architecture allows the user to build that set of pdes of particular interest and obtain solutions to those pdes rather than the classical approach of force fitting the problem into an existing black box model which may not honor all of the f t features of interest the gem is extensively documented including detailed user instructions and examples in little 2012 regarding a literature review of similar models and comparative differences we are not aware of another open source environmental modeling platform that has these generic features goldsim http www goldsim com web products goldsimpro is an environmental modeling platform that has f t capability but is proprietary in the public domain several of the fugacity models originally developed by mackay 1991 have generic capability and are typically being used for regional multi media modeling applications however they operate on fugacity based pdes and seem more generally applicable to low spatial and temporal resolution applications following a description of the gem tool s theoretical basis functionality and a summary of verification validation exercises its use is illustrated via a multi media chemical f t example the multi media domain is hypothetical but realistic and comprises soil aquifer surface water and air media a holistic gem based model is set up and dynamically executed and includes feedforward and feedback processes e g volatilization settling dispersion both within and among the media a second simulation is performed wherein inter media processes are included only in a feedforward manner i e from one medium to another to mimic conventional multi media modeling tools that couple single medium models with outputs from one model becoming inputs to another in one direction only the holistic results are compared to feedforward only results 2 functionality design motivation and numerical approach 2 1 overview of functionality the generic environmental attribute of the gem tool arises from several design features it is designed as an environmental modeling platform or framework rather than a model with specific functionality users enter their specific data regarding state variables svs environmental topology parameters and processes and the corresponding pdes are then assembled and solved it is based on the generalized pde describing advective dispersive f t of chemical mass in porous media which can be simplified to surface water or air media the numerical approximation to the spatial derivatives uses the compartment method which is mathematically equivalent to standard finite difference approximations with the exception that the spatial domain itself is discretized rather than the spatial derivative terms in the underlying pde this seemingly trivial difference has the practical benefits of being both highly intuitive to the user as well as being robust to alternative dimensionalities thus a 1 d problem is easily converted into 2 or 3 d simply by adding more compartments in those dimensions temporal derivatives are approximated by standard finite difference methods the spatial discretization options are either back space bs or center space cs temporal discretization options are forward time ft back time bt or center time ct the choices have implications for allowable time step sizes and compartment volumes vis a vis avoidance of numerical errors numerical dispersion wiggle negativity complete parameterization of the model is achieved through straightforward comma separated value csv text files fate processes types and parameterization are highly flexible and provided by the user the csv files are readily generated by the user using a spreadsheet csv files for most of the examples mentioned later are available on line https github com keithwlittle gem in particular a set of input files for a relatively simple soil column application is included along with a tutorial describing the structure of the associated input files see start here folder the new user is encouraged to first become familiar with this example and make sequential changes to its files as needed to build input files for new applications multiple interacting svs are possible including specification of the compartmental relevancy of each sv a quasi newton iterative algorithm is included allowing nonlinear capability although designed to build systems of equations describing f t of chemical mass in environmental systems where hydrodynamic flows among compartments are user inputs the gem can also be used to solve the pdes describing the hydrodynamic movement of water itself by a suitable change of variables and reinterpretation of parameters indeed the gem can be used either in environmental system mode where it assumes the user is building environmental f t type pdes or in equation solver mode equation solver mode is completely general and the user enters the equations of interest that may have nothing to do with environmental modeling and the gem subsequently solves those equations 2 2 underlying pde the gem is designed primarily for chemicals that among other processes partition between liquid solid and gaseous phases in surface water and or porous media domains despite the fact that the underlying pdes describing these partitioning processes in water porous media domains are remarkably similar the historical development of f t software has almost exclusively been compartmentalized into separate surface water models e g wasp https www epa gov exposure assessment models water quality analysis simulation program wasp and porous media models e g hydrus http www groundwatersoftware com hydrus htm this departure in software tools is principally because the underlying pdes describing system hydrodynamics are quite different between these two media however once the flows are known the f t pdes can be shown to be equivalent and can be used for either medium this commonality is the motivating principle behind the gem which receives spatial and temporal flow distributions as user inputs to the f t pdes the underlying pde solved numerically by the gem is summarized below the classical advective dispersive equation describing chemical mass balance in a 1 dimensional x porous medium with equilibrium partitioning can be written as e g schnoor 1996 or fetter 1993 1 r c d t u x c d x d x 2 c d x 2 where cd is solute dissolved concentration mc lw 3 ux is the advective pore water velocity lw t dx is the diffusive dispersive mixing coefficient through the pore water lw 2 t r is the dimensionless retardation coefficient defined as 2 r 1 k d ρ b θ where kd is the solids liquid partitioning parameter lw 3 ms ρb is the bulk density of the solids ms lt 3 θ is the porosity or water content lw 3 lt 3 we are distinguishing between chemical and solids mass by using subscripts c and s respectively and water and total volume by subscripts w and t respectively for water containing sorbing chemicals flowing through porous media the retardation parameter r acts to slow down the movement of the chemical relative to the water itself because it is being retarded by sorption onto the immobile solid media in addition in ground water applications one is typically more interested in the chemical concentration in the flowing water than in the concentration of chemical on the solids which have been left behind hence the sv cd in equation 1 is dissolved concentration equation 1 is the underlying pde in 1 d solved numerically by the gem although commonly associated with contamination in porous media the equation is extremely flexible for example consider partitioning chemicals flowing in a surface water medium containing suspended solids the chemical is partitioning between solid and liquid phases just as in the porous medium scenario however it is typical for toxic surface water models to simulate total dissolved plus sorbed chemical as the sv rather than dissolved chemical this is because the sorbed chemical is not being left behind in space and time retarded on the solids as on immobile porous media but rather exists in the same location in space and time as the water itself just in a different phase modeling total rather than dissolved may also lead to simpler model constructs substituting total concentration ct mc lt 3 for cd in equation 1 and letting r 1 because it is irrelevant when modeling total concentration results in the commonly used 1 d advective dispersive equation describing total chemical concentration mass balance in surface waters see e g thomann and mueller 1987 or chapra 1997 3 c t t u x c t x d x 2 c t x 2 a more formal mathematical proof of the equivalence between equations 1 and 3 using the equations of sorption is included on the gem website https github com keithwlittle gem the commonality of the generalized porous media and surface water pdes provided the major motivation for the architecture of the gem s numerical approach it is based on the more general porous media equation 1 and capable of simplification to pure water or even air 1 1 to the extent that the movement of air can be described as the movement of an incompressible fluid the gem can be used for chemical f t in air media media the gem s spatial numerical approach extends equations 1 and 3 to multiple dimensions as described below 2 3 compartment based spatial method to discretize the spatial derivatives in 1 a compartment approach is used wherein the problem spatial domain is assumed to consist of a set of completely mixed volumes or compartments concentrations are constant throughout a compartment but can vary among compartments the compartment method is also variously known as the finite section or finite volume method not to be confused with the finite element method commonly employed for ground water models and was pioneered by robert thomann 1972 for surface water modeling applications it is mathematically rigorous and importantly for the gem architecture highly intuitive and dimensionally flexible because one is discretizing the spatial domain itself instead of operating on the underlying pdes accordingly the gem mass balance differential equation for arbitrary compartment i for linear problems is 4 d r i v i c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i k i c d i w i θ i the terms in this equation are described below cdi is the sv and is dissolved chemical concentration mc lw 3 in the interior of compartment i where mc denotes mass of chemical and lw 3 denotes volume of pore water cdij is the concentration mc lw 3 at the interface of compartments i and j the index na number adjacent denotes that the summation is over all compartments that are adjacent to compartment i it should be noted that adjacent compartments can be in 1 2 or 3 dimensions the specific dimensionality of the problem is irrelevant to the underlying mathematics of the compartment method which was a major factor for its adoption by the gem qij is the advective water volumetric infiltration rate lw 3 t between adjacent compartments i and j in the compartment technique qij is assigned as positive or negative to differentiate between flows entering compartment i and flows leaving compartment i as 5 q i j 0 if flow is from j to i q i j 0 if flow is from i to j to define the problem in terms of concentrations only in the compartment interiors cdij can be expressed as a linear function of the concentrations in the interiors as 6 c d i j α i j c d i 1 α i j c d j if q ij 0 c d i j α i j c d j 1 α i j c d i if q ij 0 where α i j is a user specified spatial differencing parameter that can take on values between 1 and 0 for α i j 1 a back space bs differencing method is being used where concentrations at the interface are determined only by concentrations in the upstream compartment interior for α i j 0 5 a center space cs method is being used where concentrations at the interface are determined by concentrations in both adjacent compartments α i j 0 would be a forward space fs method where interface concentrations would be determined only by the downstream compartment however the fs method has nothing to recommend it by way of any advantage over the bs or cs method and is not included as an option in gem ri is the retardation factor unitless given by 7 r i 1 ρ b i θ i c s i c d i where ρ b i soil bulk density ms lt 3 where ms denotes mass of solids θ i water content lw3 lt3 csi sorbed phase concentration mc ms vi is the compartment total volume lt 3 e ij is a fickian dispersion diffusion flow transport process between i and j given by 8 e i j e i j a i j l i j where eij hydrodynamic dispersion or diffusion coefficient lt 2 t aij cross sectional interface area between compartments i and j lt 2 l i j distance l between compartments i and j across which the dispersion diffusion process is operative ki is an arbitrary 1st order loss rate constant t the first order loss term v i k i c d i in equation 4 is meant to be illustrative of source sink processes the gem input files allow the user to specify sources and sinks as appropriate to their application wi is an external mass loading of chemical to the compartment mc t for linear equilibrium partitioning between dissolved and sorbed phases cs is related to cd as 9 c s i k d i c d i where kd is the partitioning coefficient l3 w ms for the linear case 10 c s i c d i k d i 2 4 temporal methods and user options after the compartment approach is applied to discretize the spatial domain the underlying pdes are reduced to ordinary differential equations in time conventional finite difference methods are then used to discretize the time domain for a single sv e g a single chemical constituent a mass balance equation based on 4 is written for each of the n modeled compartments the mass balance states that the change in mass in a compartment over time is equal to the compartment s inputs transport in and fate sources minus its outputs transport out and fate sinks these equations can be expressed compactly using matrix notation as 11 d r v c d d t a c d w where matrices and vectors are shown in bold vectors are shown in bold and underlined r and v have dimension n x n i e rows and columns and are diagonal matrices the non diagonal elements are zero containing respectively the retardation terms and the compartmental volumes matrix a is a n x n matrix containing mass transport terms and linear source sink terms vector w is a n x 1 vector of external inputs and may also include boundary conditions vector cd is the n x 1 vector of unknown concentrations for which a solution is sought matrix equation 11 is intended to be completely general if there were 2 svs in each compartment then there are double the number of equations and the dimensions of the matrices and vectors in 11 would be 2n for multiple svs the gem functionality does not require that all svs be relevant to all compartments the solution algorithm reads the system of equations represented by 11 reads relevant user solution parameters i e time steps duration of simulation and numerical solution options and solves the equations in time user options for numerical solutions include bs or cs methods for the spatial differencing parameter α i j as mentioned for the temporal derivative finite difference approximation method one explicit and two implicit options are available the explicit option is a forward time ft method also known as an euler solution two implicit options a back time bt and center time ct are available the user choice among these spatial and temporal differencing options involves choices between numerical errors spatial and temporal resolution and simplicity of application bs methods introduce spatial numerical dispersion but are not subject to spatial oscillations or wiggles including non positivity cs methods do not introduce numerical dispersion but may require relatively small compartment volumes as determined by the peclet number first proposed by jean claude peclet in the 18th century the ft method has the general advantage of not requiring solution of simultaneous equations since it is an explicit method but introduces temporal numerical dispersion and may require relatively small time steps to avoid numerical instabilities as governed by the courant friedrichs lewy condition courant et al 1967 the bt method is implicit and requires solution of simultaneous equations but is not subject to numerical instability and large time steps are possible it does introduce temporal numerical dispersion however which increases with increasing time step the ct method is also implicit and is subject to short time steps via the courant condition to avoid numerical instabilities at least in the temporal neighborhood of significant changes in loading conditions but has no temporal numerical dispersion little 2012 provides a detailed description of these numerical considerations for the gem to illustrate consider the ctcs option commonly known as the crank nicolson method although subject to both the peclet and courant criteria and may require small compartment volumes and time steps it introduces neither spatial nor temporal numerical dispersion and is thus the most accurate option at time step t the ctcs method is for linear systems r v c d t 1 r v c d t δ t a c d w t a c d w t 1 2 which can be written in the form 12 r v t 1 δ t 2 a t 1 c d t 1 r v c d t δ t 2 w t 1 a c d t w t the unknowns cd t 1 are on the left hand side of equation 12 and all knowns are on the right hand side this resulting system of simultaneous linear equations is then solved using the gem s internal linear algebra solution algorithm at each time step 3 summary of verification and validation examples model verification is the process of confirming that a model accomplishes its intended mathematical purposes and is essentially a quality assurance step validation is a stronger test and involves using the model to successfully reproduce data that were not used for model calibration it should be noted that the unparameterized gem is simply a modeling framework not a model of anything to verify or validate the gem itself is meaningless as it is simply a tool to build user specified mass balance equations for arbitrary environmental applications nonetheless the gem has been parameterized for both surface water and ground water systems and verified and validated for those specific applications for surface water f t several verification examples including advective dispersive transport and equilibrium sorption for a single state variable dissolved chemical are presented in chapter 6 of little 2012 gem numerical results are compared to analytical solutions for the purpose of demonstrating various types of numerical errors e g numerical dispersion wiggle that may result from user options for the more general ground water f t an analytical model verification has been performed involving the concentrations of three interacting chemicals in a porous medium the example utilized the van genuchten 1985 analytical solution as implemented in a web based tool at the university of illinois http hydrolab illinois edu gw applets the example considers advection dispersion linear equilibrium sorption and sequential decay of three interacting chemicals that multi state variable application is presented on the gem website https github com keithwlittle gem in addition to those f t verification exercises another verification analysis was performed to illustrate the gem flexibility to accommodate and solve pdes that do not strictly follow the format presented above as equation 4 that exercise manipulates equation 4 by a suitable redefinition of state variables and parameters to yield the nonlinear kinematic wave equation 2 2 in the gem s equation solver mode one would simply enter the finite difference equations themselves and no redefinition of state variables and parameters is necessary this approach would be completely generic analogous to using commercial equation solvers such as matlab or mathematica however the environmental system mode is intended to facilitate environmental numerical modeling and we are here exploring the ability to simulate environmental systems other than chemical mass f t using environmental system mode which describes transient surface water flow change with respect to time and distance along a stream channel nominally the gem is designed to simulate chemical f t given user specified flows in space and time this analysis models the transient flows themselves and also demonstrates the gem s nonlinear functionality an analytical solution presented by chow et al 1988 using a finite difference scheme on the underlying pdes is compared to gem results this verification exercise is also presented on the gem website https github com keithwlittle gem comparisons between gem simulations and analytical solutions for all above verifications are excellent two validation exercises have also been performed both comparing gem results with empirical results obtained from bench scale laboratory experiments measuring leaching characteristics in porous media for sorbing chemical elements both experiments are from a suite of methods recently approved by the us epa known as the leaching environmental assessment framework leaf developed by kosson et al 2002 these methods epa methods 1313 1316 3 3 https cfpub epa gov si si public record report cfm direntryid 231332 are designed to enhance representativeness of liquid solid partitioning characteristics in porous media relative to historical methods such as the well known toxicity characteristic leaching procedure tclp epa method 1311 which is known to be biased toward estimating maximum potential for leaching in landfill environments the leaf methods are more realistic for many in situ environmental settings the first gem leaf method validation was to epa method 1314 and is described in detail in a recent paper little et al 2017 koralegedara et al 2016 evaluated the leaching characteristics of trace elements in flue gas desulfurization gypsum and a soil fgdg mixture as a function of liquid solid mass ratio ls using epa method 1314 method 1314 places a sample of the material in a plexiglas column and pumps deionized water through the column at a relatively constant rate samples of leachate are collected at intervals and the dissolved chemical concentration measured the method results in a time series of measured dissolved concentrations in leachate along with the corresponding ls ratios for this verification analysis these data were then used to estimate regression functions of kd values as a function of ls ratios the kd ls relationships were then included in the gem and a simulation performed for a model of the bench column comparisons were excellent a second leaf validation for method 1313 data was performed and is presented on the gem website https github com keithwlittle gem it illustrates a gem application using multiple state variables a dynamic sorption assumption and a minimal number of gem compartments method 1313 is a batch experiment that measures chemical release from solid materials as a function of ph it is performed by tumbling the material at a ls ratio of 10 ml extractant g dry sample for 1 2 days and subsequently evaluating the dissolved chemical mass in the extractant it is then repeated at different ph values the method 1313 data used for this analysis were obtained from experiments performed at epa s national risk management research laboratory in cincinnati oh al abed personal communication february 2016 the materials used for this method 1313 experiment were crushed mine tailings the measured chemical element used for this analysis was aluminum al and the experimental ph was 2 13 once the dynamic gem simulation attained equilibrium sorption the simulated and experimentally obtained equilibrium concentrations were essentially identical the details of these verification and validation exercises are not presented here for brevity but the interested reader is referred to the gem website 4 multi media example 4 1 introduction our selected example to illustrate gem flexibility in this paper is a multi media chemical f t application there are essentially two approaches to simulating chemical f t across environmental media in a numerical modeling framework as previously mentioned given that most extant environmental models are medium specific the obvious strategy is to assemble a linked suite of medium specific models that taken together cover the media of interest linking these separate models is then performed in a one direction manner with outputs from one model becoming inputs to another the most ambitious instance of this strategy is likely the frames 3mra system under development by the u s epa babendreier and castleton 2005 this complex system is a tool to quantify human health and ecological risks associated with the disposal of hazardous wastes in various disposal beneficial use scenarios within a multi media context the system comprises 6 source models aerated tank surface impoundment waste pile land application unit landfill and roadway with the exception of the aerated tank and surface impoundment these source models are essentially vadose zone type f t models that generate time series chemical mass loadings to 5 medium specific models vadose aquifer surface water regional watershed and air as shown in fig 1 the frames 3mra system has been under development and refinement for more than a decade and the gem has recently been used to enhance the land application unit landfill and waste pile models and for a new source model the roadway model the source and media models operate sequentially the source model is first run the resulting time series of chemical mass fluxes are then saved and used as input to the receiving media models these models are then run sequentially with inter media fluxes being passed in one direction only the second approach is to build a single holistic multi media model from first principles including all relevant media inter and intra media f t processes of interest and appropriate numerical discretization to approximate the desired spatial and temporal resolutions this approach has historically been daunting especially for high resolution needs but it has been successfully used in lower resolution applications with fugacity modeling techniques e g mackay 1991 fugacity is a chemical specific thermodynamic property that represents the partial pressure of a chemical in a medium at chemical equilibrium the fugacities of the various media will be equal the fugacity concept is not a shortcut to multi media modeling it requires the same information as a comparable concentration based approach trapp and matthies 1998 but conceptualizing media as having different capacities to store chemicals is insightful and may well facilitate a holistic multi media approach certainly the inclusion of all media in a single program allows feedforward feedback relationships among media numerically fugacity models use a similar compartment construct as the gem to discretization of the spatial domain in the temporal domain level iv dynamic fugacity models use finite differences given the compartment construct it would seem that spatial resolution could be arbitrarily determined by the modeler but it is our impression that fugacity modeling applications have typically been of relatively low resolution for broad screening purposes this would seem to be a limitation of the available software and not the underlying numerical approach the gem multi media example presented below takes advantage of the fugacity approach of having all media represented in a single holistic construct allowing feedforward feedback among media unlike fugacity however the svs are in the more common concentration metrics it also has flexible resolution depending on how many compartments are user specified per medium and the size of the temporal time step thus it has advantages of both approaches to multi media modeling the example is an idealized hypothetical scenario intended for illustrative purposes only but is motivated by the frames 3mra land application unit scenario 4 2 conceptual multimedia model in the land application scenario chemical loads arise as a result of the beneficial reuse of waste material into surficial soils of a farm field as a soil amendment in fig 2 compartment 2 represents the tilled zone of a farm field chemicals of concern in the waste material are introduced via tilling into compartment 2 compartments 3 through 11 are soil vadose layers below the tilled zone that then receive chemical contamination via vertical percolation of stormwater and diffusion dispersion compartment 1 is a dummy compartment which is neither a modeled compartment a compartment for which mass balance equations are solved nor a boundary compartment it serves only as a device to introduce advective flows rainfall percolation into the system compartments 14 through 23 reflect adjoining soil vadose layers lying downslope of the farm field that can become contaminated via stormwater runoff and advective transport of chemical from the farm field followed by vertical percolation diffusion dispersion compartment 13 is the dummy compartment atop the buffer also to introduce rainfall percolation for simplicity the only communication between the soil columns 2 11 and 14 23 is assumed to be from 2 to 14 as a result of stormwater runoff thus the field buffer portion of the overall compartmental structure is quasi 2 d vertical transport is due to advection and diffusion dispersion in the longitudinal x direction and lateral y direction the field and buffer are 100 m 100 m vertically z direction each field and buffer compartment is 0 1 m below the field buffer compartments is a 1 d aquifer receiving chemical mass leachate fluxes from the overlying field and buffer the leachate enters aquifer compartments 12 and 24 from percolation and vertical diffusion dispersion the aquifer consists of 10 compartments numbered 12 and 24 35 each 100 m longitudinal x 100 m lateral x 10 m vertical advective flow in the aquifer is from left to right in the figure and compartments 36 and 37 are upgradient and downgradient boundary compartments respectively transport along the aquifer is due to advection and diffusion dispersion above a portion of the aquifer lies a 1 d surface water medium comprised of 10 compartments numbered 25 and 38 47 each is 100 m 100 m x 1 m vertical flow is from left to right and compartments 38 and 48 are upstream and downstream boundary compartments respectively surface water compartment 25 receives chemical mass loadings from buffer compartment 14 and this connection represents chemical mass loadings transported into the surface water from stormwater runoff from the buffer longitudinal transport is due to advection and diffusion dispersion interactions between the aquifer and surface water compartments are included as vertical dispersion across the compartment interfaces overlying both the surface water and soil compartments are 8 1 d atmospheric compartments each is 200 m longitudinal x 100 m lateral x 100 m vertical compartments 55 and 56 are boundary compartments longitudinal transport is due to diffusion dispersion only no advective transport is included each modeled non boundary air compartment interacts with the underlying surface water and soil compartments interactions between the atmosphere and soil or water media can be complex and can occur as volatilization of neutral dissolved ionic species from water or soil to air or wind erosion of sorbed species from soil to air and in the opposite direction wet or dry deposition of dissolved and sorbed species from air to soil or water here we consider only volatilization of dissolved chemical from soil and surface water to air and wet deposition from air to soil and water both as 1st order transfer processes dry deposition is not included because it would have an insignificant impact given the relatively low level of atmospheric suspended solids 10 g m3 used in the lateral y dimension the sets of compartments for soil surface water aquifer and atmosphere are assumed to not interact with adjoining media for simplicity thus the aquifer surface water and atmosphere media are modeled as 1 d x direction the soil compartments are quasi 2 d x and z if lateral interaction with adjacent media were desired extending the compartment structures in this dimension to 2 d and including appropriate boundary compartments would be used with the uniform lateral width of 100 m for all compartments one could consider the conceptual model to be a representative 100 m lateral slice of an arbitrarily wide 1 d multi media system regarding the compartment numbering scheme the gem requires only that compartments be integer numbered in sequential order beginning with 1 which number applies to which compartment is irrelevant the compartment numbering scheme used in this example was selected to facilitate piece wise building up of the entire multi media structure for purposes of debugging for example the first mini model was developed for the soil medium system only i e compartments 1 25 for this mini model the aquifer compartments 12 and 24 were specified as lower boundary compartments and the surface water compartment 25 was a dummy compartment simply to receive the runoff flows as more media and more compartments are added the purpose and function of these compartments was changed we strongly recommend this step wise approach to building complex models to users of the gem or any other software the gem flexibility with regard to compartment numbering and ease of adding additional compartments as needed was key to use of this piece wise building up of the complete model domain it is further noted that in actual practice appropriate compartmental design will place more and smaller compartments in the vicinity of anticipated high concentration gradients e g near point sources to provide sufficient spatial resolution to capture peak concentrations thomann and mueller 1987 for simplicity we adhered to this practice only in the sense of relatively much smaller depths in the soil compartments relative to the other media 4 3 model parameterization a detailed description of the assumptions and parameterization of the model is presented below for the purpose of establishing the representativeness of f t processes used in each medium of our hypothetical example the reader not interested in these details may skip this section advective flows in each medium are assumed for simplicity to be at steady state and were parameterized to reflect typical conditions vertical percolation in the soil compartments is based on a national average u s precipitation rate of 715 mm year 4 4 http data worldbank org indicator ag lnd prcp mm and assumes that 25 is lost as evapotranspiration of the remainder 50 268 mm yr occurs as stormwater runoff from the surficial soil compartments the remainder after runoff is vertical percolation through the soil compartments to the underlying aquifer at an assumed soil porosity of 0 05 fully saturated this percolation has a constant pore water velocity of 0 02 m day the advective flow rate in the aquifer with a water content of 0 1 was chosen to result in a longitudinal pore water velocity 10 times the velocity of the soil pore water or 0 2 m day advective flow rate in the surface water was chosen to result in a velocity of 0 1 m second 8 6 103 m day for the air no net advective flow is assumed and all transport is due to longitudinal dispersion diffusive dispersive transport is simulated both within and between media for the soil compartments we assumed that the relatively slow percolation rate does not induce dispersion and that vertical mixing is due to molecular diffusion only a value of 1 10 4 m2 day was used both for vertical dispersion between soil compartments as well as to the underlying two aquifer compartments based on a representative molecular weight of common chemicals of concern for the aquifer with a somewhat greater advective flow velocity we used a longitudinal value two orders of magnitude higher 1 10 2 m2 day to reflect some macro dispersion for the surface water with a velocity of 0 1 m s we used a longitudinal value of 1 104 m2 day based on a measured longitudinal dispersion coefficient in the monocacy river maryland with similar velocity schnoor 1996 table 2 2 for the atmosphere dispersion at 1 7 106 m2 day was used for purposes of comparing this value to other transport rates the dispersion term in the mass balance equations can be considered as an advective flow rate by dividing the dispersion coefficient by the transfer distance to yield a mass transfer velocity considering the distance between adjacent air compartment centroids to be the transfer distance the dispersive transfer flow velocity is then 1 7 106 m2 day 200 m 8 6 103 m day identical to the surface water longitudinal flow velocity dispersion between media is assumed only for the aquifer and surface water and a value of 10 m2 day representing a mid range of the aquifer and surface water longitudinal values was used for dissolved chemical only for fate processes we considered two idealized chemicals the fate processes and their parameter values are intended to be realistic but not necessarily representing any particular chemicals in addition to solid liquid partitioning fate processes include volatilization from surficial soils and surface water to air and settling of airborne vapor from the air to soil and surface water for surface water and soil volatilization we used a transfer velocity vv of 0 1 m day similar to estimates of volatilization transfer velocities for pcb in lake huron chapra 1997 for the air to water and air to soil vapor phase settling mechanism we used a settling velocity vvd of 0 2 m day twice the volatilization transfer velocity we chose to explicitly simulate dissolved and sorbed chemical concentration in all media as separate svs the gem allows specification of which sv is relevant to which compartments so alternative designs are possible e g dissolved only in the porous media compartments soil aquifer and both dissolved and sorbed in the surface water and air or dissolved in the porous media and total in surface and air nonetheless the 2 sv formulation seemed most flexible given the 2 sv approach we also treat sorption desorption as a dynamic process that is slightly at non equilibrium for ease of agreement with the gem default dissolved concentration metric we let the sorbed chemical concentration in all media be expressed as chemical mass per water air volumetric volume g m3 w not total compartmental volume g m3 t conversion to another metric i e the standard gc gs would be accomplished by multiplying the water air volume concentration by θ bd at the relatively dilute solids concentrations we assumed see bd in table 1 this is a moot point because the water or air content θ is 1 0 for all practical purposes however for other systems e g a high solids concentration slurry this distinction would be meaningful dissolved chemical cd is sv 1 while sorbed chemical cs is sv 2 regarding the parameterization of solids partitioning parameters we simulated two chemicals with different partitioning behaviors one chemical is a weakly sorbing chemical that is transported through the modeled system relatively quickly while the second is a strong sorber with a longer persistence in the environment for simplicity we assumed that the kd values do not vary in time e g with cumulative liquid solid ls ratio or ph kd values do vary by solids characteristics in different media and we reviewed kd values for various metals by environmental medium estimated by the us epa us epa 2003 that source presents typical kd values for soil sediment and surface water for 17 hazardous metals although there is considerable variability in relative differences across these media among the metals there is a general tendency for an increasing kd from soil to sediment to surface water in many cases this increase was approximately an order of magnitude accordingly we assigned for both simulated chemicals a difference in kd among media of one order of magnitude although aquifer solids media were not a focus of the epa study we here treat the aquifer as sediment for purposes of assigning kd thus for a given kd for soil the aquifer sediment is 10 times the soil value while the surface water is 100 times the soil value for air we assume the same kd value as soil given that air particulates are comprised to some extent of soil material for the values of the equilibrium kd values we selected a soil kd that results in a 90 fraction dissolved in soil for the weakly sorbing chemical for the strong sorber we selected a soil kd that results in a 10 dissolved fraction in soil the resulting kd values by medium are shown in table 1 along with the calculated kds and ks desorption sorption rate constants needed for the dynamic sorption kinetics used for the 2 sv formulation the fd values were calculated from the relationship f d θ c d θ c d ρ b c s see discussion of equivalence of surface water and ground water equations on gem website https github com keithwlittle gem the ks and kds values were then calculated from the equilibrium kd in accordance with the relationship k s k d s 1 f d 1 see e g schnoor 1996 little 2012 4 4 mass balance equations the mass balance equations assembled and solved by the gem for this example are summarized below note due to the gem internal equation building algorithm the units on each term are not strictly mass time m t but rather m l3 t t l3 w where l3 t is total volume and l3 w is pore space volume 4 4 1 soil compartments i 13a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k v v i c d i a i k v v d k c d k 13b v i r i d c s i d t v i c s i k d s i v i c d i k s i a i k v s d k c s k where k is the index for the overlying air compartment 49 and ri is given by equation 2 for our dynamic sorption approach kd is set to zero making r 1 and the sorption desorption mechanism enters via the ks and kds sorption desorption processes for the surficial soil compartments 2 and 14 the last two terms on the right hand side of the cd equation are respectively the sink term due to volatilization and the source term due to vapor deposition where vs and vv are the vapor settling and volatilization transfer velocities m day air deposition and volatilization terms are not included for non surficial soil compartments 4 4 2 aquifer compartments i 14a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i 14b v i r i d c s i d t v i c s i k d s i v i c d i k s i 4 4 3 surface water compartments i 15a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k v v i cd i a ik vvd k cd k 15b v i r i d c s i d t j 1 n a i q i j θ i c s i j j 1 n a i e i j c s j c s i v i c s i k d s i v i c d i k s i a i k v s d k c s k where k is the index for the overlying air compartment 4 4 4 air compartments i 16a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k vv k cd k a i k a i k vvd i cd i 16b v i r i d c s i d t j 1 n a i q i j θ i c s i j j 1 n a i e i j c s j c s i v i c s i k d s i v i c d i k s i a ik vsd i cs i 4 5 results before simulating the full multi media system as developed above we performed a side analysis to demonstrate the equivalency of a 1 sv dissolved versus 2 sv dissolved sorbed approach for the soil medium only the motivation for this analysis was to emphasize the flexibility of alternative modeling designs as well as the flexibility of the gem in addition it serves as an illustration of an instantaneous sorption equilibrium approach kd method versus a dynamic sorption approach ks and kds approach the interested reader can find that side analysis in the appendix returning to the 2 sv approach we ran the gem to simulate the f t throughout the multi media system of a spill of dissolved chemical to the field surficial soil compartment 2 the spill was represented as an initial condition for sv 1 in that compartment of 1000 g m3 runs were made for both the strongly sorbing chemical 1 and weakly sorbing chemical 2 chemical parameters as described above all feedforward and feedback inter media f t processes as described above were included to avoid being restricted to relatively small compartment volumes to satisfy the peclet criterion and to avoid relatively small time steps to satisfy the courant condition we used the btbs gem option this option is both unconditionally stable and avoids negativity wiggle but does introduce temporal and spatial numerical dispersion however for purposes of our illustrative multi media example that disadvantage was not considered critical relative to having appropriately few compartments for simplicity of presentation as well as being able to make large time steps to enable tracking the spill s progress through all media the simulations covered 10 000 days at a 10 day time step boundary conditions for the upstream boundary compartments 36 aquifer and 38 surface water were specified as 0 g m3 as well as both boundary compartments for the air medium 5 5 we originally used a zero concentration gradient boundary condition for the air boundaries however for the no feedback scenario discussed subsequently where no air deposition is included the only mechanism left available to remove chemical mass from the air medium is dispersion no advective transport was included once the air compartments attained a zero concentration gradient which occurs well above zero concentration there was no gradient under which dispersion would operate thus the chemical mass present at that time remained indefinitely this is exactly what should happen under those conditions however it does not suit our illustrative purposes here and we reverted to a zero fixed boundary condition 55 and 56 for the downstream boundary compartments 37 and 48 a zero concentration gradient boundary condition was used table 2 presents the gem s internal mass balance checks over the simulation the gem keeps a running total of chemical mass initially present entering the system during the simulation and mass leaving the system either from transport or endogenous decay volume based sinks out or volatilization area based sinks out and residual system wide mass our volume and area based sinks do not leave the system but rather are among internal media mass only leaves the system through transport processes these results are reported by sv for summary user information a total mass balance error across all svs is then calculated and reported at the bottom of the table as a percentage of cumulative initial mass present and any mass input during the simulation as can be seen the mass balance errors for each chemical are insignificant it can also be seen that essentially no chemical mass remains in the system for the weakly sorbing chemical 2 while approximately 10 of the initial mass remains for the more strongly sorbing chemical 1 we consider now the propagation of the spill mass through the various media for sv 1 dissolved the more relevant chemical specie for purposes of toxics risk analysis fig 3 a presents the time series of concentrations in selected compartments over the 10 000 day simulation for strongly sorbing chemical 1 the compartments were chosen to show the spill compartment field soil top 2 in the figure as well as compartments at the downstream edge of each medium these results show the relative differences in concentrations among the media over time at the edges of each medium recalling that the chemical mass is initially concentrated in compartment 2 at 1000 g m3 we see the depuration of the soil spill in that compartment field soil top 2 with subsequent increases in concentrations at the bottoms of the soil media compartments field soil bottom 11 and buffer soil bottom 23 as chemical is transported vertically in the soil because of the 10 day time step the depuration increase prior to day 10 is not shown these profiles clearly show increasing followed by decreasing concentrations as the peak passes out the bottom of the soil medium for the downstream surface water compartment sw downstream 47 downstream aquifer compartment aquifer downstream 35 and air far right compartment air right 55 these compartments are relatively quickly contaminated due to surface runoff from the field and buffer surface water to air volatilization surface water to aquifer dispersion and subsequent downstream transport and thus show a similar depuration pattern as the field soil top 2 itself albeit with much lower concentrations as the peak concentrations begin to pass the bottom of the field and buffer soil layers the transfer of that mass into the surface water aquifer and air media is reflected in less rapid depuration or even slight increases in concentration in those media aquifer and surface water concentrations at the downstream compartments have essentially equilibrated after approximately 300 days due to their interaction peak concentrations have occurred at the edges of each medium during the 10 000 days due to among media mixing of chemical mass the peak concentrations vary by several orders of magnitude among the media fig 3b presents similar results for the weakly sorbing chemical 2 shown on the same vertical scale as fig 3a to facilitate between chemical comparison compared to fig 3a we see generally the more rapid depuration of chemical mass in all media because chemical 2 is more mobile it is also of interest to observe within medium changes over time fig 4 a and b shows these results for strongly sorbing chemical 1 and weakly sorbing chemical 2 respectively for several of the soil buffer compartments while figs 5 7 are similar chemical comparisons for the aquifer surface water and air media respectively vertical scales are fixed for each medium to facilitate comparisons the progression of the spill wave through the media is obvious and the more rapid mobility of chemical 2 can also be readily seen recall that multi media chemical f t modeling can be performed either by linking together several medium specific models in a feedforward mode or by building a holistic multi media model as we have done here the primary advantage of the holistic approach is that feedback among the media can be incorporated whereas for the linked single media approach this is problematic 6 6 but not impossible on the gem webpage we include a description of how a newton s method iterative algorithm may be used to iterate among single medium models to effect full feedback among the various media it would nonetheless be a computationally demanding approach the obvious question is then how much difference do these two approaches feedforward only or feedforward feedback make in concentrations among the media the results shown above include full feedback by removing the inter media feedback processes from the example model the feedforward only approach is easily mimicked accordingly we removed the soil aquifer and aquifer surface water dispersion transport mechanisms and the air compartments vapor deposition process under these conditions the soil compartments are contaminated only by the spill itself no feedback from air the aquifer is contaminated only by advective leachate from the lowest soil compartments no dispersion from surface water or the overlying soil compartments the surface water is contaminated only by stormwater runoff from the soil buffer no aquifer dispersion and the air is contaminated only by volatilization from the surficial soil and the surface water we ran this modified model again for 10 000 days at a 10 day time step for both chemicals 1 and 2 and compared the resulting time series concentrations to the above full feedback concentrations these comparisons were performed for each medium for compartments approximately midway in each medium as would be expected from the full feedback results above where the air concentrations are several orders of magnitude less than concentrations in any other medium the issue of whether vapor deposition is affecting soil or surface water concentrations or not would not significantly influence soil or surface water concentrations that indeed was the result however some relatively significant differences between the feedback no feedback scenarios were observed for the surface water and aquifer media fig 8 a and b shows feedback versus no feedback results for mid depth field soil compartment 6 for chemicals 1 and 2 respectively similarly figs 9 11 show feedback versus no feedback for mid length aquifer surface water and air for both chemicals results are shown on the same vertical scale by medium to facilitate comparisons between chemicals for mid depth field soils there is no appreciable difference for feedback versus no feedback for chemical 1 which is not surprising given that feedback involves inclusion of vapor deposition and the air concentrations are much much lower than soil concentrations therefore feedback is minimal for chemical 2 there are approximately 1 2 order of magnitude differences after around 300 days although these are very low concentrations however what is important in these comparisons is not the absolute value of the concentrations because those will change linearly with any change to the spill s initial condition assumption but rather the differences in orders of magnitude between the two scenarios for the mid length aquifer the no feedback results for chemical 1 are many orders of magnitude less than the feedback scenario for most of the simulation for example at day 10 the no feedback concentration is 18 orders of magnitude less than the full feedback version presumably because leachate has not yet reached the mid length aquifer and no dispersive transport is occurring from the surface water for more mobile chemical 2 there is relatively less difference in initial concentrations although the differences still span some 6 orders of magnitude the time to reach similar concentrations is also much shorter than for chemical 1 due to the enhanced mobility for the mid length surface water compartment for chemical 1 the results between the two scenarios are approximately the same until day 200 after which the no feedback concentrations become much less than the feedback scenario this is because with no feedback the chemical gets washed out of the surface water relatively quickly while with feedback the underlying immobile aquifer solids store chemical being dispersed from surface water to aquifer and leach it out to the overlying surface water much more slowly over time for chemical 2 this same pattern occurs although the concentration differences are much greater and the departure in similar concentrations occurs much earlier for mid length air compartments a similar pattern to the surface water is seen this reflects the fact that the air medium is being contaminated in large part by surface water in summary for this example we demonstrated the gem s flexibility to accommodate f t processes within and among different environmental media including both feedforward and feedback processes for maximum flexibility a 2 sv approach was used although other designs are possible the spatial compartment resolution varied from relatively fine in the soil medium to capture vertical concentration gradients to relatively coarse in the air medium where rapid dispersive transport and no retardation lessen sharp gradients examples of biases among media concentrations that might result when multi media modeling approaches do not incorporate fully holistic feedforward and feedback processes were also illustrated this example was not designed to maximize these biases but only to illustrate and quantify them for a particular chemical loading scenario and set of realistic physical chemical parameters other scenarios e g introduction of spills to the atmosphere instead of surficial soils would give rise to different relative results 5 summary and conclusions a new open source chemical f t modeling software tool has been developed and is summarized and illustrated in this paper the gem modeling framework accommodates arbitrary dimensionality 0 1 2 or 3 d due to its intuitive compartmental basis it provides dynamic solutions from three user selected options ft bt or ct and also has a steady state option spatial discretizations are available using either bs or cs methods the temporal and spatial discretization options involve relative tradeoffs between accuracy of simulations allowable time steps and compartment sizes and presence of numerical errors processes included are advective and dispersive transport instantaneous equilibrium or dynamic sorption arbitrary volume based 1st order source sinks such as decay or feedforward feedback interaction among chemicals and arbitrary area based mass transfers across intercompartmental interfaces e g settling or volatilization multiple interacting svs are allowed with the ability to user specify which svs are relevant to which compartments the theoretical basis of the framework consisting of well known mass balance partial differential equations and their numerical representation using the compartment discretization method was presented along with a hypothetical but representative multi media example to demonstrate capabilities this example also illustrates differences that might be expected between holistic models that incorporate all inter media f t processes and models that simulate processes only in one direction among media the software code as well as sets of input files for most of the examples mentioned in this paper are available on the gem website https github com keithwlittle gem we feel that this software represents a valuable contribution to the environmental modeling community and can facilitate relatively rapid development of complex environmental models using non proprietary tools our goal in making this software available is to encourage and simplify rigorous quantitative analysis of complex environmental contamination issues and the authors are available to address questions or comments that users may have via the website acknowledgments the experimental leaf methods 1313 and 1314 validation exercises were supported under a contract with rti international with the u s epa national risk management research laboratory nrmrl cincinnati ohio we also thank the three anonymous jems peer reviewers for their insightful suggestions appendix equivalency of 1 sv versus 2 sv models for soil medium of multi media example in this side analysis only the soil compartments are considered i e compartments 1 25 compartments 1 and 13 are upper boundary compartments compartments 12 and 24 serve as lower boundary compartments and compartment 25 is a dummy compartment to receive runoff flows from compartment 14 volatilization of dissolved chemical to the atmosphere is included but feedback deposition from the atmosphere wet and dry deposition are not included because the air compartments are not being simulated for the 2 sv model the two equations are given by equations 13a and 13b omitting the vvd vapor deposition source term in these equations r is understood to be 1 0 kd 0 for the 1 sv model dissolved concentration the mass balance equation is given by equation 13a omitting the vvd kds and ks terms r is determined as r 1 ρ b k d θ and kd is not equal to zero we ran both models for the strongly sorbing chemical 1 see table 1 for model 1 s kd and model 2 s ks kds ratio for the 1 sv model the initial dissolved concentration in compartment 2 was 100 g m3 representing a spill of some aqueous waste for the 2 sv model the same initial condition for cd was used for cs an initial condition of 900 g m3 was used reflecting the sorbed fraction at equilibrium corresponding to the initial cd concentration see table 1 for the 2 sv model the ks and kds values were assigned in accordance with the ks kds ratio shown in table 1 and with an assumed value for kds of 0 1 day ks 0 9 day these 1st order rate constants are reasonably fast e g an 0 1 day rate constant can be interpreted as a process affecting approximately 10 of the total chemical mass per day chapra 1997 but would still not represent instantaneous equilibrium as implicit in the 1 sv model we ran the models for 1000 days and the time series results from the two models for cd in the top and bottom compartments are shown in fig a 1 for the farm field column and the buffer column we see several phenomena in the figures first the comparison between top and bottom compartments in both figures shows the downward movement of the chemical due to transport over the 1000 days peak concentration in the field s top compartment is at time 0 the spill while the peak concentration at the bottom compartment occurs at approximately 400 days peak concentration in the buffer top compartment is at approximately 30 days as the runoff from compartment 2 transports chemical into compartment 14 the concentrations in the buffer compartments are also reduced significantly from those in the field compartments due to spatial dilution of the mass and loss due to volatilization regarding the instantaneous 1 sv model versus dynamic sorption 2 sv model assumptions results of the two models are quite similar but with some differences due to slight non instantaneous equilibrium conditions in the 2 sv model at our specified ks and kds absolute values a final run of the 2 sv model was made increasing both these values an order of magnitude and the results are essentially identical to those of the instantaneous equilibrium model thus showing the equivalence potential of these two approaches the advantage of the instantaneous equilibrium approach is parsimony in the number of svs modeled while the advantage of the dynamic sorption approach is maximum flexibility 
26408,a new open source software tool the generic environmental model gem is summarized and illustrated herein the gem is a numerical approach to solving the classical advective dispersive partial differential equations describing chemical fate and transport in environmental media its generic capabilities are due to the flexible compartment approach for spatial discretizations as well as an architecture that makes it amenable to a wide variety of environmental problems theoretical underpinnings and capabilities are described in this paper and are illustrated with a multi media example using a holistic approach to multi media modeling that includes both feedforward and feedback processes among media the holistic approach is compared to results that might arise from a more conventional approach using a set of single media models coupled together in a feedforward only fashion the gem is open source and was developed to facilitate rapid model building for a wide variety of environmental contamination problems the authors encourage users to download and use the tool keywords environmental modeling chemical fate and transport analysis multi media modeling 1 introduction this paper introduces a new software tool designed to facilitate fate and transport f t modeling of toxic chemicals in a wide variety of environmental settings the software named the generic environmental model gem is flexible mathematically rigorous based on fully numerical solutions to the underlying partial differential equations pdes and available on line as open source link https github com keithwlittle gem it is written in the java language the gem is intended as a tool that allows users to quickly develop accurate solutions to pdes that are unique to their chemicals environmental settings and spatial temporal resolution of interest the architecture allows the user to build that set of pdes of particular interest and obtain solutions to those pdes rather than the classical approach of force fitting the problem into an existing black box model which may not honor all of the f t features of interest the gem is extensively documented including detailed user instructions and examples in little 2012 regarding a literature review of similar models and comparative differences we are not aware of another open source environmental modeling platform that has these generic features goldsim http www goldsim com web products goldsimpro is an environmental modeling platform that has f t capability but is proprietary in the public domain several of the fugacity models originally developed by mackay 1991 have generic capability and are typically being used for regional multi media modeling applications however they operate on fugacity based pdes and seem more generally applicable to low spatial and temporal resolution applications following a description of the gem tool s theoretical basis functionality and a summary of verification validation exercises its use is illustrated via a multi media chemical f t example the multi media domain is hypothetical but realistic and comprises soil aquifer surface water and air media a holistic gem based model is set up and dynamically executed and includes feedforward and feedback processes e g volatilization settling dispersion both within and among the media a second simulation is performed wherein inter media processes are included only in a feedforward manner i e from one medium to another to mimic conventional multi media modeling tools that couple single medium models with outputs from one model becoming inputs to another in one direction only the holistic results are compared to feedforward only results 2 functionality design motivation and numerical approach 2 1 overview of functionality the generic environmental attribute of the gem tool arises from several design features it is designed as an environmental modeling platform or framework rather than a model with specific functionality users enter their specific data regarding state variables svs environmental topology parameters and processes and the corresponding pdes are then assembled and solved it is based on the generalized pde describing advective dispersive f t of chemical mass in porous media which can be simplified to surface water or air media the numerical approximation to the spatial derivatives uses the compartment method which is mathematically equivalent to standard finite difference approximations with the exception that the spatial domain itself is discretized rather than the spatial derivative terms in the underlying pde this seemingly trivial difference has the practical benefits of being both highly intuitive to the user as well as being robust to alternative dimensionalities thus a 1 d problem is easily converted into 2 or 3 d simply by adding more compartments in those dimensions temporal derivatives are approximated by standard finite difference methods the spatial discretization options are either back space bs or center space cs temporal discretization options are forward time ft back time bt or center time ct the choices have implications for allowable time step sizes and compartment volumes vis a vis avoidance of numerical errors numerical dispersion wiggle negativity complete parameterization of the model is achieved through straightforward comma separated value csv text files fate processes types and parameterization are highly flexible and provided by the user the csv files are readily generated by the user using a spreadsheet csv files for most of the examples mentioned later are available on line https github com keithwlittle gem in particular a set of input files for a relatively simple soil column application is included along with a tutorial describing the structure of the associated input files see start here folder the new user is encouraged to first become familiar with this example and make sequential changes to its files as needed to build input files for new applications multiple interacting svs are possible including specification of the compartmental relevancy of each sv a quasi newton iterative algorithm is included allowing nonlinear capability although designed to build systems of equations describing f t of chemical mass in environmental systems where hydrodynamic flows among compartments are user inputs the gem can also be used to solve the pdes describing the hydrodynamic movement of water itself by a suitable change of variables and reinterpretation of parameters indeed the gem can be used either in environmental system mode where it assumes the user is building environmental f t type pdes or in equation solver mode equation solver mode is completely general and the user enters the equations of interest that may have nothing to do with environmental modeling and the gem subsequently solves those equations 2 2 underlying pde the gem is designed primarily for chemicals that among other processes partition between liquid solid and gaseous phases in surface water and or porous media domains despite the fact that the underlying pdes describing these partitioning processes in water porous media domains are remarkably similar the historical development of f t software has almost exclusively been compartmentalized into separate surface water models e g wasp https www epa gov exposure assessment models water quality analysis simulation program wasp and porous media models e g hydrus http www groundwatersoftware com hydrus htm this departure in software tools is principally because the underlying pdes describing system hydrodynamics are quite different between these two media however once the flows are known the f t pdes can be shown to be equivalent and can be used for either medium this commonality is the motivating principle behind the gem which receives spatial and temporal flow distributions as user inputs to the f t pdes the underlying pde solved numerically by the gem is summarized below the classical advective dispersive equation describing chemical mass balance in a 1 dimensional x porous medium with equilibrium partitioning can be written as e g schnoor 1996 or fetter 1993 1 r c d t u x c d x d x 2 c d x 2 where cd is solute dissolved concentration mc lw 3 ux is the advective pore water velocity lw t dx is the diffusive dispersive mixing coefficient through the pore water lw 2 t r is the dimensionless retardation coefficient defined as 2 r 1 k d ρ b θ where kd is the solids liquid partitioning parameter lw 3 ms ρb is the bulk density of the solids ms lt 3 θ is the porosity or water content lw 3 lt 3 we are distinguishing between chemical and solids mass by using subscripts c and s respectively and water and total volume by subscripts w and t respectively for water containing sorbing chemicals flowing through porous media the retardation parameter r acts to slow down the movement of the chemical relative to the water itself because it is being retarded by sorption onto the immobile solid media in addition in ground water applications one is typically more interested in the chemical concentration in the flowing water than in the concentration of chemical on the solids which have been left behind hence the sv cd in equation 1 is dissolved concentration equation 1 is the underlying pde in 1 d solved numerically by the gem although commonly associated with contamination in porous media the equation is extremely flexible for example consider partitioning chemicals flowing in a surface water medium containing suspended solids the chemical is partitioning between solid and liquid phases just as in the porous medium scenario however it is typical for toxic surface water models to simulate total dissolved plus sorbed chemical as the sv rather than dissolved chemical this is because the sorbed chemical is not being left behind in space and time retarded on the solids as on immobile porous media but rather exists in the same location in space and time as the water itself just in a different phase modeling total rather than dissolved may also lead to simpler model constructs substituting total concentration ct mc lt 3 for cd in equation 1 and letting r 1 because it is irrelevant when modeling total concentration results in the commonly used 1 d advective dispersive equation describing total chemical concentration mass balance in surface waters see e g thomann and mueller 1987 or chapra 1997 3 c t t u x c t x d x 2 c t x 2 a more formal mathematical proof of the equivalence between equations 1 and 3 using the equations of sorption is included on the gem website https github com keithwlittle gem the commonality of the generalized porous media and surface water pdes provided the major motivation for the architecture of the gem s numerical approach it is based on the more general porous media equation 1 and capable of simplification to pure water or even air 1 1 to the extent that the movement of air can be described as the movement of an incompressible fluid the gem can be used for chemical f t in air media media the gem s spatial numerical approach extends equations 1 and 3 to multiple dimensions as described below 2 3 compartment based spatial method to discretize the spatial derivatives in 1 a compartment approach is used wherein the problem spatial domain is assumed to consist of a set of completely mixed volumes or compartments concentrations are constant throughout a compartment but can vary among compartments the compartment method is also variously known as the finite section or finite volume method not to be confused with the finite element method commonly employed for ground water models and was pioneered by robert thomann 1972 for surface water modeling applications it is mathematically rigorous and importantly for the gem architecture highly intuitive and dimensionally flexible because one is discretizing the spatial domain itself instead of operating on the underlying pdes accordingly the gem mass balance differential equation for arbitrary compartment i for linear problems is 4 d r i v i c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i k i c d i w i θ i the terms in this equation are described below cdi is the sv and is dissolved chemical concentration mc lw 3 in the interior of compartment i where mc denotes mass of chemical and lw 3 denotes volume of pore water cdij is the concentration mc lw 3 at the interface of compartments i and j the index na number adjacent denotes that the summation is over all compartments that are adjacent to compartment i it should be noted that adjacent compartments can be in 1 2 or 3 dimensions the specific dimensionality of the problem is irrelevant to the underlying mathematics of the compartment method which was a major factor for its adoption by the gem qij is the advective water volumetric infiltration rate lw 3 t between adjacent compartments i and j in the compartment technique qij is assigned as positive or negative to differentiate between flows entering compartment i and flows leaving compartment i as 5 q i j 0 if flow is from j to i q i j 0 if flow is from i to j to define the problem in terms of concentrations only in the compartment interiors cdij can be expressed as a linear function of the concentrations in the interiors as 6 c d i j α i j c d i 1 α i j c d j if q ij 0 c d i j α i j c d j 1 α i j c d i if q ij 0 where α i j is a user specified spatial differencing parameter that can take on values between 1 and 0 for α i j 1 a back space bs differencing method is being used where concentrations at the interface are determined only by concentrations in the upstream compartment interior for α i j 0 5 a center space cs method is being used where concentrations at the interface are determined by concentrations in both adjacent compartments α i j 0 would be a forward space fs method where interface concentrations would be determined only by the downstream compartment however the fs method has nothing to recommend it by way of any advantage over the bs or cs method and is not included as an option in gem ri is the retardation factor unitless given by 7 r i 1 ρ b i θ i c s i c d i where ρ b i soil bulk density ms lt 3 where ms denotes mass of solids θ i water content lw3 lt3 csi sorbed phase concentration mc ms vi is the compartment total volume lt 3 e ij is a fickian dispersion diffusion flow transport process between i and j given by 8 e i j e i j a i j l i j where eij hydrodynamic dispersion or diffusion coefficient lt 2 t aij cross sectional interface area between compartments i and j lt 2 l i j distance l between compartments i and j across which the dispersion diffusion process is operative ki is an arbitrary 1st order loss rate constant t the first order loss term v i k i c d i in equation 4 is meant to be illustrative of source sink processes the gem input files allow the user to specify sources and sinks as appropriate to their application wi is an external mass loading of chemical to the compartment mc t for linear equilibrium partitioning between dissolved and sorbed phases cs is related to cd as 9 c s i k d i c d i where kd is the partitioning coefficient l3 w ms for the linear case 10 c s i c d i k d i 2 4 temporal methods and user options after the compartment approach is applied to discretize the spatial domain the underlying pdes are reduced to ordinary differential equations in time conventional finite difference methods are then used to discretize the time domain for a single sv e g a single chemical constituent a mass balance equation based on 4 is written for each of the n modeled compartments the mass balance states that the change in mass in a compartment over time is equal to the compartment s inputs transport in and fate sources minus its outputs transport out and fate sinks these equations can be expressed compactly using matrix notation as 11 d r v c d d t a c d w where matrices and vectors are shown in bold vectors are shown in bold and underlined r and v have dimension n x n i e rows and columns and are diagonal matrices the non diagonal elements are zero containing respectively the retardation terms and the compartmental volumes matrix a is a n x n matrix containing mass transport terms and linear source sink terms vector w is a n x 1 vector of external inputs and may also include boundary conditions vector cd is the n x 1 vector of unknown concentrations for which a solution is sought matrix equation 11 is intended to be completely general if there were 2 svs in each compartment then there are double the number of equations and the dimensions of the matrices and vectors in 11 would be 2n for multiple svs the gem functionality does not require that all svs be relevant to all compartments the solution algorithm reads the system of equations represented by 11 reads relevant user solution parameters i e time steps duration of simulation and numerical solution options and solves the equations in time user options for numerical solutions include bs or cs methods for the spatial differencing parameter α i j as mentioned for the temporal derivative finite difference approximation method one explicit and two implicit options are available the explicit option is a forward time ft method also known as an euler solution two implicit options a back time bt and center time ct are available the user choice among these spatial and temporal differencing options involves choices between numerical errors spatial and temporal resolution and simplicity of application bs methods introduce spatial numerical dispersion but are not subject to spatial oscillations or wiggles including non positivity cs methods do not introduce numerical dispersion but may require relatively small compartment volumes as determined by the peclet number first proposed by jean claude peclet in the 18th century the ft method has the general advantage of not requiring solution of simultaneous equations since it is an explicit method but introduces temporal numerical dispersion and may require relatively small time steps to avoid numerical instabilities as governed by the courant friedrichs lewy condition courant et al 1967 the bt method is implicit and requires solution of simultaneous equations but is not subject to numerical instability and large time steps are possible it does introduce temporal numerical dispersion however which increases with increasing time step the ct method is also implicit and is subject to short time steps via the courant condition to avoid numerical instabilities at least in the temporal neighborhood of significant changes in loading conditions but has no temporal numerical dispersion little 2012 provides a detailed description of these numerical considerations for the gem to illustrate consider the ctcs option commonly known as the crank nicolson method although subject to both the peclet and courant criteria and may require small compartment volumes and time steps it introduces neither spatial nor temporal numerical dispersion and is thus the most accurate option at time step t the ctcs method is for linear systems r v c d t 1 r v c d t δ t a c d w t a c d w t 1 2 which can be written in the form 12 r v t 1 δ t 2 a t 1 c d t 1 r v c d t δ t 2 w t 1 a c d t w t the unknowns cd t 1 are on the left hand side of equation 12 and all knowns are on the right hand side this resulting system of simultaneous linear equations is then solved using the gem s internal linear algebra solution algorithm at each time step 3 summary of verification and validation examples model verification is the process of confirming that a model accomplishes its intended mathematical purposes and is essentially a quality assurance step validation is a stronger test and involves using the model to successfully reproduce data that were not used for model calibration it should be noted that the unparameterized gem is simply a modeling framework not a model of anything to verify or validate the gem itself is meaningless as it is simply a tool to build user specified mass balance equations for arbitrary environmental applications nonetheless the gem has been parameterized for both surface water and ground water systems and verified and validated for those specific applications for surface water f t several verification examples including advective dispersive transport and equilibrium sorption for a single state variable dissolved chemical are presented in chapter 6 of little 2012 gem numerical results are compared to analytical solutions for the purpose of demonstrating various types of numerical errors e g numerical dispersion wiggle that may result from user options for the more general ground water f t an analytical model verification has been performed involving the concentrations of three interacting chemicals in a porous medium the example utilized the van genuchten 1985 analytical solution as implemented in a web based tool at the university of illinois http hydrolab illinois edu gw applets the example considers advection dispersion linear equilibrium sorption and sequential decay of three interacting chemicals that multi state variable application is presented on the gem website https github com keithwlittle gem in addition to those f t verification exercises another verification analysis was performed to illustrate the gem flexibility to accommodate and solve pdes that do not strictly follow the format presented above as equation 4 that exercise manipulates equation 4 by a suitable redefinition of state variables and parameters to yield the nonlinear kinematic wave equation 2 2 in the gem s equation solver mode one would simply enter the finite difference equations themselves and no redefinition of state variables and parameters is necessary this approach would be completely generic analogous to using commercial equation solvers such as matlab or mathematica however the environmental system mode is intended to facilitate environmental numerical modeling and we are here exploring the ability to simulate environmental systems other than chemical mass f t using environmental system mode which describes transient surface water flow change with respect to time and distance along a stream channel nominally the gem is designed to simulate chemical f t given user specified flows in space and time this analysis models the transient flows themselves and also demonstrates the gem s nonlinear functionality an analytical solution presented by chow et al 1988 using a finite difference scheme on the underlying pdes is compared to gem results this verification exercise is also presented on the gem website https github com keithwlittle gem comparisons between gem simulations and analytical solutions for all above verifications are excellent two validation exercises have also been performed both comparing gem results with empirical results obtained from bench scale laboratory experiments measuring leaching characteristics in porous media for sorbing chemical elements both experiments are from a suite of methods recently approved by the us epa known as the leaching environmental assessment framework leaf developed by kosson et al 2002 these methods epa methods 1313 1316 3 3 https cfpub epa gov si si public record report cfm direntryid 231332 are designed to enhance representativeness of liquid solid partitioning characteristics in porous media relative to historical methods such as the well known toxicity characteristic leaching procedure tclp epa method 1311 which is known to be biased toward estimating maximum potential for leaching in landfill environments the leaf methods are more realistic for many in situ environmental settings the first gem leaf method validation was to epa method 1314 and is described in detail in a recent paper little et al 2017 koralegedara et al 2016 evaluated the leaching characteristics of trace elements in flue gas desulfurization gypsum and a soil fgdg mixture as a function of liquid solid mass ratio ls using epa method 1314 method 1314 places a sample of the material in a plexiglas column and pumps deionized water through the column at a relatively constant rate samples of leachate are collected at intervals and the dissolved chemical concentration measured the method results in a time series of measured dissolved concentrations in leachate along with the corresponding ls ratios for this verification analysis these data were then used to estimate regression functions of kd values as a function of ls ratios the kd ls relationships were then included in the gem and a simulation performed for a model of the bench column comparisons were excellent a second leaf validation for method 1313 data was performed and is presented on the gem website https github com keithwlittle gem it illustrates a gem application using multiple state variables a dynamic sorption assumption and a minimal number of gem compartments method 1313 is a batch experiment that measures chemical release from solid materials as a function of ph it is performed by tumbling the material at a ls ratio of 10 ml extractant g dry sample for 1 2 days and subsequently evaluating the dissolved chemical mass in the extractant it is then repeated at different ph values the method 1313 data used for this analysis were obtained from experiments performed at epa s national risk management research laboratory in cincinnati oh al abed personal communication february 2016 the materials used for this method 1313 experiment were crushed mine tailings the measured chemical element used for this analysis was aluminum al and the experimental ph was 2 13 once the dynamic gem simulation attained equilibrium sorption the simulated and experimentally obtained equilibrium concentrations were essentially identical the details of these verification and validation exercises are not presented here for brevity but the interested reader is referred to the gem website 4 multi media example 4 1 introduction our selected example to illustrate gem flexibility in this paper is a multi media chemical f t application there are essentially two approaches to simulating chemical f t across environmental media in a numerical modeling framework as previously mentioned given that most extant environmental models are medium specific the obvious strategy is to assemble a linked suite of medium specific models that taken together cover the media of interest linking these separate models is then performed in a one direction manner with outputs from one model becoming inputs to another the most ambitious instance of this strategy is likely the frames 3mra system under development by the u s epa babendreier and castleton 2005 this complex system is a tool to quantify human health and ecological risks associated with the disposal of hazardous wastes in various disposal beneficial use scenarios within a multi media context the system comprises 6 source models aerated tank surface impoundment waste pile land application unit landfill and roadway with the exception of the aerated tank and surface impoundment these source models are essentially vadose zone type f t models that generate time series chemical mass loadings to 5 medium specific models vadose aquifer surface water regional watershed and air as shown in fig 1 the frames 3mra system has been under development and refinement for more than a decade and the gem has recently been used to enhance the land application unit landfill and waste pile models and for a new source model the roadway model the source and media models operate sequentially the source model is first run the resulting time series of chemical mass fluxes are then saved and used as input to the receiving media models these models are then run sequentially with inter media fluxes being passed in one direction only the second approach is to build a single holistic multi media model from first principles including all relevant media inter and intra media f t processes of interest and appropriate numerical discretization to approximate the desired spatial and temporal resolutions this approach has historically been daunting especially for high resolution needs but it has been successfully used in lower resolution applications with fugacity modeling techniques e g mackay 1991 fugacity is a chemical specific thermodynamic property that represents the partial pressure of a chemical in a medium at chemical equilibrium the fugacities of the various media will be equal the fugacity concept is not a shortcut to multi media modeling it requires the same information as a comparable concentration based approach trapp and matthies 1998 but conceptualizing media as having different capacities to store chemicals is insightful and may well facilitate a holistic multi media approach certainly the inclusion of all media in a single program allows feedforward feedback relationships among media numerically fugacity models use a similar compartment construct as the gem to discretization of the spatial domain in the temporal domain level iv dynamic fugacity models use finite differences given the compartment construct it would seem that spatial resolution could be arbitrarily determined by the modeler but it is our impression that fugacity modeling applications have typically been of relatively low resolution for broad screening purposes this would seem to be a limitation of the available software and not the underlying numerical approach the gem multi media example presented below takes advantage of the fugacity approach of having all media represented in a single holistic construct allowing feedforward feedback among media unlike fugacity however the svs are in the more common concentration metrics it also has flexible resolution depending on how many compartments are user specified per medium and the size of the temporal time step thus it has advantages of both approaches to multi media modeling the example is an idealized hypothetical scenario intended for illustrative purposes only but is motivated by the frames 3mra land application unit scenario 4 2 conceptual multimedia model in the land application scenario chemical loads arise as a result of the beneficial reuse of waste material into surficial soils of a farm field as a soil amendment in fig 2 compartment 2 represents the tilled zone of a farm field chemicals of concern in the waste material are introduced via tilling into compartment 2 compartments 3 through 11 are soil vadose layers below the tilled zone that then receive chemical contamination via vertical percolation of stormwater and diffusion dispersion compartment 1 is a dummy compartment which is neither a modeled compartment a compartment for which mass balance equations are solved nor a boundary compartment it serves only as a device to introduce advective flows rainfall percolation into the system compartments 14 through 23 reflect adjoining soil vadose layers lying downslope of the farm field that can become contaminated via stormwater runoff and advective transport of chemical from the farm field followed by vertical percolation diffusion dispersion compartment 13 is the dummy compartment atop the buffer also to introduce rainfall percolation for simplicity the only communication between the soil columns 2 11 and 14 23 is assumed to be from 2 to 14 as a result of stormwater runoff thus the field buffer portion of the overall compartmental structure is quasi 2 d vertical transport is due to advection and diffusion dispersion in the longitudinal x direction and lateral y direction the field and buffer are 100 m 100 m vertically z direction each field and buffer compartment is 0 1 m below the field buffer compartments is a 1 d aquifer receiving chemical mass leachate fluxes from the overlying field and buffer the leachate enters aquifer compartments 12 and 24 from percolation and vertical diffusion dispersion the aquifer consists of 10 compartments numbered 12 and 24 35 each 100 m longitudinal x 100 m lateral x 10 m vertical advective flow in the aquifer is from left to right in the figure and compartments 36 and 37 are upgradient and downgradient boundary compartments respectively transport along the aquifer is due to advection and diffusion dispersion above a portion of the aquifer lies a 1 d surface water medium comprised of 10 compartments numbered 25 and 38 47 each is 100 m 100 m x 1 m vertical flow is from left to right and compartments 38 and 48 are upstream and downstream boundary compartments respectively surface water compartment 25 receives chemical mass loadings from buffer compartment 14 and this connection represents chemical mass loadings transported into the surface water from stormwater runoff from the buffer longitudinal transport is due to advection and diffusion dispersion interactions between the aquifer and surface water compartments are included as vertical dispersion across the compartment interfaces overlying both the surface water and soil compartments are 8 1 d atmospheric compartments each is 200 m longitudinal x 100 m lateral x 100 m vertical compartments 55 and 56 are boundary compartments longitudinal transport is due to diffusion dispersion only no advective transport is included each modeled non boundary air compartment interacts with the underlying surface water and soil compartments interactions between the atmosphere and soil or water media can be complex and can occur as volatilization of neutral dissolved ionic species from water or soil to air or wind erosion of sorbed species from soil to air and in the opposite direction wet or dry deposition of dissolved and sorbed species from air to soil or water here we consider only volatilization of dissolved chemical from soil and surface water to air and wet deposition from air to soil and water both as 1st order transfer processes dry deposition is not included because it would have an insignificant impact given the relatively low level of atmospheric suspended solids 10 g m3 used in the lateral y dimension the sets of compartments for soil surface water aquifer and atmosphere are assumed to not interact with adjoining media for simplicity thus the aquifer surface water and atmosphere media are modeled as 1 d x direction the soil compartments are quasi 2 d x and z if lateral interaction with adjacent media were desired extending the compartment structures in this dimension to 2 d and including appropriate boundary compartments would be used with the uniform lateral width of 100 m for all compartments one could consider the conceptual model to be a representative 100 m lateral slice of an arbitrarily wide 1 d multi media system regarding the compartment numbering scheme the gem requires only that compartments be integer numbered in sequential order beginning with 1 which number applies to which compartment is irrelevant the compartment numbering scheme used in this example was selected to facilitate piece wise building up of the entire multi media structure for purposes of debugging for example the first mini model was developed for the soil medium system only i e compartments 1 25 for this mini model the aquifer compartments 12 and 24 were specified as lower boundary compartments and the surface water compartment 25 was a dummy compartment simply to receive the runoff flows as more media and more compartments are added the purpose and function of these compartments was changed we strongly recommend this step wise approach to building complex models to users of the gem or any other software the gem flexibility with regard to compartment numbering and ease of adding additional compartments as needed was key to use of this piece wise building up of the complete model domain it is further noted that in actual practice appropriate compartmental design will place more and smaller compartments in the vicinity of anticipated high concentration gradients e g near point sources to provide sufficient spatial resolution to capture peak concentrations thomann and mueller 1987 for simplicity we adhered to this practice only in the sense of relatively much smaller depths in the soil compartments relative to the other media 4 3 model parameterization a detailed description of the assumptions and parameterization of the model is presented below for the purpose of establishing the representativeness of f t processes used in each medium of our hypothetical example the reader not interested in these details may skip this section advective flows in each medium are assumed for simplicity to be at steady state and were parameterized to reflect typical conditions vertical percolation in the soil compartments is based on a national average u s precipitation rate of 715 mm year 4 4 http data worldbank org indicator ag lnd prcp mm and assumes that 25 is lost as evapotranspiration of the remainder 50 268 mm yr occurs as stormwater runoff from the surficial soil compartments the remainder after runoff is vertical percolation through the soil compartments to the underlying aquifer at an assumed soil porosity of 0 05 fully saturated this percolation has a constant pore water velocity of 0 02 m day the advective flow rate in the aquifer with a water content of 0 1 was chosen to result in a longitudinal pore water velocity 10 times the velocity of the soil pore water or 0 2 m day advective flow rate in the surface water was chosen to result in a velocity of 0 1 m second 8 6 103 m day for the air no net advective flow is assumed and all transport is due to longitudinal dispersion diffusive dispersive transport is simulated both within and between media for the soil compartments we assumed that the relatively slow percolation rate does not induce dispersion and that vertical mixing is due to molecular diffusion only a value of 1 10 4 m2 day was used both for vertical dispersion between soil compartments as well as to the underlying two aquifer compartments based on a representative molecular weight of common chemicals of concern for the aquifer with a somewhat greater advective flow velocity we used a longitudinal value two orders of magnitude higher 1 10 2 m2 day to reflect some macro dispersion for the surface water with a velocity of 0 1 m s we used a longitudinal value of 1 104 m2 day based on a measured longitudinal dispersion coefficient in the monocacy river maryland with similar velocity schnoor 1996 table 2 2 for the atmosphere dispersion at 1 7 106 m2 day was used for purposes of comparing this value to other transport rates the dispersion term in the mass balance equations can be considered as an advective flow rate by dividing the dispersion coefficient by the transfer distance to yield a mass transfer velocity considering the distance between adjacent air compartment centroids to be the transfer distance the dispersive transfer flow velocity is then 1 7 106 m2 day 200 m 8 6 103 m day identical to the surface water longitudinal flow velocity dispersion between media is assumed only for the aquifer and surface water and a value of 10 m2 day representing a mid range of the aquifer and surface water longitudinal values was used for dissolved chemical only for fate processes we considered two idealized chemicals the fate processes and their parameter values are intended to be realistic but not necessarily representing any particular chemicals in addition to solid liquid partitioning fate processes include volatilization from surficial soils and surface water to air and settling of airborne vapor from the air to soil and surface water for surface water and soil volatilization we used a transfer velocity vv of 0 1 m day similar to estimates of volatilization transfer velocities for pcb in lake huron chapra 1997 for the air to water and air to soil vapor phase settling mechanism we used a settling velocity vvd of 0 2 m day twice the volatilization transfer velocity we chose to explicitly simulate dissolved and sorbed chemical concentration in all media as separate svs the gem allows specification of which sv is relevant to which compartments so alternative designs are possible e g dissolved only in the porous media compartments soil aquifer and both dissolved and sorbed in the surface water and air or dissolved in the porous media and total in surface and air nonetheless the 2 sv formulation seemed most flexible given the 2 sv approach we also treat sorption desorption as a dynamic process that is slightly at non equilibrium for ease of agreement with the gem default dissolved concentration metric we let the sorbed chemical concentration in all media be expressed as chemical mass per water air volumetric volume g m3 w not total compartmental volume g m3 t conversion to another metric i e the standard gc gs would be accomplished by multiplying the water air volume concentration by θ bd at the relatively dilute solids concentrations we assumed see bd in table 1 this is a moot point because the water or air content θ is 1 0 for all practical purposes however for other systems e g a high solids concentration slurry this distinction would be meaningful dissolved chemical cd is sv 1 while sorbed chemical cs is sv 2 regarding the parameterization of solids partitioning parameters we simulated two chemicals with different partitioning behaviors one chemical is a weakly sorbing chemical that is transported through the modeled system relatively quickly while the second is a strong sorber with a longer persistence in the environment for simplicity we assumed that the kd values do not vary in time e g with cumulative liquid solid ls ratio or ph kd values do vary by solids characteristics in different media and we reviewed kd values for various metals by environmental medium estimated by the us epa us epa 2003 that source presents typical kd values for soil sediment and surface water for 17 hazardous metals although there is considerable variability in relative differences across these media among the metals there is a general tendency for an increasing kd from soil to sediment to surface water in many cases this increase was approximately an order of magnitude accordingly we assigned for both simulated chemicals a difference in kd among media of one order of magnitude although aquifer solids media were not a focus of the epa study we here treat the aquifer as sediment for purposes of assigning kd thus for a given kd for soil the aquifer sediment is 10 times the soil value while the surface water is 100 times the soil value for air we assume the same kd value as soil given that air particulates are comprised to some extent of soil material for the values of the equilibrium kd values we selected a soil kd that results in a 90 fraction dissolved in soil for the weakly sorbing chemical for the strong sorber we selected a soil kd that results in a 10 dissolved fraction in soil the resulting kd values by medium are shown in table 1 along with the calculated kds and ks desorption sorption rate constants needed for the dynamic sorption kinetics used for the 2 sv formulation the fd values were calculated from the relationship f d θ c d θ c d ρ b c s see discussion of equivalence of surface water and ground water equations on gem website https github com keithwlittle gem the ks and kds values were then calculated from the equilibrium kd in accordance with the relationship k s k d s 1 f d 1 see e g schnoor 1996 little 2012 4 4 mass balance equations the mass balance equations assembled and solved by the gem for this example are summarized below note due to the gem internal equation building algorithm the units on each term are not strictly mass time m t but rather m l3 t t l3 w where l3 t is total volume and l3 w is pore space volume 4 4 1 soil compartments i 13a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k v v i c d i a i k v v d k c d k 13b v i r i d c s i d t v i c s i k d s i v i c d i k s i a i k v s d k c s k where k is the index for the overlying air compartment 49 and ri is given by equation 2 for our dynamic sorption approach kd is set to zero making r 1 and the sorption desorption mechanism enters via the ks and kds sorption desorption processes for the surficial soil compartments 2 and 14 the last two terms on the right hand side of the cd equation are respectively the sink term due to volatilization and the source term due to vapor deposition where vs and vv are the vapor settling and volatilization transfer velocities m day air deposition and volatilization terms are not included for non surficial soil compartments 4 4 2 aquifer compartments i 14a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i 14b v i r i d c s i d t v i c s i k d s i v i c d i k s i 4 4 3 surface water compartments i 15a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k v v i cd i a ik vvd k cd k 15b v i r i d c s i d t j 1 n a i q i j θ i c s i j j 1 n a i e i j c s j c s i v i c s i k d s i v i c d i k s i a i k v s d k c s k where k is the index for the overlying air compartment 4 4 4 air compartments i 16a v i r i d c d i d t j 1 n a i q i j θ i c d i j j 1 n a i e i j c d j c d i v i c s i k d s i v i c d i k s i a i k vv k cd k a i k a i k vvd i cd i 16b v i r i d c s i d t j 1 n a i q i j θ i c s i j j 1 n a i e i j c s j c s i v i c s i k d s i v i c d i k s i a ik vsd i cs i 4 5 results before simulating the full multi media system as developed above we performed a side analysis to demonstrate the equivalency of a 1 sv dissolved versus 2 sv dissolved sorbed approach for the soil medium only the motivation for this analysis was to emphasize the flexibility of alternative modeling designs as well as the flexibility of the gem in addition it serves as an illustration of an instantaneous sorption equilibrium approach kd method versus a dynamic sorption approach ks and kds approach the interested reader can find that side analysis in the appendix returning to the 2 sv approach we ran the gem to simulate the f t throughout the multi media system of a spill of dissolved chemical to the field surficial soil compartment 2 the spill was represented as an initial condition for sv 1 in that compartment of 1000 g m3 runs were made for both the strongly sorbing chemical 1 and weakly sorbing chemical 2 chemical parameters as described above all feedforward and feedback inter media f t processes as described above were included to avoid being restricted to relatively small compartment volumes to satisfy the peclet criterion and to avoid relatively small time steps to satisfy the courant condition we used the btbs gem option this option is both unconditionally stable and avoids negativity wiggle but does introduce temporal and spatial numerical dispersion however for purposes of our illustrative multi media example that disadvantage was not considered critical relative to having appropriately few compartments for simplicity of presentation as well as being able to make large time steps to enable tracking the spill s progress through all media the simulations covered 10 000 days at a 10 day time step boundary conditions for the upstream boundary compartments 36 aquifer and 38 surface water were specified as 0 g m3 as well as both boundary compartments for the air medium 5 5 we originally used a zero concentration gradient boundary condition for the air boundaries however for the no feedback scenario discussed subsequently where no air deposition is included the only mechanism left available to remove chemical mass from the air medium is dispersion no advective transport was included once the air compartments attained a zero concentration gradient which occurs well above zero concentration there was no gradient under which dispersion would operate thus the chemical mass present at that time remained indefinitely this is exactly what should happen under those conditions however it does not suit our illustrative purposes here and we reverted to a zero fixed boundary condition 55 and 56 for the downstream boundary compartments 37 and 48 a zero concentration gradient boundary condition was used table 2 presents the gem s internal mass balance checks over the simulation the gem keeps a running total of chemical mass initially present entering the system during the simulation and mass leaving the system either from transport or endogenous decay volume based sinks out or volatilization area based sinks out and residual system wide mass our volume and area based sinks do not leave the system but rather are among internal media mass only leaves the system through transport processes these results are reported by sv for summary user information a total mass balance error across all svs is then calculated and reported at the bottom of the table as a percentage of cumulative initial mass present and any mass input during the simulation as can be seen the mass balance errors for each chemical are insignificant it can also be seen that essentially no chemical mass remains in the system for the weakly sorbing chemical 2 while approximately 10 of the initial mass remains for the more strongly sorbing chemical 1 we consider now the propagation of the spill mass through the various media for sv 1 dissolved the more relevant chemical specie for purposes of toxics risk analysis fig 3 a presents the time series of concentrations in selected compartments over the 10 000 day simulation for strongly sorbing chemical 1 the compartments were chosen to show the spill compartment field soil top 2 in the figure as well as compartments at the downstream edge of each medium these results show the relative differences in concentrations among the media over time at the edges of each medium recalling that the chemical mass is initially concentrated in compartment 2 at 1000 g m3 we see the depuration of the soil spill in that compartment field soil top 2 with subsequent increases in concentrations at the bottoms of the soil media compartments field soil bottom 11 and buffer soil bottom 23 as chemical is transported vertically in the soil because of the 10 day time step the depuration increase prior to day 10 is not shown these profiles clearly show increasing followed by decreasing concentrations as the peak passes out the bottom of the soil medium for the downstream surface water compartment sw downstream 47 downstream aquifer compartment aquifer downstream 35 and air far right compartment air right 55 these compartments are relatively quickly contaminated due to surface runoff from the field and buffer surface water to air volatilization surface water to aquifer dispersion and subsequent downstream transport and thus show a similar depuration pattern as the field soil top 2 itself albeit with much lower concentrations as the peak concentrations begin to pass the bottom of the field and buffer soil layers the transfer of that mass into the surface water aquifer and air media is reflected in less rapid depuration or even slight increases in concentration in those media aquifer and surface water concentrations at the downstream compartments have essentially equilibrated after approximately 300 days due to their interaction peak concentrations have occurred at the edges of each medium during the 10 000 days due to among media mixing of chemical mass the peak concentrations vary by several orders of magnitude among the media fig 3b presents similar results for the weakly sorbing chemical 2 shown on the same vertical scale as fig 3a to facilitate between chemical comparison compared to fig 3a we see generally the more rapid depuration of chemical mass in all media because chemical 2 is more mobile it is also of interest to observe within medium changes over time fig 4 a and b shows these results for strongly sorbing chemical 1 and weakly sorbing chemical 2 respectively for several of the soil buffer compartments while figs 5 7 are similar chemical comparisons for the aquifer surface water and air media respectively vertical scales are fixed for each medium to facilitate comparisons the progression of the spill wave through the media is obvious and the more rapid mobility of chemical 2 can also be readily seen recall that multi media chemical f t modeling can be performed either by linking together several medium specific models in a feedforward mode or by building a holistic multi media model as we have done here the primary advantage of the holistic approach is that feedback among the media can be incorporated whereas for the linked single media approach this is problematic 6 6 but not impossible on the gem webpage we include a description of how a newton s method iterative algorithm may be used to iterate among single medium models to effect full feedback among the various media it would nonetheless be a computationally demanding approach the obvious question is then how much difference do these two approaches feedforward only or feedforward feedback make in concentrations among the media the results shown above include full feedback by removing the inter media feedback processes from the example model the feedforward only approach is easily mimicked accordingly we removed the soil aquifer and aquifer surface water dispersion transport mechanisms and the air compartments vapor deposition process under these conditions the soil compartments are contaminated only by the spill itself no feedback from air the aquifer is contaminated only by advective leachate from the lowest soil compartments no dispersion from surface water or the overlying soil compartments the surface water is contaminated only by stormwater runoff from the soil buffer no aquifer dispersion and the air is contaminated only by volatilization from the surficial soil and the surface water we ran this modified model again for 10 000 days at a 10 day time step for both chemicals 1 and 2 and compared the resulting time series concentrations to the above full feedback concentrations these comparisons were performed for each medium for compartments approximately midway in each medium as would be expected from the full feedback results above where the air concentrations are several orders of magnitude less than concentrations in any other medium the issue of whether vapor deposition is affecting soil or surface water concentrations or not would not significantly influence soil or surface water concentrations that indeed was the result however some relatively significant differences between the feedback no feedback scenarios were observed for the surface water and aquifer media fig 8 a and b shows feedback versus no feedback results for mid depth field soil compartment 6 for chemicals 1 and 2 respectively similarly figs 9 11 show feedback versus no feedback for mid length aquifer surface water and air for both chemicals results are shown on the same vertical scale by medium to facilitate comparisons between chemicals for mid depth field soils there is no appreciable difference for feedback versus no feedback for chemical 1 which is not surprising given that feedback involves inclusion of vapor deposition and the air concentrations are much much lower than soil concentrations therefore feedback is minimal for chemical 2 there are approximately 1 2 order of magnitude differences after around 300 days although these are very low concentrations however what is important in these comparisons is not the absolute value of the concentrations because those will change linearly with any change to the spill s initial condition assumption but rather the differences in orders of magnitude between the two scenarios for the mid length aquifer the no feedback results for chemical 1 are many orders of magnitude less than the feedback scenario for most of the simulation for example at day 10 the no feedback concentration is 18 orders of magnitude less than the full feedback version presumably because leachate has not yet reached the mid length aquifer and no dispersive transport is occurring from the surface water for more mobile chemical 2 there is relatively less difference in initial concentrations although the differences still span some 6 orders of magnitude the time to reach similar concentrations is also much shorter than for chemical 1 due to the enhanced mobility for the mid length surface water compartment for chemical 1 the results between the two scenarios are approximately the same until day 200 after which the no feedback concentrations become much less than the feedback scenario this is because with no feedback the chemical gets washed out of the surface water relatively quickly while with feedback the underlying immobile aquifer solids store chemical being dispersed from surface water to aquifer and leach it out to the overlying surface water much more slowly over time for chemical 2 this same pattern occurs although the concentration differences are much greater and the departure in similar concentrations occurs much earlier for mid length air compartments a similar pattern to the surface water is seen this reflects the fact that the air medium is being contaminated in large part by surface water in summary for this example we demonstrated the gem s flexibility to accommodate f t processes within and among different environmental media including both feedforward and feedback processes for maximum flexibility a 2 sv approach was used although other designs are possible the spatial compartment resolution varied from relatively fine in the soil medium to capture vertical concentration gradients to relatively coarse in the air medium where rapid dispersive transport and no retardation lessen sharp gradients examples of biases among media concentrations that might result when multi media modeling approaches do not incorporate fully holistic feedforward and feedback processes were also illustrated this example was not designed to maximize these biases but only to illustrate and quantify them for a particular chemical loading scenario and set of realistic physical chemical parameters other scenarios e g introduction of spills to the atmosphere instead of surficial soils would give rise to different relative results 5 summary and conclusions a new open source chemical f t modeling software tool has been developed and is summarized and illustrated in this paper the gem modeling framework accommodates arbitrary dimensionality 0 1 2 or 3 d due to its intuitive compartmental basis it provides dynamic solutions from three user selected options ft bt or ct and also has a steady state option spatial discretizations are available using either bs or cs methods the temporal and spatial discretization options involve relative tradeoffs between accuracy of simulations allowable time steps and compartment sizes and presence of numerical errors processes included are advective and dispersive transport instantaneous equilibrium or dynamic sorption arbitrary volume based 1st order source sinks such as decay or feedforward feedback interaction among chemicals and arbitrary area based mass transfers across intercompartmental interfaces e g settling or volatilization multiple interacting svs are allowed with the ability to user specify which svs are relevant to which compartments the theoretical basis of the framework consisting of well known mass balance partial differential equations and their numerical representation using the compartment discretization method was presented along with a hypothetical but representative multi media example to demonstrate capabilities this example also illustrates differences that might be expected between holistic models that incorporate all inter media f t processes and models that simulate processes only in one direction among media the software code as well as sets of input files for most of the examples mentioned in this paper are available on the gem website https github com keithwlittle gem we feel that this software represents a valuable contribution to the environmental modeling community and can facilitate relatively rapid development of complex environmental models using non proprietary tools our goal in making this software available is to encourage and simplify rigorous quantitative analysis of complex environmental contamination issues and the authors are available to address questions or comments that users may have via the website acknowledgments the experimental leaf methods 1313 and 1314 validation exercises were supported under a contract with rti international with the u s epa national risk management research laboratory nrmrl cincinnati ohio we also thank the three anonymous jems peer reviewers for their insightful suggestions appendix equivalency of 1 sv versus 2 sv models for soil medium of multi media example in this side analysis only the soil compartments are considered i e compartments 1 25 compartments 1 and 13 are upper boundary compartments compartments 12 and 24 serve as lower boundary compartments and compartment 25 is a dummy compartment to receive runoff flows from compartment 14 volatilization of dissolved chemical to the atmosphere is included but feedback deposition from the atmosphere wet and dry deposition are not included because the air compartments are not being simulated for the 2 sv model the two equations are given by equations 13a and 13b omitting the vvd vapor deposition source term in these equations r is understood to be 1 0 kd 0 for the 1 sv model dissolved concentration the mass balance equation is given by equation 13a omitting the vvd kds and ks terms r is determined as r 1 ρ b k d θ and kd is not equal to zero we ran both models for the strongly sorbing chemical 1 see table 1 for model 1 s kd and model 2 s ks kds ratio for the 1 sv model the initial dissolved concentration in compartment 2 was 100 g m3 representing a spill of some aqueous waste for the 2 sv model the same initial condition for cd was used for cs an initial condition of 900 g m3 was used reflecting the sorbed fraction at equilibrium corresponding to the initial cd concentration see table 1 for the 2 sv model the ks and kds values were assigned in accordance with the ks kds ratio shown in table 1 and with an assumed value for kds of 0 1 day ks 0 9 day these 1st order rate constants are reasonably fast e g an 0 1 day rate constant can be interpreted as a process affecting approximately 10 of the total chemical mass per day chapra 1997 but would still not represent instantaneous equilibrium as implicit in the 1 sv model we ran the models for 1000 days and the time series results from the two models for cd in the top and bottom compartments are shown in fig a 1 for the farm field column and the buffer column we see several phenomena in the figures first the comparison between top and bottom compartments in both figures shows the downward movement of the chemical due to transport over the 1000 days peak concentration in the field s top compartment is at time 0 the spill while the peak concentration at the bottom compartment occurs at approximately 400 days peak concentration in the buffer top compartment is at approximately 30 days as the runoff from compartment 2 transports chemical into compartment 14 the concentrations in the buffer compartments are also reduced significantly from those in the field compartments due to spatial dilution of the mass and loss due to volatilization regarding the instantaneous 1 sv model versus dynamic sorption 2 sv model assumptions results of the two models are quite similar but with some differences due to slight non instantaneous equilibrium conditions in the 2 sv model at our specified ks and kds absolute values a final run of the 2 sv model was made increasing both these values an order of magnitude and the results are essentially identical to those of the instantaneous equilibrium model thus showing the equivalence potential of these two approaches the advantage of the instantaneous equilibrium approach is parsimony in the number of svs modeled while the advantage of the dynamic sorption approach is maximum flexibility 
26409,quality and efficiency of computational fluid dynamics cfd simulation of pedestrian level wind environment in a complex urban area are often compromised by many influencing factors particularly mesh quality this paper first proposes a systematic and efficient mesh generation method and then performs detailed sensitivity analysis of some important computational parameters the geometrically complex hong kong polytechnic university hkpolyu campus is taken as a case study based on the high quality mesh system the influences of three important computational parameters namely turbulence model near wall mesh density and computational domain size on the cfd predicted results of pedestrian level wind environment are quantitatively evaluated validation of cfd models is conducted against wind tunnel experimental data where a good agreement is achieved it is found that the proposed mesh generation method can effectively provide a high quality and high resolution structural grid for cfd simulation of wind environment in a complex urban area keywords computational fluid dynamics cfd simulation pedestrian level wind environment mesh generation method complex urban environment sensitivity analysis 1 introduction the pedestrian level wind environment has become a pressing issue since achieving an acceptable wind environment is unsuccessful in many urban cities which results in many environmental problems including thermal comfort city ventilation and pollutant dispersion accurate modelling of wind flow in the urban environment is therefore crucial to wind comfort assessment wind safety assessment as well as thermal comfort evaluation all of which can affect the sustainable development of a built environment ai and mak 2015 blocken et al 2012 hang and li 2010 juan et al 2017 mochida and lun 2008 ng 2009 richards et al 2002 shi et al 2015 stathopoulos 2006 yuan et al 2016 in the past decades the pedestrian level wind environment in the urban environment has been extensively investigated by field measurement niu et al 2015 wind tunnel test cermak 2003 du et al 2017a tsang et al 2012 tse et al 2017 and computational fluid dynamics cfd modelling blocken et al 2016 du et al 2017b hong and lin 2015 liu et al 2016 tominaga and stathopoulos 2011 yoshie et al 2007 compared to the first two methods cfd simulation has lots of advantages in studying pedestrian level wind environment such as providing whole flow field data easily performing parametric study and less expensive however cfd simulation of urban environment has been hindered by several problems including the difficulties of generating high quality mesh for geometrically complex real urban community and the proper choice of important computational parameters therefore works that can improve the reliability and accuracy of cfd predicted results on pedestrian level wind environment still remains of great significance it is well known that the construction of high resolution and high quality mesh system for the complex urban environment is the prerequisite for successful simulation standard automatic or semi automatic generation of unstructured grid in a complex urban environment lacks adequate control of local mesh resolution and quality which often results in poor quality mesh system and hinders the accurate simulation of wind environment in urban areas besides generation of unstructured grid in a large computational terrain will lead to a striking number of cells especially when the grid is required to have high density at specific location e g at least 4 or 5 grid is required at pedestrian level for accurate prediction of pedestrian level wind environment franke et al 2007 tominaga et al 2008b the body fitted grid generation technique presented by van hooff and blocken 2010 allows full control over the mesh quality and resolution when modelling a complex stadium but it is limited to the use of unstructured prismatic cells in the computational domain on the contrary a high quality structural grid can effectively reduce the cell number save numerical cost avoid numerical diffusion and become more stable when modelling the complex urban environment than unstructured grids franke et al 2007 therefore a mesh generation method that can effectively produce high resolution and high quality structural grid with full control over the whole computational domain is in urgent need the time averaging reynolds averaged navier stokes rans turbulence models are commonly used to simulate pedestrian level wind environment in urban environment among the existing turbulence models for its acceptable performance and economic computational cost ai and mak 2013 2017 ai et al 2013 baker 2007 bechmann et al 2011 blocken 2015 cui et al 2016 du et al 2017b shi et al 2015 yoshie et al 2007 the realizable k ε turbulence model was used to predict the pedestrian level wind environment at the campus of eindhoven university of technology and the predicted results agreed generally well with the long term and short term on site measurements blocken et al 2012 janssen et al 2013 du et al 2017b utilized the renormalization group rng k ε model to evaluate the effects of lift up design between the ground and the main building structures on the pedestrian level wind comfort in different building configurations which aims to provide solid scientific evidence for improving weak wind conditions in hong kong even though the above studies have obtained overall good predicted results of pedestrian level wind environment by utilizing the rans turbulence model in cfd simulation these investigations did not conduct sensitivity tests of the computational parameters which may have major influence on the predicted results owing to the widely application of cfd simulation in urban aerodynamics a set of best practice guidelines bpgs are established to ensure the reliability and accuracy of the cfd predicted results franke et al 2007 jakeman et al 2006 laniak et al 2013 tamura et al 2008 tominaga et al 2008b in addition to this a lot of studies have been conducted to improve the accuracy in cfd simulation including achieving a homogeneous boundary layer abl ai and mak 2013 blocken et al 2007 gorlé et al 2009 yang et al 2009 while bpgs have detailed guidance on the choices of important computational parameters several choices are available for some important computational parameters when modelling a complex urban environment it has been indicated that the performance of different steady rans turbulence models are different when modelling wind flows around isolated buildings lateb et al 2013 tominaga et al 2008a tominaga and stathopoulos 2009 besides the study of ramponi and blocken 2012 reported that the different sizes of computational domain have great influence on the cross ventilation for a generic isolated building furthermore the investigation about the effect of near wall mesh density presented in our previous study of single sided ventilation ai and mak 2014 showed that the near wall mesh density can affect the predicted results of the ventilation rate however the sensitivity analyses of the computational parameters for cfd simulations are mainly focused on isolated buildings lateb et al 2013 lun et al 2007 ramponi and blocken 2012 tominaga and stathopoulos 2009 the effects of the computational parameters on the pedestrian level wind environment based on a complex urban environment are rarely reported in order to provide a reliable and accurate prediction of pedestrian level wind environment in a complex urban environment a comprehensive sensitivity analysis of the important computational parameters is required this paper presents an investigation of improving the predicted accuracy of the pedestrian level wind environment in a complex urban environment an effective and systematic mesh generation method is proposed in this study for generating a high quality structural grid which also allows full control over the grid resolution and ensures the near wall mesh density the proposed mesh generation method can be efficiently applied in meshing a complex urban area as well as simple building blocks another prominent feature of the proposed mesh generation method is that it can ensure a sufficient near wall mesh density without a significant increase of the total number of cells the computational model of the hkpolyu campus is used as a case study to illustrate the mesh generation method in which the configurations of the buildings are very complex and it has lift up design between the ground and the main building structures du et al 2017a niu et al 2015 the steady rans turbulence models are used to simulate the pedestrian level wind environment and the predicted results are validated against quality wind tunnel experimental data moreover the sensitivity analysis of three important computational parameters are conducted based on the campus model namely turbulence model near wall mesh density and computational domain size the proposed mesh generation method is described in section 2 with an illustration of the hkpolyu campus model section 3 presents the cfd simulation performance evaluation and the guidelines of the previous studies bennett et al 2013 blocken and gualtieri 2012 jakeman et al 2006 are followed to ensure the confident numerical modelling firstly the wind tunnel tests reported in section 3 1 satisfy the requirements of asce and awes asce 1999 awes 2001 for conducting quality wind tunnel tests secondly in section 3 2 and section 3 3 the best practice guidelines bpgs blocken 2015 franke et al 2007 tominaga et al 2008b for modelling urban aerodynamics are rigorously followed throughout the simulation thirdly the direct comparison method bennett et al 2013 is used to assess the model performance in section 3 4 meanwhile the quantitative evaluation method bennett et al 2013 is used in section 4 to analyse the sensitivity test results of the important computational parameters finally concluding marks are given in section 5 2 computational mesh 2 1 mesh generation method the resolution and quality of mesh system is considered crucial to accurately reproduce the wind flow in a complex urban environment especially when modelling pedestrian level wind environment however generating a computational domain with high quality and high resolution structural grid is definitely not straightforward therefore to be able to allow full control of local cells and generate a high quality structural grid in the whole computational domain an efficient and systematic mesh generation method is proposed here it is a specific procedure to effectively generate high quality structural grid in a complex urban environment and also ensures high density of near wall mesh the mesh generation method is schematically depicted in fig 1 it should be mentioned that the mesh generation method presented in this study is different from the body fitted bf mesh generation technique used by van hooff and blocken 2010 in the following two ways i the bf technique generates both prismatic cells unstructured cells and hexahedral cells structural cells in the computational domain and the prismatic cells are used in the immediate vicinity of complex buildings however the mesh generation method presented by this paper can produce hexahedral cells all over the computational domain ii the use of structural cells and near wall mesh technique in this proposed mesh generation method can potentially reduce numerical cost as shown in fig 1 a the mesh generation method contains three main parts mesh generation preparation mesh generation technique and near wall mesh generation technique fig 1 b fig 1 c and fig 1 d presents the schematic illustration of the step 2 step 3 to step 5 and step 6 in fig 1 a respectively the detailed explanation for the procedure are outlined as follows 2 2 mesh generation preparation the computational geometry should include all the buildings and obstacles that may affect the wind flow in the computational model if necessary the ground roughness boundary conditions should be specified appropriately 2 3 mesh generation techniques the application of this technique is schematically illustrated by using the hkpolyu campus model fig 1 b and the detailed division process of a sub domain is schematically illustrated by a generic lift up design model fig 1 c and fig 1 d du et al 2017b step 1 building a computational domain for the target urban area the size of the computational domain need to be chosen carefully according to bpgs blocken 2015 franke et al 2007 tominaga et al 2008b and it should be large enough to avoid artificial acceleration of the wind flow step 2 dividing the computational domain built in step 1 into sub domains according to the outermost building geometric configuration as shown in fig 1 b this is very important because the outermost geometric configuration ensures that all the building geometries are within one sub domain by doing this the difficulties in meshing the complex urban area become meshing independent sub domains which is very similar as meshing isolated buildings step 3 and 4 determining the required points in each sub domain and defining a vertical line that summarizes all the required points the required points on the vertical line a b e see fig 1 c should be chosen according to the geometric changes of each sub domains and the simulation requirements the simulation requirements refer to specific demand for mesh density vertically for instance this paper focuses on the pedestrian level wind environment and at least four to five cells should be applied at pedestrian level according to the bpgs thus point b is defined in the vertical line step 5 dividing the sub domains into layer domains according to all the points in the vertical line it should be mentioned that the sub domains should be divided according to all the required points in the computational domain instead of the required points in one sub domain step 6 dividing the layer domains to individual domains according to the building geometric configuration fig 1 d this step requires considerable patience and time since it relates to the accurate reproduction of building geometry step 7 and 8 applying grid to each individual domains and deleting the unwanted mesh this way only the spaces that are outside buildings are meshed and the local grid quality can be ensured 2 4 near wall mesh generation technique it is noted that the independence of mesh resolution in the whole computational domain is the prerequisite for using near wall mesh generation technique the near wall mesh is increased by only doubling the first cell in both horizontal and vertical directions and there will be no change to other non near wall cells which has been utilized in our previous study ai and mak 2014 the schematic view of the near wall mesh generation technique and the conventional mesh generation technique are shown in fig 2 a and fig 2 b respectively it has been proven to be effectively reducing numerical cost for achieving the same near wall mesh density compared to conventional mesh generation technique that has the same stretching ratio from the first cell in the whole computational domain the advantages of this mesh generation technique are i it is a systematic and efficient procedure for generating structural cells in the whole domain ii it allows full control over the mesh generation procedure and assures the resolution and quality of local cells which provides the prerequisite for accurately modelling wind environment in complex urban area iii the structural cells in the whole computational domain and the near wall mesh generation technique will effectively reduce the numerical cost the proposed mesh generation method was executed with the aid of pre processor icem 13 0 icem 2010 in addition this proposed mesh generation method can be used in other pre processors e g gambit pointwise etc 2 5 case study the hkpolyu campus 2 5 1 computational model the hkpolyu campus model is utilized to illustrate the application of the proposed mesh generation method the campus is located in midtown of hong kong with an area about 500m by 280m as shown in fig 3 where all the buildings are closely arranged and have complicated geometric configurations which includes lift up design between the ground and the main building structures it consists of high rise and low rise buildings and the heights of main buildings are indicated in fig 3 it is noted that the buildings in the campus are labelled in letters a b c y which represent different building cores in the campus the building that is located between two cores is known as building wing such as the building between d core and e core is de wing as for the building heights the high buildings are m building 65 7m and y building 58 4m while the low buildings are vs building 8 9m and va building 9 3m the average height of building cores in the campus is 36 1m besides there are two large courtyards below the ground in the campus that have significant influence on the pedestrian level wind environment one is surrounded by pq wing qr wing and m building and the other is surrounded by cd wing de wing ef wing and cf wing the average depths of the caves are 7m the buildings and caves in the campus are all explicitly included in the computational model it should be mentioned that the computational model of the campus is constructed in 1 200 scale which is the same scale as the wind tunnel test model since the wind tunnel test results will be used for validation the geometrical complexity of the campus is modelled in great detail in the computational model any configurations more than 1m in prototype were reproduced the computational model was generated directly from the construction drawings of the campus since the data with such high resolution at pedestrian level were not available from gis or city database the lift up design between the ground and the main building structures is a very distinctive building features in the campus with average height of 4 meters which provides shielding effect from solar radiation in the hot summer and also enhances wind velocity at pedestrian level du et al 2017a liu et al 2016 niu et al 2015 thus it is necessary to reproduce the geometry configurations in the computational model and apply high resolution and high quality grid in the lift up area 2 5 2 computational grid obviously the campus is a very complex urban area and the proposed mesh generation method is therefore utilized to construct the mesh of the campus model it should be mentioned that the vegetative elements are not included in the computational mesh since this study focuses on pedestrian level wind environment special attention is paid to the building configuration at pedestrian level during the meshing process overviews of the mesh from eastward and its corresponding image from google map are presented in fig 4 a and fig 4 b respectively the computational mesh in fig 4 a consists of 8 9 million cells it can be seen from fig 4 that the computational grid has high quality and high resolution all over the computational campus terrain and a maximum stretching ratio of 1 18 outside the campus model 1 2 in ramponi and blocken 2012 as shown in fig 4 the top structures of building m and y are not entirely included in the computational model this is because these top structures are located at a relatively large distance from the pedestrian level around 70 m in prototype which have slight influences on pedestrian level wind environment besides some differences in building models can be observed between fig 4 a and fig 4 b and the newly built v block is not included in the computational model the reason is that the simulation results in this study will be compared with wind tunnel test results for validation see section 3 1 thus the computational model is built based on the model used during the wind tunnel tests the specific view of the computational cells for lift up design and its corresponding photos are presented in fig 5 as indicated in fig 5 the quality and resolution of computational cells in lift up area are very high which provide the prerequisite for accurately reproducing wind flow at pedestrian level in the lift up area besides it can also be observed in fig 5 that at least ten cells have been applied over the height of the lift up which suggests that there are over five cells at pedestrian level this fulfils the requirement recommended by bpgs franke et al 2007 tominaga et al 2008b that the pedestrian level should be located at third or higher cell above the ground when modelling pedestrian level wind environment in an urban area 3 cfd validation basic case 3 1 wind tunnel test for validation purpose the wind tunnel tests of the campus model were conducted at a scale of 1 200 in the clp power wind wave tunnel facility wwtf at hong kong university of science and technology hkust the blockage ratio of the test is 2 2 which is less than 10 and the constraining effects were minimized in this test asce 1999 during the tests the reynolds number re was over 7 8 104 which can be considered as sufficiently large enough to obtain re independence awes 2001 the wind tunnel test photo of the campus model and the wind profile used during the test are shown in fig 6 it can be observed from fig 6 a that the campus model was reproduced in great detail the wind profiles in fig 6 b and fig 6 c are adopted from our previous study du et al 2017a and the error shown in the figures are within 5 which suggests the reliability of the measurements as shown in figs 7 and 50 tests points at pedestrian level 0 01m above podium floor in scaled model and 2m in prototype were used in this study for measurements it can be observed that the test points were evenly distributed in the campus the blue dots are test points located on the podium floor labelled in letter p and the orange dots are the test points located in the lift up area labelled in letter l the blue dashed areas in fig 7 are the lift up areas in the campus the kanomax velocity sensors were used during the wind tunnel tests the measuring frequency of the sensor was 10hz and the sampling time was set to be 2 min 1 h in reality as indicated in our previous study du et al 2017a the sensors were calibrated against a reference sensor prior to the measurement and the largest discrepancies between the kanomax velocity sensors and the reference sensors were within 5 3 2 computational domain the length of upstream domain should be kept as short as possible to avoid unintended stream wise gradients while still fulfil the recommendations of bpgs the length of downstream should be taken long enough to assure the fully development of wind flow in the domain outlet the dimensions of lateral side and domain height should be chosen carefully to avoid the occurrence of artificial acceleration due to the close distance between the lateral domain and building models in this study 5bh bh is the height of highest building in the campus is chosen for the upstream length and 22 bh is chosen for the downstream length the domain lateral and the domain height of the this case are chosen based on directional blockage ratio dbr blocken 2015 which is the decomposition of limit value 3 in lateral horizontal and vertical direction since the square root of 3 is 17 the following equations can be established 1 l a t e r a l b r l b u i l d i n g l d o m a i n 17 2 v e r t i c a l b r h b u i l d i n g h d o m a i n 17 where br stands for blockage ratio l is length and h is height it is obvious that eq 1 and eq 2 can automatically satisfied the maximum blockage ratio of 3 recommended by bpgs franke et al 2007 tominaga et al 2008b noted that in this study only east wind direction is considered the schematic view of the computational domain is presented in fig 8 the domain dimensions are l w h 14 41 m 14 5 m 2 57 m for east wind direction 3 3 boundary conditions and other computational settings the inlet boundary profiles of mean wind velocity u turbulence kinetic energy k and turbulent dissipation rate ε are specified by eq 3 eq 5 which are considered as capable of assuring a homogenous boundary layer when combined with proper near wall treatment on the domain ground the aerodynamic roughness height z 0 and the friction velocity u are determined by fitting eq 3 to the measured u profile in fig 6 b which results in z 0 0 0001 m and u 0 2998 m s the coefficients m 1 and m 2 are obtained by fitting eq 4 to the test data of k profile in fig 6 c and m 1 0 97 m 2 6 23 κ is the von karman constant which equals to 0 4187 the constant c μ is defined empirically as 0 09 3 u u κ z z 0 z 0 4 k m 1 ln z z 0 m 2 5 ε u c μ κ z z 0 m 1 ln z z 0 m 2 zero normal velocity and zero normal gradients are used on the domain ceiling and domain lateral and zero statistic pressure is used on the domain outlet as for the domain ground the two layer model is utilized ai and mak 2013 durbin et al 2001 it should be mentioned that the homogeneity of these boundary conditions is examined by comparing the inlet and incident profiles in an empty computational domain before commencing cfd simulation the results of the homogeneity examination are shown in fig 9 and it can be seen that a good homogenous boundary layer has been achieved the numerical simulation is carried out by cfd code fluent 13 0 0 fluent 2010 combing with a series of user defined functions udf the renormalized group rng k ε model is utilized to predict wind flow field because of its general good performance on predicting pedestrian level wind environment ai and mak 2013 blocken et al 2016 du et al 2017b tominaga and stathopoulos 2009 the discretization of the governing equations is conducted on a staggered grid system to algebraic equations which is based on the finite volume method fvm the equations of momentum and pressure are coupled by the simplec algorithm with second order upwind scheme the convergence of the simulation is achieved when all the scaled residuals are less than 10 5 and the monitored wind velocities at pedestrian level are stable for over 50 iterations ai and mak 2014 it should be mentioned that 16 measuring points in fig 7 are monitored during the simulations 8 points in the lift up area and 8 points in the podium area the simulations are considered converged when all the monitored points are stable for over 50 iterations apart from the basic mesh with 8 9 million cells as shown in fig 10 a one coarser mesh and one finer mesh are also constructed containing about 6 3 million and 12 2 million cells respectively the predicted results of three mesh systems is shown in fig 11 it can be seen that the prediction differences between basic mesh and coarse mesh are obvious while the prediction differences between basic mesh and fine mesh are subtle taking the computational cost and the accuracy of the simulation into consideration the basic mesh is used for the following simulations after the mesh sensitivity test of the computational model the aforementioned near wall mesh generation technique as shown in section 2 1 is utilized here to obtain the suitable near wall mesh density for simulation requirements it is noted that after second increase of near wall mesh the average values of y equals to 4 2 and results in 11 3 million cells which is good enough for two layer model ai and mak 2013 3 4 validation results the normalized mean wind parameter mean wind velocity ratio mvr which has been used in our previous studies du et al 2017a 2017b is adopted here to evaluate wind environment at pedestrian level the definition of mvr is shown as follows 6 m v r u p u r where u p is the mean wind velocity at pedestrian level u r denotes the mean wind velocity at reference height which is 200m in prototype and 1m in model scale the predicted results and the corresponding wind tunnel test data are shown in fig 12 it can be observed that a good agreement is achieved and the deviations are almost within 20 between the predicted and measurement results actually over 70 measuring points are within the deviation of 10 which can be considered sufficiently accurate for predicting pedestrian level wind environment 3 5 pedestrian level wind environment fig 13 presents the predicted results of pedestrian level wind environment in the campus model when the approaching wind comes from east according to the planning department of hksar the annual average wind velocity at 200m reference height is 5m s and the prevailing wind direction is east direction hksar thus the wind environment shown in fig 13 can generally represent the annual wind environment at pedestrian level in the campus to be able to reach the minimum wind velocity of 1 5m s which is the minimum threshold value of acceptable wind comfort in summer according to the wind comfort criteria proposed by the authors du et al 2017a and also meets the requirement of air ventilation assessment ava scheme in hong kong ng 2009 an mvr value equal or over 0 3 is required to achieve acceptable wind environment on the contrary areas with mvr values below 0 3 are designated as low wind environment as illustrated in fig 13 du et al 2017b it can be observed that a large portion of the campus has low wind environment at pedestrian level especially on the southward and westward side of the campus meanwhile the east and middle parts of the campus have relatively higher wind velocity which can be considered as acceptable wind environment besides it can be obtained that the lift up areas in the campus have higher wind environment at pedestrian level than the podium areas which corresponds with our earlier findings that the lift up design can enhance the pedestrian level wind comfort in low wind environment du et al 2017a 2017b 4 sensitivity analysis as mentioned before it is worth noting that the computational parameters of turbulence models near wall mesh density and computational domain size are important for accurately predicting pedestrian level wind environment especially in a complex urban area this section reports on detailed analysis of the above computational parameters which can also be applied to other urban regions in order to quantitatively evaluate the effects of these parameters two commonly used residual criteria mean absolute percentage error m a p e and root mean square error r m s e bennett et al 2013 are adopted here these criteria are chosen because of the fact that mape can provide an evaluation of mean predicted errors without cancellation and r m s e is a measure of evaluating the overall deviation between predicted results and measured results the definitions of m a p e and r m s e are shown as follows 7 m a p e 1 n i 1 n m v r i ˆ m v r i m v r i ˆ 100 8 r m s e 1 n i 1 n m v r ˆ i m v r i 2 here n is the total number of evaluation points m v r ˆ i stands for measured result for the same location m v r i denotes predicted result of a specific location 4 1 effects of turbulence model the selection of a suitable turbulence model is crucial for accurately predicting the pedestrian level wind environment since the reproduction of the flow structure in a built environment is strongly affected by the turbulence model in this section four types of steady rans turbulence model in the k ε family are used to predict the wind flow in the computational model i e the standard k ε turbulence model launder and spalding 1972 hereafter ske the rng k ε turbulence model yakhot et al 1992 hereafter rng the mmk k ε turbulence model tsuchiya et al 1997 hereafter mmk and the realizable k ε turbulence model shih et al 1995 hereafter rlz the transport equations of turbulence kinetic k and dissipation rate ε for the above turbulence models have similar expressions which are shown in eq 9 11 the equations of eddy viscosity for each turbulence model also have similar forms see eq 11 but the methods for calculating turbulent viscosity are different besides the turbulent prandtl numbers and the generation or destruction terms of ε are different in each turbulence model the major differences for each model are summarized in table 1 9 k t u i k x i x j ν t σ k k x j p k ε 10 ε t u i k x i x j ν t σ ε ε x j ε k c 1 ε p k c 2 ε ε 11 ν t c μ k 2 ε the predicted results of each turbulence model are presented in fig 14 except the results of rng which have been shown in fig 12 it can be observed that the predicted results obtained by all rans turbulence models agree generally well with wind tunnel measurement results the predicted results given by ske show the largest discrepancies with the measurement results among all the turbulence models because of its incapability of reproducing the flow structure in a built environment fluent 2010 franke et al 2007 tominaga and stathopoulos 2009 the results predicted by mmk display an overall good agreement with the wind tunnel measurements but most of the predicted results are overestimated the predicted results produced by rng have good agreement with wind tunnel measurements which has been explained in section 3 4 even though there are some large discrepancies in fig 14 c the predicted results obtained from rlz are generally agrees very well with the wind tunnel measurements to be able to quantitatively assess the predicted performance of each turbulence model the predicted errors for each turbulence model are given in table 2 it can be seen that the m a p e and r m s e results obtained by rng are smallest among the turbulence models which suggested that the results predicted by rng are closest to the wind tunnel measurements the second best result is given by rlz and it is followed by mmk and ske thus the rng yields the best simulation performance on the campus model among the four turbulence models 4 2 effects of near wall mesh density the near wall mesh generation technique is achieved by doubling the first near wall cell which aims to potentially save computational cost while obtaining the desired y value at pedestrian level the prerequisite for using this technique is the independence of mesh resolution in the whole computational domain as presented in section 2 1 and section 3 3 this section tests this method based on the results of hkpolyu campus model in section 3 in addition to the case in section 3 which has an average y value of 4 2 at pedestrian level one lower value y 1 9 and two larger values y 8 3 18 1 are investigated in this section the increases in near wall mesh density with the average y values from 18 1 to 1 9 are schematically shown in fig 15 it can be observed that the cells increase 0 9 million 2 5 million and 6 1 million when the average y values decrease from 18 1 to 8 3 8 3 to 4 2 and 4 2 to 1 9 respectively these results demonstrate that the near wall mesh technique can achieve an ideal y value while not causing a substantial grid number increase fig 16 shows the predicted results obtained from different near wall densities there are no obvious discrepancies between different near wall densities different y values because the first cells are all located within the inner layers y 30 the predicted results obtained from the cases of y 4 2 and 1 9 are almost the same and these results are more accurate than that of the cases of y 18 1 and 8 3 this is because the two layer model is used when the first cells are located in the viscous sublayer y 5 ai and mak 2013 fluent 2010 even though the increase of near wall density does not yield significant improvement in the predicted accuracy the decrease of the y values shows the feasibility of using transit turbulence models e g detached eddy simulation des model and large eddy simulation les model the predicted errors of the different near wall mesh densities are given in table 3 it is obvious that the values of m a p e and r m s e obtained from the cases of y 18 1 and 8 3 are larger than that of y 4 2 and 1 9 there are no significant differences between the cases of y 4 2 and 1 9 because of the fact that the two layer model has been utilized during the simulation 4 3 effects of computational domain size the size of computational domain can definitely affect the fully development of wind flow which in turn can affect the predicted accuracy of pedestrian level wind environment according to the bpgs three choices have been provided for urban models of selecting proper domain cross section size i the lateral boundary should be placed at least 5 h m a x away from the modelled area h m a x is the height of tallest building in the computational model ii the same cross section size as the wind tunnel test iii the dbr method presented in section 3 2 which aims to ensure the maximum blockage ratio is below 3 in this section the effects of above different domain cross section size on the predicted results of pedestrian level wind environment are examined based on the campus model apart from the aforementioned choices of domain cross section size a larger domain cross section is also tested in this study it should be mentioned that this section only focuses on the effects of domain cross section the summary of the four domain sizes are presented in table 4 and the longitudinal extension of other three cases are same as the case in section 3 2 see fig 8 the results of the four domain are shown in fig 17 it can be observed that the results of case 1 and case 2 show larger discrepancies with the measurement results than that of case 3 and case 4 due to the small cross sections the large deviations mainly occurred when the measuring points are on the edge of the campus model e g p01 p04 l18 this suggests that the computational cross sections of case 1 and case 2 are not large enough for the fully development of the horizontal flow besides the results obtained from case 3 and case 4 are almost the same which indicates that the domain size of the case 3 is large enough for modelling the campus model thus the computational cross section chosen by dbr method see section 3 2 can guarantee the fully development of the horizontal and vertical flow when the width is larger than the building height in the modelled area the predicted errors of the four cases are given in table 5 it can be seen that the values of m a p e and r m s e obtained from case 3 and case 4 are almost the same however the results of m a p e and r m s e given by case 1 and case 2 are significantly larger than that of case 3 and case 4 the differences are mainly caused by the measuring points that located on the edge of computational model which has been illustrated in fig 15 5 conclusions this paper presents a study of cfd simulation of pedestrian level wind environment in a complex urban area an effective and systematic mesh generation method is proposed and the detailed generation procedure is provided this mesh generation method can generate a high quality structural mesh system in a complex urban environment with full control over whole computational domain in addition it can ensure a sufficient near wall mesh density without a significant increase of the total number of cells this mesh generation method is demonstrated and evaluated based on the complex hkpolyu campus model where wind tunnel experimental data is also available a good agreement between cfd results and wind tunnel data is achieved which further confirms the reliability of this method based on the meshed hkpolyu campus model the sensitivity tests of three computational parameters namely turbulence model near wall mesh density and computational domain size are performed apart from the direct comparison of the cfd and wind tunnel results mape and rmse are utilized in this study to quantitatively assess the effects of the computational parameters the main findings of the sensitivity tests can be summarized as follows among the four rans models tested the rng turbulence model yields overall the best performance in predicting pedestrian level wind environment of the campus model while the ske turbulence model cannot provide adequate predicted results the near wall mesh generation technique can provide sufficient near wall mesh density for simulation requirement without leading to striking number of cells for the computational model that has a larger model width than building height the dbr method for choosing the domain cross section should be used to ensure the maximum blockage ratio below 3 in this study only the steady state rans turbulence models were used however the proposed mesh generation method can provide a sufficient near wall mesh density with moderate mesh number which therefore also allows the application of advanced transient turbulence models e g detached eddy simulation des model and large eddy simulation les model the time needed for constructing a complex grid is also considered very important during the pre processing stage thus further studies are still needed to compare the amounts of time used by the proposed method and other meshing methods acknowledgement the work described in this paper was fully supported by a grant from the research grants council of the hong kong special administrative region china project no c5002 14g 
26409,quality and efficiency of computational fluid dynamics cfd simulation of pedestrian level wind environment in a complex urban area are often compromised by many influencing factors particularly mesh quality this paper first proposes a systematic and efficient mesh generation method and then performs detailed sensitivity analysis of some important computational parameters the geometrically complex hong kong polytechnic university hkpolyu campus is taken as a case study based on the high quality mesh system the influences of three important computational parameters namely turbulence model near wall mesh density and computational domain size on the cfd predicted results of pedestrian level wind environment are quantitatively evaluated validation of cfd models is conducted against wind tunnel experimental data where a good agreement is achieved it is found that the proposed mesh generation method can effectively provide a high quality and high resolution structural grid for cfd simulation of wind environment in a complex urban area keywords computational fluid dynamics cfd simulation pedestrian level wind environment mesh generation method complex urban environment sensitivity analysis 1 introduction the pedestrian level wind environment has become a pressing issue since achieving an acceptable wind environment is unsuccessful in many urban cities which results in many environmental problems including thermal comfort city ventilation and pollutant dispersion accurate modelling of wind flow in the urban environment is therefore crucial to wind comfort assessment wind safety assessment as well as thermal comfort evaluation all of which can affect the sustainable development of a built environment ai and mak 2015 blocken et al 2012 hang and li 2010 juan et al 2017 mochida and lun 2008 ng 2009 richards et al 2002 shi et al 2015 stathopoulos 2006 yuan et al 2016 in the past decades the pedestrian level wind environment in the urban environment has been extensively investigated by field measurement niu et al 2015 wind tunnel test cermak 2003 du et al 2017a tsang et al 2012 tse et al 2017 and computational fluid dynamics cfd modelling blocken et al 2016 du et al 2017b hong and lin 2015 liu et al 2016 tominaga and stathopoulos 2011 yoshie et al 2007 compared to the first two methods cfd simulation has lots of advantages in studying pedestrian level wind environment such as providing whole flow field data easily performing parametric study and less expensive however cfd simulation of urban environment has been hindered by several problems including the difficulties of generating high quality mesh for geometrically complex real urban community and the proper choice of important computational parameters therefore works that can improve the reliability and accuracy of cfd predicted results on pedestrian level wind environment still remains of great significance it is well known that the construction of high resolution and high quality mesh system for the complex urban environment is the prerequisite for successful simulation standard automatic or semi automatic generation of unstructured grid in a complex urban environment lacks adequate control of local mesh resolution and quality which often results in poor quality mesh system and hinders the accurate simulation of wind environment in urban areas besides generation of unstructured grid in a large computational terrain will lead to a striking number of cells especially when the grid is required to have high density at specific location e g at least 4 or 5 grid is required at pedestrian level for accurate prediction of pedestrian level wind environment franke et al 2007 tominaga et al 2008b the body fitted grid generation technique presented by van hooff and blocken 2010 allows full control over the mesh quality and resolution when modelling a complex stadium but it is limited to the use of unstructured prismatic cells in the computational domain on the contrary a high quality structural grid can effectively reduce the cell number save numerical cost avoid numerical diffusion and become more stable when modelling the complex urban environment than unstructured grids franke et al 2007 therefore a mesh generation method that can effectively produce high resolution and high quality structural grid with full control over the whole computational domain is in urgent need the time averaging reynolds averaged navier stokes rans turbulence models are commonly used to simulate pedestrian level wind environment in urban environment among the existing turbulence models for its acceptable performance and economic computational cost ai and mak 2013 2017 ai et al 2013 baker 2007 bechmann et al 2011 blocken 2015 cui et al 2016 du et al 2017b shi et al 2015 yoshie et al 2007 the realizable k ε turbulence model was used to predict the pedestrian level wind environment at the campus of eindhoven university of technology and the predicted results agreed generally well with the long term and short term on site measurements blocken et al 2012 janssen et al 2013 du et al 2017b utilized the renormalization group rng k ε model to evaluate the effects of lift up design between the ground and the main building structures on the pedestrian level wind comfort in different building configurations which aims to provide solid scientific evidence for improving weak wind conditions in hong kong even though the above studies have obtained overall good predicted results of pedestrian level wind environment by utilizing the rans turbulence model in cfd simulation these investigations did not conduct sensitivity tests of the computational parameters which may have major influence on the predicted results owing to the widely application of cfd simulation in urban aerodynamics a set of best practice guidelines bpgs are established to ensure the reliability and accuracy of the cfd predicted results franke et al 2007 jakeman et al 2006 laniak et al 2013 tamura et al 2008 tominaga et al 2008b in addition to this a lot of studies have been conducted to improve the accuracy in cfd simulation including achieving a homogeneous boundary layer abl ai and mak 2013 blocken et al 2007 gorlé et al 2009 yang et al 2009 while bpgs have detailed guidance on the choices of important computational parameters several choices are available for some important computational parameters when modelling a complex urban environment it has been indicated that the performance of different steady rans turbulence models are different when modelling wind flows around isolated buildings lateb et al 2013 tominaga et al 2008a tominaga and stathopoulos 2009 besides the study of ramponi and blocken 2012 reported that the different sizes of computational domain have great influence on the cross ventilation for a generic isolated building furthermore the investigation about the effect of near wall mesh density presented in our previous study of single sided ventilation ai and mak 2014 showed that the near wall mesh density can affect the predicted results of the ventilation rate however the sensitivity analyses of the computational parameters for cfd simulations are mainly focused on isolated buildings lateb et al 2013 lun et al 2007 ramponi and blocken 2012 tominaga and stathopoulos 2009 the effects of the computational parameters on the pedestrian level wind environment based on a complex urban environment are rarely reported in order to provide a reliable and accurate prediction of pedestrian level wind environment in a complex urban environment a comprehensive sensitivity analysis of the important computational parameters is required this paper presents an investigation of improving the predicted accuracy of the pedestrian level wind environment in a complex urban environment an effective and systematic mesh generation method is proposed in this study for generating a high quality structural grid which also allows full control over the grid resolution and ensures the near wall mesh density the proposed mesh generation method can be efficiently applied in meshing a complex urban area as well as simple building blocks another prominent feature of the proposed mesh generation method is that it can ensure a sufficient near wall mesh density without a significant increase of the total number of cells the computational model of the hkpolyu campus is used as a case study to illustrate the mesh generation method in which the configurations of the buildings are very complex and it has lift up design between the ground and the main building structures du et al 2017a niu et al 2015 the steady rans turbulence models are used to simulate the pedestrian level wind environment and the predicted results are validated against quality wind tunnel experimental data moreover the sensitivity analysis of three important computational parameters are conducted based on the campus model namely turbulence model near wall mesh density and computational domain size the proposed mesh generation method is described in section 2 with an illustration of the hkpolyu campus model section 3 presents the cfd simulation performance evaluation and the guidelines of the previous studies bennett et al 2013 blocken and gualtieri 2012 jakeman et al 2006 are followed to ensure the confident numerical modelling firstly the wind tunnel tests reported in section 3 1 satisfy the requirements of asce and awes asce 1999 awes 2001 for conducting quality wind tunnel tests secondly in section 3 2 and section 3 3 the best practice guidelines bpgs blocken 2015 franke et al 2007 tominaga et al 2008b for modelling urban aerodynamics are rigorously followed throughout the simulation thirdly the direct comparison method bennett et al 2013 is used to assess the model performance in section 3 4 meanwhile the quantitative evaluation method bennett et al 2013 is used in section 4 to analyse the sensitivity test results of the important computational parameters finally concluding marks are given in section 5 2 computational mesh 2 1 mesh generation method the resolution and quality of mesh system is considered crucial to accurately reproduce the wind flow in a complex urban environment especially when modelling pedestrian level wind environment however generating a computational domain with high quality and high resolution structural grid is definitely not straightforward therefore to be able to allow full control of local cells and generate a high quality structural grid in the whole computational domain an efficient and systematic mesh generation method is proposed here it is a specific procedure to effectively generate high quality structural grid in a complex urban environment and also ensures high density of near wall mesh the mesh generation method is schematically depicted in fig 1 it should be mentioned that the mesh generation method presented in this study is different from the body fitted bf mesh generation technique used by van hooff and blocken 2010 in the following two ways i the bf technique generates both prismatic cells unstructured cells and hexahedral cells structural cells in the computational domain and the prismatic cells are used in the immediate vicinity of complex buildings however the mesh generation method presented by this paper can produce hexahedral cells all over the computational domain ii the use of structural cells and near wall mesh technique in this proposed mesh generation method can potentially reduce numerical cost as shown in fig 1 a the mesh generation method contains three main parts mesh generation preparation mesh generation technique and near wall mesh generation technique fig 1 b fig 1 c and fig 1 d presents the schematic illustration of the step 2 step 3 to step 5 and step 6 in fig 1 a respectively the detailed explanation for the procedure are outlined as follows 2 2 mesh generation preparation the computational geometry should include all the buildings and obstacles that may affect the wind flow in the computational model if necessary the ground roughness boundary conditions should be specified appropriately 2 3 mesh generation techniques the application of this technique is schematically illustrated by using the hkpolyu campus model fig 1 b and the detailed division process of a sub domain is schematically illustrated by a generic lift up design model fig 1 c and fig 1 d du et al 2017b step 1 building a computational domain for the target urban area the size of the computational domain need to be chosen carefully according to bpgs blocken 2015 franke et al 2007 tominaga et al 2008b and it should be large enough to avoid artificial acceleration of the wind flow step 2 dividing the computational domain built in step 1 into sub domains according to the outermost building geometric configuration as shown in fig 1 b this is very important because the outermost geometric configuration ensures that all the building geometries are within one sub domain by doing this the difficulties in meshing the complex urban area become meshing independent sub domains which is very similar as meshing isolated buildings step 3 and 4 determining the required points in each sub domain and defining a vertical line that summarizes all the required points the required points on the vertical line a b e see fig 1 c should be chosen according to the geometric changes of each sub domains and the simulation requirements the simulation requirements refer to specific demand for mesh density vertically for instance this paper focuses on the pedestrian level wind environment and at least four to five cells should be applied at pedestrian level according to the bpgs thus point b is defined in the vertical line step 5 dividing the sub domains into layer domains according to all the points in the vertical line it should be mentioned that the sub domains should be divided according to all the required points in the computational domain instead of the required points in one sub domain step 6 dividing the layer domains to individual domains according to the building geometric configuration fig 1 d this step requires considerable patience and time since it relates to the accurate reproduction of building geometry step 7 and 8 applying grid to each individual domains and deleting the unwanted mesh this way only the spaces that are outside buildings are meshed and the local grid quality can be ensured 2 4 near wall mesh generation technique it is noted that the independence of mesh resolution in the whole computational domain is the prerequisite for using near wall mesh generation technique the near wall mesh is increased by only doubling the first cell in both horizontal and vertical directions and there will be no change to other non near wall cells which has been utilized in our previous study ai and mak 2014 the schematic view of the near wall mesh generation technique and the conventional mesh generation technique are shown in fig 2 a and fig 2 b respectively it has been proven to be effectively reducing numerical cost for achieving the same near wall mesh density compared to conventional mesh generation technique that has the same stretching ratio from the first cell in the whole computational domain the advantages of this mesh generation technique are i it is a systematic and efficient procedure for generating structural cells in the whole domain ii it allows full control over the mesh generation procedure and assures the resolution and quality of local cells which provides the prerequisite for accurately modelling wind environment in complex urban area iii the structural cells in the whole computational domain and the near wall mesh generation technique will effectively reduce the numerical cost the proposed mesh generation method was executed with the aid of pre processor icem 13 0 icem 2010 in addition this proposed mesh generation method can be used in other pre processors e g gambit pointwise etc 2 5 case study the hkpolyu campus 2 5 1 computational model the hkpolyu campus model is utilized to illustrate the application of the proposed mesh generation method the campus is located in midtown of hong kong with an area about 500m by 280m as shown in fig 3 where all the buildings are closely arranged and have complicated geometric configurations which includes lift up design between the ground and the main building structures it consists of high rise and low rise buildings and the heights of main buildings are indicated in fig 3 it is noted that the buildings in the campus are labelled in letters a b c y which represent different building cores in the campus the building that is located between two cores is known as building wing such as the building between d core and e core is de wing as for the building heights the high buildings are m building 65 7m and y building 58 4m while the low buildings are vs building 8 9m and va building 9 3m the average height of building cores in the campus is 36 1m besides there are two large courtyards below the ground in the campus that have significant influence on the pedestrian level wind environment one is surrounded by pq wing qr wing and m building and the other is surrounded by cd wing de wing ef wing and cf wing the average depths of the caves are 7m the buildings and caves in the campus are all explicitly included in the computational model it should be mentioned that the computational model of the campus is constructed in 1 200 scale which is the same scale as the wind tunnel test model since the wind tunnel test results will be used for validation the geometrical complexity of the campus is modelled in great detail in the computational model any configurations more than 1m in prototype were reproduced the computational model was generated directly from the construction drawings of the campus since the data with such high resolution at pedestrian level were not available from gis or city database the lift up design between the ground and the main building structures is a very distinctive building features in the campus with average height of 4 meters which provides shielding effect from solar radiation in the hot summer and also enhances wind velocity at pedestrian level du et al 2017a liu et al 2016 niu et al 2015 thus it is necessary to reproduce the geometry configurations in the computational model and apply high resolution and high quality grid in the lift up area 2 5 2 computational grid obviously the campus is a very complex urban area and the proposed mesh generation method is therefore utilized to construct the mesh of the campus model it should be mentioned that the vegetative elements are not included in the computational mesh since this study focuses on pedestrian level wind environment special attention is paid to the building configuration at pedestrian level during the meshing process overviews of the mesh from eastward and its corresponding image from google map are presented in fig 4 a and fig 4 b respectively the computational mesh in fig 4 a consists of 8 9 million cells it can be seen from fig 4 that the computational grid has high quality and high resolution all over the computational campus terrain and a maximum stretching ratio of 1 18 outside the campus model 1 2 in ramponi and blocken 2012 as shown in fig 4 the top structures of building m and y are not entirely included in the computational model this is because these top structures are located at a relatively large distance from the pedestrian level around 70 m in prototype which have slight influences on pedestrian level wind environment besides some differences in building models can be observed between fig 4 a and fig 4 b and the newly built v block is not included in the computational model the reason is that the simulation results in this study will be compared with wind tunnel test results for validation see section 3 1 thus the computational model is built based on the model used during the wind tunnel tests the specific view of the computational cells for lift up design and its corresponding photos are presented in fig 5 as indicated in fig 5 the quality and resolution of computational cells in lift up area are very high which provide the prerequisite for accurately reproducing wind flow at pedestrian level in the lift up area besides it can also be observed in fig 5 that at least ten cells have been applied over the height of the lift up which suggests that there are over five cells at pedestrian level this fulfils the requirement recommended by bpgs franke et al 2007 tominaga et al 2008b that the pedestrian level should be located at third or higher cell above the ground when modelling pedestrian level wind environment in an urban area 3 cfd validation basic case 3 1 wind tunnel test for validation purpose the wind tunnel tests of the campus model were conducted at a scale of 1 200 in the clp power wind wave tunnel facility wwtf at hong kong university of science and technology hkust the blockage ratio of the test is 2 2 which is less than 10 and the constraining effects were minimized in this test asce 1999 during the tests the reynolds number re was over 7 8 104 which can be considered as sufficiently large enough to obtain re independence awes 2001 the wind tunnel test photo of the campus model and the wind profile used during the test are shown in fig 6 it can be observed from fig 6 a that the campus model was reproduced in great detail the wind profiles in fig 6 b and fig 6 c are adopted from our previous study du et al 2017a and the error shown in the figures are within 5 which suggests the reliability of the measurements as shown in figs 7 and 50 tests points at pedestrian level 0 01m above podium floor in scaled model and 2m in prototype were used in this study for measurements it can be observed that the test points were evenly distributed in the campus the blue dots are test points located on the podium floor labelled in letter p and the orange dots are the test points located in the lift up area labelled in letter l the blue dashed areas in fig 7 are the lift up areas in the campus the kanomax velocity sensors were used during the wind tunnel tests the measuring frequency of the sensor was 10hz and the sampling time was set to be 2 min 1 h in reality as indicated in our previous study du et al 2017a the sensors were calibrated against a reference sensor prior to the measurement and the largest discrepancies between the kanomax velocity sensors and the reference sensors were within 5 3 2 computational domain the length of upstream domain should be kept as short as possible to avoid unintended stream wise gradients while still fulfil the recommendations of bpgs the length of downstream should be taken long enough to assure the fully development of wind flow in the domain outlet the dimensions of lateral side and domain height should be chosen carefully to avoid the occurrence of artificial acceleration due to the close distance between the lateral domain and building models in this study 5bh bh is the height of highest building in the campus is chosen for the upstream length and 22 bh is chosen for the downstream length the domain lateral and the domain height of the this case are chosen based on directional blockage ratio dbr blocken 2015 which is the decomposition of limit value 3 in lateral horizontal and vertical direction since the square root of 3 is 17 the following equations can be established 1 l a t e r a l b r l b u i l d i n g l d o m a i n 17 2 v e r t i c a l b r h b u i l d i n g h d o m a i n 17 where br stands for blockage ratio l is length and h is height it is obvious that eq 1 and eq 2 can automatically satisfied the maximum blockage ratio of 3 recommended by bpgs franke et al 2007 tominaga et al 2008b noted that in this study only east wind direction is considered the schematic view of the computational domain is presented in fig 8 the domain dimensions are l w h 14 41 m 14 5 m 2 57 m for east wind direction 3 3 boundary conditions and other computational settings the inlet boundary profiles of mean wind velocity u turbulence kinetic energy k and turbulent dissipation rate ε are specified by eq 3 eq 5 which are considered as capable of assuring a homogenous boundary layer when combined with proper near wall treatment on the domain ground the aerodynamic roughness height z 0 and the friction velocity u are determined by fitting eq 3 to the measured u profile in fig 6 b which results in z 0 0 0001 m and u 0 2998 m s the coefficients m 1 and m 2 are obtained by fitting eq 4 to the test data of k profile in fig 6 c and m 1 0 97 m 2 6 23 κ is the von karman constant which equals to 0 4187 the constant c μ is defined empirically as 0 09 3 u u κ z z 0 z 0 4 k m 1 ln z z 0 m 2 5 ε u c μ κ z z 0 m 1 ln z z 0 m 2 zero normal velocity and zero normal gradients are used on the domain ceiling and domain lateral and zero statistic pressure is used on the domain outlet as for the domain ground the two layer model is utilized ai and mak 2013 durbin et al 2001 it should be mentioned that the homogeneity of these boundary conditions is examined by comparing the inlet and incident profiles in an empty computational domain before commencing cfd simulation the results of the homogeneity examination are shown in fig 9 and it can be seen that a good homogenous boundary layer has been achieved the numerical simulation is carried out by cfd code fluent 13 0 0 fluent 2010 combing with a series of user defined functions udf the renormalized group rng k ε model is utilized to predict wind flow field because of its general good performance on predicting pedestrian level wind environment ai and mak 2013 blocken et al 2016 du et al 2017b tominaga and stathopoulos 2009 the discretization of the governing equations is conducted on a staggered grid system to algebraic equations which is based on the finite volume method fvm the equations of momentum and pressure are coupled by the simplec algorithm with second order upwind scheme the convergence of the simulation is achieved when all the scaled residuals are less than 10 5 and the monitored wind velocities at pedestrian level are stable for over 50 iterations ai and mak 2014 it should be mentioned that 16 measuring points in fig 7 are monitored during the simulations 8 points in the lift up area and 8 points in the podium area the simulations are considered converged when all the monitored points are stable for over 50 iterations apart from the basic mesh with 8 9 million cells as shown in fig 10 a one coarser mesh and one finer mesh are also constructed containing about 6 3 million and 12 2 million cells respectively the predicted results of three mesh systems is shown in fig 11 it can be seen that the prediction differences between basic mesh and coarse mesh are obvious while the prediction differences between basic mesh and fine mesh are subtle taking the computational cost and the accuracy of the simulation into consideration the basic mesh is used for the following simulations after the mesh sensitivity test of the computational model the aforementioned near wall mesh generation technique as shown in section 2 1 is utilized here to obtain the suitable near wall mesh density for simulation requirements it is noted that after second increase of near wall mesh the average values of y equals to 4 2 and results in 11 3 million cells which is good enough for two layer model ai and mak 2013 3 4 validation results the normalized mean wind parameter mean wind velocity ratio mvr which has been used in our previous studies du et al 2017a 2017b is adopted here to evaluate wind environment at pedestrian level the definition of mvr is shown as follows 6 m v r u p u r where u p is the mean wind velocity at pedestrian level u r denotes the mean wind velocity at reference height which is 200m in prototype and 1m in model scale the predicted results and the corresponding wind tunnel test data are shown in fig 12 it can be observed that a good agreement is achieved and the deviations are almost within 20 between the predicted and measurement results actually over 70 measuring points are within the deviation of 10 which can be considered sufficiently accurate for predicting pedestrian level wind environment 3 5 pedestrian level wind environment fig 13 presents the predicted results of pedestrian level wind environment in the campus model when the approaching wind comes from east according to the planning department of hksar the annual average wind velocity at 200m reference height is 5m s and the prevailing wind direction is east direction hksar thus the wind environment shown in fig 13 can generally represent the annual wind environment at pedestrian level in the campus to be able to reach the minimum wind velocity of 1 5m s which is the minimum threshold value of acceptable wind comfort in summer according to the wind comfort criteria proposed by the authors du et al 2017a and also meets the requirement of air ventilation assessment ava scheme in hong kong ng 2009 an mvr value equal or over 0 3 is required to achieve acceptable wind environment on the contrary areas with mvr values below 0 3 are designated as low wind environment as illustrated in fig 13 du et al 2017b it can be observed that a large portion of the campus has low wind environment at pedestrian level especially on the southward and westward side of the campus meanwhile the east and middle parts of the campus have relatively higher wind velocity which can be considered as acceptable wind environment besides it can be obtained that the lift up areas in the campus have higher wind environment at pedestrian level than the podium areas which corresponds with our earlier findings that the lift up design can enhance the pedestrian level wind comfort in low wind environment du et al 2017a 2017b 4 sensitivity analysis as mentioned before it is worth noting that the computational parameters of turbulence models near wall mesh density and computational domain size are important for accurately predicting pedestrian level wind environment especially in a complex urban area this section reports on detailed analysis of the above computational parameters which can also be applied to other urban regions in order to quantitatively evaluate the effects of these parameters two commonly used residual criteria mean absolute percentage error m a p e and root mean square error r m s e bennett et al 2013 are adopted here these criteria are chosen because of the fact that mape can provide an evaluation of mean predicted errors without cancellation and r m s e is a measure of evaluating the overall deviation between predicted results and measured results the definitions of m a p e and r m s e are shown as follows 7 m a p e 1 n i 1 n m v r i ˆ m v r i m v r i ˆ 100 8 r m s e 1 n i 1 n m v r ˆ i m v r i 2 here n is the total number of evaluation points m v r ˆ i stands for measured result for the same location m v r i denotes predicted result of a specific location 4 1 effects of turbulence model the selection of a suitable turbulence model is crucial for accurately predicting the pedestrian level wind environment since the reproduction of the flow structure in a built environment is strongly affected by the turbulence model in this section four types of steady rans turbulence model in the k ε family are used to predict the wind flow in the computational model i e the standard k ε turbulence model launder and spalding 1972 hereafter ske the rng k ε turbulence model yakhot et al 1992 hereafter rng the mmk k ε turbulence model tsuchiya et al 1997 hereafter mmk and the realizable k ε turbulence model shih et al 1995 hereafter rlz the transport equations of turbulence kinetic k and dissipation rate ε for the above turbulence models have similar expressions which are shown in eq 9 11 the equations of eddy viscosity for each turbulence model also have similar forms see eq 11 but the methods for calculating turbulent viscosity are different besides the turbulent prandtl numbers and the generation or destruction terms of ε are different in each turbulence model the major differences for each model are summarized in table 1 9 k t u i k x i x j ν t σ k k x j p k ε 10 ε t u i k x i x j ν t σ ε ε x j ε k c 1 ε p k c 2 ε ε 11 ν t c μ k 2 ε the predicted results of each turbulence model are presented in fig 14 except the results of rng which have been shown in fig 12 it can be observed that the predicted results obtained by all rans turbulence models agree generally well with wind tunnel measurement results the predicted results given by ske show the largest discrepancies with the measurement results among all the turbulence models because of its incapability of reproducing the flow structure in a built environment fluent 2010 franke et al 2007 tominaga and stathopoulos 2009 the results predicted by mmk display an overall good agreement with the wind tunnel measurements but most of the predicted results are overestimated the predicted results produced by rng have good agreement with wind tunnel measurements which has been explained in section 3 4 even though there are some large discrepancies in fig 14 c the predicted results obtained from rlz are generally agrees very well with the wind tunnel measurements to be able to quantitatively assess the predicted performance of each turbulence model the predicted errors for each turbulence model are given in table 2 it can be seen that the m a p e and r m s e results obtained by rng are smallest among the turbulence models which suggested that the results predicted by rng are closest to the wind tunnel measurements the second best result is given by rlz and it is followed by mmk and ske thus the rng yields the best simulation performance on the campus model among the four turbulence models 4 2 effects of near wall mesh density the near wall mesh generation technique is achieved by doubling the first near wall cell which aims to potentially save computational cost while obtaining the desired y value at pedestrian level the prerequisite for using this technique is the independence of mesh resolution in the whole computational domain as presented in section 2 1 and section 3 3 this section tests this method based on the results of hkpolyu campus model in section 3 in addition to the case in section 3 which has an average y value of 4 2 at pedestrian level one lower value y 1 9 and two larger values y 8 3 18 1 are investigated in this section the increases in near wall mesh density with the average y values from 18 1 to 1 9 are schematically shown in fig 15 it can be observed that the cells increase 0 9 million 2 5 million and 6 1 million when the average y values decrease from 18 1 to 8 3 8 3 to 4 2 and 4 2 to 1 9 respectively these results demonstrate that the near wall mesh technique can achieve an ideal y value while not causing a substantial grid number increase fig 16 shows the predicted results obtained from different near wall densities there are no obvious discrepancies between different near wall densities different y values because the first cells are all located within the inner layers y 30 the predicted results obtained from the cases of y 4 2 and 1 9 are almost the same and these results are more accurate than that of the cases of y 18 1 and 8 3 this is because the two layer model is used when the first cells are located in the viscous sublayer y 5 ai and mak 2013 fluent 2010 even though the increase of near wall density does not yield significant improvement in the predicted accuracy the decrease of the y values shows the feasibility of using transit turbulence models e g detached eddy simulation des model and large eddy simulation les model the predicted errors of the different near wall mesh densities are given in table 3 it is obvious that the values of m a p e and r m s e obtained from the cases of y 18 1 and 8 3 are larger than that of y 4 2 and 1 9 there are no significant differences between the cases of y 4 2 and 1 9 because of the fact that the two layer model has been utilized during the simulation 4 3 effects of computational domain size the size of computational domain can definitely affect the fully development of wind flow which in turn can affect the predicted accuracy of pedestrian level wind environment according to the bpgs three choices have been provided for urban models of selecting proper domain cross section size i the lateral boundary should be placed at least 5 h m a x away from the modelled area h m a x is the height of tallest building in the computational model ii the same cross section size as the wind tunnel test iii the dbr method presented in section 3 2 which aims to ensure the maximum blockage ratio is below 3 in this section the effects of above different domain cross section size on the predicted results of pedestrian level wind environment are examined based on the campus model apart from the aforementioned choices of domain cross section size a larger domain cross section is also tested in this study it should be mentioned that this section only focuses on the effects of domain cross section the summary of the four domain sizes are presented in table 4 and the longitudinal extension of other three cases are same as the case in section 3 2 see fig 8 the results of the four domain are shown in fig 17 it can be observed that the results of case 1 and case 2 show larger discrepancies with the measurement results than that of case 3 and case 4 due to the small cross sections the large deviations mainly occurred when the measuring points are on the edge of the campus model e g p01 p04 l18 this suggests that the computational cross sections of case 1 and case 2 are not large enough for the fully development of the horizontal flow besides the results obtained from case 3 and case 4 are almost the same which indicates that the domain size of the case 3 is large enough for modelling the campus model thus the computational cross section chosen by dbr method see section 3 2 can guarantee the fully development of the horizontal and vertical flow when the width is larger than the building height in the modelled area the predicted errors of the four cases are given in table 5 it can be seen that the values of m a p e and r m s e obtained from case 3 and case 4 are almost the same however the results of m a p e and r m s e given by case 1 and case 2 are significantly larger than that of case 3 and case 4 the differences are mainly caused by the measuring points that located on the edge of computational model which has been illustrated in fig 15 5 conclusions this paper presents a study of cfd simulation of pedestrian level wind environment in a complex urban area an effective and systematic mesh generation method is proposed and the detailed generation procedure is provided this mesh generation method can generate a high quality structural mesh system in a complex urban environment with full control over whole computational domain in addition it can ensure a sufficient near wall mesh density without a significant increase of the total number of cells this mesh generation method is demonstrated and evaluated based on the complex hkpolyu campus model where wind tunnel experimental data is also available a good agreement between cfd results and wind tunnel data is achieved which further confirms the reliability of this method based on the meshed hkpolyu campus model the sensitivity tests of three computational parameters namely turbulence model near wall mesh density and computational domain size are performed apart from the direct comparison of the cfd and wind tunnel results mape and rmse are utilized in this study to quantitatively assess the effects of the computational parameters the main findings of the sensitivity tests can be summarized as follows among the four rans models tested the rng turbulence model yields overall the best performance in predicting pedestrian level wind environment of the campus model while the ske turbulence model cannot provide adequate predicted results the near wall mesh generation technique can provide sufficient near wall mesh density for simulation requirement without leading to striking number of cells for the computational model that has a larger model width than building height the dbr method for choosing the domain cross section should be used to ensure the maximum blockage ratio below 3 in this study only the steady state rans turbulence models were used however the proposed mesh generation method can provide a sufficient near wall mesh density with moderate mesh number which therefore also allows the application of advanced transient turbulence models e g detached eddy simulation des model and large eddy simulation les model the time needed for constructing a complex grid is also considered very important during the pre processing stage thus further studies are still needed to compare the amounts of time used by the proposed method and other meshing methods acknowledgement the work described in this paper was fully supported by a grant from the research grants council of the hong kong special administrative region china project no c5002 14g 
