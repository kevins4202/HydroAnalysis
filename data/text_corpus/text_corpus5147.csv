index,text
25735,composite indicators cis a k a indices are increasingly used as they can simplify interpretation of results by condensing the information of a plurality of underlying indicators in a single measure this paper demonstrates that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci a measure of information transfer from each indicator is proposed along with two weight optimization methods which allow the weights to be adjusted to achieve either a targeted or maximized information transfer the tools presented in this paper are applied to a case study for resilience assessment of energy systems demonstrating how they can support the tailored development of cis these findings enable analysts bridging the statistical properties of the index with the weighting preferences from the stakeholders they can thus choose a weighting scheme and possibly modify the index while achieving a more consistent by correlation index keywords composite indicators index weights optimization resilience security of electricity supply sensitivity analysis 1 introduction composite indicators cis also called indices 2 2 composite indicator ci and index are used interchangeably throughout the paper are widely used synthetic measures for ranking and benchmarking alternatives across complex concepts saisana and tarantola 2002 nardo et al 2008 a recent review by greco et al 2019 identifies an almost exponential growth of cis over the past 20 years highlighting their popularity in all domains that require aggregation of information for decision making a ci is the result of a mathematical combination of individual indicators that together act as a proxy of the phenomena being measured mazziotta and pareto 2013 by combining a plurality of variables cis are able to assess and evaluate the performance of alternatives across multidimensional concepts which are not directly measurable or clearly defined a broad range of studies can be found in the literature that address topics such as ecological and environmental quality reichert et al 2015 reale et al 2017 oţoiu and grădinaru 2018 sustainability rowley et al 2012 cinelli et al 2014 eurostat 2015 hirschberg and burgherr 2015 human development undp 2016 biggeri and mauro 2018 competitiveness world economic forum 2017 and quality of governance world bank 2020 thereby they represent flexible tools for supporting decision making when more than one criterion is being considered greco et al 2016 the purpose of constructing a ci is among other things to condense and summarise the information contained in a number of underlying indicators in a way that accurately reflects the underlying concept there are two key notions here first condensing information and second accurately representing the underlying concept these two ideas will be revisited repeatedly in this work the rankings provided by a ci represent an invaluable tool for conveying complex and sometimes elusive phenomena to a larger audience freudenberg 2003 because it is easier to interpret a single figure than finding a common trend amongst a multitude of indicators singh et al 2009 paruolo et al 2013 furthermore developers are often keen to stress that composite measures are complementary to the underlying indicators and serve as a structured access point to a complex set of data becker et al 2018 however developing a ci is far from trivial involving a number of steps where the developer is obliged to make compromises and subjective choices booysen 2002 mazziotta and pareto 2013 cinelli et al 2020 hence the complementary nature of a ci is largely contingent on its underlying construction scheme an important but often overlooked aspect in the construction of cis is the correlation structure between the underlying indicators and its effect on the overall score i e the ci ideally there should be positive correlations between the indicators as this indicates that individual variables are linked to an overarching concept meyers et al 2013 negative or weak statistical relationships can have implications for the meaningfulness of the ci as some of these might represent features different from the overarching target concept being measured furr 2011 it must however be noted that according to the area of application and scope of the analysis there can be indicators that are not necessarily positively correlated and their inclusion might be driven by stakeholders choices it is anyhow important to assess the statistical properties of cis to judge their scoring and aid its interpretation nardo et al 2008 an example of this can be found in the sustainable society index where aggregation was avoided due to negative correlations between sub dimensions saisana and philippas 2012 complex systems modelling and analysis is driven by indicators that in the majority of the cases are interwoven and interdependent allen et al 2017 information theory has been proposed as a prime solution to study and quantify such dependencies between indicators prokopenko et al 2009 dependencies mean that the information provided by one indicator can be partially or fully inferred from another one according to the structure of the system under consideration each indicator carries a certain level of information about its functioning and behaviour consequently several measures have been advanced to study how much new information each indicator can add to characterize the system such as the marginal utility of information allen et al 2017 this type of measure can be characterized as carrying a variable weight or relevance in the description of the system since the higher the utility of the information carried by one indicator the higher its influence even if there is a wide body of literature that demonstrates the need to account for dependencies and overlaps between indicators csiszár and shields 2004 prokopenko et al 2009 allen et al 2017 mao et al 2019 davoudabadi et al 2020 cis are often developed with limited attention to such interrelationships cinelli et al 2020 in turn this can have a nontrivial influence on subsequent stages of construction such as the weighting and aggregation of indicators paruolo et al 2013 becker et al 2017 davoudabadi et al 2020 as discussed below recalling the objectives of constructing a ci one key point is that the index should accurately reflect the underlying concept this requires that each indicator contributes in a way that agrees with the decision maker s views on its importance to the concept in ci aggregation weights are assigned to reflect the trade offs 3 3 algorithms used in cis are frequently weighted sums and the weights of their indicators have the meaning of trade offs munda 2008b a these indicate the level of compensation between the indicators in other works they define the improvement required in the performance on one indicator to compensate for the worsening in performance of another indicator for example if the weight of indicator 1 is half the weight of indicator 2 it means that the improvement of two units on indicator 1 are needed to compensate the worsening of one unit on indicator 2 between the indicators based on stakeholders or decision makers preferences mazziotta and pareto 2017 greco et al 2019 consequently it is usually assumed that the weight assigned can be directly interpreted as a measure of an indicator s importance independent from the dataset under analysis munda and nardo 2005 however this assumption is rarely justified in fact in order to better understand the actual trade offs i e the influence that each indicator has on the ci of each indicator on the ci paruolo et al 2013 propose a methodology based on nonlinear regression it compares the assigned weights with an ex post measure of importance in this case karl pearson s correlation ratio also known as the first order sensitivity index which is a coefficient of nonlinear association it is found that the structure of the dataset and correlations between the indicators often have a decisive effect on each indicator s influence in the index in fact their influence rarely coincides with the assigned weights in a more recent study becker et al 2017 build on this research by extending the nonlinear regression approaches to include decomposing the correlation ratio to examine the correlated and uncorrelated contributions of each indicator drawing on global sensitivity analysis literature xu and gertner 2008 da veiga et al 2009 furthermore the authors introduce a weight optimization algorithm which optimises i e reallocates the weights with the aim of achieving the indicators pre specified values of trade offs the authors thus propose an approach to adjust the value of each indicator s weight in relation to their desired trade offs however adjusting indicator trade offs is not the only issue objective of ci aggregation as previously stated the other key aim of a ci is that it should be a good summary of its underlying indicators one way to interpret this goal is that it should maximize the amount of information transferred from the underlying indicators to the ci the two issues above adjusting indicators influence on the index and maximizing information transfer from the indicators are rarely considered in ci development and when they are researchers and practitioners tend to focus on either one or the other in isolation moreover work focusing on adjusting indicator influence misses a key point that they are effectively balancing the information transferred by each indicator in addition as recently discussed in a review on ci construction the weighting of indicators based on the statistical structure of the data has been widely criticized mostly because weights are assigned with these methods on the performance matrix and not using the preferences from the stakeholders i e stakeholder based weighting greco et al 2019 the available literature on ci development seems to neglect that the statistical properties of the dataset can be used to understand the actual contribution that each indicator is going to have on the index independently from the weights assigned by the stakeholders identification of weights of indicators by means of statistical analysis of the data can be labelled as data driven and it can be used to complement or even substitute the stakeholder based weighting whenever the latter is not available or it cannot be conducted with the relevant decision makers kojadinovic 2004 even if some approaches for combining stakeholder based and data driven methods to define the weights of the indicators have been proposed zardari et al 2015 davoudabadi et al 2020 there is not yet a framework to guide the use of both types of methods in weighting ci indicators our research fills this gap by showing that stakeholder based and data driven weighting methods can be successfully combined to achieve a well informed set of weights for the indicators of the ci more specifically our contribution consists in demonstrating how the desired weight of each indicator can be achieved by means of the statistical properties in the performance matrix this work brings together the two objectives of ci construction i reaching the desired indicator trade offs and ii maximizing information transfer under a single framework built on information theory it shows that the two objectives are depending on the correlation structure usually contradictory in the context of weighting cis developed with the aim of reaching the desired indicators trade offs may come at the cost of poor information transfer while the cis built via an information transfer maximization approach can potentially have a very unbalanced contribution from the underlying indicators hence there is a pragmatic need for developing a deeper understanding on how statistical dependencies between indicators in the dataset affect the indicators influence and information transfer in cis and thus their outcomes the first objective i e adjusting information transfer is important as it relates to the essence of shaping a ci that reflects the desired trade offs between the indicators in fact even if the dm desires equal trade offs between the indicators the correlation structure might not allow to reach it with equal weights as an example if the dm chooses that the weight of indicator 1 is the same as the weight of indicator 2 it conceptually means that the improvement of one unit on indicator 1 is needed to compensate the worsening of one unit on indicator 2 the conventional approach in ci construction is that the analyst then assigns equal weights to the indicators however our statistical tools that study the nonlinear dependence between each indicator and the index show that due to the correlations in the dataset in order to achieve the same weights i e equal trade offs the actual values of the weights for these indicators should for example be twice as high for indicator 1 when compared to indicator 2 this confirms the need for considering both the requirements from the dm e g the desired trade offs and the statistical properties of the performance matrix the second objective i e maximizing information transfer is important as it accounts for a situation where the dm requests as much information transfer as possible irrespective of a pre defined value for the trade offs on the indicators in this situation the trade offs between the indicators are defined solely according to the maximization of information transfer this paper provides a number of contributions to address these issues in section 2 the concept of information transfer from indicators to the ci is formalised by showing that the correlation ratio has a theoretical link with the concept of mutual information a measure from information theory under certain conditions this formally demonstrates that the correlation ratio can be used as a tool to achieve both the objective of adjusting indicators influence e g balancing information contributions and maximizing information transfer by using an optimization approach with different objective functions in section 3 the relationship between information transfer and the underlying correlation structure of cis is explored with an analytical example and it is shown that information transfer tends to a limit as more indicators are added to the framework then in section 4 the tools proposed in this paper are applied to one version of the electricity supply resilience index esri developed at the singapore eth centre gasser et al 2020 which was called resilience index for analysis and optimization rifao discussion and conclusions complete the paper in section 5 2 the concept of information transfer this section proposes the use of the correlation ratio as a measure of the information transferred from each indicator to the ci its rationale is driven by the fact that the statistical relationships between the indicators in the dataset have an effect on how influential each indicator is in the overall system allen et al 2017 which in this case is represented by the index the correlation ratio has been used in previous studies for adjusting the weights of cis paruolo et al 2013 becker et al 2017 here this idea is extended by linking it to the more intuitive concept of information transfer or shared mutual information and by introducing two different objectives in weight adjustment one based on balancing information transfer and the other based on maximizing it consider a ci y calculated as the additive weighted average or weighted sum which is one of the most widely used methods for developing cis oecd 2008 eisenfuhr et al 2010 bandura 2011 langhans et al 2014 of n normalized variables x i 1 y j i 1 n w i x j i j 1 2 m where x j i is the normalized score of alternative j e g country based on its raw value x j i in the i th variable x i i 1 2 n and w i is the weight i e trade off assigned to the i th variable such that i 1 n w i 1 and w i 0 fig 1 illustrates this aggregation procedure now after the aggregation the objective is to understand the relationships between each indicator x i and the aggregated ci y and to see how it can be improved in terms of the two objectives mentioned above in this work the proposal is to measure the amount of information that is shared between the individual indicators and the ci or the information transferred from each indicator to the ci see again fig 1 although equation 1 looks simple correlations between indicators mean that the information transferred between y and x i is not trivial to understand and any of the three information transfer scenarios shown in fig 1 can occur even with equal weighting the information transfer measure can be used as the basis for both the previously mentioned objectives of ci aggregation i adjusting the influence of each indicator in relation to its assigned weight and ii maximizing the information transferred from the set of indicators to the ci information transfer is a more natural framework for assessing cis than speaking directly in terms of correlations because cis are effectively an information compaction problem representing many indicators with one aggregated variable in any case this work will demonstrate that the two concepts are very similar and sometimes coincident building upon this logic the concept of information transfer will in this paper be defined as the co dependence between the ci and each of its underlying indicators this could also be looked at as the information shared between the ci and each indicator however since the ci is a product created by aggregating indicators the term transfer will be used in the following sections a measure of information transfer will be described and two optimization problems which satisfy the above mentioned objectives will be formulated 2 1 sensitivity index s i as a measure of information transfer one measure of information transfer is mutual information i which is an information theory measure that can be defined via entropy shannon 1948 entropy is the foundational concept of information theory which uses probability distributions to quantify the amount of information contained in a random variable cover and thomas 2005 it can be used to measure the capacity of each variable to be used to predict the behaviour of the system in the next destination state as well as to define the statistical complexity of a system prokopenko et al 2009 with respect to the latter use it is defined as shannon s entropy and it defines the minimum amount of information required to statistically characterize the system i can be understood as the amount of information that is shared between two random variables the i between two continuous random variables i y x i such as the ci y and one of its underlying indicators x i can be defined by 2 i y x i f y x i log f y x i f y f x i d y d x where f y and f x i are the marginal probability distributions and f y x i is the joint probability distribution clearly i allows us to directly measure a fundamental issue in composite indicators how much information is passed from each indicator x i to the ci y an intuitive way to think of information transfer in composite indicators is to consider given the ranks of y how well can one infer the ranks of the underlying indicators in other words how well is each indicator represented in the final index ranking if the mutual information between y and x i is high the ranks of x i are very similar to those of y therefore it can be considered as well represented in the opposite case low mutual information the two ranks will differ markedly clearly this is an important issue because a ci aims to summarise the information in its underlying indicators although i is widely recognized within data analysis to possess ideal properties for measuring stochastic dependence accounting for both linear and nonlinear dependencies it has some drawbacks smith 2015 first its interpretation is not straightforward unlike the well known pearson correlation coefficient ρ which has an absolute value in the range of 0 complete linear independence and 1 complete linear dependence the range of i is more open ended and can take on any value between 0 complete independence and infinity complete dependence second i is difficult to calculate from empirical data as it is based on probabilities and requires knowledge of the underlying marginal and joint distributions one way to alleviate these issues is to use a regression approach which is simpler to estimate since the joint and marginal distributions do not need to be known kullback 1959 in fact under restricted conditions it is possible to derive a direct link between i and coefficient of linear determination r 2 kullback 1959 when the joint probability distribution of both x i y are normal the expression for i in equation 2 reduces to 3 i y x i 1 2 ln 1 r i 2 where r i is the correlation between y and x i thus in the case of the multivariate gaussian probability distribution i between x i and y can be fully represented by the coefficient of linear determination r i 2 this is true because the dependence between two marginal distributions of a multivariate gaussian distribution is by definition linear hence the linear regression model is sufficient to capture the overall dependence dionisio et al 2004 in the nonlinear case r i 2 may still be used to approximate i but becomes less accurate as associations start becoming nonlinear song et al 2012 smith 2015 to approximate i for more nonlinear cases the proposal here is to use the correlation ratio s i originally denoted η i 2 pearson 1905 this is a coefficient of nonlinear association which can be estimated by a nonlinear regression model see e g paruolo et al 2013 or becker et al 2017 although this cannot be analytically linked to i it is a direct nonlinear extension of r i 2 in this respect it should logically provide a good nonlinear approximation of i indeed i has been shown to be directly related to the correlation ratio through csiszár f divergences da veiga 2015 the correlation ratio also known as the first order sensitivity index is a statistical measure of global variance based sensitivity saltelli et al 2008 it is defined as 4 s i η i 2 v x i e x i y x i v y where v y is the unconditional variance of y obtained when all factors x i are allowed to vary and v x i is the variance of x i as a function of the expected value e x i y x i for y given x i the expected value is the mean of y when only x i is fixed emphasised by the term x i which is the vector containing all the variables x 1 x n except variable x i thus e x i y x i is conditional on x i and is for that reason also referred to as the main effect of x i notice that this definition the ratio of the variance explained by x i to the unconditional variance is precisely a nonlinear generalisation of the well known coefficient of determination r i 2 such that s i equals r i 2 when the regression fit is linear wooldridge 2010 in fact much like r i 2 s i can be interpreted as the expected reduction of variance in the ci scores if a given indicator could be fixed saisana and saltelli 2011 paruolo et al 2013 s i is also bounded within the range of 0 1 determining the degree of dependence between the ci and its underlying indicators for instance a value of 1 indicates complete dependence and a value of 0 implies complete independence in information terms a value of 1 means that all of the information contained in an indicator x i has been transferred to the ci y whereas a value of 0 implies that none of its information has been transferred s i is therefore a useful proxy of mutual information in more general nonlinear cases to estimate s i a regression approach is used since the main effect e x i y x i is a univariate function of x i it can be obtained by a nonlinear regression of y against x i in this study a penalized cubic spline regression approach is used along the lines of becker et al 2017 to then obtain the first order sensitivity index s i the variance of the resulting curve is taken and standardised by the unconditional variance of y indeed a comparative study by song et al 2012 showed that i can safely be replaced by a nonlinear regression model based on splines as it matches i for detecting nonlinear relationships the concept of entropy used in this study is an extension of the one presented in the work from hwang and yoon 1981 while these authors directly estimated the weights using the entropy method in our study we make use of the results of the entropy method as input for the optimization models presented below in fact we defined the results of the entropy method as influence or si whose difference with respect to the initial weights i e equal weights in our study needs to be minimized using the optimization models 2 2 adjusting the weights to optimize information transfer given the information transfer measure proposed in the previous section how can a ci be modified to either i adjust the relative information contribution of each indicator according to the desired trade offs by the dm or ii maximize the overall information transfer as hinted in the introduction these objectives are often contradictory moreover it is assumed that the input data for the indicators i e normalized set cannot be altered and the aggregation method e g arithmetic or geometric mean is kept constant in this case the adjustments can be made by altering the weights however it is far from obvious which weight values will lead to the best properties in terms of objectives i and ii the solution is found by framing the issue as a computational optimization problem the first step is to build an objective function which for any given weight values calculates a score representing either i how adjusted the mean information transferred is or ii how much information is overall transferred to the composite index by calculating correlation ratio s i values for each indicator the best set of weights are then found by an iterative optimization search algorithm in this case the nelder mead simplex search method lagarias et al 1998 mckinnon 1998 which tries to find the highest value of the objective function the two objective functions for i and ii are described in detail in the following sections 2 2 1 objective i adjusting information transfer adjusting the relative information transfer i e the influence from the indicators to the ci in relation to their assigned weight is achieved in two steps see details in becker et al 2017 first to render the correlation ratios comparable to the weights a normalization step is needed 5 s i s i i 1 n s i where s i is the normalized correlation ratio of x i and i 1 n s i 1 this allows the normalized correlation ratios to be directly compared to their target the weights w i since the w i also sum to 1 second the problem of adjusting the contribution of the indicators can be formulated by defining an objective function as the sum of squared differences between the s i at a given set of weights and the target s i accordingly 6 w o p t a r g m i n w i 1 n s i s i w 2 where w w i i 1 n and w o p t 0 here it is assumed that the initially assigned weights represent the relative information transfer that is desired from each indicator i e s i w i hence the optimization problem in equation 6 tries to find a set of weights that minimises the discrepancy between the normalized correlation ratios s i and the initially assigned weights w i from the perspective of information transfer this equates to adjust the relative information transfer of each indicator in relation to the assigned weights by the dm 2 2 2 objective ii maximizing information transfer mathematically this problem is formulated by defining an objective function as the difference between a vector of all ones 1 i e the maximum information transfer s i 1 and the s i obtained at a given set of weights accordingly 7 w o p t a r g m i n w i 1 n 1 i s i w where the weights must sum to one w w i i 1 n and are constrained to be positive w o p t 0 by minimising this objective function the weights w o p t that maximize the total sum of information transferred from the indicators to the index can be found 3 relation between information transfer and average correlation this section gives an analytical exploration of ci aggregation it discusses how correlations between a set of indicators x i n influence the information that is transferred from those indicators to the ci y here r i 2 or linear s i captures the linear dependence between x i and y as shown in equation 3 consider the definition of r i 2 8 r i 2 corr 2 y x i cov 2 y x i var y var x i now assume a set of n variables with correlation matrix for this set of variables the weighted mean is explored such that y x w where x is the m n sample matrix w is the n 1 vector of weights and y is the vector of output values by letting e i be a n 1 vector where all elements are zero except the i th element which is set to one this linear combination gives johnson and wichern 2007 9 r i 2 w e i 2 w w e e using the expression in equation 9 to obtain r i 2 fig 2 shows its convergence as the number of indicators n changes from 2 to 100 for correlation matrices with average correlation coefficients ρ ranging from 0 to 1 with an interval of 0 1 it can be seen that r 2 y x i converges to ρ for large n with faster convergence the closer ρ is to 1 this convergence is also mathematically derived in appendix a in the electronic supplementary information esi where it is shown that for indicators with equal weights and equal variance r i 2 tends to the average correlation coefficient between indicators as n tends to infinity from this analysis it can be concluded that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci a linear combination of poorly correlated indicators will on average have a weaker dependence i e information transfer between the indicators and the ci than a linear combination of highly correlated indicators although here information transfer has been framed via r i 2 the fact that s i is a nonlinear generalisation of r i 2 allows these conclusions to be extended to the nonlinear case thus the average correlation coefficient ρ of a given correlation matrix can provide a useful rule of thumb on how the information transfer capacity of a ci will be affected when considering adding subtracting indicators to a framework this relationship will be further examined in the following section by applying the proposed measure to a case study 4 case study electricity supply resilience index the management of complex socio technical systems that are also embedded in environmental ones requires a dedicate array of tools to lead i the conception of their structure ii the identification of their key variables and functions iii the development of their underlying model and iv the assessment of their integrated performance as well as the effect of uncertainty in the input variables on the model output one of the premier concepts proposed to conduct integrated assessment and management of systems is the one of resilience it empowers analysts to consider technical biophysical and socio economic factors under one framework to support the understanding of the systems roostaie et al 2019 a main example of complex socio technical systems that requires a dedicated evaluation from a resilience perspective is the one of energy the pervasive nature of this type of systems is such that it encompasses multiple others including the biophysical ones at multiple scales fernandes torres et al 2019 in fact energy systems have direct and indirect implications on the environmental systems including water land and air given the importance of this topic the tools presented in section 2 are tested with one ci developed to assess energy systems resilience more specifically they are used with one ci out of the 38 that constitute the electricity supply resilience index esri a ci developed within the future resilient systems frs program at the singapore eth centre sec it is based on 12 indicators evaluating countries security of electricity supply from a resilience perspective gasser et al 2020 the targets of the evaluation are 140 countries that represent a wide spectrum of nations from all around the world esri uses data compiled from the international energy agency iea the international renewable energy agency irena paul scherrer institute s psi energy related severe accidents database ensad the world bank the swiss reinsurance company swiss re and the u s energy information administration eia the underlying data has been treated for outperformers identified with the interquartile range iqr method values are considered as outperformers if they lay outside 1 5 times the iqr from the first and third quartiles q1 and q3 respectively these were trimmed to the nearest value that is not an outperformer 4 4 note that the trimming is based on the actual data for the chosen 140 countries not the theoretical min and max values across the 12 indicators 88 values were identified as outperformers and trimmed to the nearest value within the iqr range after trimming missing values have been replaced by the average indicator values using an unconditional mean imputation 5 5 across the 12 indicators 65 instances of missing values were identified and replaced it must be noted that the use of the indicator mean can result in a decrease of the correlations as one of the common methods to deal with missing data nardo et al 2008 the final scoring and ranking of esri is obtained by 38 different combinations of normalization methods and aggregation functions gasser et al 2020 normalization methods are used to render the raw data comparable and suitable for aggregation in the cited study eight of these approaches were selected ordinal linear and non linear normalizations were chosen to account for the variability of approaches that can be selected by the analysts in ci development once the indicators are normalized they have to be aggregated to provide a final score and ranking gasser et al 2020 considered six aggregation functions in order to include different preferences of the decision maker in the form of compensation between the indicators the research in gasser et al 2020 is an extensive exploration of how different combinations of normalization methods and aggregation functions can affect the final score and ranking of the countries however the correlation analysis is limited to the assessment of the positive and negative trends between the indicators as well as the coherence of the set of indicators i e reliability of the scale as shown in this paper in section 2 correlation analysis can be used to do much more including the exploration of the correlations between the indicators by assessing the information transferred from each indicator to the ci and study the effect that different weighting schemes have on each of them consequently the tools proposed in section 2 are used in this case study to extend the understanding of the effect of the data structure on the weighting stage in the ci it must be noted that the ci resulting from the proposed weighting scheme is not more nor less valid compared to the esri proposed in gasser et al 2020 given that cis cannot be validated with objective measures as they model a concept that is not directly measurable the value of the research resides in refining the learning about the implications of different data structure on the influence that indicators have in cis in this paper the tools presented in section 2 are applied to one ci developed with the combination of one normalization method i e min max normalization and one aggregation function i e additive weighted sum to develop esri the reason for this choice is that these are among the most commonly used approaches in their respective discipline carrino 2017 el gibari et al 2019 greco et al 2019 so the results are of interest to a large audience of analysts and decision makers the index used in this paper and obtained with this combination of normalization method and aggregation function is called resilience index for analysis and optimization rifao the software called composite indicator analysis and optimization ciao lindén et al 2021 developed by some of the authors of this paper too was used to perform the statistical analysis appendix b in the esi provides more details on the framework and the indicators that constitute rifao while appendix c in the esi includes the raw and normalized dataset used to construct rifao it must be pointed out that no final scores of rifao are actually presented and discussed since the objective of this case study is not to focus on the rankings obtained with this index but rather to apply the optimization algorithms according to the objectives i and ii presented in section 2 2 to achieve the desired information transfer from each indicator to the ci furthermore appendix d in the esi presents the results of the same analysis by using the raw dataset i e the dataset without trimming the outperformers which shows that similar trends have been found as with the application of ciao tool with the rifao dataset with the trimmed outperformers the methodology used to develop rifao conduct the statistical analysis with the tools from section 2 and elaborate the resulting recommendations for weighting scenario choice and index revision is shown in fig 3 step 1 refers to the normalization of the dataset with the min max normalization in step 2 the correlations are analysed by means of pearson correlation coefficient ρ to study the interrelations between the indicators the normalized indicators are then aggregated with the additive weighted sum in step 3 step 4 studies the information transferred s i at equal weights and discusses the average correlation measured with respect to the step wise addition of indicators lastly step 5 provides recommendations for the choice of a weighting scheme according to a set of conditions that the dm might be interested to set for the index development this leads to three scenarios i e scenario a b c which represent different combinations of three main features of the problem i the variability of the information transferred s i from each indicator to the index ii the possible removal of one or more indicators from the index and iii the possible loss of mean information transfer s i m e a n each scenario is described in detail in section 4 2 and 4 3 step 1 in rifao development leads to the normalization of the dataset for indicators with a positive polarity meaning that the higher the value the better for the evaluation the chosen normalization method is given by the formula x j i min x i max x i min x i indicators with a negative polarity meaning that the lower the value the better for the evaluation are transformed via 1 x j i min x i max x i min x i where x j i is the raw country value in the i th indicator x i i 1 2 n this procedure results in a linear transformation of the data ranging from 0 min to 1 max and is performed on all indicators to render them comparable table 1 gives an overview of each of the 12 indicators that are included in the rifao framework and fig 4 shows the pearson correlation coefficients ρ between them step 2 in fig 3 for conciseness the indicators are labelled according to their id number e g ind 1 as defined in table 1 in all graphs and figures by examining the correlation structure of rifao it can be noticed that there is a large variation in the correlation strength between the indicators with values ranging from 0 44 to 0 94 although many indicators show a positive correlation between them the highest ρ 0 94 being between i n d 3 control of corruption and i n d 10 government effectiveness there are also a number of negative trends visible i n d 6 electricity import dependence showcases negative correlations with all the other indicators this finding shows that i n d 6 is mostly capturing a trend which is opposite to the other indicators in the dataset also a few non significant correlations 6 6 defined according to significance level p 0 05 can be seen four out of the eleven negative correlations displayed by i n d 6 are non significant i n d 7 equivalent availability factor except for a high positive correlation with i n d 2 severe accident risks presents non significant correlations all close to 0 this finding confirms how i n d 7 is mostly disconnected from the trends of the other indicators in the dataset these last two indicators proved to be of high interest in the subsequent stages of the analysis especially when discussing the possible re structuring of rifao 4 1 information transfer at equal weights as far as weighting is concerned equal weights are assigned to each indicator with the modelling assumption that the trade offs between each one included in the conceptual framework should be equal this section explores information transfer in rifao at equal weights and it is performed in two steps first the rifao indicators are aggregated with equal weights step 3 in fig 3 and an ex post assessment of information transfer is performed by estimating the correlation ratios via regression analysis between the indicators and the index step 4 in fig 3 the resulting regression fits are shown in fig 5 where both a linear r i 2 and nonlinear s i regression model are fitted to the data second the resulting correlation ratios s i are then normalized and assessed in comparison to the vector of equal weights this comparison is shown in table 2 from observing the resulting regression fits and the estimated r i 2 and s i values in fig 5 it can be noted that the indicators showing a linear trend towards the index e g i n d 3 control of corruption or i n d 4 political stability also have a low discrepancy between their r i 2 and s i measure in these cases linear estimates are sufficient to capture their dependence however there are also indicators that display nonlinear tendencies towards the index e g i n d 1 saidi or i n d 2 severe accident risks in these cases the linear regression model underestimates their dependence see e g i n d 2 which has an r i 2 of 0 48 but an s i of 0 66 this highlights the importance of also considering nonlinearities between the indicators and the ci when estimating dependence what is further evident from fig 5 is that not all indicators are transferring an equal amount of information hence they do not have the same influence on the index even though they are assigned equal weights thus they are not equally influential in representing countries across the concept measured by rifao the normalized correlation ratios s i in table 2 further showcase this discrepancy see deviation ratio column with values ranging from 64 overrepresentation i n d 10 to 77 underrepresentation i n d 7 by re examining the correlation matrix in fig 4 a connection between correlation strength and information transfer is evident the information in the highly correlated indicators e g i n d 3 8 10 12 tends to be overrepresented whereas the opposite holds true for the poorly non or negatively correlated indicators e g i n d 5 6 7 11 these findings are especially relevant in relation to the previously defined link between correlation and information transfer under restricted conditions see section 3 indeed even when distributions are not strictly linear an indicator s correlation with the other aggregated indicators provides a strong indication of its capacity to transfer information to the ci based on this statistical analysis it is possible to assign the indicators to three groups table 2 group 1 i n d 3 8 10 12 high correlations and s i and also high positive deviation ratios this characterises indicators that are overrepresented group 2 i n d 5 6 7 11 low correlations and s i and also the highest negative as well as absolute deviation ratios this characterises indicators that are underrepresented group 3 i n d 1 2 4 9 intermediate correlations and deviation ratios leading to moderate over or under representation the analytical analysis presented in section 3 was adapted to rifao to study the effect of each indicator on the average correlations of the index step 4 in fig 3 the results are presented in fig 6 showing how the average r i 2 s i and pearson correlation ρ perform when indicators are added incrementally one by one to develop rifao the measures show a common trend nonetheless it can be seen how notable divergence emerges between s i and pearson correlation ρ when i n d 6 and i n d 7 are added this analysis also shows that there is a significant drop off in information transfer when i n d 6 and i n d 7 are added to the framework which confirms that low correlated indicators result in low information transfer in addition to the findings in section 3 these results show that the average correlation can provide a useful albeit not perfect rule of thumb with respect to how much information on average is transferred from a set of indicators to the ci even for a smaller sample size and when distributions are not strictly linear 4 2 information transfer at optimized weights the variance based analysis of rifao shows that the information transfer from the indicators to the ci is not equal even though equal weights are applied and strongly driven by the correlation structure in addition the information transfer from each indicator to the ci is not maximized this section explores two avenues of weighting that a decision maker might be interested in case he she wants to achieve a balanced information transfer or a maximized one while the framework of indicators has to remain the same they are contextualized as two different scenarios scenario a and scenario b with different conditions that a dm might require to be met step 5 in fig 3 scenario a considers a dm who 1 does not want to have a widely unbalanced s i for each indicator 2 does not want to revise the indicators in the index 3 can accept a possible loss of s i m e a n this scenario results in rifao with 12 indicators where the main objective is to equally balance the information transfer from each indicator balance opt scenario b considers a dm who 1 accepts a possible wide s i variability for each indicator 2 does not want to revise the indicators in the index 3 aims to have as much as s i m e a n as possible this scenario results in rifao with 12 indicators where the main objective is to maximize the total information transferred from each indicator maximize opt the scenarios are modelled by optimizing the weights in line with the objective functions equations 6 and 7 respectively defined in section 2 the next sections describe the results of each scenario 4 2 1 scenario a equally balancing the information transfer from each indicator balance opt scenario a results in the most unbalanced set of weights as shown in fig 7 most notably the negatively correlated indicator i n d 6 electricity import dependence receives the highest weight 35 and also the non correlated indicator i n d 7 equivalent availability factor receives a substantial share of the weight 10 furthermore five indicators i n d 2 severe accident risk i n d 3 control of corruption i n d 8 gdp per capita i n d 10 government effectiveness and i n d 12 ease of doing business receive zero weight and two more i n d 1 saidi and i n d 9 insurance penetration obtain a weight close to zero i e 0 01 even though only five indicators receive a weight greater than 0 01 as shown by the correlation ratios in fig 8 the information contained within the zero weighted indicators is still captured by the ci simply through correlation judging from previous observations it can be assumed that these indicators excluding i n d 2 are sufficiently represented by the inclusion of i n d 4 with which they are all highly positively correlated see fig 4 the error bars in fig 8 representing the 5 95 percentiles show that the resulting weighting vector from the balance opt objective would achieve the most well balanced information transfer from each indicator ranging from s i m i n 0 14 to s i m a x 0 25 however the average contribution is relatively low s i m e a n 0 19 the correlation ratios in fig 8 show that only two indicators i n d 6 and i n d 7 measure an increased information transfer compared to the case of equal weights hence this weighting scheme does practically not improve the total information transfer but rather reduces the information transfer from the highly correlated indicators to target a balanced contribution in other words the balance opt weighting scheme focuses mostly on the indicators which are underrepresented i n d 5 6 7 11 see group 2 in table 2 at the cost of reduced mean information transfer s i m e a n 4 2 2 scenario b maximize the total information transferred from each indicator maximize opt this scenario results in a slightly less unbalanced set of weights than in scenario a see fig 7 in this setting the weights are mostly assigned to the highly correlated indicators e g i n d 4 political stability 12 i n d 8 gdp per capita 19 and i n d 10 government effectiveness 16 whereas the two non or negatively correlated indicators i n d 6 and i n d 7 receive zero weight interestingly the correlation ratios in fig 8 reveal that the information in these two indicators is albeit only slightly for i n d 7 still represented by the ci through correlation most notably i n d 6 shows an increased information transfer compared to the equal weights and balance opt weighting scenario even though it is receiving zero weight in line with its objective most indicators show an increased information transfer to the ci when the maximize opt weighting scheme is applied only three indicators i n d 1 i n d 7 and i n d 11 show a decline in relation to the equal weighting scenario when comparing the average correlation ratios fig 8 shows that this weighting vector does achieve the highest total information transfer s i m e a n 0 54 however the large error bars even higher than for the equal weights case suggest that it is unevenly distributed amongst the indicators ranging from s i m i n 0 04 to s i m a x 0 93 it can thus be concluded that the pursuit of maximizing total information transfer comes at the expense of certain poorly correlated indicators especially i n d 7 which are barely represented by the ci 4 3 revising the ci based on the information transfer analysis for both optimized i e balance and maximize opt weighting schemes in rifao with 12 indicators the poorly correlated indicators especially i n d 6 and i n d 7 revealed to be problematic from a perspective of information transfer when the balance opt weighting scheme is employed these indicators receive a substantial share of the weights the result is a balanced information transfer from the indicators to the ci but with a low total information transfer when the maximize opt weighting scheme is deployed these indicators receive low or zero weights this results in a high total information transfer but with a large discrepancy between the individual indicators a third scenario scenario c step 5 in fig 3 has thus been developed where the dm 1 wants to keep the s i variability in a narrow range 2 is willing to revise the indicators included in the index 3 does not want to have an excessive compared to equal weights and maximize weighting schemes loss of s i m e a n this is mainly performed for exploratory reasons the previous analysis shows that these indicators are not transferring much information to the index and their inclusion does not allow achieving a balanced information transfer from each indicator hence we explore if we can achieve this by omitting them from the ci a key drawback consequence of omitting low correlated indicators is that these can contain a high information content of that indicator dimension this information would then be lost however what we have shown is that this information is not really represented by the index in the first place so removing them will have a low effect on the index scores and resulting rankings this problem framing leads to what is called rifao 10 an index with 10 indicators where i n d 6 and i n d 7 are removed from the ci see above discussion and the balance optimization is used i e rifao with 10 indicators with balance opt the resulting weights and information transfer measures are shown in fig 9 and fig 10 respectively similarly to the case of rifao with 12 indicators fig 9 shows that the balance opt still results in an unbalanced set of weights even though i n d 6 and i n d 7 are removed the same five highly correlated indicators i n d 2 severe accident risk i n d 3 control of corruption i n d 8 gdp per capita i n d 10 government effectiveness and i n d 12 ease of doing business receive zero weight however the distribution of the remaining weights is not the same as for rifao with 12 indicators in the absence of i n d 6 and i n d 7 i n d 11 now receive the most substantial share of the weights followed by i n d 5 i n d 9 i n d 4 and i n d 1 in decreasing order again it is important to note that the information in the zero weighted indicators would still be captured by the ci simply through correlation by the inclusion of i n d 4 and i n d 9 this is shown by the resulting correlation ratios in fig 10 the key difference compared to the previous case of rifao 12 however is the magnitude of information transfer achieved at balance opt weights contrary to the case of 12 indicators it is now possible to achieve a rather well balanced information transfer ranging from s i m i n 0 41 and s i m a x 0 52 see fig 10 without reducing the total information transfer to the same extent s i m e a n 0 46 compared to s i m e a n 0 19 in the case of 12 indicators for comparative purposes fig 10 also includes the s i m e a n for the maximize opt for rifao with 10 indicators it can be seen that the discrepancy between the two s i m e a n is considerably reduced with respect to the case of the ci based on 12 indicators most importantly the wide variability in the s i m e a n shows that there is still a considerable imbalance of information transfer from each indicator in this rifao with maximize opt though the mean value is higher than in rifao 12 and the lower bound increases from about 0 1 to 0 2 whereas the upper bound remains at about 0 9 maintaining the s i variability in a narrow range was a binding condition to be met for scenario c and for this reason only the balance opt is considered as a viable option in the case of rifao with 10 indicators 5 discussion information transfer and correlations are intricately related in the construction of cis in this paper it was confirmed that correlations lead the indicators to transfer information differently and hence have a different influence impact on the ci as compared to the assigned weight in order to deal with this discrepancy between desired influence of indicators i e weights and their actual influence driven by correlations we provide tools that allow a deep dive into this complex interrelationship and study the information transfer in relation to both weights and correlations the main contributions of this research consist in 1 proposing a measure of information transfer based on correlations between the indicators along with two weight optimization methods the analyst can now adjust the weights to achieve either a targeted or maximized information transfer from a set of indicators 2 showing that while targeting indicator contributions is important it is also relevant to consider the overall information conveyed by the index thereby introducing the second optimization objective maximizing information transfer 3 showing how the number of indicators and the average correlation can inform the analyst about the overall information transfer more specifically we demonstrate the convergence of information transfer towards the average correlation coefficient the resulting analysis indicates that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci in fact correlations can be a good rule of thumb of how information transfer from a set of indicators will behave in the aggregation of a ci 4 applying these tools to a case study on electricity supply resilience assessment regarding the case study we apply the proposed tools to one version of the electricity supply resilience index esri developed at the singapore eth centre which was called resilience index for analysis and optimization rifao the resulting analysis shows that correlations between rifao s underlying indicators have a direct influence on the index preventing the equal weights assigned to correspond to an equal information transfer from each indicator different weighting schemes and index revision scenarios are also proposed according to specific requests that the dm might have with respect to possible loss and balance of information transfer as well as indicators inclusion in the index when the weighting scheme used to distribute influence equally between indicators i e balance opt is employed highly correlated indicators are poorly weighted and less correlated indicators receive a substantial share of the weights the outcome is a balanced but low information transfer from the indicators to the ci when the weighting scheme proposed to maximize the information transfer from the indicators i e maximize opt is applied it is instead the less correlated indicators that are poorly weighted in favour of the more highly correlated indicators the result is a high total information transfer but with a large discrepancy between the individual indicators however when the two poorly correlated indicators are removed from rifao the results indicate a less evident trade off between the two weighting schemes with comparable average information transfer though well balanced with the balance opt scenario compared to the maximize opt scenario thus if there is a large inconsistency variation in correlation strength between the indicators it is probable that there will be an unbalanced information transfer from each indicator even though equal weights are applied this phenomenon is not possible to counterbalance by adjusting the weights without compromising the information transferred to the ci and its overall capacity to convey a representation of its underlying components the indicators our research also contributes to an ongoing debate on the inclusion of positively and or negatively correlated indicators in cis on the one hand there are authors like marttunen et al 2019 who advocate for the inclusion of not or negatively correlated indicators as they can be more informative for a decision since they bring unique perspectives on the aspects under evaluation on the other hand there are other authors like munda et al 2020 who warn about the risk of including indicators with low or negative correlations as their information might not be represented in the ci our research advocates for a balanced reasoning between these perspectives as follows when correlation exists between indicators it means that information is shared between the two indicators to take extreme cases if nonlinear correlation is zero that means that there is no shared information and the two indicators are bringing completely unique information contributions if correlation is one the indicators are collinear and encode effectively the same information clearly the second case is not useful because it implies double counting 7 7 this reasoning applies to a decision making problem with a flat structure for the indicators it would nonetheless be possible to keep the same indicator in two different dimensions if there would a hierarchy of indicators where the same indicator is present in more than one dimension in this case it would be possible keep the same indicator twice and use for example value functions to transform normalize the data so that e g value x of indicator a in dimension 1 means a 0 2 while the same value x of indicator 1 in dimension 2 means a 0 4 assuming the transformation is between 0 and 1 with an increasing order of preference however the first case comes with some pros and cons on the one hand as pointed out by marttunen et al 2019 zero correlation between indicators means that there is no overlap and that can be seen as a good but this comes at a cost as we show in fig 2 since if one combines several indicators with zero correlation this will result in a ci that contains relatively little information from any one of the indicators therefore in our opinion if a concept can be summarised by some very few indicators with low correlations this can still be acceptable as it is still possible to have a moderate information transfer however as fig 2 shows above 10 indicators with an average correlation coefficient of zero r 2 is less than 0 1 between indicators and index which contrasts with the fundamental objective of ci development itself being the condensation of information of many indicators into one consequently we recommend that when only looking at the correlations if they are low only a few indicators should be aggregated together but if they are high more indicators can be aggregated however the whole development of the ci should in the ideal case be embedded in a stakeholder consultation process i e decisions on indicators will not just be driven by correlations but influenced by the priorities of the stakeholders additionally potential interactions between the indicators might also be included in the development of the ci which are not necessarily equal to correlations the authors also think that it is relevant to separate two different concepts information transfer and information content it is true that a low correlated indicator can imply a high information content of that indicator dimension however what we show is that because of its low correlation with the other indicators it will not transfer much of that information to the index i e the index will not contain much of the information of that indicator dimension hence a low correlated indicator will have a low information transfer to the index but can still by itself have a high information content of that specific indicator dimension this research also comes with a number of limitations that are presented below together with options for future research to tackle them this study has not considered the effects of changing aggregation methods and input data which can be considered as one of the inherent uncertainties in composite indicators in order to understand the effects of changing input data and aggregation method one would have to perform an uncertainty analysis e g a monte carlo sampling along the lines of saisana et al 2005 what we propose in this research is not to investigate the uncertainties in weights but more to calibrate them to a desired objective i e target or maximize information transfer any uncertainty analysis is thus an avenue for future research the same reasoning applies to the assessment of the effect that each source of uncertainty can have on the index variance a possible option is this respect would be fuzzy mcda methods kaya et al 2019 the application of the ciao tool to the case study is based on the fully compensatory additive weighted sum which means that its results are meaningful only for this type of aggregation function however the ciao tool can be used with aggregation functions that have lower compensation levels than the additive weighted sum such as the geometric and harmonic ones like the additive weighted sum also the geometric and harmonic weighted sums are already included in the ciao tool and they can surely be a very interesting opportunity for future testing of our tool there are however aggregation functions which would not be suitable for the ciao tool like extreme aggregation operators such as the minimum and maximum operators the reason is that since only one indicator would determine the final score the worst with the minimum and the best with the maximum operators there would be no optimization of weights to be performed as only one indicator would be defining the overall performance this research has not accounted for a dm who is willing to accept a compromise between the two objectives proposed for the weight optimization this is because the goal of our research is to offer the users the ciao tool to exactly achieve the desired target behind each optimization objective in case the dm would like a compromise between these two objectives the option of applying a multi objective model could be explored finally the s i measure proposed in this research has been developed for a decision making challenge with a flat structure of the indicators meaning that there is only one level between the constructed concept and the variables used to measure it with the ci it can however be noted that for a hierarchical index with multiple pillars and based on an additive weighted average it would be also possible to calculate the effective weight of each indicator in the index by multiplying the indicator weight by its pillar weight or by optimizing one level at a time the statistical analysis presented in this paper surely adds a layer of complexity for the well informed development of composite indicators the weighting of the indicators in fact results as a combination of data driven i e statistical and stakeholder based i e value choices of the dm input which might be difficult to communicate especially if the index is developed for advocacy purposes nonetheless these types of advanced statistical analyses can be used to assess and enhance the robustness of the models that are developed ultimately leading to more sound decision making this is in line with the recent call for such type of research as presented for example by moallemi et al 2020 and saltelli et al 2019 6 conclusions the tools introduced in this study allow developers of cis to explore in detail the effect of weighting choices in an easily interpretable framework based on the concept of information transfer for the first time this work has shown that trying to balance the contributions of indicators may often come at the expense of reducing the overall information transferred from each indicator to the index most likely developers will wish to find a compromise point between balancing and maximizing information transfer and the optimization algorithms here give the means to assign selected weights in the perspective of these two competing criteria as demonstrated with the rifao case study this can sometimes be achieved by re structuring the index this research also relates to an existing discussion on the use of supervised dm driven and unsupervised machine driven methods for studying and defining the complexities and interdependencies of a certain decision problem when the complexity is such that the required knowledge cannot be easily given or the decision maker is not knowledgeable enough the unsupervised method can be useful in at least providing an initial mapping of the decision problem kojadinovic 2008 consequently unsupervised methods are not to be seen as competitors to the methods that employ active interaction with the decision makers to define these dependencies and the resulting weights rather they should be viewed as aiding tools to navigate the difficulties embedded in shaping the understanding of complex systems evaluated by means of multiple criteria furthermore it is important to note that the users of the tools proposed in this research are envisioned to be analysts with a mathematical background in statistical analysis and development of ci a key distinctive feature of this type of users is their desire of providing a bridge between two scientific communities on the one side data analysis without stakeholders involvement and on the other side decision aiding based on inclusion of stakeholders preferences the users can in fact use the tools provided by this research to achieve the desired contributions of the underlying indicators in the ci the tools proposed here are intended to provide goalposts between which developers can pick a desired target and are not meant to supersede the conceptual relevance of the indicators communication issues and methodological choices in other stages of the ci construction which are other highly relevant factors more specifically the dm can define the conditions for the index development with respect to i the possible loss of mean information transfer ii the possible variability range of the information transferred from each indicator to the index and iii the willingness to discuss the possible removal of one or more indicators from the index once these conditions are defined the weighting scheme can be obtained with the proposed tools and their results discussed among the stakeholders to decide how to proceed in the development of the index finally the current findings should not be simply generalized and applied but the wider applicability of the proposed tools requires further testing with different datasets with a varying number of indicators and alternatives and with further normalization and aggregation functions the difference of this research with respect to other sensitivity analyses is that the proposed framework does not aim to study the variability of the results according to the choices involved in its construction such as the selection of the indicators the normalization methods or the aggregation algorithms saltelli et al 2019 douglas smith et al 2020 zhang et al 2020 it instead focuses on the effect of the correlation structure on the influence that each indicator has in the ci when foreseeing a link with the other uses of sensitivity analyses the proposed framework could also be applied to different conceptualizations of the ci to study how the recommended weighting would change based on e g different normalization methods and or aggregation functions 7 software the calculations for the case study on electricity supply resilience were performed with the software composite indicator analysis and optimization ciao lindén et al 2021 which was specifically developed for this research and it is now freely available at the link https bitbucket org ensadpsi ciao tool src master declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research was conducted at the future resilient systems frs at the singapore eth centre sec which was established collaboratively between eth zürich and singapore s national research foundation fi 370074011 under its campus for research excellence and technological enterprise create program matteo spada and peter burgherr also received support from the swiss competence center for energy research sccer supply of electricity soe marco cinelli acknowledges that this project has received funding from the european union s horizon 2020 research and innovation program under the marie skłodowska curie grant agreement no 743553 the authors also thank paolo paruolo from the european commission s joint research centre for helpful input on analytical correlation analysis appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105208 
25735,composite indicators cis a k a indices are increasingly used as they can simplify interpretation of results by condensing the information of a plurality of underlying indicators in a single measure this paper demonstrates that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci a measure of information transfer from each indicator is proposed along with two weight optimization methods which allow the weights to be adjusted to achieve either a targeted or maximized information transfer the tools presented in this paper are applied to a case study for resilience assessment of energy systems demonstrating how they can support the tailored development of cis these findings enable analysts bridging the statistical properties of the index with the weighting preferences from the stakeholders they can thus choose a weighting scheme and possibly modify the index while achieving a more consistent by correlation index keywords composite indicators index weights optimization resilience security of electricity supply sensitivity analysis 1 introduction composite indicators cis also called indices 2 2 composite indicator ci and index are used interchangeably throughout the paper are widely used synthetic measures for ranking and benchmarking alternatives across complex concepts saisana and tarantola 2002 nardo et al 2008 a recent review by greco et al 2019 identifies an almost exponential growth of cis over the past 20 years highlighting their popularity in all domains that require aggregation of information for decision making a ci is the result of a mathematical combination of individual indicators that together act as a proxy of the phenomena being measured mazziotta and pareto 2013 by combining a plurality of variables cis are able to assess and evaluate the performance of alternatives across multidimensional concepts which are not directly measurable or clearly defined a broad range of studies can be found in the literature that address topics such as ecological and environmental quality reichert et al 2015 reale et al 2017 oţoiu and grădinaru 2018 sustainability rowley et al 2012 cinelli et al 2014 eurostat 2015 hirschberg and burgherr 2015 human development undp 2016 biggeri and mauro 2018 competitiveness world economic forum 2017 and quality of governance world bank 2020 thereby they represent flexible tools for supporting decision making when more than one criterion is being considered greco et al 2016 the purpose of constructing a ci is among other things to condense and summarise the information contained in a number of underlying indicators in a way that accurately reflects the underlying concept there are two key notions here first condensing information and second accurately representing the underlying concept these two ideas will be revisited repeatedly in this work the rankings provided by a ci represent an invaluable tool for conveying complex and sometimes elusive phenomena to a larger audience freudenberg 2003 because it is easier to interpret a single figure than finding a common trend amongst a multitude of indicators singh et al 2009 paruolo et al 2013 furthermore developers are often keen to stress that composite measures are complementary to the underlying indicators and serve as a structured access point to a complex set of data becker et al 2018 however developing a ci is far from trivial involving a number of steps where the developer is obliged to make compromises and subjective choices booysen 2002 mazziotta and pareto 2013 cinelli et al 2020 hence the complementary nature of a ci is largely contingent on its underlying construction scheme an important but often overlooked aspect in the construction of cis is the correlation structure between the underlying indicators and its effect on the overall score i e the ci ideally there should be positive correlations between the indicators as this indicates that individual variables are linked to an overarching concept meyers et al 2013 negative or weak statistical relationships can have implications for the meaningfulness of the ci as some of these might represent features different from the overarching target concept being measured furr 2011 it must however be noted that according to the area of application and scope of the analysis there can be indicators that are not necessarily positively correlated and their inclusion might be driven by stakeholders choices it is anyhow important to assess the statistical properties of cis to judge their scoring and aid its interpretation nardo et al 2008 an example of this can be found in the sustainable society index where aggregation was avoided due to negative correlations between sub dimensions saisana and philippas 2012 complex systems modelling and analysis is driven by indicators that in the majority of the cases are interwoven and interdependent allen et al 2017 information theory has been proposed as a prime solution to study and quantify such dependencies between indicators prokopenko et al 2009 dependencies mean that the information provided by one indicator can be partially or fully inferred from another one according to the structure of the system under consideration each indicator carries a certain level of information about its functioning and behaviour consequently several measures have been advanced to study how much new information each indicator can add to characterize the system such as the marginal utility of information allen et al 2017 this type of measure can be characterized as carrying a variable weight or relevance in the description of the system since the higher the utility of the information carried by one indicator the higher its influence even if there is a wide body of literature that demonstrates the need to account for dependencies and overlaps between indicators csiszár and shields 2004 prokopenko et al 2009 allen et al 2017 mao et al 2019 davoudabadi et al 2020 cis are often developed with limited attention to such interrelationships cinelli et al 2020 in turn this can have a nontrivial influence on subsequent stages of construction such as the weighting and aggregation of indicators paruolo et al 2013 becker et al 2017 davoudabadi et al 2020 as discussed below recalling the objectives of constructing a ci one key point is that the index should accurately reflect the underlying concept this requires that each indicator contributes in a way that agrees with the decision maker s views on its importance to the concept in ci aggregation weights are assigned to reflect the trade offs 3 3 algorithms used in cis are frequently weighted sums and the weights of their indicators have the meaning of trade offs munda 2008b a these indicate the level of compensation between the indicators in other works they define the improvement required in the performance on one indicator to compensate for the worsening in performance of another indicator for example if the weight of indicator 1 is half the weight of indicator 2 it means that the improvement of two units on indicator 1 are needed to compensate the worsening of one unit on indicator 2 between the indicators based on stakeholders or decision makers preferences mazziotta and pareto 2017 greco et al 2019 consequently it is usually assumed that the weight assigned can be directly interpreted as a measure of an indicator s importance independent from the dataset under analysis munda and nardo 2005 however this assumption is rarely justified in fact in order to better understand the actual trade offs i e the influence that each indicator has on the ci of each indicator on the ci paruolo et al 2013 propose a methodology based on nonlinear regression it compares the assigned weights with an ex post measure of importance in this case karl pearson s correlation ratio also known as the first order sensitivity index which is a coefficient of nonlinear association it is found that the structure of the dataset and correlations between the indicators often have a decisive effect on each indicator s influence in the index in fact their influence rarely coincides with the assigned weights in a more recent study becker et al 2017 build on this research by extending the nonlinear regression approaches to include decomposing the correlation ratio to examine the correlated and uncorrelated contributions of each indicator drawing on global sensitivity analysis literature xu and gertner 2008 da veiga et al 2009 furthermore the authors introduce a weight optimization algorithm which optimises i e reallocates the weights with the aim of achieving the indicators pre specified values of trade offs the authors thus propose an approach to adjust the value of each indicator s weight in relation to their desired trade offs however adjusting indicator trade offs is not the only issue objective of ci aggregation as previously stated the other key aim of a ci is that it should be a good summary of its underlying indicators one way to interpret this goal is that it should maximize the amount of information transferred from the underlying indicators to the ci the two issues above adjusting indicators influence on the index and maximizing information transfer from the indicators are rarely considered in ci development and when they are researchers and practitioners tend to focus on either one or the other in isolation moreover work focusing on adjusting indicator influence misses a key point that they are effectively balancing the information transferred by each indicator in addition as recently discussed in a review on ci construction the weighting of indicators based on the statistical structure of the data has been widely criticized mostly because weights are assigned with these methods on the performance matrix and not using the preferences from the stakeholders i e stakeholder based weighting greco et al 2019 the available literature on ci development seems to neglect that the statistical properties of the dataset can be used to understand the actual contribution that each indicator is going to have on the index independently from the weights assigned by the stakeholders identification of weights of indicators by means of statistical analysis of the data can be labelled as data driven and it can be used to complement or even substitute the stakeholder based weighting whenever the latter is not available or it cannot be conducted with the relevant decision makers kojadinovic 2004 even if some approaches for combining stakeholder based and data driven methods to define the weights of the indicators have been proposed zardari et al 2015 davoudabadi et al 2020 there is not yet a framework to guide the use of both types of methods in weighting ci indicators our research fills this gap by showing that stakeholder based and data driven weighting methods can be successfully combined to achieve a well informed set of weights for the indicators of the ci more specifically our contribution consists in demonstrating how the desired weight of each indicator can be achieved by means of the statistical properties in the performance matrix this work brings together the two objectives of ci construction i reaching the desired indicator trade offs and ii maximizing information transfer under a single framework built on information theory it shows that the two objectives are depending on the correlation structure usually contradictory in the context of weighting cis developed with the aim of reaching the desired indicators trade offs may come at the cost of poor information transfer while the cis built via an information transfer maximization approach can potentially have a very unbalanced contribution from the underlying indicators hence there is a pragmatic need for developing a deeper understanding on how statistical dependencies between indicators in the dataset affect the indicators influence and information transfer in cis and thus their outcomes the first objective i e adjusting information transfer is important as it relates to the essence of shaping a ci that reflects the desired trade offs between the indicators in fact even if the dm desires equal trade offs between the indicators the correlation structure might not allow to reach it with equal weights as an example if the dm chooses that the weight of indicator 1 is the same as the weight of indicator 2 it conceptually means that the improvement of one unit on indicator 1 is needed to compensate the worsening of one unit on indicator 2 the conventional approach in ci construction is that the analyst then assigns equal weights to the indicators however our statistical tools that study the nonlinear dependence between each indicator and the index show that due to the correlations in the dataset in order to achieve the same weights i e equal trade offs the actual values of the weights for these indicators should for example be twice as high for indicator 1 when compared to indicator 2 this confirms the need for considering both the requirements from the dm e g the desired trade offs and the statistical properties of the performance matrix the second objective i e maximizing information transfer is important as it accounts for a situation where the dm requests as much information transfer as possible irrespective of a pre defined value for the trade offs on the indicators in this situation the trade offs between the indicators are defined solely according to the maximization of information transfer this paper provides a number of contributions to address these issues in section 2 the concept of information transfer from indicators to the ci is formalised by showing that the correlation ratio has a theoretical link with the concept of mutual information a measure from information theory under certain conditions this formally demonstrates that the correlation ratio can be used as a tool to achieve both the objective of adjusting indicators influence e g balancing information contributions and maximizing information transfer by using an optimization approach with different objective functions in section 3 the relationship between information transfer and the underlying correlation structure of cis is explored with an analytical example and it is shown that information transfer tends to a limit as more indicators are added to the framework then in section 4 the tools proposed in this paper are applied to one version of the electricity supply resilience index esri developed at the singapore eth centre gasser et al 2020 which was called resilience index for analysis and optimization rifao discussion and conclusions complete the paper in section 5 2 the concept of information transfer this section proposes the use of the correlation ratio as a measure of the information transferred from each indicator to the ci its rationale is driven by the fact that the statistical relationships between the indicators in the dataset have an effect on how influential each indicator is in the overall system allen et al 2017 which in this case is represented by the index the correlation ratio has been used in previous studies for adjusting the weights of cis paruolo et al 2013 becker et al 2017 here this idea is extended by linking it to the more intuitive concept of information transfer or shared mutual information and by introducing two different objectives in weight adjustment one based on balancing information transfer and the other based on maximizing it consider a ci y calculated as the additive weighted average or weighted sum which is one of the most widely used methods for developing cis oecd 2008 eisenfuhr et al 2010 bandura 2011 langhans et al 2014 of n normalized variables x i 1 y j i 1 n w i x j i j 1 2 m where x j i is the normalized score of alternative j e g country based on its raw value x j i in the i th variable x i i 1 2 n and w i is the weight i e trade off assigned to the i th variable such that i 1 n w i 1 and w i 0 fig 1 illustrates this aggregation procedure now after the aggregation the objective is to understand the relationships between each indicator x i and the aggregated ci y and to see how it can be improved in terms of the two objectives mentioned above in this work the proposal is to measure the amount of information that is shared between the individual indicators and the ci or the information transferred from each indicator to the ci see again fig 1 although equation 1 looks simple correlations between indicators mean that the information transferred between y and x i is not trivial to understand and any of the three information transfer scenarios shown in fig 1 can occur even with equal weighting the information transfer measure can be used as the basis for both the previously mentioned objectives of ci aggregation i adjusting the influence of each indicator in relation to its assigned weight and ii maximizing the information transferred from the set of indicators to the ci information transfer is a more natural framework for assessing cis than speaking directly in terms of correlations because cis are effectively an information compaction problem representing many indicators with one aggregated variable in any case this work will demonstrate that the two concepts are very similar and sometimes coincident building upon this logic the concept of information transfer will in this paper be defined as the co dependence between the ci and each of its underlying indicators this could also be looked at as the information shared between the ci and each indicator however since the ci is a product created by aggregating indicators the term transfer will be used in the following sections a measure of information transfer will be described and two optimization problems which satisfy the above mentioned objectives will be formulated 2 1 sensitivity index s i as a measure of information transfer one measure of information transfer is mutual information i which is an information theory measure that can be defined via entropy shannon 1948 entropy is the foundational concept of information theory which uses probability distributions to quantify the amount of information contained in a random variable cover and thomas 2005 it can be used to measure the capacity of each variable to be used to predict the behaviour of the system in the next destination state as well as to define the statistical complexity of a system prokopenko et al 2009 with respect to the latter use it is defined as shannon s entropy and it defines the minimum amount of information required to statistically characterize the system i can be understood as the amount of information that is shared between two random variables the i between two continuous random variables i y x i such as the ci y and one of its underlying indicators x i can be defined by 2 i y x i f y x i log f y x i f y f x i d y d x where f y and f x i are the marginal probability distributions and f y x i is the joint probability distribution clearly i allows us to directly measure a fundamental issue in composite indicators how much information is passed from each indicator x i to the ci y an intuitive way to think of information transfer in composite indicators is to consider given the ranks of y how well can one infer the ranks of the underlying indicators in other words how well is each indicator represented in the final index ranking if the mutual information between y and x i is high the ranks of x i are very similar to those of y therefore it can be considered as well represented in the opposite case low mutual information the two ranks will differ markedly clearly this is an important issue because a ci aims to summarise the information in its underlying indicators although i is widely recognized within data analysis to possess ideal properties for measuring stochastic dependence accounting for both linear and nonlinear dependencies it has some drawbacks smith 2015 first its interpretation is not straightforward unlike the well known pearson correlation coefficient ρ which has an absolute value in the range of 0 complete linear independence and 1 complete linear dependence the range of i is more open ended and can take on any value between 0 complete independence and infinity complete dependence second i is difficult to calculate from empirical data as it is based on probabilities and requires knowledge of the underlying marginal and joint distributions one way to alleviate these issues is to use a regression approach which is simpler to estimate since the joint and marginal distributions do not need to be known kullback 1959 in fact under restricted conditions it is possible to derive a direct link between i and coefficient of linear determination r 2 kullback 1959 when the joint probability distribution of both x i y are normal the expression for i in equation 2 reduces to 3 i y x i 1 2 ln 1 r i 2 where r i is the correlation between y and x i thus in the case of the multivariate gaussian probability distribution i between x i and y can be fully represented by the coefficient of linear determination r i 2 this is true because the dependence between two marginal distributions of a multivariate gaussian distribution is by definition linear hence the linear regression model is sufficient to capture the overall dependence dionisio et al 2004 in the nonlinear case r i 2 may still be used to approximate i but becomes less accurate as associations start becoming nonlinear song et al 2012 smith 2015 to approximate i for more nonlinear cases the proposal here is to use the correlation ratio s i originally denoted η i 2 pearson 1905 this is a coefficient of nonlinear association which can be estimated by a nonlinear regression model see e g paruolo et al 2013 or becker et al 2017 although this cannot be analytically linked to i it is a direct nonlinear extension of r i 2 in this respect it should logically provide a good nonlinear approximation of i indeed i has been shown to be directly related to the correlation ratio through csiszár f divergences da veiga 2015 the correlation ratio also known as the first order sensitivity index is a statistical measure of global variance based sensitivity saltelli et al 2008 it is defined as 4 s i η i 2 v x i e x i y x i v y where v y is the unconditional variance of y obtained when all factors x i are allowed to vary and v x i is the variance of x i as a function of the expected value e x i y x i for y given x i the expected value is the mean of y when only x i is fixed emphasised by the term x i which is the vector containing all the variables x 1 x n except variable x i thus e x i y x i is conditional on x i and is for that reason also referred to as the main effect of x i notice that this definition the ratio of the variance explained by x i to the unconditional variance is precisely a nonlinear generalisation of the well known coefficient of determination r i 2 such that s i equals r i 2 when the regression fit is linear wooldridge 2010 in fact much like r i 2 s i can be interpreted as the expected reduction of variance in the ci scores if a given indicator could be fixed saisana and saltelli 2011 paruolo et al 2013 s i is also bounded within the range of 0 1 determining the degree of dependence between the ci and its underlying indicators for instance a value of 1 indicates complete dependence and a value of 0 implies complete independence in information terms a value of 1 means that all of the information contained in an indicator x i has been transferred to the ci y whereas a value of 0 implies that none of its information has been transferred s i is therefore a useful proxy of mutual information in more general nonlinear cases to estimate s i a regression approach is used since the main effect e x i y x i is a univariate function of x i it can be obtained by a nonlinear regression of y against x i in this study a penalized cubic spline regression approach is used along the lines of becker et al 2017 to then obtain the first order sensitivity index s i the variance of the resulting curve is taken and standardised by the unconditional variance of y indeed a comparative study by song et al 2012 showed that i can safely be replaced by a nonlinear regression model based on splines as it matches i for detecting nonlinear relationships the concept of entropy used in this study is an extension of the one presented in the work from hwang and yoon 1981 while these authors directly estimated the weights using the entropy method in our study we make use of the results of the entropy method as input for the optimization models presented below in fact we defined the results of the entropy method as influence or si whose difference with respect to the initial weights i e equal weights in our study needs to be minimized using the optimization models 2 2 adjusting the weights to optimize information transfer given the information transfer measure proposed in the previous section how can a ci be modified to either i adjust the relative information contribution of each indicator according to the desired trade offs by the dm or ii maximize the overall information transfer as hinted in the introduction these objectives are often contradictory moreover it is assumed that the input data for the indicators i e normalized set cannot be altered and the aggregation method e g arithmetic or geometric mean is kept constant in this case the adjustments can be made by altering the weights however it is far from obvious which weight values will lead to the best properties in terms of objectives i and ii the solution is found by framing the issue as a computational optimization problem the first step is to build an objective function which for any given weight values calculates a score representing either i how adjusted the mean information transferred is or ii how much information is overall transferred to the composite index by calculating correlation ratio s i values for each indicator the best set of weights are then found by an iterative optimization search algorithm in this case the nelder mead simplex search method lagarias et al 1998 mckinnon 1998 which tries to find the highest value of the objective function the two objective functions for i and ii are described in detail in the following sections 2 2 1 objective i adjusting information transfer adjusting the relative information transfer i e the influence from the indicators to the ci in relation to their assigned weight is achieved in two steps see details in becker et al 2017 first to render the correlation ratios comparable to the weights a normalization step is needed 5 s i s i i 1 n s i where s i is the normalized correlation ratio of x i and i 1 n s i 1 this allows the normalized correlation ratios to be directly compared to their target the weights w i since the w i also sum to 1 second the problem of adjusting the contribution of the indicators can be formulated by defining an objective function as the sum of squared differences between the s i at a given set of weights and the target s i accordingly 6 w o p t a r g m i n w i 1 n s i s i w 2 where w w i i 1 n and w o p t 0 here it is assumed that the initially assigned weights represent the relative information transfer that is desired from each indicator i e s i w i hence the optimization problem in equation 6 tries to find a set of weights that minimises the discrepancy between the normalized correlation ratios s i and the initially assigned weights w i from the perspective of information transfer this equates to adjust the relative information transfer of each indicator in relation to the assigned weights by the dm 2 2 2 objective ii maximizing information transfer mathematically this problem is formulated by defining an objective function as the difference between a vector of all ones 1 i e the maximum information transfer s i 1 and the s i obtained at a given set of weights accordingly 7 w o p t a r g m i n w i 1 n 1 i s i w where the weights must sum to one w w i i 1 n and are constrained to be positive w o p t 0 by minimising this objective function the weights w o p t that maximize the total sum of information transferred from the indicators to the index can be found 3 relation between information transfer and average correlation this section gives an analytical exploration of ci aggregation it discusses how correlations between a set of indicators x i n influence the information that is transferred from those indicators to the ci y here r i 2 or linear s i captures the linear dependence between x i and y as shown in equation 3 consider the definition of r i 2 8 r i 2 corr 2 y x i cov 2 y x i var y var x i now assume a set of n variables with correlation matrix for this set of variables the weighted mean is explored such that y x w where x is the m n sample matrix w is the n 1 vector of weights and y is the vector of output values by letting e i be a n 1 vector where all elements are zero except the i th element which is set to one this linear combination gives johnson and wichern 2007 9 r i 2 w e i 2 w w e e using the expression in equation 9 to obtain r i 2 fig 2 shows its convergence as the number of indicators n changes from 2 to 100 for correlation matrices with average correlation coefficients ρ ranging from 0 to 1 with an interval of 0 1 it can be seen that r 2 y x i converges to ρ for large n with faster convergence the closer ρ is to 1 this convergence is also mathematically derived in appendix a in the electronic supplementary information esi where it is shown that for indicators with equal weights and equal variance r i 2 tends to the average correlation coefficient between indicators as n tends to infinity from this analysis it can be concluded that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci a linear combination of poorly correlated indicators will on average have a weaker dependence i e information transfer between the indicators and the ci than a linear combination of highly correlated indicators although here information transfer has been framed via r i 2 the fact that s i is a nonlinear generalisation of r i 2 allows these conclusions to be extended to the nonlinear case thus the average correlation coefficient ρ of a given correlation matrix can provide a useful rule of thumb on how the information transfer capacity of a ci will be affected when considering adding subtracting indicators to a framework this relationship will be further examined in the following section by applying the proposed measure to a case study 4 case study electricity supply resilience index the management of complex socio technical systems that are also embedded in environmental ones requires a dedicate array of tools to lead i the conception of their structure ii the identification of their key variables and functions iii the development of their underlying model and iv the assessment of their integrated performance as well as the effect of uncertainty in the input variables on the model output one of the premier concepts proposed to conduct integrated assessment and management of systems is the one of resilience it empowers analysts to consider technical biophysical and socio economic factors under one framework to support the understanding of the systems roostaie et al 2019 a main example of complex socio technical systems that requires a dedicated evaluation from a resilience perspective is the one of energy the pervasive nature of this type of systems is such that it encompasses multiple others including the biophysical ones at multiple scales fernandes torres et al 2019 in fact energy systems have direct and indirect implications on the environmental systems including water land and air given the importance of this topic the tools presented in section 2 are tested with one ci developed to assess energy systems resilience more specifically they are used with one ci out of the 38 that constitute the electricity supply resilience index esri a ci developed within the future resilient systems frs program at the singapore eth centre sec it is based on 12 indicators evaluating countries security of electricity supply from a resilience perspective gasser et al 2020 the targets of the evaluation are 140 countries that represent a wide spectrum of nations from all around the world esri uses data compiled from the international energy agency iea the international renewable energy agency irena paul scherrer institute s psi energy related severe accidents database ensad the world bank the swiss reinsurance company swiss re and the u s energy information administration eia the underlying data has been treated for outperformers identified with the interquartile range iqr method values are considered as outperformers if they lay outside 1 5 times the iqr from the first and third quartiles q1 and q3 respectively these were trimmed to the nearest value that is not an outperformer 4 4 note that the trimming is based on the actual data for the chosen 140 countries not the theoretical min and max values across the 12 indicators 88 values were identified as outperformers and trimmed to the nearest value within the iqr range after trimming missing values have been replaced by the average indicator values using an unconditional mean imputation 5 5 across the 12 indicators 65 instances of missing values were identified and replaced it must be noted that the use of the indicator mean can result in a decrease of the correlations as one of the common methods to deal with missing data nardo et al 2008 the final scoring and ranking of esri is obtained by 38 different combinations of normalization methods and aggregation functions gasser et al 2020 normalization methods are used to render the raw data comparable and suitable for aggregation in the cited study eight of these approaches were selected ordinal linear and non linear normalizations were chosen to account for the variability of approaches that can be selected by the analysts in ci development once the indicators are normalized they have to be aggregated to provide a final score and ranking gasser et al 2020 considered six aggregation functions in order to include different preferences of the decision maker in the form of compensation between the indicators the research in gasser et al 2020 is an extensive exploration of how different combinations of normalization methods and aggregation functions can affect the final score and ranking of the countries however the correlation analysis is limited to the assessment of the positive and negative trends between the indicators as well as the coherence of the set of indicators i e reliability of the scale as shown in this paper in section 2 correlation analysis can be used to do much more including the exploration of the correlations between the indicators by assessing the information transferred from each indicator to the ci and study the effect that different weighting schemes have on each of them consequently the tools proposed in section 2 are used in this case study to extend the understanding of the effect of the data structure on the weighting stage in the ci it must be noted that the ci resulting from the proposed weighting scheme is not more nor less valid compared to the esri proposed in gasser et al 2020 given that cis cannot be validated with objective measures as they model a concept that is not directly measurable the value of the research resides in refining the learning about the implications of different data structure on the influence that indicators have in cis in this paper the tools presented in section 2 are applied to one ci developed with the combination of one normalization method i e min max normalization and one aggregation function i e additive weighted sum to develop esri the reason for this choice is that these are among the most commonly used approaches in their respective discipline carrino 2017 el gibari et al 2019 greco et al 2019 so the results are of interest to a large audience of analysts and decision makers the index used in this paper and obtained with this combination of normalization method and aggregation function is called resilience index for analysis and optimization rifao the software called composite indicator analysis and optimization ciao lindén et al 2021 developed by some of the authors of this paper too was used to perform the statistical analysis appendix b in the esi provides more details on the framework and the indicators that constitute rifao while appendix c in the esi includes the raw and normalized dataset used to construct rifao it must be pointed out that no final scores of rifao are actually presented and discussed since the objective of this case study is not to focus on the rankings obtained with this index but rather to apply the optimization algorithms according to the objectives i and ii presented in section 2 2 to achieve the desired information transfer from each indicator to the ci furthermore appendix d in the esi presents the results of the same analysis by using the raw dataset i e the dataset without trimming the outperformers which shows that similar trends have been found as with the application of ciao tool with the rifao dataset with the trimmed outperformers the methodology used to develop rifao conduct the statistical analysis with the tools from section 2 and elaborate the resulting recommendations for weighting scenario choice and index revision is shown in fig 3 step 1 refers to the normalization of the dataset with the min max normalization in step 2 the correlations are analysed by means of pearson correlation coefficient ρ to study the interrelations between the indicators the normalized indicators are then aggregated with the additive weighted sum in step 3 step 4 studies the information transferred s i at equal weights and discusses the average correlation measured with respect to the step wise addition of indicators lastly step 5 provides recommendations for the choice of a weighting scheme according to a set of conditions that the dm might be interested to set for the index development this leads to three scenarios i e scenario a b c which represent different combinations of three main features of the problem i the variability of the information transferred s i from each indicator to the index ii the possible removal of one or more indicators from the index and iii the possible loss of mean information transfer s i m e a n each scenario is described in detail in section 4 2 and 4 3 step 1 in rifao development leads to the normalization of the dataset for indicators with a positive polarity meaning that the higher the value the better for the evaluation the chosen normalization method is given by the formula x j i min x i max x i min x i indicators with a negative polarity meaning that the lower the value the better for the evaluation are transformed via 1 x j i min x i max x i min x i where x j i is the raw country value in the i th indicator x i i 1 2 n this procedure results in a linear transformation of the data ranging from 0 min to 1 max and is performed on all indicators to render them comparable table 1 gives an overview of each of the 12 indicators that are included in the rifao framework and fig 4 shows the pearson correlation coefficients ρ between them step 2 in fig 3 for conciseness the indicators are labelled according to their id number e g ind 1 as defined in table 1 in all graphs and figures by examining the correlation structure of rifao it can be noticed that there is a large variation in the correlation strength between the indicators with values ranging from 0 44 to 0 94 although many indicators show a positive correlation between them the highest ρ 0 94 being between i n d 3 control of corruption and i n d 10 government effectiveness there are also a number of negative trends visible i n d 6 electricity import dependence showcases negative correlations with all the other indicators this finding shows that i n d 6 is mostly capturing a trend which is opposite to the other indicators in the dataset also a few non significant correlations 6 6 defined according to significance level p 0 05 can be seen four out of the eleven negative correlations displayed by i n d 6 are non significant i n d 7 equivalent availability factor except for a high positive correlation with i n d 2 severe accident risks presents non significant correlations all close to 0 this finding confirms how i n d 7 is mostly disconnected from the trends of the other indicators in the dataset these last two indicators proved to be of high interest in the subsequent stages of the analysis especially when discussing the possible re structuring of rifao 4 1 information transfer at equal weights as far as weighting is concerned equal weights are assigned to each indicator with the modelling assumption that the trade offs between each one included in the conceptual framework should be equal this section explores information transfer in rifao at equal weights and it is performed in two steps first the rifao indicators are aggregated with equal weights step 3 in fig 3 and an ex post assessment of information transfer is performed by estimating the correlation ratios via regression analysis between the indicators and the index step 4 in fig 3 the resulting regression fits are shown in fig 5 where both a linear r i 2 and nonlinear s i regression model are fitted to the data second the resulting correlation ratios s i are then normalized and assessed in comparison to the vector of equal weights this comparison is shown in table 2 from observing the resulting regression fits and the estimated r i 2 and s i values in fig 5 it can be noted that the indicators showing a linear trend towards the index e g i n d 3 control of corruption or i n d 4 political stability also have a low discrepancy between their r i 2 and s i measure in these cases linear estimates are sufficient to capture their dependence however there are also indicators that display nonlinear tendencies towards the index e g i n d 1 saidi or i n d 2 severe accident risks in these cases the linear regression model underestimates their dependence see e g i n d 2 which has an r i 2 of 0 48 but an s i of 0 66 this highlights the importance of also considering nonlinearities between the indicators and the ci when estimating dependence what is further evident from fig 5 is that not all indicators are transferring an equal amount of information hence they do not have the same influence on the index even though they are assigned equal weights thus they are not equally influential in representing countries across the concept measured by rifao the normalized correlation ratios s i in table 2 further showcase this discrepancy see deviation ratio column with values ranging from 64 overrepresentation i n d 10 to 77 underrepresentation i n d 7 by re examining the correlation matrix in fig 4 a connection between correlation strength and information transfer is evident the information in the highly correlated indicators e g i n d 3 8 10 12 tends to be overrepresented whereas the opposite holds true for the poorly non or negatively correlated indicators e g i n d 5 6 7 11 these findings are especially relevant in relation to the previously defined link between correlation and information transfer under restricted conditions see section 3 indeed even when distributions are not strictly linear an indicator s correlation with the other aggregated indicators provides a strong indication of its capacity to transfer information to the ci based on this statistical analysis it is possible to assign the indicators to three groups table 2 group 1 i n d 3 8 10 12 high correlations and s i and also high positive deviation ratios this characterises indicators that are overrepresented group 2 i n d 5 6 7 11 low correlations and s i and also the highest negative as well as absolute deviation ratios this characterises indicators that are underrepresented group 3 i n d 1 2 4 9 intermediate correlations and deviation ratios leading to moderate over or under representation the analytical analysis presented in section 3 was adapted to rifao to study the effect of each indicator on the average correlations of the index step 4 in fig 3 the results are presented in fig 6 showing how the average r i 2 s i and pearson correlation ρ perform when indicators are added incrementally one by one to develop rifao the measures show a common trend nonetheless it can be seen how notable divergence emerges between s i and pearson correlation ρ when i n d 6 and i n d 7 are added this analysis also shows that there is a significant drop off in information transfer when i n d 6 and i n d 7 are added to the framework which confirms that low correlated indicators result in low information transfer in addition to the findings in section 3 these results show that the average correlation can provide a useful albeit not perfect rule of thumb with respect to how much information on average is transferred from a set of indicators to the ci even for a smaller sample size and when distributions are not strictly linear 4 2 information transfer at optimized weights the variance based analysis of rifao shows that the information transfer from the indicators to the ci is not equal even though equal weights are applied and strongly driven by the correlation structure in addition the information transfer from each indicator to the ci is not maximized this section explores two avenues of weighting that a decision maker might be interested in case he she wants to achieve a balanced information transfer or a maximized one while the framework of indicators has to remain the same they are contextualized as two different scenarios scenario a and scenario b with different conditions that a dm might require to be met step 5 in fig 3 scenario a considers a dm who 1 does not want to have a widely unbalanced s i for each indicator 2 does not want to revise the indicators in the index 3 can accept a possible loss of s i m e a n this scenario results in rifao with 12 indicators where the main objective is to equally balance the information transfer from each indicator balance opt scenario b considers a dm who 1 accepts a possible wide s i variability for each indicator 2 does not want to revise the indicators in the index 3 aims to have as much as s i m e a n as possible this scenario results in rifao with 12 indicators where the main objective is to maximize the total information transferred from each indicator maximize opt the scenarios are modelled by optimizing the weights in line with the objective functions equations 6 and 7 respectively defined in section 2 the next sections describe the results of each scenario 4 2 1 scenario a equally balancing the information transfer from each indicator balance opt scenario a results in the most unbalanced set of weights as shown in fig 7 most notably the negatively correlated indicator i n d 6 electricity import dependence receives the highest weight 35 and also the non correlated indicator i n d 7 equivalent availability factor receives a substantial share of the weight 10 furthermore five indicators i n d 2 severe accident risk i n d 3 control of corruption i n d 8 gdp per capita i n d 10 government effectiveness and i n d 12 ease of doing business receive zero weight and two more i n d 1 saidi and i n d 9 insurance penetration obtain a weight close to zero i e 0 01 even though only five indicators receive a weight greater than 0 01 as shown by the correlation ratios in fig 8 the information contained within the zero weighted indicators is still captured by the ci simply through correlation judging from previous observations it can be assumed that these indicators excluding i n d 2 are sufficiently represented by the inclusion of i n d 4 with which they are all highly positively correlated see fig 4 the error bars in fig 8 representing the 5 95 percentiles show that the resulting weighting vector from the balance opt objective would achieve the most well balanced information transfer from each indicator ranging from s i m i n 0 14 to s i m a x 0 25 however the average contribution is relatively low s i m e a n 0 19 the correlation ratios in fig 8 show that only two indicators i n d 6 and i n d 7 measure an increased information transfer compared to the case of equal weights hence this weighting scheme does practically not improve the total information transfer but rather reduces the information transfer from the highly correlated indicators to target a balanced contribution in other words the balance opt weighting scheme focuses mostly on the indicators which are underrepresented i n d 5 6 7 11 see group 2 in table 2 at the cost of reduced mean information transfer s i m e a n 4 2 2 scenario b maximize the total information transferred from each indicator maximize opt this scenario results in a slightly less unbalanced set of weights than in scenario a see fig 7 in this setting the weights are mostly assigned to the highly correlated indicators e g i n d 4 political stability 12 i n d 8 gdp per capita 19 and i n d 10 government effectiveness 16 whereas the two non or negatively correlated indicators i n d 6 and i n d 7 receive zero weight interestingly the correlation ratios in fig 8 reveal that the information in these two indicators is albeit only slightly for i n d 7 still represented by the ci through correlation most notably i n d 6 shows an increased information transfer compared to the equal weights and balance opt weighting scenario even though it is receiving zero weight in line with its objective most indicators show an increased information transfer to the ci when the maximize opt weighting scheme is applied only three indicators i n d 1 i n d 7 and i n d 11 show a decline in relation to the equal weighting scenario when comparing the average correlation ratios fig 8 shows that this weighting vector does achieve the highest total information transfer s i m e a n 0 54 however the large error bars even higher than for the equal weights case suggest that it is unevenly distributed amongst the indicators ranging from s i m i n 0 04 to s i m a x 0 93 it can thus be concluded that the pursuit of maximizing total information transfer comes at the expense of certain poorly correlated indicators especially i n d 7 which are barely represented by the ci 4 3 revising the ci based on the information transfer analysis for both optimized i e balance and maximize opt weighting schemes in rifao with 12 indicators the poorly correlated indicators especially i n d 6 and i n d 7 revealed to be problematic from a perspective of information transfer when the balance opt weighting scheme is employed these indicators receive a substantial share of the weights the result is a balanced information transfer from the indicators to the ci but with a low total information transfer when the maximize opt weighting scheme is deployed these indicators receive low or zero weights this results in a high total information transfer but with a large discrepancy between the individual indicators a third scenario scenario c step 5 in fig 3 has thus been developed where the dm 1 wants to keep the s i variability in a narrow range 2 is willing to revise the indicators included in the index 3 does not want to have an excessive compared to equal weights and maximize weighting schemes loss of s i m e a n this is mainly performed for exploratory reasons the previous analysis shows that these indicators are not transferring much information to the index and their inclusion does not allow achieving a balanced information transfer from each indicator hence we explore if we can achieve this by omitting them from the ci a key drawback consequence of omitting low correlated indicators is that these can contain a high information content of that indicator dimension this information would then be lost however what we have shown is that this information is not really represented by the index in the first place so removing them will have a low effect on the index scores and resulting rankings this problem framing leads to what is called rifao 10 an index with 10 indicators where i n d 6 and i n d 7 are removed from the ci see above discussion and the balance optimization is used i e rifao with 10 indicators with balance opt the resulting weights and information transfer measures are shown in fig 9 and fig 10 respectively similarly to the case of rifao with 12 indicators fig 9 shows that the balance opt still results in an unbalanced set of weights even though i n d 6 and i n d 7 are removed the same five highly correlated indicators i n d 2 severe accident risk i n d 3 control of corruption i n d 8 gdp per capita i n d 10 government effectiveness and i n d 12 ease of doing business receive zero weight however the distribution of the remaining weights is not the same as for rifao with 12 indicators in the absence of i n d 6 and i n d 7 i n d 11 now receive the most substantial share of the weights followed by i n d 5 i n d 9 i n d 4 and i n d 1 in decreasing order again it is important to note that the information in the zero weighted indicators would still be captured by the ci simply through correlation by the inclusion of i n d 4 and i n d 9 this is shown by the resulting correlation ratios in fig 10 the key difference compared to the previous case of rifao 12 however is the magnitude of information transfer achieved at balance opt weights contrary to the case of 12 indicators it is now possible to achieve a rather well balanced information transfer ranging from s i m i n 0 41 and s i m a x 0 52 see fig 10 without reducing the total information transfer to the same extent s i m e a n 0 46 compared to s i m e a n 0 19 in the case of 12 indicators for comparative purposes fig 10 also includes the s i m e a n for the maximize opt for rifao with 10 indicators it can be seen that the discrepancy between the two s i m e a n is considerably reduced with respect to the case of the ci based on 12 indicators most importantly the wide variability in the s i m e a n shows that there is still a considerable imbalance of information transfer from each indicator in this rifao with maximize opt though the mean value is higher than in rifao 12 and the lower bound increases from about 0 1 to 0 2 whereas the upper bound remains at about 0 9 maintaining the s i variability in a narrow range was a binding condition to be met for scenario c and for this reason only the balance opt is considered as a viable option in the case of rifao with 10 indicators 5 discussion information transfer and correlations are intricately related in the construction of cis in this paper it was confirmed that correlations lead the indicators to transfer information differently and hence have a different influence impact on the ci as compared to the assigned weight in order to deal with this discrepancy between desired influence of indicators i e weights and their actual influence driven by correlations we provide tools that allow a deep dive into this complex interrelationship and study the information transfer in relation to both weights and correlations the main contributions of this research consist in 1 proposing a measure of information transfer based on correlations between the indicators along with two weight optimization methods the analyst can now adjust the weights to achieve either a targeted or maximized information transfer from a set of indicators 2 showing that while targeting indicator contributions is important it is also relevant to consider the overall information conveyed by the index thereby introducing the second optimization objective maximizing information transfer 3 showing how the number of indicators and the average correlation can inform the analyst about the overall information transfer more specifically we demonstrate the convergence of information transfer towards the average correlation coefficient the resulting analysis indicates that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the ci in fact correlations can be a good rule of thumb of how information transfer from a set of indicators will behave in the aggregation of a ci 4 applying these tools to a case study on electricity supply resilience assessment regarding the case study we apply the proposed tools to one version of the electricity supply resilience index esri developed at the singapore eth centre which was called resilience index for analysis and optimization rifao the resulting analysis shows that correlations between rifao s underlying indicators have a direct influence on the index preventing the equal weights assigned to correspond to an equal information transfer from each indicator different weighting schemes and index revision scenarios are also proposed according to specific requests that the dm might have with respect to possible loss and balance of information transfer as well as indicators inclusion in the index when the weighting scheme used to distribute influence equally between indicators i e balance opt is employed highly correlated indicators are poorly weighted and less correlated indicators receive a substantial share of the weights the outcome is a balanced but low information transfer from the indicators to the ci when the weighting scheme proposed to maximize the information transfer from the indicators i e maximize opt is applied it is instead the less correlated indicators that are poorly weighted in favour of the more highly correlated indicators the result is a high total information transfer but with a large discrepancy between the individual indicators however when the two poorly correlated indicators are removed from rifao the results indicate a less evident trade off between the two weighting schemes with comparable average information transfer though well balanced with the balance opt scenario compared to the maximize opt scenario thus if there is a large inconsistency variation in correlation strength between the indicators it is probable that there will be an unbalanced information transfer from each indicator even though equal weights are applied this phenomenon is not possible to counterbalance by adjusting the weights without compromising the information transferred to the ci and its overall capacity to convey a representation of its underlying components the indicators our research also contributes to an ongoing debate on the inclusion of positively and or negatively correlated indicators in cis on the one hand there are authors like marttunen et al 2019 who advocate for the inclusion of not or negatively correlated indicators as they can be more informative for a decision since they bring unique perspectives on the aspects under evaluation on the other hand there are other authors like munda et al 2020 who warn about the risk of including indicators with low or negative correlations as their information might not be represented in the ci our research advocates for a balanced reasoning between these perspectives as follows when correlation exists between indicators it means that information is shared between the two indicators to take extreme cases if nonlinear correlation is zero that means that there is no shared information and the two indicators are bringing completely unique information contributions if correlation is one the indicators are collinear and encode effectively the same information clearly the second case is not useful because it implies double counting 7 7 this reasoning applies to a decision making problem with a flat structure for the indicators it would nonetheless be possible to keep the same indicator in two different dimensions if there would a hierarchy of indicators where the same indicator is present in more than one dimension in this case it would be possible keep the same indicator twice and use for example value functions to transform normalize the data so that e g value x of indicator a in dimension 1 means a 0 2 while the same value x of indicator 1 in dimension 2 means a 0 4 assuming the transformation is between 0 and 1 with an increasing order of preference however the first case comes with some pros and cons on the one hand as pointed out by marttunen et al 2019 zero correlation between indicators means that there is no overlap and that can be seen as a good but this comes at a cost as we show in fig 2 since if one combines several indicators with zero correlation this will result in a ci that contains relatively little information from any one of the indicators therefore in our opinion if a concept can be summarised by some very few indicators with low correlations this can still be acceptable as it is still possible to have a moderate information transfer however as fig 2 shows above 10 indicators with an average correlation coefficient of zero r 2 is less than 0 1 between indicators and index which contrasts with the fundamental objective of ci development itself being the condensation of information of many indicators into one consequently we recommend that when only looking at the correlations if they are low only a few indicators should be aggregated together but if they are high more indicators can be aggregated however the whole development of the ci should in the ideal case be embedded in a stakeholder consultation process i e decisions on indicators will not just be driven by correlations but influenced by the priorities of the stakeholders additionally potential interactions between the indicators might also be included in the development of the ci which are not necessarily equal to correlations the authors also think that it is relevant to separate two different concepts information transfer and information content it is true that a low correlated indicator can imply a high information content of that indicator dimension however what we show is that because of its low correlation with the other indicators it will not transfer much of that information to the index i e the index will not contain much of the information of that indicator dimension hence a low correlated indicator will have a low information transfer to the index but can still by itself have a high information content of that specific indicator dimension this research also comes with a number of limitations that are presented below together with options for future research to tackle them this study has not considered the effects of changing aggregation methods and input data which can be considered as one of the inherent uncertainties in composite indicators in order to understand the effects of changing input data and aggregation method one would have to perform an uncertainty analysis e g a monte carlo sampling along the lines of saisana et al 2005 what we propose in this research is not to investigate the uncertainties in weights but more to calibrate them to a desired objective i e target or maximize information transfer any uncertainty analysis is thus an avenue for future research the same reasoning applies to the assessment of the effect that each source of uncertainty can have on the index variance a possible option is this respect would be fuzzy mcda methods kaya et al 2019 the application of the ciao tool to the case study is based on the fully compensatory additive weighted sum which means that its results are meaningful only for this type of aggregation function however the ciao tool can be used with aggregation functions that have lower compensation levels than the additive weighted sum such as the geometric and harmonic ones like the additive weighted sum also the geometric and harmonic weighted sums are already included in the ciao tool and they can surely be a very interesting opportunity for future testing of our tool there are however aggregation functions which would not be suitable for the ciao tool like extreme aggregation operators such as the minimum and maximum operators the reason is that since only one indicator would determine the final score the worst with the minimum and the best with the maximum operators there would be no optimization of weights to be performed as only one indicator would be defining the overall performance this research has not accounted for a dm who is willing to accept a compromise between the two objectives proposed for the weight optimization this is because the goal of our research is to offer the users the ciao tool to exactly achieve the desired target behind each optimization objective in case the dm would like a compromise between these two objectives the option of applying a multi objective model could be explored finally the s i measure proposed in this research has been developed for a decision making challenge with a flat structure of the indicators meaning that there is only one level between the constructed concept and the variables used to measure it with the ci it can however be noted that for a hierarchical index with multiple pillars and based on an additive weighted average it would be also possible to calculate the effective weight of each indicator in the index by multiplying the indicator weight by its pillar weight or by optimizing one level at a time the statistical analysis presented in this paper surely adds a layer of complexity for the well informed development of composite indicators the weighting of the indicators in fact results as a combination of data driven i e statistical and stakeholder based i e value choices of the dm input which might be difficult to communicate especially if the index is developed for advocacy purposes nonetheless these types of advanced statistical analyses can be used to assess and enhance the robustness of the models that are developed ultimately leading to more sound decision making this is in line with the recent call for such type of research as presented for example by moallemi et al 2020 and saltelli et al 2019 6 conclusions the tools introduced in this study allow developers of cis to explore in detail the effect of weighting choices in an easily interpretable framework based on the concept of information transfer for the first time this work has shown that trying to balance the contributions of indicators may often come at the expense of reducing the overall information transferred from each indicator to the index most likely developers will wish to find a compromise point between balancing and maximizing information transfer and the optimization algorithms here give the means to assign selected weights in the perspective of these two competing criteria as demonstrated with the rifao case study this can sometimes be achieved by re structuring the index this research also relates to an existing discussion on the use of supervised dm driven and unsupervised machine driven methods for studying and defining the complexities and interdependencies of a certain decision problem when the complexity is such that the required knowledge cannot be easily given or the decision maker is not knowledgeable enough the unsupervised method can be useful in at least providing an initial mapping of the decision problem kojadinovic 2008 consequently unsupervised methods are not to be seen as competitors to the methods that employ active interaction with the decision makers to define these dependencies and the resulting weights rather they should be viewed as aiding tools to navigate the difficulties embedded in shaping the understanding of complex systems evaluated by means of multiple criteria furthermore it is important to note that the users of the tools proposed in this research are envisioned to be analysts with a mathematical background in statistical analysis and development of ci a key distinctive feature of this type of users is their desire of providing a bridge between two scientific communities on the one side data analysis without stakeholders involvement and on the other side decision aiding based on inclusion of stakeholders preferences the users can in fact use the tools provided by this research to achieve the desired contributions of the underlying indicators in the ci the tools proposed here are intended to provide goalposts between which developers can pick a desired target and are not meant to supersede the conceptual relevance of the indicators communication issues and methodological choices in other stages of the ci construction which are other highly relevant factors more specifically the dm can define the conditions for the index development with respect to i the possible loss of mean information transfer ii the possible variability range of the information transferred from each indicator to the index and iii the willingness to discuss the possible removal of one or more indicators from the index once these conditions are defined the weighting scheme can be obtained with the proposed tools and their results discussed among the stakeholders to decide how to proceed in the development of the index finally the current findings should not be simply generalized and applied but the wider applicability of the proposed tools requires further testing with different datasets with a varying number of indicators and alternatives and with further normalization and aggregation functions the difference of this research with respect to other sensitivity analyses is that the proposed framework does not aim to study the variability of the results according to the choices involved in its construction such as the selection of the indicators the normalization methods or the aggregation algorithms saltelli et al 2019 douglas smith et al 2020 zhang et al 2020 it instead focuses on the effect of the correlation structure on the influence that each indicator has in the ci when foreseeing a link with the other uses of sensitivity analyses the proposed framework could also be applied to different conceptualizations of the ci to study how the recommended weighting would change based on e g different normalization methods and or aggregation functions 7 software the calculations for the case study on electricity supply resilience were performed with the software composite indicator analysis and optimization ciao lindén et al 2021 which was specifically developed for this research and it is now freely available at the link https bitbucket org ensadpsi ciao tool src master declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research was conducted at the future resilient systems frs at the singapore eth centre sec which was established collaboratively between eth zürich and singapore s national research foundation fi 370074011 under its campus for research excellence and technological enterprise create program matteo spada and peter burgherr also received support from the swiss competence center for energy research sccer supply of electricity soe marco cinelli acknowledges that this project has received funding from the european union s horizon 2020 research and innovation program under the marie skłodowska curie grant agreement no 743553 the authors also thank paolo paruolo from the european commission s joint research centre for helpful input on analytical correlation analysis appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105208 
25736,approximately 15 of global land is currently in some state of protection recent conservation research suggests the need for a drastic increase of protected lands by 2050 in order to reach this target an additional 35 of lands need to be conserved or restored in a cost effective and time efficient manner in order to support the resiliency of our planet and its climate while many individuals and foundations continue raising much needed funding for the environment the development of conservation portfolios is a complex multi dimensional task agencies have limited resources for investing in new conservation areas and have differing priorities for conservation in terms of species land cover human activities etc we present an interactive conservation portfolio development system that combines visualization multicriteria analysis optimization and decision making that enables conservation planners and scientists to efficiently construct compare and modify conservation portfolios under multiple constraints keywords conservation planning geographic visualization decision support systems visual analytics discrete optimization 1 introduction biodiversity is declining at rapid rates due to human driven habitat loss and landscape deterioration stokstad 2010 human activities have resulted in species extinctions at 10 100 times normal background extinction levels sala et al 2000 pimm et al 1995 this rapid biodiversity decline threatens the provision of key ecosystem services such as food clean water and crop pollination resulting in negative consequences for economies and human health mace et al 2012 therefore protecting remaining natural areas is fundamental to preserve biodiversity and to mitigate the negative consequences of ongoing environmental change johnson et al 2017 while 15 of the earth is in some kind of protection belle et al 2018 this is still insufficient due to substantial gaps in land coverage and increasing threats rodrigues et al 2004 recent studies suggest the need of a drastic increase in protected lands by 2050 to maintain current rates of resource extraction watson and venter 2017 dinerstein et al 2019 this ambitious goal contrasts with the limited resources available to local institutions to design and implement networks of protected areas bicknell et al 2017 conservation biologists apply systematic conservation planning approaches to design protected area networks that are cost effective while meeting conservation goals this systematic process is composed of six steps that include 1 biodiversity data collection and analysis 2 identification of conservation goals 3 analysis of current conservation areas 4 identification of a set of additional areas 5 implementation of proposed conservation actions and 6 preservation of required conservation values margules and pressey 2000 out of these we focus on step 4 the identification of additional conservation areas one of the most challenging steps in the conservation planning process these conservation areas are selected with multiple conservation goals in mind such as maximizing biodiversity representation while attenuating future threats and remaining within a limited budget luck et al 2012 this is a complex selection process that if performed using inadequate quantitative tools may result in landscape or seascape portfolios that are not optimal in terms of their budget and priorities therefore the success of systematic conservation planning rests in part in the development of appropriate data driven methodologies for designing protected area networks at the regional level williams et al 2005 web based geographic information systems wb gis provide an ideal setting to translate the result of complex spatial mathematical models used in systematic conservation planning into simple qualitative visual scenarios dragicevic 2004 these wb gis can summarize multiple layers of information allowing planners to analyze various future hypothetical scenarios rao et al 2007 while multiple mathematical models are available to prioritize areas for conservation sarkar et al 2006 moilanen et al 2009 designing a network of protected areas requires the quantification visualization and adjustment of multiple hypothetical scenarios almost simultaneously tress and tress 2003 pettit et al 2011 in this context typical questions faced by conservation analysts include what areas should be selected as part of a network of protected areas to have all species of conservation concern under protection while minimizing the acquiring costs if we decrease the budget by 10 which areas should be protected what happens if instead we increase the budget by 5 therefore there is a need for wb gis applications for conservation planning that combine cost optimization with efficient visualization tools that can provide alternative future scenarios in real time portman 2014 in this paper we present an interactive conservation portfolio development system that combines visualization multicriteria analysis optimization and decision making that enables conservation planners and scientists to explore different land purchasing portfolios under a variety of constraints in real time our system incorporates a multi layer map view a parallel coordinates attribute view a control area for optimization modeling and a multiple portfolio visualization for solution comparison to support automatic portfolio optimization we implemented a median ranking algorithm to allow parcel filtering by an aggregated indicator of all the attributes and an integer programming model to generate land purchase recommendations given user defined constraints and objective function the visual analytics system is designed to support the efficient selection of conservation areas by enabling portfolio generation and interactive modification multiple land portfolios can be generated and saved for comparison our system complements the existing body of tools by providing new visual analytical and mathematical features while also allowing loading of a shape compatible conservation plan obtained with any other tool for further visual analysis from the software systems perspective we propose a novel combination of visualization components where our design has focused on featuring credibility saliency and legitimacy white et al 2010 the software tool itself serves as a boundary object to enable decision making our design ranges from providing detail on demand for the data source to enable analysts to determine credibility of data layers interactive selection of optimization criteria and provenance analysis specifically for supporting provenance analysis and comparison we propose new visualization designs to capture different portfolios and provide comparison between them along with novel integration of techniques and the proposal of a visualization design we have also designed a pre processing scheme to match data across different levels of granularity our down sampling technique allows data comparisons at a high resolution level and supports land purchases which can only occur at the parcel level the human machine combination is also innovative where our framework is designed to present an optimal solution within the problem formulation however the problem formulation needed to be computationally efficient for real time exploration by providing an optimal conservation portfolio as a first pass we allow users to refine their choices in a human machine teaming process our system enables conservation planners to develop consecutive portfolios in real time and adjust the outputs of the multiple criteria optimization selections from an optimization perspective our proposed approach uses the analyst s preferences to drastically reduce the problem size by supporting interaction with the optimization results planners can utilize species specific knowledge to enforce different landscape features i e connectivity compactness corridor width which can be challenging for automatic optimization either due to the difficulty in representing the corresponding constraints or the computational complexity of the resulting model although we propose a simple and fast optimization model to support land acquisition decisions the proposed system can accommodate the results from other spatial models as a result our system can be seen as a visualization framework that supports user interaction with an optimal solution to our knowledge currently there is no tool available with the proposed features to support conservation decisions 2 related work our system is designed to support decision making through multicriteria analysis and solution comparison in this section we summarize previous work in visual analytics for decision making and multicriteria optimization for conservation planning recent visualization work has focused on how to best display multiple attributes for analysis a key component of multicriteria analysis turkay et al 2014 explored the geographic variation of multivariate data and developed attribute signatures consisting of dynamically generated graphs to summarize the change of statistics over a sequence of geospatial data selection pajer et al 2017 proposed weightlifter a technique that allows the exploration of weight space with up to ten criteria and helps to explore the sensitivity of candidate solutions to the change of weights weng et al 2018b designed a visual analytics system reach which helps analysts identify their ideal home given multiple purchasing constraints common amongst many of these systems are the use of parallel coordinates plots and a wide variety of extensions to parallel coordinate plots systems extending the parallel coordinated plots e g lind et al 2009 johansson et al 2005 rosenbaum et al 2012 xie et al 2017 provided a means to easily explore multivariate data other works have focused more on supporting analysis and decision making through the integration of interactive models afzal et al 2011 developed a decision support environment to evaluate disease control strategies by predicting the course of an outbreak and analyzing the response measures the severity of the epidemic is visualized by different color intensities on the map and a custom split timeline is used to show the solution path konev et al 2014 proposed an automatic simulation based approach for flood management the decision trees are automatically generated and visualized by clustered timelines rinner 2007 developed a geographic visualization system to support multi criteria decision making an index rank for each tract is calculated and users can explore attributes through a linked parallel coordinate plot similar to the work of rinner 2007 cassol et al 2017 proposed a framework to explore the optimal evacuation plan for crowd egress based on multiple factors which were taken as input by the proposed metric to calculate the optimal plan in both systems interactive optimization methods are not fully considered these systems support multi criteria analysis through interactions with a parallel coordinate plot and quality indices similar to our use of median ranking however portfolio comparisons and interaction with the optimization results are limited other major issues underlying such decision support systems are the mechanisms used to compare across candidate solutions the work by gleicher 2018 summarized the basic designs of comparison into three categories juxtaposition i e which places the compared items are in different screen spaces superposition i e which places the compared items fit into the same screen space and explicit encoding i e visualization of the relationship between the compared items kehrer et al 2013 and munzner et al 2003 utilized juxtaposition design for their comparisons of bar charts lists and trees dasgupta et al 2015 combined juxtaposition and superposition for climate model comparison law et al 2018 developed duet a visual analytics system for pairwise comparison integrating all three categories duet uses visualizations and textual descriptions to explain the recommended object groups which are similar to or different from the user specified object with a focus on the similarity and difference weng et al 2018a proposed a spatial ranking visualization technique to explore and analyze ranking datasets and annotate the cause of the ranking with spatial context which involves the three design categories of comparison from the optimization perspective multi criteria analysis and modeling have been integrated in a number of visual analytics systems for domains ranging from epidemiology to emergency response however little work in the visual analytics community has focused on conservation planning conservation planning requires the integration of optimization algorithms for conservation portfolios given the myriad of possible parcel configurations available these conservation portfolios must allocate resources efficiently while considering current and future threats and their influence on the biodiversity assets the problem of designing natural reserves has received considerable attention since the 1980s kirkpatrick 1983 cocks and baird 1989 mostly through the use of exact optimization models ando et al 1998 church et al 1996 polasky et al 2001 sefair et al 2017 acevedo et al 2015 and heuristic approaches pressey et al 1997 arthur et al 1997 margules et al 1988 the use of operations research techniques in this area have become more prevalent in recent years including deterministic and stochastic approaches see moilanen et al 2009 for a comprehensive review moreover these methodological efforts have evolved into free software designed to support conservation planning decision making processes e g zonation and marxan lehtomäki and moilanen 2013 ball et al 2009 although available tools cover several pressing issues in conservation problems some contemporary challenges are still unsolved some of the existing approaches focus on cost minimization subject to ecological outcomes ignoring the more realistic dual problems of maximizing such outcomes subject to a given budget approaches that optimize the ecological performance of the conservation portfolio approximate the quality of candidate patches by species representation i e whether a species is present in a patch and other single static patch attributes toregas and revelle 1973 underhill 1994 williams and revelle 1996 camm et al 2002 moilanen et al 2009 ignoring the multiobjective nature of the conservation decisions other works focus on desirable geographical properties of protected areas such as landscape connectivity önal and briers 2006 dilkina and gomes 2010 dissanayake et al 2012 jafari and hearne 2013 and compactness önal and briers 2002 nalle et al 2002 dissanayake et al 2012 jafari and hearne 2013 but ignore the subjacent ecological processes the majority of the works studying connected and compact reserves are mixed integer programming models that are difficult to solve for realistic size instances and that provide a single solution i e a single connected and compact set of parcels to purchase without the visual support analysts cannot easily modify an existing solution to incorporate expert knowledge and other attributes not included in the optimization model although optimization models in conservation planning are difficult to solve margules and pressey 2000 they are a fundamental part in the conservation decision process however ignoring other equally important components such as the interaction with experts for the inclusion of non quantifiable or other aspects that are hard to express as constraints or objectives may reduce their applicability in real life conservation decisions 3 visual analytics framework this section describes the design process and components of the proposed framework the design of the proposed system is the result of a collaborative effort with a variety of stakeholders including donors ecologists and conservation planners through discussions and planning with domain experts we identified key data needs tasks and design requirements the proposed framework avoids the manual processing of the attributes of each candidate parcel to determine its relative convenience with respect to other parcels in the area of interest it also consolidates the data processing visualization and optimization processes into a single intuitive tool the interaction with potential users resulted in the following functionalities of our framework data storage and downscaling stores map data for conservation planning including biological physical or socio economic attributes currently includes 12 attributes suggested by conservation planners and is scalable to further attributes the data set is categorized into land use physical geospatial and biodiversity layers input data is downscaled to the parcel scale to facilitate the calculation of the quality of land portfolios multi layer map view allows the investigation of parcel attribute values and the visualization of one of more attributes over a common area of analysis attribute selection view filters parcels whose attributes fall within a certain range of interest provides the distributions of attribute values in any selected search area and allows the user to turn on off each attribute layer on the map define the ranking order of each attribute e g higher values are preferred and filter parcels based on a ranking aggregation metric calculated using selected attributes conservation portfolio optimization allows the specification of requirements for the land purchase portfolio such as area of interest on the map desired criteria for candidate parcels objective to optimize constraints and maximum budget embeds a multicriteria optimization functionality to automatically provide land purchase recommendations and allow the user to visually interact with a solution to induce other desirable performance metrics e g landscape connectivity and compactness porfolio comparison view provides comparison tools to help portfolio managers explore their criteria of interest compare land purchase portfolios and work together to realize their final solution space we build upon previous works on multicriteria analysis and visualization integrating geographic visualization and optimization to recommend land portfolios our system is designed to support the comparison of candidate land portfolios generated between the optimization recommendation and the analyst adjustments similar to previous work our system uses a color code to visually inform the analyst on the quality of patches and land portfolios linking attribute analysis and filtering to a parallel coordinates plot instead of displaying a sequence of portfolios to illustrate the impact of parameter changes our system provides a unique visualization method to help comparing the attributes and their differences between various candidate portfolios our target users are conservation planning decision makers in the broad sense this could be an analyst assessing the ecological benefits of land patches an ecologist surveying alternatives to expand current reserves or ngos and government agencies deciding which patches of land to restore or purchase fig 1 shows a snapshot of our system and its features which is freely available at zhang et al 2021a we have deployed this system to conservation planners and our use cases demonstrate the effectiveness of optimizing their decision process given limited resources a step by step demonstration video is available at zhang et al 2021b the final product functionalities are explained in detail in sections 3 1 4 4 3 1 data storage and downscaling typical data for conservation planning is characterized by biological physical or socio economic attributes our framework includes 12 common attributes and is scalable to additional data we use the state of montana as an example to describe the properties of a typical dataset for this system and the data downscaling steps table 1 describes the used datasets and their attributes that were chosen by conservation experts our system supports a wide variety of shapefiles geotiffs open street map layers among other types including conservation portfolios built in other tools e g marxan as long as they are compatible with the shapefiles in our system we note that our system is flexible to any geographical data where users only need to select a base layer for analysis typically the parcel layer would be used for this purpose as this is the level at which land can be purchased once the base spatial unit is chosen attributes are aggregated or dis aggregated through a downscaling step to conform to the level of spatial granularity under analysis for each data category we use different processing rules to derive the corresponding attribute s other than cost which is directly provided in the parcel shapefile dataset we downscale the remaining datasets to calculate the parcel level attributes we calculate the distances to the existing protected areas metro area highway and hydrology areas and aggregate the hii and other biodiversity attributes some conservation attributes measure the distance from a parcel to a feature of interest in our dataset examples include pa ma hw and hy which require us to calculate the distance from the parcel to the areas described in the attribute datasets we first discretize each dataset into 30 30 m 2 patches a request from our conservation planning partners which will be later used to calculate the attributes of the larger sized parcels parcels can be different in shape and size and there are a variety of geographic aggregation methods that can be employed to calculate their attributes out of the patch attributes unwin 1996 then we calculate the distance from the center of a patch to its nearest feature of interest from there we can aggregate all patches that fall within a parcel using min max average or other aggregation functions in our system we use the average value of all patches within a parcel other attributes focus on measurements and estimates from sensors reports and other sources examples of these attributes include tree bird fish and other attributes in the biodiversity layer we overlay the parcels onto the datasets and perform an aggregation operation to estimate the parcel attributes 3 2 multi layer map view in order to support the multicriteria analysis during the decision making process we incorporated a multi layer map view to visualize each attribute and their combinations over space for distance based attributes pa ma hw and hy a sequential color scheme is used the darker color means a patch is closer to the feature of interest as an illustration fig 2a shows the visualization of the distance to the metro area ma attribute the pink region is the metro area and the peripheral region around the metro area is colored based on the distance the red and blue highlighted regions in fig 2a are the parcels in the user selected region of interest for region based attributes hii cost tree fish bird am mm and rp the original datasets are overlaid on the map and colored based on their attribute values using diverging color schemes the color scheme is designed to match the nasa analysis sedac 2018 and biodiversitymapping org jenkins 2017 fig 2b shows the visualization of fish the region with redder color has higher species richness while the region with bluer color has lower species richness for this variable in the map view the user can define their conservation area by drawing a rectangle on the map once the area is selected the optimization algorithm suggests which parcels within this area to buy the red blue area seen in fig 2 the parcels are colored based on an aggregation of the parcel attributes through a ranking function see section 3 4 filtering updates the optimization algorithm s solution and other user modifications results in the selection are influenced by the attribute selection view 3 3 attribute selection view the attribute selection view integrates parallel coordinates line charts and attribute controllers see fig 1b the user can explore value distributions of attributes in the search area turn on off each attribute layer on the map define the ranking order of each attribute in the median ranking and filter attributes for median ranking and based on the attribute value on each attribute controller the user can click the top switch button see fig 1b3 to turn on off the corresponding attribute layer and mouse over the attribute name see fig 1b1 to see the explanation of the attribute and the color legend or the layer the bottom switch button see fig 1b4 is used to enable disable the filtering function of this attribute the user can still explore the value distribution of an attribute when its filtering function is disabled but interactions on disabled attributes won t impact the map view or the optimization model the triangles pointing up and down see fig 1b2 are used to decide the priority direction of the attribute value when used in the median ranking aggregation e g whether near or far proximity is desirable for example if the user wants to buy parcels near a protected area then the priority direction is non decreasing that is the user prioritizes low pa values by turning on the up triangle for the pa attribute by turning on their down triangles the user prioritizes high values of tree bird fish and am attributes the line charts and parallel coordinates display the value distribution of each attribute and support parcel filtering by attribute value such filtering is only active when the attribute s filtering function is enabled to explore attribute correlations and observe patterns of the data the user can drag the axes of the parallel coordinates to change the order of the attributes on each axis of the parallel coordinates we add a box plot to help reveal the statistical distribution of the data we use a categorical color scheme for the box plots to represent different attributes and the attribute uses the same color in the portfolio comparison view which we describe in detail in section 3 5 when the number of parcels increases it is difficult to observe the distribution on the parallel coordinates due to visual clutters therefore each attribute is also associated with a line chart where the x axis represents the attribute value and the y axis represents the frequency of the attribute value the line chart is adjacent to each axis in the parallel coordinate plot and is used to show the value distribution of both the original data and the filtered data to filter parcels brush interaction is supported on the axes of the parallel coordinates as well as on the x axis of the line chart parcels removed from the filtering will be grayed out on the parallel coordinates while brushed parcels are highlighted in blue on the top line chart the black line shows the value distribution of all parcels in the search area and once filtered a blue line is used to display the value distribution of the filtered parcels and the original line will become gray in our system all the interactions are coordinated with the map view once attributes are selected the parcels in the user selected area will be colored based on their median ranking order the legend for the median ranking results is in the left bottom of the map the result of the median ranking depends on which parcels are selected and which filters have been applied to the data the attributes of the parcels in the selected area are then used to generate a potential conservation portfolio 3 4 conservation portfolio optimization once the region and attributes are defined our system employs a mathematical programming model to identify an optimal portfolio of patches for conservation we define p as the set of candidate parcels eligible for purchase and a as the set of attributes of interest we assume that all attributes are or can be converted to numerical values and that all attributes are available for each parcel we denote the value of attribute j a for parcel i p by a ij depending on the discretization of the area of analysis chosen by the user the number of candidate parcels may be very large to reduce the computational effort in our system we implement two pre processing techniques both aim to reduce the set of candidate parcels by ignoring some that are not of interest for the decision maker the first technique is based on user defined attribute filters in this case the user explicitly sets thresholds for a subset of the attributes and the system discards those parcels with attributes violating the thresholds mathematically we denote the set of attributes with threshold values as a a and the corresponding lower and upper threshold values by a j and a j for attribute j a respectively using these values the set of eligible parcels can be calculated as p i p a j a i j a j j a the a and a parameters are calculated via user interactions with the map and the attributes value distribution depending on the magnitude and meaning of an attribute it may not be intuitive for the user to specify the a and a parameters we also determine the set of eligible patches using a ranking based procedure that describes the relative performance of a parcel with respect to other parcels parcels are sorted in non decreasing order based on each attribute and then ranked such that r ij r kj if a ij a kj where r ij 1 p is the rank of candidate parcel i p on attribute j a in other words the smaller the value of an attribute the higher the ranking of the parcel on that attribute i e closer to 1 in the case where attributes with larger values are preferred e g distance to human settlements then the attribute values are sorted in non increasing order and ranked such that r ij r kj if a ij a kj if two parcels have the same value on a particular attribute then their ranking on that attribute is the same i e r ij r kj if a ij a kj the ranking describes the parcel s relative performance on each attribute we aggregate such rankings into a single number r i using the median value of the rankings across attributes in other words r i median r i 1 r i a i p we add the aggregated rank to the set of attributes for each parcel allowing the user to specify more intuitive filters on the r values for instance the user can choose to discard parcels that are not among the top k parcels according to the median ranking by setting the corresponding a parameter to k we use median ranking aggregation because among other properties it eliminates the effect of extreme r values and it can be computed efficiently sculley 2007 our system is flexible to accommodate any other ranking aggregation model for a review on ranking aggregation readers are referred to sculley 2007 lin 2010 and ailon et al 2008 and the references therein to find an optimal set of parcels for conservation we use an integer programming model with variables x i where x i 1 if parcel i is recommended for purchase and x i 0 otherwise i p the model constraints represent conditions that a portfolio of parcels must satisfy as opposed to the individual parcel conditions described in the pre processing analysis these include land purchase budget minimum population area to protect among others we use linear constraints reflecting that the aggregated value of an attribute for the selected parcels must be less than or greater than or equal to a threshold value we denote by a and a the set of attributes with a less than or equal to and greater than or equal to constraints respectively we use b j as the threshold value for attribute j a a note that not all attributes need to be included in such constraints which means that a a and a a we pay special attention to the cost and area of each parcel which we denote by c i and α i i p respectively our optimization problem maximizes the total purchase area 1a subject to attribute constraints 1b 1c and variable type constraints 1d the optimal purchased area will be a subset of the available area given that the purchasing cost will be part of a with a corresponding b parameter equal to the budget available for land purchases 1a max i p α i x i 1b s t i p a i j x i b j j a 1c i p a i j x i b j j a 1d x i 0 1 i p an alternative model minimizes the total purchase cost subject to constraints 1b 1d in this case the area will be part of a with a corresponding b parameter equal to a minimum required area to conserve mathematically this problem can be written as min i p c i x i subject to 1b 1d although some of the constraints in our models may indirectly induce some landscape attributes e g landscape connectivity by selecting the distance to existing protected areas as an attribute the conservation portfolio produced by our models may not satisfy some those landscape requirements this is because of the complexity and computing demand of enforcing such constraints for any arbitrary sized area selected by the user instead our system allows the user to interactively modify an existing solution through clicks on the map to induce these landscape features this allows the exploration of solutions that are infeasible for the optimization model but that provide a good compromise between the ecological values gained and the extra cost required the user is allowed to add or remove attribute constraints as well as select the objective function to optimize maximize the protected area or minimize the purchasing cost our models produce an optimal purchasing plan that satisfies all the selected attribute constraints at the same time using the optimal values of the decision variables denoted by x i we define an optimal conservation portfolio as p i p x i 1 these optimal portfolios are displayed for further user analysis the analyst interacts with the optimization model through the configuration view see fig 1c the analyst can filter parcels based on their median ranking and sets the constraints and objective function of the optimization model the analyst can also save the current portfolio to the comparison view for further exploration and comparison the median ranking slider shows the rankings of all selected parcels and the analyst can drag the two ends of the slider to remove low ranked or high ranked parcels the filtering tool changes the parcels used in the automatic optimization algorithm the sliders under constraints are used to set the constraints for the optimization model currently our system is able to answer the questions what is the largest total area that can be protected given a fixed budget and other ecological constraints and what is the least expensive set of parcels to protect with an area of at least b km2 while satisfying other ecological constraints therefore the analyst needs to select one variable between cost and area to be the constraints and leave the other to be the objective function our mathematical algorithm and system can support multiple constraints for both cost and area the maximum value of the slider updates to represent the sum of the cost and area of user selected parcels dragging the ends of the slider can change the value range we set for the constraint to set the objective function the analyst can choose either to maximize or minimize the variable when the configuration is done the analyst can click on the optimization button to run the algorithm for an easy comparison of multiple optimal portfolios under different right hand sides of the constraints the constraint value from the previous run of the optimization algorithm is recorded in the slider 3 5 portfolio comparison view multiple land purchasing portfolios may satisfy the planners requirements under different attribute priorities the analyst can make different modifications on top of the same suggested portfolio or change the selected parcels fig 3 shows our portfolio comparison view which uses a multiple portfolio visualization to display all saved portfolios each portfolio visualization has three visual components the map screenshot the optimization setting and the attribute pie the map screenshot represents the exact status of the map view when the portfolio is saved and it records the details of the parcel selection in the portfolio the optimization setting uses the same design as the lower right legend on the map to present the constraint and objective function for the optimization algorithm the attribute pie is a glyph designed to visualize the attribute distribution of selected parcels under the setting of each portfolio and allow the analyst to compare their customized portfolio to that suggested by the optimization model the pie shows all attributes with evenly split sectors and each attribute is assigned one color this is because even if not all the attributes are used to filter parcels their value distribution may need to be considered in the final decision making process to compare the influence of an attribute value in the portfolio three circles with different radius are used the outermost circle represents all the parcels in the search area the middle circle represents the parcels suggested by the optimization model and the inner circle represents the analyst selected parcels which are those finalized in the portfolio the three circles are arranged into the same value scale which ranges from the minimum to the maximum of all parcels in the search area the range of the outermost circle for the middle and the innermost circle the value range is also emphasized with a brushed color arc within this color arc a box plot visualizes the attribute value s statistical distribution we use a brushed color arc on the outermost circle to indicate the attribute value range that was used by the analyst to filter the attribute if there is no such colored arc it means the analyst did not filter on this attribute the box plot on the outermost circle shows the quartiles of the attribute value with these filtered parcels by using these glyphs analysts can compare the attribute distribution of filtered parcels suggested parcels and the user selected parcels to explore how the choice of parcels affects the attribute distribution analysts can also directly compare different portfolios allowing multiple analysts to provide input and serving as a mechanism for both provenance and analysis the map screenshots of the portfolios provide an overview of the differences between search areas and parcel selection a black vertical line across all saved portfolio appears when the analyst mouses over the optimal setting view so that the analyst can easily compare the value of the constraint and objective function cost and area for these portfolios the analyst can also compare the attribute value distribution of different portfolios by mousing over one arc of an attribute to turn on the comparison signs of this attribute for all portfolios in this case the average attribute values are compared both between the parcels represented of the three circles within one portfolio and also between the parcel selections represented by the circles of other portfolios the reference circle arc is colored gray if the average value equals the reference value it shows an sign when the average value of the parcels represented by the circle is larger than the reference value a sign will appear and when the value is smaller a sign will appear 4 case studies in this section we illustrate the use of our tool for the selection of conservation areas in montana usa the state of montana has a long wildlife conservation tradition dating back to 1895 when the game and fish commission was established brownell 1987 the evolution of wildlife legislation in this state reflects a serious commitment to the protection of wildlife yet less than 3 7 of its total area is designated as a wilderness protection area furthermore most of the currently designated protected areas are composed of isolated mountain ranges clustered in a limited number of counties therefore there is a need to complement existing protected areas by establishing new protection zones in counties that have limited designated wilderness areas and establishing corridors that facilitate movement and gene flow among wildlife populations living in isolated conservation areas hodgson et al 2009 4 1 multi species conservation scenarios for the judith gap in montana in this case study a conservation planner the analyst hereafter selects a set of areas to acquire or restore near the judith gap in wheatland county this gap represents a region of unprotected land between protected areas in the little belt mountains in the west and the big snowy mountains in the east the analyst s overall goal is to identify the largest possible total area to purchase subject to a budget constraint while at the same time maximizing the number of terrestrial vertebrate species under protection within the corridor there is evidence showing that in many instances the negative effects of human populations on protected areas decreases with distance to population centers mcdonald et al 2009 in this case our system allows the analyst to visually explore a variety of attributes related to human use using the distance to metro area ma layer as shown in fig 4b it is possible to assess the spatial relationship between existing protected areas and urbanized centers the figure shows how protected areas are generally distant from major urban centers the highway layer hw as shown in fig 4c illustrates how major roads may influence accessibility to protected areas the figure shows how highway 91 is located between the two major conservation areas that the analyst seeks to connect alternatively the analyst can visualize a human influence index hii as shown in fig 4d which summarizes in a scale from 0 to 64 the overall influence of humans on terrestrial ecosystems this view shows that areas near the metro and highway areas usually have high human influence index meeting cost constrains is a central goal of conservation planning because resources for conservation are always limited naidoo et al 2006 fig 4 e shows the spatial distribution of costs and its relationship with existing protected areas or other attributes the cost layer shows that the average cost to purchase land near the big snowy mountains is higher than that near the little belt mountains after the exploratory spatial analysis of existing protected areas human influence and cost the analyst can define a candidate region between the big snowy mountains and little belt mountains conservation areas in fig 4f using the drawing tool protecting sites that are closer to existing conservation areas both east and west will encourage connectivity therefore the analyst selected pa as an attribute for the ranking calculation and the optimization model as well as terrestrial vertebrate species richness which includes mammals birds reptiles and amphibians because the overall goal is to promote movement and gene flow of wildlife species among existing conservation areas mammal conservation is a regional conservation priority thus using the brushed axis the analyst imposed a constraint to include sites that have a total richness of mammals index of at least 54 species the model s goal is to maximize the area under protection while adding a budget as a constraint as it is a common practice in conservation planning cabeza and moilanen 2001 williams et al 2005 acquiring the whole candidate region would cost 47m which is higher than conservation budgets in many instances therefore the analyst sets a target total cost of 0 10 000 000 to test if this budget range allows to meet the conservation goal of acquiring land to connect the conservation areas the prescribed solution is shown in fig 5a the figure shows that the current budget allows purchasing a limited number of isolated patches that will contribute little to the overall goal of promoting connectivity the budget is increased to 15 000 000 obtaining the area in fig 5b which better promotes connectivity between the two existing protected areas this budget level also allows connecting the southern portion of the little belt mountains as is common in conservation planning the prescribed optimal set requires manual refinement by the analyst to incorporate expert opinion on attributes that are not necessarily accounted for in the optimization model for example a land parcel may be already zoned for other uses it may be prone to fire or flood disturbance or it may be spatially isolated and therefore not desirable as a conservation unit this manual refinement is a key component of the conservation planning process that is lacking in many computational applications and is intuitively incorporated in this tool given its spatial nature in these examples the analyst replaced isolated regions with little contribution wildlife movement with areas in the west that ensure connection to the big snowy mountains because the map reports the total selected area and cost after any analyst action e g selection or removal of a parcel the analyst was able to select an area within the given budget while using the manual refinement tool the customized portfolio in fig 5c results in a set of areas to protect of 564 km2 and a cost of 16 142 555 although this solution is not optimal i e the optimal solution recommends the purchase of 589 km2 within the same budget it reflects the complementary insight of the mathematical model and expert judgment based on attributes such as landscape connectivity that are not included in the mathematical model we compare the three portfolios in the left part of fig 5 using the spatial and non spatial information the first two examples have the same search area and different budgets the customized portfolio in the second example consists of a larger area within its budget than that in the first example which exceeds its budget as shown in fig 5ab the screenshot of the map shows how the parcels of each portfolio distribute besides the difference of constraints and goals reached by the portfolios the change on the distribution of each attribute is visualized on the arcs in fig 5ab we hover the inner arc to get the comparison result of the average attribute value among different portfolios to show the hovered result of each attribute we list five attributes we concern in the right part of fig 5ab we observe that the customized portfolio in the second example has a higher average richness of amphibian bird and tree species moreover it consists of a larger area within its budget to connect the two protected areas the analyst decides that the second portfolio is better than the first one with the same budget we generate the third portfolio based on a larger search area fig 5bc shows the comparison result of the second and the third portfolios the third portfolio consists of a larger area within the same budget to connect three protected areas in addition it has a higher average richness of reptile species and lower cost based on our analysis from fig 5 the analyst selects the third portfolio as the final choice the comparison result of the customized and suggested portfolios in the third example which is represented near the inner and middle arc gives more evidence to support the analyst s decision in fig 5bc the average richness of mammal tree and bird species is higher in the customized portfolio 4 2 creating a protected area in montana s park and sweet grass counties in this section we illustrate the creation of a protected area at the boundary of montana s park and sweet grass counties between highways 89 90 191 and 371 the region of interest consists of federal land and other unprotected areas and is within 100 mi from urban areas such as bozeman livingston big timber and white sulphur springs as well as other unincorporated communities while in the first case study we were interested in designing a conservation area distant from areas of human influence in this case study we have an opposite goal recent studies argue for a positive role of nature parks and protected areas close to human population centers more et al 1988 proximity to natural areas has been associated with improved mental health sturm and cohen 2014 and positive attitudes towards nature lin et al 2014 therefore in this case the analyst is interested in creating conservation areas that promote the protection of biodiversity while at the same time being accessible by the community in addition to the mammals reptiles amphibians and bird richness layers the analyst includes the distance to metropolitan areas and the distance to highway as attributes in non decreasing order using the attribute analysis view in this way areas closer to highways and metro areas are given a higher preference using the selected attributes fig 6 a shows the initial ranking of areas within the region of interest this ranking combines both biological and geographical features because the cost of purchasing the whole region of interest is prohibitively high 32m the analyst decided to exclude from the analysis such areas whose median ranking is larger than 5 for the selected attributes in other words discards those areas that are not ranked in the top five in at least half of the selected attributes this was done using the pre processing slider in the optimization configuration panel which reduced the area from 863 km2 to 420 km2 with an updated total cost of 11 5m see fig 6b the optimization model s goal is to minimize the total purchasing cost subject to a minimum protected area of 300 km2 the results of this baseline scenario are shown in fig 6c the size of the optimal area is 299 km2 with a total cost of 5 27m this area is neither connected nor compact having some isolated parcels and gaps inside the main cluster of selected areas to improve the geographical properties of the selected area the analyst manually induced these properties using the point and click feature of our system ultimately producing the area shown in fig 6d in this case the size of the analyst selected area is 291 km2 with a total cost of 5 6m regarding the ecological features the attribute comparison in fig 6e shows that the analyst selected landscape has a higher average richness for mammals reptiles and amphibians but not birds in this case the increase in some species coverage as well as the connectivity and compactness properties of the resulting landscape are achieved at the expense of a higher land purchase cost with respect to the baseline scenario 370k 5 conclusions and future work we propose a visual analytics framework to help conservation planners and scientists to explore compare and modify conservation portfolios under a variety of constraints to explore the candidate parcels our system proposes the multi layer map view and the parallel coordinates based attribute analysis view the suggested portfolios and the user defined portfolios are generated based on an optimization model and users domain knowledge the comparison between these portfolios is supported by the portfolio comparison view using our system analysts can incorporate their decision preferences and add selection attributes that are not easily incorporated as constraints or objectives or that delay the construction of a portfolio given the resulting model complexity currently our optimization model is fast for moderately sized landscapes and allows the construction of what if scenarios almost in real time our framework has been validated by conservation experts through two case studies which demonstrate how our framework can help analysts to generate conservation portfolios for different goals under a variety of constraints moreover our system has been received design feedback from multiple conservation experts including two co authors and four external partners although the feedback received was generally positive some limitations have been identified for future work specifically analysts appreciated the option to compare portfolios however more automation for supporting detailed comparison could improve the analysis process the analysts also noted that while the framework is flexible to the underlying optimization approach an api that would allow users to directly integrate their own optimization routines could greatly enhance their workflow a possible avenue is to explore alternative multi objective approaches to explore the trade off between objectives in the portfolio optimization see e g miettinen 2012 and sawaragi et al 1985 for alternatives further work will focus on the automatic comparison of candidate portfolios and add customized algorithms to induce other spatial properties to the framework in case the user decides to use them e g connectivity and compactness an interesting conjecture is whether adding human interaction with the optimization helps with the run time issues when spatial properties are enforced further studies exploring the tradeoffs between human input and ability to explore reasonable solutions is an interesting future direction as of now the analyst can manually load a candidate conservation portfolio for further analysis using a shapefile or a file specifying whether a parcel is selected we will add modifications in this aspect to facilitate the upload and compatibility check of a candidate portfolio as this will allow our system to complement the analysis of other existing tools like marxan and zonation although our framework focuses on conservation planning decisions it can be extended to other spatial problems including electoral districting location of urban parks and land use planning such applications will require the proper data inputs and specification of the related optimization problems acknowledgment this work is supported by the national science foundation grant numbers 1350573 1639227 1740042 and 2047961 authors would like to thank penny langhammer curtis freese lance craighead wes sechrest and karl burkart for their advice and valuable comments we also thank the anonymous reviewers and associate editor whose constructive comments helped us improve the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25736,approximately 15 of global land is currently in some state of protection recent conservation research suggests the need for a drastic increase of protected lands by 2050 in order to reach this target an additional 35 of lands need to be conserved or restored in a cost effective and time efficient manner in order to support the resiliency of our planet and its climate while many individuals and foundations continue raising much needed funding for the environment the development of conservation portfolios is a complex multi dimensional task agencies have limited resources for investing in new conservation areas and have differing priorities for conservation in terms of species land cover human activities etc we present an interactive conservation portfolio development system that combines visualization multicriteria analysis optimization and decision making that enables conservation planners and scientists to efficiently construct compare and modify conservation portfolios under multiple constraints keywords conservation planning geographic visualization decision support systems visual analytics discrete optimization 1 introduction biodiversity is declining at rapid rates due to human driven habitat loss and landscape deterioration stokstad 2010 human activities have resulted in species extinctions at 10 100 times normal background extinction levels sala et al 2000 pimm et al 1995 this rapid biodiversity decline threatens the provision of key ecosystem services such as food clean water and crop pollination resulting in negative consequences for economies and human health mace et al 2012 therefore protecting remaining natural areas is fundamental to preserve biodiversity and to mitigate the negative consequences of ongoing environmental change johnson et al 2017 while 15 of the earth is in some kind of protection belle et al 2018 this is still insufficient due to substantial gaps in land coverage and increasing threats rodrigues et al 2004 recent studies suggest the need of a drastic increase in protected lands by 2050 to maintain current rates of resource extraction watson and venter 2017 dinerstein et al 2019 this ambitious goal contrasts with the limited resources available to local institutions to design and implement networks of protected areas bicknell et al 2017 conservation biologists apply systematic conservation planning approaches to design protected area networks that are cost effective while meeting conservation goals this systematic process is composed of six steps that include 1 biodiversity data collection and analysis 2 identification of conservation goals 3 analysis of current conservation areas 4 identification of a set of additional areas 5 implementation of proposed conservation actions and 6 preservation of required conservation values margules and pressey 2000 out of these we focus on step 4 the identification of additional conservation areas one of the most challenging steps in the conservation planning process these conservation areas are selected with multiple conservation goals in mind such as maximizing biodiversity representation while attenuating future threats and remaining within a limited budget luck et al 2012 this is a complex selection process that if performed using inadequate quantitative tools may result in landscape or seascape portfolios that are not optimal in terms of their budget and priorities therefore the success of systematic conservation planning rests in part in the development of appropriate data driven methodologies for designing protected area networks at the regional level williams et al 2005 web based geographic information systems wb gis provide an ideal setting to translate the result of complex spatial mathematical models used in systematic conservation planning into simple qualitative visual scenarios dragicevic 2004 these wb gis can summarize multiple layers of information allowing planners to analyze various future hypothetical scenarios rao et al 2007 while multiple mathematical models are available to prioritize areas for conservation sarkar et al 2006 moilanen et al 2009 designing a network of protected areas requires the quantification visualization and adjustment of multiple hypothetical scenarios almost simultaneously tress and tress 2003 pettit et al 2011 in this context typical questions faced by conservation analysts include what areas should be selected as part of a network of protected areas to have all species of conservation concern under protection while minimizing the acquiring costs if we decrease the budget by 10 which areas should be protected what happens if instead we increase the budget by 5 therefore there is a need for wb gis applications for conservation planning that combine cost optimization with efficient visualization tools that can provide alternative future scenarios in real time portman 2014 in this paper we present an interactive conservation portfolio development system that combines visualization multicriteria analysis optimization and decision making that enables conservation planners and scientists to explore different land purchasing portfolios under a variety of constraints in real time our system incorporates a multi layer map view a parallel coordinates attribute view a control area for optimization modeling and a multiple portfolio visualization for solution comparison to support automatic portfolio optimization we implemented a median ranking algorithm to allow parcel filtering by an aggregated indicator of all the attributes and an integer programming model to generate land purchase recommendations given user defined constraints and objective function the visual analytics system is designed to support the efficient selection of conservation areas by enabling portfolio generation and interactive modification multiple land portfolios can be generated and saved for comparison our system complements the existing body of tools by providing new visual analytical and mathematical features while also allowing loading of a shape compatible conservation plan obtained with any other tool for further visual analysis from the software systems perspective we propose a novel combination of visualization components where our design has focused on featuring credibility saliency and legitimacy white et al 2010 the software tool itself serves as a boundary object to enable decision making our design ranges from providing detail on demand for the data source to enable analysts to determine credibility of data layers interactive selection of optimization criteria and provenance analysis specifically for supporting provenance analysis and comparison we propose new visualization designs to capture different portfolios and provide comparison between them along with novel integration of techniques and the proposal of a visualization design we have also designed a pre processing scheme to match data across different levels of granularity our down sampling technique allows data comparisons at a high resolution level and supports land purchases which can only occur at the parcel level the human machine combination is also innovative where our framework is designed to present an optimal solution within the problem formulation however the problem formulation needed to be computationally efficient for real time exploration by providing an optimal conservation portfolio as a first pass we allow users to refine their choices in a human machine teaming process our system enables conservation planners to develop consecutive portfolios in real time and adjust the outputs of the multiple criteria optimization selections from an optimization perspective our proposed approach uses the analyst s preferences to drastically reduce the problem size by supporting interaction with the optimization results planners can utilize species specific knowledge to enforce different landscape features i e connectivity compactness corridor width which can be challenging for automatic optimization either due to the difficulty in representing the corresponding constraints or the computational complexity of the resulting model although we propose a simple and fast optimization model to support land acquisition decisions the proposed system can accommodate the results from other spatial models as a result our system can be seen as a visualization framework that supports user interaction with an optimal solution to our knowledge currently there is no tool available with the proposed features to support conservation decisions 2 related work our system is designed to support decision making through multicriteria analysis and solution comparison in this section we summarize previous work in visual analytics for decision making and multicriteria optimization for conservation planning recent visualization work has focused on how to best display multiple attributes for analysis a key component of multicriteria analysis turkay et al 2014 explored the geographic variation of multivariate data and developed attribute signatures consisting of dynamically generated graphs to summarize the change of statistics over a sequence of geospatial data selection pajer et al 2017 proposed weightlifter a technique that allows the exploration of weight space with up to ten criteria and helps to explore the sensitivity of candidate solutions to the change of weights weng et al 2018b designed a visual analytics system reach which helps analysts identify their ideal home given multiple purchasing constraints common amongst many of these systems are the use of parallel coordinates plots and a wide variety of extensions to parallel coordinate plots systems extending the parallel coordinated plots e g lind et al 2009 johansson et al 2005 rosenbaum et al 2012 xie et al 2017 provided a means to easily explore multivariate data other works have focused more on supporting analysis and decision making through the integration of interactive models afzal et al 2011 developed a decision support environment to evaluate disease control strategies by predicting the course of an outbreak and analyzing the response measures the severity of the epidemic is visualized by different color intensities on the map and a custom split timeline is used to show the solution path konev et al 2014 proposed an automatic simulation based approach for flood management the decision trees are automatically generated and visualized by clustered timelines rinner 2007 developed a geographic visualization system to support multi criteria decision making an index rank for each tract is calculated and users can explore attributes through a linked parallel coordinate plot similar to the work of rinner 2007 cassol et al 2017 proposed a framework to explore the optimal evacuation plan for crowd egress based on multiple factors which were taken as input by the proposed metric to calculate the optimal plan in both systems interactive optimization methods are not fully considered these systems support multi criteria analysis through interactions with a parallel coordinate plot and quality indices similar to our use of median ranking however portfolio comparisons and interaction with the optimization results are limited other major issues underlying such decision support systems are the mechanisms used to compare across candidate solutions the work by gleicher 2018 summarized the basic designs of comparison into three categories juxtaposition i e which places the compared items are in different screen spaces superposition i e which places the compared items fit into the same screen space and explicit encoding i e visualization of the relationship between the compared items kehrer et al 2013 and munzner et al 2003 utilized juxtaposition design for their comparisons of bar charts lists and trees dasgupta et al 2015 combined juxtaposition and superposition for climate model comparison law et al 2018 developed duet a visual analytics system for pairwise comparison integrating all three categories duet uses visualizations and textual descriptions to explain the recommended object groups which are similar to or different from the user specified object with a focus on the similarity and difference weng et al 2018a proposed a spatial ranking visualization technique to explore and analyze ranking datasets and annotate the cause of the ranking with spatial context which involves the three design categories of comparison from the optimization perspective multi criteria analysis and modeling have been integrated in a number of visual analytics systems for domains ranging from epidemiology to emergency response however little work in the visual analytics community has focused on conservation planning conservation planning requires the integration of optimization algorithms for conservation portfolios given the myriad of possible parcel configurations available these conservation portfolios must allocate resources efficiently while considering current and future threats and their influence on the biodiversity assets the problem of designing natural reserves has received considerable attention since the 1980s kirkpatrick 1983 cocks and baird 1989 mostly through the use of exact optimization models ando et al 1998 church et al 1996 polasky et al 2001 sefair et al 2017 acevedo et al 2015 and heuristic approaches pressey et al 1997 arthur et al 1997 margules et al 1988 the use of operations research techniques in this area have become more prevalent in recent years including deterministic and stochastic approaches see moilanen et al 2009 for a comprehensive review moreover these methodological efforts have evolved into free software designed to support conservation planning decision making processes e g zonation and marxan lehtomäki and moilanen 2013 ball et al 2009 although available tools cover several pressing issues in conservation problems some contemporary challenges are still unsolved some of the existing approaches focus on cost minimization subject to ecological outcomes ignoring the more realistic dual problems of maximizing such outcomes subject to a given budget approaches that optimize the ecological performance of the conservation portfolio approximate the quality of candidate patches by species representation i e whether a species is present in a patch and other single static patch attributes toregas and revelle 1973 underhill 1994 williams and revelle 1996 camm et al 2002 moilanen et al 2009 ignoring the multiobjective nature of the conservation decisions other works focus on desirable geographical properties of protected areas such as landscape connectivity önal and briers 2006 dilkina and gomes 2010 dissanayake et al 2012 jafari and hearne 2013 and compactness önal and briers 2002 nalle et al 2002 dissanayake et al 2012 jafari and hearne 2013 but ignore the subjacent ecological processes the majority of the works studying connected and compact reserves are mixed integer programming models that are difficult to solve for realistic size instances and that provide a single solution i e a single connected and compact set of parcels to purchase without the visual support analysts cannot easily modify an existing solution to incorporate expert knowledge and other attributes not included in the optimization model although optimization models in conservation planning are difficult to solve margules and pressey 2000 they are a fundamental part in the conservation decision process however ignoring other equally important components such as the interaction with experts for the inclusion of non quantifiable or other aspects that are hard to express as constraints or objectives may reduce their applicability in real life conservation decisions 3 visual analytics framework this section describes the design process and components of the proposed framework the design of the proposed system is the result of a collaborative effort with a variety of stakeholders including donors ecologists and conservation planners through discussions and planning with domain experts we identified key data needs tasks and design requirements the proposed framework avoids the manual processing of the attributes of each candidate parcel to determine its relative convenience with respect to other parcels in the area of interest it also consolidates the data processing visualization and optimization processes into a single intuitive tool the interaction with potential users resulted in the following functionalities of our framework data storage and downscaling stores map data for conservation planning including biological physical or socio economic attributes currently includes 12 attributes suggested by conservation planners and is scalable to further attributes the data set is categorized into land use physical geospatial and biodiversity layers input data is downscaled to the parcel scale to facilitate the calculation of the quality of land portfolios multi layer map view allows the investigation of parcel attribute values and the visualization of one of more attributes over a common area of analysis attribute selection view filters parcels whose attributes fall within a certain range of interest provides the distributions of attribute values in any selected search area and allows the user to turn on off each attribute layer on the map define the ranking order of each attribute e g higher values are preferred and filter parcels based on a ranking aggregation metric calculated using selected attributes conservation portfolio optimization allows the specification of requirements for the land purchase portfolio such as area of interest on the map desired criteria for candidate parcels objective to optimize constraints and maximum budget embeds a multicriteria optimization functionality to automatically provide land purchase recommendations and allow the user to visually interact with a solution to induce other desirable performance metrics e g landscape connectivity and compactness porfolio comparison view provides comparison tools to help portfolio managers explore their criteria of interest compare land purchase portfolios and work together to realize their final solution space we build upon previous works on multicriteria analysis and visualization integrating geographic visualization and optimization to recommend land portfolios our system is designed to support the comparison of candidate land portfolios generated between the optimization recommendation and the analyst adjustments similar to previous work our system uses a color code to visually inform the analyst on the quality of patches and land portfolios linking attribute analysis and filtering to a parallel coordinates plot instead of displaying a sequence of portfolios to illustrate the impact of parameter changes our system provides a unique visualization method to help comparing the attributes and their differences between various candidate portfolios our target users are conservation planning decision makers in the broad sense this could be an analyst assessing the ecological benefits of land patches an ecologist surveying alternatives to expand current reserves or ngos and government agencies deciding which patches of land to restore or purchase fig 1 shows a snapshot of our system and its features which is freely available at zhang et al 2021a we have deployed this system to conservation planners and our use cases demonstrate the effectiveness of optimizing their decision process given limited resources a step by step demonstration video is available at zhang et al 2021b the final product functionalities are explained in detail in sections 3 1 4 4 3 1 data storage and downscaling typical data for conservation planning is characterized by biological physical or socio economic attributes our framework includes 12 common attributes and is scalable to additional data we use the state of montana as an example to describe the properties of a typical dataset for this system and the data downscaling steps table 1 describes the used datasets and their attributes that were chosen by conservation experts our system supports a wide variety of shapefiles geotiffs open street map layers among other types including conservation portfolios built in other tools e g marxan as long as they are compatible with the shapefiles in our system we note that our system is flexible to any geographical data where users only need to select a base layer for analysis typically the parcel layer would be used for this purpose as this is the level at which land can be purchased once the base spatial unit is chosen attributes are aggregated or dis aggregated through a downscaling step to conform to the level of spatial granularity under analysis for each data category we use different processing rules to derive the corresponding attribute s other than cost which is directly provided in the parcel shapefile dataset we downscale the remaining datasets to calculate the parcel level attributes we calculate the distances to the existing protected areas metro area highway and hydrology areas and aggregate the hii and other biodiversity attributes some conservation attributes measure the distance from a parcel to a feature of interest in our dataset examples include pa ma hw and hy which require us to calculate the distance from the parcel to the areas described in the attribute datasets we first discretize each dataset into 30 30 m 2 patches a request from our conservation planning partners which will be later used to calculate the attributes of the larger sized parcels parcels can be different in shape and size and there are a variety of geographic aggregation methods that can be employed to calculate their attributes out of the patch attributes unwin 1996 then we calculate the distance from the center of a patch to its nearest feature of interest from there we can aggregate all patches that fall within a parcel using min max average or other aggregation functions in our system we use the average value of all patches within a parcel other attributes focus on measurements and estimates from sensors reports and other sources examples of these attributes include tree bird fish and other attributes in the biodiversity layer we overlay the parcels onto the datasets and perform an aggregation operation to estimate the parcel attributes 3 2 multi layer map view in order to support the multicriteria analysis during the decision making process we incorporated a multi layer map view to visualize each attribute and their combinations over space for distance based attributes pa ma hw and hy a sequential color scheme is used the darker color means a patch is closer to the feature of interest as an illustration fig 2a shows the visualization of the distance to the metro area ma attribute the pink region is the metro area and the peripheral region around the metro area is colored based on the distance the red and blue highlighted regions in fig 2a are the parcels in the user selected region of interest for region based attributes hii cost tree fish bird am mm and rp the original datasets are overlaid on the map and colored based on their attribute values using diverging color schemes the color scheme is designed to match the nasa analysis sedac 2018 and biodiversitymapping org jenkins 2017 fig 2b shows the visualization of fish the region with redder color has higher species richness while the region with bluer color has lower species richness for this variable in the map view the user can define their conservation area by drawing a rectangle on the map once the area is selected the optimization algorithm suggests which parcels within this area to buy the red blue area seen in fig 2 the parcels are colored based on an aggregation of the parcel attributes through a ranking function see section 3 4 filtering updates the optimization algorithm s solution and other user modifications results in the selection are influenced by the attribute selection view 3 3 attribute selection view the attribute selection view integrates parallel coordinates line charts and attribute controllers see fig 1b the user can explore value distributions of attributes in the search area turn on off each attribute layer on the map define the ranking order of each attribute in the median ranking and filter attributes for median ranking and based on the attribute value on each attribute controller the user can click the top switch button see fig 1b3 to turn on off the corresponding attribute layer and mouse over the attribute name see fig 1b1 to see the explanation of the attribute and the color legend or the layer the bottom switch button see fig 1b4 is used to enable disable the filtering function of this attribute the user can still explore the value distribution of an attribute when its filtering function is disabled but interactions on disabled attributes won t impact the map view or the optimization model the triangles pointing up and down see fig 1b2 are used to decide the priority direction of the attribute value when used in the median ranking aggregation e g whether near or far proximity is desirable for example if the user wants to buy parcels near a protected area then the priority direction is non decreasing that is the user prioritizes low pa values by turning on the up triangle for the pa attribute by turning on their down triangles the user prioritizes high values of tree bird fish and am attributes the line charts and parallel coordinates display the value distribution of each attribute and support parcel filtering by attribute value such filtering is only active when the attribute s filtering function is enabled to explore attribute correlations and observe patterns of the data the user can drag the axes of the parallel coordinates to change the order of the attributes on each axis of the parallel coordinates we add a box plot to help reveal the statistical distribution of the data we use a categorical color scheme for the box plots to represent different attributes and the attribute uses the same color in the portfolio comparison view which we describe in detail in section 3 5 when the number of parcels increases it is difficult to observe the distribution on the parallel coordinates due to visual clutters therefore each attribute is also associated with a line chart where the x axis represents the attribute value and the y axis represents the frequency of the attribute value the line chart is adjacent to each axis in the parallel coordinate plot and is used to show the value distribution of both the original data and the filtered data to filter parcels brush interaction is supported on the axes of the parallel coordinates as well as on the x axis of the line chart parcels removed from the filtering will be grayed out on the parallel coordinates while brushed parcels are highlighted in blue on the top line chart the black line shows the value distribution of all parcels in the search area and once filtered a blue line is used to display the value distribution of the filtered parcels and the original line will become gray in our system all the interactions are coordinated with the map view once attributes are selected the parcels in the user selected area will be colored based on their median ranking order the legend for the median ranking results is in the left bottom of the map the result of the median ranking depends on which parcels are selected and which filters have been applied to the data the attributes of the parcels in the selected area are then used to generate a potential conservation portfolio 3 4 conservation portfolio optimization once the region and attributes are defined our system employs a mathematical programming model to identify an optimal portfolio of patches for conservation we define p as the set of candidate parcels eligible for purchase and a as the set of attributes of interest we assume that all attributes are or can be converted to numerical values and that all attributes are available for each parcel we denote the value of attribute j a for parcel i p by a ij depending on the discretization of the area of analysis chosen by the user the number of candidate parcels may be very large to reduce the computational effort in our system we implement two pre processing techniques both aim to reduce the set of candidate parcels by ignoring some that are not of interest for the decision maker the first technique is based on user defined attribute filters in this case the user explicitly sets thresholds for a subset of the attributes and the system discards those parcels with attributes violating the thresholds mathematically we denote the set of attributes with threshold values as a a and the corresponding lower and upper threshold values by a j and a j for attribute j a respectively using these values the set of eligible parcels can be calculated as p i p a j a i j a j j a the a and a parameters are calculated via user interactions with the map and the attributes value distribution depending on the magnitude and meaning of an attribute it may not be intuitive for the user to specify the a and a parameters we also determine the set of eligible patches using a ranking based procedure that describes the relative performance of a parcel with respect to other parcels parcels are sorted in non decreasing order based on each attribute and then ranked such that r ij r kj if a ij a kj where r ij 1 p is the rank of candidate parcel i p on attribute j a in other words the smaller the value of an attribute the higher the ranking of the parcel on that attribute i e closer to 1 in the case where attributes with larger values are preferred e g distance to human settlements then the attribute values are sorted in non increasing order and ranked such that r ij r kj if a ij a kj if two parcels have the same value on a particular attribute then their ranking on that attribute is the same i e r ij r kj if a ij a kj the ranking describes the parcel s relative performance on each attribute we aggregate such rankings into a single number r i using the median value of the rankings across attributes in other words r i median r i 1 r i a i p we add the aggregated rank to the set of attributes for each parcel allowing the user to specify more intuitive filters on the r values for instance the user can choose to discard parcels that are not among the top k parcels according to the median ranking by setting the corresponding a parameter to k we use median ranking aggregation because among other properties it eliminates the effect of extreme r values and it can be computed efficiently sculley 2007 our system is flexible to accommodate any other ranking aggregation model for a review on ranking aggregation readers are referred to sculley 2007 lin 2010 and ailon et al 2008 and the references therein to find an optimal set of parcels for conservation we use an integer programming model with variables x i where x i 1 if parcel i is recommended for purchase and x i 0 otherwise i p the model constraints represent conditions that a portfolio of parcels must satisfy as opposed to the individual parcel conditions described in the pre processing analysis these include land purchase budget minimum population area to protect among others we use linear constraints reflecting that the aggregated value of an attribute for the selected parcels must be less than or greater than or equal to a threshold value we denote by a and a the set of attributes with a less than or equal to and greater than or equal to constraints respectively we use b j as the threshold value for attribute j a a note that not all attributes need to be included in such constraints which means that a a and a a we pay special attention to the cost and area of each parcel which we denote by c i and α i i p respectively our optimization problem maximizes the total purchase area 1a subject to attribute constraints 1b 1c and variable type constraints 1d the optimal purchased area will be a subset of the available area given that the purchasing cost will be part of a with a corresponding b parameter equal to the budget available for land purchases 1a max i p α i x i 1b s t i p a i j x i b j j a 1c i p a i j x i b j j a 1d x i 0 1 i p an alternative model minimizes the total purchase cost subject to constraints 1b 1d in this case the area will be part of a with a corresponding b parameter equal to a minimum required area to conserve mathematically this problem can be written as min i p c i x i subject to 1b 1d although some of the constraints in our models may indirectly induce some landscape attributes e g landscape connectivity by selecting the distance to existing protected areas as an attribute the conservation portfolio produced by our models may not satisfy some those landscape requirements this is because of the complexity and computing demand of enforcing such constraints for any arbitrary sized area selected by the user instead our system allows the user to interactively modify an existing solution through clicks on the map to induce these landscape features this allows the exploration of solutions that are infeasible for the optimization model but that provide a good compromise between the ecological values gained and the extra cost required the user is allowed to add or remove attribute constraints as well as select the objective function to optimize maximize the protected area or minimize the purchasing cost our models produce an optimal purchasing plan that satisfies all the selected attribute constraints at the same time using the optimal values of the decision variables denoted by x i we define an optimal conservation portfolio as p i p x i 1 these optimal portfolios are displayed for further user analysis the analyst interacts with the optimization model through the configuration view see fig 1c the analyst can filter parcels based on their median ranking and sets the constraints and objective function of the optimization model the analyst can also save the current portfolio to the comparison view for further exploration and comparison the median ranking slider shows the rankings of all selected parcels and the analyst can drag the two ends of the slider to remove low ranked or high ranked parcels the filtering tool changes the parcels used in the automatic optimization algorithm the sliders under constraints are used to set the constraints for the optimization model currently our system is able to answer the questions what is the largest total area that can be protected given a fixed budget and other ecological constraints and what is the least expensive set of parcels to protect with an area of at least b km2 while satisfying other ecological constraints therefore the analyst needs to select one variable between cost and area to be the constraints and leave the other to be the objective function our mathematical algorithm and system can support multiple constraints for both cost and area the maximum value of the slider updates to represent the sum of the cost and area of user selected parcels dragging the ends of the slider can change the value range we set for the constraint to set the objective function the analyst can choose either to maximize or minimize the variable when the configuration is done the analyst can click on the optimization button to run the algorithm for an easy comparison of multiple optimal portfolios under different right hand sides of the constraints the constraint value from the previous run of the optimization algorithm is recorded in the slider 3 5 portfolio comparison view multiple land purchasing portfolios may satisfy the planners requirements under different attribute priorities the analyst can make different modifications on top of the same suggested portfolio or change the selected parcels fig 3 shows our portfolio comparison view which uses a multiple portfolio visualization to display all saved portfolios each portfolio visualization has three visual components the map screenshot the optimization setting and the attribute pie the map screenshot represents the exact status of the map view when the portfolio is saved and it records the details of the parcel selection in the portfolio the optimization setting uses the same design as the lower right legend on the map to present the constraint and objective function for the optimization algorithm the attribute pie is a glyph designed to visualize the attribute distribution of selected parcels under the setting of each portfolio and allow the analyst to compare their customized portfolio to that suggested by the optimization model the pie shows all attributes with evenly split sectors and each attribute is assigned one color this is because even if not all the attributes are used to filter parcels their value distribution may need to be considered in the final decision making process to compare the influence of an attribute value in the portfolio three circles with different radius are used the outermost circle represents all the parcels in the search area the middle circle represents the parcels suggested by the optimization model and the inner circle represents the analyst selected parcels which are those finalized in the portfolio the three circles are arranged into the same value scale which ranges from the minimum to the maximum of all parcels in the search area the range of the outermost circle for the middle and the innermost circle the value range is also emphasized with a brushed color arc within this color arc a box plot visualizes the attribute value s statistical distribution we use a brushed color arc on the outermost circle to indicate the attribute value range that was used by the analyst to filter the attribute if there is no such colored arc it means the analyst did not filter on this attribute the box plot on the outermost circle shows the quartiles of the attribute value with these filtered parcels by using these glyphs analysts can compare the attribute distribution of filtered parcels suggested parcels and the user selected parcels to explore how the choice of parcels affects the attribute distribution analysts can also directly compare different portfolios allowing multiple analysts to provide input and serving as a mechanism for both provenance and analysis the map screenshots of the portfolios provide an overview of the differences between search areas and parcel selection a black vertical line across all saved portfolio appears when the analyst mouses over the optimal setting view so that the analyst can easily compare the value of the constraint and objective function cost and area for these portfolios the analyst can also compare the attribute value distribution of different portfolios by mousing over one arc of an attribute to turn on the comparison signs of this attribute for all portfolios in this case the average attribute values are compared both between the parcels represented of the three circles within one portfolio and also between the parcel selections represented by the circles of other portfolios the reference circle arc is colored gray if the average value equals the reference value it shows an sign when the average value of the parcels represented by the circle is larger than the reference value a sign will appear and when the value is smaller a sign will appear 4 case studies in this section we illustrate the use of our tool for the selection of conservation areas in montana usa the state of montana has a long wildlife conservation tradition dating back to 1895 when the game and fish commission was established brownell 1987 the evolution of wildlife legislation in this state reflects a serious commitment to the protection of wildlife yet less than 3 7 of its total area is designated as a wilderness protection area furthermore most of the currently designated protected areas are composed of isolated mountain ranges clustered in a limited number of counties therefore there is a need to complement existing protected areas by establishing new protection zones in counties that have limited designated wilderness areas and establishing corridors that facilitate movement and gene flow among wildlife populations living in isolated conservation areas hodgson et al 2009 4 1 multi species conservation scenarios for the judith gap in montana in this case study a conservation planner the analyst hereafter selects a set of areas to acquire or restore near the judith gap in wheatland county this gap represents a region of unprotected land between protected areas in the little belt mountains in the west and the big snowy mountains in the east the analyst s overall goal is to identify the largest possible total area to purchase subject to a budget constraint while at the same time maximizing the number of terrestrial vertebrate species under protection within the corridor there is evidence showing that in many instances the negative effects of human populations on protected areas decreases with distance to population centers mcdonald et al 2009 in this case our system allows the analyst to visually explore a variety of attributes related to human use using the distance to metro area ma layer as shown in fig 4b it is possible to assess the spatial relationship between existing protected areas and urbanized centers the figure shows how protected areas are generally distant from major urban centers the highway layer hw as shown in fig 4c illustrates how major roads may influence accessibility to protected areas the figure shows how highway 91 is located between the two major conservation areas that the analyst seeks to connect alternatively the analyst can visualize a human influence index hii as shown in fig 4d which summarizes in a scale from 0 to 64 the overall influence of humans on terrestrial ecosystems this view shows that areas near the metro and highway areas usually have high human influence index meeting cost constrains is a central goal of conservation planning because resources for conservation are always limited naidoo et al 2006 fig 4 e shows the spatial distribution of costs and its relationship with existing protected areas or other attributes the cost layer shows that the average cost to purchase land near the big snowy mountains is higher than that near the little belt mountains after the exploratory spatial analysis of existing protected areas human influence and cost the analyst can define a candidate region between the big snowy mountains and little belt mountains conservation areas in fig 4f using the drawing tool protecting sites that are closer to existing conservation areas both east and west will encourage connectivity therefore the analyst selected pa as an attribute for the ranking calculation and the optimization model as well as terrestrial vertebrate species richness which includes mammals birds reptiles and amphibians because the overall goal is to promote movement and gene flow of wildlife species among existing conservation areas mammal conservation is a regional conservation priority thus using the brushed axis the analyst imposed a constraint to include sites that have a total richness of mammals index of at least 54 species the model s goal is to maximize the area under protection while adding a budget as a constraint as it is a common practice in conservation planning cabeza and moilanen 2001 williams et al 2005 acquiring the whole candidate region would cost 47m which is higher than conservation budgets in many instances therefore the analyst sets a target total cost of 0 10 000 000 to test if this budget range allows to meet the conservation goal of acquiring land to connect the conservation areas the prescribed solution is shown in fig 5a the figure shows that the current budget allows purchasing a limited number of isolated patches that will contribute little to the overall goal of promoting connectivity the budget is increased to 15 000 000 obtaining the area in fig 5b which better promotes connectivity between the two existing protected areas this budget level also allows connecting the southern portion of the little belt mountains as is common in conservation planning the prescribed optimal set requires manual refinement by the analyst to incorporate expert opinion on attributes that are not necessarily accounted for in the optimization model for example a land parcel may be already zoned for other uses it may be prone to fire or flood disturbance or it may be spatially isolated and therefore not desirable as a conservation unit this manual refinement is a key component of the conservation planning process that is lacking in many computational applications and is intuitively incorporated in this tool given its spatial nature in these examples the analyst replaced isolated regions with little contribution wildlife movement with areas in the west that ensure connection to the big snowy mountains because the map reports the total selected area and cost after any analyst action e g selection or removal of a parcel the analyst was able to select an area within the given budget while using the manual refinement tool the customized portfolio in fig 5c results in a set of areas to protect of 564 km2 and a cost of 16 142 555 although this solution is not optimal i e the optimal solution recommends the purchase of 589 km2 within the same budget it reflects the complementary insight of the mathematical model and expert judgment based on attributes such as landscape connectivity that are not included in the mathematical model we compare the three portfolios in the left part of fig 5 using the spatial and non spatial information the first two examples have the same search area and different budgets the customized portfolio in the second example consists of a larger area within its budget than that in the first example which exceeds its budget as shown in fig 5ab the screenshot of the map shows how the parcels of each portfolio distribute besides the difference of constraints and goals reached by the portfolios the change on the distribution of each attribute is visualized on the arcs in fig 5ab we hover the inner arc to get the comparison result of the average attribute value among different portfolios to show the hovered result of each attribute we list five attributes we concern in the right part of fig 5ab we observe that the customized portfolio in the second example has a higher average richness of amphibian bird and tree species moreover it consists of a larger area within its budget to connect the two protected areas the analyst decides that the second portfolio is better than the first one with the same budget we generate the third portfolio based on a larger search area fig 5bc shows the comparison result of the second and the third portfolios the third portfolio consists of a larger area within the same budget to connect three protected areas in addition it has a higher average richness of reptile species and lower cost based on our analysis from fig 5 the analyst selects the third portfolio as the final choice the comparison result of the customized and suggested portfolios in the third example which is represented near the inner and middle arc gives more evidence to support the analyst s decision in fig 5bc the average richness of mammal tree and bird species is higher in the customized portfolio 4 2 creating a protected area in montana s park and sweet grass counties in this section we illustrate the creation of a protected area at the boundary of montana s park and sweet grass counties between highways 89 90 191 and 371 the region of interest consists of federal land and other unprotected areas and is within 100 mi from urban areas such as bozeman livingston big timber and white sulphur springs as well as other unincorporated communities while in the first case study we were interested in designing a conservation area distant from areas of human influence in this case study we have an opposite goal recent studies argue for a positive role of nature parks and protected areas close to human population centers more et al 1988 proximity to natural areas has been associated with improved mental health sturm and cohen 2014 and positive attitudes towards nature lin et al 2014 therefore in this case the analyst is interested in creating conservation areas that promote the protection of biodiversity while at the same time being accessible by the community in addition to the mammals reptiles amphibians and bird richness layers the analyst includes the distance to metropolitan areas and the distance to highway as attributes in non decreasing order using the attribute analysis view in this way areas closer to highways and metro areas are given a higher preference using the selected attributes fig 6 a shows the initial ranking of areas within the region of interest this ranking combines both biological and geographical features because the cost of purchasing the whole region of interest is prohibitively high 32m the analyst decided to exclude from the analysis such areas whose median ranking is larger than 5 for the selected attributes in other words discards those areas that are not ranked in the top five in at least half of the selected attributes this was done using the pre processing slider in the optimization configuration panel which reduced the area from 863 km2 to 420 km2 with an updated total cost of 11 5m see fig 6b the optimization model s goal is to minimize the total purchasing cost subject to a minimum protected area of 300 km2 the results of this baseline scenario are shown in fig 6c the size of the optimal area is 299 km2 with a total cost of 5 27m this area is neither connected nor compact having some isolated parcels and gaps inside the main cluster of selected areas to improve the geographical properties of the selected area the analyst manually induced these properties using the point and click feature of our system ultimately producing the area shown in fig 6d in this case the size of the analyst selected area is 291 km2 with a total cost of 5 6m regarding the ecological features the attribute comparison in fig 6e shows that the analyst selected landscape has a higher average richness for mammals reptiles and amphibians but not birds in this case the increase in some species coverage as well as the connectivity and compactness properties of the resulting landscape are achieved at the expense of a higher land purchase cost with respect to the baseline scenario 370k 5 conclusions and future work we propose a visual analytics framework to help conservation planners and scientists to explore compare and modify conservation portfolios under a variety of constraints to explore the candidate parcels our system proposes the multi layer map view and the parallel coordinates based attribute analysis view the suggested portfolios and the user defined portfolios are generated based on an optimization model and users domain knowledge the comparison between these portfolios is supported by the portfolio comparison view using our system analysts can incorporate their decision preferences and add selection attributes that are not easily incorporated as constraints or objectives or that delay the construction of a portfolio given the resulting model complexity currently our optimization model is fast for moderately sized landscapes and allows the construction of what if scenarios almost in real time our framework has been validated by conservation experts through two case studies which demonstrate how our framework can help analysts to generate conservation portfolios for different goals under a variety of constraints moreover our system has been received design feedback from multiple conservation experts including two co authors and four external partners although the feedback received was generally positive some limitations have been identified for future work specifically analysts appreciated the option to compare portfolios however more automation for supporting detailed comparison could improve the analysis process the analysts also noted that while the framework is flexible to the underlying optimization approach an api that would allow users to directly integrate their own optimization routines could greatly enhance their workflow a possible avenue is to explore alternative multi objective approaches to explore the trade off between objectives in the portfolio optimization see e g miettinen 2012 and sawaragi et al 1985 for alternatives further work will focus on the automatic comparison of candidate portfolios and add customized algorithms to induce other spatial properties to the framework in case the user decides to use them e g connectivity and compactness an interesting conjecture is whether adding human interaction with the optimization helps with the run time issues when spatial properties are enforced further studies exploring the tradeoffs between human input and ability to explore reasonable solutions is an interesting future direction as of now the analyst can manually load a candidate conservation portfolio for further analysis using a shapefile or a file specifying whether a parcel is selected we will add modifications in this aspect to facilitate the upload and compatibility check of a candidate portfolio as this will allow our system to complement the analysis of other existing tools like marxan and zonation although our framework focuses on conservation planning decisions it can be extended to other spatial problems including electoral districting location of urban parks and land use planning such applications will require the proper data inputs and specification of the related optimization problems acknowledgment this work is supported by the national science foundation grant numbers 1350573 1639227 1740042 and 2047961 authors would like to thank penny langhammer curtis freese lance craighead wes sechrest and karl burkart for their advice and valuable comments we also thank the anonymous reviewers and associate editor whose constructive comments helped us improve the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25737,calibration the estimation of model parameters based on fitting the model to experimental data is among the first steps in many applications of process based models and has an important impact on simulated values we propose a novel method of developing guidelines for calibration of process based models based on development of recommendations for calibration of the phenology component of crop models the approach was based on a multi model study where all teams were provided with the same data and asked to return simulations for the same conditions all teams were asked to document in detail their calibration approach including choices with respect to criteria for best parameters choice of parameters to estimate and software based on an analysis of the advantages and disadvantages of the various choices we propose calibration recommendations that cover a comprehensive list of decisions and that are based on actual practices keywords calibration recommendations process based models parameter estimation phenology 1 introduction calibration is an important part of the modeling process since it enables the numerical model results and their reliable use in model applications it is undertaken in many fields that use process based models including environmental models jakeman et al 2006 hydrological models badham et al 2019 atmospheric models steele and werndl 2013 models of pest and disease dynamics donatelli et al 2017 and agricultural models seidel et al 2018 essentially model calibration involves adjusting model parameters to reduce the error between the model results and the measured data we will talk of calibrating a model which involves estimating the values of the model parameters the majority of simulation studies involve some type of calibration prior to model application calibration is often necessary because parameter values are usually not universally valid as explained by fath and jorgensen 2011 in the context of ecological models and as explained in the context of crop models based on statistical principles wallach 2011 calibration of nonlinear models is a major area of study in statistics seber and wild 1989 sen and srivastava 1990 but process based models have several features which make calibration particularly challenging wallach et al 2019 firstly process based models often have a large number of parameters often many more than the number of observed data which means estimation of values for all parameters or even just for those parameters that might have a sizeable impact on the simulated results is often not possible so one must decide which parameters to estimate doherty and hunt 2009 necpálová et al 2015 even when one estimates a subset of the model parameters there is often a problem of equifinality meaning that various different combinations of parameter values can give the same results and so calibration does not lead to unique parameter values beven and freer 2001 furthermore process based models usually simulate multiple different variables which can be compared with observed data leading to the problem of combining information about the fit of the different variables into a single criterion for calibration or possibly of defining multiple criteria wöhling et al 2013b software used for the calibration is an additional problem often one externally couples the existing model software to calibration software buis et al 2011 he et al 2010 hunt et al 1993 but this can require substantial effort as a result calibration for crop models is often done by manual trial and error without using an automated routine seidel et al 2018 though this could also be a deliberate choice to exert more supervision over the calibration a further difficulty is that the observed data might have substantial errors in response to these and other difficulties there have been numerous studies published concerning calibration recommendations for process based models in multiple disciplines one type of study focuses on a particular model it identifies the most important parameters in that model and explains how they can be estimated from data ahuja et al 2011 other studies have focused on the implementation of a bayesian approach or on the comparison of frequentist and bayesian approaches gao et al 2020 jansen and hagenaars 2004 sexton et al 2016 van oijen et al 2005 on numerical methods of seeking best parameter values bhar et al 2020 franchini and galeati 1997 madsen et al 2002 on the choice of parameters to estimate angulo et al 2013 on the definition of multiple objective functions as a way of handling multiple simulated responses efstratiadis and koutsoyiannis 2010 or on which observed data to use for calibration hunt et al 2001 there do not seem to be calibration recommendations based on a holistic view of model calibration for process based models such that the recommendations cover the full range of decisions that calibration involves and that are based on actual practice a holistic treatment is of importance because potentially any or all of the decisions involved in the calibration process might have an important impact on the results it is important to base recommendations on the range of actual practices to ensure that a wide range of feasible approaches is considered the specific process based models considered here are crop models which consist of a set of mathematical equations representing physically based or semi empirical processes that describe plant development and growth as well as soil conditions as affected by weather soil characteristics and crop management crop models are widely used to study understand and optimize crop production in current and future environments ewert et al 2015 keating and thorburn 2018 tsuji et al 1998 in our study we focus specifically on the use of crop models to simulate crop phenology i e the cycle of biological events in plants because matching the phenology of crop varieties to the climate in which they grow is a critical crop production strategy hunt et al 2019 rezaei et al 2015 2018 in general the simulation of crop phenology is an essential part of crop models and implemented as a phenological model component or submodel in the crop model in many crop modeling studies the focus is specifically on simulating phenology gao et al 2020 kawakita et al 2020 wu et al 2017 but also calibration of crop models using just phenological observations can be a first step in crop simulation studies kimball et al 2019 li et al 2015 modeling plant phenology is also important in understanding ecosystem response to global warming piao et al 2019 the objective of the present study is to define a novel approach to developing recommendations for calibration of process based models and to apply it to derive recommendations for calibration of phenology simulation using crop models the approach considers the full range of decisions involved in the calibration process and is based on information about the actual choices made by multiple modeling groups the proposed approach involves three steps first one or more multi model simulation studies are organized where all participating modeling teams are given the same data for calibration and asked to provide simulated values for the same outputs using their usual calibration technique secondly each team is asked to complete a detailed questionnaire about the choices made for each calibration decision this provides information about the range of choices that are made in practice finally the advantages and disadvantages of the different choices for each calibration decision are analyzed which provides the basis for a set of recommended practices 2 materials and methods this study is based on two multi model simulation studies the studies were organized within the framework of the agricultural modeling intercomparison and improvement project agmip rosenzweig et al 2013 the co leaders of the calibration activity designed the studies as a way of obtaining information on crop model calibration practices when the objective is simulation of crop phenology they invited crop modeling groups to participate by announcements on the agmip website and in messages to the mailing lists of several widely used models all modeling groups that asked to participate were accepted we use here the term model structure to designate a specific set of model equations the model structures used by the participants are listed in supplementary table s1 we speak of modeling group to designate the group of researchers that implemented the model structure in a specific case the modeling group was responsible for determining all aspects of the calibration procedure and also for choosing the values of fixed parameters i e those not determined by calibration twenty seven modeling groups participated in the first simulation study based on data from wheat fields in france twenty six of those groups and two additional groups participated in the second simulation study based on data from wheat fields in australia thus overall 29 different groups participated in at least one of the studies the modeling groups are identified as m1 m29 and the same identifier is used for the same modeling group working with the french and australian data the name of the model structure used by each group is not given since this might give the erroneous impression that the calibration approach and simulation results are specific to that model structure while in fact they depend on both the model structure and the decisions made by the modeling group three of the model structures coded as s1 s2 and s3 were used by multiple groups structure s1 was used by four french dataset or three australian dataset groups structure s2 by three groups and structure s3 by two groups comparisons within these three groups provide information about the variability as to calibration approach between different modeling groups calibrating the same model structure in each study participants were given the input data usually required for running a crop model namely daily weather data information on crop management and information on soil characteristics for multiple environments an environment refers to a specific combination of site and sowing date the modeling teams were also given data on wheat phenology from a subset of those environments for calibration the calibration environments and asked to provide simulated phenology for the remaining environments the evaluation environments in each study the calibration and evaluation environments were drawn from the same target population fields with conventional wheat management in typical wheat growing regions in france or in australia the split between the calibration and evaluation data was made so that the two sets were roughly of equal size and included multiple sites and years subject to the major constraint that none of the sites or years represented in the evaluation environments occurred in the calibration environments while the participants were given full information about all the environments at no point did they have access to the evaluation results thus this was a rigorous evaluation of how well the modeling groups were able to simulate wheat phenology for new environments details for the evaluation results are provided in wallach et al 2021b 2021a the prediction errors are also summarized in fig 1 and supplementary table s7 the observations in the french dataset were for days from sowing to two phenological stages namely beginning of stem elongation growth stage 30 on the bbch and zadoks scales zadoks et al 1974 and middle of heading growth stage 55 on the bbch and zadoks scales table 1 these two stages are of practical importance because they can easily be determined visually and are closely related to the recommended dates for the second and third n fertilizer applications in france observed data were provided for two varieties namely apache and bermude in all cases the modeling groups used the same calibration approach for both varieties therefore in this study we only report a single calibration approach for each modeling group for the french dataset the modeling groups were asked to provide simulated values for those same two growth stages for the evaluation environments the australian data set had a different structure each environment was visited once every two weeks and the zadoks stage in that environment was recorded the data were then interpolated to give days from sowing to every integer zadoks stage from the first to the last observed stage for each environment these were the data provided to each modeling group for the calibration environments for the evaluation environments participants were asked to provide the simulated values for the number of days from sowing to stages z30 zadoks stage 30 pseudostem i e youngest leaf sheath erection z65 zadoks stage 65 anthesis half way i e anthers occurring half way to tip and base of ear and z90 zadoks stage 90 grain hard difficult to divide these stages are often used for management decisions or to characterize phenology in both simulation exercises each participating modeling group was asked to calibrate the model in their usual way using the calibration data provided each group estimated one set of parameters based on the data from the french calibration environments and a second set based on the data from the australian calibration environments and then used those parameters to simulate results for the french and australian evaluation environments respectively each group was also asked to complete a questionnaire detailing how the calibration was conducted table 2 the questions were chosen to cover as completely as possible the full set of the decisions that are associated with the calibration approach and to provide information about the underlying reasoning for the choice of modeling approach see questions 3 and 6 3 results the decisions required for calibration can be divided into three groups i decisions related to the criterion that defines the best parameter values ii decisions related to the choice of parameters to be estimated and iii decisions related to the numerical calculation of the best parameter values the more detailed decisions within each group are shown in figs 2 and 3 which also indicate the choices made by the participating modeling groups and the number of groups that made each particular choice details for each individual modeling group are shown in supplementary tables s3 s4 and s5 for choices concerning the criterion of best parameter values the parameters to estimate and the algorithm and software respectively 3 1 criterion for best parameters a first calibration decision in this category is which variables to use in the criterion that defines the best parameter values and in particular whether to use only those variables for which predictions are sought or also additional observed variables by variable we mean the type of data for example days to flowering and days to heading would be two different variables the french dataset only had observations for two variables namely days to phenological stages bbch30 and bbch55 see table 1 and groups were requested to report the simulated values for those same two variables for both the calibration and evaluation environments thus it was a logical consequence that for almost all groups the criterion of best parameters included observations of both those variables two groups m9 and m18 used model structures that did not simulate the number of days to stage bbch30 so these groups only used a subset of the observed variables i e the observations of days to stage bbch55 in the criterion defining best parameters the australian dataset on the other hand had many observed variables i e days to many phenology stages while evaluation was based just on simulated days to three stages z30 z65 and z90 here the choice of variables to include in the criterion was not so straightforward for the australian dataset about 40 of the groups used only the variables to be simulated days to z30 z65 and z90 or a subset if the model structure did not simulate all those variables while about 60 of the modeling groups included other observed variables in the criterion of best parameters different groups that used the same model structure did not necessarily make the same choice here considering structure s1 used by groups m2 m3 and m4 and the australian dataset all three groups used minimum sum of squared errors as the criterion defining the best parameter values however group m2 included additional variables in addition to the variables to be simulated in their sum of squared errors group m3 used only squared errors for z65 and z90 and group m4 used squared errors for z30 z65 and z90 a second decision concerns the definition of error almost all groups expressed error in terms of days to reach a specified stage table s3 however it is also possible to express error in terms of phenological stage in the simplest case suppose that a model structure calculates zadoks stage each day i e the internal counter each day is directly or is translated into a value for zadoks stage suppose that for a particular environment it is observed that stage z30 is attained on day 45 but the simulated day is 40 the error in days is 5 days suppose that the simulated stage on day 45 is z33 4 then the error in terms of development stage is 30 33 4 3 4 for the french dataset all groups calculated error in days but for the australian dataset three groups expressed error in terms of development stage rather than days a third decision is whether to use a frequentist or bayesian perspective if a frequentist perspective is chosen one must define the mathematical form of the objective function if a bayesian perspective is chosen one must define the form of the likelihood and the prior distributions for the parameters the large majority of groups followed a frequentist approach where the estimated parameter values are those values that minimize some measure of error between the simulated and observed values table s3 most of the frequentist groups sought to minimize the sum of squared errors where the sum is over calibration environments and over all variables included in the criterion this is the ordinary least squares ols criterion one french data or four australian data groups used a different measure of distance between observed and simulated values namely the sum of root mean squared errors for the different variables or a weighted sum of squared errors two groups chose to minimize the sum of absolute errors this is the least absolute value criterion lav four groups for the french dataset and the same four groups for the australian dataset did not define an explicit objective function to be minimized but rather sought parameter values to give a best fit to the data where best fit was determined visually or by some subjective combination of mean squared error r2 or other fit metrics groups using the same model structure did not necessarily make the same decisions for example among the three groups that used model structure s2 for the australian dataset one used the ols criterion and two had no explicit objective function another decision for the frequentist perspective is whether to fit all the observed variables in a single calculation step or to use multiple steps adjusting parameters to different variables in each step almost all groups estimated all parameters simultaneously table s3 however two french data or four australian data groups estimated parameters in more than one step fitting for example three parameters to the bbch30 data in the french dataset and then fixing those parameters at their estimated values and fitting another parameter to the bbch55 data again groups using the same model structure did not always make the same decisions for example among the two groups that used model structure s3 for the australian dataset group m23 estimated all parameters simultaneously while group m24 estimated parameters in two steps of the total of six bayesian calibrations two for the french dataset four for the australian dataset three assumed a normal distribution of errors and one a student s t distribution one group worked with the concentrated likelihood which replaces the model variance for each variable by its maximum likelihood value in all cases errors were all assumed to be independent for the bayesian groups parameters were assumed to have either uniform or truncated normal prior distributions no group took correlations of errors for different variables in the same environment into account that is all groups treated all the errors as though they were independent only two groups m19 m21 see table s3 took into account the possibility of different error variances for different variables m21 by using the method of concentrated likelihood seber and wild 1989 and m19 by dividing the likelihood for each variable by the number of observations of that variable 3 2 choice of parameters to estimate each model structure is parameterized differently so it is not possible to directly compare names of parameters between model structures it is however possible to identify the role of estimated parameters in the model and base the comparison between groups on that details related to the choice of parameters by each group are given in supplementary table s4 most groups estimated at least some parameters that concern the physiological time required to attain one or more phenological stages fifteen groups for both datasets estimated one or more parameters related to vernalization and 13 groups for both datasets estimated one or more parameters related to photoperiod sensitivity a smaller number of groups estimated parameters related to the temperature response function for example minimum temperature or optimum temperature for development or to tillering or leaf appearance rate phyllochron two groups for each dataset estimated parameters related to the effect of stress on the development rate and two french dataset or three australian dataset groups estimated parameters related to time to emergence the number of estimated parameters ranged from one to nine for the french dataset and from two to ten for the australian dataset in most cases the choice of parameters to estimate was based on expert opinion but four groups for each dataset combined expert opinion with data based information for example testing various combinations of parameters to see which gives the best fit five french dataset or four australian dataset groups based the choice of parameters to estimate on sensitivity analysis we have identified the use of expert knowledge and data based choice of parameters to estimate as two separate categories but it should be noted that expert knowledge by itself usually adapts the choice of parameters to estimate to the data at least to some extent this can be seen from the fact that almost every group that based the choice of parameters on expert knowledge estimated a different and in most cases larger set of parameters based on the australian dataset with more observed variables than based on the french dataset fig 4 the specific parameters estimated by each group depend of course on the way the model is parameterized it is therefore not possible to make general recommendations about which parameters to estimate even within the limits of phenology modeling on the other hand the underlying rationale for deciding which parameters to estimate is applicable in general for process based models and could be the object of recommendations it is thus the different possible choices of rationale that are shown in fig 3 there were differences even between groups using the same model structure consider for example structure s2 the three groups that used this model structure m7 m12 and m13 estimated respectively four three and two parameters for the french dataset and nine four and two parameters for the australian dataset two of those groups based the choice on expert opinion while group m7 made a partially data driven choice 3 3 numerical methods the basic decision here is the algorithm to use for estimating the parameters fig 3 a second practical decision is the software to use to implement that algorithm the choices made by each modeling group are shown in supplementary table s5 and information about the specific software used is given in supplementary table s6 among the groups that chose a frequentist approach slightly over half used trial and error to search for the optimal parameter values in some of those cases available software was used as an aid but the final values were found by simply trying different parameter values the remaining half was split between groups that used a derivative free search algorithm usually an algorithm designed to find a global optimum and those that used a gradient based algorithm many different software solutions were used including multi purpose software packages as well as software written expressly for calibration of that particular model structure four groups french data or six groups australian data used a markov chain monte carlo mcmc algorithm to estimate the posterior distribution using various software packages that included one group that used an mcmc algorithm though the objective was to minimize the sum of absolute errors the groups that used the same model structure in general did not use the same algorithm and software for example considering the two groups using model structure s3 group m23 used a combination of global and local search algorithms and available software packages while group m24 used trial and error and no software packages 3 4 level of error table s7 shows mean absolute error mae where the average is over the calibration data or the evaluation data for each modeling group for the french and australian datasets mae can be quite different even for different groups using the same model structure for example four modeling groups used model structure s1 for the french dataset and had mae for the calibration data of 3 9 4 9 6 8 and 12 8 days respectively 4 discussion there is substantial variability in calibration approach between modeling groups even between groups that use the same model structure thus a first overall conclusion is that we are far from having a consensus on how to calibrate crop models even for a given model structure and dataset and even for the relatively simple case which focuses just on phenology in the following we discuss the advantages and drawbacks of each choice for each calibration decision and on that basis make recommendations for good practices we do not base these recommendations on the levels of error of the different modeling groups because there is no simple relation between calibration approach and resulting error differences in error could be due to differences in model structure to differences in parameter values for those parameters not estimated by calibration as well as to differences in any or all of the calibration decisions made by the individual groups 4 1 criteria for best parameters a major calibration decision is the list of observed variables here observed development stages to include in the criterion of best fit from a modeling point of view using as many variables as possible for fitting the model reduces the risk of getting the right answer for the wrong reason i e getting a good fit for some variables while other variables that describe other aspects of system behavior are poorly simulated meyer oliveira et al 2021 wang et al 2011 fitting the model to more variables will reduce the aspects of the system that could unknowingly be poorly simulated this is important since the same calibrated model might be used for more than one specific purpose more generally process based crop models are argued to be meaningful tools for understanding crop growth and production in response to climate variability and change keating and thorburn 2018 particularly as they cover interconnections of different system variables in their structures ewert et al 2015 calibration using multiple observed variables should improve the representation of these interconnections from a statistical point of view more data in general leads to predictors with smaller variance which argues for using all the available data however this assumes that the model is correctly specified in the statistical sense meaning that model errors have expectation zero for all values of the explanatory variables it has been argued that crop models are most likely statistically incorrectly specified and as a result the best parameters for predicting one variable may be different than the best parameters for predicting a different variable wallach 2011 in that case using additional variables in the objective function may degrade predictive accuracy for the variables of primary interest this was found to be the case in the study of guillaume et al 2011 in fact it has been suggested that the differences in calibration results using different observed variables could be a diagnostic tool for model structural errors wöhling et al 2013a if however one is willing to assume that statistical misspecification is not too extreme then it seems worthwhile to include as many of the observed variables as possible in the objective function so here this is the recommended practice this does not imply that all variables should have equal weight in the criterion for best parameters statistical theory shows that if different variables have different error variances then that should be taken into account most groups defined error as the difference between the simulated and observed days to reach a given phenological stage but in a few cases error was defined as the difference between the simulated development stage and the observed stage on the day of observation this option requires that the model include some internal counter whose observed and simulated values at observed phenological stages are known but this is often the case it has been argued that the problem of minimizing errors is much better behaved numerically when errors are in terms of development stage rather than days wallach et al 2018 on the other hand one is usually interested in how large the error is in days so this is a more intuitive error measure furthermore this does not require that the model have an internal measurement of development stage we suggest then to measure error in days for the groups that adopted a frequentist perspective the large majority framed the problem as an ordinary least squares ols problem two modeling groups chose parameters by minimizing the sum of absolute errors which has been argued to have advantages over ols as it is less sensitive to outliers willmott and matsuura 2005 however if evaluation is based on squared error then ols is the more logical choice and is recommended four groups did not have an explicit objective function one obvious disadvantage of this approach is its subjectivity adding uncertainty in the definition of best fit to other uncertainties in calibration a second disadvantage is that one cannot automate the search for the best parameters in most cases a single objective function combining all errors was used in a few cases however parameters were fitted sequentially first to one variable then to the next etc this sequential technique has often been recommended for full crop models l r ahuja et al 2011 anothai et al 2008 this simplifies the mechanics of finding the best parameter values but it will lead to sub optimal results with respect to an overall objective function if the objective is to minimize the total sum of squared errors for example the best parameter values are those that minimize exactly that objective function necpálová et al 2015 similarly recommended simultaneous estimation even for multiple observed variables for an ecosystem biogeochemical model a few groups chose a bayesian rather than a frequentist perspective there are fundamental differences between frequentist and bayesian approaches berger and bayarri 2004 however for the practical prediction problem here there are also important similarities a major difference is that the bayesian approach focuses on the posterior distribution which is a distribution of predicted values while the frequentist approach focuses on point predictions i e one single predicted value here however all groups were asked for point predictions so the groups that used bayesian approach had to choose a single result from the posterior distribution in all cases they chose the parameter values that maximized the posterior distribution which then plays the same role as the objective function for the frequentist approach another important difference is that for the bayesian approach the prior information about parameter values is included in the calculation while this is not in general included in a frequentist approach however in almost all cases here the frequentist approach included lower and upper bounds on the parameter values table s5 which is also based on prior information in fact a bayesian approach with normal likelihood and uniform priors leads to exactly the same criterion for best fit namely minimum squared error subject to the constraints on the parameters as ols with bounds on the parameter values if the main objective is to obtain a point predictor then we suggest using a frequentist approach since adopting a bayesian approach and calculating a posterior distribution may require unnecessarily long calculation times if one is also interested in uncertainty information as it is often the case then a bayesian approach has advantages as far as parameter uncertainty is concerned since the posterior distribution is directly a representation of the uncertainty in the parameter vector however it is important to keep in mind that parameter uncertainty is only part of overall uncertainty and in fact it has been found in several cases that parameter uncertainty is quite a bit less than structure uncertainty zhang et al 2017 in almost all cases the calibration approach was directly based on regression methods in statistics either frequentist or bayesian this seems logical insofar as these statistical methods have desirable properties however these properties in general require that certain assumptions be satisfied the standard assumptions for the ols method are that the model errors be independent and identically distributed with expectation 0 seber and wild 1989 sen and srivastava 1990 for the bayesian methods one must make explicit assumptions about the distribution of errors including whether all errors have the same distribution and whether errors for different variables are correlated in the case of crop models with multiple observed variables in each environment the assumptions of independent identically distributed errors with expectation 0 are not likely to be satisfied wallach et al 2019 most obviously errors for different variables in the same environment e g days to development stages z30 and z65 may be correlated since any particularities of the environment affect all variables for that environment furthermore the errors for different variables may have different variances which violates the assumption that all errors have the same distribution no group took correlations of errors into account and only two groups took into account the possibility of different variances for errors of different variables in general it would be worthwhile to go a step further in applying statistical methods beyond employing standard techniques in order to examine whether the standard assumptions about model error are satisfied to detect unacceptably large violations of the standard assumptions one should examine the model residuals observed minus simulated values after calibration as is standard procedure in regression see for example nist sematech 2013 one should examine overall bias of model residuals which should be zero the variances of residuals for different variables which should be similar and correlations between residuals for different variables in the same environment which should be small only phenology data were available in the datasets here and thus all errors had the same units days or phenological stage in cases where variables with different units are observed for example days to phenological stages and yield it is meaningless to simply combine errors in that case a first step could be to divide all simulated and observed values by an estimated standard deviation of error for that variable as in weighted least squares seber and wild 1989 then all errors would be unitless and could be combined however it would still be important to test residuals after calibration 4 2 choice of parameters to estimate of particular interest is the rationale behind the choice of parameters to estimate and what this implies for the adaptation of the choice of parameters to the dataset in most cases the choice of parameters to estimate was based on expert knowledge of the model to some extent this takes into account the dataset however expert knowledge only takes the amount and type of observed data into account approximately an alternative which we recommend would be to formally consider the choice of parameters to estimate as a problem of model selection where the selection is of the subset of parameters to estimate by calibration while the other parameters retain their default values for example one could use the akaike information criterion aic akaike 1973 which has been widely used for model choice in ecology burnham et al 2011 to choose the parameters to estimate the use of a model selection rule would automatically adapt the choice of parameters to estimate to the calibration dataset consider for example the question of whether or not to estimate parameters related to water stress for those models that include effects of water stress on phenology a model selection rule would include such a parameter if it had a relatively large effect on improving the fit to the calibration data and would not include it otherwise however given the large number of possible parameters to estimate it would probably be necessary to combine expert knowledge in order to choose a fairly small number of candidate parameters with a formal model selection criterion all parameters that are not estimated using the calibration data retain their default values and this in general concerns the majority of model parameters while some parameters will not have an effect on the simulated values many others will have an effect it is clear then that the choice of these default values is extremely important and should reflect whatever information one has about the cultivars and environments of interest the choice of default values probably merits more attention than it usually receives 4 3 algorithm and software there are several disadvantages to the trial and error approach which was used by somewhat over a third of groups it is time consuming it is likely to end in a non optimal solution especially if several parameters are estimated and it cannot be replicated for example to estimate prediction error using cross validation a wide range of algorithms and software was used by the remaining groups the problem of choosing a calibration algorithm and software to search for optimal parameter values has received much attention in the field of hydrological modeling skahill and doherty 2006 gradient based algorithms are in general very efficient but may converge to a local rather than global optimum blasone et al 2006 also simulated values are often non continuous functions of the parameters as a result it may not be possible to calculate a gradient removing the discontinuities may be possible but at the price of detailed intervention in the model code liu et al 2018 global search algorithms such as a grid search or genetic algorithms may avoid converging to a local optimum but in general require many more model runs a third possibility is a gradient free search algorithm such as the simplex method nelder and mead 1965 which seems to be a good choice for calibration of crop models there is calibration software that has been developed specifically for some crop models buddhaboon et al 2018 buis et al 2011 hunt et al 1993 and also some software that is designed to be easily coupled to any model doherty et al 2010 coupling parameter estimation software to a crop model is not simple and so modeling groups tend to use available software or even no software rather than developing new calibration software themselves this implies that for the improvement of calibration approaches for crop models it is not sufficient to propose guidelines for good calibration practices for the guidelines to be effective they must include software solutions that can be used by any model 4 4 summary of recommendations and of the overall approach based on an analysis of calibration approach by multiple modeling groups we propose the following guidelines for calibration of the phenology component of crop models before calibration one should pay careful attention to the choice of default values for all parameters if the objective is as here to make point predictions then a frequentist approach to calibration is fully justified one can use an ols criterion initially based on errors in units of days but the statistical assumptions underlying ols should be checked by analyzing residuals after calibration the choice of parameters to estimate should be adapted to the available data for example using the aic criterion however the choice of parameters also requires knowledge of the model and the environments studied as well as of the reliability of the data this suggests that a fully automatic calibration procedure may not be advisable for the calculations a derivative free but efficient search algorithm like the simplex is recommended these are guidelines and the detailed implementation will depend on the crop model these recommendations are specific to calibration of the phenology component of crop models even though many of them are no doubt more widely applicable calibration of process based models in other fields may involve aspects specific to those fields requiring somewhat different recommendations these could be developed by following the same three steps followed here namely a multi model calibration exercise then detailed description of the calibration approach by each modeling group and finally analysis of the calibration choices a simplified version would simply document hypothetical choices by each group without actually requiring the groups to perform the calibration this would greatly simplify the study but might not truly represent what each group would do in practice 5 conclusions calibration of crop models involves multiple decisions which can be grouped into choice of criteria for defining the best parameter values choice of parameters to estimate and choice of algorithm and software different modeling groups make quite different decisions even for modeling groups using the same model structure it seems that we are far from having a consensus on how to calibrate crop models even in the relatively simple case with only phenology data which emphasizes the need for calibration guidelines such as those suggested here we propose an original approach to development of crop model calibration recommendations and apply it to develop recommendations for calibration of the phenology component of crop models one original aspect is that we consider a very broad range of decisions involved in calibration whereas other studies usually concentrate on one or two specific calibration decisions this is important because each of these decisions can have an important impact on calibration secondly our starting point for recommendations is the choices made in practice by multiple modeling teams this ensures that we consider a wide range of possible choices for each of the calibration decisions and that the choices considered are anchored in actual practice the recommendations are then based on analyzing the advantages and disadvantages of the different choices that are made of course it will be important in the future to test these recommendations as to their ability to improve model predictions in developing recommendations for calibration in other fields we suggest that a procedure similar to ours could be of interest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was in part supported by the collaborative research center 1253 campos project 7 stochastic modeling framework funded by the german research foundation dfg grant agreement sfb 1253 1 2017 the academy of finland through projects aicroppro 316172 and divcsa 316215 and natural resources institute finland luke through a strategic project boostia the bonares project soil3 boma 03037514 of the federal ministry of education and research bmbf germany the deutsche forschungsgemeinschaft dfg german research foundation under germany s excellence strategy exc 2070 390732324 exc phenorob the project biomassweb of the globee programme grant number fkz031a258b funded by the federal ministry of education and research bmbf germany the inra accaf meta programme the german federal ministry of education and research bmbf in the framework of the funding measure soil as a sustainable resource for the bioeconomy bonares project bonares module b bonares centre for soil research subproject b grant 031b0511b the national key research and development program of china 2017yfd0300205 the national science foundation for distinguished young scholars 31725020 the priority academic program development of jiangsu higher education institutions papd the 111 project b16026 and china scholarship council the agriculture and agri food canada s project 1387 under he canadian agricultural partnership the dfg research unit for 1695 agricultural landscapes under global climate change processes and feedbacks on a regional scale the u s department of agriculture national institute of food and agriculture award no 2015 68007 23133 and usda nifa hatch grant n mcl02368 the national key research and development program of china 2016yfd0300105 the broadacre agriculture initiative a research partnership between university of southern queensland and the queensland department of agriculture and fisheries the academy of finland through project ai croppro 315896 the jpi facce macsur2 project funded by the italian ministry for agricultural food and forestry policies d m 24064 7303 15 of 6 nov 2015 the ministry of education youth and sports of czech republic through sustes adaption strategies for sustainable ecosystem services and food security under adverse environmental conditions project no cz 02 1 01 0 0 0 0 16 019 000797 the order in which the donors are listed is arbitrary appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105206 
25737,calibration the estimation of model parameters based on fitting the model to experimental data is among the first steps in many applications of process based models and has an important impact on simulated values we propose a novel method of developing guidelines for calibration of process based models based on development of recommendations for calibration of the phenology component of crop models the approach was based on a multi model study where all teams were provided with the same data and asked to return simulations for the same conditions all teams were asked to document in detail their calibration approach including choices with respect to criteria for best parameters choice of parameters to estimate and software based on an analysis of the advantages and disadvantages of the various choices we propose calibration recommendations that cover a comprehensive list of decisions and that are based on actual practices keywords calibration recommendations process based models parameter estimation phenology 1 introduction calibration is an important part of the modeling process since it enables the numerical model results and their reliable use in model applications it is undertaken in many fields that use process based models including environmental models jakeman et al 2006 hydrological models badham et al 2019 atmospheric models steele and werndl 2013 models of pest and disease dynamics donatelli et al 2017 and agricultural models seidel et al 2018 essentially model calibration involves adjusting model parameters to reduce the error between the model results and the measured data we will talk of calibrating a model which involves estimating the values of the model parameters the majority of simulation studies involve some type of calibration prior to model application calibration is often necessary because parameter values are usually not universally valid as explained by fath and jorgensen 2011 in the context of ecological models and as explained in the context of crop models based on statistical principles wallach 2011 calibration of nonlinear models is a major area of study in statistics seber and wild 1989 sen and srivastava 1990 but process based models have several features which make calibration particularly challenging wallach et al 2019 firstly process based models often have a large number of parameters often many more than the number of observed data which means estimation of values for all parameters or even just for those parameters that might have a sizeable impact on the simulated results is often not possible so one must decide which parameters to estimate doherty and hunt 2009 necpálová et al 2015 even when one estimates a subset of the model parameters there is often a problem of equifinality meaning that various different combinations of parameter values can give the same results and so calibration does not lead to unique parameter values beven and freer 2001 furthermore process based models usually simulate multiple different variables which can be compared with observed data leading to the problem of combining information about the fit of the different variables into a single criterion for calibration or possibly of defining multiple criteria wöhling et al 2013b software used for the calibration is an additional problem often one externally couples the existing model software to calibration software buis et al 2011 he et al 2010 hunt et al 1993 but this can require substantial effort as a result calibration for crop models is often done by manual trial and error without using an automated routine seidel et al 2018 though this could also be a deliberate choice to exert more supervision over the calibration a further difficulty is that the observed data might have substantial errors in response to these and other difficulties there have been numerous studies published concerning calibration recommendations for process based models in multiple disciplines one type of study focuses on a particular model it identifies the most important parameters in that model and explains how they can be estimated from data ahuja et al 2011 other studies have focused on the implementation of a bayesian approach or on the comparison of frequentist and bayesian approaches gao et al 2020 jansen and hagenaars 2004 sexton et al 2016 van oijen et al 2005 on numerical methods of seeking best parameter values bhar et al 2020 franchini and galeati 1997 madsen et al 2002 on the choice of parameters to estimate angulo et al 2013 on the definition of multiple objective functions as a way of handling multiple simulated responses efstratiadis and koutsoyiannis 2010 or on which observed data to use for calibration hunt et al 2001 there do not seem to be calibration recommendations based on a holistic view of model calibration for process based models such that the recommendations cover the full range of decisions that calibration involves and that are based on actual practice a holistic treatment is of importance because potentially any or all of the decisions involved in the calibration process might have an important impact on the results it is important to base recommendations on the range of actual practices to ensure that a wide range of feasible approaches is considered the specific process based models considered here are crop models which consist of a set of mathematical equations representing physically based or semi empirical processes that describe plant development and growth as well as soil conditions as affected by weather soil characteristics and crop management crop models are widely used to study understand and optimize crop production in current and future environments ewert et al 2015 keating and thorburn 2018 tsuji et al 1998 in our study we focus specifically on the use of crop models to simulate crop phenology i e the cycle of biological events in plants because matching the phenology of crop varieties to the climate in which they grow is a critical crop production strategy hunt et al 2019 rezaei et al 2015 2018 in general the simulation of crop phenology is an essential part of crop models and implemented as a phenological model component or submodel in the crop model in many crop modeling studies the focus is specifically on simulating phenology gao et al 2020 kawakita et al 2020 wu et al 2017 but also calibration of crop models using just phenological observations can be a first step in crop simulation studies kimball et al 2019 li et al 2015 modeling plant phenology is also important in understanding ecosystem response to global warming piao et al 2019 the objective of the present study is to define a novel approach to developing recommendations for calibration of process based models and to apply it to derive recommendations for calibration of phenology simulation using crop models the approach considers the full range of decisions involved in the calibration process and is based on information about the actual choices made by multiple modeling groups the proposed approach involves three steps first one or more multi model simulation studies are organized where all participating modeling teams are given the same data for calibration and asked to provide simulated values for the same outputs using their usual calibration technique secondly each team is asked to complete a detailed questionnaire about the choices made for each calibration decision this provides information about the range of choices that are made in practice finally the advantages and disadvantages of the different choices for each calibration decision are analyzed which provides the basis for a set of recommended practices 2 materials and methods this study is based on two multi model simulation studies the studies were organized within the framework of the agricultural modeling intercomparison and improvement project agmip rosenzweig et al 2013 the co leaders of the calibration activity designed the studies as a way of obtaining information on crop model calibration practices when the objective is simulation of crop phenology they invited crop modeling groups to participate by announcements on the agmip website and in messages to the mailing lists of several widely used models all modeling groups that asked to participate were accepted we use here the term model structure to designate a specific set of model equations the model structures used by the participants are listed in supplementary table s1 we speak of modeling group to designate the group of researchers that implemented the model structure in a specific case the modeling group was responsible for determining all aspects of the calibration procedure and also for choosing the values of fixed parameters i e those not determined by calibration twenty seven modeling groups participated in the first simulation study based on data from wheat fields in france twenty six of those groups and two additional groups participated in the second simulation study based on data from wheat fields in australia thus overall 29 different groups participated in at least one of the studies the modeling groups are identified as m1 m29 and the same identifier is used for the same modeling group working with the french and australian data the name of the model structure used by each group is not given since this might give the erroneous impression that the calibration approach and simulation results are specific to that model structure while in fact they depend on both the model structure and the decisions made by the modeling group three of the model structures coded as s1 s2 and s3 were used by multiple groups structure s1 was used by four french dataset or three australian dataset groups structure s2 by three groups and structure s3 by two groups comparisons within these three groups provide information about the variability as to calibration approach between different modeling groups calibrating the same model structure in each study participants were given the input data usually required for running a crop model namely daily weather data information on crop management and information on soil characteristics for multiple environments an environment refers to a specific combination of site and sowing date the modeling teams were also given data on wheat phenology from a subset of those environments for calibration the calibration environments and asked to provide simulated phenology for the remaining environments the evaluation environments in each study the calibration and evaluation environments were drawn from the same target population fields with conventional wheat management in typical wheat growing regions in france or in australia the split between the calibration and evaluation data was made so that the two sets were roughly of equal size and included multiple sites and years subject to the major constraint that none of the sites or years represented in the evaluation environments occurred in the calibration environments while the participants were given full information about all the environments at no point did they have access to the evaluation results thus this was a rigorous evaluation of how well the modeling groups were able to simulate wheat phenology for new environments details for the evaluation results are provided in wallach et al 2021b 2021a the prediction errors are also summarized in fig 1 and supplementary table s7 the observations in the french dataset were for days from sowing to two phenological stages namely beginning of stem elongation growth stage 30 on the bbch and zadoks scales zadoks et al 1974 and middle of heading growth stage 55 on the bbch and zadoks scales table 1 these two stages are of practical importance because they can easily be determined visually and are closely related to the recommended dates for the second and third n fertilizer applications in france observed data were provided for two varieties namely apache and bermude in all cases the modeling groups used the same calibration approach for both varieties therefore in this study we only report a single calibration approach for each modeling group for the french dataset the modeling groups were asked to provide simulated values for those same two growth stages for the evaluation environments the australian data set had a different structure each environment was visited once every two weeks and the zadoks stage in that environment was recorded the data were then interpolated to give days from sowing to every integer zadoks stage from the first to the last observed stage for each environment these were the data provided to each modeling group for the calibration environments for the evaluation environments participants were asked to provide the simulated values for the number of days from sowing to stages z30 zadoks stage 30 pseudostem i e youngest leaf sheath erection z65 zadoks stage 65 anthesis half way i e anthers occurring half way to tip and base of ear and z90 zadoks stage 90 grain hard difficult to divide these stages are often used for management decisions or to characterize phenology in both simulation exercises each participating modeling group was asked to calibrate the model in their usual way using the calibration data provided each group estimated one set of parameters based on the data from the french calibration environments and a second set based on the data from the australian calibration environments and then used those parameters to simulate results for the french and australian evaluation environments respectively each group was also asked to complete a questionnaire detailing how the calibration was conducted table 2 the questions were chosen to cover as completely as possible the full set of the decisions that are associated with the calibration approach and to provide information about the underlying reasoning for the choice of modeling approach see questions 3 and 6 3 results the decisions required for calibration can be divided into three groups i decisions related to the criterion that defines the best parameter values ii decisions related to the choice of parameters to be estimated and iii decisions related to the numerical calculation of the best parameter values the more detailed decisions within each group are shown in figs 2 and 3 which also indicate the choices made by the participating modeling groups and the number of groups that made each particular choice details for each individual modeling group are shown in supplementary tables s3 s4 and s5 for choices concerning the criterion of best parameter values the parameters to estimate and the algorithm and software respectively 3 1 criterion for best parameters a first calibration decision in this category is which variables to use in the criterion that defines the best parameter values and in particular whether to use only those variables for which predictions are sought or also additional observed variables by variable we mean the type of data for example days to flowering and days to heading would be two different variables the french dataset only had observations for two variables namely days to phenological stages bbch30 and bbch55 see table 1 and groups were requested to report the simulated values for those same two variables for both the calibration and evaluation environments thus it was a logical consequence that for almost all groups the criterion of best parameters included observations of both those variables two groups m9 and m18 used model structures that did not simulate the number of days to stage bbch30 so these groups only used a subset of the observed variables i e the observations of days to stage bbch55 in the criterion defining best parameters the australian dataset on the other hand had many observed variables i e days to many phenology stages while evaluation was based just on simulated days to three stages z30 z65 and z90 here the choice of variables to include in the criterion was not so straightforward for the australian dataset about 40 of the groups used only the variables to be simulated days to z30 z65 and z90 or a subset if the model structure did not simulate all those variables while about 60 of the modeling groups included other observed variables in the criterion of best parameters different groups that used the same model structure did not necessarily make the same choice here considering structure s1 used by groups m2 m3 and m4 and the australian dataset all three groups used minimum sum of squared errors as the criterion defining the best parameter values however group m2 included additional variables in addition to the variables to be simulated in their sum of squared errors group m3 used only squared errors for z65 and z90 and group m4 used squared errors for z30 z65 and z90 a second decision concerns the definition of error almost all groups expressed error in terms of days to reach a specified stage table s3 however it is also possible to express error in terms of phenological stage in the simplest case suppose that a model structure calculates zadoks stage each day i e the internal counter each day is directly or is translated into a value for zadoks stage suppose that for a particular environment it is observed that stage z30 is attained on day 45 but the simulated day is 40 the error in days is 5 days suppose that the simulated stage on day 45 is z33 4 then the error in terms of development stage is 30 33 4 3 4 for the french dataset all groups calculated error in days but for the australian dataset three groups expressed error in terms of development stage rather than days a third decision is whether to use a frequentist or bayesian perspective if a frequentist perspective is chosen one must define the mathematical form of the objective function if a bayesian perspective is chosen one must define the form of the likelihood and the prior distributions for the parameters the large majority of groups followed a frequentist approach where the estimated parameter values are those values that minimize some measure of error between the simulated and observed values table s3 most of the frequentist groups sought to minimize the sum of squared errors where the sum is over calibration environments and over all variables included in the criterion this is the ordinary least squares ols criterion one french data or four australian data groups used a different measure of distance between observed and simulated values namely the sum of root mean squared errors for the different variables or a weighted sum of squared errors two groups chose to minimize the sum of absolute errors this is the least absolute value criterion lav four groups for the french dataset and the same four groups for the australian dataset did not define an explicit objective function to be minimized but rather sought parameter values to give a best fit to the data where best fit was determined visually or by some subjective combination of mean squared error r2 or other fit metrics groups using the same model structure did not necessarily make the same decisions for example among the three groups that used model structure s2 for the australian dataset one used the ols criterion and two had no explicit objective function another decision for the frequentist perspective is whether to fit all the observed variables in a single calculation step or to use multiple steps adjusting parameters to different variables in each step almost all groups estimated all parameters simultaneously table s3 however two french data or four australian data groups estimated parameters in more than one step fitting for example three parameters to the bbch30 data in the french dataset and then fixing those parameters at their estimated values and fitting another parameter to the bbch55 data again groups using the same model structure did not always make the same decisions for example among the two groups that used model structure s3 for the australian dataset group m23 estimated all parameters simultaneously while group m24 estimated parameters in two steps of the total of six bayesian calibrations two for the french dataset four for the australian dataset three assumed a normal distribution of errors and one a student s t distribution one group worked with the concentrated likelihood which replaces the model variance for each variable by its maximum likelihood value in all cases errors were all assumed to be independent for the bayesian groups parameters were assumed to have either uniform or truncated normal prior distributions no group took correlations of errors for different variables in the same environment into account that is all groups treated all the errors as though they were independent only two groups m19 m21 see table s3 took into account the possibility of different error variances for different variables m21 by using the method of concentrated likelihood seber and wild 1989 and m19 by dividing the likelihood for each variable by the number of observations of that variable 3 2 choice of parameters to estimate each model structure is parameterized differently so it is not possible to directly compare names of parameters between model structures it is however possible to identify the role of estimated parameters in the model and base the comparison between groups on that details related to the choice of parameters by each group are given in supplementary table s4 most groups estimated at least some parameters that concern the physiological time required to attain one or more phenological stages fifteen groups for both datasets estimated one or more parameters related to vernalization and 13 groups for both datasets estimated one or more parameters related to photoperiod sensitivity a smaller number of groups estimated parameters related to the temperature response function for example minimum temperature or optimum temperature for development or to tillering or leaf appearance rate phyllochron two groups for each dataset estimated parameters related to the effect of stress on the development rate and two french dataset or three australian dataset groups estimated parameters related to time to emergence the number of estimated parameters ranged from one to nine for the french dataset and from two to ten for the australian dataset in most cases the choice of parameters to estimate was based on expert opinion but four groups for each dataset combined expert opinion with data based information for example testing various combinations of parameters to see which gives the best fit five french dataset or four australian dataset groups based the choice of parameters to estimate on sensitivity analysis we have identified the use of expert knowledge and data based choice of parameters to estimate as two separate categories but it should be noted that expert knowledge by itself usually adapts the choice of parameters to estimate to the data at least to some extent this can be seen from the fact that almost every group that based the choice of parameters on expert knowledge estimated a different and in most cases larger set of parameters based on the australian dataset with more observed variables than based on the french dataset fig 4 the specific parameters estimated by each group depend of course on the way the model is parameterized it is therefore not possible to make general recommendations about which parameters to estimate even within the limits of phenology modeling on the other hand the underlying rationale for deciding which parameters to estimate is applicable in general for process based models and could be the object of recommendations it is thus the different possible choices of rationale that are shown in fig 3 there were differences even between groups using the same model structure consider for example structure s2 the three groups that used this model structure m7 m12 and m13 estimated respectively four three and two parameters for the french dataset and nine four and two parameters for the australian dataset two of those groups based the choice on expert opinion while group m7 made a partially data driven choice 3 3 numerical methods the basic decision here is the algorithm to use for estimating the parameters fig 3 a second practical decision is the software to use to implement that algorithm the choices made by each modeling group are shown in supplementary table s5 and information about the specific software used is given in supplementary table s6 among the groups that chose a frequentist approach slightly over half used trial and error to search for the optimal parameter values in some of those cases available software was used as an aid but the final values were found by simply trying different parameter values the remaining half was split between groups that used a derivative free search algorithm usually an algorithm designed to find a global optimum and those that used a gradient based algorithm many different software solutions were used including multi purpose software packages as well as software written expressly for calibration of that particular model structure four groups french data or six groups australian data used a markov chain monte carlo mcmc algorithm to estimate the posterior distribution using various software packages that included one group that used an mcmc algorithm though the objective was to minimize the sum of absolute errors the groups that used the same model structure in general did not use the same algorithm and software for example considering the two groups using model structure s3 group m23 used a combination of global and local search algorithms and available software packages while group m24 used trial and error and no software packages 3 4 level of error table s7 shows mean absolute error mae where the average is over the calibration data or the evaluation data for each modeling group for the french and australian datasets mae can be quite different even for different groups using the same model structure for example four modeling groups used model structure s1 for the french dataset and had mae for the calibration data of 3 9 4 9 6 8 and 12 8 days respectively 4 discussion there is substantial variability in calibration approach between modeling groups even between groups that use the same model structure thus a first overall conclusion is that we are far from having a consensus on how to calibrate crop models even for a given model structure and dataset and even for the relatively simple case which focuses just on phenology in the following we discuss the advantages and drawbacks of each choice for each calibration decision and on that basis make recommendations for good practices we do not base these recommendations on the levels of error of the different modeling groups because there is no simple relation between calibration approach and resulting error differences in error could be due to differences in model structure to differences in parameter values for those parameters not estimated by calibration as well as to differences in any or all of the calibration decisions made by the individual groups 4 1 criteria for best parameters a major calibration decision is the list of observed variables here observed development stages to include in the criterion of best fit from a modeling point of view using as many variables as possible for fitting the model reduces the risk of getting the right answer for the wrong reason i e getting a good fit for some variables while other variables that describe other aspects of system behavior are poorly simulated meyer oliveira et al 2021 wang et al 2011 fitting the model to more variables will reduce the aspects of the system that could unknowingly be poorly simulated this is important since the same calibrated model might be used for more than one specific purpose more generally process based crop models are argued to be meaningful tools for understanding crop growth and production in response to climate variability and change keating and thorburn 2018 particularly as they cover interconnections of different system variables in their structures ewert et al 2015 calibration using multiple observed variables should improve the representation of these interconnections from a statistical point of view more data in general leads to predictors with smaller variance which argues for using all the available data however this assumes that the model is correctly specified in the statistical sense meaning that model errors have expectation zero for all values of the explanatory variables it has been argued that crop models are most likely statistically incorrectly specified and as a result the best parameters for predicting one variable may be different than the best parameters for predicting a different variable wallach 2011 in that case using additional variables in the objective function may degrade predictive accuracy for the variables of primary interest this was found to be the case in the study of guillaume et al 2011 in fact it has been suggested that the differences in calibration results using different observed variables could be a diagnostic tool for model structural errors wöhling et al 2013a if however one is willing to assume that statistical misspecification is not too extreme then it seems worthwhile to include as many of the observed variables as possible in the objective function so here this is the recommended practice this does not imply that all variables should have equal weight in the criterion for best parameters statistical theory shows that if different variables have different error variances then that should be taken into account most groups defined error as the difference between the simulated and observed days to reach a given phenological stage but in a few cases error was defined as the difference between the simulated development stage and the observed stage on the day of observation this option requires that the model include some internal counter whose observed and simulated values at observed phenological stages are known but this is often the case it has been argued that the problem of minimizing errors is much better behaved numerically when errors are in terms of development stage rather than days wallach et al 2018 on the other hand one is usually interested in how large the error is in days so this is a more intuitive error measure furthermore this does not require that the model have an internal measurement of development stage we suggest then to measure error in days for the groups that adopted a frequentist perspective the large majority framed the problem as an ordinary least squares ols problem two modeling groups chose parameters by minimizing the sum of absolute errors which has been argued to have advantages over ols as it is less sensitive to outliers willmott and matsuura 2005 however if evaluation is based on squared error then ols is the more logical choice and is recommended four groups did not have an explicit objective function one obvious disadvantage of this approach is its subjectivity adding uncertainty in the definition of best fit to other uncertainties in calibration a second disadvantage is that one cannot automate the search for the best parameters in most cases a single objective function combining all errors was used in a few cases however parameters were fitted sequentially first to one variable then to the next etc this sequential technique has often been recommended for full crop models l r ahuja et al 2011 anothai et al 2008 this simplifies the mechanics of finding the best parameter values but it will lead to sub optimal results with respect to an overall objective function if the objective is to minimize the total sum of squared errors for example the best parameter values are those that minimize exactly that objective function necpálová et al 2015 similarly recommended simultaneous estimation even for multiple observed variables for an ecosystem biogeochemical model a few groups chose a bayesian rather than a frequentist perspective there are fundamental differences between frequentist and bayesian approaches berger and bayarri 2004 however for the practical prediction problem here there are also important similarities a major difference is that the bayesian approach focuses on the posterior distribution which is a distribution of predicted values while the frequentist approach focuses on point predictions i e one single predicted value here however all groups were asked for point predictions so the groups that used bayesian approach had to choose a single result from the posterior distribution in all cases they chose the parameter values that maximized the posterior distribution which then plays the same role as the objective function for the frequentist approach another important difference is that for the bayesian approach the prior information about parameter values is included in the calculation while this is not in general included in a frequentist approach however in almost all cases here the frequentist approach included lower and upper bounds on the parameter values table s5 which is also based on prior information in fact a bayesian approach with normal likelihood and uniform priors leads to exactly the same criterion for best fit namely minimum squared error subject to the constraints on the parameters as ols with bounds on the parameter values if the main objective is to obtain a point predictor then we suggest using a frequentist approach since adopting a bayesian approach and calculating a posterior distribution may require unnecessarily long calculation times if one is also interested in uncertainty information as it is often the case then a bayesian approach has advantages as far as parameter uncertainty is concerned since the posterior distribution is directly a representation of the uncertainty in the parameter vector however it is important to keep in mind that parameter uncertainty is only part of overall uncertainty and in fact it has been found in several cases that parameter uncertainty is quite a bit less than structure uncertainty zhang et al 2017 in almost all cases the calibration approach was directly based on regression methods in statistics either frequentist or bayesian this seems logical insofar as these statistical methods have desirable properties however these properties in general require that certain assumptions be satisfied the standard assumptions for the ols method are that the model errors be independent and identically distributed with expectation 0 seber and wild 1989 sen and srivastava 1990 for the bayesian methods one must make explicit assumptions about the distribution of errors including whether all errors have the same distribution and whether errors for different variables are correlated in the case of crop models with multiple observed variables in each environment the assumptions of independent identically distributed errors with expectation 0 are not likely to be satisfied wallach et al 2019 most obviously errors for different variables in the same environment e g days to development stages z30 and z65 may be correlated since any particularities of the environment affect all variables for that environment furthermore the errors for different variables may have different variances which violates the assumption that all errors have the same distribution no group took correlations of errors into account and only two groups took into account the possibility of different variances for errors of different variables in general it would be worthwhile to go a step further in applying statistical methods beyond employing standard techniques in order to examine whether the standard assumptions about model error are satisfied to detect unacceptably large violations of the standard assumptions one should examine the model residuals observed minus simulated values after calibration as is standard procedure in regression see for example nist sematech 2013 one should examine overall bias of model residuals which should be zero the variances of residuals for different variables which should be similar and correlations between residuals for different variables in the same environment which should be small only phenology data were available in the datasets here and thus all errors had the same units days or phenological stage in cases where variables with different units are observed for example days to phenological stages and yield it is meaningless to simply combine errors in that case a first step could be to divide all simulated and observed values by an estimated standard deviation of error for that variable as in weighted least squares seber and wild 1989 then all errors would be unitless and could be combined however it would still be important to test residuals after calibration 4 2 choice of parameters to estimate of particular interest is the rationale behind the choice of parameters to estimate and what this implies for the adaptation of the choice of parameters to the dataset in most cases the choice of parameters to estimate was based on expert knowledge of the model to some extent this takes into account the dataset however expert knowledge only takes the amount and type of observed data into account approximately an alternative which we recommend would be to formally consider the choice of parameters to estimate as a problem of model selection where the selection is of the subset of parameters to estimate by calibration while the other parameters retain their default values for example one could use the akaike information criterion aic akaike 1973 which has been widely used for model choice in ecology burnham et al 2011 to choose the parameters to estimate the use of a model selection rule would automatically adapt the choice of parameters to estimate to the calibration dataset consider for example the question of whether or not to estimate parameters related to water stress for those models that include effects of water stress on phenology a model selection rule would include such a parameter if it had a relatively large effect on improving the fit to the calibration data and would not include it otherwise however given the large number of possible parameters to estimate it would probably be necessary to combine expert knowledge in order to choose a fairly small number of candidate parameters with a formal model selection criterion all parameters that are not estimated using the calibration data retain their default values and this in general concerns the majority of model parameters while some parameters will not have an effect on the simulated values many others will have an effect it is clear then that the choice of these default values is extremely important and should reflect whatever information one has about the cultivars and environments of interest the choice of default values probably merits more attention than it usually receives 4 3 algorithm and software there are several disadvantages to the trial and error approach which was used by somewhat over a third of groups it is time consuming it is likely to end in a non optimal solution especially if several parameters are estimated and it cannot be replicated for example to estimate prediction error using cross validation a wide range of algorithms and software was used by the remaining groups the problem of choosing a calibration algorithm and software to search for optimal parameter values has received much attention in the field of hydrological modeling skahill and doherty 2006 gradient based algorithms are in general very efficient but may converge to a local rather than global optimum blasone et al 2006 also simulated values are often non continuous functions of the parameters as a result it may not be possible to calculate a gradient removing the discontinuities may be possible but at the price of detailed intervention in the model code liu et al 2018 global search algorithms such as a grid search or genetic algorithms may avoid converging to a local optimum but in general require many more model runs a third possibility is a gradient free search algorithm such as the simplex method nelder and mead 1965 which seems to be a good choice for calibration of crop models there is calibration software that has been developed specifically for some crop models buddhaboon et al 2018 buis et al 2011 hunt et al 1993 and also some software that is designed to be easily coupled to any model doherty et al 2010 coupling parameter estimation software to a crop model is not simple and so modeling groups tend to use available software or even no software rather than developing new calibration software themselves this implies that for the improvement of calibration approaches for crop models it is not sufficient to propose guidelines for good calibration practices for the guidelines to be effective they must include software solutions that can be used by any model 4 4 summary of recommendations and of the overall approach based on an analysis of calibration approach by multiple modeling groups we propose the following guidelines for calibration of the phenology component of crop models before calibration one should pay careful attention to the choice of default values for all parameters if the objective is as here to make point predictions then a frequentist approach to calibration is fully justified one can use an ols criterion initially based on errors in units of days but the statistical assumptions underlying ols should be checked by analyzing residuals after calibration the choice of parameters to estimate should be adapted to the available data for example using the aic criterion however the choice of parameters also requires knowledge of the model and the environments studied as well as of the reliability of the data this suggests that a fully automatic calibration procedure may not be advisable for the calculations a derivative free but efficient search algorithm like the simplex is recommended these are guidelines and the detailed implementation will depend on the crop model these recommendations are specific to calibration of the phenology component of crop models even though many of them are no doubt more widely applicable calibration of process based models in other fields may involve aspects specific to those fields requiring somewhat different recommendations these could be developed by following the same three steps followed here namely a multi model calibration exercise then detailed description of the calibration approach by each modeling group and finally analysis of the calibration choices a simplified version would simply document hypothetical choices by each group without actually requiring the groups to perform the calibration this would greatly simplify the study but might not truly represent what each group would do in practice 5 conclusions calibration of crop models involves multiple decisions which can be grouped into choice of criteria for defining the best parameter values choice of parameters to estimate and choice of algorithm and software different modeling groups make quite different decisions even for modeling groups using the same model structure it seems that we are far from having a consensus on how to calibrate crop models even in the relatively simple case with only phenology data which emphasizes the need for calibration guidelines such as those suggested here we propose an original approach to development of crop model calibration recommendations and apply it to develop recommendations for calibration of the phenology component of crop models one original aspect is that we consider a very broad range of decisions involved in calibration whereas other studies usually concentrate on one or two specific calibration decisions this is important because each of these decisions can have an important impact on calibration secondly our starting point for recommendations is the choices made in practice by multiple modeling teams this ensures that we consider a wide range of possible choices for each of the calibration decisions and that the choices considered are anchored in actual practice the recommendations are then based on analyzing the advantages and disadvantages of the different choices that are made of course it will be important in the future to test these recommendations as to their ability to improve model predictions in developing recommendations for calibration in other fields we suggest that a procedure similar to ours could be of interest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was in part supported by the collaborative research center 1253 campos project 7 stochastic modeling framework funded by the german research foundation dfg grant agreement sfb 1253 1 2017 the academy of finland through projects aicroppro 316172 and divcsa 316215 and natural resources institute finland luke through a strategic project boostia the bonares project soil3 boma 03037514 of the federal ministry of education and research bmbf germany the deutsche forschungsgemeinschaft dfg german research foundation under germany s excellence strategy exc 2070 390732324 exc phenorob the project biomassweb of the globee programme grant number fkz031a258b funded by the federal ministry of education and research bmbf germany the inra accaf meta programme the german federal ministry of education and research bmbf in the framework of the funding measure soil as a sustainable resource for the bioeconomy bonares project bonares module b bonares centre for soil research subproject b grant 031b0511b the national key research and development program of china 2017yfd0300205 the national science foundation for distinguished young scholars 31725020 the priority academic program development of jiangsu higher education institutions papd the 111 project b16026 and china scholarship council the agriculture and agri food canada s project 1387 under he canadian agricultural partnership the dfg research unit for 1695 agricultural landscapes under global climate change processes and feedbacks on a regional scale the u s department of agriculture national institute of food and agriculture award no 2015 68007 23133 and usda nifa hatch grant n mcl02368 the national key research and development program of china 2016yfd0300105 the broadacre agriculture initiative a research partnership between university of southern queensland and the queensland department of agriculture and fisheries the academy of finland through project ai croppro 315896 the jpi facce macsur2 project funded by the italian ministry for agricultural food and forestry policies d m 24064 7303 15 of 6 nov 2015 the ministry of education youth and sports of czech republic through sustes adaption strategies for sustainable ecosystem services and food security under adverse environmental conditions project no cz 02 1 01 0 0 0 0 16 019 000797 the order in which the donors are listed is arbitrary appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105206 
25738,land use land cover lulc change assessment and prediction are essential for optimised water resources planning and management this paper attempts to intercompare the different lulc change modelling techniques two hybrid and two traditional models to predict the future lulc change these include multi layer perceptron markov chain mlp mc logistic regression markov model lr mc and two hybrid models namely multi layer perceptron markov chain cellular automata mlp mc ca logistic regression markov model cellular automata lr mc ca models these models were tested on nagavali river basin nrb a river basin in southern india which has seen significant land use changes over the past two decades over the past two decades the study region experienced dominant changes on the order of 17 42 and 15 22 decrease in scrubland and forest respectively at the same time the agricultural land cover is increased by 35 28 for the lulc prediction the model was initially trained using the relevant driver variables and lulc maps of 2010 and 2015 the calibrated model was validated using the 2020 lulc map the statistical results in terms of kappa values and chi square results reveal that the hybrid model mlp mc ca has a better agreement kappa coefficient 0 902 compared to the other models further it is also observed that the ca based models have a better ability to capture spatial connections after combining the mlp mc model with the cellular automata the former model was improved by 10 8 in terms of the overall kappa coefficient the best model for lulc prediction over the next decade lulc map of 2030 showed that the forest area would decrease by 9 02 and the agricultural land would increase by 8 74 further the results from the study indicate that the hybrid machine learning models provide a promising alternative for land use change prediction keywords land use land cover prediction multi layer perceptron markov chain cellular automata machine learning techniques 1 introduction land use land cover change lulcc is a transformation taking on the terrestrial surface of the earth to provide shelter and food to the increasing population defries and eshleman 2004 gaur et al 2020 land use is referred to as an anthropogenic utilisation of the earth surface like cultivation and industrialisation and land cover is referred to as the unaltered biological physical and terrestrial surfaces of the earth like water bodies buildings and forest behera et al 2012 islam et al 2018a 2018b different studies have demonstrated that land use land cover changes are crucial for environmental impact assessments policymakers hydrologists and ecological issues hu et al 2008 yirsaw et al 2017 lulc change can be attributed to urbanisation infrastructural development increased irrigation projects industrialisation agriculture intensification modification of the grassland extensive deforestation and overexploitation goldewijk 2001 more recently the lulc change grabbed attention due to its extreme impact on the environment at the basin at the watershed level and global scales abuelaish and olmedo 2016 islam et al 2018b setti et al 2020 have shown that the lulc changes in the southern part of india lead to an increase in streamflow similarly astuti et al 2019 dosdogru et al 2020 and hu and shrestha 2020 have shown significant changes in catchment dynamics due to these changes these implications of lulc changes make future lulc prediction as one of the inevitable steps for water resources planning and management studies in the recent past lulc change prediction studies were also play a vital role in desalination technologies and renewable energy sources panagopoulos 2021 panagopoulos and haralambous 2020 the prediction of the future lulc changes is key to the hydrologist environmentalist and water policymakers to make the appropriate decision policies for future development and sustainability modelling lulc changes with spatially explicit techniques helps to understand underlying processes and anticipate alternative future scenarios modelling of lulc changes is focused on developing mathematical relationships between the land cover that existed in the past and the set of driver variables like topographic demographic ecological and socio economic required to anticipate change in the future the variables that lead to the major changes in the lulc of the basin are the driver variables the physical biophysical and socio economic factors act as the most influenced drivers variables for the lulc changes araya and cabral 2010 hegazy and kaloop 2015 megahed et al 2015 yang et al 2015 some of the critical driver variables extracted from the proximity topography and demography variables are proximity to the roads streams rail network water bodies major cities elevation slope population population density and literacy rate such driver variables of the basin are extracted from the relationships between socio economic institutional policy factors and human environment systems and are multifaceted spatially and temporally in the recent past several methods have been adopted using the remote sensing data and the driver variables mentioned above to predict the future lulc these methods can be broadly classified into non hybrid approaches and hybrid methods the non hybrid approaches include cellular automata markov models ghosh et al 2017 islam et al 2018b similarity weights abdullahi et al 2018 mozumder et al 2016 logistic regression methods behera et al 2020 mozumder et al 2016 and more recently the machine learning models wherein the past land use change patterns were investigated and used for predicting the future land use the hybrid models whereas involve combining one of these methods with others for improved land use prediction the cellular automata ca model works on continuing the historical development and the surrounding impact amini parsa et al 2016 ghosh et al 2017 islam et al 2018b owing to its simplicity compatibility and ease of combining with the other models it has been widely used for the past decades see e g arsanjani et al 2012 li and yeh 2002 louca et al 2015 stevens and dragicevic 2007 yang et al 2016 however the main limitation of ca include its inability to consider the trend from the previous observations and the driver variables responsible for the changes within the basin area aburas et al 2019 kavanagh et al 2004 noszczyk 2019 markov chain mc is an analytical model which assumes that future lulc change is only a function of the historical transitions of the lulc categories due to its flexibility and adaptability in producing the complex transitions into a simple matrix of transition probabilities it was used in many studies muller and middleton 1994 jianping et al 2005 guan et al 2008 iacono et al 2015 mc model would result in transition matrices based on the statistical analysis of the land use classes from the historical maps one major drawback of this model is that it only considers the temporal trends without considering spatial considerations recently machine learning ml techniques like lr ann and svm support vector machine have been used to identify the transitions in the land use classes and have been accurate in predicting land use changes machine learning multi layer perceptron grekousis et al 2013 mishra et al 2018 and regression based models al juaidi et al 2018 hu and lo 2007 siddiqui et al 2018 are prevalent in lulc change predictions to overcome the limitations of individual models individual mc and ca the hybrid models are introduced by combining machine learning or regression based and transition based modelling approaches with the ca and mc models arsanjani et al 2012 gidey et al 2017 munshi et al 2014 munthali et al 2020 wang and maduako 2018 the hybridisation of the model can be done by combining the models like mlp mc lr mc is with the ca module and named mlp mc ca and lr mc ca respectively gaur et al 2020 kourosh niya et al 2020 sinha et al 2015 in the hybrid models the regression models are used to identify the transition potentials of the individual lulc class and the ca is used to capture the spatial aspects through transition sets neighbourhood effects and expert elicitation temporal effects are quantified by integrating with markov chain mc models the hybridisation of models allows the development of novel methods to address the complexity of real world systems apart from the primary individual models individual ca and mc hybrid models were able to capture land use and land cover with greater accuracy at the basin level barakat et al 2019 gaur et al 2020 gidey et al 2017 kourosh niya et al 2020 mishra et al 2018 sinha et al 2015 recently gaur et al 2020 utilised hybrid and non hybrid models to captured the lulc scenarios for the subarnarekha river and found that the mlp mc model was fit best according to gaur et al 2020 the relative performance of different hybrid models and their benefits over non hybridised models is however uncertain the literature review of the previous works on lulc prediction suggests that there are no conclusive results on the better performance of a single model and there have been significantly fewer works on comparing different methods therefore the main aim of the present study is to intercompare different machine learning methods for lulc change prediction further from the literature hamad et al 2018 kafy et al 2020 khawaldah et al 2020 li et al 2015 mirici et al 2018 mohamed and worku 2020 mondal et al 2016 rimal et al 2020 it is clear that the minimal work has done in the direction of the hybridisation of the lulc change models therefore in this paper an attempt was made to compare individual and hybrid models over a single platform for this purpose nagavalli river basin nrb southern india is considered wherein significant changes in the lulc has been observed in the last two decades setti et al 2020 nagavali river is a major source for irrigation and drinking water supply in the andhra pradesh and odisha states apart from choosing the model used for the lulc prediction the spatial scale of analysis also plays a significant role generally global national level analysis is preferred for overall lulc change trends assessment and global climate model simulations jardeleza et al 2019 liu et al 2016 roy et al 2015 however local basin scale analysis is chosen for water resources planning and management at the basin level loucks and van beek 2017 the basin scale lulc changes are crucial in understanding ecosystem functioning on which the critical human activities depend even though land use predictions at a national scale is available it is essential to investigate at a local scale as the parameters like topography climate hydrological components and anthropogenic factors have significant spatial variations gaur et al 2020 islam et al 2018b mohamed and worku 2020 shawul and chakma 2019 therefore estimation of the lulc changes at the basin scale is becoming a prominent component in recent research with the above discussed issues the present study is concentrated on the following tasks a temporal evolution of the land cover land use land use and land cover changes within the nrb catchment area for the last three decades b identifying the influence of the critical driver variables at the basin scale nrb basin lulc changes c comparison of the different machine learning based lulc change modelling techniques hybrid and non hybrid modelling techniques and application of the best model for predicting future land use in the present study four different modelling techniques two non hybrid and two hybrid modelling techniques were applied to identify the lulc changes within the nrb area the non hybrid modelling techniques like mlp mc and lr mc and hybrid models like mlp mc ca and lr mc ca were applied to the nrb area to predict the lulc changes 2 data and methodology 2 1 study area nagavali river basin nrb is one of the important rivers that flow in the states of orissa and andhra pradesh india the catchment of nrb covers an area of 9055 km2 and situated on 83 54 82 53 east longitudes and north latitudes of 19 44 18 17 in between the godavari and mahanadi river basins of southern india see fig 1 it originates at an elevation of 1300 m and flows along the eastern ghats of south india the catchment experiences an average annual rainfall of 1132 mm and the majority of the rainfall was received during the monsoon june to september the maximum and minimum temperatures are on the order of 34 5 c and 28 3 c respectively the catchment elevation ranges from 8 m to 1663 m above the mean sea level forest and cropland were the dominant land use in the nrb river with an occupancy of more than 70 per cent the major irrigation projects like the jhanjavathi rubber dam madduvalsa reservoir 2002 thotapalli reservoir peddagedda and the vattigedda reservoir have increased the irrigated area and thereby playing a significant role in the land use conversion based on the study by setti et al 2020 agricultural land in the basin has been increasing in the past two decades and continues to change due to infrastructure development and urban settlements along the river 2 2 data for the present study the georeferenced data were obtained from the various sources mentioned in table 1 lulc maps were obtained from the united states geological survey usgs portal using two different satellite imagery for the different periods cloud free lulc images in 1990 and 2000 were obtained from landsat 5 tm and 2010 data was obtained from the landsat 7 tm whereas for periods 2015 and 2020 the images were obtained from the landsat 8 oli tirs all lulc imageries were obtained at a 30 m 30 m spatial resolution with the utm 44 n projection and then all the data were classified supervised classification using the arcgis 10 4 1 to obtain six different lulc classes the srtm dem 30 m 30 m is used for extracting the surface topography and the slope map of the study region spatial input maps for the driver variables such as road network major cities river network and major water bodies were digitised and extracted from google earth images google earth pro 2020 and depicated in the fig 3 2 3 methodology in this section the stepwise methodology is presented and the schematic of the same is shown in fig 2 step 1 data extraction the major component of the lulc mapping is the pre processing and classification of the lulc images the former involves the composition of available bands removal of the zero value pixels mosaic and clipping the spatial extent of the data re projection whereas the latter is related to the classification of the pre processed image to obtain the lulc map for the study region for the nrb landsat 5 landsat 7 and landsat 8 satellite cloud free images were obtained from the usgs web portal https earthexplorer usgs gov then these images were projected to utm 44 north and with world geodetic system 1984 wgs 1984 as the datum the supervised maximum likelihood classification was adopted for classifying the registered satellite imageries of 1990 2000 2010 2015 and 2020 using arcgis 10 4 1 initially a training sample was created based on different band combinations of rgb finally rgb 4 3 2 a natural colour composite was used to identify most of the lulc classes apart from this a clear differentiation between the water bodies land surface and the forest area agriculture was made by using the rgb 7 5 3 and 5 4 3 band combinations the signature file was extracted from the training sample prepared in the earlier steps using different band combinations used as an input file for the supervised maximum likelihood lulc classification the nrb was identified with the six different significant classes of lulc viz built up waterbodies orchards forest agriculture and scrubland the accuracy assessment was carried out by using the confusion matrix or error matrix the accuracy assessment of the classified lulc maps was carried out to determine the degree of agreement between the classified maps with the ground truth points collected from the different sources areal photos field observations and lulc maps from the revenue department the accuracy assessment was done using the different metrics confusion matrix overall accuracy and kappa index of the agreement kappa coefficient the confusion matrix was generated between the reference ground points and classified lulc classes the producer s accuracy the number of pixels being accurately classified and the user s accuracy total number of correct pixels by the number of classified pixels of a category was estimated from the confusion matrix a detailed description of the methodology can be found in alemayehu a shawul and chakma 2019 and story and congalton 1986 the estimation of the kappa coefficient was carried out by using the statistical formula presented in the congalton 1991 shawul and chakma 2019 as given below 1 k n i 1 r x i i i 1 r x i x i n 2 i 1 r x i x i where k is the kappa coefficient r is the number of rows in the matrix xii is the number of observations in row i and column i diagonal elements xi and x i are the marginal totals of row i and column i respectively and n is the total number of observations step 2 spatiotemporal evolution of the lulc changes the trend surface analysis tsa is used to identify spatial trends of the major transitions between different lulc categories chorley and haggett 1965 tsa is an interpolation method that disaggregates the broad regional patterns from the fine scale non systematic variations in the data eastman 2006 it is designed to extract the regional component from a map such as the general location of a specific land change trend from the residual component chorley and haggett 1965 tsa performs trend analysis by considering the spatial distribution of the feature m with respect to the p number of coordinate points xi yi here feature m is taken as the dependent variable and is measured based on the coordinate points xi yi taken as independent variables the mathematical relation between the dependent feature and independent variables using third order polynomial is given as ii m x y a1 a2x a3x2 a4xy a5y2 a6 x2y a7xy2 a8y3 a9x3 here m is the spatially distributed variable which indicates the major transitions of the lulc maps between the two different periods whereas the a1 a2 a3 a9 are the coefficients of the third order polynomial and y represents the coordinates of the transition locations pixels with transitions were given a value of 1 and those with no transitions were represented with a value of 0 further the pixel values with 0 were excluded and the surface analysis was carried out to fit the third order polynomial for the remaining pixels in tsa the coefficients of the third order polynomials were estimated through the least squares method the temporal change analysis of the lulc maps of the two different periods was done using the cross tabulation matrix in terms of total changes losses gains net change by category swapping and persistence between the two time periods the total changes in any class are indicated through the cumulative gains and losses between the two periods whereas persistence indicates the unchanged portion of the lulc maps step 3 input variable selection selection of the input driver variables driving the land use changes is an essential step in modelling lulc changes three different categories of driver variables viz proximity biophysical and socioeconomic variables were considered driver variables the proximity variables considered in this study include euclidean distance to the major cities euclidean distance to the major roads national and state highways and major district roads euclidean distance to stream network and euclidean distance to major reservoirs the biophysical variables include climate soil topography lithology and vegetation the lithology and soil type changes within a specific area over time would significantly influence the variations occurring within agriculture scrubland and forest lands the topography would play a major role in developmental activities like urbanisation industrialisation and intensification of agriculture the parameters like elevation and slope derived from the dem are considered in the model due to its significant variation within the basin area as the built up area percentage within the basin is minimal compared with the remaining land use classes the population and population density parameters are eliminated in the prediction process of the lulc maps setti et al 2020 have shown that agricultural development is prominent in recent decades however the built up area percentage within the basin is minimal compared with the remaining land use classes hence the population and population density parameters were not included in the prediction process of the lulc maps the driver variables such as distance to the reservoir major cities were extracted for the recent period from lulc maps and road and rail network were extracted from the google earth images before analysing the lulc transitions between the different time periods the influence of different driver variables on the lulc maps was estimated using cramer s values v cramer s values explain the strength of association between the lulc changes and different driver variables by considering the chi square values if the cramer s values lie between 0 15 and 0 4 then variables moderately influence the land use changes were considered good if the values greater than the 0 4 strong association is observed between the change patterns and driver variables and when the values are less than 0 1 the variables are not considered for the model gaur et al 2020 the transitions between the two initial lulc maps here 2010 and 2015 are represented as boolean maps and the driver variables expressed as stretched values between 0 and 256 are considered an input in the analysis step 4 model calibration model calibration involves combining the lulc maps at time t and t 1 and the driver variables to find change locations the driver variables acquired and changes observed from the past lulc maps were used to identify the spatial variability of each lulc transition probable locations of changes are called as transition potential maps tpms and the generation of the lulc transition potential maps tpms was carried out by using the different models these maps indicate the probability of the areas that have undergone transition and vary between 0 and 1 mathematical models can be used to approximate and generate the transition potential in a precise way this study used two machine learning models multi layer perceptron mlp and logistic regression lr to generate the tpms the future prediction of the lulc is performed by combining the generated tpms with markov chain mc markov chain mc is a stochastic model in which the present state of the system at a specific time tt is dependent only on the preceding time step of the lulc map tt 1 gaur et al 2020 markov chain mc model utilises the two lulc maps at the previous time step to develop the transition probability matrix tij the transition probability matrix tij explains the probability of change from one state to another gaur et al 2020 making use of the bayes probability theorem and markov process the prediction using mc is given by 3 l t 1 i 0 m tijxlt where l t 1 and lt represent the lulc at the periods t 1 and t respectively and tij represents the changing transition probability of the ith land cover to jth land cover here m is the number of lulc classes within the basin area a detailed description of each of the modelling strategy adopted in this study is given below a multi layer perceptron markov chain model mlp mc model using the procedure explained at the beginning of this section the mlp based model can be combined with the mc to predict future land use wherein the mlp generates the transition potential maps and mc is used to predict the temporal changes mlp involves a feed forward neural network with three layers input hidden and output layers mas and flores 2008 the training of the mlp model involves adjusting the weights at the input and output layers using the backpropagation method the model is trained till satisfactory results in terms of accuracy is achieved after training the model is used for testing the model performance depends on hyperparameters such as the number of layers learning rate iterations in this study driver variables are taken as input layers and the transition probability is taken as the output layer based on aguejdad et al 2017 a minimum accuracy of 80 is required for the model results to be used for land use predictions b logistic regression markov model lr mc the lr mc is an integrated model in which the lr model is used for generating the tpms and mc captures with the temporal changes the model the logistic regression is used for estimating the association between the dependent and independent variables statistically here lulc classes were considered as dependent variables whereas driver variables are considered as independent variables the spatial maps of the dependent variables are converted into boolean maps made of two binary values 1 existence and 0 non existence the model assumes that the probability of the dependent variable is 1 and follows the sigmoidal curve given by σ x 1 1 e x lr is mathematically expressed as given below 4 ln p 1 p 1 β 0 β 1 x 1 β 2 x 2 β 3 x 3 β n x n ε here probability of the dependent variables represented with the p whereas the β 0 is an intercept variable β 1 β 2 β 3 β n are the coefficients associated with the independent variables x 1 x 2 x 3 x n and ε is considered as residual lr model capable of using only one transition at a time while utilising the lr models spatial data has to be corrected for autocorrelation and heterogeneity so that the errors in the simulation can be minimised in this study a stratified random sampling technique is used to reduce errors due to autocorrelation and heterogeneity arsanjani et al 2012 xie et al 2000 yirsaw et al 2017 the accuracy of the tpms was estimated by using the roc module in the idrisi tool the tpms with an accuracy rate of 0 65 or higher are incorporated in the model to predict lulc it is to be noted that the outcomes from the above methods lack spatial connection i e the model results are provided for the individual land cover but do not give any information about the spatial connection of the results even though the tpms consider the spatial components it relates to the change of one particular lulc class and therefore cannot capture information regarding remaining lulc classes within the basin area defries and eshleman 2004 further mlp and lr based regression models sometimes have difficulties estimating the temporal and spatial changes of the lulc exactly wu 2002 gaur et al 2020 to overcome this limitation these models can be combined with cellular automata ca models wherein the ca can add the spatial connection ghosh et al 2017 a brief description of ca and the hybrid model formation is given in the following sub section c hybrid model the combination of the mlp mc or lr mc with the cellular automata ca is defined as a hybrid model mlp mc ca or lr mc ca gaur et al 2020 sinha et al 2015 in hybrid models the mlp mc or mlp lr is used to simulate the lulc maps by considering changes related to the temporal scale time based changes and the ca model is used to capture the spatial changes in lulc the ca module was explained in details lu et al 2019 ca are incorporated by using the group of the grid cells and set of states specifying the grid cells in general the ca is defined by a group of cells and characterised by the set of states in these cells the temporal configuration of the states of these grid cells is defined through certain transition rules which can be mathematically presented as 5 c t 1 h c t j t m where c t 1 and c t are represented as cell states at the time steps t 1 and t respectively and the h is transition function rule j t m refers to the neighbourhood grid cells and here m represents the size of the neighbourhood grid cells the framework of the hybrid models was set up by integrating the markov chain processes neighbourhood interaction rules and the driver variables in a unique platform the hybrid model transition probability suggested by poelmans and van rompaey 2010 is given below 6 p h y b r i d p r e g m n e i g h b o u r here p h y b r i d a n d p r e g are the transition probability matrix generated by using the hybrid and regression models respectively mneighbour captures the effect of neighbourhood considered in the cellular automata ca in essence the hybrid models use neighbourhood grid cells information and the driver variables to predict the lulc maps in this paper two different hybrid models namely i combination of lr mc and ca denoted as lr mc ca and ii combination of mlp mc and ca i e mlp mc ca is used in this study and compared with the regression models step 5 validation stage from the observed lulc maps at three time steps t1 t2 t3 the first two was used model calibration and the recent one t3 was used for validation the validation was carried out by assessing the model s ability to capture the actual amount of land use change the persistence of each class and also the spatial distribution of the change generally there are several metrics available for model validation and among these kappa indices is the most widely used method kappa indices can precisely answer the questions like how well do a pair of maps agree in terms of the number of cells in each category and how well a pair of maps agree in terms of the location of cells in each category kappa indices consider different parameters of agreement and disagreement the kappa values like kappa number kno kappa location klocation kappa stratum kstrata and kappa standard kstandard were estimated for identifying the overall performance of the model klocation and kstrata were explained how well the grid cells are located within the lulc class and stratum respectively kappa standard is also known as the kappa index of the agreement kia which explains the overall agreement between reference and predicted lulc maps stage 6 model application future land use prediction once the validation was done for all the models the best model in terms of error is utilised to predict future lulc maps in this study we have predicted the lulc map of 2025 and 2030 using the tpms generated using 2010 and 2015 land use maps and the driver variables were extracted in 2015 in general the land use models can predict the future lulc maps reliably for about 2 3 decades defries and eshleman 2004 serneels et al 2001 yatoo et al 2020 3 results 3 1 lulc temporal mapping for nagavali river basin the classified land use land cover lulc maps were considered for 1990 2000 2010 2015 and 2020 the accuracy assessment of the lulc maps in terms of overall accuracy and kappa values was carried out using the confusion matrix and shown in table 2 overall the accuracy values were good with all values greater than 80 and the kappa coefficient was in the range of 0 81 0 88 these results indicate the reliability of the classification method adopted in the present study see table 2 the lulc over the interval 1990 2010 shows severe decrement within the scrubland 18 79 3 91 due to establishing the reservoirs and canal networks setti et al 2020 most of the scrublands were converted into agricultural lands 17 27 further there is a considerable degradation of the forest land 49 78 46 96 also the waterbodies within the nrb has increased 0 58 1 25 due to the construction of reservoirs during the period lulc maps over the time interval 2010 2020 demonstrate that forest and agricultural lands are prominent within the basin area fig 4 in nrb an enormous increase in agricultural land was identified 46 60 and the forest area decreased to a great extent 47 35 scrubland decreased from 3 9 to 1 5 and there was a slight increase in the built up area 0 11 0 36 and the orchards land cover area 0 76 0 95 3 1 1 nrb spatial dynamic of change in the present study area the spatial trends of lulc change maps were presented over a recent decade 2010 2020 even though 30 years of land use data from 1990 to 2020 for the trend analysis and the same has been presented in fig 4 however we have considered the recent 10 years of transition for the land use change prediction because the future trends would follow the recent past than the historical changes further a significant development was observed in the nrb basin in the recent decades compared to the previous decade therefore it would be more appropriate to consider the transformation in the recent decades for the model development initially the classified lulc maps over 2010 2020 were used to identify the spatial trends the spatial trends of the lulc maps for the significant transitions were generated using the tsa module tsa is used to generate spatial trends by fitting 3rd order polynomial to capture the spatial changes from agriculture to built up agriculture to the forest agriculture to waterbodies and so on and the obtained spatial changes are shown in fig 5 the nrb shows a declining trend in the forest whereas a drastic increasing trend is identified in agricultural land the forest area along the periphery of the nrb basin has been converted into agriculture the built up areas have expanded around cities like rayagada kalahandi srikakulam and parvathipuram 3 1 2 temporal dynamics of change fig 6 shows the net gains in terms of sq km for each land use for the period from 1990 to 2020 the major changes include scrubland to agriculture and forest to agriculture to capture the trend for the time window 2010 2015 the statistics in terms of percentage gains losses net changes total changes and swapping are shown in table 3 there is a considerable loss in the forest and scrubland and gain in the agricultural land during the five years further the temporal dynamics of the lulc maps were analysed using the cross tabulation module to estimate the percentages in different land cover categories from 2010 to 2015 the cross tabulation values show that the scrubland and orchards have the lowest percentage of persistence with values holding 24 40 and 38 46 of pixels respectively within the nrb area the agricultural land is the most persistent with 95 75 of the pixels and the remaining 14 25 is converted into built up generally the built up area s persistence values would be around 100 because the possibility of converting the built up area to other land use classes is minimum however in the present analysis the built up area is captured with the persistence values of 87 78 this deviation could be attributed to the submergence of villages due to the construction of reservoirs mainly thotapalli reservoir within the nrb area the forest land use possessed a persistence of 82 29 and out of the remaining 17 71 of forest land 15 was converted into agricultural land the agricultural land has a gain of 724 017 km2 from the forest land the water bodies are having moderate persistence values of 69 07 the cross tabulation results for the interval 2010 2015 are shown in table 4 and help summarise the gains losses and swapping of the different land use classes among them among all the land use classes agriculture has a maximum net gain of 816 41 km2 and the gain of agricultural land use was mainly from the forest 648 16 km2 and scrubland land 176 73 km2 the spatial maps of net gains losses and persistence for each land use are shown in fig 7 from fig 7 it was evident that the scrubland is showing the least persistence values and agriculture and urban areas showing the highest persistence values within the basin area the area along the eastern ghats of andhra pradesh and odisha states of india is converted into agricultural land cover due to improved irrigation technologies the scrubland land cover was converted into agriculture due to the water resources projects in the river basin kosam 2015 cwc report 2015 it was observed that the built up land use is doubled 10 26 km2 22 86 km2 within this interval it was extended all around the major cities like rayagada kalahandi and parvathipuram srikakulam of the odisha and andhra pradesh states respectively from fig 7 it was evident that forest land cover is following the decreasing trend within the nrb area within the interval 2010 2020 the forest land cover was decreased by 1069 94 km2 11 81 of the total area within the basin area this decrease can be attributed to the increasing irrigation potential on hilly terrains due to its soil fertility and increasing population all along the nrb area the urban area development is also one of the primary reasons for the decreasing the forest area the reservoirs on the nagavali river basin also decreased the forest area and increased agricultural land use even though irrigation structures were constructed just before 2010 the functioning of these irrigation structures was during the period 2010 2020 due to this the scrubland land cover is converted into agricultural land leading to 1261 8 km2 there were also changes observed in water bodies orchards and built up area but the corresponding areas were not significant compared to the forest and agriculture 3 2 calibration 3 2 1 selection of the driver variables the cramer s v test was performed to identify the association between the driver variables and land cover class lulc the cramer s v values were measured between the lulc and driver variables for the intervals of 2010 2015 and 2015 2020 shown in table 5 the cramer s v values for the variable association with lulc greater than 0 15 can be considered in the model to simulate the transition potential maps and vice versa gaur et al 2020 the values greater than 0 4 or higher are considered to be good from table 5 it was clear that the geographical driver variables like slope and elevation show the maximum association values like 0 44 and 0 42 with the lulc for the interval 2010 2015 further the same variables elevation and slope shows a better association 0 54 and 0 49 with lulc classes in the next interval 2015 2020 this behaviour could be explained as flat areas areas with minimum slopes or slopes 10 within the nrb areas are converted into agricultural land and this change can be observed in the lulc maps as shown in fig 4 from the cramer s v values it can be observed that the elevation and slope have significant influence on the land use changes while the other factors have comparatively less influence this could be attributed to the fact that the nrb area is a forest and agriculture land use dominant basin and slope and elevation act as a physical control for locations of agriculture further paddy is the primary crop irrigated in nrb so areas with mild or moderate slopes would be considered for agriculture in that context only the areas with moderate to mild slope have been observed to be converted to agricultural land use the same can be visualised from fig 12 wherein it can be seen that the agriculture land use change was pertaining to the areas with mild slopes for an agricultural dominant land use change the topographical factors such as elevation slope have greater influence than other proximity variables everest et al 2021 ewunetu et al 2021 said et al 2021 the proximity variables like distance to the major roads and distance to stream network have a low degree of association with lulc classes because of their less influence on the lulc changes 3 2 2 generation of the lulc transition potential maps in the present study transition potential maps tpms were generated by utilising the two different lulc models namely multi layer perceptron mlp and logistic regression lr along with the selected driver variables while training the mlp model initially 10 000 random points from input lulc were selected by the model as the initial iteration in the training procedure the model hyperparameters in the training process of the mlp are learning rate momentum factor a number of the hidden layers and these values were altered till accurate results were achieved based on different trials their values were estimated as 0 01 0 5 and 8 respectively all tpms were generated by using the selected driver variables and the tpms having an accuracy rate of more than 80 were used in the prediction of the lulc maps the tpms were also generated using the logistic regression model and the model s accuracy was assessed using the roc module the tpms with values of the roc greater than 0 65 were used in the model to predict future lulc maps gaur et al 2020 the suitable transition potential maps from both mlp and lr models were shown in fig 8 in fig 8 high and low susceptible transitions areas were shown with the colour variation from fig 8a it was observed that the major transitions in nrb over the last decades include conversion of the forest to agriculture and scrubland to agriculture having transition probability on the order of 0 64 and 0 5 respectively apart from these two dominant land use changes there are no significant changes observed within the catchment the agriculture and water bodies seem to have high persistence values within the catchment as expected there is a significant conversion of forest to agriculture particularly in the areas close to the flood plains and in the areas with mild to moderate slopes the scrubland also showed a high tendency to convert into agricultural land which matched with the original lulc2020 a lower rate of conversion from the orchards to agriculture was shown within the mlp based transitions the agriculture and waterbodies have high persistence values within the nrb area from fig 8b the major transitions like forest to agriculture land use had a low magnitude of 0 38 in the lr model the spatial extent of the conversion from forest to the agricultural land use was identified to be low within the study area even though scrubland to agriculture land use transition was having the highest magnitude of 0 96 but it was pertaining to the lower portion of the area from the temporal evolution shown in fig 4 it was identified that the agricultural land use is increasing due to a decrease in the forest land use during the time from 2010 to 2015 these transitions are captured in the mlp model transitions rather than in the lr model transitions the transition from forest land use to agriculture was having the high magnitude of 0 64 compared with the lr transitions of 0 38 the lr based transition maps show a higher probability of water bodies with magnitude of 0 99 wherein the mlp model shows only 0 55 overall comparing the actual changes and the transition maps the mlp model can capture all the temporal observations to a greater extent 3 3 simulation the transition probability matrix for the year 2020 was obtained using the markov chain mc model the mc model utilises lulc maps of 2010 and 2015 to develop the transition probability matrix of the lulc 2020 the selected tpms from the mlp and lr models were combined with the mc model transition probability matrix for simulating the future lulc maps here initially for comparison purposes two models mlp mc and lr mc models were used to predict the lulc map of 2020 further the cellular automata ca model was combined with mlp mc and lr mc models for implementing the hybrid models in the lulc prediction in the present study two hybrid models mlp mc ca and lr mc ca were also utilised to simulate the lulc map for 2020 the predicted lulc 2020 map by using the four different methods are shown in fig 9 the predicted lulc 2020 map by using these four models was validated with different validation techniques comparing the results from the four different methods and the actual land use map for 2020 fig 4e it can be observed that the mlp mc ca model results are closer to the observed maps the spatial extent of forest conversion and scrubland conversion has been captured to greater accuracy by the mlp mc ca model fig 9e from fig 8 mlp model transitions were observed to capture all temporal evolutions more accurately than the lr model transitions fig 9 show the predicted land use from different model combinations and the observed land use in 2020 the model outcome from mlp mc ca closely matches the actual land use in 2020 in terms of the spatial extent of the different land use patterns compared to the other model s outcome fig 9e summarises the results of the percentage error in model prediction for different land uses compared with the actual lulc 2020 map the percentage error for agricultural land and forest cover was lowest 1 for the mlp mc ca model however there is an underestimation of forest cover by mlp mc and lr mc models there is also an overestimation of the agricultural land use in all the models except for the mlp mc ca model there is a significant overestimation for scrubland on the order of 40 60 by the mlp mc lr mc and lr mc ca models whereas the error is only around 5 for the mlp mc ca model a similar observation was seen in the orchards landuse wherein the mlp mc ca models performed better than the other models compared to ca and without ca the hybrid models with ca have performed better than the models without ca the percentage error values of the mlp mc ca models for the forest agriculture and scrubland land uses were close to zero 3 4 validation of lulc maps the validation of the predicted lulc2020 map obtained from mlp mc lr mc mlp mc ca and lr mc ca models was done using the actual lulc 2020 the accuracy of the model results was estimated by using the kappa values like kappa number kno kappa location klocation kappa stratum kstrata and kappa standard kstandard table 6 provides the validation results for all models and it is clear that mlp mc ca is performing better in terms of all performance statistics than the mlp mc lr mc and lr mc ca apart from the kappa values the persistence was also tested at the grid cell level to identify the model suitability the chi square goodness of fit test results with alpha 0 05 between the actual land use classes and predicted land use classes of the lulc 2020 is shown in table 7 from table 7 it is clear that the mlp mc ca model outperforms other models further it can be observed that the mlp based models have outperformed the lr based model for the study region and among lr based models there is no significant difference in the performance metrics each land cover category of the predicted and actual lulc 2020 maps was compared to assess its persistence and obtained persistence values of each class presented in fig 10 from fig 10 the mlp mc ca model was having the highest average persistence values average persistence values of all land cover classes 91 for all land cover categories among the four methods and the mlp mc model had the second highest persistence values with the actual lulc 2020 map the selected best model mlp mc ca was utilised to predict lulc changes for future periods 3 5 prediction of future lulc of the nagavali river basin mlp mc ca model is selected as the best model for predicting the lulc changes the selected mlp mc ca model was used to predict lulc changes for the nagavali river basin for 2025 and 2030 from fig 11 it was clear that the forest area is continuously decreasing while agricultural land is increasing within the nrb area it could also be observed that the built up area agricultural land and the waterbodies follow the increasing trend whereas the remaining land use like forest area scrubland and orchards shows the decreasing trend within the nagavali river basin it is evident that from fig 11 forest area was decreased by 9 02 34 41 25 29 over one decade from 2020 to 2030 simultaneously the agricultural land is increased by 8 74 61 59 70 53 within the same period built up area waterbodies orchards and scrublands followed the historical trends but the changes with the land use classes over 2020 2030 were not significant due to the less extent of the area from fig 11 it was evident that forest area would decrease by 5 04 34 42 29 37 and 3 98 34 41 25 29 in the following two consecutive five year spans from 2020 to 2025 and 2025 2030 respectively simultaneously the agricultural land use would increase by 5 09 61 59 66 68 and 3 65 66 68 70 33 within the same period built up area water bodies orchards and scrublands followed the trends but the changes with the land use classes over 2020 2030 were not significant due to the less extent of the area 4 discussions the present study focused on investigating different machine learning based lulc prediction models for this purpose nagavalli river basin was considered for analysis as there were considerable changes over the last three decades the analysis was done in three different steps like temporal evolution of the land use identifying the suitable driver variable and model development for predicting the lulc changes and predicting future lulc changes using the best model 4 1 temporal evolution of land use the results from temporal evolution analysis showed that there are significant changes in the catchment notably the agricultural land use in the nrb area is increased by 35 28 while scrubland and forest area is decreased by 17 42 and 15 22 respectively it was also observed that most of the scrubland and forest areas were converted into agricultural land use these changes can be attributed to the implementation of the irrigation projects at different time period over the three decades for example madduvalasa reservoir stage i was completed in 2002 with an irrigation command area of 23 000 acres https en wikipedia org wiki madduvalasa similarly the thotapalli reservoir project was completed in the year 2015 with an irrigation command area of 1 20 000 acres by using the canal length of 110 km in vizianagaram and srikakulam districts of the ap covering the both vizianagaram and srikakulam districts of andhra pradesh https en wikipedia org wiki thotapalli barrage the hindu 2015 new indian express 2015 further several incentive schemes by the government of andhra pradesh and odisha encouraged the people to take up agriculture department of agriculture and farm empowerment dafe odisha 2015 the hindu 2015 new indian express 2015 dofe odisha 2015 reported that the odisha state people were encouraged to do the farming by providing crop insurance policies these developmental activities at the watershed and the management policies have significantly led to the scrubland s conversion into agricultural land use 4 2 influence of driver variables further four different lulc change prediction modelling techniques namely mlp mc lr mc mlp mc ca and lr mc ca were compared for their ability in capturing the lulc changes before the model development input driver variables were tested for strength associated with the lulc maps to identify the lulc changes the cramer s v test shows that the topographic parameters like elevation and slope have good strength v 0 4 associated with the lulc changes in nrb most of the forest area was converted into agricultural land during 2010 2020 see fig 4 adhikari et al 2017 stated that the same trend was observed along the eastern ghats of south india the present study showed that the areas pertaining to the mild slopes even with high elevations show greater transitions than the steep slopes from fig 12 it can be noted that the areas having mild slopes slopes varying between 0 and 10 even at higher elevations were converted into agricultural land use many studies like birhane et al 2019 lee et al 2018 and munroe et al 2004 showed that there is no significant land use change at higher elevations however the present study shows that expansion of agricultural land use was observed even at higher elevations further the analysis also showed that the proximity driver variables like distance to major roads distance to the major cities distance to the water bodies show lower association with the lulc maps 4 3 performance of the hybrid models after selecting the suitable driver variables machine learning models were developed to identify the suitable changes between the past lulc maps between 2010 and 2015 lulc change prediction accuracy dominantly relies on the machine learning techniques ability to produce the transition potential maps and transition matrix yuzar et al 2006 among the different models compared in the study the integrated hybrid model mlp mc ca model outperformed all others it was observed that the mlp model scored better than the lr models as the mlp models can capture the complex relationship between the predictor and predictands than the logistic regression models these observations are congruent with previous studies guidigan et al 2019 kafy et al 2020 mirici et al 2018 rimal et al 2020 across the globe which have also shown that the mlp model performs better than the lr model another study by shawul and chakma 2019 upper awash basin area have shown that the ml based model performed better in capturing the land use changes when compared to the traditional model similarly gaur et al 2020 have shown that for a river basin in india mlp mc models yield accurate prediction compared to models based on logistic regression and similarity weights instance method from the present study it is observed that combining the mlp mc and lr mc models with ca significantly improved the model results in terms of kappa location tables 6 and 7 and fig 10 show that the mlp mc ca models perform well compared to the other three modelling techniques to predict the lulc changes the improvement in the model performance can be attributed to the fact that the ca model can simulate the hierarchical and complex structures at a large scale batty and xie 1994 lauf et al 2012 stevens et al 2007 tang and di 2019 wu and martin 2002 further the ca based models consider the spatiotemporal variations in the catchment and geographical connections arsanjani et al 2012 lin et al 2020 munthali et al 2020 according to tang and di 2019 the ca model has the innate ability to adjust the rules based on the neighbourhood the results from the study showed that the integration of the ca module to the machine learning techniques like mlp mc had improved the prediction of the lulc change simulation to a greater extent from tables 6 and 7 and fig 10 the kappa coefficient kappa standard was improved by 10 8 0 816 0 909 and the chi square metric was reduced by 77 28 64 6 73 the average persistence values of the lulc changes were increased by 10 33 81 33 91 respectively overall it was observed that the integrated hybrid model based on ca provided the best results when compared to the non hybrid models without ca it is also important to note that the present work shows a way forward for applying machine learning based hybrid models in land use prediction studies in the present study the lulc changes were predicted up to the 2030 but the same approach can be applied to predict long term land use but the reliability of the predictions mainly depends on the initial training data sets and driver variables further such long term prediction requires more training period and also the associated uncertainty with such prediction is high the performance of the lulc prediction models is affected by two kinds of uncertainties aleatory and epistemic these uncertainties can be further categorised into input data uncertainty parameter uncertainty estimation of the model parameters and structural uncertainty model ability to capture the catchment s response these uncertainties certainly will be very high for long term prediction as the land use changes are highly dynamic and dependent on many unaccountable factors such as local government policies future climate etc and it is important to quantify the uncertainty involved in such prediction since the present study focuses on comparing different models the estimation of the associated uncertainty can be considered the future scope the results from this study are essential for policymakers and water resources development since there is a considerable increase in the agricultural landscape and reduction in the forest there would be changes in the hydrological regimes and water balance as shown in setti et al 2018 therefore the results from the present work might help the government develop policies and water resources projects the present study results can be directly used to interlink the nagavali river basin with the vamsadhara river basin further this research work can generate the different scenarios of the lulc change implication on the nrb area hydrological components 5 conclusions the present compared the different machine learning approaches used for future land use land cover predictions the study compared two non hybrid models mlp mc and lr mc and hybrid mlp mc ca and lr mc ca models for this purpose nagavalli river basin a river basin in southern india which has undergone significant land use changes is considered a testbed preliminary analysis of temporal evolution of lulc changes over 1990 and 2020 showed the drastic increase in the agricultural land use class due to establishing the irrigation projects within the nrb area over the past two decades the study region experienced dominant changes on the order of 17 42 and 15 22 decrease in scrubland and forest respectively at the same time the agricultural land cover is increased by 35 28 the agriculture land use class is increased at an expanse of the scrubland and the forest land use classes the physical control of the lulc changes in nrb was identified by using cramer s v method and it was found that the among different driver variables considered slope and elevation were the major influencing parameters only the areas with moderate to mild slopes have been observed to be converted to agricultural land use the performance of four different models was evaluated based on the different metrics like kappa values chi square and persistence of the different lulc classes among the four models mlp mc ca outperform the remaining three ml models like mlp mc lr mc and lr mc ca the spatial accuracy of the mlp mc ca model is having the best value kappa as 0 909 and which is 10 12 and 15 more accurate than the mlp mc lr mc and lr mc ca models respectively the chi squared statistical analysis of the different models with the original lulc 2020 shows that the mlp mc ca models perform better than the remaining models the persistence of the mlp mc ca model is 91 compared with the original lulc 2020 showing that the hybrid mlp mc ca modelling technique can identify the lulc changes to a greater extent it was identified that the hybrid models could replicate ground reality due to the inclusion of the spatiotemporal evolution and geographic connections the results from this study are essential for policymakers and water resources development since there is a considerable increase in the agricultural landscape and reduction in the forest there would be changes in the hydrological regimes and water balance therefore the results from the present work might help the government develop policies and water resources projects the present study results can be directly used to interlink the nagavali river basin with the vamsadhara river basin further this research work can generate the different scenarios of the lulc change implication on the nrb area hydrological components declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements dr rm gratefully acknowledges the funding received from the department of science and technology government of india under the grant dst wti dd tm 2k17 79 
25738,land use land cover lulc change assessment and prediction are essential for optimised water resources planning and management this paper attempts to intercompare the different lulc change modelling techniques two hybrid and two traditional models to predict the future lulc change these include multi layer perceptron markov chain mlp mc logistic regression markov model lr mc and two hybrid models namely multi layer perceptron markov chain cellular automata mlp mc ca logistic regression markov model cellular automata lr mc ca models these models were tested on nagavali river basin nrb a river basin in southern india which has seen significant land use changes over the past two decades over the past two decades the study region experienced dominant changes on the order of 17 42 and 15 22 decrease in scrubland and forest respectively at the same time the agricultural land cover is increased by 35 28 for the lulc prediction the model was initially trained using the relevant driver variables and lulc maps of 2010 and 2015 the calibrated model was validated using the 2020 lulc map the statistical results in terms of kappa values and chi square results reveal that the hybrid model mlp mc ca has a better agreement kappa coefficient 0 902 compared to the other models further it is also observed that the ca based models have a better ability to capture spatial connections after combining the mlp mc model with the cellular automata the former model was improved by 10 8 in terms of the overall kappa coefficient the best model for lulc prediction over the next decade lulc map of 2030 showed that the forest area would decrease by 9 02 and the agricultural land would increase by 8 74 further the results from the study indicate that the hybrid machine learning models provide a promising alternative for land use change prediction keywords land use land cover prediction multi layer perceptron markov chain cellular automata machine learning techniques 1 introduction land use land cover change lulcc is a transformation taking on the terrestrial surface of the earth to provide shelter and food to the increasing population defries and eshleman 2004 gaur et al 2020 land use is referred to as an anthropogenic utilisation of the earth surface like cultivation and industrialisation and land cover is referred to as the unaltered biological physical and terrestrial surfaces of the earth like water bodies buildings and forest behera et al 2012 islam et al 2018a 2018b different studies have demonstrated that land use land cover changes are crucial for environmental impact assessments policymakers hydrologists and ecological issues hu et al 2008 yirsaw et al 2017 lulc change can be attributed to urbanisation infrastructural development increased irrigation projects industrialisation agriculture intensification modification of the grassland extensive deforestation and overexploitation goldewijk 2001 more recently the lulc change grabbed attention due to its extreme impact on the environment at the basin at the watershed level and global scales abuelaish and olmedo 2016 islam et al 2018b setti et al 2020 have shown that the lulc changes in the southern part of india lead to an increase in streamflow similarly astuti et al 2019 dosdogru et al 2020 and hu and shrestha 2020 have shown significant changes in catchment dynamics due to these changes these implications of lulc changes make future lulc prediction as one of the inevitable steps for water resources planning and management studies in the recent past lulc change prediction studies were also play a vital role in desalination technologies and renewable energy sources panagopoulos 2021 panagopoulos and haralambous 2020 the prediction of the future lulc changes is key to the hydrologist environmentalist and water policymakers to make the appropriate decision policies for future development and sustainability modelling lulc changes with spatially explicit techniques helps to understand underlying processes and anticipate alternative future scenarios modelling of lulc changes is focused on developing mathematical relationships between the land cover that existed in the past and the set of driver variables like topographic demographic ecological and socio economic required to anticipate change in the future the variables that lead to the major changes in the lulc of the basin are the driver variables the physical biophysical and socio economic factors act as the most influenced drivers variables for the lulc changes araya and cabral 2010 hegazy and kaloop 2015 megahed et al 2015 yang et al 2015 some of the critical driver variables extracted from the proximity topography and demography variables are proximity to the roads streams rail network water bodies major cities elevation slope population population density and literacy rate such driver variables of the basin are extracted from the relationships between socio economic institutional policy factors and human environment systems and are multifaceted spatially and temporally in the recent past several methods have been adopted using the remote sensing data and the driver variables mentioned above to predict the future lulc these methods can be broadly classified into non hybrid approaches and hybrid methods the non hybrid approaches include cellular automata markov models ghosh et al 2017 islam et al 2018b similarity weights abdullahi et al 2018 mozumder et al 2016 logistic regression methods behera et al 2020 mozumder et al 2016 and more recently the machine learning models wherein the past land use change patterns were investigated and used for predicting the future land use the hybrid models whereas involve combining one of these methods with others for improved land use prediction the cellular automata ca model works on continuing the historical development and the surrounding impact amini parsa et al 2016 ghosh et al 2017 islam et al 2018b owing to its simplicity compatibility and ease of combining with the other models it has been widely used for the past decades see e g arsanjani et al 2012 li and yeh 2002 louca et al 2015 stevens and dragicevic 2007 yang et al 2016 however the main limitation of ca include its inability to consider the trend from the previous observations and the driver variables responsible for the changes within the basin area aburas et al 2019 kavanagh et al 2004 noszczyk 2019 markov chain mc is an analytical model which assumes that future lulc change is only a function of the historical transitions of the lulc categories due to its flexibility and adaptability in producing the complex transitions into a simple matrix of transition probabilities it was used in many studies muller and middleton 1994 jianping et al 2005 guan et al 2008 iacono et al 2015 mc model would result in transition matrices based on the statistical analysis of the land use classes from the historical maps one major drawback of this model is that it only considers the temporal trends without considering spatial considerations recently machine learning ml techniques like lr ann and svm support vector machine have been used to identify the transitions in the land use classes and have been accurate in predicting land use changes machine learning multi layer perceptron grekousis et al 2013 mishra et al 2018 and regression based models al juaidi et al 2018 hu and lo 2007 siddiqui et al 2018 are prevalent in lulc change predictions to overcome the limitations of individual models individual mc and ca the hybrid models are introduced by combining machine learning or regression based and transition based modelling approaches with the ca and mc models arsanjani et al 2012 gidey et al 2017 munshi et al 2014 munthali et al 2020 wang and maduako 2018 the hybridisation of the model can be done by combining the models like mlp mc lr mc is with the ca module and named mlp mc ca and lr mc ca respectively gaur et al 2020 kourosh niya et al 2020 sinha et al 2015 in the hybrid models the regression models are used to identify the transition potentials of the individual lulc class and the ca is used to capture the spatial aspects through transition sets neighbourhood effects and expert elicitation temporal effects are quantified by integrating with markov chain mc models the hybridisation of models allows the development of novel methods to address the complexity of real world systems apart from the primary individual models individual ca and mc hybrid models were able to capture land use and land cover with greater accuracy at the basin level barakat et al 2019 gaur et al 2020 gidey et al 2017 kourosh niya et al 2020 mishra et al 2018 sinha et al 2015 recently gaur et al 2020 utilised hybrid and non hybrid models to captured the lulc scenarios for the subarnarekha river and found that the mlp mc model was fit best according to gaur et al 2020 the relative performance of different hybrid models and their benefits over non hybridised models is however uncertain the literature review of the previous works on lulc prediction suggests that there are no conclusive results on the better performance of a single model and there have been significantly fewer works on comparing different methods therefore the main aim of the present study is to intercompare different machine learning methods for lulc change prediction further from the literature hamad et al 2018 kafy et al 2020 khawaldah et al 2020 li et al 2015 mirici et al 2018 mohamed and worku 2020 mondal et al 2016 rimal et al 2020 it is clear that the minimal work has done in the direction of the hybridisation of the lulc change models therefore in this paper an attempt was made to compare individual and hybrid models over a single platform for this purpose nagavalli river basin nrb southern india is considered wherein significant changes in the lulc has been observed in the last two decades setti et al 2020 nagavali river is a major source for irrigation and drinking water supply in the andhra pradesh and odisha states apart from choosing the model used for the lulc prediction the spatial scale of analysis also plays a significant role generally global national level analysis is preferred for overall lulc change trends assessment and global climate model simulations jardeleza et al 2019 liu et al 2016 roy et al 2015 however local basin scale analysis is chosen for water resources planning and management at the basin level loucks and van beek 2017 the basin scale lulc changes are crucial in understanding ecosystem functioning on which the critical human activities depend even though land use predictions at a national scale is available it is essential to investigate at a local scale as the parameters like topography climate hydrological components and anthropogenic factors have significant spatial variations gaur et al 2020 islam et al 2018b mohamed and worku 2020 shawul and chakma 2019 therefore estimation of the lulc changes at the basin scale is becoming a prominent component in recent research with the above discussed issues the present study is concentrated on the following tasks a temporal evolution of the land cover land use land use and land cover changes within the nrb catchment area for the last three decades b identifying the influence of the critical driver variables at the basin scale nrb basin lulc changes c comparison of the different machine learning based lulc change modelling techniques hybrid and non hybrid modelling techniques and application of the best model for predicting future land use in the present study four different modelling techniques two non hybrid and two hybrid modelling techniques were applied to identify the lulc changes within the nrb area the non hybrid modelling techniques like mlp mc and lr mc and hybrid models like mlp mc ca and lr mc ca were applied to the nrb area to predict the lulc changes 2 data and methodology 2 1 study area nagavali river basin nrb is one of the important rivers that flow in the states of orissa and andhra pradesh india the catchment of nrb covers an area of 9055 km2 and situated on 83 54 82 53 east longitudes and north latitudes of 19 44 18 17 in between the godavari and mahanadi river basins of southern india see fig 1 it originates at an elevation of 1300 m and flows along the eastern ghats of south india the catchment experiences an average annual rainfall of 1132 mm and the majority of the rainfall was received during the monsoon june to september the maximum and minimum temperatures are on the order of 34 5 c and 28 3 c respectively the catchment elevation ranges from 8 m to 1663 m above the mean sea level forest and cropland were the dominant land use in the nrb river with an occupancy of more than 70 per cent the major irrigation projects like the jhanjavathi rubber dam madduvalsa reservoir 2002 thotapalli reservoir peddagedda and the vattigedda reservoir have increased the irrigated area and thereby playing a significant role in the land use conversion based on the study by setti et al 2020 agricultural land in the basin has been increasing in the past two decades and continues to change due to infrastructure development and urban settlements along the river 2 2 data for the present study the georeferenced data were obtained from the various sources mentioned in table 1 lulc maps were obtained from the united states geological survey usgs portal using two different satellite imagery for the different periods cloud free lulc images in 1990 and 2000 were obtained from landsat 5 tm and 2010 data was obtained from the landsat 7 tm whereas for periods 2015 and 2020 the images were obtained from the landsat 8 oli tirs all lulc imageries were obtained at a 30 m 30 m spatial resolution with the utm 44 n projection and then all the data were classified supervised classification using the arcgis 10 4 1 to obtain six different lulc classes the srtm dem 30 m 30 m is used for extracting the surface topography and the slope map of the study region spatial input maps for the driver variables such as road network major cities river network and major water bodies were digitised and extracted from google earth images google earth pro 2020 and depicated in the fig 3 2 3 methodology in this section the stepwise methodology is presented and the schematic of the same is shown in fig 2 step 1 data extraction the major component of the lulc mapping is the pre processing and classification of the lulc images the former involves the composition of available bands removal of the zero value pixels mosaic and clipping the spatial extent of the data re projection whereas the latter is related to the classification of the pre processed image to obtain the lulc map for the study region for the nrb landsat 5 landsat 7 and landsat 8 satellite cloud free images were obtained from the usgs web portal https earthexplorer usgs gov then these images were projected to utm 44 north and with world geodetic system 1984 wgs 1984 as the datum the supervised maximum likelihood classification was adopted for classifying the registered satellite imageries of 1990 2000 2010 2015 and 2020 using arcgis 10 4 1 initially a training sample was created based on different band combinations of rgb finally rgb 4 3 2 a natural colour composite was used to identify most of the lulc classes apart from this a clear differentiation between the water bodies land surface and the forest area agriculture was made by using the rgb 7 5 3 and 5 4 3 band combinations the signature file was extracted from the training sample prepared in the earlier steps using different band combinations used as an input file for the supervised maximum likelihood lulc classification the nrb was identified with the six different significant classes of lulc viz built up waterbodies orchards forest agriculture and scrubland the accuracy assessment was carried out by using the confusion matrix or error matrix the accuracy assessment of the classified lulc maps was carried out to determine the degree of agreement between the classified maps with the ground truth points collected from the different sources areal photos field observations and lulc maps from the revenue department the accuracy assessment was done using the different metrics confusion matrix overall accuracy and kappa index of the agreement kappa coefficient the confusion matrix was generated between the reference ground points and classified lulc classes the producer s accuracy the number of pixels being accurately classified and the user s accuracy total number of correct pixels by the number of classified pixels of a category was estimated from the confusion matrix a detailed description of the methodology can be found in alemayehu a shawul and chakma 2019 and story and congalton 1986 the estimation of the kappa coefficient was carried out by using the statistical formula presented in the congalton 1991 shawul and chakma 2019 as given below 1 k n i 1 r x i i i 1 r x i x i n 2 i 1 r x i x i where k is the kappa coefficient r is the number of rows in the matrix xii is the number of observations in row i and column i diagonal elements xi and x i are the marginal totals of row i and column i respectively and n is the total number of observations step 2 spatiotemporal evolution of the lulc changes the trend surface analysis tsa is used to identify spatial trends of the major transitions between different lulc categories chorley and haggett 1965 tsa is an interpolation method that disaggregates the broad regional patterns from the fine scale non systematic variations in the data eastman 2006 it is designed to extract the regional component from a map such as the general location of a specific land change trend from the residual component chorley and haggett 1965 tsa performs trend analysis by considering the spatial distribution of the feature m with respect to the p number of coordinate points xi yi here feature m is taken as the dependent variable and is measured based on the coordinate points xi yi taken as independent variables the mathematical relation between the dependent feature and independent variables using third order polynomial is given as ii m x y a1 a2x a3x2 a4xy a5y2 a6 x2y a7xy2 a8y3 a9x3 here m is the spatially distributed variable which indicates the major transitions of the lulc maps between the two different periods whereas the a1 a2 a3 a9 are the coefficients of the third order polynomial and y represents the coordinates of the transition locations pixels with transitions were given a value of 1 and those with no transitions were represented with a value of 0 further the pixel values with 0 were excluded and the surface analysis was carried out to fit the third order polynomial for the remaining pixels in tsa the coefficients of the third order polynomials were estimated through the least squares method the temporal change analysis of the lulc maps of the two different periods was done using the cross tabulation matrix in terms of total changes losses gains net change by category swapping and persistence between the two time periods the total changes in any class are indicated through the cumulative gains and losses between the two periods whereas persistence indicates the unchanged portion of the lulc maps step 3 input variable selection selection of the input driver variables driving the land use changes is an essential step in modelling lulc changes three different categories of driver variables viz proximity biophysical and socioeconomic variables were considered driver variables the proximity variables considered in this study include euclidean distance to the major cities euclidean distance to the major roads national and state highways and major district roads euclidean distance to stream network and euclidean distance to major reservoirs the biophysical variables include climate soil topography lithology and vegetation the lithology and soil type changes within a specific area over time would significantly influence the variations occurring within agriculture scrubland and forest lands the topography would play a major role in developmental activities like urbanisation industrialisation and intensification of agriculture the parameters like elevation and slope derived from the dem are considered in the model due to its significant variation within the basin area as the built up area percentage within the basin is minimal compared with the remaining land use classes the population and population density parameters are eliminated in the prediction process of the lulc maps setti et al 2020 have shown that agricultural development is prominent in recent decades however the built up area percentage within the basin is minimal compared with the remaining land use classes hence the population and population density parameters were not included in the prediction process of the lulc maps the driver variables such as distance to the reservoir major cities were extracted for the recent period from lulc maps and road and rail network were extracted from the google earth images before analysing the lulc transitions between the different time periods the influence of different driver variables on the lulc maps was estimated using cramer s values v cramer s values explain the strength of association between the lulc changes and different driver variables by considering the chi square values if the cramer s values lie between 0 15 and 0 4 then variables moderately influence the land use changes were considered good if the values greater than the 0 4 strong association is observed between the change patterns and driver variables and when the values are less than 0 1 the variables are not considered for the model gaur et al 2020 the transitions between the two initial lulc maps here 2010 and 2015 are represented as boolean maps and the driver variables expressed as stretched values between 0 and 256 are considered an input in the analysis step 4 model calibration model calibration involves combining the lulc maps at time t and t 1 and the driver variables to find change locations the driver variables acquired and changes observed from the past lulc maps were used to identify the spatial variability of each lulc transition probable locations of changes are called as transition potential maps tpms and the generation of the lulc transition potential maps tpms was carried out by using the different models these maps indicate the probability of the areas that have undergone transition and vary between 0 and 1 mathematical models can be used to approximate and generate the transition potential in a precise way this study used two machine learning models multi layer perceptron mlp and logistic regression lr to generate the tpms the future prediction of the lulc is performed by combining the generated tpms with markov chain mc markov chain mc is a stochastic model in which the present state of the system at a specific time tt is dependent only on the preceding time step of the lulc map tt 1 gaur et al 2020 markov chain mc model utilises the two lulc maps at the previous time step to develop the transition probability matrix tij the transition probability matrix tij explains the probability of change from one state to another gaur et al 2020 making use of the bayes probability theorem and markov process the prediction using mc is given by 3 l t 1 i 0 m tijxlt where l t 1 and lt represent the lulc at the periods t 1 and t respectively and tij represents the changing transition probability of the ith land cover to jth land cover here m is the number of lulc classes within the basin area a detailed description of each of the modelling strategy adopted in this study is given below a multi layer perceptron markov chain model mlp mc model using the procedure explained at the beginning of this section the mlp based model can be combined with the mc to predict future land use wherein the mlp generates the transition potential maps and mc is used to predict the temporal changes mlp involves a feed forward neural network with three layers input hidden and output layers mas and flores 2008 the training of the mlp model involves adjusting the weights at the input and output layers using the backpropagation method the model is trained till satisfactory results in terms of accuracy is achieved after training the model is used for testing the model performance depends on hyperparameters such as the number of layers learning rate iterations in this study driver variables are taken as input layers and the transition probability is taken as the output layer based on aguejdad et al 2017 a minimum accuracy of 80 is required for the model results to be used for land use predictions b logistic regression markov model lr mc the lr mc is an integrated model in which the lr model is used for generating the tpms and mc captures with the temporal changes the model the logistic regression is used for estimating the association between the dependent and independent variables statistically here lulc classes were considered as dependent variables whereas driver variables are considered as independent variables the spatial maps of the dependent variables are converted into boolean maps made of two binary values 1 existence and 0 non existence the model assumes that the probability of the dependent variable is 1 and follows the sigmoidal curve given by σ x 1 1 e x lr is mathematically expressed as given below 4 ln p 1 p 1 β 0 β 1 x 1 β 2 x 2 β 3 x 3 β n x n ε here probability of the dependent variables represented with the p whereas the β 0 is an intercept variable β 1 β 2 β 3 β n are the coefficients associated with the independent variables x 1 x 2 x 3 x n and ε is considered as residual lr model capable of using only one transition at a time while utilising the lr models spatial data has to be corrected for autocorrelation and heterogeneity so that the errors in the simulation can be minimised in this study a stratified random sampling technique is used to reduce errors due to autocorrelation and heterogeneity arsanjani et al 2012 xie et al 2000 yirsaw et al 2017 the accuracy of the tpms was estimated by using the roc module in the idrisi tool the tpms with an accuracy rate of 0 65 or higher are incorporated in the model to predict lulc it is to be noted that the outcomes from the above methods lack spatial connection i e the model results are provided for the individual land cover but do not give any information about the spatial connection of the results even though the tpms consider the spatial components it relates to the change of one particular lulc class and therefore cannot capture information regarding remaining lulc classes within the basin area defries and eshleman 2004 further mlp and lr based regression models sometimes have difficulties estimating the temporal and spatial changes of the lulc exactly wu 2002 gaur et al 2020 to overcome this limitation these models can be combined with cellular automata ca models wherein the ca can add the spatial connection ghosh et al 2017 a brief description of ca and the hybrid model formation is given in the following sub section c hybrid model the combination of the mlp mc or lr mc with the cellular automata ca is defined as a hybrid model mlp mc ca or lr mc ca gaur et al 2020 sinha et al 2015 in hybrid models the mlp mc or mlp lr is used to simulate the lulc maps by considering changes related to the temporal scale time based changes and the ca model is used to capture the spatial changes in lulc the ca module was explained in details lu et al 2019 ca are incorporated by using the group of the grid cells and set of states specifying the grid cells in general the ca is defined by a group of cells and characterised by the set of states in these cells the temporal configuration of the states of these grid cells is defined through certain transition rules which can be mathematically presented as 5 c t 1 h c t j t m where c t 1 and c t are represented as cell states at the time steps t 1 and t respectively and the h is transition function rule j t m refers to the neighbourhood grid cells and here m represents the size of the neighbourhood grid cells the framework of the hybrid models was set up by integrating the markov chain processes neighbourhood interaction rules and the driver variables in a unique platform the hybrid model transition probability suggested by poelmans and van rompaey 2010 is given below 6 p h y b r i d p r e g m n e i g h b o u r here p h y b r i d a n d p r e g are the transition probability matrix generated by using the hybrid and regression models respectively mneighbour captures the effect of neighbourhood considered in the cellular automata ca in essence the hybrid models use neighbourhood grid cells information and the driver variables to predict the lulc maps in this paper two different hybrid models namely i combination of lr mc and ca denoted as lr mc ca and ii combination of mlp mc and ca i e mlp mc ca is used in this study and compared with the regression models step 5 validation stage from the observed lulc maps at three time steps t1 t2 t3 the first two was used model calibration and the recent one t3 was used for validation the validation was carried out by assessing the model s ability to capture the actual amount of land use change the persistence of each class and also the spatial distribution of the change generally there are several metrics available for model validation and among these kappa indices is the most widely used method kappa indices can precisely answer the questions like how well do a pair of maps agree in terms of the number of cells in each category and how well a pair of maps agree in terms of the location of cells in each category kappa indices consider different parameters of agreement and disagreement the kappa values like kappa number kno kappa location klocation kappa stratum kstrata and kappa standard kstandard were estimated for identifying the overall performance of the model klocation and kstrata were explained how well the grid cells are located within the lulc class and stratum respectively kappa standard is also known as the kappa index of the agreement kia which explains the overall agreement between reference and predicted lulc maps stage 6 model application future land use prediction once the validation was done for all the models the best model in terms of error is utilised to predict future lulc maps in this study we have predicted the lulc map of 2025 and 2030 using the tpms generated using 2010 and 2015 land use maps and the driver variables were extracted in 2015 in general the land use models can predict the future lulc maps reliably for about 2 3 decades defries and eshleman 2004 serneels et al 2001 yatoo et al 2020 3 results 3 1 lulc temporal mapping for nagavali river basin the classified land use land cover lulc maps were considered for 1990 2000 2010 2015 and 2020 the accuracy assessment of the lulc maps in terms of overall accuracy and kappa values was carried out using the confusion matrix and shown in table 2 overall the accuracy values were good with all values greater than 80 and the kappa coefficient was in the range of 0 81 0 88 these results indicate the reliability of the classification method adopted in the present study see table 2 the lulc over the interval 1990 2010 shows severe decrement within the scrubland 18 79 3 91 due to establishing the reservoirs and canal networks setti et al 2020 most of the scrublands were converted into agricultural lands 17 27 further there is a considerable degradation of the forest land 49 78 46 96 also the waterbodies within the nrb has increased 0 58 1 25 due to the construction of reservoirs during the period lulc maps over the time interval 2010 2020 demonstrate that forest and agricultural lands are prominent within the basin area fig 4 in nrb an enormous increase in agricultural land was identified 46 60 and the forest area decreased to a great extent 47 35 scrubland decreased from 3 9 to 1 5 and there was a slight increase in the built up area 0 11 0 36 and the orchards land cover area 0 76 0 95 3 1 1 nrb spatial dynamic of change in the present study area the spatial trends of lulc change maps were presented over a recent decade 2010 2020 even though 30 years of land use data from 1990 to 2020 for the trend analysis and the same has been presented in fig 4 however we have considered the recent 10 years of transition for the land use change prediction because the future trends would follow the recent past than the historical changes further a significant development was observed in the nrb basin in the recent decades compared to the previous decade therefore it would be more appropriate to consider the transformation in the recent decades for the model development initially the classified lulc maps over 2010 2020 were used to identify the spatial trends the spatial trends of the lulc maps for the significant transitions were generated using the tsa module tsa is used to generate spatial trends by fitting 3rd order polynomial to capture the spatial changes from agriculture to built up agriculture to the forest agriculture to waterbodies and so on and the obtained spatial changes are shown in fig 5 the nrb shows a declining trend in the forest whereas a drastic increasing trend is identified in agricultural land the forest area along the periphery of the nrb basin has been converted into agriculture the built up areas have expanded around cities like rayagada kalahandi srikakulam and parvathipuram 3 1 2 temporal dynamics of change fig 6 shows the net gains in terms of sq km for each land use for the period from 1990 to 2020 the major changes include scrubland to agriculture and forest to agriculture to capture the trend for the time window 2010 2015 the statistics in terms of percentage gains losses net changes total changes and swapping are shown in table 3 there is a considerable loss in the forest and scrubland and gain in the agricultural land during the five years further the temporal dynamics of the lulc maps were analysed using the cross tabulation module to estimate the percentages in different land cover categories from 2010 to 2015 the cross tabulation values show that the scrubland and orchards have the lowest percentage of persistence with values holding 24 40 and 38 46 of pixels respectively within the nrb area the agricultural land is the most persistent with 95 75 of the pixels and the remaining 14 25 is converted into built up generally the built up area s persistence values would be around 100 because the possibility of converting the built up area to other land use classes is minimum however in the present analysis the built up area is captured with the persistence values of 87 78 this deviation could be attributed to the submergence of villages due to the construction of reservoirs mainly thotapalli reservoir within the nrb area the forest land use possessed a persistence of 82 29 and out of the remaining 17 71 of forest land 15 was converted into agricultural land the agricultural land has a gain of 724 017 km2 from the forest land the water bodies are having moderate persistence values of 69 07 the cross tabulation results for the interval 2010 2015 are shown in table 4 and help summarise the gains losses and swapping of the different land use classes among them among all the land use classes agriculture has a maximum net gain of 816 41 km2 and the gain of agricultural land use was mainly from the forest 648 16 km2 and scrubland land 176 73 km2 the spatial maps of net gains losses and persistence for each land use are shown in fig 7 from fig 7 it was evident that the scrubland is showing the least persistence values and agriculture and urban areas showing the highest persistence values within the basin area the area along the eastern ghats of andhra pradesh and odisha states of india is converted into agricultural land cover due to improved irrigation technologies the scrubland land cover was converted into agriculture due to the water resources projects in the river basin kosam 2015 cwc report 2015 it was observed that the built up land use is doubled 10 26 km2 22 86 km2 within this interval it was extended all around the major cities like rayagada kalahandi and parvathipuram srikakulam of the odisha and andhra pradesh states respectively from fig 7 it was evident that forest land cover is following the decreasing trend within the nrb area within the interval 2010 2020 the forest land cover was decreased by 1069 94 km2 11 81 of the total area within the basin area this decrease can be attributed to the increasing irrigation potential on hilly terrains due to its soil fertility and increasing population all along the nrb area the urban area development is also one of the primary reasons for the decreasing the forest area the reservoirs on the nagavali river basin also decreased the forest area and increased agricultural land use even though irrigation structures were constructed just before 2010 the functioning of these irrigation structures was during the period 2010 2020 due to this the scrubland land cover is converted into agricultural land leading to 1261 8 km2 there were also changes observed in water bodies orchards and built up area but the corresponding areas were not significant compared to the forest and agriculture 3 2 calibration 3 2 1 selection of the driver variables the cramer s v test was performed to identify the association between the driver variables and land cover class lulc the cramer s v values were measured between the lulc and driver variables for the intervals of 2010 2015 and 2015 2020 shown in table 5 the cramer s v values for the variable association with lulc greater than 0 15 can be considered in the model to simulate the transition potential maps and vice versa gaur et al 2020 the values greater than 0 4 or higher are considered to be good from table 5 it was clear that the geographical driver variables like slope and elevation show the maximum association values like 0 44 and 0 42 with the lulc for the interval 2010 2015 further the same variables elevation and slope shows a better association 0 54 and 0 49 with lulc classes in the next interval 2015 2020 this behaviour could be explained as flat areas areas with minimum slopes or slopes 10 within the nrb areas are converted into agricultural land and this change can be observed in the lulc maps as shown in fig 4 from the cramer s v values it can be observed that the elevation and slope have significant influence on the land use changes while the other factors have comparatively less influence this could be attributed to the fact that the nrb area is a forest and agriculture land use dominant basin and slope and elevation act as a physical control for locations of agriculture further paddy is the primary crop irrigated in nrb so areas with mild or moderate slopes would be considered for agriculture in that context only the areas with moderate to mild slope have been observed to be converted to agricultural land use the same can be visualised from fig 12 wherein it can be seen that the agriculture land use change was pertaining to the areas with mild slopes for an agricultural dominant land use change the topographical factors such as elevation slope have greater influence than other proximity variables everest et al 2021 ewunetu et al 2021 said et al 2021 the proximity variables like distance to the major roads and distance to stream network have a low degree of association with lulc classes because of their less influence on the lulc changes 3 2 2 generation of the lulc transition potential maps in the present study transition potential maps tpms were generated by utilising the two different lulc models namely multi layer perceptron mlp and logistic regression lr along with the selected driver variables while training the mlp model initially 10 000 random points from input lulc were selected by the model as the initial iteration in the training procedure the model hyperparameters in the training process of the mlp are learning rate momentum factor a number of the hidden layers and these values were altered till accurate results were achieved based on different trials their values were estimated as 0 01 0 5 and 8 respectively all tpms were generated by using the selected driver variables and the tpms having an accuracy rate of more than 80 were used in the prediction of the lulc maps the tpms were also generated using the logistic regression model and the model s accuracy was assessed using the roc module the tpms with values of the roc greater than 0 65 were used in the model to predict future lulc maps gaur et al 2020 the suitable transition potential maps from both mlp and lr models were shown in fig 8 in fig 8 high and low susceptible transitions areas were shown with the colour variation from fig 8a it was observed that the major transitions in nrb over the last decades include conversion of the forest to agriculture and scrubland to agriculture having transition probability on the order of 0 64 and 0 5 respectively apart from these two dominant land use changes there are no significant changes observed within the catchment the agriculture and water bodies seem to have high persistence values within the catchment as expected there is a significant conversion of forest to agriculture particularly in the areas close to the flood plains and in the areas with mild to moderate slopes the scrubland also showed a high tendency to convert into agricultural land which matched with the original lulc2020 a lower rate of conversion from the orchards to agriculture was shown within the mlp based transitions the agriculture and waterbodies have high persistence values within the nrb area from fig 8b the major transitions like forest to agriculture land use had a low magnitude of 0 38 in the lr model the spatial extent of the conversion from forest to the agricultural land use was identified to be low within the study area even though scrubland to agriculture land use transition was having the highest magnitude of 0 96 but it was pertaining to the lower portion of the area from the temporal evolution shown in fig 4 it was identified that the agricultural land use is increasing due to a decrease in the forest land use during the time from 2010 to 2015 these transitions are captured in the mlp model transitions rather than in the lr model transitions the transition from forest land use to agriculture was having the high magnitude of 0 64 compared with the lr transitions of 0 38 the lr based transition maps show a higher probability of water bodies with magnitude of 0 99 wherein the mlp model shows only 0 55 overall comparing the actual changes and the transition maps the mlp model can capture all the temporal observations to a greater extent 3 3 simulation the transition probability matrix for the year 2020 was obtained using the markov chain mc model the mc model utilises lulc maps of 2010 and 2015 to develop the transition probability matrix of the lulc 2020 the selected tpms from the mlp and lr models were combined with the mc model transition probability matrix for simulating the future lulc maps here initially for comparison purposes two models mlp mc and lr mc models were used to predict the lulc map of 2020 further the cellular automata ca model was combined with mlp mc and lr mc models for implementing the hybrid models in the lulc prediction in the present study two hybrid models mlp mc ca and lr mc ca were also utilised to simulate the lulc map for 2020 the predicted lulc 2020 map by using the four different methods are shown in fig 9 the predicted lulc 2020 map by using these four models was validated with different validation techniques comparing the results from the four different methods and the actual land use map for 2020 fig 4e it can be observed that the mlp mc ca model results are closer to the observed maps the spatial extent of forest conversion and scrubland conversion has been captured to greater accuracy by the mlp mc ca model fig 9e from fig 8 mlp model transitions were observed to capture all temporal evolutions more accurately than the lr model transitions fig 9 show the predicted land use from different model combinations and the observed land use in 2020 the model outcome from mlp mc ca closely matches the actual land use in 2020 in terms of the spatial extent of the different land use patterns compared to the other model s outcome fig 9e summarises the results of the percentage error in model prediction for different land uses compared with the actual lulc 2020 map the percentage error for agricultural land and forest cover was lowest 1 for the mlp mc ca model however there is an underestimation of forest cover by mlp mc and lr mc models there is also an overestimation of the agricultural land use in all the models except for the mlp mc ca model there is a significant overestimation for scrubland on the order of 40 60 by the mlp mc lr mc and lr mc ca models whereas the error is only around 5 for the mlp mc ca model a similar observation was seen in the orchards landuse wherein the mlp mc ca models performed better than the other models compared to ca and without ca the hybrid models with ca have performed better than the models without ca the percentage error values of the mlp mc ca models for the forest agriculture and scrubland land uses were close to zero 3 4 validation of lulc maps the validation of the predicted lulc2020 map obtained from mlp mc lr mc mlp mc ca and lr mc ca models was done using the actual lulc 2020 the accuracy of the model results was estimated by using the kappa values like kappa number kno kappa location klocation kappa stratum kstrata and kappa standard kstandard table 6 provides the validation results for all models and it is clear that mlp mc ca is performing better in terms of all performance statistics than the mlp mc lr mc and lr mc ca apart from the kappa values the persistence was also tested at the grid cell level to identify the model suitability the chi square goodness of fit test results with alpha 0 05 between the actual land use classes and predicted land use classes of the lulc 2020 is shown in table 7 from table 7 it is clear that the mlp mc ca model outperforms other models further it can be observed that the mlp based models have outperformed the lr based model for the study region and among lr based models there is no significant difference in the performance metrics each land cover category of the predicted and actual lulc 2020 maps was compared to assess its persistence and obtained persistence values of each class presented in fig 10 from fig 10 the mlp mc ca model was having the highest average persistence values average persistence values of all land cover classes 91 for all land cover categories among the four methods and the mlp mc model had the second highest persistence values with the actual lulc 2020 map the selected best model mlp mc ca was utilised to predict lulc changes for future periods 3 5 prediction of future lulc of the nagavali river basin mlp mc ca model is selected as the best model for predicting the lulc changes the selected mlp mc ca model was used to predict lulc changes for the nagavali river basin for 2025 and 2030 from fig 11 it was clear that the forest area is continuously decreasing while agricultural land is increasing within the nrb area it could also be observed that the built up area agricultural land and the waterbodies follow the increasing trend whereas the remaining land use like forest area scrubland and orchards shows the decreasing trend within the nagavali river basin it is evident that from fig 11 forest area was decreased by 9 02 34 41 25 29 over one decade from 2020 to 2030 simultaneously the agricultural land is increased by 8 74 61 59 70 53 within the same period built up area waterbodies orchards and scrublands followed the historical trends but the changes with the land use classes over 2020 2030 were not significant due to the less extent of the area from fig 11 it was evident that forest area would decrease by 5 04 34 42 29 37 and 3 98 34 41 25 29 in the following two consecutive five year spans from 2020 to 2025 and 2025 2030 respectively simultaneously the agricultural land use would increase by 5 09 61 59 66 68 and 3 65 66 68 70 33 within the same period built up area water bodies orchards and scrublands followed the trends but the changes with the land use classes over 2020 2030 were not significant due to the less extent of the area 4 discussions the present study focused on investigating different machine learning based lulc prediction models for this purpose nagavalli river basin was considered for analysis as there were considerable changes over the last three decades the analysis was done in three different steps like temporal evolution of the land use identifying the suitable driver variable and model development for predicting the lulc changes and predicting future lulc changes using the best model 4 1 temporal evolution of land use the results from temporal evolution analysis showed that there are significant changes in the catchment notably the agricultural land use in the nrb area is increased by 35 28 while scrubland and forest area is decreased by 17 42 and 15 22 respectively it was also observed that most of the scrubland and forest areas were converted into agricultural land use these changes can be attributed to the implementation of the irrigation projects at different time period over the three decades for example madduvalasa reservoir stage i was completed in 2002 with an irrigation command area of 23 000 acres https en wikipedia org wiki madduvalasa similarly the thotapalli reservoir project was completed in the year 2015 with an irrigation command area of 1 20 000 acres by using the canal length of 110 km in vizianagaram and srikakulam districts of the ap covering the both vizianagaram and srikakulam districts of andhra pradesh https en wikipedia org wiki thotapalli barrage the hindu 2015 new indian express 2015 further several incentive schemes by the government of andhra pradesh and odisha encouraged the people to take up agriculture department of agriculture and farm empowerment dafe odisha 2015 the hindu 2015 new indian express 2015 dofe odisha 2015 reported that the odisha state people were encouraged to do the farming by providing crop insurance policies these developmental activities at the watershed and the management policies have significantly led to the scrubland s conversion into agricultural land use 4 2 influence of driver variables further four different lulc change prediction modelling techniques namely mlp mc lr mc mlp mc ca and lr mc ca were compared for their ability in capturing the lulc changes before the model development input driver variables were tested for strength associated with the lulc maps to identify the lulc changes the cramer s v test shows that the topographic parameters like elevation and slope have good strength v 0 4 associated with the lulc changes in nrb most of the forest area was converted into agricultural land during 2010 2020 see fig 4 adhikari et al 2017 stated that the same trend was observed along the eastern ghats of south india the present study showed that the areas pertaining to the mild slopes even with high elevations show greater transitions than the steep slopes from fig 12 it can be noted that the areas having mild slopes slopes varying between 0 and 10 even at higher elevations were converted into agricultural land use many studies like birhane et al 2019 lee et al 2018 and munroe et al 2004 showed that there is no significant land use change at higher elevations however the present study shows that expansion of agricultural land use was observed even at higher elevations further the analysis also showed that the proximity driver variables like distance to major roads distance to the major cities distance to the water bodies show lower association with the lulc maps 4 3 performance of the hybrid models after selecting the suitable driver variables machine learning models were developed to identify the suitable changes between the past lulc maps between 2010 and 2015 lulc change prediction accuracy dominantly relies on the machine learning techniques ability to produce the transition potential maps and transition matrix yuzar et al 2006 among the different models compared in the study the integrated hybrid model mlp mc ca model outperformed all others it was observed that the mlp model scored better than the lr models as the mlp models can capture the complex relationship between the predictor and predictands than the logistic regression models these observations are congruent with previous studies guidigan et al 2019 kafy et al 2020 mirici et al 2018 rimal et al 2020 across the globe which have also shown that the mlp model performs better than the lr model another study by shawul and chakma 2019 upper awash basin area have shown that the ml based model performed better in capturing the land use changes when compared to the traditional model similarly gaur et al 2020 have shown that for a river basin in india mlp mc models yield accurate prediction compared to models based on logistic regression and similarity weights instance method from the present study it is observed that combining the mlp mc and lr mc models with ca significantly improved the model results in terms of kappa location tables 6 and 7 and fig 10 show that the mlp mc ca models perform well compared to the other three modelling techniques to predict the lulc changes the improvement in the model performance can be attributed to the fact that the ca model can simulate the hierarchical and complex structures at a large scale batty and xie 1994 lauf et al 2012 stevens et al 2007 tang and di 2019 wu and martin 2002 further the ca based models consider the spatiotemporal variations in the catchment and geographical connections arsanjani et al 2012 lin et al 2020 munthali et al 2020 according to tang and di 2019 the ca model has the innate ability to adjust the rules based on the neighbourhood the results from the study showed that the integration of the ca module to the machine learning techniques like mlp mc had improved the prediction of the lulc change simulation to a greater extent from tables 6 and 7 and fig 10 the kappa coefficient kappa standard was improved by 10 8 0 816 0 909 and the chi square metric was reduced by 77 28 64 6 73 the average persistence values of the lulc changes were increased by 10 33 81 33 91 respectively overall it was observed that the integrated hybrid model based on ca provided the best results when compared to the non hybrid models without ca it is also important to note that the present work shows a way forward for applying machine learning based hybrid models in land use prediction studies in the present study the lulc changes were predicted up to the 2030 but the same approach can be applied to predict long term land use but the reliability of the predictions mainly depends on the initial training data sets and driver variables further such long term prediction requires more training period and also the associated uncertainty with such prediction is high the performance of the lulc prediction models is affected by two kinds of uncertainties aleatory and epistemic these uncertainties can be further categorised into input data uncertainty parameter uncertainty estimation of the model parameters and structural uncertainty model ability to capture the catchment s response these uncertainties certainly will be very high for long term prediction as the land use changes are highly dynamic and dependent on many unaccountable factors such as local government policies future climate etc and it is important to quantify the uncertainty involved in such prediction since the present study focuses on comparing different models the estimation of the associated uncertainty can be considered the future scope the results from this study are essential for policymakers and water resources development since there is a considerable increase in the agricultural landscape and reduction in the forest there would be changes in the hydrological regimes and water balance as shown in setti et al 2018 therefore the results from the present work might help the government develop policies and water resources projects the present study results can be directly used to interlink the nagavali river basin with the vamsadhara river basin further this research work can generate the different scenarios of the lulc change implication on the nrb area hydrological components 5 conclusions the present compared the different machine learning approaches used for future land use land cover predictions the study compared two non hybrid models mlp mc and lr mc and hybrid mlp mc ca and lr mc ca models for this purpose nagavalli river basin a river basin in southern india which has undergone significant land use changes is considered a testbed preliminary analysis of temporal evolution of lulc changes over 1990 and 2020 showed the drastic increase in the agricultural land use class due to establishing the irrigation projects within the nrb area over the past two decades the study region experienced dominant changes on the order of 17 42 and 15 22 decrease in scrubland and forest respectively at the same time the agricultural land cover is increased by 35 28 the agriculture land use class is increased at an expanse of the scrubland and the forest land use classes the physical control of the lulc changes in nrb was identified by using cramer s v method and it was found that the among different driver variables considered slope and elevation were the major influencing parameters only the areas with moderate to mild slopes have been observed to be converted to agricultural land use the performance of four different models was evaluated based on the different metrics like kappa values chi square and persistence of the different lulc classes among the four models mlp mc ca outperform the remaining three ml models like mlp mc lr mc and lr mc ca the spatial accuracy of the mlp mc ca model is having the best value kappa as 0 909 and which is 10 12 and 15 more accurate than the mlp mc lr mc and lr mc ca models respectively the chi squared statistical analysis of the different models with the original lulc 2020 shows that the mlp mc ca models perform better than the remaining models the persistence of the mlp mc ca model is 91 compared with the original lulc 2020 showing that the hybrid mlp mc ca modelling technique can identify the lulc changes to a greater extent it was identified that the hybrid models could replicate ground reality due to the inclusion of the spatiotemporal evolution and geographic connections the results from this study are essential for policymakers and water resources development since there is a considerable increase in the agricultural landscape and reduction in the forest there would be changes in the hydrological regimes and water balance therefore the results from the present work might help the government develop policies and water resources projects the present study results can be directly used to interlink the nagavali river basin with the vamsadhara river basin further this research work can generate the different scenarios of the lulc change implication on the nrb area hydrological components declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements dr rm gratefully acknowledges the funding received from the department of science and technology government of india under the grant dst wti dd tm 2k17 79 
25739,when decision making in the case of imminent water related humanitarian disasters or violent conflict depends on a quick assessment of water related risks there may not be enough time to collect high quality local data online available global data may offer an alternative data source we present a method to construct a water resources model based on global datasets we apply it to the upper niger basin in west africa and test its credibility with hydrological performance metrics and fit for purpose indicators the fit for purpose indicators are tailored to questions the model should support answering and allow assessment of whether a better performing model would result in a different basis for decision making we find that the model scores satisfactory on both types of indicators thus models based on global open datasets may be suitable for rapid preliminary assessments where time is of the essence or high quality local data is unavailable graphical abstract image 1 keywords global online datasets water resources water related conflict and humanitarian risks rapid model development inner niger delta upper niger 1 introduction shortage of water can have various economic and social consequences low river discharges can lead to poor water quality and reduced fish catch unsatisfactory rain and irrigation water can result in failed harvests or reduced availability of grasslands for livestock reduced water supply for households can have implications for human health these impacts of water shortage can subsequently lead to loss of lives of livelihoods as well as of socioeconomic opportunities and may result in humanitarian crises perceptions of underlying causes of water shortage and of an unequal distribution of impacts can further contribute to dissatisfaction and could trigger social unrest and violent conflict between different water users different communities or larger regions united nations and world bank 2018 to respond effectively to emerging or on going disasters or conflicts and avoid humanitarian diplomatic or water management measures that have unintended consequences it is important to have information regarding the current water resources situation and the effectiveness of possible risk reducing actions also the development and sharing of incorrect perceptions about water resources availability and distribution that could lead to competition and conflict can be prevented by timely producing and sharing data and information that is easily accessible to all especially in protracted conflict situations using openly available data can contribute to neutrality and trust of the data and of the resulting analysis results water resources models are commonly used to generate information on how water availability and shortage vary under the impacts of climatological variation increased water use or increased river regulation these models are typically based on information on land use and elevation climate and water use and are used to understand the impacts of possible future developments or of policy and management actions on water infrastructure water users and ecosystems at the same time they inform decision makers in their decisions concerning the use the development or the protection of water resources several requirements can be identified for the model and resulting information models and resulting information need to be 1 salient 2 credible and 3 legitimate van voorn et al 2016 salience refers to the relevance of the model for the policy questions that need to be answered credibility refers to the scientific quality of the model and legitimacy refers to the extent to which stakeholder views and concerns have been considered in the process of model development various authors discuss model choices in relation to the selection of the model concept e g addor and melsen 2019 babel et al 2019 what has received less attention is the approach to developing these models and the opportunities of making use of the increasing availability of global scale data model based information is currently mainly available either through detailed catchment studies or through coarse global level models both have limitations which we discuss below therefore in this paper we argue that an intermediate level is required at which global data are used for catchment level analysis van voorn et al babel et al 2019 suggest that often a trade off needs to be made between the three model quality criteria and found that stakeholders value salience over credibility such trade offs are usually necessary due to time and budget constraints using global data for catchment level analysis can provide a suitable method to develop a model relatively quickly a question that this paper seeks to help answer is to what extent credibility and salience are compromised if this approach is taken an additional benefit of using global data is its replicability the method including the data processing scripts can potentially be re used further reducing time and budget requirements in fig 1 we present the three levels 1 catchment models based on local data 2 catchment models based on global data and 3 global hydrological models based on global data in relation to salience credibility and resource constraints because our focus is on testing the credibility and salience of a model based on global data based on a desk study we leave legitimacy out of consideration which would have required consultation with stakeholders in relation to an actual policy question however we believe that using global data sets that are accessible to all is beneficial for model legitimacy here we first discuss the strengths and weaknesses of the catchment analysis using local data and of the global water resources models to then make our case for the intermediate level of catchment level analysis using global data catchment scale water allocation models using locally collected data have been the traditional approach for water resources planning studies for many decades see e g loucks and van beek 2005 because of the local level of the study models and data processing could be tailored to the specific characteristics of the catchment and the information needs of stakeholders thus potentially resulting in high salience and legitimacy moreover to achieve this these models usually have a high level of detail and need high quality locally measured data which improve the credibility of the model these studies are generally commissioned by government actors and are useful for strategic long term planning which requires detailed analysis of what type of policy actions are suitable under a range of possible future situations the results will answer the question of what measures will be most effective to achieve objectives for the water resources system and will help actors to decide what measures to implement however developing such models may be time consuming and resource intensive and the locally measured data for this type of high resolution model is not always available or of unsatisfactory quality where high quality local data is lacking global hydrological models can offer an alternative data sets amongst others on climate weather soil geology and topography that cover large regions or the entire world have increasingly become available over the past couple of decades the availability of such near global data sets has enabled the development of global hydrological models ghms global hydrological models have long development and run times but increasingly model results and underlying data are directly available free of charge online for example through the earth2observe water cycle integrator wci plymouth marine laboratory 2020 in such cases required resources to use the information are very low and accessibility is high the level of detail of ghms is low compared to catchment models ghms are gridded models in which all water balance computations take place at the grid cell level with a typical spatial resolution of 30 arc minutes which is around 50 50 km at the equator see bierkens et al 2015 for an overview of various ghms over time ghms have developed into increasingly finer resolutions down to 5 5 km increasing their applicability at the local level e g pcr globwb 2 0 wetlands international 2019 watergap 3 verzano et al 2012 and the w3 version 2 model van dijk et al 2018 many of these ghms also take water demand and river regulation by reservoirs into account despite the increasing resolution and possibilities to include water demand and regulation ghms follow a single approach for all catchments and are not designed to be tailored to the specific needs and interests of stakeholders in a specific catchment also most ghms do not offer functionalities to easily test different reservoir operation rules or water demand changes this means that salience of these models is likely to be unsatisfactory for specific catchment level questions moreover the ghms are generally not calibrated for specific catchments as a result the representation of the local hydrology is unknown and credibility may be inadequate ghms are very suitable to quickly identify where and when water shortage related risks could occur and to get a general indication of the causes of water shortage and possible solution options they are less suitable to analyze catchment specific variations in water use water regulation or options to explore different water management strategies and thus might not meet decision makers needs because of the limitations of ghms regarding salience and credibility and of models based on local data regarding resource requirements we present here an intermediate level in which catchment models are developed using online global data in this paper we focus on this intermediate level which has been much less applied and researched than the other two levels existing basin scale applications of global data tend to focus only on the biophysical components e g gusev et al 2006 conway and mahé 2009 andersson et al 2017 liersch et al 2019 developed a model of the niger basin that includes water abstractions for irrigation and reservoir operation in that study only the rainfall runoff component was done using global data additional local data and information were used for the water allocation and demand component pechlivanidis and arheimer 2015 developed a hydrological model for india only based on global datasets including reservoirs agricultural and irrigation datasets and calibrated the irrigation parameters that regulate water demand and abstraction siderius et al 2018 applied a ghm lpjml including irrigation abstractions to the rufiji river basin in eastern africa and changes to the input rainfall model schematization and parameters were required to improve model performance from generally poor to reasonable or good the use of solely open data to develop a local basin level water resources model has not yet been extensively tested in this paper we extend the use of open data for catchment level analysis with water use and water regulation for this purpose we develop and evaluate a combined model in which a gridded rainfall runoff model provides input to a network based water allocation model both based solely on global open data sets that are available online we evaluate the credibility of the combined model in two ways by considering both standard hydrological performance metrics and dedicated fit for purpose indicators we apply this approach to the upper niger basin in mali the inner niger delta that depends on inflowing water from the upper niger basin is prone to violent conflict that may have a link with natural resource availability decisions need to be taken on a new dam and expansion of irrigated areas in the upper niger basin that are likely to alter the natural flooding regime of the wetland area and will subsequently impact the ecosystem services that form the basis for the livelihoods of its inhabitants information on how upstream interventions will impact the livelihoods of the people in the inner niger delta and the conflicts over natural resources is important to into account in decision on theses interventions the water resources in the catchment have been studied using local data a recent example is the study by liersch et al 2019 we therefore use that study as comparison material for the global data based model presented in this study 2 methods 2 1 study area the study area consists of the entire upper niger catchment including the inner niger delta ind until the border between the tombouctou and gao regions in mali fig 2 a the catchment is located in guinea ivory coast burkina faso and mali and is around 380 000 km2 in size the headwaters of the main niger river branches are located in the daro massif in guinea while the main tributary the bani originates in southern mali and ivory coast the bani enters the niger river at the upper boundary of the inner niger delta a vast heavily vegetated floodplain with highly productive farmlands and fishing areas just upstream of the ind lies office du niger a large irrigation scheme of around 100 000 ha that produces most of mali s rice and sugar hertzog et al 2012 the movement of the intertropical convergence zone itcz has led to a sharp north south gradient in annual rainfall from around 2000 mm in the southern part of guinee to around 300 mm in the sahel zone in the north andersen et al 2005 the entire catchment has a distinct dry season from december to may and a monsoon season that is longer and heavier in the south high average temperatures and low humidity lead to evaporation rates of around 40 of the annual inflows andersen et al 2005 supported by natural variations in high and low flows these wetlands provide important ecosystem services to around 1 5 2 million people who inhabit the ind these people base their livelihoods on farming cattle herding and fisheries zwarts et al 2005 cattle herding is nomadic and cattle spend part of the year in the ind to feed on the water grass bourgou that grows in and along the wetlands pressure on wetland resources has reduced livelihood opportunities and increased competition between the different socio ethnic groups present in the area these pressures are the combined result of population growth and reduced resource availability enhanced by various social economic and political dynamics wetlands international 2019 changes in water abstraction and regulation in the upstream catchment as currently planned through a new dam at fomi or moussako in guinea and expansion of the irrigated perimeter managed by office du niger in mali could lead to a further alteration of the inundation regime of the ind that could result in loss of ecosystem services and related livelihoods and trigger increased competition and conflict 2 2 model setup the model should allow for the analysis of water resources their use and related risks how these are impacted by climate change and socio economic developments and how interventions e g alternative dam locations dimensions operation irrigation expansion crops and irrigation types could reduce or exacerbate water related risks this leads to the following requirements the model needs to be able to simulate seasonal forecasts or time series of 20 50 years or more to account for possible climate variability within acceptable time frames when assessing intervention options the model needs to include explicit water demand and water regulation options that can be altered to simulate interventions the model should provide the following output o scenario specific discharges at key locations in the system oparameters related to the use of water in the basin for example for irrigation and hydropower the core of the model framework consists of the gridded wflow rainfall runoff model schellekens et al 2019 and the ribasim water resources management network model van der krogt 2008 fig 2 b wflow simulates discharge hydrographs at the level of subcatchments provides water discharges from the subcatchments at the chosen inflow locations of the ribasim model next these are grouped at the level of ribasim units before being used by ribasim ribasim then simulates the effects of reservoir operation and of abstractions for irrigation and household water use on the river discharges and computes hydropower production and supply demand ratios for irrigated areas discharges at the downstream end of the niger and the bani upstream of the inner niger delta are combined with historical inundation areas extracted from earth observation images into discharge inundation area curves for the downstream section of the study area the inner niger delta these curves are then used to estimate the inundation areas the method is developed to apply it in data scarce catchments or in areas where a model is needed quickly no existing model is available and extensive model calibration and validation is not feasible or necessary given the application of the model for this reason the models are not fully calibrated only wflow parameters for which no global data exist were estimated through a couple of test runs available monitoring data were mainly used for model performance testing 2 2 1 wflow rainfall runoff model the hydrological model used is the spatially distributed hydrological model wflow sbm part of the openstreams wflow distributed hydrological modelling platform schellekens et al 2019 the wflow sbm model is programmed in python and based on the pcraster python framework karssenberg et al 2010 a raster based gis package suitable for dynamic computations fig 2 c shows the different processes and fluxes that are part of the wflow sbm hydrological concept the soil part of wflow sbm model follows the same concepts as the topog sbm model vertessy and elsenbeer 1999 topog sbm is specifically designed to simulate fast runoff processes during discrete storm events in small catchments 10 km2 evapotranspiration losses are ignored the derived wflow sbm approach can be applied to a wider variety of catchments because evapotranspiration losses and capillary rise were added to wflow sbm for the channel overland and lateral subsurface flow a kinematic wave approach is used similar to topkapi benning 1995 todini and ciarapica 2002 g2g bell et al 2007 1k dhm tanaka and tachikawa 2015 and topog sbm vertessy and elsenbeer 1999 rainfall interception is based on the analytical approach by gash 1979 the soil in every model grid cell is considered as a single bucket divided into a saturated and unsaturated store with the option to divide the soil column into different layers soil infiltration depends on the soil infiltration capacity and the fraction of paved and unpaved area transfer of water from the unsaturated store to the saturated store is controlled by the saturated hydraulic conductivity at the water table the relative saturation within the unsaturated zone above the water table and a power coefficient depending on the soil texture brooks and corey 1964 transpiration is first derived from the saturated store if roots intersect with the saturated store and then from the unsaturated store capillary rise from the saturated store also results in a flux from the saturated store to the unsaturated store the kinematic wave equation is used to route overland flow river flow and lateral subsurface flow over a d8 network the wflow sbm model for the upper niger has been setup at a grid resolution of 30 arc sec and simulations were performed at a daily time step the development of a wflow model using online data sources is typically done within a couple of days running simulations and fine tuning typically require one or several weeks 2 2 2 wflow parametrization and forcing table 1 lists the global datasets that were used for setting up the wflow sbm model for the upper niger basin and to generate the forcing two different precipitation datasets were used the climate hazards group infrared precipitation with station data chirps v 2 0 dataset at 0 05 resolution and the multi source weighted ensemble precipitation mswep version 1 0 at 0 25 resolution potential evapotranspiration e t 0 was based on the hargreaves equation calculated from hourly era5 variables temperature at 2 m and extraterrestrial radiation at 0 25 resolution hargreaves and samani 1982 potential evapotranspiration e t 0 was calculated as follows e t 0 0 0023 r a t m a x t m i n 0 5 t 17 8 where r a mm is the extraterrestrial solar radiation evaporation equivalent per day t m a x c and t m i n c are the daily maximum and minimum temperature respectively and t c is the daily mean temperature the forcing was interpolated to the wflow sbm model grid with nearest neighbor interpolation river cells for the wflow sbm model were defined as grid cells with a stream order strahler 1957 of three and larger smaller stream orders 1 and 2 are associated with widths up to 10 m downing et al 2012 which is small in comparison to the model s grid size the channel width was estimated by scaling yearly average discharge at the outlet for each point in the drainage basin based on the upstream area and using the approach of finnegan et al 2005 that requires the river slope β r i v e r manning s roughness coeﬃcient n r i v e r and a bank full width to depth ratio α that was estimated at 120 to enforce the drainage pattern of the hydrosheds river network the dem was adjusted for this river network before generating the local drain direction map the river length l r i v e r was estimated by generating a river map from a higher resolution dem 0 005 based on srtm v4 and to compute the total high resolution river cell length within a model resolution cell most wflow sbm parameters were estimated with pedo transfer functions ptfs from literature based on imhoff et al 2020 imhoff et al 2020 found that for most sensitive wflow sbm parameters a ptf is available only for the sensitive wflow sbm parameter k h o r f r a c a ptf is not available and a default value is applied also for the non sensitive wflow sbm parameters without a ptf a default value is used in line with the objective of the study we performed a limited calibration of the wflow sbm model for the upper niger basin by manually tuning the most sensitive parameters k h o r f r a c m r t d and θ s to improve the model performance for the tuning of the rooting depth parameter r t d we relied on kleidon and heimann 1998 the grdc stations in fig 2 d were used for the model parameter tuning these were selected because of data availability table 2 and because these stations are not influenced by reservoirs reservoirs were not included in the wflow sbm model the model parameter tuning was done for the years 1981 1985 2000 2007 and 2008 for the precipitation datasets mswep and chirps in combination with hargreaves potential evapotranspiration these years were chosen based on a combination of data availability and to include years throughout the complete simulation period this is important since both mswep and chirps also rely on satellite based precipitation products available since 1998 the wflow sbm parameters were adjusted as follows the m parameter was increased with a factor of 3 k h o r f r a c was set to 500 r t d was increased based on schenk and jackson 2002 and kleidon and heimann 1998 and θ s was increased with a factor of 1 025 after the model tuning wflow sbm was simulated with mswep for the period 1979 2012 and with chirps for the period 1981 2012 to assess the model performance for these periods results from these simulations provided discharge time series at the inflow points of the ribasim model fig 2d for longer time frames mswep 1979 2014 and chirps 1981 2018 2 2 3 ribasim water allocation model ribasim river basin simulation is a generic water resources modelling package van der krogt 2008a van der krogt 2008b that has been applied in various countries to analyze impacts of changes in water allocation and water regulation on different water users including the environment a ribasim application consists of a network of nodes representing either sources of water demands for water or regulation of water connected by links the configuration of the network and the associated division of the catchment into subcatchments is usually constructed based on the location of large river regulation infrastructure and corresponding large water demands to construct the network for the upper niger model five maps were overlayed subcatchment delineation country borders within the upper niger catchment mali guine cote d ivoire burkina faso irrigation density population density and location of large reservoirs and river regulation structures taking into account the natural runoff processes large abstractions and regulations and administrative information needs we then constructed a network of water source nodes i e wflow inflow points demand nodes regulation nodes and links fig 2e the resulting network was filled with information from open data sources on river regulation and demand table 3 this network also provided the spatial resolution combination of subcatchment and admin units at which grid based input data was aggregated to derive information that was included in the ribasim model nodes in addition the same precipitation and evaporation data as used for wflow were used to compute irrigation demands and reservoir water balances with the available online data and scripts to process them to be used in the ribasim model the development of the ribasim model would take typically a couple of days depending on the size of the catchment and required level of detail the calculation time step for the ribasim model was set at one month this is a commonly used timestep for analysis of long time series since the model does not compute physical processes but aggregated water balances this is generally considered a suitable timestep that allows for analysis of seasonal variations yet has a limited simulation time after the model construction the ribasim model was run for the same periods as wflow with mswep for the period 1979 2012 and with chirps for the period 1981 2012 using the resampled monthly wflow discharge timeseries at the inflow points as input because initial storage in reservoirs has a large influence on river discharge in the first years of the model simulation we added ten years of long term average inflow and climate data before the actual wflow output the ribasim results for these synthetic ten years only served to set initial conditions and were left out of consideration in model testing 2 2 4 the relation between river discharge and inundation area ecosystem services in the ind are supported by natural variations in high and low flows especially by the maximum inundation extent during the annual high flow period this maximum inundation extent is therefore an important model output variable in this research we estimate this parameter by correlating it with the combined discharge of the rivers niger and bani during the annual high flow period which is a model output of the ribasim model we use the following steps 1 derive inundation area from all available medium resolution satellite images 2 correlate these to the combined simulated niger bani discharges during the annual high flow period season derived with the ribasim model here we describe these steps in more detail 1 we derive inundation area for all catchments used in the study by estimating it from landsat 4 5 7 and 8 satellite images in total 6810 images were used the inundation area was computed employing the modified normalized difference spectral water index mndwi xu et al 2006 followed by the use of dynamic thresholding to estimate the surface water mask and to compute the inundation area donchyts et al 2016 all computations were performed with the google earth engine parallel processing platform gorelick et al 2017 2 here we take the combined niger bani discharge in m3 s averaged over the annual high flow period in the ind august to november and carry out a linear regression analysis with the method of least squares to correlate it to the maximum inundation extent within the inner niger delta for the period 2000 2014 this period was chosen because the required satellite images are not available before 2000 and input data sets for the combined wflow ribasim simulation were available only until the end of 2014 2 3 evaluating the credibility of the global data based model 2 3 1 hydrological performance we test the combined wflow ribasim inundation extent model by considering whether the model is 1 accurate and 2 fit for purpose to assess the accuracy of the wflow and ribasim models we compare monthly time series of simulated discharges at appropriate locations in the system with observed grdc discharges by assessing the model performance with the kling gupta efficiency kge gupta et al 2009 commonly used as a model calibration objective function we also report nash sutcliﬀe eﬃciency nse since this efficiency metric has been used traditionally for calibration and evaluation of hydrological models however nse has two important shortcomings that are addressed by kge the bias component is normalized by the standard deviation and as a result for basins with a high runoff variability the bias component tends to have a smaller contribution and nse will favor hydrological model parameters that underestimate variability in flows gupta et al 2009 kge and nse are not directly comparable while nse kge 1 means a perfect match between simulated and observed flows nse 0 and kge 0 41 indicates the model simulation is as accurate as the observed mean knoben et al 2019 here we apply the following ranges based on kge kge 0 4 unsatisfactory 0 4 kge 0 7 satisfactory kge 0 7 good the accuracy of the wflow sbm model is assessed at five grdc stations upstream of the major reservoirs and irrigation areas fig 2d the accuracy of the ribasim model at two locations downstream in the main niger branch just downstream of markala and on the downstream section of the bani tributary fig 2e of all locations the accuracy at the two downstream locations is considered most important as the combined discharge of the rivers niger and bani represents the bulk of the water that enters the inner niger delta the goodness of fit between the combined niger bani river discharges during the annual high flow period and the maximum yearly inundation extent is assessed by evaluating the coefficient of determination r2 of the fit 2 3 2 fit for purpose the purpose of the model for the upper niger catchment including the inner niger delta is to understand the trade offs between upstream interventions in the upper niger catchment specifically the construction and operation of a new dam and the expansion of the irrigated area and the wetland ecosystem of the inner niger delta and its further social and security consequences although the implementation of the upstream interventions will take many years for international actors involved in financing these interventions a rapidly constructed model using open data can give them insights in the various intended and unintended impacts of the proposed developments based on which it could be decided whether more detailed research would be required to assess whether the model is fit for purpose we apply the method of guillaume and jakeman 2012 of asking closed questions for our model these closed questions refer to the trade offs between the introduction of a new dam expansion of the area irrigated by office du niger and the ecosystem services of the inner niger delta this trade off is a trade off between hydropower production irrigation production and ecosystem services the condition of water related ecosystems and the availability of related ecosystems are understood to largely depend on the flow regime i e the variations in discharge that occur over time bunn and arthington 2002 arthington et al 2018 both high flows and low flows are important and not just their magnitude but also their timing frequency duration and rate of change richter et al 1996 richter et al 1997 for the inner niger delta these dynamics refer to combined inflows from niger and bani rivers and to how these translate in inundations patters of the wetland area because the purpose of the model is to understand trade offs between different users of the niger river as a result of proposed interventions fit for purpose for this case is less about the exact flow regime and more about the relative change in this flow regime as a result of specific policy interventions about which a decision needs to be made we therefore translated this purpose into the following questions to test its fitness 1 does the model represent well the change in inner niger delta inflow regime and inundation extent and irrigation water supply in a situation in the current situation 2 does the model represent well the change in inner niger delta inflow regime and inundation extent and irrigation water supply in a situation in which the irrigated area of office du niger is expanded 3 does the model represent well the change in inner niger delta inflow regime and inundation extent irrigation water supply and hydropower production in a situation in which a new dam is built at fomi in guinea 4 does the model represent well the change in inner niger delta inflow regime and inundation extent irrigation water supply and hydropower production in a situation in which the irrigated area of office du niger is expanded and a new dam is built at fomi in guinea ideally very strict boundaries are set by stakeholders regarding the deviation from reference data or models the model may show as guillaume and jakeman 2012 state stakeholders will ultimately decide whether a model s performance is acceptable this paper presents a test of building a model using global data and we chose not to involve stakeholders directly in order not to cause confusion over other more detailed models that were developed together with stakeholders for the same area instead we chose to compare our approach with the purpose and results of such a recently developed model liersch et al 2019 similar to our study liersch et al 2019 developed a modelling framework to analyze the effect of upstream interventions on the inner niger delta apart from the models used which are likely to have a relatively small impact on the model outcome and the simulation period we chose to use the most recent datasets available the main difference between the two studies is that liersch et al 2019 complemented the global datasets with data derived from various reports for the model parameterization table 4 to describe the changes in dynamics of flows reaching the inner niger delta we identify a number of indicators based on the so called hydrological environmental flow methods such as the method that determines indicators of hydrological alteration iha richter et al 1997 this method postulates that five characteristics of flow regimes are important to describe the flow dynamics that trigger or maintain ecosystem functioning magnitude timing frequency duration rate of change magnitude of flows gives insight into changes in volume in river flows all of these characteristics of flows of different magnitudes are important to trigger various ecosystem processes bunn and arthington 2002 in our simplified model we assume that the main event is the annual flooding of the inner niger delta since this is already included in the magnitude of the annual discharge and is likely to continue occurring on an annual basis no specific frequency indicator is included rate of change is important in relation to sudden releases from hydropower dams but since these cannot be distinguished with a model simulated with a monthly time step this aspect is left out of consideration for all of these indicators it would be important to compute changes as a result of scenarios or strategies with existing analysis to determine whether our model is fit for purpose of these five characteristics the study by liersch et al 2019 addresses flow magnitude through three types of indicators which we consider as well these are average discharge low flows here defined as discharge that is exceeded 90 of the time q90 following the flow duration curve approach vogel and fennessey 1994 smakhtin 2001 and high flow discharge that is exceeded 10 of the time q10 as well as the maximum inundation extent of the inner niger delta following the study by liersch et al 2019 we compute these indicators for only the niger discharge not for the entire inner niger delta inflow which would have included also the bani discharge this makes sense since both interventions the fomi dam and the expansion of the office du niger irrigated area take place on the niger to describe the changes in irrigation water supply we consider three indicators 1 the percentage of niger discharge allocated to the office du niger irrigation area 2 the extent to which irrigation demands are not met at the office du niger irrigation area on average average annual supply gap 3 the extent to which irrigation demands are not met at the office du niger irrigation area during the dry season average dry season irrigation supply gap to describe the change in hydropower production we compute the average annual production in megawatt mw table 5 summarizes the indicators unfortunately the grdc data were only available for discrete periods and did not allow for the calculation of long term statistics to compare our fit for purpose indicators to compare our results with those of liersch et al 2019 the same situations are analyzed table 6 similar to liersch et al 2019 we analyzed an expansion to 460 000 ha of the irrigated area managed by office du niger because the fomi moussako dam is new we were unable to derive information on dimensions or operation from an open data set for this first exploration we chose to use the same dimensions and simple operation rules as for the selingue dam the other large reservoir in the niger catchment however where we assumed that selingue would be operated to meet downstream water demands in mali we assumed that the fomi moussaka dam that will be located in guinea only serves to meet guinean hydropower and water demands we evaluate the results as follows by which we compare the results as computed by liersch et al 2019 with the best results of our two simulations based on chirps and mswep if the value by liersch et al 2019 falls between the chirps and mswep results we evaluate our result as good if the value by liersch et al 2019 falls outside the chirps mswep range we evaluate the results as follows 1 results expressed as percentage change compared to the natural flow situation o 5 percentage point difference good o 6 15 percentage point difference satisfactory o 15 percentage point different unsatisfactory 2 results expressed as absolute values o 5 percent difference good o 6 15 percent difference satisfactory o 15 percent different unsatisfactory 3 results expressed as percentage demand met o 5 percentage point difference good o 6 15 percentage point difference satisfactory o 15 percentage point different unsatisfactory 3 model credibility results 3 1 hydrological performance tables 7 and 8 present the kge and nse performance indices for the wflow sbm model with the adjusted parameters for discharge simulation at a daily time step and the resampled discharge simulations to a monthly time step fig 3 a and b presents a comparison between the simulated and observed monthly discharges for simulations with mswep and chirps respectively the performance indices were computed for the simulation with mswep for the period 1980 2012 and for the simulation with chirps this was done for the period 1982 2012 for the simulation at the daily time step with mswep the wflow sbm model performance is satisfactorily 0 4 kge 0 70 and good kge 0 70 for two stations at a daily time step respectively for the monthly time step the wflow sbm model performance is satisfactorily and good for two stations respectively only for the station douna located on the bani tributary the wflow sbm model performance with mswep is unsatisfactory kge 0 4 at a daily and monthly time step for this station wflow sbm underestimates the discharge β 0 56 and shows less variability α 0 44 for the simulation at the daily time step with chirps the wflow sbm model performance is satisfactorily for one station and good for two stations in the most upstream part of the upper niger basin the wflow sbm model performance for the stations kouroussa and kankan is unsatisfactory with overestimations of the discharge relatively high α and β values for the monthly time step the wflow sbm model performance is satisfactorily and good for two stations respectively only for station kouroussa the wflow sbm model performance is unsatisfactory caused by relatively high α and β values interestingly the performance of the wflow sbm simulation with chirps for station douna located in the bani river is much better good than the wflow sbm simulation with mswep unsatisfactory for this station the performance indices for the ribasim model vary significantly with location and rainfall data used table 9 grdc station kirango aval is located in the main niger branch and at the beginning of the inner niger delta it therefore represents most of the water that enters the ind at this location the simulation with mswep gives good results kge 0 70 and the simulation with chirps satisfactory results 0 4 kge 0 70 when we look at the performance over time chirps more or less consistently leads to an overestimation of peak discharges for the entire 1981 2012 simulation period while mswep performs quite well until the year 2000 afterwards the mswep peak discharges are significantly and constantly underestimated this behavior cannot be explained by the physical system on station beneny kegny located on the lower section of the bani tributary both mswep and chirps give a satisfactory model performance although chirps performs significantly better here than mswep the simulated discharge at this location is clearly underestimated with mswep low value for β see also fig 3 c and overestimated with chirps high value for β see also fig 3 d the goodness of fit between the average niger bani discharge and the maximum inundation area is small for both mswep r2 0 27 and chirps r2 0 29 fig 3 e for mswep this is partly explained by the relatively poor performance of the wflow ribasim model for the period 2000 2012 fig 3c which unfortunately coincides completely with the period that was used for the regression analysis 2000 2014 to summarize the model performance results in terms of accuracy for simulations with mswep and a monthly time step the combined wflow ribasim model performance is good kge 0 7 for three stations satisfactory 0 4 kge 0 70 for three stations and poor kge 0 4 for one station located on a niger tributary the simulations with chirps lead to a good performance at two stations satisfactory performance at four stations and poor performance at one station at arguably the most important gauging station in the system grdc station kirango aval in the main niger branch at the beginning of the inner niger delta mswep gives far better model performance kge 0 82 than chrips kge 0 44 based on these results we argue that 1 mswep gives the best model performance in terms of accuracy and that 2 because the model results are satisfactory or better at 6 out of 7 stations for mswep and 5 out of 7 stations chirps we consider the overall model results with both options as satisfactorily accurate 3 2 fit for purpose comparing our results for the fit for purpose indicators with those published by liersch et al 2019 shows that the majority of the indicators can be judged as scoring good or satisfactory we identified 24 indicators for which results were available and that could be reproduced with our wflow ribasim inundation extent model using the assessment criteria based on percentage or percentage point deviation as explained above we find that results for nine of the 24 indicators can be considered good ten can be considered satisfactory and five can be considered unsatisfactory see table 10 it is important to note that despite the low accuracy of the inundation extent regression model the model estimates of inundation extent have only little deviation from the results by liersch et al 2019 for the five indicators of which the deviation from the values found by liersch et al 2019 is large we notice two things 1 we see that for the scenario with both the construction of the fomi dam and an expansion of the irrigated area of office du niger our model estimates changes compared to the natural situation that are in the same direction as the results by liersch et al 2019 but larger this is the case for annual average river flows low flows and irrigation supply gaps 2 striking is that for the change in the magnitude of low flows q90 the direction of the change differs between our results and those of liersch et al 2019 for the base case and the fomi scenario whereas liersch et al 2019 finds that the low flow discharge increases in these two cases compared to the natural situation our analysis finds that low flows are reduced however we also find that the construction of the fomi dam greatly increases low flows however because low flows had decreased since the natural situation due to abstractions for irrigation and other water use the increase in low flows due to regulation brings the low flows back to a similar level as it was in the natural situation differences may result from modelling choices that would in a real case be discussed with the client rather than from the data used we therefore judge the performance as satisfactorily and fit for purpose 4 discussion in this paper we present a new method to quickly construct water resources models based solely on open data that can be used for rapid assessments where time is of the essence or high quality local data is unavailable and which is suitable for the evaluation of alternative interventions in land and water management considering the combined results of the hydrological performance metrics and the fit for purpose indicators we argue that our model based on open data with minimal tuning is of satisfactory quality for initial exploration of a water resources system and fit for the purpose of gaining insight in how relevant water system indicators change as a result of upstream interventions this is a promising finding that would allow applying this method in catchments with low data availability in order to inform decisions it is important to emphasize that despite our conclusion that the model is considered to be fit for purpose this does not eliminate the need to develop detailed water resources models with local data for more long term and detailed water resources planning and to involve stakeholders accordingly if data is available at this stage for calibration we recommend that it is done however we wanted to test how well the model was performing without calibration as an indication of how suitable the method would be in case no calibration data would be available for any application it is important to critically assess the question that needs to be answered the required accuracy of the model and the use of locally available data to further refine calibrate and validate the model before applying it in an analysis we envisage an interesting approach in which global data can be used to quickly develop initial models that can inform questions about directions for interventions and early stages of a project or in cases where quick but informed decision making is required such as in humanitarian interventions or conflict management the suggested approach can form the basis for the development of a more detailed model once stakeholder questions require this and available data allows it here we discuss the method in comparison with earlier modelling of the upper niger basin as well as its relevance beyond this specific application by considering 1 its salience 2 its credibility 3 its resource requirements and 4 its replicability replicability is not a specific model criterion but rather needed to illustrate the relevance of the proposed modelling approach using global data beyond the specific application presented here 1 salience the relevance of modelling water allocation salience of the model refers to the relevance of the model in relation to policy questions that stakeholders like to have answered although we did not involve stakeholders in this study we made use of earlier studies to identify typical policy questions for water management in the upper niger basin these questions pertain to different ways of regulating and allocating water within the basin this thus required a model suitable to analyze water management options at a sub basin resolution this was the background to combine a hydrological rainfall run off model with a water allocation model these types of water management questions are common to water management at catchment scale as demonstrated by the many publications on catchment level water allocation kimmage and adams 1992 van beek et al 2008 making the application of the upper niger basin a salient example to explore new ways of catchment level modelling 2 model credibility comparison of results to earlier studies this method was applied to the case study of the upper niger catchment and evaluated the performance of the main model output parameters simulated discharges and total inundation extent for simulated discharges although the accuracy is not as good as for similar models of the same study area based on a combination of open and local data had better performance see for example pedinotti et al 2012 although these studies applied other performance metrics liersch et al 2019 we have argued that it is indeed satisfactory enough for the model to be used to evaluate interventions for total inundation extent the goodness of fit criterium of the relation between simulated discharges and observed total inundation extent based on remote sensing is poor r2 0 3 especially compared to the correlation between water levels at a central gauging station and observed inundation extent found by zwarts et al 2005 r2 0 99 zwarts et al 2005 first derived a basic correlation using locally measured water levels at a central location in the downstream inundation area and then refined this correlation with detailed local information on variations in inundation extent over time this has led to increased accuracy the correlation between water levels and flood extent found by zwarts et al 2005 was combined with correlation of annual discharge and water level to derive a discharge inundation extent curve with high accuracy r2 0 98 wetlands international 2019 this shows that while using global data can support the quick development of models local data are needed to increase its accuracy overall we conclude that the model is of relevance it gives insight in results of alternative interventions it is of satisfactory accuracy and detail as shown by the fit for purpose indicators it is based on accessible open data contributing to a level playing field and it can be developed in a short timeframe in this paper we apply the concept of fit for purpose after having established the accuracy of the separate model components we analyzed whether the model would be fit for purpose a model can be considered to be fit for purpose if a better performing model would not have led to a different decision ideally it should also be considered whether a simpler and less accurate model or no model at all would have resulted in the same policy advice as purpose we specified the provision on information on the consequences of possible future developments in the upper niger catchment we found that results were satisfactory in comparison with results found in an earlier paper in which more local data and the better performing inundation model developed by zwarts et al 2005 were used the main reason for the better performance of the fit for purpose indicators compared to the accuracy indicators is that fitness for purpose is based on statistics average q10 q90 over the entire time series that is analyzed while for model accuracy the results for each individual time step are compared with data and evaluated this type of model is therefore best suited to assess relative changes in indicators as a result of possible future scenarios or interventions and less suited to research absolute values and variations over time of model parameters 3 resource constraints a major advantage of a global data based model is that it can be developed in a relatively short time frame to discuss the time required for model development it is useful to distinguish five steps in the model development process table 11 1 modelling preparation before starting the actual model development it is important to identify the purposes of the model how to assess its performance and specific water resources challenges in the area 2 data identification as discussed above the quality and recency of these data varies across the globe so it is important to critically assess the available data sources identify the most appropriate datasets or ideally test several of them and to be aware of the limitations of the data for the water resources system under consideration 3 model construction specialized tools and computer code to process data into models are increasingly available see for example the blueearth model builder tools deltares 2021 using the scripts developed for data processing for the upper niger basin construction of a similar model for a different area can be done almost automatically with some manual actions to connect the various inputs 4 model testing and tuning pre agreed performance metrics need to be assessed and compared with thresholds if desired model tuning or fast calibration could be carried out the time required to simulate a grid based hydrological model will vary based on model extent and resolution and on the length of the timeseries to be analyzed the time required to run a network based model is negligible 5 model application and results processing once the model is ready for application the desired outcome indicators can be computed and presented the time for a full cycle of steps is estimated at 2 4 weeks we suggest an approach with no or limited calibration but assume that the hydrological model is simulated with several alternative climatological forcings and that a few iterations may be required to choose the dataset that leads to the best model performance additional calibration as well as assessing multiple scenarios or intervention options would of course result in an increased throughput time 4 replicability looking beyond the specific application an additional advantage of the proposed method is that it has the potential to be repeated almost anywhere in the world because we make use of data that has near global coverage and in principle is available to all although some data sets require permission requests and some require special tools to access and further process however it is important to always critically review the data and where possible use alternative datasets to test the sensitivity in the output results local data and information will always be valuable to complement the global data and to test and increase the accuracy of the model in this study we encountered two issues regarding the available data that we discuss here 1 alternative rainfall data sets resulted in significant variations in simulated discharges hydrological models are normally very sensitive to errors in precipitation even small deviations can result in much larger deviations in discharge when it exceeds or falls below the amount of precipitation that can infiltrate in the soil and does not contribute to the quick run off component in this research we applied two alternative rainfall datasets that were easily available chirps and mswep both these data sets were generated with global circulation models using reanalysis techniques this type of historical climate data has the advantage that it normally covers the entire world and is available for extended periods however the differences between the two alternatives turned out to be significant it is difficult to assess which of the two datasets performs best for the niger basin because chirps performs better for the bani stations and mswep performs better for the niger stations this is especially visible at the downstream niger grdc station kirango aval arguably the most important grdc station in the area chirps more or less consistently leads to an overestimation of peak discharges for the entire 1981 2012 simulation period fig 3e while mswep performs quite well but only for the period 1979 2000 fig 3e after the year 2000 the mswep peak discharges are significantly underestimated this behavior cannot be explained by changes in the physical system the uncertainty in precipitation datasets and the need to include various datasets instead has been indicated before for example by biemans et al 2009 2 there is a lack of overlap of the periods of time for which data are available many datasets were developed at a certain moment in time and may therefore not accurately represent the current situation an example is the myneni et al 2015 dataset which was constructed between 1998 and 2002 and which seems to underestimate the current cropped areas in the project area frequent updating of such datasets would greatly benefit the quality of water resources models that are based on this type of open data the grdc discharge data set is still being updated but happened to lack recent data for the study area this negatively impacted the model performance especially of the goodness of fit of the relation between simulated discharges and total inundation area extent some data sets especially those based on satellite images are based on new technologies and as a result are not available further back in time for example the landsat satellite images used for the derivation of the inundation extent are only available from 2000 onwards which is rather short for many applications for the specific purpose of quickly assessing imminent water risks near real time data must be available especially for the climate forcing of the models the recency of these data varies by climate product quality assured monthly updates of era5 are published within 3 months of real time preliminary daily updates of the dataset are available to users within 5 days of real time preliminary chirps gts only 2nd day after new pentad chirps final product has an average latency of about 3 weeks historical forcing can also be based on ecmwf and gfs control forecasts like gloffis emerton et al 2016 for rainfall there are near real time satellite based products such as imerg and gsmap available huffman et al 2015 kubota et al 2020 an aspect that deserves attention are the characteristics of water resources systems some types of water resources systems may pose specific challenges that require a modified version of our method for example the inner niger delta that is included in our model is a flat inundated wetland area that is difficult to model as part of the hydrological model because the variations in terrain elevation may be smaller than the error margins in the digital terrain map this was solved in this study by applying a regression model to satellite observations of inundation extent and modelled discharges other water resources systems may be characterized by important groundwater resources and abstractions groundwater is included in the water balance of hydrological models such as wflow however in a system where groundwater abstractions play an important role as a water resource estimating water shortage requires more detailed insights in groundwater levels this would require complementing the suite of models with a dedicated groundwater model another aspect not considered in this study but may be important in other areas is water quality in many basins around the world it is poor water quality that renders water unsuitable for human agricultural or industrial use and thus results in shortage of water of satisfactory quality for its intended use as far as we are aware there are no global data sets for water quality however based on information on water use such as population size presence of agriculture and industry combined with generic estimations of pollution loads per unit and type of water use pollution loads can be estimated 5 conclusions in this paper we presented a new method to quickly construct water resources models based solely on open data and which is suitable for the evaluation of alternative interventions in land and water management especially in situations where decision making is required to maintain or reestablish cooperation stability and peace this type of model presents an intermediate level between coarse global models and detailed models based on local data this modelling level can provide results within shorter timeframes then required for local models yet has the possibility to assess alternative interventions something which is not possible with global models as such it can help decision makers in quickly deciding whether their involvement or a change in their involvement in an area at risk would be desirable and could be effective the time aspect is important in situations of imminent water risks moreover the use of data with near global coverage and which are available online allows for replication of the method in other areas it also allows for replication by others for the same area adding to transparency and a level playing field something that is important in situations where water related interventions could cause or exacerbate conflicts we applied this approach to the upper niger basin and evaluated the resulting model as satisfactory by considering not only traditional performance metrics but also fit for purpose indicators with which we evaluate whether the rapidly constructed model would not lead to a different decision than a higher performing model we find that the models scores satisfactory on both types of indicators even though the model performance in terms of hydrological model performance indicators is not as good as for dedicated models constructed using local data we therefore conclude that models based on global open datasets may offer a suitable approach for rapid assessments where time is of the essence or high quality local data is unavailable different basins and situations will all have their own challenges in terms of societal water use and water related risks and in terms of physical basin characteristics we promote replicating the approach in different basins to draw lessons regarding performance and fitness for purpose under different circumstances to further develop knowledge and tools and to know under which circumstances these methods have their limitations and requires complementation with additional locally measured data software availability name of software wflow developer deltares contact information wflow deltares nl year first available 2014 required hardware and software 64 bit windows python or linux python program language python 3 6 program size 2 5 gb conda environment availability and cost wflow is released as free open source code under the gnu general public license version 3 https github com openstreams wflow free license for executable name of software ribasim river basin simulation model developer deltares contact information wil nm van der krogt deltares boussinesqweg 1 2629 hv delft the netherlands email wil vanderkrogt deltares nl year first available 1985 required hardware and software no specific hardware or software is required a normal windows pc will due program language fortran vbasic c program size ribasim version 7 01 22 covers various components of different size computational core component is 6 65 mbytes availability and cost the software is free for educational and research purposes like for msc and phd students but a fee has to be paid for commercial use declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research for this paper was done in the framework of the water peace and security wps partnership supported by the netherlands ministry of foreign affairs grant number 4000003751 the specific component focusing on the use of global data for rapid water allocation model development which is the topic of this paper received additional support from deltares research program on water and societal stability we thank two anonymous reviewers for their constructive comments which helped to improve our manuscript 
25739,when decision making in the case of imminent water related humanitarian disasters or violent conflict depends on a quick assessment of water related risks there may not be enough time to collect high quality local data online available global data may offer an alternative data source we present a method to construct a water resources model based on global datasets we apply it to the upper niger basin in west africa and test its credibility with hydrological performance metrics and fit for purpose indicators the fit for purpose indicators are tailored to questions the model should support answering and allow assessment of whether a better performing model would result in a different basis for decision making we find that the model scores satisfactory on both types of indicators thus models based on global open datasets may be suitable for rapid preliminary assessments where time is of the essence or high quality local data is unavailable graphical abstract image 1 keywords global online datasets water resources water related conflict and humanitarian risks rapid model development inner niger delta upper niger 1 introduction shortage of water can have various economic and social consequences low river discharges can lead to poor water quality and reduced fish catch unsatisfactory rain and irrigation water can result in failed harvests or reduced availability of grasslands for livestock reduced water supply for households can have implications for human health these impacts of water shortage can subsequently lead to loss of lives of livelihoods as well as of socioeconomic opportunities and may result in humanitarian crises perceptions of underlying causes of water shortage and of an unequal distribution of impacts can further contribute to dissatisfaction and could trigger social unrest and violent conflict between different water users different communities or larger regions united nations and world bank 2018 to respond effectively to emerging or on going disasters or conflicts and avoid humanitarian diplomatic or water management measures that have unintended consequences it is important to have information regarding the current water resources situation and the effectiveness of possible risk reducing actions also the development and sharing of incorrect perceptions about water resources availability and distribution that could lead to competition and conflict can be prevented by timely producing and sharing data and information that is easily accessible to all especially in protracted conflict situations using openly available data can contribute to neutrality and trust of the data and of the resulting analysis results water resources models are commonly used to generate information on how water availability and shortage vary under the impacts of climatological variation increased water use or increased river regulation these models are typically based on information on land use and elevation climate and water use and are used to understand the impacts of possible future developments or of policy and management actions on water infrastructure water users and ecosystems at the same time they inform decision makers in their decisions concerning the use the development or the protection of water resources several requirements can be identified for the model and resulting information models and resulting information need to be 1 salient 2 credible and 3 legitimate van voorn et al 2016 salience refers to the relevance of the model for the policy questions that need to be answered credibility refers to the scientific quality of the model and legitimacy refers to the extent to which stakeholder views and concerns have been considered in the process of model development various authors discuss model choices in relation to the selection of the model concept e g addor and melsen 2019 babel et al 2019 what has received less attention is the approach to developing these models and the opportunities of making use of the increasing availability of global scale data model based information is currently mainly available either through detailed catchment studies or through coarse global level models both have limitations which we discuss below therefore in this paper we argue that an intermediate level is required at which global data are used for catchment level analysis van voorn et al babel et al 2019 suggest that often a trade off needs to be made between the three model quality criteria and found that stakeholders value salience over credibility such trade offs are usually necessary due to time and budget constraints using global data for catchment level analysis can provide a suitable method to develop a model relatively quickly a question that this paper seeks to help answer is to what extent credibility and salience are compromised if this approach is taken an additional benefit of using global data is its replicability the method including the data processing scripts can potentially be re used further reducing time and budget requirements in fig 1 we present the three levels 1 catchment models based on local data 2 catchment models based on global data and 3 global hydrological models based on global data in relation to salience credibility and resource constraints because our focus is on testing the credibility and salience of a model based on global data based on a desk study we leave legitimacy out of consideration which would have required consultation with stakeholders in relation to an actual policy question however we believe that using global data sets that are accessible to all is beneficial for model legitimacy here we first discuss the strengths and weaknesses of the catchment analysis using local data and of the global water resources models to then make our case for the intermediate level of catchment level analysis using global data catchment scale water allocation models using locally collected data have been the traditional approach for water resources planning studies for many decades see e g loucks and van beek 2005 because of the local level of the study models and data processing could be tailored to the specific characteristics of the catchment and the information needs of stakeholders thus potentially resulting in high salience and legitimacy moreover to achieve this these models usually have a high level of detail and need high quality locally measured data which improve the credibility of the model these studies are generally commissioned by government actors and are useful for strategic long term planning which requires detailed analysis of what type of policy actions are suitable under a range of possible future situations the results will answer the question of what measures will be most effective to achieve objectives for the water resources system and will help actors to decide what measures to implement however developing such models may be time consuming and resource intensive and the locally measured data for this type of high resolution model is not always available or of unsatisfactory quality where high quality local data is lacking global hydrological models can offer an alternative data sets amongst others on climate weather soil geology and topography that cover large regions or the entire world have increasingly become available over the past couple of decades the availability of such near global data sets has enabled the development of global hydrological models ghms global hydrological models have long development and run times but increasingly model results and underlying data are directly available free of charge online for example through the earth2observe water cycle integrator wci plymouth marine laboratory 2020 in such cases required resources to use the information are very low and accessibility is high the level of detail of ghms is low compared to catchment models ghms are gridded models in which all water balance computations take place at the grid cell level with a typical spatial resolution of 30 arc minutes which is around 50 50 km at the equator see bierkens et al 2015 for an overview of various ghms over time ghms have developed into increasingly finer resolutions down to 5 5 km increasing their applicability at the local level e g pcr globwb 2 0 wetlands international 2019 watergap 3 verzano et al 2012 and the w3 version 2 model van dijk et al 2018 many of these ghms also take water demand and river regulation by reservoirs into account despite the increasing resolution and possibilities to include water demand and regulation ghms follow a single approach for all catchments and are not designed to be tailored to the specific needs and interests of stakeholders in a specific catchment also most ghms do not offer functionalities to easily test different reservoir operation rules or water demand changes this means that salience of these models is likely to be unsatisfactory for specific catchment level questions moreover the ghms are generally not calibrated for specific catchments as a result the representation of the local hydrology is unknown and credibility may be inadequate ghms are very suitable to quickly identify where and when water shortage related risks could occur and to get a general indication of the causes of water shortage and possible solution options they are less suitable to analyze catchment specific variations in water use water regulation or options to explore different water management strategies and thus might not meet decision makers needs because of the limitations of ghms regarding salience and credibility and of models based on local data regarding resource requirements we present here an intermediate level in which catchment models are developed using online global data in this paper we focus on this intermediate level which has been much less applied and researched than the other two levels existing basin scale applications of global data tend to focus only on the biophysical components e g gusev et al 2006 conway and mahé 2009 andersson et al 2017 liersch et al 2019 developed a model of the niger basin that includes water abstractions for irrigation and reservoir operation in that study only the rainfall runoff component was done using global data additional local data and information were used for the water allocation and demand component pechlivanidis and arheimer 2015 developed a hydrological model for india only based on global datasets including reservoirs agricultural and irrigation datasets and calibrated the irrigation parameters that regulate water demand and abstraction siderius et al 2018 applied a ghm lpjml including irrigation abstractions to the rufiji river basin in eastern africa and changes to the input rainfall model schematization and parameters were required to improve model performance from generally poor to reasonable or good the use of solely open data to develop a local basin level water resources model has not yet been extensively tested in this paper we extend the use of open data for catchment level analysis with water use and water regulation for this purpose we develop and evaluate a combined model in which a gridded rainfall runoff model provides input to a network based water allocation model both based solely on global open data sets that are available online we evaluate the credibility of the combined model in two ways by considering both standard hydrological performance metrics and dedicated fit for purpose indicators we apply this approach to the upper niger basin in mali the inner niger delta that depends on inflowing water from the upper niger basin is prone to violent conflict that may have a link with natural resource availability decisions need to be taken on a new dam and expansion of irrigated areas in the upper niger basin that are likely to alter the natural flooding regime of the wetland area and will subsequently impact the ecosystem services that form the basis for the livelihoods of its inhabitants information on how upstream interventions will impact the livelihoods of the people in the inner niger delta and the conflicts over natural resources is important to into account in decision on theses interventions the water resources in the catchment have been studied using local data a recent example is the study by liersch et al 2019 we therefore use that study as comparison material for the global data based model presented in this study 2 methods 2 1 study area the study area consists of the entire upper niger catchment including the inner niger delta ind until the border between the tombouctou and gao regions in mali fig 2 a the catchment is located in guinea ivory coast burkina faso and mali and is around 380 000 km2 in size the headwaters of the main niger river branches are located in the daro massif in guinea while the main tributary the bani originates in southern mali and ivory coast the bani enters the niger river at the upper boundary of the inner niger delta a vast heavily vegetated floodplain with highly productive farmlands and fishing areas just upstream of the ind lies office du niger a large irrigation scheme of around 100 000 ha that produces most of mali s rice and sugar hertzog et al 2012 the movement of the intertropical convergence zone itcz has led to a sharp north south gradient in annual rainfall from around 2000 mm in the southern part of guinee to around 300 mm in the sahel zone in the north andersen et al 2005 the entire catchment has a distinct dry season from december to may and a monsoon season that is longer and heavier in the south high average temperatures and low humidity lead to evaporation rates of around 40 of the annual inflows andersen et al 2005 supported by natural variations in high and low flows these wetlands provide important ecosystem services to around 1 5 2 million people who inhabit the ind these people base their livelihoods on farming cattle herding and fisheries zwarts et al 2005 cattle herding is nomadic and cattle spend part of the year in the ind to feed on the water grass bourgou that grows in and along the wetlands pressure on wetland resources has reduced livelihood opportunities and increased competition between the different socio ethnic groups present in the area these pressures are the combined result of population growth and reduced resource availability enhanced by various social economic and political dynamics wetlands international 2019 changes in water abstraction and regulation in the upstream catchment as currently planned through a new dam at fomi or moussako in guinea and expansion of the irrigated perimeter managed by office du niger in mali could lead to a further alteration of the inundation regime of the ind that could result in loss of ecosystem services and related livelihoods and trigger increased competition and conflict 2 2 model setup the model should allow for the analysis of water resources their use and related risks how these are impacted by climate change and socio economic developments and how interventions e g alternative dam locations dimensions operation irrigation expansion crops and irrigation types could reduce or exacerbate water related risks this leads to the following requirements the model needs to be able to simulate seasonal forecasts or time series of 20 50 years or more to account for possible climate variability within acceptable time frames when assessing intervention options the model needs to include explicit water demand and water regulation options that can be altered to simulate interventions the model should provide the following output o scenario specific discharges at key locations in the system oparameters related to the use of water in the basin for example for irrigation and hydropower the core of the model framework consists of the gridded wflow rainfall runoff model schellekens et al 2019 and the ribasim water resources management network model van der krogt 2008 fig 2 b wflow simulates discharge hydrographs at the level of subcatchments provides water discharges from the subcatchments at the chosen inflow locations of the ribasim model next these are grouped at the level of ribasim units before being used by ribasim ribasim then simulates the effects of reservoir operation and of abstractions for irrigation and household water use on the river discharges and computes hydropower production and supply demand ratios for irrigated areas discharges at the downstream end of the niger and the bani upstream of the inner niger delta are combined with historical inundation areas extracted from earth observation images into discharge inundation area curves for the downstream section of the study area the inner niger delta these curves are then used to estimate the inundation areas the method is developed to apply it in data scarce catchments or in areas where a model is needed quickly no existing model is available and extensive model calibration and validation is not feasible or necessary given the application of the model for this reason the models are not fully calibrated only wflow parameters for which no global data exist were estimated through a couple of test runs available monitoring data were mainly used for model performance testing 2 2 1 wflow rainfall runoff model the hydrological model used is the spatially distributed hydrological model wflow sbm part of the openstreams wflow distributed hydrological modelling platform schellekens et al 2019 the wflow sbm model is programmed in python and based on the pcraster python framework karssenberg et al 2010 a raster based gis package suitable for dynamic computations fig 2 c shows the different processes and fluxes that are part of the wflow sbm hydrological concept the soil part of wflow sbm model follows the same concepts as the topog sbm model vertessy and elsenbeer 1999 topog sbm is specifically designed to simulate fast runoff processes during discrete storm events in small catchments 10 km2 evapotranspiration losses are ignored the derived wflow sbm approach can be applied to a wider variety of catchments because evapotranspiration losses and capillary rise were added to wflow sbm for the channel overland and lateral subsurface flow a kinematic wave approach is used similar to topkapi benning 1995 todini and ciarapica 2002 g2g bell et al 2007 1k dhm tanaka and tachikawa 2015 and topog sbm vertessy and elsenbeer 1999 rainfall interception is based on the analytical approach by gash 1979 the soil in every model grid cell is considered as a single bucket divided into a saturated and unsaturated store with the option to divide the soil column into different layers soil infiltration depends on the soil infiltration capacity and the fraction of paved and unpaved area transfer of water from the unsaturated store to the saturated store is controlled by the saturated hydraulic conductivity at the water table the relative saturation within the unsaturated zone above the water table and a power coefficient depending on the soil texture brooks and corey 1964 transpiration is first derived from the saturated store if roots intersect with the saturated store and then from the unsaturated store capillary rise from the saturated store also results in a flux from the saturated store to the unsaturated store the kinematic wave equation is used to route overland flow river flow and lateral subsurface flow over a d8 network the wflow sbm model for the upper niger has been setup at a grid resolution of 30 arc sec and simulations were performed at a daily time step the development of a wflow model using online data sources is typically done within a couple of days running simulations and fine tuning typically require one or several weeks 2 2 2 wflow parametrization and forcing table 1 lists the global datasets that were used for setting up the wflow sbm model for the upper niger basin and to generate the forcing two different precipitation datasets were used the climate hazards group infrared precipitation with station data chirps v 2 0 dataset at 0 05 resolution and the multi source weighted ensemble precipitation mswep version 1 0 at 0 25 resolution potential evapotranspiration e t 0 was based on the hargreaves equation calculated from hourly era5 variables temperature at 2 m and extraterrestrial radiation at 0 25 resolution hargreaves and samani 1982 potential evapotranspiration e t 0 was calculated as follows e t 0 0 0023 r a t m a x t m i n 0 5 t 17 8 where r a mm is the extraterrestrial solar radiation evaporation equivalent per day t m a x c and t m i n c are the daily maximum and minimum temperature respectively and t c is the daily mean temperature the forcing was interpolated to the wflow sbm model grid with nearest neighbor interpolation river cells for the wflow sbm model were defined as grid cells with a stream order strahler 1957 of three and larger smaller stream orders 1 and 2 are associated with widths up to 10 m downing et al 2012 which is small in comparison to the model s grid size the channel width was estimated by scaling yearly average discharge at the outlet for each point in the drainage basin based on the upstream area and using the approach of finnegan et al 2005 that requires the river slope β r i v e r manning s roughness coeﬃcient n r i v e r and a bank full width to depth ratio α that was estimated at 120 to enforce the drainage pattern of the hydrosheds river network the dem was adjusted for this river network before generating the local drain direction map the river length l r i v e r was estimated by generating a river map from a higher resolution dem 0 005 based on srtm v4 and to compute the total high resolution river cell length within a model resolution cell most wflow sbm parameters were estimated with pedo transfer functions ptfs from literature based on imhoff et al 2020 imhoff et al 2020 found that for most sensitive wflow sbm parameters a ptf is available only for the sensitive wflow sbm parameter k h o r f r a c a ptf is not available and a default value is applied also for the non sensitive wflow sbm parameters without a ptf a default value is used in line with the objective of the study we performed a limited calibration of the wflow sbm model for the upper niger basin by manually tuning the most sensitive parameters k h o r f r a c m r t d and θ s to improve the model performance for the tuning of the rooting depth parameter r t d we relied on kleidon and heimann 1998 the grdc stations in fig 2 d were used for the model parameter tuning these were selected because of data availability table 2 and because these stations are not influenced by reservoirs reservoirs were not included in the wflow sbm model the model parameter tuning was done for the years 1981 1985 2000 2007 and 2008 for the precipitation datasets mswep and chirps in combination with hargreaves potential evapotranspiration these years were chosen based on a combination of data availability and to include years throughout the complete simulation period this is important since both mswep and chirps also rely on satellite based precipitation products available since 1998 the wflow sbm parameters were adjusted as follows the m parameter was increased with a factor of 3 k h o r f r a c was set to 500 r t d was increased based on schenk and jackson 2002 and kleidon and heimann 1998 and θ s was increased with a factor of 1 025 after the model tuning wflow sbm was simulated with mswep for the period 1979 2012 and with chirps for the period 1981 2012 to assess the model performance for these periods results from these simulations provided discharge time series at the inflow points of the ribasim model fig 2d for longer time frames mswep 1979 2014 and chirps 1981 2018 2 2 3 ribasim water allocation model ribasim river basin simulation is a generic water resources modelling package van der krogt 2008a van der krogt 2008b that has been applied in various countries to analyze impacts of changes in water allocation and water regulation on different water users including the environment a ribasim application consists of a network of nodes representing either sources of water demands for water or regulation of water connected by links the configuration of the network and the associated division of the catchment into subcatchments is usually constructed based on the location of large river regulation infrastructure and corresponding large water demands to construct the network for the upper niger model five maps were overlayed subcatchment delineation country borders within the upper niger catchment mali guine cote d ivoire burkina faso irrigation density population density and location of large reservoirs and river regulation structures taking into account the natural runoff processes large abstractions and regulations and administrative information needs we then constructed a network of water source nodes i e wflow inflow points demand nodes regulation nodes and links fig 2e the resulting network was filled with information from open data sources on river regulation and demand table 3 this network also provided the spatial resolution combination of subcatchment and admin units at which grid based input data was aggregated to derive information that was included in the ribasim model nodes in addition the same precipitation and evaporation data as used for wflow were used to compute irrigation demands and reservoir water balances with the available online data and scripts to process them to be used in the ribasim model the development of the ribasim model would take typically a couple of days depending on the size of the catchment and required level of detail the calculation time step for the ribasim model was set at one month this is a commonly used timestep for analysis of long time series since the model does not compute physical processes but aggregated water balances this is generally considered a suitable timestep that allows for analysis of seasonal variations yet has a limited simulation time after the model construction the ribasim model was run for the same periods as wflow with mswep for the period 1979 2012 and with chirps for the period 1981 2012 using the resampled monthly wflow discharge timeseries at the inflow points as input because initial storage in reservoirs has a large influence on river discharge in the first years of the model simulation we added ten years of long term average inflow and climate data before the actual wflow output the ribasim results for these synthetic ten years only served to set initial conditions and were left out of consideration in model testing 2 2 4 the relation between river discharge and inundation area ecosystem services in the ind are supported by natural variations in high and low flows especially by the maximum inundation extent during the annual high flow period this maximum inundation extent is therefore an important model output variable in this research we estimate this parameter by correlating it with the combined discharge of the rivers niger and bani during the annual high flow period which is a model output of the ribasim model we use the following steps 1 derive inundation area from all available medium resolution satellite images 2 correlate these to the combined simulated niger bani discharges during the annual high flow period season derived with the ribasim model here we describe these steps in more detail 1 we derive inundation area for all catchments used in the study by estimating it from landsat 4 5 7 and 8 satellite images in total 6810 images were used the inundation area was computed employing the modified normalized difference spectral water index mndwi xu et al 2006 followed by the use of dynamic thresholding to estimate the surface water mask and to compute the inundation area donchyts et al 2016 all computations were performed with the google earth engine parallel processing platform gorelick et al 2017 2 here we take the combined niger bani discharge in m3 s averaged over the annual high flow period in the ind august to november and carry out a linear regression analysis with the method of least squares to correlate it to the maximum inundation extent within the inner niger delta for the period 2000 2014 this period was chosen because the required satellite images are not available before 2000 and input data sets for the combined wflow ribasim simulation were available only until the end of 2014 2 3 evaluating the credibility of the global data based model 2 3 1 hydrological performance we test the combined wflow ribasim inundation extent model by considering whether the model is 1 accurate and 2 fit for purpose to assess the accuracy of the wflow and ribasim models we compare monthly time series of simulated discharges at appropriate locations in the system with observed grdc discharges by assessing the model performance with the kling gupta efficiency kge gupta et al 2009 commonly used as a model calibration objective function we also report nash sutcliﬀe eﬃciency nse since this efficiency metric has been used traditionally for calibration and evaluation of hydrological models however nse has two important shortcomings that are addressed by kge the bias component is normalized by the standard deviation and as a result for basins with a high runoff variability the bias component tends to have a smaller contribution and nse will favor hydrological model parameters that underestimate variability in flows gupta et al 2009 kge and nse are not directly comparable while nse kge 1 means a perfect match between simulated and observed flows nse 0 and kge 0 41 indicates the model simulation is as accurate as the observed mean knoben et al 2019 here we apply the following ranges based on kge kge 0 4 unsatisfactory 0 4 kge 0 7 satisfactory kge 0 7 good the accuracy of the wflow sbm model is assessed at five grdc stations upstream of the major reservoirs and irrigation areas fig 2d the accuracy of the ribasim model at two locations downstream in the main niger branch just downstream of markala and on the downstream section of the bani tributary fig 2e of all locations the accuracy at the two downstream locations is considered most important as the combined discharge of the rivers niger and bani represents the bulk of the water that enters the inner niger delta the goodness of fit between the combined niger bani river discharges during the annual high flow period and the maximum yearly inundation extent is assessed by evaluating the coefficient of determination r2 of the fit 2 3 2 fit for purpose the purpose of the model for the upper niger catchment including the inner niger delta is to understand the trade offs between upstream interventions in the upper niger catchment specifically the construction and operation of a new dam and the expansion of the irrigated area and the wetland ecosystem of the inner niger delta and its further social and security consequences although the implementation of the upstream interventions will take many years for international actors involved in financing these interventions a rapidly constructed model using open data can give them insights in the various intended and unintended impacts of the proposed developments based on which it could be decided whether more detailed research would be required to assess whether the model is fit for purpose we apply the method of guillaume and jakeman 2012 of asking closed questions for our model these closed questions refer to the trade offs between the introduction of a new dam expansion of the area irrigated by office du niger and the ecosystem services of the inner niger delta this trade off is a trade off between hydropower production irrigation production and ecosystem services the condition of water related ecosystems and the availability of related ecosystems are understood to largely depend on the flow regime i e the variations in discharge that occur over time bunn and arthington 2002 arthington et al 2018 both high flows and low flows are important and not just their magnitude but also their timing frequency duration and rate of change richter et al 1996 richter et al 1997 for the inner niger delta these dynamics refer to combined inflows from niger and bani rivers and to how these translate in inundations patters of the wetland area because the purpose of the model is to understand trade offs between different users of the niger river as a result of proposed interventions fit for purpose for this case is less about the exact flow regime and more about the relative change in this flow regime as a result of specific policy interventions about which a decision needs to be made we therefore translated this purpose into the following questions to test its fitness 1 does the model represent well the change in inner niger delta inflow regime and inundation extent and irrigation water supply in a situation in the current situation 2 does the model represent well the change in inner niger delta inflow regime and inundation extent and irrigation water supply in a situation in which the irrigated area of office du niger is expanded 3 does the model represent well the change in inner niger delta inflow regime and inundation extent irrigation water supply and hydropower production in a situation in which a new dam is built at fomi in guinea 4 does the model represent well the change in inner niger delta inflow regime and inundation extent irrigation water supply and hydropower production in a situation in which the irrigated area of office du niger is expanded and a new dam is built at fomi in guinea ideally very strict boundaries are set by stakeholders regarding the deviation from reference data or models the model may show as guillaume and jakeman 2012 state stakeholders will ultimately decide whether a model s performance is acceptable this paper presents a test of building a model using global data and we chose not to involve stakeholders directly in order not to cause confusion over other more detailed models that were developed together with stakeholders for the same area instead we chose to compare our approach with the purpose and results of such a recently developed model liersch et al 2019 similar to our study liersch et al 2019 developed a modelling framework to analyze the effect of upstream interventions on the inner niger delta apart from the models used which are likely to have a relatively small impact on the model outcome and the simulation period we chose to use the most recent datasets available the main difference between the two studies is that liersch et al 2019 complemented the global datasets with data derived from various reports for the model parameterization table 4 to describe the changes in dynamics of flows reaching the inner niger delta we identify a number of indicators based on the so called hydrological environmental flow methods such as the method that determines indicators of hydrological alteration iha richter et al 1997 this method postulates that five characteristics of flow regimes are important to describe the flow dynamics that trigger or maintain ecosystem functioning magnitude timing frequency duration rate of change magnitude of flows gives insight into changes in volume in river flows all of these characteristics of flows of different magnitudes are important to trigger various ecosystem processes bunn and arthington 2002 in our simplified model we assume that the main event is the annual flooding of the inner niger delta since this is already included in the magnitude of the annual discharge and is likely to continue occurring on an annual basis no specific frequency indicator is included rate of change is important in relation to sudden releases from hydropower dams but since these cannot be distinguished with a model simulated with a monthly time step this aspect is left out of consideration for all of these indicators it would be important to compute changes as a result of scenarios or strategies with existing analysis to determine whether our model is fit for purpose of these five characteristics the study by liersch et al 2019 addresses flow magnitude through three types of indicators which we consider as well these are average discharge low flows here defined as discharge that is exceeded 90 of the time q90 following the flow duration curve approach vogel and fennessey 1994 smakhtin 2001 and high flow discharge that is exceeded 10 of the time q10 as well as the maximum inundation extent of the inner niger delta following the study by liersch et al 2019 we compute these indicators for only the niger discharge not for the entire inner niger delta inflow which would have included also the bani discharge this makes sense since both interventions the fomi dam and the expansion of the office du niger irrigated area take place on the niger to describe the changes in irrigation water supply we consider three indicators 1 the percentage of niger discharge allocated to the office du niger irrigation area 2 the extent to which irrigation demands are not met at the office du niger irrigation area on average average annual supply gap 3 the extent to which irrigation demands are not met at the office du niger irrigation area during the dry season average dry season irrigation supply gap to describe the change in hydropower production we compute the average annual production in megawatt mw table 5 summarizes the indicators unfortunately the grdc data were only available for discrete periods and did not allow for the calculation of long term statistics to compare our fit for purpose indicators to compare our results with those of liersch et al 2019 the same situations are analyzed table 6 similar to liersch et al 2019 we analyzed an expansion to 460 000 ha of the irrigated area managed by office du niger because the fomi moussako dam is new we were unable to derive information on dimensions or operation from an open data set for this first exploration we chose to use the same dimensions and simple operation rules as for the selingue dam the other large reservoir in the niger catchment however where we assumed that selingue would be operated to meet downstream water demands in mali we assumed that the fomi moussaka dam that will be located in guinea only serves to meet guinean hydropower and water demands we evaluate the results as follows by which we compare the results as computed by liersch et al 2019 with the best results of our two simulations based on chirps and mswep if the value by liersch et al 2019 falls between the chirps and mswep results we evaluate our result as good if the value by liersch et al 2019 falls outside the chirps mswep range we evaluate the results as follows 1 results expressed as percentage change compared to the natural flow situation o 5 percentage point difference good o 6 15 percentage point difference satisfactory o 15 percentage point different unsatisfactory 2 results expressed as absolute values o 5 percent difference good o 6 15 percent difference satisfactory o 15 percent different unsatisfactory 3 results expressed as percentage demand met o 5 percentage point difference good o 6 15 percentage point difference satisfactory o 15 percentage point different unsatisfactory 3 model credibility results 3 1 hydrological performance tables 7 and 8 present the kge and nse performance indices for the wflow sbm model with the adjusted parameters for discharge simulation at a daily time step and the resampled discharge simulations to a monthly time step fig 3 a and b presents a comparison between the simulated and observed monthly discharges for simulations with mswep and chirps respectively the performance indices were computed for the simulation with mswep for the period 1980 2012 and for the simulation with chirps this was done for the period 1982 2012 for the simulation at the daily time step with mswep the wflow sbm model performance is satisfactorily 0 4 kge 0 70 and good kge 0 70 for two stations at a daily time step respectively for the monthly time step the wflow sbm model performance is satisfactorily and good for two stations respectively only for the station douna located on the bani tributary the wflow sbm model performance with mswep is unsatisfactory kge 0 4 at a daily and monthly time step for this station wflow sbm underestimates the discharge β 0 56 and shows less variability α 0 44 for the simulation at the daily time step with chirps the wflow sbm model performance is satisfactorily for one station and good for two stations in the most upstream part of the upper niger basin the wflow sbm model performance for the stations kouroussa and kankan is unsatisfactory with overestimations of the discharge relatively high α and β values for the monthly time step the wflow sbm model performance is satisfactorily and good for two stations respectively only for station kouroussa the wflow sbm model performance is unsatisfactory caused by relatively high α and β values interestingly the performance of the wflow sbm simulation with chirps for station douna located in the bani river is much better good than the wflow sbm simulation with mswep unsatisfactory for this station the performance indices for the ribasim model vary significantly with location and rainfall data used table 9 grdc station kirango aval is located in the main niger branch and at the beginning of the inner niger delta it therefore represents most of the water that enters the ind at this location the simulation with mswep gives good results kge 0 70 and the simulation with chirps satisfactory results 0 4 kge 0 70 when we look at the performance over time chirps more or less consistently leads to an overestimation of peak discharges for the entire 1981 2012 simulation period while mswep performs quite well until the year 2000 afterwards the mswep peak discharges are significantly and constantly underestimated this behavior cannot be explained by the physical system on station beneny kegny located on the lower section of the bani tributary both mswep and chirps give a satisfactory model performance although chirps performs significantly better here than mswep the simulated discharge at this location is clearly underestimated with mswep low value for β see also fig 3 c and overestimated with chirps high value for β see also fig 3 d the goodness of fit between the average niger bani discharge and the maximum inundation area is small for both mswep r2 0 27 and chirps r2 0 29 fig 3 e for mswep this is partly explained by the relatively poor performance of the wflow ribasim model for the period 2000 2012 fig 3c which unfortunately coincides completely with the period that was used for the regression analysis 2000 2014 to summarize the model performance results in terms of accuracy for simulations with mswep and a monthly time step the combined wflow ribasim model performance is good kge 0 7 for three stations satisfactory 0 4 kge 0 70 for three stations and poor kge 0 4 for one station located on a niger tributary the simulations with chirps lead to a good performance at two stations satisfactory performance at four stations and poor performance at one station at arguably the most important gauging station in the system grdc station kirango aval in the main niger branch at the beginning of the inner niger delta mswep gives far better model performance kge 0 82 than chrips kge 0 44 based on these results we argue that 1 mswep gives the best model performance in terms of accuracy and that 2 because the model results are satisfactory or better at 6 out of 7 stations for mswep and 5 out of 7 stations chirps we consider the overall model results with both options as satisfactorily accurate 3 2 fit for purpose comparing our results for the fit for purpose indicators with those published by liersch et al 2019 shows that the majority of the indicators can be judged as scoring good or satisfactory we identified 24 indicators for which results were available and that could be reproduced with our wflow ribasim inundation extent model using the assessment criteria based on percentage or percentage point deviation as explained above we find that results for nine of the 24 indicators can be considered good ten can be considered satisfactory and five can be considered unsatisfactory see table 10 it is important to note that despite the low accuracy of the inundation extent regression model the model estimates of inundation extent have only little deviation from the results by liersch et al 2019 for the five indicators of which the deviation from the values found by liersch et al 2019 is large we notice two things 1 we see that for the scenario with both the construction of the fomi dam and an expansion of the irrigated area of office du niger our model estimates changes compared to the natural situation that are in the same direction as the results by liersch et al 2019 but larger this is the case for annual average river flows low flows and irrigation supply gaps 2 striking is that for the change in the magnitude of low flows q90 the direction of the change differs between our results and those of liersch et al 2019 for the base case and the fomi scenario whereas liersch et al 2019 finds that the low flow discharge increases in these two cases compared to the natural situation our analysis finds that low flows are reduced however we also find that the construction of the fomi dam greatly increases low flows however because low flows had decreased since the natural situation due to abstractions for irrigation and other water use the increase in low flows due to regulation brings the low flows back to a similar level as it was in the natural situation differences may result from modelling choices that would in a real case be discussed with the client rather than from the data used we therefore judge the performance as satisfactorily and fit for purpose 4 discussion in this paper we present a new method to quickly construct water resources models based solely on open data that can be used for rapid assessments where time is of the essence or high quality local data is unavailable and which is suitable for the evaluation of alternative interventions in land and water management considering the combined results of the hydrological performance metrics and the fit for purpose indicators we argue that our model based on open data with minimal tuning is of satisfactory quality for initial exploration of a water resources system and fit for the purpose of gaining insight in how relevant water system indicators change as a result of upstream interventions this is a promising finding that would allow applying this method in catchments with low data availability in order to inform decisions it is important to emphasize that despite our conclusion that the model is considered to be fit for purpose this does not eliminate the need to develop detailed water resources models with local data for more long term and detailed water resources planning and to involve stakeholders accordingly if data is available at this stage for calibration we recommend that it is done however we wanted to test how well the model was performing without calibration as an indication of how suitable the method would be in case no calibration data would be available for any application it is important to critically assess the question that needs to be answered the required accuracy of the model and the use of locally available data to further refine calibrate and validate the model before applying it in an analysis we envisage an interesting approach in which global data can be used to quickly develop initial models that can inform questions about directions for interventions and early stages of a project or in cases where quick but informed decision making is required such as in humanitarian interventions or conflict management the suggested approach can form the basis for the development of a more detailed model once stakeholder questions require this and available data allows it here we discuss the method in comparison with earlier modelling of the upper niger basin as well as its relevance beyond this specific application by considering 1 its salience 2 its credibility 3 its resource requirements and 4 its replicability replicability is not a specific model criterion but rather needed to illustrate the relevance of the proposed modelling approach using global data beyond the specific application presented here 1 salience the relevance of modelling water allocation salience of the model refers to the relevance of the model in relation to policy questions that stakeholders like to have answered although we did not involve stakeholders in this study we made use of earlier studies to identify typical policy questions for water management in the upper niger basin these questions pertain to different ways of regulating and allocating water within the basin this thus required a model suitable to analyze water management options at a sub basin resolution this was the background to combine a hydrological rainfall run off model with a water allocation model these types of water management questions are common to water management at catchment scale as demonstrated by the many publications on catchment level water allocation kimmage and adams 1992 van beek et al 2008 making the application of the upper niger basin a salient example to explore new ways of catchment level modelling 2 model credibility comparison of results to earlier studies this method was applied to the case study of the upper niger catchment and evaluated the performance of the main model output parameters simulated discharges and total inundation extent for simulated discharges although the accuracy is not as good as for similar models of the same study area based on a combination of open and local data had better performance see for example pedinotti et al 2012 although these studies applied other performance metrics liersch et al 2019 we have argued that it is indeed satisfactory enough for the model to be used to evaluate interventions for total inundation extent the goodness of fit criterium of the relation between simulated discharges and observed total inundation extent based on remote sensing is poor r2 0 3 especially compared to the correlation between water levels at a central gauging station and observed inundation extent found by zwarts et al 2005 r2 0 99 zwarts et al 2005 first derived a basic correlation using locally measured water levels at a central location in the downstream inundation area and then refined this correlation with detailed local information on variations in inundation extent over time this has led to increased accuracy the correlation between water levels and flood extent found by zwarts et al 2005 was combined with correlation of annual discharge and water level to derive a discharge inundation extent curve with high accuracy r2 0 98 wetlands international 2019 this shows that while using global data can support the quick development of models local data are needed to increase its accuracy overall we conclude that the model is of relevance it gives insight in results of alternative interventions it is of satisfactory accuracy and detail as shown by the fit for purpose indicators it is based on accessible open data contributing to a level playing field and it can be developed in a short timeframe in this paper we apply the concept of fit for purpose after having established the accuracy of the separate model components we analyzed whether the model would be fit for purpose a model can be considered to be fit for purpose if a better performing model would not have led to a different decision ideally it should also be considered whether a simpler and less accurate model or no model at all would have resulted in the same policy advice as purpose we specified the provision on information on the consequences of possible future developments in the upper niger catchment we found that results were satisfactory in comparison with results found in an earlier paper in which more local data and the better performing inundation model developed by zwarts et al 2005 were used the main reason for the better performance of the fit for purpose indicators compared to the accuracy indicators is that fitness for purpose is based on statistics average q10 q90 over the entire time series that is analyzed while for model accuracy the results for each individual time step are compared with data and evaluated this type of model is therefore best suited to assess relative changes in indicators as a result of possible future scenarios or interventions and less suited to research absolute values and variations over time of model parameters 3 resource constraints a major advantage of a global data based model is that it can be developed in a relatively short time frame to discuss the time required for model development it is useful to distinguish five steps in the model development process table 11 1 modelling preparation before starting the actual model development it is important to identify the purposes of the model how to assess its performance and specific water resources challenges in the area 2 data identification as discussed above the quality and recency of these data varies across the globe so it is important to critically assess the available data sources identify the most appropriate datasets or ideally test several of them and to be aware of the limitations of the data for the water resources system under consideration 3 model construction specialized tools and computer code to process data into models are increasingly available see for example the blueearth model builder tools deltares 2021 using the scripts developed for data processing for the upper niger basin construction of a similar model for a different area can be done almost automatically with some manual actions to connect the various inputs 4 model testing and tuning pre agreed performance metrics need to be assessed and compared with thresholds if desired model tuning or fast calibration could be carried out the time required to simulate a grid based hydrological model will vary based on model extent and resolution and on the length of the timeseries to be analyzed the time required to run a network based model is negligible 5 model application and results processing once the model is ready for application the desired outcome indicators can be computed and presented the time for a full cycle of steps is estimated at 2 4 weeks we suggest an approach with no or limited calibration but assume that the hydrological model is simulated with several alternative climatological forcings and that a few iterations may be required to choose the dataset that leads to the best model performance additional calibration as well as assessing multiple scenarios or intervention options would of course result in an increased throughput time 4 replicability looking beyond the specific application an additional advantage of the proposed method is that it has the potential to be repeated almost anywhere in the world because we make use of data that has near global coverage and in principle is available to all although some data sets require permission requests and some require special tools to access and further process however it is important to always critically review the data and where possible use alternative datasets to test the sensitivity in the output results local data and information will always be valuable to complement the global data and to test and increase the accuracy of the model in this study we encountered two issues regarding the available data that we discuss here 1 alternative rainfall data sets resulted in significant variations in simulated discharges hydrological models are normally very sensitive to errors in precipitation even small deviations can result in much larger deviations in discharge when it exceeds or falls below the amount of precipitation that can infiltrate in the soil and does not contribute to the quick run off component in this research we applied two alternative rainfall datasets that were easily available chirps and mswep both these data sets were generated with global circulation models using reanalysis techniques this type of historical climate data has the advantage that it normally covers the entire world and is available for extended periods however the differences between the two alternatives turned out to be significant it is difficult to assess which of the two datasets performs best for the niger basin because chirps performs better for the bani stations and mswep performs better for the niger stations this is especially visible at the downstream niger grdc station kirango aval arguably the most important grdc station in the area chirps more or less consistently leads to an overestimation of peak discharges for the entire 1981 2012 simulation period fig 3e while mswep performs quite well but only for the period 1979 2000 fig 3e after the year 2000 the mswep peak discharges are significantly underestimated this behavior cannot be explained by changes in the physical system the uncertainty in precipitation datasets and the need to include various datasets instead has been indicated before for example by biemans et al 2009 2 there is a lack of overlap of the periods of time for which data are available many datasets were developed at a certain moment in time and may therefore not accurately represent the current situation an example is the myneni et al 2015 dataset which was constructed between 1998 and 2002 and which seems to underestimate the current cropped areas in the project area frequent updating of such datasets would greatly benefit the quality of water resources models that are based on this type of open data the grdc discharge data set is still being updated but happened to lack recent data for the study area this negatively impacted the model performance especially of the goodness of fit of the relation between simulated discharges and total inundation area extent some data sets especially those based on satellite images are based on new technologies and as a result are not available further back in time for example the landsat satellite images used for the derivation of the inundation extent are only available from 2000 onwards which is rather short for many applications for the specific purpose of quickly assessing imminent water risks near real time data must be available especially for the climate forcing of the models the recency of these data varies by climate product quality assured monthly updates of era5 are published within 3 months of real time preliminary daily updates of the dataset are available to users within 5 days of real time preliminary chirps gts only 2nd day after new pentad chirps final product has an average latency of about 3 weeks historical forcing can also be based on ecmwf and gfs control forecasts like gloffis emerton et al 2016 for rainfall there are near real time satellite based products such as imerg and gsmap available huffman et al 2015 kubota et al 2020 an aspect that deserves attention are the characteristics of water resources systems some types of water resources systems may pose specific challenges that require a modified version of our method for example the inner niger delta that is included in our model is a flat inundated wetland area that is difficult to model as part of the hydrological model because the variations in terrain elevation may be smaller than the error margins in the digital terrain map this was solved in this study by applying a regression model to satellite observations of inundation extent and modelled discharges other water resources systems may be characterized by important groundwater resources and abstractions groundwater is included in the water balance of hydrological models such as wflow however in a system where groundwater abstractions play an important role as a water resource estimating water shortage requires more detailed insights in groundwater levels this would require complementing the suite of models with a dedicated groundwater model another aspect not considered in this study but may be important in other areas is water quality in many basins around the world it is poor water quality that renders water unsuitable for human agricultural or industrial use and thus results in shortage of water of satisfactory quality for its intended use as far as we are aware there are no global data sets for water quality however based on information on water use such as population size presence of agriculture and industry combined with generic estimations of pollution loads per unit and type of water use pollution loads can be estimated 5 conclusions in this paper we presented a new method to quickly construct water resources models based solely on open data and which is suitable for the evaluation of alternative interventions in land and water management especially in situations where decision making is required to maintain or reestablish cooperation stability and peace this type of model presents an intermediate level between coarse global models and detailed models based on local data this modelling level can provide results within shorter timeframes then required for local models yet has the possibility to assess alternative interventions something which is not possible with global models as such it can help decision makers in quickly deciding whether their involvement or a change in their involvement in an area at risk would be desirable and could be effective the time aspect is important in situations of imminent water risks moreover the use of data with near global coverage and which are available online allows for replication of the method in other areas it also allows for replication by others for the same area adding to transparency and a level playing field something that is important in situations where water related interventions could cause or exacerbate conflicts we applied this approach to the upper niger basin and evaluated the resulting model as satisfactory by considering not only traditional performance metrics but also fit for purpose indicators with which we evaluate whether the rapidly constructed model would not lead to a different decision than a higher performing model we find that the models scores satisfactory on both types of indicators even though the model performance in terms of hydrological model performance indicators is not as good as for dedicated models constructed using local data we therefore conclude that models based on global open datasets may offer a suitable approach for rapid assessments where time is of the essence or high quality local data is unavailable different basins and situations will all have their own challenges in terms of societal water use and water related risks and in terms of physical basin characteristics we promote replicating the approach in different basins to draw lessons regarding performance and fitness for purpose under different circumstances to further develop knowledge and tools and to know under which circumstances these methods have their limitations and requires complementation with additional locally measured data software availability name of software wflow developer deltares contact information wflow deltares nl year first available 2014 required hardware and software 64 bit windows python or linux python program language python 3 6 program size 2 5 gb conda environment availability and cost wflow is released as free open source code under the gnu general public license version 3 https github com openstreams wflow free license for executable name of software ribasim river basin simulation model developer deltares contact information wil nm van der krogt deltares boussinesqweg 1 2629 hv delft the netherlands email wil vanderkrogt deltares nl year first available 1985 required hardware and software no specific hardware or software is required a normal windows pc will due program language fortran vbasic c program size ribasim version 7 01 22 covers various components of different size computational core component is 6 65 mbytes availability and cost the software is free for educational and research purposes like for msc and phd students but a fee has to be paid for commercial use declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research for this paper was done in the framework of the water peace and security wps partnership supported by the netherlands ministry of foreign affairs grant number 4000003751 the specific component focusing on the use of global data for rapid water allocation model development which is the topic of this paper received additional support from deltares research program on water and societal stability we thank two anonymous reviewers for their constructive comments which helped to improve our manuscript 
