index,text
26265,the emplacement of lava flows is mainly controlled by the topography which is an essential input parameter to all numerical simulation models the sensitivity of these models to the uncertainties in topographic data and the consequent error propagation may have non negligible effects on the accuracy and reliability of the modelling of the lava flow hazard quantified analysis of the model sensitivity is thus of primary importance in constraining the uncertainty in the prospected lava inundation hazard with implications for civil protection purposes we assess the impact of errors uncertainties and level of detail in topographic data on the lava flow emplacement modelled by the magflow cellular automaton we show that the most influential external factors are rheology and vent geolocation and that increasing the automaton resolution improves the accuracy of simulations while making them more sensitive to perturbations in topographic data keywords sensitivity analysis digital elevation model numerical simulations scenario forecasting lava flow hazards msc 49q12 90c31 93b35 1 introduction lava flows are complex systems whose evolution is controlled by a variety of properties such as temperature chemical composition water content degree of crystallization and driven by factors such as the rheology of the fluid and the topography over which the fluid flows these properties and factors interact following complex and typically nonlinear physical laws the knowledge of which has seen a continuous progress the advances in our understanding of the processes underlying this phenomenon allow the development of more accurate physical models and conversely these models help improve our understanding of the lava flow emplacement process as more sophisticated models become available they can be included in scenario forecasting tools for the assessment of lava flow hazard the use of accurate physical models increases the reliability of the scenarios predicted by such forecasting tools vicari et al 2011b ganci et al 2012b on the other hand these models require more input parameters measured or estimated from field observations or other means e g remote sensing errors and uncertainty in our knowledge of these parameters reduce the confidence of the application of such tools presenting the need for theoretical and empirical constraints on the parameter values as well as on the effect of these uncertainties on the employed models kereszturi et al 2014 sargent 2011 this sensitivity analysis complements the validation of the numerical models for lava flow simulation stimulated by recent work such as cordonnier et al 2015 and dietterich et al 2017 an essential input parameter needed by numerical models for the simulation and prediction of lava flow emplacement regardless of the complexity of the model itself is the topography sparks and aspinall 2013 topography has an obvious impact on lava flow emplacement because gravity is the dominant force to drive lava flow paths as a consequence vent geolocation is important especially in complex terrain consider for example a vent or fracture that opens near a ridge the side of the ridge it opens on will drastically alter the direction of the flow in such circumstances even a relatively small geolocation error can lead to a complete mismatch between simulated and actual lava flows tarquini and favalli 2013 this is important to take into account considering the low vent geolocation precision during the early phases of an eruption when the only available data may come from remote sensing ganci et al 2012b harris et al 2016 aside from such extreme cases topography can still have a noticeable influence miyamoto and papp 2004 turner et al 2017 for example by presenting obstacles that a flow may or may not be able to overpass depending on the circumstances e g fluidity of the lava viscous model etc and the study of its influence is an important part of hazard assessment stevens et al 2003 stefanescu et al 2012a model validation stefanescu et al 2012b and spatial modelling in general heuvelink et al 1989 here we present a methodology to assess the impact of level of detail uncertainties and errors in the topographic data on the simulation of lava flow emplacements using the magflow physical mathematical model vicari et al 2007 del negro et al 2008 as an example application in this sense our work extends the sensitivity analysis on the rheological parameters of the model presented in bilotta et al 2012 and will thus rely on the same reference data i e the 2006 mt etna eruption fig 1 developed at the laboratory for the technological advance in volcano geophysics tecnolab at the catania section italy of the istituto nazionale di geofisica e vulcanologia ingv magflow is a physics based numerical model for the spatial and temporal evolution of lava flow fields based on the cellular automaton paradigm magflow has been successfully used on mt etna to reproduce several lava flow paths vicari et al 2011b ganci et al 2012b del negro et al 2016 and for lava flow hazard assessment cappello et al 2011a b del negro et al 2013 more recently it has been used to gain insights on the emplacement mechanism of past lava flows in the auckland volcanic field kereszturi et al 2014 to explore lava flow hazards in the azores archipelago cappello et al 2015b pedrazzi et al 2015 on the miyakejima volcano cappello et al 2015a and during the 2014 2015 fogo eruption cappello et al 2016a in order to speed up lava flow simulations the latest version of magflow has been implemented on graphic processing units gpus using cuda compute unified device architecture a parallel computing architecture provided by nvidia for the deployment of the latest generation of gpus as high performance parallel computing hardware bilotta et al 2011 cappello et al 2016b thanks to its fast execution magflow on gpu has become the core of the satellite driven modelling strategy lav hazard for the production of real time eruptive scenarios and the forecasting of lava flow hazards vicari et al 2011a ganci et al 2012b like most computational models for flow emplacement magflow relies on topographic information provided in the form of a digital elevation model dem a georeferenced rectangular grid of cells describing the average terrain height at regularly spaced points dems can be produced from a number of sources such as orthophotos lidar scans satellite imagery ganci et al 2018 and with a number of methods such as digital cartography sar interferometry and structure from motion baldi et al 2002 tarquini et al 2007 bagnardi et al 2016 but ultimately they represent an approximation of reality with different levels of detail and affected by a number of errors and uncertainties such as geolocation errors systematic errors in the data acquisition approximations due to averaging and interpolation and missing incomplete or out of date source data while quantifying the extent of the uncertainty weng 2002 wechsler and kroll 2006 in the topographic data is an essential step in the evaluation of its influence in the model output our work focuses on the model itself and specifically on how such uncertainty however determined impacts the results of the simulation specifically our main objective is to quantify the direct influence of precision i e dem resolution or level of detail and accuracy both vertical and horizontal of topographic data on the model output indirect influence will also be considered through an analysis of internal model parameters automaton resolution and discretized vent width we prefer a deterministic rather than a stochastic approach to the sensitivity analysis in bilotta et al 2012 this was achieved by applying polynomial chaos expansion to compute appropriate sobol indices an application of the same approach to the topography is impractical because there is no set of parameters which can describe the uncertainty of the topographic data in a deterministic manner 1 1 it would be possible to describe it e g in terms of the height of each individual dem cell but this is not computationally feasible requiring a different approach to the problem in the following we will first present the methodology we have used to prepare topographic data with deterministically measurable uncertainties we then assess their impact on the flow emplacements produced by magflow and finally we discuss the results in the context of hazard assessment and scenario forecasting the two main areas of application of the magflow model 2 the magflow model in this section we present the key aspects of the magflow model that are relevant to our sensitivity analysis a more detailed description can be found in cappello et al 2016b in magflow the computational domain of a simulation is chosen large enough to include the prospected maximum extent of the lava flow emplacement topographic data for the region is provided in the form of a dem a rectangular matrix of cells with values representing ground height the magflow cellular automaton for the simulation is generated such that a cell of the automaton corresponds to one dem cell the ground elevation provided by the dem the thickness of flowing lava the amount of solidified lava the heat quantity and temperature are the five properties that define the status of each cell at each iteration the evolution of the system is purely local in the sense that each cell evolves according to its present status and the status of its moore neighborhood i e its eight immediate neighbors lava thickness varies according to lava influx from the vent for cells corresponding to a vent location plus any lava flux between neighboring cells cross cell lava flux is derived from a steady state solution for the navier stokes equations for a fluid flowing on an inclined plane with bingham rheology dragoni et al 1986 the approximation coming from the use of a steady state solution is justified by the high viscosity of lava flows and the choice of the time step that guarantees numerical stability and mass conservation hérault et al 2009 the rheological parameters are the yield strength s y and the plastic viscosity η the bingham flow condition requiring that shear stress is higher than the yield strength is modelled by introducing a critical thickness h c r and having flux between two adjacent cells only when h h c r with h being the lava thickness in the cell with greater total height the critical thickness h c r is computed from the yield strength and slope angle to account for both pressure and gravity driven flows cappello et al 2016b 1 h c r s y ρ g sin α h x cos α s y δ z 2 δ x 2 ρ g δ z δ h where ρ is the lava density α the slope angle of the inclined plane g the gravity acceleration δ z the overall solid height difference considering ground elevation and solid lava height and δ h the difference in lava thickness as computed at the cell with highest total height fig 2 this aspect is the one most directly influenced by perturbations in topographic data cells are considered at constant temperature no vertical temperature gradient and heat flux is computed from mass flux considering the temperature of the cell losing mass heat loss is only considered with radiation from the surface ignoring the effects of conduction to the ground and convection in the atmosphere solidification follows a simplified model that does not take into consideration phenomena such as plug or crust formation when the temperature of a cell drops below a given solidus temperature t s defined as the temperature below which lava stops flowing a corresponding fraction of the lava in it is converted to solid lava which is assumed to lie at the bottom of the cell thereby contributing to the total elevation of the cell but not to the amount of fluid that can move the model therefore takes as input 1 the rheological properties of lava expressed in terms of its eruption and solidification temperature and water content 2 the flux rate over time usually provided as time averaged discharge rate tadr between given sampling intervals depending on the availability of field observations or thermal satellite remote sensing imagery wright et al 2001 ganci et al 2012b a 3 the topography of the area of interest in the form of a dem 4 the location of the vent s or fracture s including vent or fracture width if available otherwise the dem cell width is taken as vent diameter or fracture width the numerical stability of the model was assessed in our previous sensitivity analysis bilotta et al 2012 this identified water content and solidus temperature as critical rheological parameters and showed that to obtain more accurate simulations it is better to have continuous monitoring of the effusion rates even if with moderate errors rather than sparse accurate measurements quantifying the sensitivity to the remaining two parameters topography and vent fracture geolocation is the topic of this paper 3 materials and methods level of detail uncertainty and errors in the topographic information can influence the simulated flow emplacement in multiple ways our aim is both to quantify the impact itself as well as to gain insight on which aspects of the uncertainty itself have a greater influence dem resolution has both a direct and a secondary impact since magflow uses the dem grid cells themselves in the cellular automaton we will therefore separate the influence of dem resolution due to the coarseness of the data from the influence due to the automaton cell size a purely numerical effect that can be compensated within the model itself in a similar vein the vent size may have an influence on the emplacement a single physical vent may activate one or more cells in the automaton with the instantaneous flux rate divided among them by default magflow will activate a single cell per vent but it s possible to specify a vent width which will lead to the activation of multiple cells surrounding the georeferenced position of the vent affecting the final emplacement finally vent geolocation errors as mentioned in the introduction can have a dramatic influence in critical regions e g near a ridge we will avoid such extreme cases and only study the influence in contexts where the geolocation error does not lead to a change of hydrological basin it should be remarked that the four issues are different in nature the first and last one concern specifically the topographic data itself while the other two are focused on the way the magflow model treats the data while this distinction is important particularly when considering how the methodology can be extended to other models in this work we will try to address all of them while still differentiating the impact of each of them on the emplacement 3 1 dem perturbation sensitivity analysis requires the choice of a reference against which perturbed results can be compared following the argument presented in bilotta et al 2012 in the context of the sensitivity analsysis of a physical mathematical model it is more appropriate to choose as reference a simulation produced by the same model rather than an actual lava flow this allows us to have complete control on the parameters and the output and avoids pitfalls related to our incomplete knowledge of the phenomenon itself or any bias that may be caused by the possible systematic discrepancies between simulated and real flows in order to account for the variations in the topographic data different approaches are possible if multiple dems are available for the same area a possibility is to compare the model output on each of them stefanescu et al 2012b monte carlo algorithms can also be used to produce random perturbations within given parameters selecting a reference dem and using either a numerical target or other available dems to control the amplitude of the perturbations stefanescu et al 2012a for our analysis we have instead decided to apply a deterministic approach that can be regarded as a combination of the two approaches described above we choose a higher resolution dem as reference and we derive multiple lower resolution dems for our analysis using a variety of deterministic strategies that model the typical sources of data uncertainty our choice for the reference topography is the 1 m dem of mt etna updated to 2005 gwinner et al 2006 that we downscaled to a 5 m resolution our area of interest is that of the 2006 mt etna eruption fig 1 details about this event and its simulation using magflow can be found in vicari et al 2009 and hérault et al 2009 the choice to work with a 5 m resolution as ground truth rather than the dem original datum is dictated by consistency with the previous sensitivity analysis bilotta et al 2012 and with most applications of the magflow model including the nowcasting done during the 2006 mt etna eruption an important aspect to be considered is also the computational cost of using higher resolutions for example the entire 2006 eruption can be simulated with magflow in a couple of minutes on a 5 m dem while the same simulation takes over 16 h on a 1 m dem preventing its use for real time forecasting the perturbed dems for the analysis are obtained from the reference topography at a 10 m resolution using a variety of classical selection schemes since a 10 m dem cell covers exactly a 2 2 square of 5 m dem cells fig 3 the perturbed dem value for each cell is obtained from the square of matching reference cells using one of the following methods nw ne se sw select the reference cell corresponding to the given corner min max pick the lowest resp highest of the values midpoint pick the average of the minimum and maximum median pick the median of the values this is typically the midpoint of the non extremal values unless one of the corresponding reference cells holds no data mean pick the arithmetic mean of the values since the perturbed dems created with these approaches have a different resolution from the reference dem we also produce refined versions by constant interpolation i e each of the 2 2 cells square of a refined dem maintains the value of the corresponding cell of the corresponding coarse dem while there are better choices to obtain higher resolution dems from coarser data this particular choice will allow us to analyze the impact of data resolution versus automaton cell size since a simulation running on these dem will have 10 m effective data resolution but 5 m cell size the distribution of perturbation for the height and slope is represented in fig 4 grouped by selection schemes with the corner selection schemes nw ne sw se in fig 4a and c and the statistical schemes min median mean midpoint max in fig 4b and d a more detailed statistical analysis of the vertical error between the refined perturbed dem and the reference one is given in table 1 we observe that the typical error is low computed as rmse median or mean plus standard deviation although the maximum error can be quite high over 50 m in the most extreme cases for comparison the displacement error between tracks for the original dem had a statistical mode of less than 30 cm gwinner et al 2006 unsurprisingly the least error is achieved using the median selection scheme while the minimum and maximum selection schemes provide the statistically worst results the process described in this section to produce perturbed dems both at lower and at reference resolutions can be extended to produce additional perturbed dems with even lower resolution and the corresponding refined dems including such dems in the full sensitivity analysis is however computationally prohibitive a separate reduced set of results can still provide significant insights on the effect of the level of detail and automaton resolution on the model sensitivity these results have been gathered and presented separately in appendix a 3 2 additional sources of error to model geolocation errors we assume that the information about the vent location relative to the dem reference system has an uncertainty of up to 100 m 10 cells at the lowest resolution larger uncertainties were not considered due to significant drop in overall consistency as explained in section 3 3 reached at 100 m in addition to the reference source vent location we thus consider 32 other possible locations shifting the position of the reference by 25 50 75 and 100 m in the 8 cardinal and sub cardinal directions n ne e se s sw w nw we use the term lag to describe the vent position shift in analogy with the shift in overlap used to compute variograms finally to study how the number of cells activated by magflow for the vent impacts the simulation we run each case on the 5 m dems with both a 5 m and a 10 m vent diameter we remark that even with an eruptive vent width of 10 m the 5 m dem cells activated in magflow for a refined w10 simulation may not correspond to the two by two cell square underlying a single cell in a 10 m dem in the corresponding coarse simulation as illustrated in fig 5 we thus name our simulations based on the dem and vent configurations as coarse simulations made on the perturbed 10 m dems refined w5 simulations made on the perturbed 5 m dems with a vent width of 5 m single cell refined w10 simulations made on the perturbed 5 m dems with a vent width of 10 m two by two cells square to match the vent size at the coarser resolution reference w5 simulations made on the reference 5 m dem with a vent width of 5 m single cell reference w10 simulations made on the reference 5 m dem with a vent width of 10 m two by two cells square to match the vent size at the coarser resolution 3 3 fitness and confidence indices the magflow sensitivity analysis presented in bilotta et al 2012 highlights that the rheological parameters used in magflow can have a significant influence on the emplacement this is expected and reflects the physical nature of the evolution function in magflow we can expect that these parameters will also influence the sensitivity of magflow to the topography which makes it necessary to differentiate our analysis based on the rheological properties of the lava we only consider variations in the rheological parameters to which the model is most sensitive water content which controls the lava fluidity and the solidus temperature which controls the temperature under which lava stops flowing for each combination of these parameters we thus have two reference emplacements one obtained using a 5 m vent width single cell vent henceforth reference w5 and the other using a 10 m vent width two by two square of vent cells henceforth reference w10 both using the reference dem simulations on the perturbed dems can then be compared against a reference emplacement to obtain a fitness index φ bilotta et al 2012 computed as the ratio of the intersection to the union of the areas of the two simulated emplacements a and b 2 φ a b a b a b the fitness index can be considered a worst case indicator of the accuracy and precision of the model this is explained in detail in appendix b together with some essential information about how the two terms relate to each other in the context of this sensitivity analysis due to the difficulty in describing topographic data uncertainty with a single scalar value it is not computationally feasible to calculate sensitivity indices as was done in bilotta et al 2012 a quantitative albeit less rigorous estimate of the sensitivity can still be obtained by looking both at the individual fitness indices and aggregate information about overall variation we therefore introduce a natural extension of the fitness index to multiple emplacements given a set a i i i of emplacements the overall consistency φ is computed as 3 φ a i i i i i a i i i a i where the denominator represents the total area of emplacement i e the area reached by at least one emplacement and the numerator is the area of the intersection of all emplacements the overall consistency index φ has two significant differences compared to the standard fitness index the first is that it can be computed without a reference simulation it can therefore be used to assess the overall sensitivity of the simulation to a specific parameter the other important difference is that the overall fitness index is a very strict measure of sensitivity as it only considers the area where all emplacements agree on a 100 simulations set a single disagreeing emplacement will exclude the corresponding area even though that same area is covered by the other 99 simulations a more general extension of the fitness index to multiple emplacement is thus the consistency confidence index ψ depending on a confidence parameter α 4 ψ a i i i α i i α i a i i i a i where the numerator indicates the area covered by at least a fraction α of the emplacements in general ψ a i i i α β means that a fraction β of the total area is reached by at least a fraction α of emplacements for example ψ a i i i 0 9 0 2 would mean that 20 of the total area is reached by at least 90 of the emplacements the consistency confidence index is always monotonically decreasing with a maximum ψ a i i i 0 1 and a minimum ψ a i i i 1 φ a i i i and it represents the fraction of total area that will be inundated by the given percentage of total simulations an example application is illustrated in fig 6 showing the inundation area colored by confidence when considering all simulations in the 100 m lag case on 5 m dems with rheological parameters h 2 o 0 2 wt t s 900 k with 9 refined dems 1 reference dem 9 possible vent positions reference and the 8 lagged vents and two possible vent widths 5 m and 10 m the total area is the area reached by at least 1 in 180 simulations only 12 of the total area is reached by all 180 simulations the consistency confidence index for this set of simulations is plotted in fig 7 showing a rapid decrease in the initial and final phases 0 10 and 90 100 4 results 4 1 summary of scenarios for the analysis the results of our analysis will be grouped in different categories providing information about different components in the model sensitivity without taking into account geolocation error all dem perturbations will be considered for both coarse and refined simulations compared between them and against the reference simulations following the scheme presented in table 2 to gather information about the influence of level of detail and automaton resolution from the comparison of both coarse and refined simulations against lagged counterparts with the vents shifted by 25 m 100 m the influence of geolocation will also be quantified we vary the water content 0 01 0 02 0 05 0 1 0 2 wt and solidus temperature 800 900 1000 1100 k to determine how rheology impacts sensitivity producing a total of 29 33 20 19140 runs 3 runs coarse refined w5 refined w10 for each of the 9 perturbed dems plus 2 reference w5 reference w10 simulations on the reference dem 8 additional simulations for each of the four lags 8 4 plus 1 to take into account the no lag simulations each of the combinations above for 5 water content values and 4 solidus temperature values we first present statistics about the fitness indices computed on individual simulations over perturbed versus reference dems for each set of rheological parameters this provides a general overview of the variation in fitness based on individual parameters and dem perturbations section 4 2 following this we will show the trend of the overall consistency index taking into account the entire range of perturbations and compare this with the confidence index plots this will provide both lower bounds on the model sensitivity and a better appreciation of its variability section 4 3 these first two sets of results will not take into account the influence of lag and are thus obtained assuming that the vent is known at sub pixel accuracy in the dem s coordinate system the results are then complemented by a separate analysis of the influence of lag on the consistency indices section 4 4 4 2 single fitness statistics for each perturbed dem considering the coarse refined w5 and refined w10 simulation we can compute six fitness indices coarse to reference w5 coarse to reference w10 coarse to the corresponding refined w5 coarse to the corresponding refined w10 refined w5 to reference w5 refined w10 to reference w10 the first two fitness indices include changes that occur as a function of both topographic input data and model parameters i e the automaton cell size while the last two fitness indices refined versus reference indicate the proportion of the change that occurs due to different input topographies only and the remaining two indices coarse versus refined describe the proportion of the fitness that is due to the model parameters i e the automaton cell size only statistics on the fitness change across perturbed dems 9 realizations for each data point are summarized in tables b1 and b2 using respectively 5 m and 10 m vents in the high resolution simulations in fig 8 we plot the median values only separately for 5 m left and 10 m right vent widths the highlight of the results is that the fitness variation is relatively small the worst fitness values are mostly above 60 with the exception of φ 0 58 for h 2 o 0 2 wt t s 1100 k in the coarse vs refined w5 case and the best values are above 80 for the coarse simulations and manage to achieve 100 in a few cases with the refined dems we also observe that the refined simulations have a consistently better fit against the reference ones compared to the coarse ones this indicates that the automaton cell size has a larger impact on the simulation than the topographic data even when using the otherwise suboptimal piecewise constant interpolant this is further confirmed by supplementary tests at even lower resolutions appendix a while the rheological parameters affect the fitness there seems to be no obvious monotonic relationship between the fluidity of the lava and the fitness range gap between minimum and maximum fitness for a given rheology covered by the dem perturbations even though the most fluid h 2 o 0 2 wt lava generally do have a wider fitness range than the least fluid h 2 o 0 02 wt the maximum range is often found for intermediate values of h 2 o the results in tables b1 and b2 also show that vent width has a larger impact at higher resolutions when comparing the coarse simulations with their refined counterparts we see that the fitness range maximum minus minimum values for the 5 m vent case goes from a minimum of 0 03 h 2 o 0 02 wt t s 1100 k to a maximum of 0 27 h 2 o 0 2 wt t s 800 k whereas in the 10 m vent cases we go from a minimum of 0 04 h 2 o 0 01 wt t s 1100 k to a maximum of 0 22 h 2 o 0 05 wt t s 800 k by contrast in the refined versus reference comparisons the fitness ranges are instead slightly worse in the 10 m vent case 4 3 overall consistency and confidence indices from the same set of simulations we can also extract aggregate variability information using the overall consistency index presented in equation 3 this index provides a single index value that considers the overlap of all simulations across perturbed dems for each combination of the two rheological parameters water content and solidus temperature we thus obtain three overall consistency values intersecting all the simulations done respectively on the coarse dems and the refined dems with 10 m and 5 m vents the results are presented in fig 9 we observe that coarse simulations are more consistent in their emplacement this is shown both by the fact that coarser simulations have a higher overall consistency and that the faster decrease in consistency is only observed for the lowest viscosity and solidification temperature fig 9 top conversely on the refined dems fig 9 middle and bottom the overall consistency is lower and decreases much faster with the decrease in viscosity and solidification temperature dropping by as much as 60 in the worst cases there is also a much clearer dependency on the rheology particularly in the higher resolution cases although the relationship is still not monotonic the worst consistency values are achieved as expected for more fluid lava although solidification temperature appears to have a higher impact than water content in most cases we observe that the overall consistency results differ from the minimum values of the individual fitness presented previously the overall consistency reaches values as low as 40 fig 9 while the minimum individual fitness value is barely below 60 table b1 this is due to the different meaning that the two indices have for the individual fitness φ we are looking at the worst possible agreement between a single simulation and the reference emplacement the overall consistency values φ instead represent the fraction of total emplacement on which all perturbed simulations agree going from the overall consistency 100 confidence to the 90 confidence index ψ 0 9 already shows a sharp increase in consistency for most rheologies as illustrated in fig 10 with the exception of the cases where the overall consistency is already high e g most coarse simulations have φ 80 considering the 90 confidence leads to an increase in consistency of about 10 and brings the overall results more in line with the statistics for the individual fitness values we remark that while the confidence index provides more complete information the qualitative analysis of the results compared to the overall consistency remains substantially unchanged for example from the overall consistency analysis it was already apparent that lower resolution simulations were less sensitive to dem perturbations having higher overall consistency per rheology than the corresponding 5 m simulations this is clearly visible from fig 10 too with the exception of the most fluid lava flows the confidence index is indeed consistently above 80 in the coarse case while it presents much sharper decreases on the 5 m dems for most rheologies additionally fig 10 confirms the stronger significance of the solidification temperature over the water content for the variation in emplacement and the small difference in behavior for different vent widths 4 4 influence of lag for geolocation errors and uncertainties we only present the overall consistency data plotted in fig 11 for growing values of lag from 25 m to 100 m in 25 m steps our maximum value was chosen based on major changes in slope as illustrated in fig 6 the 100 m simulations are already affected by the lava plateau against the mt centenari formed by the 2004 2005 mt etna eruption del negro et al 2016 which starts deviating some of the flows towards the south instead of the preferential eastward direction of the other simulations this reflects in the very low values reached by the overall consistency and indicates that further lagging of the vent versus dem geolocation would not provide additional information while the quantitative results presented here are specific to the test case being used we can make some qualitative considerations that are of general application in particular the influence of lag is directly related to the presence of major convex morphological features such as the lava plateau in the reference dem in regions without such obstacles the influence of lag would be significantly diminished as illustrated by the comparable values in overall consistency for the 25 m and 50 m lags in our case fig 11 indeed this could potentially be used as a technique to determine which morphological features have the highest impact on lava flow emplacement when comparing the plots for the coarse and high resolution simulations we observe again that the consistency decrease is lower in the coarse case additionally for the higher resolution perturbed dems there is little difference in overall consistency for 5 m versus 10 m vent width indicating that the influence of the vent location is dominant indeed the plots for the two choices of vent widths are barely discernible 5 discussion the main takeaway of our results is that the level of detail of the topographic data has limited influence on the simulation results compared to the influence of other aspects such as the numerical resolution of the automaton or the geolocation error increasing the automaton resolution i e reducing the cell size has two apparently competing effects on the one hand it improves the accuracy of the simulation tables b1 and b2 on the other hand the simulations are more sensitive to perturbations in the topography figs 9 and 10 it should be noted however that both of these effects derive from the improved physical accuracy of the automaton as the resolution increases and reflect the nature of the phenomenon therefore the lower sensitivity of the model at lower resolution is not necessarily a positive thing the automaton resolution has a non trivial impact on performance therefore the optimal resolution should strike a balance between faster computation and loss of overall accuracy as illustrated from the results in appendix a an automaton cell size between 5 m and 10 m is a good compromise for magflow since the drop in accuracy becomes significant for larger cell sizes and computational times become prohibitive at higher resolutions this suggests that in applications to short term hazard forecasting the faster lower resolution simulations can be used to provide useful scenarios in near real time computationally more expensive higher resolution simulations can provide more accurate longer term results in the latter case to take into account the higher sensitivity it may be appropriate to consider multiple sets of simulations with dem perturbations these can then be used to further improve the accuracy of the results by providing a probabilistic assessment of lava inundation hazard all the results indicate that the effect of the vent width is quite marginal while small differences can be observed in all cases from the individual fitness values per dem perturbation to the overall consistency values with and without geolocation lag they do not affect the results in any meaningful way among the elements analyzed in this work the most influential factor that emerges is the geolocation of the vent with respect to the dem even with the smallest lag considered in our tests 25 m fig 11 upper left an additional drop in overall consistency of about 20 can be observed compared to the case of matching geolocation fig 9 with a higher lag the overall consistency decreases further as major topographic features rather than the perturbations we considered start influencing the flow paths in fact it could be argued that the influence of the geolocation error isn t controlled directly by the error per se but rather indirectly from the large scale topographic features convexity of the location presence of large obstacles etc this has already been remarked in vicari et al 2011b where the absence of the post 2005 lava flows from the topographic data affected the emplacement simulated by magflow for the 2011 mt etna eruption 6 conclusions and future work we introduced a new methodology to analyze the sensitivity of numerical models to topographic data and applied it to the magflow model for lava flow simulations the analysis involves three key aspects in the topographic data i e uncertainty in the data vertical accuracy level of detail data resolution and geolocation accuracy and as such can be applied to a wide variety of models for the simulation of any gravity driven flow see e g stefanescu et al 2012a in the specific case of magflow the model presents different sensitivity to each of the key aspects which can be ranked as geolocation accuracy most sensitive level of detail and vertical accuracy least sensitive the model sensitivity depends on the rheological parameters and increases with higher water content and or lower solidification temperature we have shown that even if the original topographic data has low resolution cell size of 20 m or more simply reducing the automaton cell size is sufficient to provide more accurate simulations these results were obtained with a very simple interpolation method to achieve the higher resolution dems and may be further improved combined with an analysis of dem interpolation methods arun 2013 kalimuthu et al 2016 to find the most appropriate method which in this context may be model specific given the relevance of lag between dem and vent geolocation lava flow hazard forecasting can take advantage of the results of our analysis by considering the possible uncertainty in the vent location within the dem reference system the computational cost of the additional simulations needed to take lag into account is usually affordable since the accuracy of vent geolocation is lowest in the earliest phases of an eruption when hazard assessment needs to simulate scenarios with shorter durations the uncertainty in the output associated to geolocation can easily be incorporated in the model output by producing an inundation probability map such as the one illustrated in fig 6 an indirect discovery of the analysis is also in the assessment of the importance of the availability of updated topographic data on an active volcano such as mt etna flow emplacements from eruptions that have occurred since the last update of the dem may reduce the accuracy of subsequent scenarios due to the formation of new topographic features these results align with similar findings for both a ā miyamoto and papp 2004 and pahoehoe lava turner et al 2017 a strategy for more frequent updates to reference dem data is thus recommended even at lower resolutions since our analysis indicates that magflow is able to produce accurate results with a high automaton resolution even with lower resolution dem data provided that no artifacts are introduced aside from the specific results for magflow the presented methodology is a powerful tool of wide and general applicability in the field of sensitivity analysis for computational fluid dynamics and hazard forecasting a field of growing importance with the growing complexity of the models as they follow and lead in our advancement of knowledge of the phenomena involved as more physical laws are incorporated into the models such sensitivity analysis cannot disregard how the physical laws themselves are sensitive to the topographical data and its resolution for our future work we intend to conduct a sensitivity analysis of the coupled navier stokes and thermal equations in the context of lava flows and to apply this to the gpusph particle engine bilotta et al 2016 that incorporates a higher degree of physics additional improvements to the methodology itself are also possible particularly in the selection of the perturbed dems as the choices we present can be considered somewhat arbitrary and depend on the assumption that they are equally probable better selections may be possible relying on a more sound statistical model while still retaining the deterministic nature of the methodology e g using low discrepancy sequences and quasi monte carlo methods acknowledgments this work was developed within the framework of athos research programme organized by ingv catania italy and johns hopkins university baltimore usa and was partially financially supported from the dpc ingv 2016 b2 contract the authors wish to thank the editor dr d p ames and nicole richter and two anonymous reviewers for their detailed and extensive suggestions on how to improve the initial version of this paper appendix a a additional statistics for lower levels of detail to study the behavior of the magflow model in case of low level of detail in topographic data and with large automaton cell sizes the process applied in section 3 1 to create 10 m dems from 5 m dems can be extended to produce dems with an even lower level of detail given a reference 5 m dem a 20 m dem can be generated either by producing a 10 m dem with any of the suggested methods and then applying any of them a second time with 81 possible combinations or directly by applying similar selection schemes but considering that each 20 m cell now corresponds to a square of 4 4 high resolution 5 m cells resulting in 16 5 possible selection criteria one for each high resolution to low resolution cell position plus the min max median midpoint and mean schemes overall this gives 102 different methods some of which will however produce the same low resolution data for example applying the min selection scheme to produce an intermediate 10 m dem and then again to produce a 20 m dem will produce the same result as applying the min selection scheme to produce the 20 m dem directly even discarding the methods that produce identical results the number of perturbed low resolution dems grows exponentially as the resolution is lowered since at least 9 n distinct dems can be produced when halving the resolution n times these would then have to be doubled to produce the corresponding refined dems and the consequent explosion in the number of possible topographies on which to run simulations increases the computational cost of a full fledged sensitivity analysis with more than two levels of detail beyond the point of practicality despite these limitations we can still study how lower levels of detail affect the model behavior by reducing the set of lower resolution dems to this end we have produced lower resolution dems from our reference 5 m dem at 20 m and 40 m resolutions using the midpoint selection scheme applied directly to the 4 4 and 8 8 chunks of reference cells contained within each lower resolution cell refined 5 m dems were also produced by constant interpolation in order to differentiate between the influence of the level of detail given by the fitness index between the refined and reference simulations from the influence of the automaton resolution given by the fitness index between the coarse and refined simulations as discussed in section 4 2 due to the smaller number of simulations produced in this set of tests we can gather additional information about the influence of resolution on the results by looking beyond the fitness index and comparing some morphological aspects of the flows inundated area computed as the number of inundated cells multiplied by the cell area determined by the dem resolution median thickness and the principal flow length approximated by taking the length of the diagonal of the bounding rectangle statistics on the relative variations for area median thickness and length across all levels of detail and rheological parameters are gathered in table a1 the relative variations have been computed for each combination of rheological parameters as a 1 1 value reference where value is the one obtained in the simulation at a given level of detail and automaton cell size and reference is the value obtained in the reference simulation we separate the refined cases automaton cell size kept at 5 m for all levels of detail in topographic data from the coarse cases automaton cell size matching dem cell size to better highlight the influence of the automaton resolution so that all statistics are calculated from 20 realizations one per combination of rheological parameter table a 1 overall statistics for the relative variations in area median thickness and length across all levels of detail and rheological parameters table a 1 min median mean max variation variation variation variation area refined 2 7 11 12 30 area coarse 0 92 8 3 7 8 17 thickness refined 1 0 7 6 8 9 22 thickness coarse 1 2 7 3 9 9 25 length refined 1 4 13 12 21 length coarse 36 47 46 52 additionally to show the trend of the morphological properties across level of detail again separately for the coarse and refined case we plot the results for rheological parameters h 2 o 0 2 wt t s 900 k in figures a 1 and a 2 other combinations of rheological parameters exhibit qualitatively similar behavior we observe that the represented morphological characteristics behave in a significantly different manner the inundated area and median flow thickness have mostly small fluctuations in both the coarse and refined cases the median relative errors is around or below 10 on the other hand flow length shows a much more significant difference in behavior between the refined and coarse cases with the latter showing a distinct drop in flow length best 36 median 46 worst 52 which is much less evident in the refined case best 1 4 median 13 worst 21 even with the dem refinement being done using the suboptimal constant interpolation method the main takeaway of these results is that automaton resolution has a much larger impact than dem level of detail and that better results can be achieved even with lower resolution dems by using a higher resolution automaton fig a 1 inundated area and median flow thickness for coarse and refined resolutions at progressively lower levels of detail 5 m to 40 m with rheological parameters h 2 o 0 2 wt t s 900 k fig a 2 flow length for coarse and refined resolutions at progressively lower levels of detail 5 m to 40 m with rheological parameters h 2 o 0 2 wt t s 900 k appendix b b fitness accuracy and precision when comparing a test and a reference emplacement as typically done in sensitivity analysis the areas inundated by the two emplacements can be classified depending on their respective intersection borrowing the terms from statistical classification we can group the cells in four classes tp true positive i e the cells inundated both in the reference and test emplacement tn true negative i e the cells not inundated in the reference nor in the test emplacement fp false positive i e the cells inundated in the test emplacement but not in the reference emplacement fn false negative i e the cells inundated in the reference emplacement but not in the test emplacement we remark that for lava flow emplacements tn is not well defined as it depends on the dem section taken into consideration additionally we have that tp is the area of the intersection of the test and reference emplacements tp fp is the area of the test emplacement tp fn the area of the reference emplacement and tp fp fn is the area of the union of the emplacements from the classified areas we can compute multiple indices to determine the robustness of the model and in particular the accuracy acc as b 1 acc tp tn tp fp fn tn the precision also known as the positive predictive value ppv as b 2 ppv tp tp fp and the true positive rate tpr frequently called the sensitivity in the medical field a term which we avoid in this paper as it would generate confusion as b 3 tpr tp tp fn we observe that the fitness value we use throughout the paper is the accuracy acc in the case tn 0 since tn is non negative the case tn 0 is the one giving the lowest value for acc in this sense our fitness value can be considered a worst case accuracy moreover since fp and fn are also non negative we always have that ppv acc and tpr acc therefore the fitness value is always the worst lowest of the indicators a comparison between the ppv tpr and acc indices can give some additional insights on how the test emplacements change compared to the reference specifically precision is higher than accuracy when the false negative i e underestimate area is higher and conversely the tpr is higher than the accuracy when the false positive i e overestimate area is higher specifically ppv acc 1 gives the ratio of false negative to test emplacement area whereas tpr acc 1 gives the ratio of false positive to reference emplacement area these indicators are not named in related literature so we will name them test relative underestimation tru and reference relative overestimation rro respectively in appendix b 1 we will present and discuss their values in relation to the sensitivity analysis of the magflow model to topographic data uncertainty b 1 fitness and precision statistics to present the qualitative behavior of the fitness index equation 2 due to topographic data uncertainty we show the minimum maximum mean and median fitness achieved across perturbed dems for each vent width we tabulate tables b1 and b2 the statistics when comparing the simulation done on the coarse dem to the simulation done on the reference dem the coarse one to the refined one and finally the refined to the reference in order to differentiate the influence of automaton cell size and dem resolution the results are discussed in sections 4 2 and 5 table b 1 statistics of the standard fitness indices across all perturbed dems for each combination of rheological parameters with 5 m vents at higher resolution highlighted in orange are the worst fitness values highlighted in green the best fitness values table b 1 table b 2 statistics of the standard fitness indices across all perturbed dems for each combination of rheological parameters with 10 m vents at higher resolution highlighted in orange are the worst fitness values highlighted in green the best fitness values table b 2 additionally we present table b 3 and discuss more synthetic statistics about the rro and tru indices introduced in appendix b table b 3 range for the reference relative overestimate rro and test relative underestimate tru indices across all perturbed dems and combination of rheological parameters grouped by vent width used at higher resolution table b 3 coarse vs reference coarse vs refined refined vs reference index vent width min max mean median min max mean median min max mean median rro 5 m 0 04 0 32 0 18 0 21 0 0 48 0 23 0 26 0 0 12 0 032 0 029 rro 10 m 0 04 0 31 0 18 0 2 0 04 0 31 0 15 0 13 0 001 0 17 0 071 0 061 tru 5 m 0 047 0 31 0 14 0 13 0 0024 0 2 0 1 0 1 0 0 18 0 037 0 023 tru 10 m 0 11 0 35 0 18 0 17 0 0088 0 29 0 17 0 18 0 0 14 0 034 0 023 the results are consistent with the behavior of the fitness index automaton resolution is the most important factor leading to larger rro and tru values in the coarse versus higher resolution cases than in the refined versus reference comparisons in contrast to the fitness index vent width seems to have a larger impact in this case particularly for rro for the coarse simulations median rro is over 10 but less than 30 indicating that overestimation will typically be less than one third of the target emplacement an interesting result is that the worst overestimate is found in the coarse to refined comparison for 5 m vents this corresponds to the h 2 o 0 2 wt t s 1100 k simulation that produces the lowest overall consistency the median tru in coarse simulations is less than the median rro with a lower bound of 10 but an upper bound of less than 20 indicating that underestimation will typically be less than one fifth of the simulated emplacement even though this can grow to over 30 in the worst cases at higher automaton resolutions refined vs reference columns we observe much smaller rro and tru values with medians less than 10 and worst cases less than 20 matching the much higher fitness observed in these cases again the median tru is lower than the median rro indicating that simulations underestimate less than they overestimate a result that may have interesting implications in applications such as hazard assessment and scenario forecasting under the assumption that rheological parameters were correctly identified 
26265,the emplacement of lava flows is mainly controlled by the topography which is an essential input parameter to all numerical simulation models the sensitivity of these models to the uncertainties in topographic data and the consequent error propagation may have non negligible effects on the accuracy and reliability of the modelling of the lava flow hazard quantified analysis of the model sensitivity is thus of primary importance in constraining the uncertainty in the prospected lava inundation hazard with implications for civil protection purposes we assess the impact of errors uncertainties and level of detail in topographic data on the lava flow emplacement modelled by the magflow cellular automaton we show that the most influential external factors are rheology and vent geolocation and that increasing the automaton resolution improves the accuracy of simulations while making them more sensitive to perturbations in topographic data keywords sensitivity analysis digital elevation model numerical simulations scenario forecasting lava flow hazards msc 49q12 90c31 93b35 1 introduction lava flows are complex systems whose evolution is controlled by a variety of properties such as temperature chemical composition water content degree of crystallization and driven by factors such as the rheology of the fluid and the topography over which the fluid flows these properties and factors interact following complex and typically nonlinear physical laws the knowledge of which has seen a continuous progress the advances in our understanding of the processes underlying this phenomenon allow the development of more accurate physical models and conversely these models help improve our understanding of the lava flow emplacement process as more sophisticated models become available they can be included in scenario forecasting tools for the assessment of lava flow hazard the use of accurate physical models increases the reliability of the scenarios predicted by such forecasting tools vicari et al 2011b ganci et al 2012b on the other hand these models require more input parameters measured or estimated from field observations or other means e g remote sensing errors and uncertainty in our knowledge of these parameters reduce the confidence of the application of such tools presenting the need for theoretical and empirical constraints on the parameter values as well as on the effect of these uncertainties on the employed models kereszturi et al 2014 sargent 2011 this sensitivity analysis complements the validation of the numerical models for lava flow simulation stimulated by recent work such as cordonnier et al 2015 and dietterich et al 2017 an essential input parameter needed by numerical models for the simulation and prediction of lava flow emplacement regardless of the complexity of the model itself is the topography sparks and aspinall 2013 topography has an obvious impact on lava flow emplacement because gravity is the dominant force to drive lava flow paths as a consequence vent geolocation is important especially in complex terrain consider for example a vent or fracture that opens near a ridge the side of the ridge it opens on will drastically alter the direction of the flow in such circumstances even a relatively small geolocation error can lead to a complete mismatch between simulated and actual lava flows tarquini and favalli 2013 this is important to take into account considering the low vent geolocation precision during the early phases of an eruption when the only available data may come from remote sensing ganci et al 2012b harris et al 2016 aside from such extreme cases topography can still have a noticeable influence miyamoto and papp 2004 turner et al 2017 for example by presenting obstacles that a flow may or may not be able to overpass depending on the circumstances e g fluidity of the lava viscous model etc and the study of its influence is an important part of hazard assessment stevens et al 2003 stefanescu et al 2012a model validation stefanescu et al 2012b and spatial modelling in general heuvelink et al 1989 here we present a methodology to assess the impact of level of detail uncertainties and errors in the topographic data on the simulation of lava flow emplacements using the magflow physical mathematical model vicari et al 2007 del negro et al 2008 as an example application in this sense our work extends the sensitivity analysis on the rheological parameters of the model presented in bilotta et al 2012 and will thus rely on the same reference data i e the 2006 mt etna eruption fig 1 developed at the laboratory for the technological advance in volcano geophysics tecnolab at the catania section italy of the istituto nazionale di geofisica e vulcanologia ingv magflow is a physics based numerical model for the spatial and temporal evolution of lava flow fields based on the cellular automaton paradigm magflow has been successfully used on mt etna to reproduce several lava flow paths vicari et al 2011b ganci et al 2012b del negro et al 2016 and for lava flow hazard assessment cappello et al 2011a b del negro et al 2013 more recently it has been used to gain insights on the emplacement mechanism of past lava flows in the auckland volcanic field kereszturi et al 2014 to explore lava flow hazards in the azores archipelago cappello et al 2015b pedrazzi et al 2015 on the miyakejima volcano cappello et al 2015a and during the 2014 2015 fogo eruption cappello et al 2016a in order to speed up lava flow simulations the latest version of magflow has been implemented on graphic processing units gpus using cuda compute unified device architecture a parallel computing architecture provided by nvidia for the deployment of the latest generation of gpus as high performance parallel computing hardware bilotta et al 2011 cappello et al 2016b thanks to its fast execution magflow on gpu has become the core of the satellite driven modelling strategy lav hazard for the production of real time eruptive scenarios and the forecasting of lava flow hazards vicari et al 2011a ganci et al 2012b like most computational models for flow emplacement magflow relies on topographic information provided in the form of a digital elevation model dem a georeferenced rectangular grid of cells describing the average terrain height at regularly spaced points dems can be produced from a number of sources such as orthophotos lidar scans satellite imagery ganci et al 2018 and with a number of methods such as digital cartography sar interferometry and structure from motion baldi et al 2002 tarquini et al 2007 bagnardi et al 2016 but ultimately they represent an approximation of reality with different levels of detail and affected by a number of errors and uncertainties such as geolocation errors systematic errors in the data acquisition approximations due to averaging and interpolation and missing incomplete or out of date source data while quantifying the extent of the uncertainty weng 2002 wechsler and kroll 2006 in the topographic data is an essential step in the evaluation of its influence in the model output our work focuses on the model itself and specifically on how such uncertainty however determined impacts the results of the simulation specifically our main objective is to quantify the direct influence of precision i e dem resolution or level of detail and accuracy both vertical and horizontal of topographic data on the model output indirect influence will also be considered through an analysis of internal model parameters automaton resolution and discretized vent width we prefer a deterministic rather than a stochastic approach to the sensitivity analysis in bilotta et al 2012 this was achieved by applying polynomial chaos expansion to compute appropriate sobol indices an application of the same approach to the topography is impractical because there is no set of parameters which can describe the uncertainty of the topographic data in a deterministic manner 1 1 it would be possible to describe it e g in terms of the height of each individual dem cell but this is not computationally feasible requiring a different approach to the problem in the following we will first present the methodology we have used to prepare topographic data with deterministically measurable uncertainties we then assess their impact on the flow emplacements produced by magflow and finally we discuss the results in the context of hazard assessment and scenario forecasting the two main areas of application of the magflow model 2 the magflow model in this section we present the key aspects of the magflow model that are relevant to our sensitivity analysis a more detailed description can be found in cappello et al 2016b in magflow the computational domain of a simulation is chosen large enough to include the prospected maximum extent of the lava flow emplacement topographic data for the region is provided in the form of a dem a rectangular matrix of cells with values representing ground height the magflow cellular automaton for the simulation is generated such that a cell of the automaton corresponds to one dem cell the ground elevation provided by the dem the thickness of flowing lava the amount of solidified lava the heat quantity and temperature are the five properties that define the status of each cell at each iteration the evolution of the system is purely local in the sense that each cell evolves according to its present status and the status of its moore neighborhood i e its eight immediate neighbors lava thickness varies according to lava influx from the vent for cells corresponding to a vent location plus any lava flux between neighboring cells cross cell lava flux is derived from a steady state solution for the navier stokes equations for a fluid flowing on an inclined plane with bingham rheology dragoni et al 1986 the approximation coming from the use of a steady state solution is justified by the high viscosity of lava flows and the choice of the time step that guarantees numerical stability and mass conservation hérault et al 2009 the rheological parameters are the yield strength s y and the plastic viscosity η the bingham flow condition requiring that shear stress is higher than the yield strength is modelled by introducing a critical thickness h c r and having flux between two adjacent cells only when h h c r with h being the lava thickness in the cell with greater total height the critical thickness h c r is computed from the yield strength and slope angle to account for both pressure and gravity driven flows cappello et al 2016b 1 h c r s y ρ g sin α h x cos α s y δ z 2 δ x 2 ρ g δ z δ h where ρ is the lava density α the slope angle of the inclined plane g the gravity acceleration δ z the overall solid height difference considering ground elevation and solid lava height and δ h the difference in lava thickness as computed at the cell with highest total height fig 2 this aspect is the one most directly influenced by perturbations in topographic data cells are considered at constant temperature no vertical temperature gradient and heat flux is computed from mass flux considering the temperature of the cell losing mass heat loss is only considered with radiation from the surface ignoring the effects of conduction to the ground and convection in the atmosphere solidification follows a simplified model that does not take into consideration phenomena such as plug or crust formation when the temperature of a cell drops below a given solidus temperature t s defined as the temperature below which lava stops flowing a corresponding fraction of the lava in it is converted to solid lava which is assumed to lie at the bottom of the cell thereby contributing to the total elevation of the cell but not to the amount of fluid that can move the model therefore takes as input 1 the rheological properties of lava expressed in terms of its eruption and solidification temperature and water content 2 the flux rate over time usually provided as time averaged discharge rate tadr between given sampling intervals depending on the availability of field observations or thermal satellite remote sensing imagery wright et al 2001 ganci et al 2012b a 3 the topography of the area of interest in the form of a dem 4 the location of the vent s or fracture s including vent or fracture width if available otherwise the dem cell width is taken as vent diameter or fracture width the numerical stability of the model was assessed in our previous sensitivity analysis bilotta et al 2012 this identified water content and solidus temperature as critical rheological parameters and showed that to obtain more accurate simulations it is better to have continuous monitoring of the effusion rates even if with moderate errors rather than sparse accurate measurements quantifying the sensitivity to the remaining two parameters topography and vent fracture geolocation is the topic of this paper 3 materials and methods level of detail uncertainty and errors in the topographic information can influence the simulated flow emplacement in multiple ways our aim is both to quantify the impact itself as well as to gain insight on which aspects of the uncertainty itself have a greater influence dem resolution has both a direct and a secondary impact since magflow uses the dem grid cells themselves in the cellular automaton we will therefore separate the influence of dem resolution due to the coarseness of the data from the influence due to the automaton cell size a purely numerical effect that can be compensated within the model itself in a similar vein the vent size may have an influence on the emplacement a single physical vent may activate one or more cells in the automaton with the instantaneous flux rate divided among them by default magflow will activate a single cell per vent but it s possible to specify a vent width which will lead to the activation of multiple cells surrounding the georeferenced position of the vent affecting the final emplacement finally vent geolocation errors as mentioned in the introduction can have a dramatic influence in critical regions e g near a ridge we will avoid such extreme cases and only study the influence in contexts where the geolocation error does not lead to a change of hydrological basin it should be remarked that the four issues are different in nature the first and last one concern specifically the topographic data itself while the other two are focused on the way the magflow model treats the data while this distinction is important particularly when considering how the methodology can be extended to other models in this work we will try to address all of them while still differentiating the impact of each of them on the emplacement 3 1 dem perturbation sensitivity analysis requires the choice of a reference against which perturbed results can be compared following the argument presented in bilotta et al 2012 in the context of the sensitivity analsysis of a physical mathematical model it is more appropriate to choose as reference a simulation produced by the same model rather than an actual lava flow this allows us to have complete control on the parameters and the output and avoids pitfalls related to our incomplete knowledge of the phenomenon itself or any bias that may be caused by the possible systematic discrepancies between simulated and real flows in order to account for the variations in the topographic data different approaches are possible if multiple dems are available for the same area a possibility is to compare the model output on each of them stefanescu et al 2012b monte carlo algorithms can also be used to produce random perturbations within given parameters selecting a reference dem and using either a numerical target or other available dems to control the amplitude of the perturbations stefanescu et al 2012a for our analysis we have instead decided to apply a deterministic approach that can be regarded as a combination of the two approaches described above we choose a higher resolution dem as reference and we derive multiple lower resolution dems for our analysis using a variety of deterministic strategies that model the typical sources of data uncertainty our choice for the reference topography is the 1 m dem of mt etna updated to 2005 gwinner et al 2006 that we downscaled to a 5 m resolution our area of interest is that of the 2006 mt etna eruption fig 1 details about this event and its simulation using magflow can be found in vicari et al 2009 and hérault et al 2009 the choice to work with a 5 m resolution as ground truth rather than the dem original datum is dictated by consistency with the previous sensitivity analysis bilotta et al 2012 and with most applications of the magflow model including the nowcasting done during the 2006 mt etna eruption an important aspect to be considered is also the computational cost of using higher resolutions for example the entire 2006 eruption can be simulated with magflow in a couple of minutes on a 5 m dem while the same simulation takes over 16 h on a 1 m dem preventing its use for real time forecasting the perturbed dems for the analysis are obtained from the reference topography at a 10 m resolution using a variety of classical selection schemes since a 10 m dem cell covers exactly a 2 2 square of 5 m dem cells fig 3 the perturbed dem value for each cell is obtained from the square of matching reference cells using one of the following methods nw ne se sw select the reference cell corresponding to the given corner min max pick the lowest resp highest of the values midpoint pick the average of the minimum and maximum median pick the median of the values this is typically the midpoint of the non extremal values unless one of the corresponding reference cells holds no data mean pick the arithmetic mean of the values since the perturbed dems created with these approaches have a different resolution from the reference dem we also produce refined versions by constant interpolation i e each of the 2 2 cells square of a refined dem maintains the value of the corresponding cell of the corresponding coarse dem while there are better choices to obtain higher resolution dems from coarser data this particular choice will allow us to analyze the impact of data resolution versus automaton cell size since a simulation running on these dem will have 10 m effective data resolution but 5 m cell size the distribution of perturbation for the height and slope is represented in fig 4 grouped by selection schemes with the corner selection schemes nw ne sw se in fig 4a and c and the statistical schemes min median mean midpoint max in fig 4b and d a more detailed statistical analysis of the vertical error between the refined perturbed dem and the reference one is given in table 1 we observe that the typical error is low computed as rmse median or mean plus standard deviation although the maximum error can be quite high over 50 m in the most extreme cases for comparison the displacement error between tracks for the original dem had a statistical mode of less than 30 cm gwinner et al 2006 unsurprisingly the least error is achieved using the median selection scheme while the minimum and maximum selection schemes provide the statistically worst results the process described in this section to produce perturbed dems both at lower and at reference resolutions can be extended to produce additional perturbed dems with even lower resolution and the corresponding refined dems including such dems in the full sensitivity analysis is however computationally prohibitive a separate reduced set of results can still provide significant insights on the effect of the level of detail and automaton resolution on the model sensitivity these results have been gathered and presented separately in appendix a 3 2 additional sources of error to model geolocation errors we assume that the information about the vent location relative to the dem reference system has an uncertainty of up to 100 m 10 cells at the lowest resolution larger uncertainties were not considered due to significant drop in overall consistency as explained in section 3 3 reached at 100 m in addition to the reference source vent location we thus consider 32 other possible locations shifting the position of the reference by 25 50 75 and 100 m in the 8 cardinal and sub cardinal directions n ne e se s sw w nw we use the term lag to describe the vent position shift in analogy with the shift in overlap used to compute variograms finally to study how the number of cells activated by magflow for the vent impacts the simulation we run each case on the 5 m dems with both a 5 m and a 10 m vent diameter we remark that even with an eruptive vent width of 10 m the 5 m dem cells activated in magflow for a refined w10 simulation may not correspond to the two by two cell square underlying a single cell in a 10 m dem in the corresponding coarse simulation as illustrated in fig 5 we thus name our simulations based on the dem and vent configurations as coarse simulations made on the perturbed 10 m dems refined w5 simulations made on the perturbed 5 m dems with a vent width of 5 m single cell refined w10 simulations made on the perturbed 5 m dems with a vent width of 10 m two by two cells square to match the vent size at the coarser resolution reference w5 simulations made on the reference 5 m dem with a vent width of 5 m single cell reference w10 simulations made on the reference 5 m dem with a vent width of 10 m two by two cells square to match the vent size at the coarser resolution 3 3 fitness and confidence indices the magflow sensitivity analysis presented in bilotta et al 2012 highlights that the rheological parameters used in magflow can have a significant influence on the emplacement this is expected and reflects the physical nature of the evolution function in magflow we can expect that these parameters will also influence the sensitivity of magflow to the topography which makes it necessary to differentiate our analysis based on the rheological properties of the lava we only consider variations in the rheological parameters to which the model is most sensitive water content which controls the lava fluidity and the solidus temperature which controls the temperature under which lava stops flowing for each combination of these parameters we thus have two reference emplacements one obtained using a 5 m vent width single cell vent henceforth reference w5 and the other using a 10 m vent width two by two square of vent cells henceforth reference w10 both using the reference dem simulations on the perturbed dems can then be compared against a reference emplacement to obtain a fitness index φ bilotta et al 2012 computed as the ratio of the intersection to the union of the areas of the two simulated emplacements a and b 2 φ a b a b a b the fitness index can be considered a worst case indicator of the accuracy and precision of the model this is explained in detail in appendix b together with some essential information about how the two terms relate to each other in the context of this sensitivity analysis due to the difficulty in describing topographic data uncertainty with a single scalar value it is not computationally feasible to calculate sensitivity indices as was done in bilotta et al 2012 a quantitative albeit less rigorous estimate of the sensitivity can still be obtained by looking both at the individual fitness indices and aggregate information about overall variation we therefore introduce a natural extension of the fitness index to multiple emplacements given a set a i i i of emplacements the overall consistency φ is computed as 3 φ a i i i i i a i i i a i where the denominator represents the total area of emplacement i e the area reached by at least one emplacement and the numerator is the area of the intersection of all emplacements the overall consistency index φ has two significant differences compared to the standard fitness index the first is that it can be computed without a reference simulation it can therefore be used to assess the overall sensitivity of the simulation to a specific parameter the other important difference is that the overall fitness index is a very strict measure of sensitivity as it only considers the area where all emplacements agree on a 100 simulations set a single disagreeing emplacement will exclude the corresponding area even though that same area is covered by the other 99 simulations a more general extension of the fitness index to multiple emplacement is thus the consistency confidence index ψ depending on a confidence parameter α 4 ψ a i i i α i i α i a i i i a i where the numerator indicates the area covered by at least a fraction α of the emplacements in general ψ a i i i α β means that a fraction β of the total area is reached by at least a fraction α of emplacements for example ψ a i i i 0 9 0 2 would mean that 20 of the total area is reached by at least 90 of the emplacements the consistency confidence index is always monotonically decreasing with a maximum ψ a i i i 0 1 and a minimum ψ a i i i 1 φ a i i i and it represents the fraction of total area that will be inundated by the given percentage of total simulations an example application is illustrated in fig 6 showing the inundation area colored by confidence when considering all simulations in the 100 m lag case on 5 m dems with rheological parameters h 2 o 0 2 wt t s 900 k with 9 refined dems 1 reference dem 9 possible vent positions reference and the 8 lagged vents and two possible vent widths 5 m and 10 m the total area is the area reached by at least 1 in 180 simulations only 12 of the total area is reached by all 180 simulations the consistency confidence index for this set of simulations is plotted in fig 7 showing a rapid decrease in the initial and final phases 0 10 and 90 100 4 results 4 1 summary of scenarios for the analysis the results of our analysis will be grouped in different categories providing information about different components in the model sensitivity without taking into account geolocation error all dem perturbations will be considered for both coarse and refined simulations compared between them and against the reference simulations following the scheme presented in table 2 to gather information about the influence of level of detail and automaton resolution from the comparison of both coarse and refined simulations against lagged counterparts with the vents shifted by 25 m 100 m the influence of geolocation will also be quantified we vary the water content 0 01 0 02 0 05 0 1 0 2 wt and solidus temperature 800 900 1000 1100 k to determine how rheology impacts sensitivity producing a total of 29 33 20 19140 runs 3 runs coarse refined w5 refined w10 for each of the 9 perturbed dems plus 2 reference w5 reference w10 simulations on the reference dem 8 additional simulations for each of the four lags 8 4 plus 1 to take into account the no lag simulations each of the combinations above for 5 water content values and 4 solidus temperature values we first present statistics about the fitness indices computed on individual simulations over perturbed versus reference dems for each set of rheological parameters this provides a general overview of the variation in fitness based on individual parameters and dem perturbations section 4 2 following this we will show the trend of the overall consistency index taking into account the entire range of perturbations and compare this with the confidence index plots this will provide both lower bounds on the model sensitivity and a better appreciation of its variability section 4 3 these first two sets of results will not take into account the influence of lag and are thus obtained assuming that the vent is known at sub pixel accuracy in the dem s coordinate system the results are then complemented by a separate analysis of the influence of lag on the consistency indices section 4 4 4 2 single fitness statistics for each perturbed dem considering the coarse refined w5 and refined w10 simulation we can compute six fitness indices coarse to reference w5 coarse to reference w10 coarse to the corresponding refined w5 coarse to the corresponding refined w10 refined w5 to reference w5 refined w10 to reference w10 the first two fitness indices include changes that occur as a function of both topographic input data and model parameters i e the automaton cell size while the last two fitness indices refined versus reference indicate the proportion of the change that occurs due to different input topographies only and the remaining two indices coarse versus refined describe the proportion of the fitness that is due to the model parameters i e the automaton cell size only statistics on the fitness change across perturbed dems 9 realizations for each data point are summarized in tables b1 and b2 using respectively 5 m and 10 m vents in the high resolution simulations in fig 8 we plot the median values only separately for 5 m left and 10 m right vent widths the highlight of the results is that the fitness variation is relatively small the worst fitness values are mostly above 60 with the exception of φ 0 58 for h 2 o 0 2 wt t s 1100 k in the coarse vs refined w5 case and the best values are above 80 for the coarse simulations and manage to achieve 100 in a few cases with the refined dems we also observe that the refined simulations have a consistently better fit against the reference ones compared to the coarse ones this indicates that the automaton cell size has a larger impact on the simulation than the topographic data even when using the otherwise suboptimal piecewise constant interpolant this is further confirmed by supplementary tests at even lower resolutions appendix a while the rheological parameters affect the fitness there seems to be no obvious monotonic relationship between the fluidity of the lava and the fitness range gap between minimum and maximum fitness for a given rheology covered by the dem perturbations even though the most fluid h 2 o 0 2 wt lava generally do have a wider fitness range than the least fluid h 2 o 0 02 wt the maximum range is often found for intermediate values of h 2 o the results in tables b1 and b2 also show that vent width has a larger impact at higher resolutions when comparing the coarse simulations with their refined counterparts we see that the fitness range maximum minus minimum values for the 5 m vent case goes from a minimum of 0 03 h 2 o 0 02 wt t s 1100 k to a maximum of 0 27 h 2 o 0 2 wt t s 800 k whereas in the 10 m vent cases we go from a minimum of 0 04 h 2 o 0 01 wt t s 1100 k to a maximum of 0 22 h 2 o 0 05 wt t s 800 k by contrast in the refined versus reference comparisons the fitness ranges are instead slightly worse in the 10 m vent case 4 3 overall consistency and confidence indices from the same set of simulations we can also extract aggregate variability information using the overall consistency index presented in equation 3 this index provides a single index value that considers the overlap of all simulations across perturbed dems for each combination of the two rheological parameters water content and solidus temperature we thus obtain three overall consistency values intersecting all the simulations done respectively on the coarse dems and the refined dems with 10 m and 5 m vents the results are presented in fig 9 we observe that coarse simulations are more consistent in their emplacement this is shown both by the fact that coarser simulations have a higher overall consistency and that the faster decrease in consistency is only observed for the lowest viscosity and solidification temperature fig 9 top conversely on the refined dems fig 9 middle and bottom the overall consistency is lower and decreases much faster with the decrease in viscosity and solidification temperature dropping by as much as 60 in the worst cases there is also a much clearer dependency on the rheology particularly in the higher resolution cases although the relationship is still not monotonic the worst consistency values are achieved as expected for more fluid lava although solidification temperature appears to have a higher impact than water content in most cases we observe that the overall consistency results differ from the minimum values of the individual fitness presented previously the overall consistency reaches values as low as 40 fig 9 while the minimum individual fitness value is barely below 60 table b1 this is due to the different meaning that the two indices have for the individual fitness φ we are looking at the worst possible agreement between a single simulation and the reference emplacement the overall consistency values φ instead represent the fraction of total emplacement on which all perturbed simulations agree going from the overall consistency 100 confidence to the 90 confidence index ψ 0 9 already shows a sharp increase in consistency for most rheologies as illustrated in fig 10 with the exception of the cases where the overall consistency is already high e g most coarse simulations have φ 80 considering the 90 confidence leads to an increase in consistency of about 10 and brings the overall results more in line with the statistics for the individual fitness values we remark that while the confidence index provides more complete information the qualitative analysis of the results compared to the overall consistency remains substantially unchanged for example from the overall consistency analysis it was already apparent that lower resolution simulations were less sensitive to dem perturbations having higher overall consistency per rheology than the corresponding 5 m simulations this is clearly visible from fig 10 too with the exception of the most fluid lava flows the confidence index is indeed consistently above 80 in the coarse case while it presents much sharper decreases on the 5 m dems for most rheologies additionally fig 10 confirms the stronger significance of the solidification temperature over the water content for the variation in emplacement and the small difference in behavior for different vent widths 4 4 influence of lag for geolocation errors and uncertainties we only present the overall consistency data plotted in fig 11 for growing values of lag from 25 m to 100 m in 25 m steps our maximum value was chosen based on major changes in slope as illustrated in fig 6 the 100 m simulations are already affected by the lava plateau against the mt centenari formed by the 2004 2005 mt etna eruption del negro et al 2016 which starts deviating some of the flows towards the south instead of the preferential eastward direction of the other simulations this reflects in the very low values reached by the overall consistency and indicates that further lagging of the vent versus dem geolocation would not provide additional information while the quantitative results presented here are specific to the test case being used we can make some qualitative considerations that are of general application in particular the influence of lag is directly related to the presence of major convex morphological features such as the lava plateau in the reference dem in regions without such obstacles the influence of lag would be significantly diminished as illustrated by the comparable values in overall consistency for the 25 m and 50 m lags in our case fig 11 indeed this could potentially be used as a technique to determine which morphological features have the highest impact on lava flow emplacement when comparing the plots for the coarse and high resolution simulations we observe again that the consistency decrease is lower in the coarse case additionally for the higher resolution perturbed dems there is little difference in overall consistency for 5 m versus 10 m vent width indicating that the influence of the vent location is dominant indeed the plots for the two choices of vent widths are barely discernible 5 discussion the main takeaway of our results is that the level of detail of the topographic data has limited influence on the simulation results compared to the influence of other aspects such as the numerical resolution of the automaton or the geolocation error increasing the automaton resolution i e reducing the cell size has two apparently competing effects on the one hand it improves the accuracy of the simulation tables b1 and b2 on the other hand the simulations are more sensitive to perturbations in the topography figs 9 and 10 it should be noted however that both of these effects derive from the improved physical accuracy of the automaton as the resolution increases and reflect the nature of the phenomenon therefore the lower sensitivity of the model at lower resolution is not necessarily a positive thing the automaton resolution has a non trivial impact on performance therefore the optimal resolution should strike a balance between faster computation and loss of overall accuracy as illustrated from the results in appendix a an automaton cell size between 5 m and 10 m is a good compromise for magflow since the drop in accuracy becomes significant for larger cell sizes and computational times become prohibitive at higher resolutions this suggests that in applications to short term hazard forecasting the faster lower resolution simulations can be used to provide useful scenarios in near real time computationally more expensive higher resolution simulations can provide more accurate longer term results in the latter case to take into account the higher sensitivity it may be appropriate to consider multiple sets of simulations with dem perturbations these can then be used to further improve the accuracy of the results by providing a probabilistic assessment of lava inundation hazard all the results indicate that the effect of the vent width is quite marginal while small differences can be observed in all cases from the individual fitness values per dem perturbation to the overall consistency values with and without geolocation lag they do not affect the results in any meaningful way among the elements analyzed in this work the most influential factor that emerges is the geolocation of the vent with respect to the dem even with the smallest lag considered in our tests 25 m fig 11 upper left an additional drop in overall consistency of about 20 can be observed compared to the case of matching geolocation fig 9 with a higher lag the overall consistency decreases further as major topographic features rather than the perturbations we considered start influencing the flow paths in fact it could be argued that the influence of the geolocation error isn t controlled directly by the error per se but rather indirectly from the large scale topographic features convexity of the location presence of large obstacles etc this has already been remarked in vicari et al 2011b where the absence of the post 2005 lava flows from the topographic data affected the emplacement simulated by magflow for the 2011 mt etna eruption 6 conclusions and future work we introduced a new methodology to analyze the sensitivity of numerical models to topographic data and applied it to the magflow model for lava flow simulations the analysis involves three key aspects in the topographic data i e uncertainty in the data vertical accuracy level of detail data resolution and geolocation accuracy and as such can be applied to a wide variety of models for the simulation of any gravity driven flow see e g stefanescu et al 2012a in the specific case of magflow the model presents different sensitivity to each of the key aspects which can be ranked as geolocation accuracy most sensitive level of detail and vertical accuracy least sensitive the model sensitivity depends on the rheological parameters and increases with higher water content and or lower solidification temperature we have shown that even if the original topographic data has low resolution cell size of 20 m or more simply reducing the automaton cell size is sufficient to provide more accurate simulations these results were obtained with a very simple interpolation method to achieve the higher resolution dems and may be further improved combined with an analysis of dem interpolation methods arun 2013 kalimuthu et al 2016 to find the most appropriate method which in this context may be model specific given the relevance of lag between dem and vent geolocation lava flow hazard forecasting can take advantage of the results of our analysis by considering the possible uncertainty in the vent location within the dem reference system the computational cost of the additional simulations needed to take lag into account is usually affordable since the accuracy of vent geolocation is lowest in the earliest phases of an eruption when hazard assessment needs to simulate scenarios with shorter durations the uncertainty in the output associated to geolocation can easily be incorporated in the model output by producing an inundation probability map such as the one illustrated in fig 6 an indirect discovery of the analysis is also in the assessment of the importance of the availability of updated topographic data on an active volcano such as mt etna flow emplacements from eruptions that have occurred since the last update of the dem may reduce the accuracy of subsequent scenarios due to the formation of new topographic features these results align with similar findings for both a ā miyamoto and papp 2004 and pahoehoe lava turner et al 2017 a strategy for more frequent updates to reference dem data is thus recommended even at lower resolutions since our analysis indicates that magflow is able to produce accurate results with a high automaton resolution even with lower resolution dem data provided that no artifacts are introduced aside from the specific results for magflow the presented methodology is a powerful tool of wide and general applicability in the field of sensitivity analysis for computational fluid dynamics and hazard forecasting a field of growing importance with the growing complexity of the models as they follow and lead in our advancement of knowledge of the phenomena involved as more physical laws are incorporated into the models such sensitivity analysis cannot disregard how the physical laws themselves are sensitive to the topographical data and its resolution for our future work we intend to conduct a sensitivity analysis of the coupled navier stokes and thermal equations in the context of lava flows and to apply this to the gpusph particle engine bilotta et al 2016 that incorporates a higher degree of physics additional improvements to the methodology itself are also possible particularly in the selection of the perturbed dems as the choices we present can be considered somewhat arbitrary and depend on the assumption that they are equally probable better selections may be possible relying on a more sound statistical model while still retaining the deterministic nature of the methodology e g using low discrepancy sequences and quasi monte carlo methods acknowledgments this work was developed within the framework of athos research programme organized by ingv catania italy and johns hopkins university baltimore usa and was partially financially supported from the dpc ingv 2016 b2 contract the authors wish to thank the editor dr d p ames and nicole richter and two anonymous reviewers for their detailed and extensive suggestions on how to improve the initial version of this paper appendix a a additional statistics for lower levels of detail to study the behavior of the magflow model in case of low level of detail in topographic data and with large automaton cell sizes the process applied in section 3 1 to create 10 m dems from 5 m dems can be extended to produce dems with an even lower level of detail given a reference 5 m dem a 20 m dem can be generated either by producing a 10 m dem with any of the suggested methods and then applying any of them a second time with 81 possible combinations or directly by applying similar selection schemes but considering that each 20 m cell now corresponds to a square of 4 4 high resolution 5 m cells resulting in 16 5 possible selection criteria one for each high resolution to low resolution cell position plus the min max median midpoint and mean schemes overall this gives 102 different methods some of which will however produce the same low resolution data for example applying the min selection scheme to produce an intermediate 10 m dem and then again to produce a 20 m dem will produce the same result as applying the min selection scheme to produce the 20 m dem directly even discarding the methods that produce identical results the number of perturbed low resolution dems grows exponentially as the resolution is lowered since at least 9 n distinct dems can be produced when halving the resolution n times these would then have to be doubled to produce the corresponding refined dems and the consequent explosion in the number of possible topographies on which to run simulations increases the computational cost of a full fledged sensitivity analysis with more than two levels of detail beyond the point of practicality despite these limitations we can still study how lower levels of detail affect the model behavior by reducing the set of lower resolution dems to this end we have produced lower resolution dems from our reference 5 m dem at 20 m and 40 m resolutions using the midpoint selection scheme applied directly to the 4 4 and 8 8 chunks of reference cells contained within each lower resolution cell refined 5 m dems were also produced by constant interpolation in order to differentiate between the influence of the level of detail given by the fitness index between the refined and reference simulations from the influence of the automaton resolution given by the fitness index between the coarse and refined simulations as discussed in section 4 2 due to the smaller number of simulations produced in this set of tests we can gather additional information about the influence of resolution on the results by looking beyond the fitness index and comparing some morphological aspects of the flows inundated area computed as the number of inundated cells multiplied by the cell area determined by the dem resolution median thickness and the principal flow length approximated by taking the length of the diagonal of the bounding rectangle statistics on the relative variations for area median thickness and length across all levels of detail and rheological parameters are gathered in table a1 the relative variations have been computed for each combination of rheological parameters as a 1 1 value reference where value is the one obtained in the simulation at a given level of detail and automaton cell size and reference is the value obtained in the reference simulation we separate the refined cases automaton cell size kept at 5 m for all levels of detail in topographic data from the coarse cases automaton cell size matching dem cell size to better highlight the influence of the automaton resolution so that all statistics are calculated from 20 realizations one per combination of rheological parameter table a 1 overall statistics for the relative variations in area median thickness and length across all levels of detail and rheological parameters table a 1 min median mean max variation variation variation variation area refined 2 7 11 12 30 area coarse 0 92 8 3 7 8 17 thickness refined 1 0 7 6 8 9 22 thickness coarse 1 2 7 3 9 9 25 length refined 1 4 13 12 21 length coarse 36 47 46 52 additionally to show the trend of the morphological properties across level of detail again separately for the coarse and refined case we plot the results for rheological parameters h 2 o 0 2 wt t s 900 k in figures a 1 and a 2 other combinations of rheological parameters exhibit qualitatively similar behavior we observe that the represented morphological characteristics behave in a significantly different manner the inundated area and median flow thickness have mostly small fluctuations in both the coarse and refined cases the median relative errors is around or below 10 on the other hand flow length shows a much more significant difference in behavior between the refined and coarse cases with the latter showing a distinct drop in flow length best 36 median 46 worst 52 which is much less evident in the refined case best 1 4 median 13 worst 21 even with the dem refinement being done using the suboptimal constant interpolation method the main takeaway of these results is that automaton resolution has a much larger impact than dem level of detail and that better results can be achieved even with lower resolution dems by using a higher resolution automaton fig a 1 inundated area and median flow thickness for coarse and refined resolutions at progressively lower levels of detail 5 m to 40 m with rheological parameters h 2 o 0 2 wt t s 900 k fig a 2 flow length for coarse and refined resolutions at progressively lower levels of detail 5 m to 40 m with rheological parameters h 2 o 0 2 wt t s 900 k appendix b b fitness accuracy and precision when comparing a test and a reference emplacement as typically done in sensitivity analysis the areas inundated by the two emplacements can be classified depending on their respective intersection borrowing the terms from statistical classification we can group the cells in four classes tp true positive i e the cells inundated both in the reference and test emplacement tn true negative i e the cells not inundated in the reference nor in the test emplacement fp false positive i e the cells inundated in the test emplacement but not in the reference emplacement fn false negative i e the cells inundated in the reference emplacement but not in the test emplacement we remark that for lava flow emplacements tn is not well defined as it depends on the dem section taken into consideration additionally we have that tp is the area of the intersection of the test and reference emplacements tp fp is the area of the test emplacement tp fn the area of the reference emplacement and tp fp fn is the area of the union of the emplacements from the classified areas we can compute multiple indices to determine the robustness of the model and in particular the accuracy acc as b 1 acc tp tn tp fp fn tn the precision also known as the positive predictive value ppv as b 2 ppv tp tp fp and the true positive rate tpr frequently called the sensitivity in the medical field a term which we avoid in this paper as it would generate confusion as b 3 tpr tp tp fn we observe that the fitness value we use throughout the paper is the accuracy acc in the case tn 0 since tn is non negative the case tn 0 is the one giving the lowest value for acc in this sense our fitness value can be considered a worst case accuracy moreover since fp and fn are also non negative we always have that ppv acc and tpr acc therefore the fitness value is always the worst lowest of the indicators a comparison between the ppv tpr and acc indices can give some additional insights on how the test emplacements change compared to the reference specifically precision is higher than accuracy when the false negative i e underestimate area is higher and conversely the tpr is higher than the accuracy when the false positive i e overestimate area is higher specifically ppv acc 1 gives the ratio of false negative to test emplacement area whereas tpr acc 1 gives the ratio of false positive to reference emplacement area these indicators are not named in related literature so we will name them test relative underestimation tru and reference relative overestimation rro respectively in appendix b 1 we will present and discuss their values in relation to the sensitivity analysis of the magflow model to topographic data uncertainty b 1 fitness and precision statistics to present the qualitative behavior of the fitness index equation 2 due to topographic data uncertainty we show the minimum maximum mean and median fitness achieved across perturbed dems for each vent width we tabulate tables b1 and b2 the statistics when comparing the simulation done on the coarse dem to the simulation done on the reference dem the coarse one to the refined one and finally the refined to the reference in order to differentiate the influence of automaton cell size and dem resolution the results are discussed in sections 4 2 and 5 table b 1 statistics of the standard fitness indices across all perturbed dems for each combination of rheological parameters with 5 m vents at higher resolution highlighted in orange are the worst fitness values highlighted in green the best fitness values table b 1 table b 2 statistics of the standard fitness indices across all perturbed dems for each combination of rheological parameters with 10 m vents at higher resolution highlighted in orange are the worst fitness values highlighted in green the best fitness values table b 2 additionally we present table b 3 and discuss more synthetic statistics about the rro and tru indices introduced in appendix b table b 3 range for the reference relative overestimate rro and test relative underestimate tru indices across all perturbed dems and combination of rheological parameters grouped by vent width used at higher resolution table b 3 coarse vs reference coarse vs refined refined vs reference index vent width min max mean median min max mean median min max mean median rro 5 m 0 04 0 32 0 18 0 21 0 0 48 0 23 0 26 0 0 12 0 032 0 029 rro 10 m 0 04 0 31 0 18 0 2 0 04 0 31 0 15 0 13 0 001 0 17 0 071 0 061 tru 5 m 0 047 0 31 0 14 0 13 0 0024 0 2 0 1 0 1 0 0 18 0 037 0 023 tru 10 m 0 11 0 35 0 18 0 17 0 0088 0 29 0 17 0 18 0 0 14 0 034 0 023 the results are consistent with the behavior of the fitness index automaton resolution is the most important factor leading to larger rro and tru values in the coarse versus higher resolution cases than in the refined versus reference comparisons in contrast to the fitness index vent width seems to have a larger impact in this case particularly for rro for the coarse simulations median rro is over 10 but less than 30 indicating that overestimation will typically be less than one third of the target emplacement an interesting result is that the worst overestimate is found in the coarse to refined comparison for 5 m vents this corresponds to the h 2 o 0 2 wt t s 1100 k simulation that produces the lowest overall consistency the median tru in coarse simulations is less than the median rro with a lower bound of 10 but an upper bound of less than 20 indicating that underestimation will typically be less than one fifth of the simulated emplacement even though this can grow to over 30 in the worst cases at higher automaton resolutions refined vs reference columns we observe much smaller rro and tru values with medians less than 10 and worst cases less than 20 matching the much higher fitness observed in these cases again the median tru is lower than the median rro indicating that simulations underestimate less than they overestimate a result that may have interesting implications in applications such as hazard assessment and scenario forecasting under the assumption that rheological parameters were correctly identified 
26266,although the system dynamics sd modeling approach has been used frequently to model various systems there is little research on the development of hybrid sd individual oriented io modeling the aim of this study is to demonstrate the use of an array variable for the hybrid sd io modeling approach using the sd platform and to provide a detailed modeling process of an algae daphnid system consisting of two freshwater algal species and daphnia our study showed that the array variable in the sd approach is useful for modeling multiple individuals and as a function to control the switch and integration variable for the hybrid modeling approach the proposed method can improve the usability of hybrid modeling by enabling hybrid sd io modeling without requiring extensive knowledge of programming given the ongoing development and advancement of the sd modeling approach this modeling technique will be a useful method to study complex systems keywords system dynamics individual oriented modeling individual based modeling dynamic energy budget array variable 1 introduction various modeling methods have been developed and applied to study ecosystems with multiple components that interact in complex ways system dynamics sd models agent based ab also referred in ecology as individual based ib models bayesian networks bns knowledge based models kbms and coupled component models ccms are the five types of models used to model complex systems and these models can handle multiple issues scales and complex interactions of ecosystems kelly et al 2013 among them sd and ib approaches have been widely used to improve our understanding of a system s behavior owing to advances in modeling software e g sd stella vensim and powersim ib netlogo and repast sd and ib modeling have not typically been combined due to their contrasting characteristics vincenot et al 2011 sd modeling is a typical deductive rule i e top down approach to describe the behavior of a system an sd model is constructed by the formulation of the system using ordinary differential equations and represented by stock flow and auxiliary inputs through the conceptualization of causal relationships and feedback loops kelly et al 2013 this modeling method offers a relatively effective way to extend the model structure from basic stock and flow models to complex system models by simplifying model representations and provisioning a graphical modeling interface vincenot et al 2011 on the other hand ib modeling describes the dynamics of the system by the sum of its parts from an inductive perspective i e bottom up approach in an ib model individuals can determine their behaviors such as interactions with the environment movements with a direction change and selection of resources by algorithmic rules therefore this modeling method has been widely used to study species distributions and population dynamics macal 2010 recently interest in hybrid sd ib models that combine the advantages of sd and ib modeling methods has increased to provide potentially useful tools particularly for modeling to understand more complex systems that contain components of various scales vincenot et al 2011 reviewed the literature regarding hybrid sd ib models applied to ecology and categorized the use of hybrid models in four ways table 1 since then swinerd and mcnaught 2012 have proposed three conceptual categories of hybrid sd ib simulations the integrated hybrid design the interfaced hybrid design and the sequential hybrid design in particular they classified the integrated hybrid design into three types according to the combination structure of sd and ib models table 1 these proposed conceptual categories provided useful guidelines for integrating the two different modeling methods recently wallentin neuwirth 2017 suggested six alternative designs of integrated hybrid models that dynamically switch between sd and ib models and discussed the information loss that occurs from switching between modeling methods these conceptual guides for the hybrid sd ib approach have helped improve the design and modeling of hybrid approaches however practical and detailed studies are limited and rudimentary wallentin and neuwirth 2017 also noted that despite the conceptual promise of hybrid sd ib modeling there are still few examples of such hybrid modeling in research hybrid sd ib modeling is not being widely used because many limitations exist in creating hybrid models for example to create a hybrid model both the sd platform and the ib platform as well as the middleware that link them should be used together alternatively hybrid modeling software suitable for this purpose should be developed using a programming language using hybrid modeling toolsets such as netlogo originally an ib modeling software with the capability for sd modeling and anylogic a typical hybrid modeling software may be an alternative however given that these platforms are software that support ib modeling as a core feature they have limitations in sophisticated sd modeling for example the former only supports the basic functionality for sd modeling wallentin and neuwirth 2017 the latter is one of the most widely used hybrid modeling software however it does not support detailed setting tools for high level sd modeling such as variable type e g complex logical real and integer integration order e g first order zero order and zero order immediate and series type e g first last accumulated and average in addition the hybrid modeling platform is relatively difficult to work with because of the integration of various modeling features the integration of sd and ib using a single platform can improve the accessibility and convenience for researchers in the development of hybrid models the sd platform has several advantages as the single platform for creating the hybrid sd ib model first the sd platform provides a convenient interface for researchers to model using an intuitive graphical language e g stocks and auxiliary variables are represented by squares and circles respectively allowing them to build complex systems without a high level proficiency in the programming language second sd platforms can make it easier to understand the feedback structures of the whole system lättilä et al 2010 previously kim juhn 1997 proposed the idea that sd can be integrated with ib using array variables other researchers who focused on the conceptual development of hybrid sd ib models have referred to their integration lättilä et al 2010 vincenot et al 2011 however its applications and development are still rare jo et al 2015 especially in the fields of ecology and environmental science the advantages of hybrid sd ib modeling approach based on the sd are obvious but several issues need to be solved such as the individual level modeling and the spatial representation in the sd model in this study a hybrid sd individual oriented io modeling method based on the sd approach was proposed that focused on integrating the sd and the individual level model conceptual frameworks and detailed modeling processes for integrating sd and io approach into the sd based model were demonstrated to explore population dynamics in a complex system to implement the proposed methods we used the algae daphnid system with three species and their interactions as a case study the algal population was modeled as a simple sd model and daphnids were modeled as stocked agents and agents with rich internal structure according to the integrated hybrid sd ib designs proposed by swinerd and mcnaught 2012 fig 1 this proposed hybrid modeling approach enables the modeling of complex systems using a single platform and can be the basis of the sd based hybrid sd ib approach 2 frameworks for hybrid sd io modeling in population dynamics 2 1 sd and ib models in population dynamics in ecology to date sd and ib approaches have been used individually to study the population dynamics of specific species bald et al 2006 2009 and weller et al 2014 showed that sd models can be used to simulate population dynamics and are useful for scenario simulation and management strategies additionally sd models have been used to predict the effects of various factors on population dynamics by combining age class models and simulate scenarios that include different management decisions or environmental conditions meanwhile ib model approaches have been focused on the reliable prediction of population dynamics through combinations of simulations of the responses of each individual recently ib models have increasingly been combined with the dynamic energy budget deb theory which provides a consistent framework and logic to explore the effects of environmental factors on vital rates of the species such as growth reproduction and survival bacher and gangnery 2006 johnston et al 2014 martin et al 2013 both sd and ib methods are useful tools and have their own advantages in studies of population modeling however in recent years interest in integrating the two methods has increased to explore population dynamics in natural ecosystems given that it is appropriate to represent individuals in a population using ib and to express environments or resources using sd the hybrid sd ib approach can effectively model a complex ecosystem rather than relying on only one approach to simulate one or the other vincenot et al 2011 previously akkermans 2001 proposed an individual oriented sd model that expressed 100 individuals as stocks of the sd approach however the method to represent all individuals as each stock is inefficient to apply to population dynamics because the population sizes can be very large or fluctuate over time the lack of further studies on population dynamics using hybrid modeling based on the sd approach might be due to these limitations the ib modeling approach on the other hand is relatively well suited to population dynamics studies generally ib modeling approaches have been preferred in complex situations such as the heterogeneity of individuals requirement of flexible model structure and presence of complex events lättilä et al 2010 because ib modeling can handle both discrete and continuous simulations schieritz 2002 in terms of the ib approach independent simulations for individual organisms are essential functions that are useful when each individual born at different points in time must be individually simulated in addition the model structure should reflect the changes in the number of individuals representing population dynamics over time however the structure of a typical sd model is fixed during the simulation time these significant drawbacks of the sd approach need to be improved to integrate ib models into the sd model 2 2 proposed hybrid sd io modeling using the sd approach to improve the inflexibility of sd modeling approach array variables and some additional model components are used in the model development process all sd model components such as stock auxiliary and flow can be used as scalar variables or array variables in an sd platform a scalar variable has a single value to represent the object as a single representative value whereas an array variable can have multiple values to represent a set of entities for example given that the scalar variable can represent only one individual the number of scalar variables equal to the total number of individuals are required to represent a population fig 2 a on the other hand the population can be represented by a single array variable because the array variable can have the number of values equal to the total number of individuals fig 2b by using an array variable all individuals within the population can be integrated into a single sd model structure in a simple and effective manner independent simulations for each individual can be made by using a switch variable which is a kind of array variable that controls the start of the simulation for each entity in other array variables the switch variable has the same number of values as the array variable being controlled for one to one correspondence fig 3 a at each time step each value of the switch variable has a numeric number or a logical value e g true or false that indicates the state of the corresponding individual on a one to one basis for example when the ith value of the switch variable is true or false it either starts or does not start the simulation for the ith individual of the array variable representing the population respectively this process allows independent entities of array variables to be simulated separately on the sd modeling approach in the hybrid sd io modeling process it may be necessary to link the array variables e g population to the scalar variables e g single components such as resource and environment the connection of these two types of variables can be performed using an integration variable which is a scalar variable that uses conditional functions typically if then else syntax or array functions e g the sum of array elements fig 3b the integration variable combines the array variable values into a single scalar variable value which is determined to be present according to the values of the switch variable for example when expressing resource consumption by the population only the values of the array variable corresponding to the values of the switch variables indicating survival are selected and are summed as a single value by applying this method a hybrid sd io model based on the sd modeling approach can be developed and simulated in summary array variables are used to represent groups containing multiple entities easily and the simulation time of all entities is controlled individually using switch variables more detailed model development processes and examples of use are described in the following sections 3 case study an algae daphnid system 3 1 overview a case simulation study of an algae daphnid system was conducted to demonstrate the suitability of the hybrid sd io model using the sd modeling approach in this case simulation study changes in the densities of two freshwater algal species pseudokirchneriella subcapitata also known as selenastrum capricornutum and raphidocelis subcapitata and chlorella vulgaris and their predators daphnia sp over time under various environmental conditions were investigated although this algae daphnid system is a simple system consisting of three species it was expected that each species would be affected by complex effects and interactions continuously for example the environmental conditions interspecific competition between the two algal species growth rates feeding rates of daphnia and density of each species not only affect the dynamics of the algae daphnid system independently or in combination but also affect each of the other species through feedback effects in this complex situation an sd approach is appropriate to express the dynamics of system structure with complicated feedback effects while an io approach is better at describing the independent behaviors of individuals through the modeling of rules jo et al 2015 therefore among the components of the algae daphnid system the dynamics of the algal density over time and the individual characteristics of daphnia e g growth and reproduction were modeled with sd and io methods respectively 3 2 the modeling process 3 2 1 density dynamic modeling for the freshwater algae a simple sd model the purpose of the algae model was to represent the density dynamics of two freshwater algal species p subcapitata and c vulgaris as with the study of wallentin and neuwirth 2017 the density changes in each algal population were described by the stocks of the sd model because the number of algae cells is too large to describe each cell as an individual of the io approach as c vulgaris is known to inhibit the growth of p subcapitata by producing allelochemicals during their growth fergola et al 2007 the algae model included an allelochemical component as well as the density of the two algal species according to fergola et al 2007 and yang et al 2011 the algal growth and allelochemical formation can be expressed as growth rate exp inhibition constant allelochemical concentration and formation coefficient algal population growth respectively thus the three ordinary differential equations that describe the time dependent algal density p subcapitata ps x c vulgaris cv y and allelochemical formation p can be expressed as follows simplified equations from kim et al 2018 1 d x d t μ ps exp r ps p x d ps 2 d y d t μ cv exp r cv p y d cv 3 d p d t α d y d t where μ ps and μ cv are the specific growth rates r ps and r cv the inhibition constants d ps and d cv the decrease rates and α is the growth related formation coefficient 3 2 2 modeling for the daphnids hybrid sd io model based on the hybrid sd ib modeling concept proposed by swinerd and mcnaught 2012 each daphnia individual and their population were modeled as agents with rich internal structure and stocked agents respectively fig 1 in the individual modeling process biological processes such as the energy allocation growth and reproduction of each daphnia individual were expressed based on the deb theory the purpose of this section was to simulate the population dynamics of daphnia with hybrid sd io modeling based on the sd approach thus descriptions of the deb theory and its equations are not included in this paper detailed information on the deb theory and its application can be found in several previously published articles e g kooijman et al 2008 peeters et al 2010 martin et al 2013 jager et al 2014 and the sd platform based deb model structure used in this hybrid model is shown in the appendix fig a1 and table a2 to make one to one pairs of each deb model and daphnia individual a large number of deb models deb 1 n were created by converting stock auxiliary and flow from scalar variables into array variables the deb models for each generation of the population were stored side by side in a single array variable fig 4 in this case study we established 30 1 970 and 8000 deb models of the first second and third generations of a daphnia population respectively taking into consideration their reproduction because each deb model corresponds to one daphnia individual the life cycle of the ith individual is simulated by the ith deb model deb i the total number of deb models generated and the number of deb models allocated to each generation can be expressed as follows 4 d e b 1 n d e b 1 s t d e b 2 n d d e b 3 r d 5 d e b 1 s t d e b 1 d e b 2 d e b a 1 d e b a 6 d e b 2 n d d e b a 1 d e b a 2 d e b b 1 d e b b 7 d e b 3 r d d e b b 1 d e b b 2 d e b n 1 d e b n where deb 1 n is all deb models in the system deb 1st deb 2nd and deb 3rd are the deb models distributed to the first second and the third generation of the daphnia population respectively n is the total number of created deb models and a and b are the specific constants for the separation of deb models for the first and second generations respectively a switch variable was used to simulate only the deb models expressing the individuals alive at time t among all of the generated deb models the switch variable was denoted as 1 or 0 for the ith individuals that exist or are absent at time t respectively for example the switch variables for each generation at the beginning of the simulations can be described as switch 1st 1 1 1 1 switch 2nd 0 0 0 0 and switch 3rd 0 0 0 0 fig 5 shows a set of switch variables representing the number of juvenile and adult individuals in each generation in this case study 30 juveniles starting from the stock juve p1 are processed by the simulation results of the deb model to be removed from the system by a flow death i1 or moving to the next stock adult p1 for example if the first and third juveniles become adults and the second juvenile dies the switch variables of each stock can be represented as follows juve p1 0 0 0 1 1 1 1 and adult p1 1 0 1 0 0 0 0 juveniles born from the adults of the first generation were introduced into stocks representing the juvenile of the second generation juve p2 according to the birth order through this process the growth reproduction and death of daphnia individuals from the juvenile of the first generation juve p1 to the adult of the third generation adult p3 could be simulated when calculating the population size of each generation at time t the number of 1 s in the stocks was counted counteq syntax was used in powersim fig 6 shows how the states of each deb model are determined over simulation time all of the deb models are individually activated depending on whether an individual exists or is absent and changes in the individual state such as newly born or dead are immediately reflected in each deb model the time of maturity of each individual was predicted by the deb model according to kooijman 2010 as follows when the scaled body length at time t l reached to the scaled body length at puberty l p an individual can produce new juveniles the death of each individual was also determined by mathematical equations in the deb model according to martin et al 2012 d d t q q l 3 l m 3 s g h a e v l r r q where 8 r 3 l d d t l 9 d d t h q r h where q is the aging acceleration h the hazard rate l the structural length l m the maximum length s g the gompertz stress coefficient h a the weibull aging acceleration e the reserve density v the energy conductance and r the dilution effect 3 3 integration of the algae and daphnid models into a sd model in this simulation study the two algal populations modeled by the sd approach section 3 2 1 and the daphnia population modeled by the hybrid sd io approach section 3 2 2 are interacting with each other according to a prey predator relationship the number of algal cells consumed by daphnia individuals was set based on the deb theory that it varies with the body length of the predator and the density of the prey appendix table a1 and a2 in this hybrid sd io model structure the algae model had a single value representing the algal density however the daphnid model had multiple values representing each daphnia individual during the simulation time an integration variable was used to calculate the interaction between the two models which consisted of scalar or array variables the number of algal cells eaten by the daphnia population per time step was expressed using the integration variable that converts the array variable to a single variable in detail six stocks that denoted the juveniles and adults of the first to third generations were concatenated concat syntax in powersim into one array variable denoting the whole population and then the sum of the algal density eaten by living daphnia arrsum syntax in powersim was subtracted from the present algal population at each time step 3 4 simulation conditions to expand our understanding of the ecological study of population dynamics to more complex situations the hybrid sd io approach based on the sd modeling approach was proposed the simulation study aimed to propose a hybrid sd io model using the sd modeling approach in population studies and implementing it rather than predicting the accurate density dynamics of the target species in this study the simulation was conducted in two steps first the case simulations were conducted using observed data to predict the dynamics of algal density and the daphnia population under different temperature conditions 15 20 25 and 30 c the initial density for both p subcapitata and c vulgaris species was 2 0 106 cells ml and the daphnia density was set at 30 juveniles to represent the individual differences in growth and aging of daphnia four parameters namely the initial length at birth length at puberty gompertz stress coefficient and weibull aging acceleration were set to be randomly selected from the standard normal distribution of initial values for each simulation the parameter values of the algae model and the daphnia model for each temperature condition were set using the previous experimental results kim et al 2018 and unpublished data shown in table a 4 and the literature of kooijman 2010 respectively second scenario simulations were performed based on the simulation results at 20 c the interspecific competition and prey predator interactions in the algae daphnid system can vary depending on which species benefit from given environmental conditions therefore four hypothetical scenarios were selected and the effect of each environmental condition on the algae daphnid system was tested the hypothetical scenarios used in the simulation were as follows i the condition that p subcapitata continuously flows from the outside to the algae daphnid system 2 0 105 cells ml day ii the condition that c vulgaris continuously flows from the outside to the algae daphnid system 2 0 105 cells ml day iii more favorable environmental conditions exist for the algal population growth growth rates and initial algal density of both algal species are increased by 20 and 50 respectively and iv the initial daphnia density is doubled to 60 in the algae daphnid system all model development and simulations were performed with powersim studio powersim software as bergen norway all ordinary differential equations in this study were integrated numerically using the first order euler method provided by powersim software while the time step was 1 h and the period of simulations was 40 d in total 1000 iterations were performed per simulation a detailed description of all parameters and mathematical equations for the algae and daphnia models can be found in the appendix algae model table a1 daphnia model table a2 4 results 4 1 simulation of temperature effects in the case simulation results for the effects of temperature on the algae daphnid system the p subcapitata density was relatively lower at 25 c and 30 c than that at 15 c and 20 c because c vulgaris had a competitive advantage over p subcapitata owing to the relatively rapid growth at high temperatures and the increase in allelochemical production because of c vulgaris growth fig 7 a therefore owing to the low p subcapitata density the p subcapitata uptake rate by daphnia sp had to be low at 25 c and 30 c fig 7c the simulation results for c vulgaris showed a complex situation in which the response to the temperature of c vulgaris and daphnia sp the uptake rate associated with the c vulgaris density and the change in the daphnia sp population density were all intertwined fig 7b and d in particular a lower c vulgaris density at 25 c than at lower temperature conditions appeared to be due to an increase in the amount of c vulgaris consumed by the daphnia population than the growth rate of c vulgaris on the other hand the simulation results for the daphnia population showed that its growth was faster as the temperature increased fig 7e 4 2 simulation of hypothetical scenarios the simulation results for the four hypothetical scenarios are shown in fig 8 the maximum density of the simulated daphnia populations was higher in the order of scenarios i 1 037 ii 971 iv 914 and iii 882 and reached relatively high densities under the continuous inflow of prey fig 8e under the condition that p subcapitata was introduced continuously the p subcapitata density and the predation rate by the daphnia population were the highest among the scenario results however in the case of continuous inflow of c vulgaris the c vulgaris density was lower compared to the case of high initial algal density and growth rate scenario iii because the c vulgaris uptake by the daphnia population was greater than the c vulgaris inflow in the case of high initial predator density scenario iv the rapid growth of the daphnia population was linked to the early extinction of both algal populations the results show that a hybrid sd io model using the sd modeling approach can be used to simulate the population dynamics of various hypothetical scenarios 4 3 potential availability of the proposed method in population dynamics studies the simulation results proved that the hybrid sd io model proposed in this study could describe the complex interactions of various factors including biological interactions such as interspecific competition between algal species and prey predator relationships as well as the direct effects of environmental conditions on the algae daphnid system figs 7 and 8 in addition the hybrid sd io model can be used for analysis and management tools such as scenario simulations as well as for simulations at both the individual e g body length of daphnia and system e g dynamics of daphnia age classes level fig 9 therefore the proposed method is useful for studying the population dynamics in a complex system composed of various components for example it can be used to study the impact of pollutant behavior in the environment on population dynamics or the competition of different populations for limited resources because the understanding of both individual level responses and systemic feedback effects are important in these examples as the collection of experimental data and model validation are involved in the development of a hybrid model studies on population dynamics can be conducted in a comprehensive and multifaceted way given that the approach proposed in this study combines sd and io modeling it has utilities of both modeling methods in population dynamics studies table 2 in particular the availability to represent the feedback effects among the system components and the individual heterogeneity which are the core features of sd and ib modeling approach is the most useful point of hybrid sd io modeling approach on the other hand there are restrictions on handling some of the features such as individual behavior and direct interactions among individuals in population dynamics that can be handled by ib modeling approach these limitations can be overcome by further studies because these are not fundamental problems of the proposed method for example the concept of expressing the state of individuals using the elements of the array variable can be extended to indicate the behavior and movement of the individual and the level of interaction among the individuals the elementary methods of using an array variable in hybrid sd io modeling approach were presented in this study however various applications of this method can take the advantages of both sd and ib modeling methods in population dynamics studies 5 discussion the sd method was used to model the density dynamics of two algal species and the biological process of daphnia based on the deb theory and the io method was used to simulate the population dynamics of daphnia using algorithmic rules for the life cycle of each individual the case simulation study for the algae daphnid system which included interspecific competition and prey predator interactions was conducted to demonstrate that the proposed hybrid sd io modeling method in this study can be used to simulate population dynamics with more complex situations such as variations in prey density multiple effects of environmental factors and individual heterogeneity of predators in this study there were no validation experiments however our study showed that the proposed method could be a useful approach to combine the sd and io approaches into a hybrid model population dynamics in ecosystems are very complex owing to the complex interactions within environmental or biological factors or both at different temporal scales a single modeling approach is suitable for modeling a specific type of system only in particular an sd approach is suitable for expressing group to group interactions and feedback effects whereas an ib approach is known to be suitable for describing the heterogeneity among individuals rahmandad and sterman 2008 jo et al 2015 given that the populations and the ecosystems interacting with them are composed of different levels of hierarchy the scope of use for a single model approach is limited in this regard this hybrid approach which can effectively integrate sd and io modeling is required to simulate the population dynamics under various environmental and biological conditions there are also limitations associated with the proposed method the first limitation is that the number of elements in the array variable must be predetermined to be greater than the expected maximum number of individuals before the simulation sd software programs do not provide a function for controlling the number of elements in array variables during the simulation and thus the system structure and interaction among the system components must be fixed at the initial simulation time when using the sd approach fig 10 shows that the required time for a simulation using proposed hybrid modeling increases with an increasing number of elements in the array variable which represents each daphnia individual the increase in simulation time due to an increase in the number of individuals is not surprising as one of the problems of the ib approach is that it requires computation for each individual during the simulation in the proposed method unnecessary time consuming simulations can occur if more individuals are created than the actual population size because the total number of individuals i e a sum of the activated and inactivated individuals created by the array variable not the activated individuals i e the actual population size affects the simulation time fig 10 for time efficient model simulation the modeler must define the number of individuals close to the maximum population density in the proposed method in further studies the use of additional modeling techniques or dynamic array variables can overcome this limitation however the purpose of modeling should be carefully considered for example depending on the purpose of the study it may be more important to record the change over time of each element in the array variable than to minimize the simulation time vincenot et al 2011 suggested that sd ib model swapping switching hybrid model which switches the sd and ib model depending on which modeling method is more efficient at each simulation process could help to solve the high computational cost in hybrid sd ib modeling approach wallentin and neuwirth 2017 showed that using the model swapping method based on the ib approach can minimize the loss of information on the hybrid sd ib model however the use of the swapping method based on the sd approach has not been well studied yet the second limitation is that the interactions between daphnia individuals were not clearly included in the hybrid model although in this simulation study each daphnia individual had indirect effects on other daphnia individuals by changing the algal density exploitation competition direct interactions between them were not included interference competition this limitation can also be found in previous hybrid modeling studies sterman and wittenberg 1999 akkermans 2001 jo et al 2015 because the development of hybrid sd ib models using the sd approach has generally been focused on expressing the many numbers of independent individuals as each element of the array variable represents one individual in the proposed method it is possible to incorporate the direct effects between the individuals into the model by adding functions which use information about the elements at each time step even so it is true that it is inconvenient to implement the direct effects between the individuals in the model compared to ib modeling using common ib software this issue remains the most important and fundamental challenge for developing the hybrid sd ib modeling method based on the sd approach beyond the proposed hybrid sd io modeling method although the above mentioned limitations exist the method proposed in this study has several advantages firstly it is relatively simple and easy to apply because the io modeling part is performed by expressing each individual as elements of the array variable it does not require a separate modeling technique or programming proficiency for io modeling in the proposed method after modeling only one individual using the sd method scalar variables are converted into array variables to represent multiple individuals as the elements representing each individual are simulated separately within the array variable individuals can be modeled individually secondly there is no need for multiple different modeling software programs for hybrid sd io modeling and only the sd platform is required most of the existing hybrid modeling methods use several modeling software programs together this means that the researchers need a programmer for modeling or time to improve their proficiency in various modeling software programs therefore as previously mentioned by lättilä et al 2010 hybrid sd ib modeling using only the sd platform will help researchers to more easily access hybrid modeling besides because it is based on the sd approach it will be useful for improving knowledge integration and system thinking skills for both modelers and end users kelly et al 2013 lastly the method can be extended to modeling for other topics such as the spatial distribution of individuals in our case study population modeling was performed by processing the information on the life cycle of daphnia e g survival growth feeding and reproduction however ib approach is possible for various purposes and objects depending on what information is processed by the array variable for example when the position information of the individual is represented by the element of the array variable e g representing a coordinate of the individual in a plane by a number at each time step the distribution of the individuals over time can be simulated further studies on the use of the array variable will help overcome the current rigidity of sd modeling approach until now the integration of sd and ib models using the sd approach has not been considered a key method in the development of hybrid modeling however recent studies including this study that have increased the usability and applicability of sd have made the sd a potential major approach for hybrid modeling table 3 in addition to the hybrid sd ib model studies have been actively conducted to integrate the sd model with other types of modeling methods and various approaches using sd to simulate spatial distribution one of the major limitations of the sd approach have been attempted furthermore methods have been developed that use a variety of algorithms and techniques to manage uncertainty in sd models or to improve the outputs these continued developments of the sd approach will be a great advantage of sd based hybrid modeling in recent years attempts have been made to use modeling as an integrated tool for solving complex and global problems further from the previous piecemeal approaches that have investigated local and specific problems typically there is a concept of integrated environmental modeling iem that is a holistic modeling approach to solving real world problems involving environmental and human systems laniak et al 2013 hamilton et al 2015 there has been considerable discussion about the concept and importance of integrated modeling however there is still a lack of research on how to integrate it although this study is only an attempt to develop a small part of integrated modeling it has proposed a practical method for sd io hybrid modeling approach on a single platform given that all integration methods have their own advantages and disadvantages lättilä et al 2010 various integration methods should be actively attempted and be discussed in depth 6 conclusions in this study frameworks and modeling processes for a hybrid sd io model using a sd modeling approach were demonstrated to demonstrate the detailed method and model outputs the proposed approach was applied to a case simulation study of population dynamics in algae daphnid system where the algal density and the daphnia population were represented using sd and hybrid sd io approaches respectively the case studies showed that using an array variable in the sd approach can model various individuals in the population and control the simulation time and the hybrid sd io approach can efficiently model the overall system structure by applying the appropriate model i e sd or io to each system component the results of the case study also showed that the hybrid sd io model using the sd modeling approach allowed for successful exploration of the systems involving complex interactions between components in a comprehensive and multifaceted way by providing both individual and system level viewpoints there are some limitations in the process of hybrid sd io modeling method developments of the method to optimize the computational efficiency and to include the effect between individuals in the model are needed to improve the utilization of the proposed method however the proposed method allows the researcher to perform hybrid modeling relatively easily and implements the hybrid sd io model using only the sd platform in addition due to the development of the sd approach that is being reported in various fields and the continued advance of the sd platform hybrid sd io modeling using the sd approach is becoming increasingly useful for studying complex systems hybrid sd io modeling can be used as a partial approach to integrated environmental modeling through further validation studies using large amounts of observed data to evaluate its application to various disciplines acknowledgments this study was supported by the korea ministry of environment moe as climate change correspondence program project number 2014001310008 author contribution y kim y s lee and k cho designed the research j son and j hong collected the data set y kim and m lee developed the model structures and algorithms the authors sincerely thank three anonymous reviewers for their constructive advice and comments for improvement appendix a supplementary data the following is the supplementary data to this article manuscript ems 2nd final docx 40 49 manuscript ems 2nd final docx 40 49 data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 11 009 
26266,although the system dynamics sd modeling approach has been used frequently to model various systems there is little research on the development of hybrid sd individual oriented io modeling the aim of this study is to demonstrate the use of an array variable for the hybrid sd io modeling approach using the sd platform and to provide a detailed modeling process of an algae daphnid system consisting of two freshwater algal species and daphnia our study showed that the array variable in the sd approach is useful for modeling multiple individuals and as a function to control the switch and integration variable for the hybrid modeling approach the proposed method can improve the usability of hybrid modeling by enabling hybrid sd io modeling without requiring extensive knowledge of programming given the ongoing development and advancement of the sd modeling approach this modeling technique will be a useful method to study complex systems keywords system dynamics individual oriented modeling individual based modeling dynamic energy budget array variable 1 introduction various modeling methods have been developed and applied to study ecosystems with multiple components that interact in complex ways system dynamics sd models agent based ab also referred in ecology as individual based ib models bayesian networks bns knowledge based models kbms and coupled component models ccms are the five types of models used to model complex systems and these models can handle multiple issues scales and complex interactions of ecosystems kelly et al 2013 among them sd and ib approaches have been widely used to improve our understanding of a system s behavior owing to advances in modeling software e g sd stella vensim and powersim ib netlogo and repast sd and ib modeling have not typically been combined due to their contrasting characteristics vincenot et al 2011 sd modeling is a typical deductive rule i e top down approach to describe the behavior of a system an sd model is constructed by the formulation of the system using ordinary differential equations and represented by stock flow and auxiliary inputs through the conceptualization of causal relationships and feedback loops kelly et al 2013 this modeling method offers a relatively effective way to extend the model structure from basic stock and flow models to complex system models by simplifying model representations and provisioning a graphical modeling interface vincenot et al 2011 on the other hand ib modeling describes the dynamics of the system by the sum of its parts from an inductive perspective i e bottom up approach in an ib model individuals can determine their behaviors such as interactions with the environment movements with a direction change and selection of resources by algorithmic rules therefore this modeling method has been widely used to study species distributions and population dynamics macal 2010 recently interest in hybrid sd ib models that combine the advantages of sd and ib modeling methods has increased to provide potentially useful tools particularly for modeling to understand more complex systems that contain components of various scales vincenot et al 2011 reviewed the literature regarding hybrid sd ib models applied to ecology and categorized the use of hybrid models in four ways table 1 since then swinerd and mcnaught 2012 have proposed three conceptual categories of hybrid sd ib simulations the integrated hybrid design the interfaced hybrid design and the sequential hybrid design in particular they classified the integrated hybrid design into three types according to the combination structure of sd and ib models table 1 these proposed conceptual categories provided useful guidelines for integrating the two different modeling methods recently wallentin neuwirth 2017 suggested six alternative designs of integrated hybrid models that dynamically switch between sd and ib models and discussed the information loss that occurs from switching between modeling methods these conceptual guides for the hybrid sd ib approach have helped improve the design and modeling of hybrid approaches however practical and detailed studies are limited and rudimentary wallentin and neuwirth 2017 also noted that despite the conceptual promise of hybrid sd ib modeling there are still few examples of such hybrid modeling in research hybrid sd ib modeling is not being widely used because many limitations exist in creating hybrid models for example to create a hybrid model both the sd platform and the ib platform as well as the middleware that link them should be used together alternatively hybrid modeling software suitable for this purpose should be developed using a programming language using hybrid modeling toolsets such as netlogo originally an ib modeling software with the capability for sd modeling and anylogic a typical hybrid modeling software may be an alternative however given that these platforms are software that support ib modeling as a core feature they have limitations in sophisticated sd modeling for example the former only supports the basic functionality for sd modeling wallentin and neuwirth 2017 the latter is one of the most widely used hybrid modeling software however it does not support detailed setting tools for high level sd modeling such as variable type e g complex logical real and integer integration order e g first order zero order and zero order immediate and series type e g first last accumulated and average in addition the hybrid modeling platform is relatively difficult to work with because of the integration of various modeling features the integration of sd and ib using a single platform can improve the accessibility and convenience for researchers in the development of hybrid models the sd platform has several advantages as the single platform for creating the hybrid sd ib model first the sd platform provides a convenient interface for researchers to model using an intuitive graphical language e g stocks and auxiliary variables are represented by squares and circles respectively allowing them to build complex systems without a high level proficiency in the programming language second sd platforms can make it easier to understand the feedback structures of the whole system lättilä et al 2010 previously kim juhn 1997 proposed the idea that sd can be integrated with ib using array variables other researchers who focused on the conceptual development of hybrid sd ib models have referred to their integration lättilä et al 2010 vincenot et al 2011 however its applications and development are still rare jo et al 2015 especially in the fields of ecology and environmental science the advantages of hybrid sd ib modeling approach based on the sd are obvious but several issues need to be solved such as the individual level modeling and the spatial representation in the sd model in this study a hybrid sd individual oriented io modeling method based on the sd approach was proposed that focused on integrating the sd and the individual level model conceptual frameworks and detailed modeling processes for integrating sd and io approach into the sd based model were demonstrated to explore population dynamics in a complex system to implement the proposed methods we used the algae daphnid system with three species and their interactions as a case study the algal population was modeled as a simple sd model and daphnids were modeled as stocked agents and agents with rich internal structure according to the integrated hybrid sd ib designs proposed by swinerd and mcnaught 2012 fig 1 this proposed hybrid modeling approach enables the modeling of complex systems using a single platform and can be the basis of the sd based hybrid sd ib approach 2 frameworks for hybrid sd io modeling in population dynamics 2 1 sd and ib models in population dynamics in ecology to date sd and ib approaches have been used individually to study the population dynamics of specific species bald et al 2006 2009 and weller et al 2014 showed that sd models can be used to simulate population dynamics and are useful for scenario simulation and management strategies additionally sd models have been used to predict the effects of various factors on population dynamics by combining age class models and simulate scenarios that include different management decisions or environmental conditions meanwhile ib model approaches have been focused on the reliable prediction of population dynamics through combinations of simulations of the responses of each individual recently ib models have increasingly been combined with the dynamic energy budget deb theory which provides a consistent framework and logic to explore the effects of environmental factors on vital rates of the species such as growth reproduction and survival bacher and gangnery 2006 johnston et al 2014 martin et al 2013 both sd and ib methods are useful tools and have their own advantages in studies of population modeling however in recent years interest in integrating the two methods has increased to explore population dynamics in natural ecosystems given that it is appropriate to represent individuals in a population using ib and to express environments or resources using sd the hybrid sd ib approach can effectively model a complex ecosystem rather than relying on only one approach to simulate one or the other vincenot et al 2011 previously akkermans 2001 proposed an individual oriented sd model that expressed 100 individuals as stocks of the sd approach however the method to represent all individuals as each stock is inefficient to apply to population dynamics because the population sizes can be very large or fluctuate over time the lack of further studies on population dynamics using hybrid modeling based on the sd approach might be due to these limitations the ib modeling approach on the other hand is relatively well suited to population dynamics studies generally ib modeling approaches have been preferred in complex situations such as the heterogeneity of individuals requirement of flexible model structure and presence of complex events lättilä et al 2010 because ib modeling can handle both discrete and continuous simulations schieritz 2002 in terms of the ib approach independent simulations for individual organisms are essential functions that are useful when each individual born at different points in time must be individually simulated in addition the model structure should reflect the changes in the number of individuals representing population dynamics over time however the structure of a typical sd model is fixed during the simulation time these significant drawbacks of the sd approach need to be improved to integrate ib models into the sd model 2 2 proposed hybrid sd io modeling using the sd approach to improve the inflexibility of sd modeling approach array variables and some additional model components are used in the model development process all sd model components such as stock auxiliary and flow can be used as scalar variables or array variables in an sd platform a scalar variable has a single value to represent the object as a single representative value whereas an array variable can have multiple values to represent a set of entities for example given that the scalar variable can represent only one individual the number of scalar variables equal to the total number of individuals are required to represent a population fig 2 a on the other hand the population can be represented by a single array variable because the array variable can have the number of values equal to the total number of individuals fig 2b by using an array variable all individuals within the population can be integrated into a single sd model structure in a simple and effective manner independent simulations for each individual can be made by using a switch variable which is a kind of array variable that controls the start of the simulation for each entity in other array variables the switch variable has the same number of values as the array variable being controlled for one to one correspondence fig 3 a at each time step each value of the switch variable has a numeric number or a logical value e g true or false that indicates the state of the corresponding individual on a one to one basis for example when the ith value of the switch variable is true or false it either starts or does not start the simulation for the ith individual of the array variable representing the population respectively this process allows independent entities of array variables to be simulated separately on the sd modeling approach in the hybrid sd io modeling process it may be necessary to link the array variables e g population to the scalar variables e g single components such as resource and environment the connection of these two types of variables can be performed using an integration variable which is a scalar variable that uses conditional functions typically if then else syntax or array functions e g the sum of array elements fig 3b the integration variable combines the array variable values into a single scalar variable value which is determined to be present according to the values of the switch variable for example when expressing resource consumption by the population only the values of the array variable corresponding to the values of the switch variables indicating survival are selected and are summed as a single value by applying this method a hybrid sd io model based on the sd modeling approach can be developed and simulated in summary array variables are used to represent groups containing multiple entities easily and the simulation time of all entities is controlled individually using switch variables more detailed model development processes and examples of use are described in the following sections 3 case study an algae daphnid system 3 1 overview a case simulation study of an algae daphnid system was conducted to demonstrate the suitability of the hybrid sd io model using the sd modeling approach in this case simulation study changes in the densities of two freshwater algal species pseudokirchneriella subcapitata also known as selenastrum capricornutum and raphidocelis subcapitata and chlorella vulgaris and their predators daphnia sp over time under various environmental conditions were investigated although this algae daphnid system is a simple system consisting of three species it was expected that each species would be affected by complex effects and interactions continuously for example the environmental conditions interspecific competition between the two algal species growth rates feeding rates of daphnia and density of each species not only affect the dynamics of the algae daphnid system independently or in combination but also affect each of the other species through feedback effects in this complex situation an sd approach is appropriate to express the dynamics of system structure with complicated feedback effects while an io approach is better at describing the independent behaviors of individuals through the modeling of rules jo et al 2015 therefore among the components of the algae daphnid system the dynamics of the algal density over time and the individual characteristics of daphnia e g growth and reproduction were modeled with sd and io methods respectively 3 2 the modeling process 3 2 1 density dynamic modeling for the freshwater algae a simple sd model the purpose of the algae model was to represent the density dynamics of two freshwater algal species p subcapitata and c vulgaris as with the study of wallentin and neuwirth 2017 the density changes in each algal population were described by the stocks of the sd model because the number of algae cells is too large to describe each cell as an individual of the io approach as c vulgaris is known to inhibit the growth of p subcapitata by producing allelochemicals during their growth fergola et al 2007 the algae model included an allelochemical component as well as the density of the two algal species according to fergola et al 2007 and yang et al 2011 the algal growth and allelochemical formation can be expressed as growth rate exp inhibition constant allelochemical concentration and formation coefficient algal population growth respectively thus the three ordinary differential equations that describe the time dependent algal density p subcapitata ps x c vulgaris cv y and allelochemical formation p can be expressed as follows simplified equations from kim et al 2018 1 d x d t μ ps exp r ps p x d ps 2 d y d t μ cv exp r cv p y d cv 3 d p d t α d y d t where μ ps and μ cv are the specific growth rates r ps and r cv the inhibition constants d ps and d cv the decrease rates and α is the growth related formation coefficient 3 2 2 modeling for the daphnids hybrid sd io model based on the hybrid sd ib modeling concept proposed by swinerd and mcnaught 2012 each daphnia individual and their population were modeled as agents with rich internal structure and stocked agents respectively fig 1 in the individual modeling process biological processes such as the energy allocation growth and reproduction of each daphnia individual were expressed based on the deb theory the purpose of this section was to simulate the population dynamics of daphnia with hybrid sd io modeling based on the sd approach thus descriptions of the deb theory and its equations are not included in this paper detailed information on the deb theory and its application can be found in several previously published articles e g kooijman et al 2008 peeters et al 2010 martin et al 2013 jager et al 2014 and the sd platform based deb model structure used in this hybrid model is shown in the appendix fig a1 and table a2 to make one to one pairs of each deb model and daphnia individual a large number of deb models deb 1 n were created by converting stock auxiliary and flow from scalar variables into array variables the deb models for each generation of the population were stored side by side in a single array variable fig 4 in this case study we established 30 1 970 and 8000 deb models of the first second and third generations of a daphnia population respectively taking into consideration their reproduction because each deb model corresponds to one daphnia individual the life cycle of the ith individual is simulated by the ith deb model deb i the total number of deb models generated and the number of deb models allocated to each generation can be expressed as follows 4 d e b 1 n d e b 1 s t d e b 2 n d d e b 3 r d 5 d e b 1 s t d e b 1 d e b 2 d e b a 1 d e b a 6 d e b 2 n d d e b a 1 d e b a 2 d e b b 1 d e b b 7 d e b 3 r d d e b b 1 d e b b 2 d e b n 1 d e b n where deb 1 n is all deb models in the system deb 1st deb 2nd and deb 3rd are the deb models distributed to the first second and the third generation of the daphnia population respectively n is the total number of created deb models and a and b are the specific constants for the separation of deb models for the first and second generations respectively a switch variable was used to simulate only the deb models expressing the individuals alive at time t among all of the generated deb models the switch variable was denoted as 1 or 0 for the ith individuals that exist or are absent at time t respectively for example the switch variables for each generation at the beginning of the simulations can be described as switch 1st 1 1 1 1 switch 2nd 0 0 0 0 and switch 3rd 0 0 0 0 fig 5 shows a set of switch variables representing the number of juvenile and adult individuals in each generation in this case study 30 juveniles starting from the stock juve p1 are processed by the simulation results of the deb model to be removed from the system by a flow death i1 or moving to the next stock adult p1 for example if the first and third juveniles become adults and the second juvenile dies the switch variables of each stock can be represented as follows juve p1 0 0 0 1 1 1 1 and adult p1 1 0 1 0 0 0 0 juveniles born from the adults of the first generation were introduced into stocks representing the juvenile of the second generation juve p2 according to the birth order through this process the growth reproduction and death of daphnia individuals from the juvenile of the first generation juve p1 to the adult of the third generation adult p3 could be simulated when calculating the population size of each generation at time t the number of 1 s in the stocks was counted counteq syntax was used in powersim fig 6 shows how the states of each deb model are determined over simulation time all of the deb models are individually activated depending on whether an individual exists or is absent and changes in the individual state such as newly born or dead are immediately reflected in each deb model the time of maturity of each individual was predicted by the deb model according to kooijman 2010 as follows when the scaled body length at time t l reached to the scaled body length at puberty l p an individual can produce new juveniles the death of each individual was also determined by mathematical equations in the deb model according to martin et al 2012 d d t q q l 3 l m 3 s g h a e v l r r q where 8 r 3 l d d t l 9 d d t h q r h where q is the aging acceleration h the hazard rate l the structural length l m the maximum length s g the gompertz stress coefficient h a the weibull aging acceleration e the reserve density v the energy conductance and r the dilution effect 3 3 integration of the algae and daphnid models into a sd model in this simulation study the two algal populations modeled by the sd approach section 3 2 1 and the daphnia population modeled by the hybrid sd io approach section 3 2 2 are interacting with each other according to a prey predator relationship the number of algal cells consumed by daphnia individuals was set based on the deb theory that it varies with the body length of the predator and the density of the prey appendix table a1 and a2 in this hybrid sd io model structure the algae model had a single value representing the algal density however the daphnid model had multiple values representing each daphnia individual during the simulation time an integration variable was used to calculate the interaction between the two models which consisted of scalar or array variables the number of algal cells eaten by the daphnia population per time step was expressed using the integration variable that converts the array variable to a single variable in detail six stocks that denoted the juveniles and adults of the first to third generations were concatenated concat syntax in powersim into one array variable denoting the whole population and then the sum of the algal density eaten by living daphnia arrsum syntax in powersim was subtracted from the present algal population at each time step 3 4 simulation conditions to expand our understanding of the ecological study of population dynamics to more complex situations the hybrid sd io approach based on the sd modeling approach was proposed the simulation study aimed to propose a hybrid sd io model using the sd modeling approach in population studies and implementing it rather than predicting the accurate density dynamics of the target species in this study the simulation was conducted in two steps first the case simulations were conducted using observed data to predict the dynamics of algal density and the daphnia population under different temperature conditions 15 20 25 and 30 c the initial density for both p subcapitata and c vulgaris species was 2 0 106 cells ml and the daphnia density was set at 30 juveniles to represent the individual differences in growth and aging of daphnia four parameters namely the initial length at birth length at puberty gompertz stress coefficient and weibull aging acceleration were set to be randomly selected from the standard normal distribution of initial values for each simulation the parameter values of the algae model and the daphnia model for each temperature condition were set using the previous experimental results kim et al 2018 and unpublished data shown in table a 4 and the literature of kooijman 2010 respectively second scenario simulations were performed based on the simulation results at 20 c the interspecific competition and prey predator interactions in the algae daphnid system can vary depending on which species benefit from given environmental conditions therefore four hypothetical scenarios were selected and the effect of each environmental condition on the algae daphnid system was tested the hypothetical scenarios used in the simulation were as follows i the condition that p subcapitata continuously flows from the outside to the algae daphnid system 2 0 105 cells ml day ii the condition that c vulgaris continuously flows from the outside to the algae daphnid system 2 0 105 cells ml day iii more favorable environmental conditions exist for the algal population growth growth rates and initial algal density of both algal species are increased by 20 and 50 respectively and iv the initial daphnia density is doubled to 60 in the algae daphnid system all model development and simulations were performed with powersim studio powersim software as bergen norway all ordinary differential equations in this study were integrated numerically using the first order euler method provided by powersim software while the time step was 1 h and the period of simulations was 40 d in total 1000 iterations were performed per simulation a detailed description of all parameters and mathematical equations for the algae and daphnia models can be found in the appendix algae model table a1 daphnia model table a2 4 results 4 1 simulation of temperature effects in the case simulation results for the effects of temperature on the algae daphnid system the p subcapitata density was relatively lower at 25 c and 30 c than that at 15 c and 20 c because c vulgaris had a competitive advantage over p subcapitata owing to the relatively rapid growth at high temperatures and the increase in allelochemical production because of c vulgaris growth fig 7 a therefore owing to the low p subcapitata density the p subcapitata uptake rate by daphnia sp had to be low at 25 c and 30 c fig 7c the simulation results for c vulgaris showed a complex situation in which the response to the temperature of c vulgaris and daphnia sp the uptake rate associated with the c vulgaris density and the change in the daphnia sp population density were all intertwined fig 7b and d in particular a lower c vulgaris density at 25 c than at lower temperature conditions appeared to be due to an increase in the amount of c vulgaris consumed by the daphnia population than the growth rate of c vulgaris on the other hand the simulation results for the daphnia population showed that its growth was faster as the temperature increased fig 7e 4 2 simulation of hypothetical scenarios the simulation results for the four hypothetical scenarios are shown in fig 8 the maximum density of the simulated daphnia populations was higher in the order of scenarios i 1 037 ii 971 iv 914 and iii 882 and reached relatively high densities under the continuous inflow of prey fig 8e under the condition that p subcapitata was introduced continuously the p subcapitata density and the predation rate by the daphnia population were the highest among the scenario results however in the case of continuous inflow of c vulgaris the c vulgaris density was lower compared to the case of high initial algal density and growth rate scenario iii because the c vulgaris uptake by the daphnia population was greater than the c vulgaris inflow in the case of high initial predator density scenario iv the rapid growth of the daphnia population was linked to the early extinction of both algal populations the results show that a hybrid sd io model using the sd modeling approach can be used to simulate the population dynamics of various hypothetical scenarios 4 3 potential availability of the proposed method in population dynamics studies the simulation results proved that the hybrid sd io model proposed in this study could describe the complex interactions of various factors including biological interactions such as interspecific competition between algal species and prey predator relationships as well as the direct effects of environmental conditions on the algae daphnid system figs 7 and 8 in addition the hybrid sd io model can be used for analysis and management tools such as scenario simulations as well as for simulations at both the individual e g body length of daphnia and system e g dynamics of daphnia age classes level fig 9 therefore the proposed method is useful for studying the population dynamics in a complex system composed of various components for example it can be used to study the impact of pollutant behavior in the environment on population dynamics or the competition of different populations for limited resources because the understanding of both individual level responses and systemic feedback effects are important in these examples as the collection of experimental data and model validation are involved in the development of a hybrid model studies on population dynamics can be conducted in a comprehensive and multifaceted way given that the approach proposed in this study combines sd and io modeling it has utilities of both modeling methods in population dynamics studies table 2 in particular the availability to represent the feedback effects among the system components and the individual heterogeneity which are the core features of sd and ib modeling approach is the most useful point of hybrid sd io modeling approach on the other hand there are restrictions on handling some of the features such as individual behavior and direct interactions among individuals in population dynamics that can be handled by ib modeling approach these limitations can be overcome by further studies because these are not fundamental problems of the proposed method for example the concept of expressing the state of individuals using the elements of the array variable can be extended to indicate the behavior and movement of the individual and the level of interaction among the individuals the elementary methods of using an array variable in hybrid sd io modeling approach were presented in this study however various applications of this method can take the advantages of both sd and ib modeling methods in population dynamics studies 5 discussion the sd method was used to model the density dynamics of two algal species and the biological process of daphnia based on the deb theory and the io method was used to simulate the population dynamics of daphnia using algorithmic rules for the life cycle of each individual the case simulation study for the algae daphnid system which included interspecific competition and prey predator interactions was conducted to demonstrate that the proposed hybrid sd io modeling method in this study can be used to simulate population dynamics with more complex situations such as variations in prey density multiple effects of environmental factors and individual heterogeneity of predators in this study there were no validation experiments however our study showed that the proposed method could be a useful approach to combine the sd and io approaches into a hybrid model population dynamics in ecosystems are very complex owing to the complex interactions within environmental or biological factors or both at different temporal scales a single modeling approach is suitable for modeling a specific type of system only in particular an sd approach is suitable for expressing group to group interactions and feedback effects whereas an ib approach is known to be suitable for describing the heterogeneity among individuals rahmandad and sterman 2008 jo et al 2015 given that the populations and the ecosystems interacting with them are composed of different levels of hierarchy the scope of use for a single model approach is limited in this regard this hybrid approach which can effectively integrate sd and io modeling is required to simulate the population dynamics under various environmental and biological conditions there are also limitations associated with the proposed method the first limitation is that the number of elements in the array variable must be predetermined to be greater than the expected maximum number of individuals before the simulation sd software programs do not provide a function for controlling the number of elements in array variables during the simulation and thus the system structure and interaction among the system components must be fixed at the initial simulation time when using the sd approach fig 10 shows that the required time for a simulation using proposed hybrid modeling increases with an increasing number of elements in the array variable which represents each daphnia individual the increase in simulation time due to an increase in the number of individuals is not surprising as one of the problems of the ib approach is that it requires computation for each individual during the simulation in the proposed method unnecessary time consuming simulations can occur if more individuals are created than the actual population size because the total number of individuals i e a sum of the activated and inactivated individuals created by the array variable not the activated individuals i e the actual population size affects the simulation time fig 10 for time efficient model simulation the modeler must define the number of individuals close to the maximum population density in the proposed method in further studies the use of additional modeling techniques or dynamic array variables can overcome this limitation however the purpose of modeling should be carefully considered for example depending on the purpose of the study it may be more important to record the change over time of each element in the array variable than to minimize the simulation time vincenot et al 2011 suggested that sd ib model swapping switching hybrid model which switches the sd and ib model depending on which modeling method is more efficient at each simulation process could help to solve the high computational cost in hybrid sd ib modeling approach wallentin and neuwirth 2017 showed that using the model swapping method based on the ib approach can minimize the loss of information on the hybrid sd ib model however the use of the swapping method based on the sd approach has not been well studied yet the second limitation is that the interactions between daphnia individuals were not clearly included in the hybrid model although in this simulation study each daphnia individual had indirect effects on other daphnia individuals by changing the algal density exploitation competition direct interactions between them were not included interference competition this limitation can also be found in previous hybrid modeling studies sterman and wittenberg 1999 akkermans 2001 jo et al 2015 because the development of hybrid sd ib models using the sd approach has generally been focused on expressing the many numbers of independent individuals as each element of the array variable represents one individual in the proposed method it is possible to incorporate the direct effects between the individuals into the model by adding functions which use information about the elements at each time step even so it is true that it is inconvenient to implement the direct effects between the individuals in the model compared to ib modeling using common ib software this issue remains the most important and fundamental challenge for developing the hybrid sd ib modeling method based on the sd approach beyond the proposed hybrid sd io modeling method although the above mentioned limitations exist the method proposed in this study has several advantages firstly it is relatively simple and easy to apply because the io modeling part is performed by expressing each individual as elements of the array variable it does not require a separate modeling technique or programming proficiency for io modeling in the proposed method after modeling only one individual using the sd method scalar variables are converted into array variables to represent multiple individuals as the elements representing each individual are simulated separately within the array variable individuals can be modeled individually secondly there is no need for multiple different modeling software programs for hybrid sd io modeling and only the sd platform is required most of the existing hybrid modeling methods use several modeling software programs together this means that the researchers need a programmer for modeling or time to improve their proficiency in various modeling software programs therefore as previously mentioned by lättilä et al 2010 hybrid sd ib modeling using only the sd platform will help researchers to more easily access hybrid modeling besides because it is based on the sd approach it will be useful for improving knowledge integration and system thinking skills for both modelers and end users kelly et al 2013 lastly the method can be extended to modeling for other topics such as the spatial distribution of individuals in our case study population modeling was performed by processing the information on the life cycle of daphnia e g survival growth feeding and reproduction however ib approach is possible for various purposes and objects depending on what information is processed by the array variable for example when the position information of the individual is represented by the element of the array variable e g representing a coordinate of the individual in a plane by a number at each time step the distribution of the individuals over time can be simulated further studies on the use of the array variable will help overcome the current rigidity of sd modeling approach until now the integration of sd and ib models using the sd approach has not been considered a key method in the development of hybrid modeling however recent studies including this study that have increased the usability and applicability of sd have made the sd a potential major approach for hybrid modeling table 3 in addition to the hybrid sd ib model studies have been actively conducted to integrate the sd model with other types of modeling methods and various approaches using sd to simulate spatial distribution one of the major limitations of the sd approach have been attempted furthermore methods have been developed that use a variety of algorithms and techniques to manage uncertainty in sd models or to improve the outputs these continued developments of the sd approach will be a great advantage of sd based hybrid modeling in recent years attempts have been made to use modeling as an integrated tool for solving complex and global problems further from the previous piecemeal approaches that have investigated local and specific problems typically there is a concept of integrated environmental modeling iem that is a holistic modeling approach to solving real world problems involving environmental and human systems laniak et al 2013 hamilton et al 2015 there has been considerable discussion about the concept and importance of integrated modeling however there is still a lack of research on how to integrate it although this study is only an attempt to develop a small part of integrated modeling it has proposed a practical method for sd io hybrid modeling approach on a single platform given that all integration methods have their own advantages and disadvantages lättilä et al 2010 various integration methods should be actively attempted and be discussed in depth 6 conclusions in this study frameworks and modeling processes for a hybrid sd io model using a sd modeling approach were demonstrated to demonstrate the detailed method and model outputs the proposed approach was applied to a case simulation study of population dynamics in algae daphnid system where the algal density and the daphnia population were represented using sd and hybrid sd io approaches respectively the case studies showed that using an array variable in the sd approach can model various individuals in the population and control the simulation time and the hybrid sd io approach can efficiently model the overall system structure by applying the appropriate model i e sd or io to each system component the results of the case study also showed that the hybrid sd io model using the sd modeling approach allowed for successful exploration of the systems involving complex interactions between components in a comprehensive and multifaceted way by providing both individual and system level viewpoints there are some limitations in the process of hybrid sd io modeling method developments of the method to optimize the computational efficiency and to include the effect between individuals in the model are needed to improve the utilization of the proposed method however the proposed method allows the researcher to perform hybrid modeling relatively easily and implements the hybrid sd io model using only the sd platform in addition due to the development of the sd approach that is being reported in various fields and the continued advance of the sd platform hybrid sd io modeling using the sd approach is becoming increasingly useful for studying complex systems hybrid sd io modeling can be used as a partial approach to integrated environmental modeling through further validation studies using large amounts of observed data to evaluate its application to various disciplines acknowledgments this study was supported by the korea ministry of environment moe as climate change correspondence program project number 2014001310008 author contribution y kim y s lee and k cho designed the research j son and j hong collected the data set y kim and m lee developed the model structures and algorithms the authors sincerely thank three anonymous reviewers for their constructive advice and comments for improvement appendix a supplementary data the following is the supplementary data to this article manuscript ems 2nd final docx 40 49 manuscript ems 2nd final docx 40 49 data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 11 009 
26267,wdslib is an extensible simulation toolkit for the steady state analysis of a water distribution system it includes a range of solution methods the forest core partitioning algorithm the global gradient algorithm the reformulated co tree flows method and also combinations of these methods wdslib has been created using a modularized object oriented design and implemented in the c programming language and has been validated against a reference matlab implementation wdslib has been designed i to avoid unnecessary computations by hoisting each of the modules to its appropriate level of repetition ii to perform the computations independently of measurement units using scaled variables iii to accurately report the execution time of all the modules in that it is possible to produce a timing model to parameterize multiple simulation times such as in an optimization using a genetic algorithm from a series of sampling simulation runs and iv to guard against numerical failures two example applications a once off simulation and a network optimization design application simulation are presented this toolkit can be used i to implement test and compare different solution methods ii to focus the research on the most time consuming parts of a solution method and iii to guide the choice of solution method when multiple simulation runs are required keywords water distribution system c toolkit object oriented design forest core partitioning algorithm reformulated co tree flows method global gradient algorithm open source software software availability name of the software wdslib version 1 0 available from https github com a1184182 wdslib language c supported system windows macos linux unix year first available 2018 1 introduction hydraulic simulation has been used to model water distribution systems wdss for several decades and is an essential tool for the design operation and management of wdss in industry and research hydraulic simulation allows users 1 to optimize wds network parameters such as pipe diameters in a design setting 2 to calibrate network parameters such as demand patterns in a conventional operational setting 3 to conduct real time monitoring and calibration of the network elements in a supervisory control and data acquisition scada operational setting and 4 to adjust control devices such as valves in a management setting in the design setting and both the above operational settings repeated hydraulic assessment is required on a network with fixed topology in the management setting repeated hydraulic assessment is required on a network with flexible network parameter settings with ever increasing network sizes and the need for real time management using a scada system it is important to have a robust simulation package which can be configured to be maximally efficient whatever the setting in the field of hydraulic simulation the system of equations can be formulated as a large and sparse non linear saddle point problem saddle point problems are defined to be a function of two variables say f q h and the objective is to find a pair of variables q h such that the value f q h is minimized with respect to the first variable and maximized with respect to the second variable or equivalently f q h f q h f q h q h there are several well known iteration methods for solving the non linear saddle point problem these include range space methods methods that operate in the subspace defined by the rows of the unknown head node arc incidences matrix global gradient algorithm todini and pilati 1988 null space methods methods that operate in the subspace defined by a null space that is orthogonal to the column of the unknown head node arc incidences matrix co tree flows formulation variations rahal 1995 elhay et al 2014 loop based methods methods that are based on the sum of head losses around a loop must be zero loop flow correction cross 1936 and pre and post processing methods forest core partitioning algorithm simpson et al 2014 domain decomposition diao et al 2014 network clustering perelman and ostfeld 2011 their relative performance in terms of speed rate of convergence and accuracy depends among other things on the topology of the target network size of the forest component the number of network loops and the density of these network loops it is difficult to evaluate the impact of these topology factors by only examining the incidence matrix that describes the pipe network connectivity as a result the best method to use for a particular network cannot be easily determined a priori moreover extra complexity is introduced when a multi run hydraulic assessment is required during a multi run hydraulic simulation the elapsed computation time of each method can be broken down into two parts the components that are only required to be performed once at the very beginning for the same network called the overhead and the components that are required to be carried out repeatedly for each separate run until the required number of iterations has been met called the hydraulic phase it is desirable to have a simulation platform given the different levels of repetition to implement these alternative algorithms efficiently equipped with such a platform a user would be able to easily benchmark the performance of alternative methods on a small number of evaluations for a given network and use that performance to inform the choice of algorithm to use for either a once off simulation setting or for a multiple simulation setting such as for an evolutionary algorithm ea this work describes an extensible wds simulation platform called wdslib wdslib is a numerically robust efficient and accurate c library that implements many wds simulation methods wdslib is written using a modular object oriented design which allows users to easily mix and interchange solution components thereby enabling users to avoid redundant computations it has been optimized to use sparse data structures which are oriented to the pattern of access required for each solution method wdslib has been validated for accuracy on a range of realistic benchmark water distribution networks against reference implementations and tested for speed the program accepts the input file formats of the industry standard epanet2 rossman 2000 toolkit and its performance is faster than epanet2 in all tested settings and benchmarks the remainder of this paper is structured as follows the next section describes related methodologies and implementations section 3 presents a general description of the wds demand driven steady state problem and the solution methods that are used in wdslib the tool kit structure is then given in section 4 this is followed in section 5 by the toolkit implementation details section 6 provides some examples of how the toolkit can be utilized in a simulation work flow the results are discussed in section 7 finally section 8 summarizes the results of this paper and describes future extensions to the toolkit 2 background this section describes related water distribution system network solution methods and implementations the first subsection describes solution methods including those used by wdslib this is followed by a description of currently available implementations and compares these with wdslib 2 1 related methods this research considers a water distribution model made up of energy conservation equations and the demand driven model continuity equations the hardy cross method cross 1936 also known as the loop flow corrections method is one of the oldest methods and uses successive approximations solving for each loop flow correction independently it is a method that was widely used for its simplicity at the time when it was introduced more than three decades later epp and fowler 1970 developed a computer version of cross s method and replaced the numerical solver with the newton method which solves for all loop flow corrections simultaneously however this method has not been widely used because of the need i to identify the network loops ii to find initial flows that satisfy continuity and iii to use pseudo loops the gga is a range space method that solves for both flows and heads it was the first algorithm in the field of hydraulics to exploit the block structure of the jacobian matrix to reduce the size of the key matrix in the linearization of the newton method the gga has gained popularity through its rapid convergence rate for a wide range of starting values this is the result of using the newton method on an optimizations problem that has a quadratic surface however it was reported by elhay and simpson 2011 that the gga fails catastrophically in the presence of zero flows in a wds when the head loss is modelled by the hazen williams formula regularization methods have been proposed by both elhay and simpson 2011 and gorev et al 2012 to deal with zero flows when the head loss is modelled by the hazen williams formula the gga as it was first proposed applied only for the wdss in which the head loss is modelled by the hazen williams formula where the resistance factor was independent of flow rossman 2000 extended the gga to allow the use of the darcy weisbach formula it has been pointed out in simpson and elhay 2010 however that rossman incorrectly treated the darcy weisbach resistance factor as independent of the flow they introduced the correct jacobian matrix to deal with this it has been demonstrated that once the correct jacobian matrix is used the quadratic convergence rate of the newton method is restored furthermore elhay and simpson 2011 reported that the gga does not fail in the presence of zero flows when the derivatives of the darcy weisbach jacobian matrix are correctly computed for laminar flows the co trees flow method ctm rahal 1995 is a null space method that solves for the co tree flows and spanning tree flows separately the ctm unlike the loop flow corrections method does not require the initial flows to satisfy continuity however it does require i the identification of the associated circulating graph ii the determination of the demands that are to be carried by tree branches iii finding the associated chain of branches closing a circuit for each co tree chord iv computing pseudo link head losses the reformulated co trees flow method rctm elhay et al 2014 is also a null space method that solves for co tree flows and spanning trees flows separately it represents a significant improvement on the ctm by removing requirements i to iv above it uses the schilders factorization schilders 2009 to permute the node arc incidence matrix into an invertible spanning tree block and a co tree block this permutation reduces the dimension of the schur complement from being equal to the number of junctions as in the gga to approximately the number of loops in the network abraham and stoianov 2015 proposed a novel idea to speed up the solution process when using a null space method to solve a wds network their idea exploits the fact that a significant proportion of run time is spent computing the head losses at the same time flows within some pipes exhibit negligible changes after a few iterations as a result there is no point in wasting computer resources to re compute the pipe head losses for the pipes that have little or no change in flows this partial update can be used to economize the computational complexity of the gga the rctm and their variations the forest core partitioning algorithm fcpa simpson et al 2014 speeds up the solution process in the case where the network has a significant forest component this algorithm permutes the system equations to partition the linear component of the problem which is the forest of the wds from the non linear component which is the core of the wds it can be viewed as a method that simplifies the problem by solving for the flows and the heads in the forest just once instead of at every iteration the fcpa reduces the number of pipes number of junctions and the dimension of the jacobian matrix in the core by the number of forest pipes or nodes the graph matrix partitioning algorithm gmpa deuerlein et al 2015 exploited the linear relationships between flows of the internal trees within the core and the flows of the corresponding super links after the forest of the network has been removed this was a major breakthrough the gmpa permutes the node arc incidence matrix in such a way that all of the nodes with degree two in the core can be treated as a group by partitioning the network this way the network can be solved by a global step which solves for the nodes with degree greater than two super nodes and the pipes which connect to them path chords and a local step which solves for the nodes with degree two interior path nodes and pipes connected to them path tree links 2 2 related implementations epanet 2 rossman 2000 is a widely used wds simulation package epanet 2 implemented the gga to provide a demand driven steady state solution of a wds the code for epanet 2 is in the public domain allowing many extensions to be developed currently available extensions include the implementation of a pressure dependent model cheung et al 2005 morley and tricarico 2008 siew and tanyimboh 2012 jun and guoping 2012 and a real time simulation capability vassiljev and koppel 2015 the epanet 2 implementation is not explicitly designed to necessarily be easy to understand or accommodate alternative solution methods guidolin et al 2010 the elements that are used in epanet 2 are stored by the variables that describe their graph properties for example 1 junctions reservoirs and tanks are stored as a c struct called node and 2 all valves pipes and pumps are stored as a c struct called link the abundant use of global variables limits the reusability and the possibility of the thread safe design guidolin et al 2010 consequently it is difficult to cleanly incorporate new solution methods into epanet 2 in a manner that allows a fair comparison of performance between these methods moreover because there are no clearly defined interfaces for the incorporation of third party code components in epanet 2 there is no guarantee that independently authored extensions will be easy to combine with each other in the absence of a popular easy to modify wds simulation platform there is currently no straightforward means for comparing different solution methods to date when new solution methods have been developed they have been compared using different research systems on different platforms with different implementation languages this leads to difficulty in comparing methods limits the reusability of code and creates a barrier for researchers to reproduce and replicate results to address these issues an extensible framework is required that allows implementation of new methodologies to be easily incorporated without an adverse impact on the performance of the rest of the system to this end a number of attempts have been made to implement an object oriented wrapper to encapsulate the epanet 2 solver opennet morley et al 2000 and ooten van zyl et al 2003 however these two systems were focused on providing more flexibility in the processing of input to the core epanet solver they did not address any issues relating to the solution process cwsnet a c implementation in object oriented style was produced by guidolin et al 2010 as an alternative to epanet 2 in cwsnet more attention has been given to the hydraulic elements of the wds network in addition cwsnet provides a pressure driven model and takes advantage of the computing power of the computer s graphics processing unit gpu however in cswnet the data structures representing the network are specialized to the solution methods that it uses these data structures are not easily adapted to work efficiently with the different traversal orders and graph algorithms used by newly developed solution methods in addition cwsnet still uses the same hydraulic solver and the same linear solver techniques implemented in epanet 2 guidolin et al 2010 to accommodate the deficiencies referred to above this paper presents a new hydraulic simulation toolkit wdslib wdslib is coded in c and incorporates a number of recently published techniques this toolkit offers users the ability to i choose from or modify different approaches and implementations of different wds model analyses and ii extend the toolkit to include new developments these features have been implemented using fast and modularized code a focus of attention in this research has been program correctness robustness and code efficiency the correctness of the toolkit has been validated against a reference matlab implementation the differences between all results intermediate and final produced by the c toolkit and the matlab implementation were shown to be smaller than 10 10 in the interest of toolkit robustness special attention has been paid to numerical processes to guard against avoidable failures such as loss of significance through subtractive cancellation and numerical errors such as division by zero the data structures and code libraries in wdslib are shared and all implementations have been carefully designed to ensure fairness of performance comparisons between algorithms wdslib uses a pluggable architecture where solution methods and their accompanying pre processing and post processing code are easily substituted in addition different numerical linear algebra techniques can be incorporated using a well defined interface this concludes the discussion of related work the mathematical formulations of the solution methods used in wdslib are presented in the next section 3 general wds demand driven steady state problem this section describes the general wds demand driven steady state problem the following starts with the basic definitions and notation followed by the system equations finally the relevant equations are shown for each of the different solution methods that are implemented in wdslib all variables are described in the nomenclature section in appendix e 3 1 definitions and notation consider a water distribution system that contains n p pipes n j junctions n r fixed head nodes and n f forest pipes and nodes the j t h pipe of the network can be characterized by its diameter d j length l j resistance factor r j the i t h node of the network has two properties its nodal demand d i and its elevation z i let q q 1 q 2 q n p t denote the vector of unknown flows h h 1 h 2 h n j t denote the vector of unknown heads r r 1 r 2 r n p t denote the vector of resistance factors d d 1 d 2 d n j t denote the vector of nodal demands e l e l 1 e l 2 e l n r t denote the vector of fixed head elevations the head loss exponent n is assumed to be dependent only on the head loss model n 2 for the darcy weisbach head loss model and n 1 852 for hazen williams head loss model the head loss within the pipe j which connects the node i and the node k is modelled by h i h k r j q j q j n 1 denote by g q ℝ n p n p a diagonal square matrix with element g j j r j q j n 1 for j 1 2 n p denote by f q ℝ n p n p a diagonal square matrix where the j th element on its diagonal f j j d d q j g j j q j a 1 is the full rank unknown head node arc incidence matrix where a 1 j i is used to represent the relationship between pipe j and node i a 1 j i 1 if pipe j enters node i a 1 j i 1 if pipe j leaves node i and a 1 j i 0 if pipe j is not connected to node i a 2 is the fixed head node arc incidence matrix where a 2 j i is used to represent the relationship between pipe j and fixed head node i a 2 j i 1 if pipe j enters fixed head node i a 2 j i 1 if pipe j leaves fixed head node i and a 2 j i 0 if pipe j is not connected to fixed head node i 3 2 system of equations the steady state flows and heads in the wds system are modelled by the demand driven model ddm continuity equation 1 and the energy conservation equation 2 1 a 1 t q d o 2 g q q a 1 h a 2 e l o which can be expressed as 3 g q a 1 a 1 t o q h a 2 e l d 0 where its jacobian matrix is 4 j f q a 1 a 1 t o and it is sometimes referred to as a nonlinear saddle point problem benzi et al 2005 this non linear system is normally solved by the newton method in which q m 1 and h m 1 are repeatedly computed from q m and h m by 5 f m q m a 1 a 1 t o q m 1 q m h m 1 h m g m q m a 1 h m a 2 e l a 1 t q m d until the relative differences q m 1 q m q m 1 and h m 1 h m h m 1 are sufficiently small 3 3 global gradient algorithm todini and pilati 1988 applied block elimination to eq 5 to yield a two step hazen william solver eq 6 for the heads and eq 7 for the flows 6 h m 1 u 1 n d a 1 t 1 n q m g 1 a 2 e l where u a 1 t g 1 a 1 7 q m 1 1 n n 1 q m g 1 a 2 e l a 1 h m 1 later simpson and elhay 2010 proposed 8 v h m 1 d a 1 t f 1 g f q m a 2 e l where v a 1 t f 1 a 1 9 q m 1 q m f 1 a 1 h m 1 f 1 g q m a 2 e l as the generalized equations that can be applied when the head loss is modelled by the hazen williams equation or the darcy weisbach equation the correct jacobian matrix with the formula for f when head loss is modelled by darcy weisbach equation can be found in simpson and elhay 2010 they showed that the use of the correct jacobian matrix restores the quadratic rate of convergence it is important to note that the gga as it was originally proposed solves the entire network by a non linear solver and this can include some unnecessary computations which can be avoided by exploiting the structural properties of the wds graph composition the methods described below exploit these structural properties to potentially improve the speed of the solution process 3 4 forest core partitioning associated with a wds is a graph g v e where the elements of v are the nodes vertices of the graph g and elements of e are the pipes links of the graph g the graph g can be partitioned into smaller subgraphs with special properties the special properties that are exploited in wdslib and their formulations are described in this subsection the concept of partitioning the wds network was proposed by deuerlein 2008 in order to simplify the wds solution process simpson et al 2014 extended the idea of the network partitioning of deuerlein 2008 and introduced the forest core partitioning algorithm fcpa which partitions the network into a treed component and a looped or core component the fcpa starts with a searching algorithm which identifies the forest subgraph g f v f e f in which s ℕ n f n p is the permutation matrix which identifies the pipes in the forest e f as distinct from the pipes in the core e c and t ℕ n f n j is the permutation matrix which identifies the nodes in the forest v f as distinct from the nodes in the core v c as distinct from the core subgraph g c v c e c in which p ℕ n p c n p is the permutation matrix for e c and c ℕ n j c n j is the permutation matrix for v c the flows of the pipes in the forest sq can be found directly from 10 sq t a 1 t s t 1 td finally once the iterative solution process for the core has stopped the forest heads can be found by solving a linear system 11 th s a 1 t t 1 s a 2 e l sg s t sq s a 1 c t ch 3 5 reformulated co tree flows method a graph with or without forest can be partitioned into two sub graphs a spanning tree subgraph and a complementary co tree subgraph the reformulated co tree flows method rctm elhay et al 2014 exploited the relationship between the spanning tree pipes and the co tree pipes the rctm starts with a spanning tree search algorithm which identifies a spanning tree subgraph g s t v e s t in which k 1 ℕ n p s t n p is the permutation matrix that identifies the pipes in the spanning tree e s t as distinct from the pipes in the co tree e c t r is the permutation matrix for the nodes which traverse the same sequence as the corresponding spanning tree pipes e s t and k 2 ℕ n p c t n p is the permutation matrix for the pipes in the complementary co tree edges e c t it is important to note that there are many choices of spanning tree for any cyclic graph the choice of spanning tree and co tree combination does not affect the correctness of the method by exploiting the relationship between the spanning tree and cotree elhay et al 2014 proposed the following equations to solve the wds for the flows first for the spanning tree flows q 1 m 1 12 q 1 m 1 l 21 t q 2 m r 1 t d and second for the co tree flows q 2 m 1 13 w m 1 q 2 m 1 l 21 f 1 m 1 g 1 m 1 q 1 m 1 f 2 m g 2 m q 2 m a 2 where r 1 k 1 a 1 r t r 2 k 2 a 1 r t l 21 r 2 r 1 t f 1 m k 1 f m k 1 t f 2 m k 2 f m k 2 t w m l 21 f 1 m 1 l 21 t f 2 m 1 g 1 m k 1 g m k 1 t g 2 m k 2 g m k 2 t a 1 k 1 a 2 e l a 2 l 21 k 1 a 2 e l k 2 a 2 e l note that in eq 12 an initial set of the co tree flows q 2 0 is needed to commence the solution process the heads are found after the iterative process of the rctm has been completed by using a linear solution process 14 r 1 h f 1 q 1 m 1 f 1 g 1 q 1 m a 1 this partitioning of the network equations reduces the size of the non linear component of the solver to n p n j the number of co tree elements in the network it has been proven by elhay et al 2014 that the rctm and the gga have identical iterative results and solutions if the same starting values are used however for the rctm the user only needs to set the initial flow estimates for the co tree pipes q 2 0 in contrast to gga where initial flow estimates are required for all pipes the flows in the complementary spanning pipes are generated by eq 12 in the rctm 4 wdslib structure wdslib is a wds simulation toolkit consisting of a set of c member functions which henceforth will be referred to just as functions that can be composed to solve for the steady state solution of a wds wdslib can be used for a once off simulation or a multi run simulation pre packaged driver code is provided to perform once off simulations using a choice of solver methods for a multi simulation setting where the use cases are very diverse the user is able to select the desired components of wdslib to compose and compile their own driver individual functions in wdslib are classified according to their role in the simulation workflow in any simulation workflow there will be functions that will only have to be executed once for example functions to read the input file or partition the network will only have to execute once at the start of the simulation or of all simulations likewise code to reverse the network partitioning and write simulation results will only have to execute once at the end of the simulation in this work these functions that are only required to be run once are called level one l 1 functions l 1 functions relate to network topology which is invariant for the whole simulation in a multi simulation setting certain functions will need to be run once for every hydraulic phase an example of such a module is the module making the initial guesses of pipe flow rates for the updated network configuration in this work these once per assessment functions are called level two l 2 functions finally for every hydraulic assessment there is a non linear iterative phase in the solution process the functions in this phase run many times for each hydraulic assessment until the stopping test has been satisfied examples of these include the functions to calculate the g and f matrices see eqs 3 and 4 and running the cholesky solver these iterative phase functions are called level three l 3 functions fig 1 illustrates the global structure of wdslib under a once off simulation setting and a multi run simulation setting the modular setup of wdslib allows each module to be run the minimum number of times determined by its simulation setting under the module structure described above a once off simulation setting can be viewed as a special case where the l 1 functions and l 2 functions are both run once note that after running the initial l 1 functions it is possible to run hydraulic assessments of the network in parallel this mode of execution might be used in a design setting such as using a genetic algorithm ga to optimize pipe diameter sizes l 1 and l 2 functions are classified into parts a and b according to whether they run before or after the lower level processing that they embed these functions are detailed in fig 2 the l 1 functions that run at the start of the simulation are called l 1 a functions these include the module 1 to read the configuration file 2 to parse the epanet i n p file using epanet programmer s toolkit 3 to partition the network and 4 to solve the linear part s of the network the corresponding l 1 b functions are run at the end of the simulation these include tasks such as reversing the network partitioning note that certain l 1 a functions require their corresponding l 1 b functions to be used for example the forest search module needs to be paired with the reverse fcpa permutation there is a similar structure for l 2 functions l 2 a functions are run at the start of each hydraulic assessment and l 2 b functions run at the end the functions that must be included for the fcpa method are denoted with single asterisks likewise the functions that must be included with the rctm method are denoted with double asterisks for these methods to work correctly all affiliated functions must be included in the simulation workflow note that it is also possible to run both the rctm and fcpa in the same workflow also note that the user cannot run both gga and rctm in the same workflow the user must choose between these solution methods table 1 provides a mapping from the function descriptions in fig 2 to the function names in wdslib in addition the dependencies between functions for each solution method are shown in table 1 a d the columns in each table list respectively the description of the function its name in wdslib the c class in which it appears its input parameters and its output values note that void is used in these latter two columns to denote that the function interacts with the class variables rather than through its parameters and return value examples of how these functions can be coded are presented in section 6 the key data access functions in wdslib are described next 4 1 getter and setter methods each class in wdslib has various methods available for setting the network parameters and retrieving the results of the wds network these methods allow the user to reconfigure the network before and during simulation runs the names of the setter methods all start with a prefix set and the names of the getter methods all start with a prefix get for example a user can set write to the diameter of pipe index to value by calling pipe setd index value and get read from the head of node index by calling h index result gethsol index a summary of the variables that can be read from read access through getter methods and written to write access through setter methods for each key classes is specified in table 2 this concludes the discussion of the broad structure of the wdslib package the next section describes key aspects of the implementation of the package 5 wdslib toolkit implementation this section outlines key implementation details of wdslib as previously mentioned the overall aim of wdslib is to provide a clearly structured flexible and extensible hydraulic simulation toolkit that allows testing evaluation and use in production settings of both existing and new wds solution techniques these aims require wdslib be implemented so that it is fast to execute flexible to configure robust to challenging input data cases and easy to understand and modify the following describes aspects of the implementation of wdslib that enable it to meet these requirements the next subsection describes the general considerations that informed the design of the whole toolkit this general discussion is followed by a summary of key improvements to the solution processes encoded in forest searching and spanning tree searching in the wdslib package 5 1 general capabilities and properties this sub section describes design aspects underpinning the utility and performance of wdslib in turn the following outlines measures taken to 1 maximize code clarity and modularity 2 increase the efficiency of memory access and storage 3 maximize numerical robustness 4 facilitate accurate timing of code execution and 5 maximize simulation speed for different settings 5 1 1 design considerations 1 modularity the modular design of wdslib is central to the evaluation and testing of different wds solution methods all methods have been defined to perform a single well defined function and each class can be compiled used and tested independently these features allow users to assemble the methods of interest from independently developed components to create a customized wds solution method in a reliable way wdslib s modular design also allows the users to profile the computation time of each individual component of an algorithm functions communicate through well defined interfaces and the function code has been factored to minimize development and testing cost this architecture allows customized simulation applications i to combine the functions of interest and ii to implement new solution algorithms to extend the functionalities of wdslib 5 1 2 design considerations 2 memory considerations care was taken to minimize the memory footprint of executing code in order to reduce memory requirements and prevent memory leaks in the interest of the toolkit efficiency and toolkit robustness reducing memory requirements allows the solution of larger wds problems for a given memory capacity in wdslib memory reduction was achieved through both using sparse matrix representations and the systematic allocation and deallocation of working structures in the c code the matrices used in wds simulation are often sparse with the density of the full node arc incidence matrix being only 2 n j consequently it is more efficient to store these matrices using sparse storage schemes which store only the non zero elements of the matrix and pointers to their locations davis et al 2014 it is important to note that the choice of a sparse matrix representation is made based on 1 the storage requirements of the matrix and 2 common search orders to column elements and row elements this latter factor means that the best format for sparse matrix representation varies with the preponderant orders of search row wise column wise or both employed by each method there is a number of common storage formats for sparse matrices compressed column storage ccs of duff et al 1989 compressed row storage crs block compressed column storage bccs block compressed row storage bcrs and adjacency lists as will be described shortly wdslib uses a modified adjacency list representation other implementations use a variety of storage schemes in epanet 2 the a 1 matrix is stored as two arrays of node indices which represent start nodes s n and the end nodes e n of each pipe the i t h entry of the s n and e n arrays represent the start node and end node of i t h pipe of the network this storage format minimizes the memory required to store the a 1 matrix because only the indices are required to be stored because a 1 i s n i 1 and a 1 i e n i 1 as shown in table 4 searching through rows pipes of matrices that are stored in this format is efficient however searching though the columns nodes is relatively inefficient this storage format is also used in cwsnet both ccs and crs are used in the fcpa implementation reported in simpson et al 2014 and the rctm implementation reported in elhay et al 2014 the partial update null space method abraham and stoianov 2015 used ccs the memory requirement for storing the a 1 matrix in ccs is 2 n n z n j 1 as shown in table 4 this storage scheme is fast for searching through columns nodes of matrices that are stored in ccs and slow for searching though rows pipes in wdslib a modified adjacency list described in table 3 tailored for wds hydraulic simulation is used an adjacency list for an undirected and unweighted graph consists of n j unordered lists for each vertex n i which contains all the vertices to which vertex n i is adjacent the network that is shown in fig 3 has one source three nodes and four pipes the adjacency list for this network can be described by four lists 2 3 1 4 1 4 2 3 each list describes the set of adjacent vertices of a vertex in the graph for example the first list 2 3 represents that the vertex 1 is adjacent to the vertex 2 and vertex 3 the adjacency list is modified to include a directed and weighted graph for wdslib this modified adjacency list for a directed and weighted wds graph consists of n j unordered lists for each vertex n i this list contains all the vertex and edge pairs to which vertex n i is adjacent for example the adjacency list for the same network that is shown in fig 3 can be described by four lists 2 1 3 4 1 1 4 2 1 4 4 3 2 2 3 3 each list represents the set of adjacent vertex and edge pair of a vertex in the graph for example the first list 2 1 3 4 describes that the vertex 1 is adjacent to the vertex 2 by edge 1 from vertex 1 to vertex 2 and the vertex 3 by edge 4 from vertex 3 to vertex 1 it is fast to search through both the rows and columns of the a 1 matrices that are stored in this format in addition to these optimized encodings both g and f are diagonal square matrices which require less storage when stored as vectors than in sparse matrix form the storage methods used for the variables in wdslib and their associated memory usage are given in table 5 as a final note to offer further assurance of the correctness of memory management in wdslib valgrind nethercote and seward 2007 a programming debugging tool was deployed during testing to detect any memory leaks memory corruption and double freeing 5 1 3 design considerations 3 numerical considerations the calculations in wdslib are performed in c under ieee standard double precision floating point arithmetic with machine epsilon ε m a c h 2 22 10 16 invariant terms and parameters in every equation were evaluated in advance and replaced by full 20 decimal digit accuracy constants intermediate results of calculations which are not easily accessible in epanet can be output at the user s request the stopping tolerance and stopping test can be set by the user either through the configuration file or by the relevant setter method in the parameter class in the construction of any numerical solver there are two primary dangers that are associated with floating point arithmetic that cannot be ignored i subtractive cancellation and ii overflow and underflow to avoid problems associated with these all input variables are scaled to a similar range to minimize the risk of avoidable computational inaccuracy or failure in floating point arithmetic it is important to note that unscaled or poorly scaled variables can unnecessarily confound a computation these scaled input variables are physically dimensionless which allows computation which is independent of the system of measurement units the variables that are provided in epanet input file for the package and their corresponding units in us customary and si units are shown in table 6 as with the input variables the system equations were modified to use dimensionless variables once the stopping test is satisfied the original variables are then recovered by reversing the initial scaling details of the scaling are shown in appendix a to help ensure that wdslib solution methods are both fast and reliable the sparse matrix operations are implemented using suitesparse davis et al 2014 suitesparse is a state of the art sparse arithmetic suite with exceptional performance from which the approximate minimum degree permutation amd and the sparse cholesky decomposition routines have been used 5 1 4 design considerations 4 timing considerations when executing wdslib each function reports the time spent in it by sampling wall clock time at the start and end of its execution although the overhead for sampling wall clock time is small there are at least two special considerations involved in the interpretation of these timings i the operating system at its own discretion may launch background processes for example anti virus software which will distort the timings and ii extrapolating the timing for multiple hydraulic simulations from a single analysis as may be required for example in a genetic algorithm or other evolutionary algorithm run must be done with care because the relationship between the different settings is not linear this concludes the discussion of the main considerations concerning the global design of wdslib in the following key details of the implementation of selected parts of the solution processes are described 5 2 key improvements to solution processes the wdslib implementation makes several improvements to extant solution processes this section focuses on the improvement of the network partitioning processes in fcpa and rctm 5 2 1 key optimization 1 improvements to partitioning in forest search the forest core partitioning algorithm fcpa in this paper is a substantial improvement over the algorithm of the original paper simpson et al 2014 specifically the original fcpa algorithm almost always required many sweeps of the columns nodes of the a 1 matrix in order to reduce the forest component down to the core component the refined algorithm exploits the adjacency list representation of the a 1 matrix so that the partitioning process is achieved in a single sweep this improves the speed of the partitioning process from being o a n p to o n p n f where a is the depth of the deepest tree component in the forest this can lead to substantial time savings in the case when a is relatively large the pseudo code for this refined forest search algorithm is shown in appendix b this algorithm traverses each tree component in turn from its leaf nodes which maximizes the locality of operations with respect to the graph representation in this algorithm a node is identified as a leaf node when its node degree is one every time a leaf node node k is identified the node pointer is moved to its adjacent node node k and the node degree of node k is reduced by one this process repeats if the adjusted node degree of node k is one otherwise node k is the root node for this tree and the algorithm progresses to the next tree in the forest 5 2 2 key optimization 2 improvements to spanning tree search the reformulated co tree flows method rctm in this paper is also a substantial improvement over the algorithm of the original paper elhay et al 2014 the original spanning tree search algorithm sweeps the rows of the a 1 matrix pipes in order to identify the singleton rows and their corresponding columns the spanning tree search in the original rctm required a sweep of the a 1 matrix to identify the next pipe in the spanning tree this algorithm is o n p n j which is relatively inefficient the pseudo code for the refined spanning tree forest search algorithm is shown in appendix c this improved algorithm takes as input the adjacency list describing the network and the pipe indexes of the core component of the network from the algorithm 1 if the fcpa is used in this algorithm all water sources are the starting point of the search process s n and marked as visited the nodes in s n are then used as to identify a spanning tree within the wds this is achieved by repeatedly finding all adjacent pairs node t and pipe s of and removing the first node in s n by using the adjacency list if the adjacent node t is not visited then node t is inserted into the spanning tree node vector s t n and search node vector s n and node t is marked as visited and pipe s is inserted into the spanning tree pipe vector s t p and pipe s is marked as visited if the adjacent node t is visited and the pipe s is not visited then the pipe s is inserted into the co tree pipe vector c t p and mark pipe s as visited this process is repeated until s n is empty the overall time complexity of this algorithm is o n p n j compared to o n p n j as mentioned above is the same as the best asymptotic complexity of breadth first search on a graph 6 example applications wdslib consists of a collection of functions which can be used either as a standalone application for fast one off simulations or as a library of software components that can be integrated into a user s own wds solution processes this section presents two example applications the first application is the setup for a basic one off simulation of a wds the second application described in subsection 6 2 presents an example using wdslib to implement a simple 1 1 evolutionary strategy beyer and schwefel 2002 1 1 es or more commonly 1 1 ea for sizing pipes in a wds 6 1 example 1 once off simulation the setup for wdslib as a standalone application is straightforward the user provides a configuration text file that specifies input and output filenames the name of the solver the desired output variables and simulation parameters these values have sensible defaults so the user can set up the solver by using a minimal configuration such as that shown in fig 4 by using this config file wdslib is configured to run a single hydraulic analysis of the network that is stored as say hanoi inp an epanet formatted input file under network sub directory using the reformulated co tree flows method with the forest core partitioning algorithm the full set of configuration parameters for once off simulations is shown in fig 11 in appendix d 6 2 example 2 a simple network design application as a minimalist example of the application of wdslib to a wds network design problem the following example uses 1 1 ea for optimally sizing pipe diameters this algorithm takes an existing network with randomly generated pipe diameters and optimizes the network to minimize cost subject to given pressure head constraints a 1 1 ea is a very simple evolutionary strategy beyer and schwefel 2002 which starts with a randomly generated individual in this case a wds diameter configuration this 1 1 ea then progresses by applying a mutation to a random pipe diameter size and then evaluating the new individual if the new individual is better it replaces the old network this process continues in a loop until a given number of evaluations is reached the c code for this example is shown in figs 5 8 if the name of the file containing this code is simpea cc then the simplest command to compile this code is g simpea cc o simpea llib lwdslib to run this code the user would type simpea config txt where config txt contains the same configuration text as for the previous example starting with the main function in fig 5 line 15 points to the config file specified by the command line the next two lines initialize the result and the simulation according to the configuration file this is followed by the l 1 a module to perform the user selected l 1 a functions line 19 generates the initial pipe diameters of the network and line 20 initializes the workspace for the mutated string line 23 sets the pipe diameters of the network line 24 evaluates the current network configuration the permutation and scaling for the current individual is reversed by l 1 b in line 25 of fig 5 line 26 calculates the fitness of the current network configuration by using the evaluate function in fig 8 this function applies a penalty for pressure head constraint violations and pipe material costs the body of the 1 1 ea is contained in the selection operator and mutation operator that follow lines 27 to 31 compare the string in the current generation with the current best string if the individual p1 as measured by evaluate is better than the individual p2 then p1 replaces p2 line 32 mutates the current network p2 using mutate see fig 7 the mutate function changes the diameter of a randomly selected pipe in the network to a randomly selected diameter chosen from a set of commercially available pipe diameters the mutated individual stored in the workplace p1 is used as the network configuration for the next iteration until the total number of generations is reached the user selected information about the best individual is outputted by dispresult in line 34 of fig 5 it should be noted that the algorithm described above can be used to design a simple wds but is not optimal in terms of speed of convergence other ea s such as genetic algorithms simpson et al 1994 will perform better however the above example has the advantage of simplicity and contains all the basic elements that a ga would use when interacting with wdslib this concludes the presentation of examples in this work the next section presents a case study that illustrates the performance of wdslib in a multi simulation setting 7 case study the following presents timing results for wdslib running the 1 1 ea described in the previous section the results below compare the four different solvers plus epanet2 note that detailed timings for once off simulations comparing the four methods can be found in qiu et al 2018 three networks were benchmarked in these experiments these were the n 1 n 3 and n 4 case study networks used in simpson et al 2014 table 7 summarizes the characteristics of these networks table 8 shows the results of the 1 1 ea from fig 5 for the gga gga with fcpa rctm rctm with fcpa and the epanet2 solvers for each of the four wdslib solvers above the timings are given for running the ea with and without the l1 modules hoisted out the main ea loop each experiment evaluates the wds network 100 000 times and the best performing method for each network is highlighted in bold the results show that the ea runs using wdslib are substantially faster than the runs using the epanet2 solver this is in part due to the fact that the epanet2 solver is designed as a standalone solver which does not facilitate lifting out of invariant computations from the ea loop as a demonstration of how the performance of an ea can be traced fig 9 shows the evolution of the fitness values of the n 1 network these traces were extracted from a file written to in line 30 in fig 5 as can be seen the cost and the pressure head violation terms drop during the ea run note that there will be considerable variation between 1 1 ea runs due to its highly stochastic nature 8 conclusions this paper has described wdslib a library for steady state hydraulic simulation of wds networks wdslib is fast modular and portable with implementation of several standard and recently published hydraulic solution methods we have outlined the supported solution methodologies the structure of the package and key aspects of wdslib s implementation two example applications have been presented including a design case study using a simple ea the ea results were benchmarked for different solvers demonstrating a substantial improvement in speed over the industry standard epanet2 package these benchmarks also have illustrated how the modular structure of wdslib could be exploited to speed up execution time in a design setting as well as providing a fast simulation platform for both once off and multi run simulations wdslib also provides a testbed for comparing different solution methods in different settings and network topologies as such wdslib is designed with a pluggable architecture which can extended to efficiently incorporate new solution methods as they are created this will enhance the capability of the research community to demonstrate the efficacy of new methods without having to re engineer the content of shared wdslib functions and data representations wdslib accepts the input file format used by epanet2 which allows for simple deployment the amount of coding required to use the provided solvers in wdslib is minimized by the use of a simple and extensible configuration file wdslib is currently limited to finding solutions of demand driven wds systems without control devices the development of software components to simulate control devices and pressure driven models is future work appendix a scaling in wdslib all input variables are scaled to a similar range to minimize the risk of avoidable computational inaccuracy or failures in floating point arithmetic the variables are scaled as following g ˆ g g 0 q ˆ q q 0 h ˆ h h 0 e ˆ l e l e l 0 d ˆ d d 0 where g h e l and d are the original input vectors g 0 h 0 e l 0 and d 0 are the scaling factors and g ˆ h ˆ e ˆ l and d ˆ are the scaled input vectors by substituting g g ˆ g 0 q q ˆ q 0 h h ˆ h 0 e l e ˆ l e l 0 d d ˆ d 0 eq 1 and eq 2 become 15 g 0 q 0 g ˆ q ˆ h 0 a 1 h ˆ e l 0 a 2 e l ˆ 0 16 q 0 a 1 t q ˆ d 0 d ˆ 0 eq 15 and eq 16 can be further simplified by introducing the notation a h 0 g 0 q 0 b e l 0 g 0 q 0 and c d 0 q 0 17 g ˆ q ˆ a a 1 h ˆ b a 2 e ˆ l 0 18 a 1 t q ˆ c d ˆ 0 with the following matrix form 19 g ˆ a 1 a 1 t o q ˆ a h ˆ b a 2 e ˆ l c d ˆ 0 finally the network can be solved by using eq 20 and eq 21 20 h ˆ m 1 1 a v 1 c d ˆ a 1 f ˆ 1 f ˆ g ˆ q ˆ m b a 2 e ˆ l 21 q ˆ m 1 q ˆ m a f ˆ 1 a 1 h ˆ m 1 f ˆ 1 g ˆ q ˆ m b a 2 e ˆ l choice of scaling factors the choice of the scaling factor despite much research is not well understood in this subsection a choice for each scaling factor based on the experience of the authors is recommended there are two types of variables and parameters that need to be scaled invariants and variants data sets that have very wide range of values can confound numerical accuracy as a result it may be preferable to scale the data to a narrower range the default scaling factor for each of the input data is chosen to be its maximum absolute value for example the scaling factor for demand is max d so that its values range from zero to one in contrast it is more difficult to choose a scaling factor a priori for values that vary between iterations variants this is because the range of variants can change as the iteration progresses as a result the intermediate and the final results might not be within the same range as the initial guesses there are two variants that need to be scaled q h a good choice of the scaling factor for the flow rate is d n s because the demand at each node must be satisfied by the water sources in the wds and it is a reasonable assumption that the all demands are equally carried by each pipe that is directly connected to a water source and a good choice of the scaling factor for nodal head is m a x e l because the maximum nodal head cannot exceed the maximum elevation head of the fixed nodes during the process of the computation the matrices g and f are scaled because their input variables are scaled for the darcy weisbach head loss model the diagonal elements of the matrix g are modelled by g j j d i a g 8 π 2 g l j d j 5 f j q j for j 1 n p where the friction factor f is modelled by the swamee jain formula 22 f j 64 r e j if r e j 2000 k 0 3 α k β k θ j r e j 2000 k if 2000 r e i 4000 0 25 log 2 θ j if r e j 4000 where θ j ε j 3 7 d j 5 74 r e j 0 9 and r e j 4 q j π ν d j for j 1 n p note that α k and β k are constant the values of which can be found in elhay and simpson 2011 in order to make sure the reynolds number a dimensionless variable is not affected by scaling ν ˆ d 0 q 0 ν is introduced reynolds number r e j becomes 4 q ˆ j q 0 π ν ˆ q 0 d 0 d ˆ j d 0 which can be further simplified to 4 q ˆ j π ν ˆ d ˆ j where the scaling factors are canceled in order to make sure f is also not affected by the scaling ε ˆ d 0 ε is introduced θ j becomes θ j ε ˆ j d 0 3 7 d ˆ j d 0 5 74 r e j 0 9 for j 1 n p which can be further simplified to θ j ε ˆ j 3 7 d ˆ j 5 74 r e j 0 9 for j 1 n p where the scaling factors are canceled it is evident that the friction factors remain the same because the values for the only two variables re and θ are unchanged finally diagonal elements of g can be expressed as g 0 g ˆ where g 0 8 π 2 g l 0 d 0 5 q 0 and g ˆ j j d i a g l ˆ j d ˆ j 5 f j q ˆ j for j 1 n p for the hazen williams head loss model the diagonal elements of the matrix g are modelled by g j j d i a g 10 67 l j c j 1 852 d j 4 871 q j n 1 for j 1 n p where c j is the hazen williams coefficient for the j th pipe the hazen williams coefficient unlike the friction factor in the darcy weisbach head loss model is independent of flow rate pipe wall condition and flow regimes which means it is an independent variable as a result the scaling factor for hazen williams g can simply be derived by g j j d i a g 10 67 l ˆ j l 0 c ˆ j 1 852 c 0 1 852 d ˆ j 4 871 d 0 4 871 q ˆ j n 1 q 0 n 1 and the equation for diagonal elements of g for hazen williams equation can be expressed as g d i a g g 0 g ˆ where g 0 10 67 l 0 c 0 1 852 d 0 4 871 q 0 n 1 and g j j d i a g l ˆ j c ˆ j 1 852 d ˆ j 4 871 q ˆ j n 1 for j 1 n p why scaling should be applied case 1 from estrada et al 2009 is used here to demonstrate the efficacy of using scaled variables the layout of the case 1 from estrada et al 2009 is given in fig 10 details of the pipe and node values are given in table 9 and table 10 fig 10 the layout of the case 1 from estrada et al 2009 fig 10 table 9 the nodal values of the case 1 from estrada et al 2009 table 9 node id elevation m demand lps reservoir a 10 reservoir b 10 junc c 0 2 7 junc d 0 0 table 10 the pipe values of the case 1 from estrada et al 2009 table 10 pipe id start node end node length m diameter mm pipe 1 reservoir a junc c 0 25 1200 pipe 2 reservoir b junc c 0 5 1200 pipe 3 junc d junc c 100 1200 pipe 4 reservoir a junc d 100 1200 running this example network from epanet and wdslib we get the following results table 11 the nodal heads found by using epanet and wdslib table 11 epanet wdslib node id head m junc c 10 10 junc d 10 10 table 12 the flows found by using epanet and wdslib table 12 epanet wdslib link id flow lps pipe 1 1 3446 1 5924 pipe 2 1 3446 1 5924 pipe 3 0 0108 0 0124 pipe 4 0 0108 0 0124 the energy residual and the continuity residual of the epanet result are 2 6973 and 5 3575 10 4 respectively these two residuals are too large which indicates an inaccurate solution in contrast the energy residual and the continuity residual of the wdslib result are 1 8723 10 12 and 1 7849 10 9 respectively which are both acceptable appendix b forest search algorithm algorithm 1 forest search algorithm image appendix c spanning tree search algorithm algorithm 2 spanning tree search algorithm image appendix d complete configuration file fig 11 a configuration file to run the rctm in wdslib fig 11 appendix e nomenclature acronyms ct co tree ctp co tree pipes dw darcy weisbach head loss formula fcpa forest core partitioning algorithm gga global gradient algorithm hw hazen william head loss formula wds water distribution system rctm reformulated co tree flows method nnz number of non zeros stp spanning tree pipes stn spanning tree nodes st spanning tree constants g gravitational acceleration constant ν kinematic viscosity of water fcpa variables c permutation matrix for the nodes in the core e c of the network e c set of core pipes edges in g c e f set of forest pipes edges in g f g c core subgraph g f forest subgraph p permutation matrix for the pipes in the core e c of the network s permutation matrix for the pipes in the forest e f of the network t permutation matrix for the nodes in the forest e f of the network v c set of core nodes vertices in g c v f set of forest nodes vertices in g f hydraulic variables for gga a 1 unknown head node arc incidence matrix a 2 fixed head node arc incidence matrix d vector of nodal demands d i demand of node i d vector of pipe diameters d j diameter of pipe j e l vector of fixed head nodes elevation heads e l k fixed head nodes elevation heads at node k e set of pipes in graph g en vector of end nodes e n j end nodes of pipe j f vector of darcy weisbach friction factors f j darcy weisbach friction factor of pipe j f diagonal matrix of generalized headloss derivatives when the headloss is modelled by either the hw and the dw f j j generalized headloss derivatives for pipe j g full wds graph g diagonal matrix with elements r j q j n 1 g j j r j q j n 1 h vectors of unknown heads h i heads at node i j jacobian matrix l 1 a functions run once before multiple simulation l 2 a functions run once before hydraulic assessment l 3 functions run every iteration l 2 b functions run once after hydraulic assessment l 1 b functions run once after multiple simulation l vector of pipe lengths l j length of pipe j n head loss exponent n f number of forest pipes and nodes n j number of junctions n p number of pipes n r number of fixed head nodes n s t number of st pipes and nodes n c t number of ct pipes q vector of unknown flows q j flow in pipe j re vector of reynolds numbers re j reynolds number for pipe j sn vector of start nodes s n j start nodes of pipe j u diagonal matrix of schur complement when headloss is modelled by hw v generalized schur complement when the headloss is modelled by both the hw and the dw v set of node in graph g z i elevation at node i α k interpolating spline coefficient β k interpolating spline coefficient θ vector as defined in eq 22 ε vector of pipe roughness heights ε j roughness height for pipe j rctm variables e s t set of st pipes edges e c t set of complementary ct pipes edges k 1 orthogonal permutation matrix for pipes in the st k 2 orthogonal permutation matrix for pipes in the ct l 21 a part of a basis for the null space of the permuted node arc incidence for the rctm r orthogonal permutation matrix for nodes in the st v s t set of st nodes vertices w schur complement for the rctm 
26267,wdslib is an extensible simulation toolkit for the steady state analysis of a water distribution system it includes a range of solution methods the forest core partitioning algorithm the global gradient algorithm the reformulated co tree flows method and also combinations of these methods wdslib has been created using a modularized object oriented design and implemented in the c programming language and has been validated against a reference matlab implementation wdslib has been designed i to avoid unnecessary computations by hoisting each of the modules to its appropriate level of repetition ii to perform the computations independently of measurement units using scaled variables iii to accurately report the execution time of all the modules in that it is possible to produce a timing model to parameterize multiple simulation times such as in an optimization using a genetic algorithm from a series of sampling simulation runs and iv to guard against numerical failures two example applications a once off simulation and a network optimization design application simulation are presented this toolkit can be used i to implement test and compare different solution methods ii to focus the research on the most time consuming parts of a solution method and iii to guide the choice of solution method when multiple simulation runs are required keywords water distribution system c toolkit object oriented design forest core partitioning algorithm reformulated co tree flows method global gradient algorithm open source software software availability name of the software wdslib version 1 0 available from https github com a1184182 wdslib language c supported system windows macos linux unix year first available 2018 1 introduction hydraulic simulation has been used to model water distribution systems wdss for several decades and is an essential tool for the design operation and management of wdss in industry and research hydraulic simulation allows users 1 to optimize wds network parameters such as pipe diameters in a design setting 2 to calibrate network parameters such as demand patterns in a conventional operational setting 3 to conduct real time monitoring and calibration of the network elements in a supervisory control and data acquisition scada operational setting and 4 to adjust control devices such as valves in a management setting in the design setting and both the above operational settings repeated hydraulic assessment is required on a network with fixed topology in the management setting repeated hydraulic assessment is required on a network with flexible network parameter settings with ever increasing network sizes and the need for real time management using a scada system it is important to have a robust simulation package which can be configured to be maximally efficient whatever the setting in the field of hydraulic simulation the system of equations can be formulated as a large and sparse non linear saddle point problem saddle point problems are defined to be a function of two variables say f q h and the objective is to find a pair of variables q h such that the value f q h is minimized with respect to the first variable and maximized with respect to the second variable or equivalently f q h f q h f q h q h there are several well known iteration methods for solving the non linear saddle point problem these include range space methods methods that operate in the subspace defined by the rows of the unknown head node arc incidences matrix global gradient algorithm todini and pilati 1988 null space methods methods that operate in the subspace defined by a null space that is orthogonal to the column of the unknown head node arc incidences matrix co tree flows formulation variations rahal 1995 elhay et al 2014 loop based methods methods that are based on the sum of head losses around a loop must be zero loop flow correction cross 1936 and pre and post processing methods forest core partitioning algorithm simpson et al 2014 domain decomposition diao et al 2014 network clustering perelman and ostfeld 2011 their relative performance in terms of speed rate of convergence and accuracy depends among other things on the topology of the target network size of the forest component the number of network loops and the density of these network loops it is difficult to evaluate the impact of these topology factors by only examining the incidence matrix that describes the pipe network connectivity as a result the best method to use for a particular network cannot be easily determined a priori moreover extra complexity is introduced when a multi run hydraulic assessment is required during a multi run hydraulic simulation the elapsed computation time of each method can be broken down into two parts the components that are only required to be performed once at the very beginning for the same network called the overhead and the components that are required to be carried out repeatedly for each separate run until the required number of iterations has been met called the hydraulic phase it is desirable to have a simulation platform given the different levels of repetition to implement these alternative algorithms efficiently equipped with such a platform a user would be able to easily benchmark the performance of alternative methods on a small number of evaluations for a given network and use that performance to inform the choice of algorithm to use for either a once off simulation setting or for a multiple simulation setting such as for an evolutionary algorithm ea this work describes an extensible wds simulation platform called wdslib wdslib is a numerically robust efficient and accurate c library that implements many wds simulation methods wdslib is written using a modular object oriented design which allows users to easily mix and interchange solution components thereby enabling users to avoid redundant computations it has been optimized to use sparse data structures which are oriented to the pattern of access required for each solution method wdslib has been validated for accuracy on a range of realistic benchmark water distribution networks against reference implementations and tested for speed the program accepts the input file formats of the industry standard epanet2 rossman 2000 toolkit and its performance is faster than epanet2 in all tested settings and benchmarks the remainder of this paper is structured as follows the next section describes related methodologies and implementations section 3 presents a general description of the wds demand driven steady state problem and the solution methods that are used in wdslib the tool kit structure is then given in section 4 this is followed in section 5 by the toolkit implementation details section 6 provides some examples of how the toolkit can be utilized in a simulation work flow the results are discussed in section 7 finally section 8 summarizes the results of this paper and describes future extensions to the toolkit 2 background this section describes related water distribution system network solution methods and implementations the first subsection describes solution methods including those used by wdslib this is followed by a description of currently available implementations and compares these with wdslib 2 1 related methods this research considers a water distribution model made up of energy conservation equations and the demand driven model continuity equations the hardy cross method cross 1936 also known as the loop flow corrections method is one of the oldest methods and uses successive approximations solving for each loop flow correction independently it is a method that was widely used for its simplicity at the time when it was introduced more than three decades later epp and fowler 1970 developed a computer version of cross s method and replaced the numerical solver with the newton method which solves for all loop flow corrections simultaneously however this method has not been widely used because of the need i to identify the network loops ii to find initial flows that satisfy continuity and iii to use pseudo loops the gga is a range space method that solves for both flows and heads it was the first algorithm in the field of hydraulics to exploit the block structure of the jacobian matrix to reduce the size of the key matrix in the linearization of the newton method the gga has gained popularity through its rapid convergence rate for a wide range of starting values this is the result of using the newton method on an optimizations problem that has a quadratic surface however it was reported by elhay and simpson 2011 that the gga fails catastrophically in the presence of zero flows in a wds when the head loss is modelled by the hazen williams formula regularization methods have been proposed by both elhay and simpson 2011 and gorev et al 2012 to deal with zero flows when the head loss is modelled by the hazen williams formula the gga as it was first proposed applied only for the wdss in which the head loss is modelled by the hazen williams formula where the resistance factor was independent of flow rossman 2000 extended the gga to allow the use of the darcy weisbach formula it has been pointed out in simpson and elhay 2010 however that rossman incorrectly treated the darcy weisbach resistance factor as independent of the flow they introduced the correct jacobian matrix to deal with this it has been demonstrated that once the correct jacobian matrix is used the quadratic convergence rate of the newton method is restored furthermore elhay and simpson 2011 reported that the gga does not fail in the presence of zero flows when the derivatives of the darcy weisbach jacobian matrix are correctly computed for laminar flows the co trees flow method ctm rahal 1995 is a null space method that solves for the co tree flows and spanning tree flows separately the ctm unlike the loop flow corrections method does not require the initial flows to satisfy continuity however it does require i the identification of the associated circulating graph ii the determination of the demands that are to be carried by tree branches iii finding the associated chain of branches closing a circuit for each co tree chord iv computing pseudo link head losses the reformulated co trees flow method rctm elhay et al 2014 is also a null space method that solves for co tree flows and spanning trees flows separately it represents a significant improvement on the ctm by removing requirements i to iv above it uses the schilders factorization schilders 2009 to permute the node arc incidence matrix into an invertible spanning tree block and a co tree block this permutation reduces the dimension of the schur complement from being equal to the number of junctions as in the gga to approximately the number of loops in the network abraham and stoianov 2015 proposed a novel idea to speed up the solution process when using a null space method to solve a wds network their idea exploits the fact that a significant proportion of run time is spent computing the head losses at the same time flows within some pipes exhibit negligible changes after a few iterations as a result there is no point in wasting computer resources to re compute the pipe head losses for the pipes that have little or no change in flows this partial update can be used to economize the computational complexity of the gga the rctm and their variations the forest core partitioning algorithm fcpa simpson et al 2014 speeds up the solution process in the case where the network has a significant forest component this algorithm permutes the system equations to partition the linear component of the problem which is the forest of the wds from the non linear component which is the core of the wds it can be viewed as a method that simplifies the problem by solving for the flows and the heads in the forest just once instead of at every iteration the fcpa reduces the number of pipes number of junctions and the dimension of the jacobian matrix in the core by the number of forest pipes or nodes the graph matrix partitioning algorithm gmpa deuerlein et al 2015 exploited the linear relationships between flows of the internal trees within the core and the flows of the corresponding super links after the forest of the network has been removed this was a major breakthrough the gmpa permutes the node arc incidence matrix in such a way that all of the nodes with degree two in the core can be treated as a group by partitioning the network this way the network can be solved by a global step which solves for the nodes with degree greater than two super nodes and the pipes which connect to them path chords and a local step which solves for the nodes with degree two interior path nodes and pipes connected to them path tree links 2 2 related implementations epanet 2 rossman 2000 is a widely used wds simulation package epanet 2 implemented the gga to provide a demand driven steady state solution of a wds the code for epanet 2 is in the public domain allowing many extensions to be developed currently available extensions include the implementation of a pressure dependent model cheung et al 2005 morley and tricarico 2008 siew and tanyimboh 2012 jun and guoping 2012 and a real time simulation capability vassiljev and koppel 2015 the epanet 2 implementation is not explicitly designed to necessarily be easy to understand or accommodate alternative solution methods guidolin et al 2010 the elements that are used in epanet 2 are stored by the variables that describe their graph properties for example 1 junctions reservoirs and tanks are stored as a c struct called node and 2 all valves pipes and pumps are stored as a c struct called link the abundant use of global variables limits the reusability and the possibility of the thread safe design guidolin et al 2010 consequently it is difficult to cleanly incorporate new solution methods into epanet 2 in a manner that allows a fair comparison of performance between these methods moreover because there are no clearly defined interfaces for the incorporation of third party code components in epanet 2 there is no guarantee that independently authored extensions will be easy to combine with each other in the absence of a popular easy to modify wds simulation platform there is currently no straightforward means for comparing different solution methods to date when new solution methods have been developed they have been compared using different research systems on different platforms with different implementation languages this leads to difficulty in comparing methods limits the reusability of code and creates a barrier for researchers to reproduce and replicate results to address these issues an extensible framework is required that allows implementation of new methodologies to be easily incorporated without an adverse impact on the performance of the rest of the system to this end a number of attempts have been made to implement an object oriented wrapper to encapsulate the epanet 2 solver opennet morley et al 2000 and ooten van zyl et al 2003 however these two systems were focused on providing more flexibility in the processing of input to the core epanet solver they did not address any issues relating to the solution process cwsnet a c implementation in object oriented style was produced by guidolin et al 2010 as an alternative to epanet 2 in cwsnet more attention has been given to the hydraulic elements of the wds network in addition cwsnet provides a pressure driven model and takes advantage of the computing power of the computer s graphics processing unit gpu however in cswnet the data structures representing the network are specialized to the solution methods that it uses these data structures are not easily adapted to work efficiently with the different traversal orders and graph algorithms used by newly developed solution methods in addition cwsnet still uses the same hydraulic solver and the same linear solver techniques implemented in epanet 2 guidolin et al 2010 to accommodate the deficiencies referred to above this paper presents a new hydraulic simulation toolkit wdslib wdslib is coded in c and incorporates a number of recently published techniques this toolkit offers users the ability to i choose from or modify different approaches and implementations of different wds model analyses and ii extend the toolkit to include new developments these features have been implemented using fast and modularized code a focus of attention in this research has been program correctness robustness and code efficiency the correctness of the toolkit has been validated against a reference matlab implementation the differences between all results intermediate and final produced by the c toolkit and the matlab implementation were shown to be smaller than 10 10 in the interest of toolkit robustness special attention has been paid to numerical processes to guard against avoidable failures such as loss of significance through subtractive cancellation and numerical errors such as division by zero the data structures and code libraries in wdslib are shared and all implementations have been carefully designed to ensure fairness of performance comparisons between algorithms wdslib uses a pluggable architecture where solution methods and their accompanying pre processing and post processing code are easily substituted in addition different numerical linear algebra techniques can be incorporated using a well defined interface this concludes the discussion of related work the mathematical formulations of the solution methods used in wdslib are presented in the next section 3 general wds demand driven steady state problem this section describes the general wds demand driven steady state problem the following starts with the basic definitions and notation followed by the system equations finally the relevant equations are shown for each of the different solution methods that are implemented in wdslib all variables are described in the nomenclature section in appendix e 3 1 definitions and notation consider a water distribution system that contains n p pipes n j junctions n r fixed head nodes and n f forest pipes and nodes the j t h pipe of the network can be characterized by its diameter d j length l j resistance factor r j the i t h node of the network has two properties its nodal demand d i and its elevation z i let q q 1 q 2 q n p t denote the vector of unknown flows h h 1 h 2 h n j t denote the vector of unknown heads r r 1 r 2 r n p t denote the vector of resistance factors d d 1 d 2 d n j t denote the vector of nodal demands e l e l 1 e l 2 e l n r t denote the vector of fixed head elevations the head loss exponent n is assumed to be dependent only on the head loss model n 2 for the darcy weisbach head loss model and n 1 852 for hazen williams head loss model the head loss within the pipe j which connects the node i and the node k is modelled by h i h k r j q j q j n 1 denote by g q ℝ n p n p a diagonal square matrix with element g j j r j q j n 1 for j 1 2 n p denote by f q ℝ n p n p a diagonal square matrix where the j th element on its diagonal f j j d d q j g j j q j a 1 is the full rank unknown head node arc incidence matrix where a 1 j i is used to represent the relationship between pipe j and node i a 1 j i 1 if pipe j enters node i a 1 j i 1 if pipe j leaves node i and a 1 j i 0 if pipe j is not connected to node i a 2 is the fixed head node arc incidence matrix where a 2 j i is used to represent the relationship between pipe j and fixed head node i a 2 j i 1 if pipe j enters fixed head node i a 2 j i 1 if pipe j leaves fixed head node i and a 2 j i 0 if pipe j is not connected to fixed head node i 3 2 system of equations the steady state flows and heads in the wds system are modelled by the demand driven model ddm continuity equation 1 and the energy conservation equation 2 1 a 1 t q d o 2 g q q a 1 h a 2 e l o which can be expressed as 3 g q a 1 a 1 t o q h a 2 e l d 0 where its jacobian matrix is 4 j f q a 1 a 1 t o and it is sometimes referred to as a nonlinear saddle point problem benzi et al 2005 this non linear system is normally solved by the newton method in which q m 1 and h m 1 are repeatedly computed from q m and h m by 5 f m q m a 1 a 1 t o q m 1 q m h m 1 h m g m q m a 1 h m a 2 e l a 1 t q m d until the relative differences q m 1 q m q m 1 and h m 1 h m h m 1 are sufficiently small 3 3 global gradient algorithm todini and pilati 1988 applied block elimination to eq 5 to yield a two step hazen william solver eq 6 for the heads and eq 7 for the flows 6 h m 1 u 1 n d a 1 t 1 n q m g 1 a 2 e l where u a 1 t g 1 a 1 7 q m 1 1 n n 1 q m g 1 a 2 e l a 1 h m 1 later simpson and elhay 2010 proposed 8 v h m 1 d a 1 t f 1 g f q m a 2 e l where v a 1 t f 1 a 1 9 q m 1 q m f 1 a 1 h m 1 f 1 g q m a 2 e l as the generalized equations that can be applied when the head loss is modelled by the hazen williams equation or the darcy weisbach equation the correct jacobian matrix with the formula for f when head loss is modelled by darcy weisbach equation can be found in simpson and elhay 2010 they showed that the use of the correct jacobian matrix restores the quadratic rate of convergence it is important to note that the gga as it was originally proposed solves the entire network by a non linear solver and this can include some unnecessary computations which can be avoided by exploiting the structural properties of the wds graph composition the methods described below exploit these structural properties to potentially improve the speed of the solution process 3 4 forest core partitioning associated with a wds is a graph g v e where the elements of v are the nodes vertices of the graph g and elements of e are the pipes links of the graph g the graph g can be partitioned into smaller subgraphs with special properties the special properties that are exploited in wdslib and their formulations are described in this subsection the concept of partitioning the wds network was proposed by deuerlein 2008 in order to simplify the wds solution process simpson et al 2014 extended the idea of the network partitioning of deuerlein 2008 and introduced the forest core partitioning algorithm fcpa which partitions the network into a treed component and a looped or core component the fcpa starts with a searching algorithm which identifies the forest subgraph g f v f e f in which s ℕ n f n p is the permutation matrix which identifies the pipes in the forest e f as distinct from the pipes in the core e c and t ℕ n f n j is the permutation matrix which identifies the nodes in the forest v f as distinct from the nodes in the core v c as distinct from the core subgraph g c v c e c in which p ℕ n p c n p is the permutation matrix for e c and c ℕ n j c n j is the permutation matrix for v c the flows of the pipes in the forest sq can be found directly from 10 sq t a 1 t s t 1 td finally once the iterative solution process for the core has stopped the forest heads can be found by solving a linear system 11 th s a 1 t t 1 s a 2 e l sg s t sq s a 1 c t ch 3 5 reformulated co tree flows method a graph with or without forest can be partitioned into two sub graphs a spanning tree subgraph and a complementary co tree subgraph the reformulated co tree flows method rctm elhay et al 2014 exploited the relationship between the spanning tree pipes and the co tree pipes the rctm starts with a spanning tree search algorithm which identifies a spanning tree subgraph g s t v e s t in which k 1 ℕ n p s t n p is the permutation matrix that identifies the pipes in the spanning tree e s t as distinct from the pipes in the co tree e c t r is the permutation matrix for the nodes which traverse the same sequence as the corresponding spanning tree pipes e s t and k 2 ℕ n p c t n p is the permutation matrix for the pipes in the complementary co tree edges e c t it is important to note that there are many choices of spanning tree for any cyclic graph the choice of spanning tree and co tree combination does not affect the correctness of the method by exploiting the relationship between the spanning tree and cotree elhay et al 2014 proposed the following equations to solve the wds for the flows first for the spanning tree flows q 1 m 1 12 q 1 m 1 l 21 t q 2 m r 1 t d and second for the co tree flows q 2 m 1 13 w m 1 q 2 m 1 l 21 f 1 m 1 g 1 m 1 q 1 m 1 f 2 m g 2 m q 2 m a 2 where r 1 k 1 a 1 r t r 2 k 2 a 1 r t l 21 r 2 r 1 t f 1 m k 1 f m k 1 t f 2 m k 2 f m k 2 t w m l 21 f 1 m 1 l 21 t f 2 m 1 g 1 m k 1 g m k 1 t g 2 m k 2 g m k 2 t a 1 k 1 a 2 e l a 2 l 21 k 1 a 2 e l k 2 a 2 e l note that in eq 12 an initial set of the co tree flows q 2 0 is needed to commence the solution process the heads are found after the iterative process of the rctm has been completed by using a linear solution process 14 r 1 h f 1 q 1 m 1 f 1 g 1 q 1 m a 1 this partitioning of the network equations reduces the size of the non linear component of the solver to n p n j the number of co tree elements in the network it has been proven by elhay et al 2014 that the rctm and the gga have identical iterative results and solutions if the same starting values are used however for the rctm the user only needs to set the initial flow estimates for the co tree pipes q 2 0 in contrast to gga where initial flow estimates are required for all pipes the flows in the complementary spanning pipes are generated by eq 12 in the rctm 4 wdslib structure wdslib is a wds simulation toolkit consisting of a set of c member functions which henceforth will be referred to just as functions that can be composed to solve for the steady state solution of a wds wdslib can be used for a once off simulation or a multi run simulation pre packaged driver code is provided to perform once off simulations using a choice of solver methods for a multi simulation setting where the use cases are very diverse the user is able to select the desired components of wdslib to compose and compile their own driver individual functions in wdslib are classified according to their role in the simulation workflow in any simulation workflow there will be functions that will only have to be executed once for example functions to read the input file or partition the network will only have to execute once at the start of the simulation or of all simulations likewise code to reverse the network partitioning and write simulation results will only have to execute once at the end of the simulation in this work these functions that are only required to be run once are called level one l 1 functions l 1 functions relate to network topology which is invariant for the whole simulation in a multi simulation setting certain functions will need to be run once for every hydraulic phase an example of such a module is the module making the initial guesses of pipe flow rates for the updated network configuration in this work these once per assessment functions are called level two l 2 functions finally for every hydraulic assessment there is a non linear iterative phase in the solution process the functions in this phase run many times for each hydraulic assessment until the stopping test has been satisfied examples of these include the functions to calculate the g and f matrices see eqs 3 and 4 and running the cholesky solver these iterative phase functions are called level three l 3 functions fig 1 illustrates the global structure of wdslib under a once off simulation setting and a multi run simulation setting the modular setup of wdslib allows each module to be run the minimum number of times determined by its simulation setting under the module structure described above a once off simulation setting can be viewed as a special case where the l 1 functions and l 2 functions are both run once note that after running the initial l 1 functions it is possible to run hydraulic assessments of the network in parallel this mode of execution might be used in a design setting such as using a genetic algorithm ga to optimize pipe diameter sizes l 1 and l 2 functions are classified into parts a and b according to whether they run before or after the lower level processing that they embed these functions are detailed in fig 2 the l 1 functions that run at the start of the simulation are called l 1 a functions these include the module 1 to read the configuration file 2 to parse the epanet i n p file using epanet programmer s toolkit 3 to partition the network and 4 to solve the linear part s of the network the corresponding l 1 b functions are run at the end of the simulation these include tasks such as reversing the network partitioning note that certain l 1 a functions require their corresponding l 1 b functions to be used for example the forest search module needs to be paired with the reverse fcpa permutation there is a similar structure for l 2 functions l 2 a functions are run at the start of each hydraulic assessment and l 2 b functions run at the end the functions that must be included for the fcpa method are denoted with single asterisks likewise the functions that must be included with the rctm method are denoted with double asterisks for these methods to work correctly all affiliated functions must be included in the simulation workflow note that it is also possible to run both the rctm and fcpa in the same workflow also note that the user cannot run both gga and rctm in the same workflow the user must choose between these solution methods table 1 provides a mapping from the function descriptions in fig 2 to the function names in wdslib in addition the dependencies between functions for each solution method are shown in table 1 a d the columns in each table list respectively the description of the function its name in wdslib the c class in which it appears its input parameters and its output values note that void is used in these latter two columns to denote that the function interacts with the class variables rather than through its parameters and return value examples of how these functions can be coded are presented in section 6 the key data access functions in wdslib are described next 4 1 getter and setter methods each class in wdslib has various methods available for setting the network parameters and retrieving the results of the wds network these methods allow the user to reconfigure the network before and during simulation runs the names of the setter methods all start with a prefix set and the names of the getter methods all start with a prefix get for example a user can set write to the diameter of pipe index to value by calling pipe setd index value and get read from the head of node index by calling h index result gethsol index a summary of the variables that can be read from read access through getter methods and written to write access through setter methods for each key classes is specified in table 2 this concludes the discussion of the broad structure of the wdslib package the next section describes key aspects of the implementation of the package 5 wdslib toolkit implementation this section outlines key implementation details of wdslib as previously mentioned the overall aim of wdslib is to provide a clearly structured flexible and extensible hydraulic simulation toolkit that allows testing evaluation and use in production settings of both existing and new wds solution techniques these aims require wdslib be implemented so that it is fast to execute flexible to configure robust to challenging input data cases and easy to understand and modify the following describes aspects of the implementation of wdslib that enable it to meet these requirements the next subsection describes the general considerations that informed the design of the whole toolkit this general discussion is followed by a summary of key improvements to the solution processes encoded in forest searching and spanning tree searching in the wdslib package 5 1 general capabilities and properties this sub section describes design aspects underpinning the utility and performance of wdslib in turn the following outlines measures taken to 1 maximize code clarity and modularity 2 increase the efficiency of memory access and storage 3 maximize numerical robustness 4 facilitate accurate timing of code execution and 5 maximize simulation speed for different settings 5 1 1 design considerations 1 modularity the modular design of wdslib is central to the evaluation and testing of different wds solution methods all methods have been defined to perform a single well defined function and each class can be compiled used and tested independently these features allow users to assemble the methods of interest from independently developed components to create a customized wds solution method in a reliable way wdslib s modular design also allows the users to profile the computation time of each individual component of an algorithm functions communicate through well defined interfaces and the function code has been factored to minimize development and testing cost this architecture allows customized simulation applications i to combine the functions of interest and ii to implement new solution algorithms to extend the functionalities of wdslib 5 1 2 design considerations 2 memory considerations care was taken to minimize the memory footprint of executing code in order to reduce memory requirements and prevent memory leaks in the interest of the toolkit efficiency and toolkit robustness reducing memory requirements allows the solution of larger wds problems for a given memory capacity in wdslib memory reduction was achieved through both using sparse matrix representations and the systematic allocation and deallocation of working structures in the c code the matrices used in wds simulation are often sparse with the density of the full node arc incidence matrix being only 2 n j consequently it is more efficient to store these matrices using sparse storage schemes which store only the non zero elements of the matrix and pointers to their locations davis et al 2014 it is important to note that the choice of a sparse matrix representation is made based on 1 the storage requirements of the matrix and 2 common search orders to column elements and row elements this latter factor means that the best format for sparse matrix representation varies with the preponderant orders of search row wise column wise or both employed by each method there is a number of common storage formats for sparse matrices compressed column storage ccs of duff et al 1989 compressed row storage crs block compressed column storage bccs block compressed row storage bcrs and adjacency lists as will be described shortly wdslib uses a modified adjacency list representation other implementations use a variety of storage schemes in epanet 2 the a 1 matrix is stored as two arrays of node indices which represent start nodes s n and the end nodes e n of each pipe the i t h entry of the s n and e n arrays represent the start node and end node of i t h pipe of the network this storage format minimizes the memory required to store the a 1 matrix because only the indices are required to be stored because a 1 i s n i 1 and a 1 i e n i 1 as shown in table 4 searching through rows pipes of matrices that are stored in this format is efficient however searching though the columns nodes is relatively inefficient this storage format is also used in cwsnet both ccs and crs are used in the fcpa implementation reported in simpson et al 2014 and the rctm implementation reported in elhay et al 2014 the partial update null space method abraham and stoianov 2015 used ccs the memory requirement for storing the a 1 matrix in ccs is 2 n n z n j 1 as shown in table 4 this storage scheme is fast for searching through columns nodes of matrices that are stored in ccs and slow for searching though rows pipes in wdslib a modified adjacency list described in table 3 tailored for wds hydraulic simulation is used an adjacency list for an undirected and unweighted graph consists of n j unordered lists for each vertex n i which contains all the vertices to which vertex n i is adjacent the network that is shown in fig 3 has one source three nodes and four pipes the adjacency list for this network can be described by four lists 2 3 1 4 1 4 2 3 each list describes the set of adjacent vertices of a vertex in the graph for example the first list 2 3 represents that the vertex 1 is adjacent to the vertex 2 and vertex 3 the adjacency list is modified to include a directed and weighted graph for wdslib this modified adjacency list for a directed and weighted wds graph consists of n j unordered lists for each vertex n i this list contains all the vertex and edge pairs to which vertex n i is adjacent for example the adjacency list for the same network that is shown in fig 3 can be described by four lists 2 1 3 4 1 1 4 2 1 4 4 3 2 2 3 3 each list represents the set of adjacent vertex and edge pair of a vertex in the graph for example the first list 2 1 3 4 describes that the vertex 1 is adjacent to the vertex 2 by edge 1 from vertex 1 to vertex 2 and the vertex 3 by edge 4 from vertex 3 to vertex 1 it is fast to search through both the rows and columns of the a 1 matrices that are stored in this format in addition to these optimized encodings both g and f are diagonal square matrices which require less storage when stored as vectors than in sparse matrix form the storage methods used for the variables in wdslib and their associated memory usage are given in table 5 as a final note to offer further assurance of the correctness of memory management in wdslib valgrind nethercote and seward 2007 a programming debugging tool was deployed during testing to detect any memory leaks memory corruption and double freeing 5 1 3 design considerations 3 numerical considerations the calculations in wdslib are performed in c under ieee standard double precision floating point arithmetic with machine epsilon ε m a c h 2 22 10 16 invariant terms and parameters in every equation were evaluated in advance and replaced by full 20 decimal digit accuracy constants intermediate results of calculations which are not easily accessible in epanet can be output at the user s request the stopping tolerance and stopping test can be set by the user either through the configuration file or by the relevant setter method in the parameter class in the construction of any numerical solver there are two primary dangers that are associated with floating point arithmetic that cannot be ignored i subtractive cancellation and ii overflow and underflow to avoid problems associated with these all input variables are scaled to a similar range to minimize the risk of avoidable computational inaccuracy or failure in floating point arithmetic it is important to note that unscaled or poorly scaled variables can unnecessarily confound a computation these scaled input variables are physically dimensionless which allows computation which is independent of the system of measurement units the variables that are provided in epanet input file for the package and their corresponding units in us customary and si units are shown in table 6 as with the input variables the system equations were modified to use dimensionless variables once the stopping test is satisfied the original variables are then recovered by reversing the initial scaling details of the scaling are shown in appendix a to help ensure that wdslib solution methods are both fast and reliable the sparse matrix operations are implemented using suitesparse davis et al 2014 suitesparse is a state of the art sparse arithmetic suite with exceptional performance from which the approximate minimum degree permutation amd and the sparse cholesky decomposition routines have been used 5 1 4 design considerations 4 timing considerations when executing wdslib each function reports the time spent in it by sampling wall clock time at the start and end of its execution although the overhead for sampling wall clock time is small there are at least two special considerations involved in the interpretation of these timings i the operating system at its own discretion may launch background processes for example anti virus software which will distort the timings and ii extrapolating the timing for multiple hydraulic simulations from a single analysis as may be required for example in a genetic algorithm or other evolutionary algorithm run must be done with care because the relationship between the different settings is not linear this concludes the discussion of the main considerations concerning the global design of wdslib in the following key details of the implementation of selected parts of the solution processes are described 5 2 key improvements to solution processes the wdslib implementation makes several improvements to extant solution processes this section focuses on the improvement of the network partitioning processes in fcpa and rctm 5 2 1 key optimization 1 improvements to partitioning in forest search the forest core partitioning algorithm fcpa in this paper is a substantial improvement over the algorithm of the original paper simpson et al 2014 specifically the original fcpa algorithm almost always required many sweeps of the columns nodes of the a 1 matrix in order to reduce the forest component down to the core component the refined algorithm exploits the adjacency list representation of the a 1 matrix so that the partitioning process is achieved in a single sweep this improves the speed of the partitioning process from being o a n p to o n p n f where a is the depth of the deepest tree component in the forest this can lead to substantial time savings in the case when a is relatively large the pseudo code for this refined forest search algorithm is shown in appendix b this algorithm traverses each tree component in turn from its leaf nodes which maximizes the locality of operations with respect to the graph representation in this algorithm a node is identified as a leaf node when its node degree is one every time a leaf node node k is identified the node pointer is moved to its adjacent node node k and the node degree of node k is reduced by one this process repeats if the adjusted node degree of node k is one otherwise node k is the root node for this tree and the algorithm progresses to the next tree in the forest 5 2 2 key optimization 2 improvements to spanning tree search the reformulated co tree flows method rctm in this paper is also a substantial improvement over the algorithm of the original paper elhay et al 2014 the original spanning tree search algorithm sweeps the rows of the a 1 matrix pipes in order to identify the singleton rows and their corresponding columns the spanning tree search in the original rctm required a sweep of the a 1 matrix to identify the next pipe in the spanning tree this algorithm is o n p n j which is relatively inefficient the pseudo code for the refined spanning tree forest search algorithm is shown in appendix c this improved algorithm takes as input the adjacency list describing the network and the pipe indexes of the core component of the network from the algorithm 1 if the fcpa is used in this algorithm all water sources are the starting point of the search process s n and marked as visited the nodes in s n are then used as to identify a spanning tree within the wds this is achieved by repeatedly finding all adjacent pairs node t and pipe s of and removing the first node in s n by using the adjacency list if the adjacent node t is not visited then node t is inserted into the spanning tree node vector s t n and search node vector s n and node t is marked as visited and pipe s is inserted into the spanning tree pipe vector s t p and pipe s is marked as visited if the adjacent node t is visited and the pipe s is not visited then the pipe s is inserted into the co tree pipe vector c t p and mark pipe s as visited this process is repeated until s n is empty the overall time complexity of this algorithm is o n p n j compared to o n p n j as mentioned above is the same as the best asymptotic complexity of breadth first search on a graph 6 example applications wdslib consists of a collection of functions which can be used either as a standalone application for fast one off simulations or as a library of software components that can be integrated into a user s own wds solution processes this section presents two example applications the first application is the setup for a basic one off simulation of a wds the second application described in subsection 6 2 presents an example using wdslib to implement a simple 1 1 evolutionary strategy beyer and schwefel 2002 1 1 es or more commonly 1 1 ea for sizing pipes in a wds 6 1 example 1 once off simulation the setup for wdslib as a standalone application is straightforward the user provides a configuration text file that specifies input and output filenames the name of the solver the desired output variables and simulation parameters these values have sensible defaults so the user can set up the solver by using a minimal configuration such as that shown in fig 4 by using this config file wdslib is configured to run a single hydraulic analysis of the network that is stored as say hanoi inp an epanet formatted input file under network sub directory using the reformulated co tree flows method with the forest core partitioning algorithm the full set of configuration parameters for once off simulations is shown in fig 11 in appendix d 6 2 example 2 a simple network design application as a minimalist example of the application of wdslib to a wds network design problem the following example uses 1 1 ea for optimally sizing pipe diameters this algorithm takes an existing network with randomly generated pipe diameters and optimizes the network to minimize cost subject to given pressure head constraints a 1 1 ea is a very simple evolutionary strategy beyer and schwefel 2002 which starts with a randomly generated individual in this case a wds diameter configuration this 1 1 ea then progresses by applying a mutation to a random pipe diameter size and then evaluating the new individual if the new individual is better it replaces the old network this process continues in a loop until a given number of evaluations is reached the c code for this example is shown in figs 5 8 if the name of the file containing this code is simpea cc then the simplest command to compile this code is g simpea cc o simpea llib lwdslib to run this code the user would type simpea config txt where config txt contains the same configuration text as for the previous example starting with the main function in fig 5 line 15 points to the config file specified by the command line the next two lines initialize the result and the simulation according to the configuration file this is followed by the l 1 a module to perform the user selected l 1 a functions line 19 generates the initial pipe diameters of the network and line 20 initializes the workspace for the mutated string line 23 sets the pipe diameters of the network line 24 evaluates the current network configuration the permutation and scaling for the current individual is reversed by l 1 b in line 25 of fig 5 line 26 calculates the fitness of the current network configuration by using the evaluate function in fig 8 this function applies a penalty for pressure head constraint violations and pipe material costs the body of the 1 1 ea is contained in the selection operator and mutation operator that follow lines 27 to 31 compare the string in the current generation with the current best string if the individual p1 as measured by evaluate is better than the individual p2 then p1 replaces p2 line 32 mutates the current network p2 using mutate see fig 7 the mutate function changes the diameter of a randomly selected pipe in the network to a randomly selected diameter chosen from a set of commercially available pipe diameters the mutated individual stored in the workplace p1 is used as the network configuration for the next iteration until the total number of generations is reached the user selected information about the best individual is outputted by dispresult in line 34 of fig 5 it should be noted that the algorithm described above can be used to design a simple wds but is not optimal in terms of speed of convergence other ea s such as genetic algorithms simpson et al 1994 will perform better however the above example has the advantage of simplicity and contains all the basic elements that a ga would use when interacting with wdslib this concludes the presentation of examples in this work the next section presents a case study that illustrates the performance of wdslib in a multi simulation setting 7 case study the following presents timing results for wdslib running the 1 1 ea described in the previous section the results below compare the four different solvers plus epanet2 note that detailed timings for once off simulations comparing the four methods can be found in qiu et al 2018 three networks were benchmarked in these experiments these were the n 1 n 3 and n 4 case study networks used in simpson et al 2014 table 7 summarizes the characteristics of these networks table 8 shows the results of the 1 1 ea from fig 5 for the gga gga with fcpa rctm rctm with fcpa and the epanet2 solvers for each of the four wdslib solvers above the timings are given for running the ea with and without the l1 modules hoisted out the main ea loop each experiment evaluates the wds network 100 000 times and the best performing method for each network is highlighted in bold the results show that the ea runs using wdslib are substantially faster than the runs using the epanet2 solver this is in part due to the fact that the epanet2 solver is designed as a standalone solver which does not facilitate lifting out of invariant computations from the ea loop as a demonstration of how the performance of an ea can be traced fig 9 shows the evolution of the fitness values of the n 1 network these traces were extracted from a file written to in line 30 in fig 5 as can be seen the cost and the pressure head violation terms drop during the ea run note that there will be considerable variation between 1 1 ea runs due to its highly stochastic nature 8 conclusions this paper has described wdslib a library for steady state hydraulic simulation of wds networks wdslib is fast modular and portable with implementation of several standard and recently published hydraulic solution methods we have outlined the supported solution methodologies the structure of the package and key aspects of wdslib s implementation two example applications have been presented including a design case study using a simple ea the ea results were benchmarked for different solvers demonstrating a substantial improvement in speed over the industry standard epanet2 package these benchmarks also have illustrated how the modular structure of wdslib could be exploited to speed up execution time in a design setting as well as providing a fast simulation platform for both once off and multi run simulations wdslib also provides a testbed for comparing different solution methods in different settings and network topologies as such wdslib is designed with a pluggable architecture which can extended to efficiently incorporate new solution methods as they are created this will enhance the capability of the research community to demonstrate the efficacy of new methods without having to re engineer the content of shared wdslib functions and data representations wdslib accepts the input file format used by epanet2 which allows for simple deployment the amount of coding required to use the provided solvers in wdslib is minimized by the use of a simple and extensible configuration file wdslib is currently limited to finding solutions of demand driven wds systems without control devices the development of software components to simulate control devices and pressure driven models is future work appendix a scaling in wdslib all input variables are scaled to a similar range to minimize the risk of avoidable computational inaccuracy or failures in floating point arithmetic the variables are scaled as following g ˆ g g 0 q ˆ q q 0 h ˆ h h 0 e ˆ l e l e l 0 d ˆ d d 0 where g h e l and d are the original input vectors g 0 h 0 e l 0 and d 0 are the scaling factors and g ˆ h ˆ e ˆ l and d ˆ are the scaled input vectors by substituting g g ˆ g 0 q q ˆ q 0 h h ˆ h 0 e l e ˆ l e l 0 d d ˆ d 0 eq 1 and eq 2 become 15 g 0 q 0 g ˆ q ˆ h 0 a 1 h ˆ e l 0 a 2 e l ˆ 0 16 q 0 a 1 t q ˆ d 0 d ˆ 0 eq 15 and eq 16 can be further simplified by introducing the notation a h 0 g 0 q 0 b e l 0 g 0 q 0 and c d 0 q 0 17 g ˆ q ˆ a a 1 h ˆ b a 2 e ˆ l 0 18 a 1 t q ˆ c d ˆ 0 with the following matrix form 19 g ˆ a 1 a 1 t o q ˆ a h ˆ b a 2 e ˆ l c d ˆ 0 finally the network can be solved by using eq 20 and eq 21 20 h ˆ m 1 1 a v 1 c d ˆ a 1 f ˆ 1 f ˆ g ˆ q ˆ m b a 2 e ˆ l 21 q ˆ m 1 q ˆ m a f ˆ 1 a 1 h ˆ m 1 f ˆ 1 g ˆ q ˆ m b a 2 e ˆ l choice of scaling factors the choice of the scaling factor despite much research is not well understood in this subsection a choice for each scaling factor based on the experience of the authors is recommended there are two types of variables and parameters that need to be scaled invariants and variants data sets that have very wide range of values can confound numerical accuracy as a result it may be preferable to scale the data to a narrower range the default scaling factor for each of the input data is chosen to be its maximum absolute value for example the scaling factor for demand is max d so that its values range from zero to one in contrast it is more difficult to choose a scaling factor a priori for values that vary between iterations variants this is because the range of variants can change as the iteration progresses as a result the intermediate and the final results might not be within the same range as the initial guesses there are two variants that need to be scaled q h a good choice of the scaling factor for the flow rate is d n s because the demand at each node must be satisfied by the water sources in the wds and it is a reasonable assumption that the all demands are equally carried by each pipe that is directly connected to a water source and a good choice of the scaling factor for nodal head is m a x e l because the maximum nodal head cannot exceed the maximum elevation head of the fixed nodes during the process of the computation the matrices g and f are scaled because their input variables are scaled for the darcy weisbach head loss model the diagonal elements of the matrix g are modelled by g j j d i a g 8 π 2 g l j d j 5 f j q j for j 1 n p where the friction factor f is modelled by the swamee jain formula 22 f j 64 r e j if r e j 2000 k 0 3 α k β k θ j r e j 2000 k if 2000 r e i 4000 0 25 log 2 θ j if r e j 4000 where θ j ε j 3 7 d j 5 74 r e j 0 9 and r e j 4 q j π ν d j for j 1 n p note that α k and β k are constant the values of which can be found in elhay and simpson 2011 in order to make sure the reynolds number a dimensionless variable is not affected by scaling ν ˆ d 0 q 0 ν is introduced reynolds number r e j becomes 4 q ˆ j q 0 π ν ˆ q 0 d 0 d ˆ j d 0 which can be further simplified to 4 q ˆ j π ν ˆ d ˆ j where the scaling factors are canceled in order to make sure f is also not affected by the scaling ε ˆ d 0 ε is introduced θ j becomes θ j ε ˆ j d 0 3 7 d ˆ j d 0 5 74 r e j 0 9 for j 1 n p which can be further simplified to θ j ε ˆ j 3 7 d ˆ j 5 74 r e j 0 9 for j 1 n p where the scaling factors are canceled it is evident that the friction factors remain the same because the values for the only two variables re and θ are unchanged finally diagonal elements of g can be expressed as g 0 g ˆ where g 0 8 π 2 g l 0 d 0 5 q 0 and g ˆ j j d i a g l ˆ j d ˆ j 5 f j q ˆ j for j 1 n p for the hazen williams head loss model the diagonal elements of the matrix g are modelled by g j j d i a g 10 67 l j c j 1 852 d j 4 871 q j n 1 for j 1 n p where c j is the hazen williams coefficient for the j th pipe the hazen williams coefficient unlike the friction factor in the darcy weisbach head loss model is independent of flow rate pipe wall condition and flow regimes which means it is an independent variable as a result the scaling factor for hazen williams g can simply be derived by g j j d i a g 10 67 l ˆ j l 0 c ˆ j 1 852 c 0 1 852 d ˆ j 4 871 d 0 4 871 q ˆ j n 1 q 0 n 1 and the equation for diagonal elements of g for hazen williams equation can be expressed as g d i a g g 0 g ˆ where g 0 10 67 l 0 c 0 1 852 d 0 4 871 q 0 n 1 and g j j d i a g l ˆ j c ˆ j 1 852 d ˆ j 4 871 q ˆ j n 1 for j 1 n p why scaling should be applied case 1 from estrada et al 2009 is used here to demonstrate the efficacy of using scaled variables the layout of the case 1 from estrada et al 2009 is given in fig 10 details of the pipe and node values are given in table 9 and table 10 fig 10 the layout of the case 1 from estrada et al 2009 fig 10 table 9 the nodal values of the case 1 from estrada et al 2009 table 9 node id elevation m demand lps reservoir a 10 reservoir b 10 junc c 0 2 7 junc d 0 0 table 10 the pipe values of the case 1 from estrada et al 2009 table 10 pipe id start node end node length m diameter mm pipe 1 reservoir a junc c 0 25 1200 pipe 2 reservoir b junc c 0 5 1200 pipe 3 junc d junc c 100 1200 pipe 4 reservoir a junc d 100 1200 running this example network from epanet and wdslib we get the following results table 11 the nodal heads found by using epanet and wdslib table 11 epanet wdslib node id head m junc c 10 10 junc d 10 10 table 12 the flows found by using epanet and wdslib table 12 epanet wdslib link id flow lps pipe 1 1 3446 1 5924 pipe 2 1 3446 1 5924 pipe 3 0 0108 0 0124 pipe 4 0 0108 0 0124 the energy residual and the continuity residual of the epanet result are 2 6973 and 5 3575 10 4 respectively these two residuals are too large which indicates an inaccurate solution in contrast the energy residual and the continuity residual of the wdslib result are 1 8723 10 12 and 1 7849 10 9 respectively which are both acceptable appendix b forest search algorithm algorithm 1 forest search algorithm image appendix c spanning tree search algorithm algorithm 2 spanning tree search algorithm image appendix d complete configuration file fig 11 a configuration file to run the rctm in wdslib fig 11 appendix e nomenclature acronyms ct co tree ctp co tree pipes dw darcy weisbach head loss formula fcpa forest core partitioning algorithm gga global gradient algorithm hw hazen william head loss formula wds water distribution system rctm reformulated co tree flows method nnz number of non zeros stp spanning tree pipes stn spanning tree nodes st spanning tree constants g gravitational acceleration constant ν kinematic viscosity of water fcpa variables c permutation matrix for the nodes in the core e c of the network e c set of core pipes edges in g c e f set of forest pipes edges in g f g c core subgraph g f forest subgraph p permutation matrix for the pipes in the core e c of the network s permutation matrix for the pipes in the forest e f of the network t permutation matrix for the nodes in the forest e f of the network v c set of core nodes vertices in g c v f set of forest nodes vertices in g f hydraulic variables for gga a 1 unknown head node arc incidence matrix a 2 fixed head node arc incidence matrix d vector of nodal demands d i demand of node i d vector of pipe diameters d j diameter of pipe j e l vector of fixed head nodes elevation heads e l k fixed head nodes elevation heads at node k e set of pipes in graph g en vector of end nodes e n j end nodes of pipe j f vector of darcy weisbach friction factors f j darcy weisbach friction factor of pipe j f diagonal matrix of generalized headloss derivatives when the headloss is modelled by either the hw and the dw f j j generalized headloss derivatives for pipe j g full wds graph g diagonal matrix with elements r j q j n 1 g j j r j q j n 1 h vectors of unknown heads h i heads at node i j jacobian matrix l 1 a functions run once before multiple simulation l 2 a functions run once before hydraulic assessment l 3 functions run every iteration l 2 b functions run once after hydraulic assessment l 1 b functions run once after multiple simulation l vector of pipe lengths l j length of pipe j n head loss exponent n f number of forest pipes and nodes n j number of junctions n p number of pipes n r number of fixed head nodes n s t number of st pipes and nodes n c t number of ct pipes q vector of unknown flows q j flow in pipe j re vector of reynolds numbers re j reynolds number for pipe j sn vector of start nodes s n j start nodes of pipe j u diagonal matrix of schur complement when headloss is modelled by hw v generalized schur complement when the headloss is modelled by both the hw and the dw v set of node in graph g z i elevation at node i α k interpolating spline coefficient β k interpolating spline coefficient θ vector as defined in eq 22 ε vector of pipe roughness heights ε j roughness height for pipe j rctm variables e s t set of st pipes edges e c t set of complementary ct pipes edges k 1 orthogonal permutation matrix for pipes in the st k 2 orthogonal permutation matrix for pipes in the ct l 21 a part of a basis for the null space of the permuted node arc incidence for the rctm r orthogonal permutation matrix for nodes in the st v s t set of st nodes vertices w schur complement for the rctm 
26268,despite recent advances in quantifying land use cover change lucc transition potential transition rules are often not transparent and uncertainty is rarely made explicit here we introduce dotrules a dictionary of trusted rules as a transparent alternative to calculate transition potential in cellular automata models rules relate lucc variables to the observed historical changes shannon entropy is calculated to assess the uncertainty of each rule and the most trusted rules are used to project future lucc dotrules produces rule level uncertainty estimates which can be mapped in a case study of the ahvaz region of iran the overall accuracy of lucc simulation calibrated using dotrules was very similar to simulations calibrated with the state of the art random forest but dotrules provides a more transparent approach where transition rule information and uncertainty can be readily accessed and interpreted the results demonstrate that dotrules has potential to derive new insights into lucc processes keywords entropy land use cover change uncertainty urban planning modelling simulation 1 introduction cellular automata ca were conceptually established by john von neumann 1903 1957 during the 1950s due to their simplicity and capacity to simulate spatial patterns ca have rapidly gained popularity as a tool for modelling spatial dynamics of many environmental phenomena such as plant population dynamics xu et al 2010 forest fire spread zheng et al 2017 slope failure liucci et al 2017 debris flow d ambrosio et al 2003 d ambrosio et al 2006 urban sprawl mustafa et al 2018a van vliet et al 2009 land use cover change lucc hewitt and díaz pacheco 2017 hewitt et al 2014 white and engelen 1997 and more though cellular automata can handle very complex spatial situations for modelling environmental phenomena their conceptual basis is straightforward a cellular automaton consists of a large number of cells which can change their state according to specific rules in many applications such as modelling fire spread i e bushfire or forest fire urban sprawl modelling and specially lucc simulation a set of neighbourhood and suitability values are defined reflecting the influence of external factors affecting the state transitions for each cell finally there is a set of rules defining transition potential of a cell from one state to another in terms of lucc models the transition demand and the transition potential are typically the two main requirements white and engelen 1993 for model implementation using cellular automata first historical rates of land use change are calculated which are used to calibrate the total amount of land use change occurring in each time step this is termed transition demand second the spatially explicit probabilities of land use change or transition potential are calculated transition potential represents the behavioural propensities of the actors determining land use change and is defined based on the inferred logic from a set of transition rules transition rules are general structures that offer an easily understandable and transparent way to find the most reliable land use class allocation russell et al 2003 in practice transition rules capture the relationships between land use and a suite of independent predictor variables the effectiveness of cellular automata land use models in informing land use planning depends upon the efficient extraction of reliable and transparent transition rules han et al 2015 hewitt et al 2014 numerous machine learning and statistical methods have been used to calculate land use transition rules and map transition potential for use in cellular automata land use models basse et al 2014 berberoğlu et al 2016 clarke et al 1997 ku 2016 liu et al 2014 mustafa et al 2017 2018a 2018b methods frequently applied include association rule learning al kheder et al 2008 liu and jiang 2011 artificial neural network basse et al 2014 li et al 2013 maximum margin rienow and goetzke 2015 yang et al 2008 instance based castilla and blas 2008 li et al 2015 regression ku 2016 long et al 2014 decision tree ballestores jr and qiu 2012 basse et al 2016 and probabilistic arsanjani et al 2011 vaz et al 2015 methods others such as evolutionary deep learning reinforcement learning dimensionality reduction bayesian and regularisation methods have been used less frequently kamusoko and gamba 2015 li et al 2015 verstegen et al 2014 zhang et al 2008 each of these methods employs structurally different numerical formulations which affect the accuracy and transparency of automata based lucc models few of these methods facilitate the transparent extraction of transition rules and their corresponding uncertainty transparent transition rules enable both an enhanced ability to appreciate the relationships between land use change or other similar environmental phenomena and predictor variables this is necessary to understand model structure hence the inferred logic of model structure captured in transition rules can be visualised dissected and deciphered and can be generalized and applied to address other similar problems this can be invaluable for informing error checking and enabling model validation the explicit identification of transition rules can also help understand the nature of the major land use transitions tayyebi et al 2014 incomplete information about transition rules and their uncertainty may impede the understanding of land use change processes koomen and borsboom van beurden 2012 pozoukidou 2005 however many approaches have been subject to limitations in their ability to clearly identify transition rules and have been criticised as being black boxes islam et al 2018 li and yeh 2002 qiu and jensen 2004 waddell 2002 for instance kamusoko and gamba 2015 compared cellular automata calibrated using random forest support vector machine and logistic regression the higher performance of the random forest model was attributed to the relatively accurate transition potential maps however apart from logistic regression which is capable of revealing the relative global contribution of each variable to the land use change process none of these methods is capable of implementing accessible and transparent sets of transition rules at the pixel level transparent approaches are more desirable than black box approaches for transition rule detection even if this preference trades off some performance tseng et al 2008 uzuner et al 2009 this includes but is not limited to lucc plans where it is helpful to provide insight into the internal decision making process of algorithms for a better interpretation of the result similar requirements also exist for other environmental applications aimed at improving the quality of management plans in the context of natural hazards lai et al 2016 royston et al 2012 shadman roodposhti et al 2016 water treatment gibert et al 2010 soil erosion adinarayana et al 1999 and farming systems moore et al 2014 simulated change and persistence in land use patterns need to be interpreted and validated via a better understanding of uncertainty both at the rule level and spatially while a few studies have successfully mapped the spatial distribution of classification uncertainty bryan et al 2009 khatami et al 2017 providing estimates of classification uncertainty at rule level i e for each rule may also provide complementary insights into land use change processes however rather than providing uncertainty estimates land use modelling studies typically report on the accuracy of lucc analyses using global methods such as confusion matrices and the kappa index congalton 1991 confusion matrices are usually calculated to allow for global measures of accuracy i e overall accuracy to be generated but which lack the ability to quantify the spatial distribution of classification accuracy tsutsumida and comber 2015 similarly the kappa index does not consider the disagreement in classification accuracy pontius jr and millones 2011 stein et al 2005 global measures of accuracy require ground truth data however it is a time consuming process to prepare land use ground truth maps for every simulated map of land use in addition there is no ground truth for future land use scenarios and therefore these methods provide no basis for quantifying confidence in simulated future land use maps explicit rule level uncertainty estimates can assist land use planners and policymakers to understand the uncertainties associated with major land use transitions the mapping of these uncertainties can identify locations where simulated land use allocation occurs with high confidence or areas of low confidence which can both be useful in land use planning here we develop a new algorithm dotrules dictionary of trusted rules for modelling land use transition potential for application in automata based lucc models which features the transparent identification of transition rules and quantifies their uncertainty it also enables the mapping of corresponding land use transition uncertainties in dotrules the uncertainty of transition rules is quantified using shannon entropy dissecting transition rules and their corresponding uncertainty enables the better understanding of the core rules governing major land use cover dynamics which is useful for informing land use planning we also show that the uncertainty values can be applied as an approximation of simulation accuracy we describe the dotrules algorithm and demonstrate its application to the ahvaz region iran we quantify the uncertainty of lucc simulation calibrated using dotrules to calculate land use transition rules and compare the results with simulations based on random forest transition rule detection we discuss the advantages and disadvantages of the new approach for lucc modelling more generally and the application of dotrules in the calculation of transition potential maps for lucc models to broader environmental processes where it is necessary to understand the direction and magnitude of state transitions 2 description of dotrules dotrules is a moderate speed rule based algorithm for calculating transition potential in lucc according to a dictionary of trusted rules where categorical discrete data are involved it is similar to the random forest algorithm breiman 1996 2001 insofar as rule sets are used to select the mode response i e most frequent land use class among every available potential response variable however instead of generating random trees dotrules operates by constructing many transition rules within various rule sets derived from a training dataset with land use assigned to the most frequently occurring class the rule construction process is fulfilled using concatenation of discrete predictor variables and the entropy of each rule is then calculated as an estimate of accuracy the dotrules procedure was implemented in r r core team 2017 and consists of the following six steps 2 1 step 1 assembling the data training data is represented by a set of grid cells i i 1 i 2 i m each grid cell i in i has a value x ij for each of the independent predictor variables or criteria j j 1 j 2 j n where in this study there are nine independent predictor variables table 1 criteria are discretised predictor variables which can be derived from either native categorical data e g land use class or classified continuous data e g distance to road thus for each criterion x ij can adopt one of a fixed set of possible classes specific to that criterion which we represent as the set h for every j in j table 1 note that each criterion j will have a different set of classes h but for clarity here we do not index h by j each grid cell i has a corresponding land use class l i which are also discrete semantic attributes from the set of five land uses l u urban a agriculture b bare lands r roads and w water bodies 2 2 step 2 calculating shannon entropy and prioritising criteria for each criterion j in j we calculated the frequency of grid cells i within each criterion class h in h occurring within each land use class l in l represented as p l h j 1 p l h j i i x i j h l i l i i x i j h for l in l h in h and j in j note that are iverson brackets where q quantity equals 1 if true and 0 if false the term i i x i j h is the number of grid cells in criterion class h in information theory entropy is the quantitative measure of system disorder instability and uncertainty shannon 2001 the shannon entropy is the quantitative measure of uncertainty in this study here we calculate the entropy of land use class occurrences within each criteria class h across all criteria j 2 e h j l l p l h j ln p l h j the entropy of each criterion was then calculated as the average entropy of its classes e h j weighted by the proportion of cells in each class 3 e j h h e h j i i x i j h i where i is the set of grid cells in the training dataset the criteria were then ranked and prioritised according to their average entropy e j with higher priority criteria being those with the lower entropy represented by the ordered set of criteria priority j 2 3 step 3 creating a rule set we then concatenate grid cell criteria values x ij as per criteria priority j in order to form a rule set d the concatenation of two or more characters is the string formed by them in a series i e the concatenation of 12 a7 and 5 is 12a75 equation 4 illustrates the grid cell values for criteria ranked in order of priority i e lowest entropy concatenated for each grid cell row i thereby creating a unique rule for each grid cell in the training dataset 4 d x 11 x 21 x i 1 x i 1 x 12 x 22 x i 2 x i 2 x 1 j x 2 j x i j x i j x 1 j x 2 j x i j x i j x 11 x 21 x i 1 x i 1 x 12 x 22 x i 2 x i 2 x 1 j x 2 j x i j x i j x 1 j x 2 j x i j x i j d 1 d 2 d i d i note that following the concatenation and extraction of rules every rule within the dictionary has maintained its single land use class l i l we then aggregate duplicate rules where grid cells have exactly the same values for all criteria leaving a parsimonious new rule set of unique rules d derived by aggregating d the frequency of occurrence of all potential land use classes l in l is then calculated for each unique rule d in d 5 l 1 l 2 l d f l 1 u f l 1 a f l 1 b f l 1 r f l 1 w f l 2 u f l 2 a f l 2 b f l 2 r f l 2 w f l d u f l d a f l d b f l d r f l d w the land use class i e u a b r w from set l with the highest frequency i e the mode is then assigned to each corresponding unique rule d 2 4 step 4 calculating and mapping the uncertainty of land use prediction considering every unique rule d from our rule set d a shannon entropy value is then calculated based on the frequencies of each possible land use class equation 5 using equation 2 this can inform both the spatial distribution of uncertainty in land use predictions and provides transparent transition rules for informing land use planning the spatial distribution of uncertainty is quantified and mapped by the entropy of each unique rule back to the grid cells corresponding to each rule each grid cell is then allocated to the land use class with the highest frequency for its corresponding rule 2 5 step 5 classify land use of test dataset according to the dictionary of trusted rules above we describe the process of creating the dictionary of trusted rules and allocating the most likely land use class for each rule based on frequency the land use class can now be predicted for the rest of the study area dataset i e the test dataset to do this we follow the same procedure to set up a rule set for the test dataset we then match each test data rule with its equivalent in the dictionary of trusted rules using many to one matching i e matching many rules concatenated from test grid cells to individual trusted rules calculated using the training dataset and allocate the most likely land use to each test data rule this can then be mapped back to the grid cell level as each rule in the test dataset corresponds to a grid cell 2 6 step 6 handling null values finally as there is always a possibility of encountering null values using the dotrules approach where new cells in the test dataset present combinations of criteria states not encountered in the training data here using the same training and test sample we will sequentially exclude the least informative i e highest entropy and lowest ranked independent predictor variables or criteria j from our analysis and re execute dotrules step 3 to 5 this generates a second rule set i e a sub rule set which contains fewer unique rules d in the corresponding d and null records as it contains fewer criteria classes we then repeat in developing some new sub rule sets until all null values in our primary rule set are covered by some corresponding sub rules among secondary sub rule sets the best sub rule to replace a null matching rule in our test rule set is the one with the lowest entropy while those rules with higher entropy values higher than the specified threshold are eliminated 3 methods 3 1 study area the study area was the ahvaz region of south west iran ahvaz city is the capital and largest city of khuzestan province fig 1 the population of ahvaz increased from 334 399 to 1 338 126 from 1976 to 2015 with attendant growth in urban areas the karun river 850 km long and iran s largest splits the city into western and eastern parts then joins the arvand rood river and continues toward the persian gulf the climate is semi arid with a mean annual precipitation of 252 mm and an average annual temperature of 26 9 c june is the driest and warmest month and january is wettest and coolest the major land use transition trends in the study area included the rapid growth of built up urban areas transformed from bare lands and agricultural lands and agricultural lands transformed from bare lands from 1985 up to 2015 a minor transition from agricultural lands to bare lands was also observed urban growth was driven by rapid population growth agricultural production has increased to meet increasing local demand supported by the abundance of water and fertile soils various agricultural commodities are produced such as wheat barley oilseeds rice sugar cane medicinal herbs as well as orchard crops such as palm citrus and olives rangzan et al 2008 3 2 lucc simulation process overview lucc simulation was implemented in five phases fig 2 phase 1 involved data collection preparation and pre processing including land use cover classification undertaken using geospatial analysis software envi and arcgis neighbourhood analysis cost distance layer preparation and data discretisation was then done using the raster package hijmans and van etten 2014 in r phase 2 involved calculating rates of land use change between 1985 and 2000 and identifying major land use change transitions for specifying land use change demand islam et al 2018 kamusoko and gamba 2015 phase 3 involved the use of dotrules and random forest rf algorithms to calculate lucc transition potential maps using a training sample of randomly selected grid cells in phase 4 the land use map of 1985 transition potential maps and the estimated rates of land use change for the primary land use classes were integrated into a cellular automata model in r the ca model was calibrated to apply 30 annual iterations one for each year 1985 2015 finally in phase 5 the predictive accuracy of the simulated land use maps for 2015 was validated against the classified land use map for the same year using 100 000 random points for three simulated land use classes i e urban agriculture and bare lands finally we then compared the accuracy of cellular automata land use change models calibrated using dotrules and rf algorithms 3 3 lucc modelling variables and data sources to analyse the trend of change and calculate transition potential maps which are required for simulating future lucc landsat images of the years 1985 2000 and 2015 were used the two earlier landsat images i e 1985 2000 were used for land use change analysis and the calculation of land use transition potential maps while the landsat image for 2015 was used for validation five groups of variables including cell state cs neighbourhood variables nv suitability variables sv a target variable and validation data were extracted from the main data sources for lucc simulation table 1 note that for the nv as neighbourhood configuration is known to affect cellular automata simulation fuglsang et al 2013 lauf et al 2012 verstegen et al 2014 white and engelen 1993 the optimal kernel size k 8 for neighbourhood analysis was selected based on initial cross validation all variables were resampled to the 30 m grid cell resolution of the landsat data totalling 734 328 cells across the study area the derivation and use of these variables is described in detail below 3 3 1 landsat archive and image classification image pre processing involved normalization for the region of interest land use cover maps of the study area were then classified using a support vector machine classifier with geospatial analysis software envi and arcgis achieving an overall accuracy of 85 during this process all grid cells were allocated to one of five land use cover classes urban area agricultural land bare land roads and water bodies for the 1985 2000 and 2015 images fig 3 3 4 land use change analysis to compute transition demand the rate of simulated land use change or transition demand in cellular automata models needs to be calibrated to observed rates by quantifying the historical amount of change for each land use type hewitt and díaz pacheco 2017 kamusoko et al 2009 kamusoko and gamba 2015 pastor et al 1991 we calculated transition demand based on the landsat derived land use cover maps for 1985 and 2000 fig 3 the type and frequency of land use change between 1985 and 2000 were cross tabulated the time interval used for calibration for the 1985 2000 transition matrix was 15 years land use transition probabilities were calculated as average annual rates of change following previous studies hewitt and díaz pacheco 2017 in order to take account of annual change demand for each cellular automata iteration 3 5 computation of land use cover transition potential maps in constructing a training dataset for calculating transition potential maps we selected 300 000 grid cells randomly from the major 1985 2000 land use change categories of bare lands to urban agriculture to urban bare lands to agriculture agriculture to bare lands and no change that is areas which remained unchanged as urban agriculture bare lands and water bodies independent predictor variables were then calculated for the initial year t 1985 including the cell state i e land use map along with neighbourhood and suitability variables table 1 we then computed the transition potential map for 1985 using dotrules and the random forest algorithm as implemented in the randomforest package liaw and wiener 2002 available in r cs nv and sv were recalculated each year based on the simulated land use and used to update the land use transition potential map each year 3 6 ca based land use change simulation in traditional cellular automata models the evolution of the future cell state is determined by the following formula al shalabi et al 2013 martinez et al 2012 wu 1998 6 s x y t 1 f s x y t ρ x y t ω x y t where s x y t represents the land use state for a cell at location x y at time t ρ x y t is composed of a set of suitability measures for the cell at time t ω x y t is the state of neighbouring cells at time t and f is a transition function three datasets 1 the initial land use cover map 1985 2 the transition potential maps 1985 2000 and 3 the transition demand were used to simulate land use cover up to 2015 using a cellular automata hewitt et al 2013 transition demand calculated via the land use change analysis determined the amount of land use change in each simulated year while the land use transition potential determined the location and type of change yang et al 2016 for each simulation year land use was allocated by finding the grid cell with the maximum transition potential if the new land use was less than that demanded then the change was made the cell with the next highest transition potential was then found and the change made if the new land use was less than that demanded this process was repeated until all land use demands were met for that year yang et al 2016 this whole process was repeated for 30 annual iterations to simulate land use from 1985 to 2015 where a new land use map is produced at the end of each iteration 3 7 comparing dotrules with random forests to quantify dotrules performance in calculating land use transition potential we implemented the same ca based lucc simulation scheme but using the rf algorithm kamusoko and gamba 2015 to calculate transition rules the rf algorithm provides an appropriate benchmark for assessing the performance of the dotrules scheme because of the high performance typically found in predictive modelling caruana and niculescu mizil 2006 rf is also computationally efficient and suitable for large training data mahapatra 2014 we compared the overall accuracy of both dotrules and rf based cellular automata simulation of lucc for the ahvaz study area 4 results 4 1 variable importance and transition rules a major product of dotrules is calculated shannon entropy values which are used for prioritising criteria before assembling transition rules criteria are ranked and prioritised according to their average entropy e j equation 3 with higher priority criteria being those with the lower entropy table 2 in our case study 24 437 transition rules were assembled and applied for the purpose of lucc simulation here every single rule sub rule is defined by a unique string which represents criteria values along with a frequency distribution of potential land use class labels rule exclusive hit ratio rule exclusive entropy value and mode land use label fig 4 the transparency of dotrules opens up information contained in the transition rules for critical observation or examination for example retained information in the following rules of fig 4 can be dissected and retrieved once the variable priority is clarified here both rules have the same value for cell state neighbouring agricultural cells distance to road distance to drainage and neighbouring water and road cells however the rules have different values for neighbouring bare land cells distance from urban neighbouring urban cells and slope the two rules have the same mode label but different uncertainties and hit ratios 4 2 simulation performance following the simulation procedure based on the transition potential maps as calculated by dotrules and rf the results are mapped and then validated by a 100 000 validation test points of 2015 land use data map fig 5 this comparison is done only for simulated land use classes including urban agriculture and bare lands considering the fact that land use map of 2015 was not involved in the preparation of transition potential maps the overall accuracy of the lucc simulation using dotrules 75 4 was very similar to that based on rf 75 8 although both algorithms demonstrate broad spatial similarities lucc simulation results of the ahvaz study area for the target year of 2015 also display localised differences land use simulation of both dotrules and rf were promising in identifying retaining and preserving the spatial details of the 2015 land use cover maps 4 3 uncertainty of major land use transitions a key advantage of dotrules is that all major transitions can be identified and dissected enabling the analysis of major trends of change persistence along with their corresponding information such as uncertainty and frequency table 3 in terms of land use change persistence transitions and their uncertainty values bare lands to bare lands persistence had the lowest transition rule uncertainty while agriculture to urban change was the most uncertain land use transition transition rules indicating land use persistence tended to have lower uncertainty than did rules indicating a change from one land use to another 4 4 rule level spatial uncertainty the dotrules spatial uncertainty product can facilitate the better understanding of uncertainty of transition rules in the mapped land use predictions as there will be one uncertainty map for every simulation year it helps to understand where lucc simulation results are less or more reliable for each iteration considering the results of lucc simulation using dotrules the mean uncertainty maps for 30 iterations demonstrates a large extent of low l very low vl and extremely low el uncertainty classes i e class labels characterised by a low uncertainty estimate may be observed within the inner boundaries where lucc is less active patches of high h very high vh and extremely high eh uncertainty occurred where lucc is more active particularly those areas located at the interface between urban and agricultural lands fig 6 4 5 uncertainty and accuracy in this study 24 437 unique rules were detected for the primary rule set where their relevant hit ratio is measured using available test data of 2015 uncertainty was closely related to hit ratio i e percent of correctly assigned land use labels for a rule of transition rules where the hit ratio of transitions exponentially decreases with increasing uncertainty r2 0 89 thus low uncertainty transitions are associated with a higher hit ratio fig 7 lower uncertainty means there is an obvious land use class mode land use class label for a rule while high uncertainty reflects that there are several candidate land use classes for that rule which results in less accurate land use prediction 5 discussion 5 1 dotrules for lucc simulation the method used to calculate transition potential maps greatly affects the performance of automata based lucc models charif et al 2012 mas et al 2014 van vliet et al 2016 transparency and uncertainty of the transition rules are important for providing a better understanding of the model structure and of the nature of land use transitions here we introduced and applied a competent and transparent algorithm for the calculation of lucc transition potential maps for use in cellular automata modelling of land use change as a predictive algorithm dotrules represents a new way to extract transition rules and map transition potential for use in lucc simulation models while also enabling the mapping of uncertainty values at both pixel and rule levels this makes uncertainty explicit and opens up the information contained in the transition rules to better model scrutiny and to better informed land use planning and policymaking the potential of dotrules is demonstrated here for the ahvaz study area and more applications and experiments are now required in different geographic contexts to fully explore the general applicability of dotrules as a land use transition rule detection algorithm below we discuss expand on the advantages and limitations of using dotrules for cellular automata based lucc simulation 5 1 1 transparency of transition rules depending on the method used to extract transition rules some rule sets may be omitted which impairs the quality of the transition potential map and subsequent lucc simulation for instance in applying black box algorithms such as rf to calculate lucc transition potential it is difficult if not impossible to interpret the derived relationships between future land use and predictor variables such as cell state neighbourhood and suitability variables as a result we cannot gain a clear understanding of the problem at hand due to the lack of an explanatory capability to provide insight into the characteristics of the target dataset qiu and jensen 2004 the aim of developing and applying dotrules for lucc simulation was to improve the quality of urban planning by identifying reliable and accessible land use transition rules dotrules provides an opportunity to uncover model structure by adding more clarity to implemented transition rules and revealing information such as rules component priority order and values frequency distribution of potential matching land use labels for every rule and rule exclusive hit ratio and uncertainty fig 4 it can also identify simplified sub rule sets compromising only the more informative variables for instance considering major types of land use change persistence dotrules identifies the rules governing the most common change persistence patterns within the study area land use planners and policy makers can explore alternative possibilities for land use if past transparent and well understood land use transition rules continue into the future a transparent set of land use transition rules can also help uncover model structure hidden by other black box algorithms and aid model verification and error identification bauer and steinnocher 2001 tseng et al 2008 although most lucc modellers are aware of the shortcomings and the limited accuracy of their input data very little is known about the propagation of such errors in lucc models dong et al 2015 5 1 2 uncertainty of transition rules we have demonstrated dotrules ability to quantify the frequency and uncertainty of land use transition rules table 3 fig 4 it is beneficial to gain reliable information corresponding to the functionality of those rules including their uncertainty and their corresponding hit ratio fig 4 regardless of the fact that a rule is indicating the change or persistence of land use in a grid cell the rule level uncertainty values provide a strong indication of prediction hit ratio and can be applied as a filter to remove low accuracy portions of a lucc map derived from unreliable transitions the uncertainty of rules belonging to primary land use transitions table 3 is also useful for the prior exploration of potential accuracy values i e accuracy estimate and can reduce or eliminate the need for post hoc validation this can be done for the primary rule set or a sub rule set extracted from training data focusing on some specific land use variables forming land use transition potential maps dotrules rule level uncertainty product contains detailed information about the multiple specific land use transitions in a study area which provide landscape and urban planners with the opportunity to better understand the nature of future land use transitions and the level of confidence in their prediction for instance considering the results of uncertainty assessment for lucc simulation fig 6 and table 3 the land use transition between urban and agriculture bare lands is more uncertain than simulated land use transitions between agricultural lands and bare lands by specifying uncertainty thresholds land use transitions in which we are most confident or least confident can be identified and these can be made explicit in mapped planning outputs to guide decision making thereby uncertainty thresholds may be applied to any type of transitions for reducing the rule population to include only the most trusted i e those below a specific uncertainty value the high degree of correspondence between uncertainty and hit ratio means that planners can use dotrules uncertainty products as an indicator of land use simulation accuracy for example if the uncertainty threshold of 0 2 is applied then planners can be confident that the hit ratio of all the remaining rules should be high i e above 90 fig 7 5 1 3 mapping uncertainty in addressing the limitations of widely used accuracy measures and indices using the dotrules algorithm we can also map and apply localised uncertainty values within different classification stages as an estimate of accuracy thus a strength of dotrules in lucc modelling is its demonstrated ability to quantify the uncertainty of simulated land use patterns at pixel level regardless of test data availability uncertainty maps can be produced as an estimate of prediction accuracy even for future land use change scenarios which lack ground truth data this enables the most recent data to be used in model building rather than model validation which should produce more reliable land use simulation further into the future yet still providing an estimate of uncertainty spatially explicit uncertainty also assists landscape and urban planners to foresee the degree of susceptibility to prediction error for specific localities rule level uncertainty maps are created for every lucc iteration 30 iterations in the present study as every pixel corresponds to one transition rule for every iteration the mean uncertainty of these transition rules allocated to a single grid cell may be calculated mapped and analysed for one or many iterations uncertainty values vary over time for lucc simulation for each pixel and this can be graphed over time this helps landscape and urban planners to keep track of lucc modelling uncertainty and or hit ratio at pixel level at different time steps of a lucc model other indicators of uncertainty may also be produced including the maximum minimum or median each of which may be useful depending on the planning application 5 2 broader application to environmental modelling dotrules quantifies the level of correspondence between each predictor variable and response variable table 1 through the calculation of entropy values equation 3 in addition it also extracts the transparent rules each with a quantified frequency and entropy which provides insight into the observed dynamics table 3 in this paper we have applied dotrules for calculating transition potential in cellular automata models of land use cover change nonetheless the conceptual framework of dotrules can be applied to other applications of automata models such as plant population dynamics forest bush fire spreading slope failure debris flow and urban sprawl where transition potential mapping is required and where it is necessary to understand the direction and magnitude of state transitions with greater transparency for instance in terms of bushfire simulation using cellular automata a same set of transition rules should be determined to form a transition potential map quartieri et al 2010 while different set of predictor variables such as bush density flammability land height and wind li and magill 2001 simultaneously the application of rule base methods such as dotrules for a better understanding of dynamic environmental phenomena is not limited to automata models amini et al 2010 luo et al 2017 moore et al 2014 and it involves a broader range of environmental models the authors are currently trialling dotrules in other critical applications such as image classification and landslide susceptibility zonation 5 3 limitations of dotrules the major limitation of dotrules corresponds with the calculated entropy value for low frequency rules where in calculating shannon entropy values some rules may be attributed a low uncertainty value by chance due to a very low frequency value for instance if the entropy value of two different rules is equal they would be attributed the same uncertainty class nonetheless they may be different in terms of frequency values fig 8 in this regard among the rules with the same entropy values those with higher frequency would be more reliable as they occurred more often a subjective thresholding approach may be beneficial to deal with this problem thus those low frequency rules should be ignored if they belong to a frequency value lower than a threshold defined by the user 6 conclusion cellular automata have long been used to capture the complex dynamics of lucc processes we have presented a new and innovative algorithm called dotrules dictionary of trusted rules for calculation of transition rules and transition potential maps for use in ca based simulation models we have then presented an application of the proposed approach in the context of lucc simulation where cellular automata models are increasingly popular dotrules enables land use allocation to be implemented with a new level of transparency and transition rule characteristics are accessible and can be monitored dotrules also enables the spatial exploration of lucc prediction uncertainty these estimates can assist urban planners in avoiding risky land use predictions where rules are not reliable assessing rule information also enables the detection of trends and understanding processes of land use change in a study area the performance of automata simulation based on dotrules transition potential calculation was very similar to simulation based on the state of the art random forest method hence we conclude that dotrules is a promising approach for extraction of transition rules and providing transition potential maps for ca based simulation due to its predictive ability transparency and ability to produce multiple uncertainty products these capabilities can enhance the utility of automata based simulation as a tool for end users and improve the quality of the resultant decision making process more experiments in different environmental applications with a different set of variables are now required to verify the broader utility of dotrules as a ca calibration tool the application of dotrules as a transparent rule based approach to other environmental modelling applications should also be explored 6 1 software availability the following software has been used in this study for spatial data preparation and processing land use simulation and map creation dotrules script is implemented in r v 3 5 0 envi 5 2 harris geospatial broomfield colorado united states r v 3 5 0 r foundation for statistical computing vienna austria arcgis v 10 2 esri inc redlands usa note no specific software component has been developed for this study acknowledgements this research was supported by csiro australian sustainable agriculture scholarship asas as a top up scholarship to majid shadman a phd scholar in the discipline of geography and spatial sciences school of technology environments and design at the university of tasmania rt109121 bab was supported by deakin university we thank three anonymous reviewers for their suggestions for improving the manuscript 
26268,despite recent advances in quantifying land use cover change lucc transition potential transition rules are often not transparent and uncertainty is rarely made explicit here we introduce dotrules a dictionary of trusted rules as a transparent alternative to calculate transition potential in cellular automata models rules relate lucc variables to the observed historical changes shannon entropy is calculated to assess the uncertainty of each rule and the most trusted rules are used to project future lucc dotrules produces rule level uncertainty estimates which can be mapped in a case study of the ahvaz region of iran the overall accuracy of lucc simulation calibrated using dotrules was very similar to simulations calibrated with the state of the art random forest but dotrules provides a more transparent approach where transition rule information and uncertainty can be readily accessed and interpreted the results demonstrate that dotrules has potential to derive new insights into lucc processes keywords entropy land use cover change uncertainty urban planning modelling simulation 1 introduction cellular automata ca were conceptually established by john von neumann 1903 1957 during the 1950s due to their simplicity and capacity to simulate spatial patterns ca have rapidly gained popularity as a tool for modelling spatial dynamics of many environmental phenomena such as plant population dynamics xu et al 2010 forest fire spread zheng et al 2017 slope failure liucci et al 2017 debris flow d ambrosio et al 2003 d ambrosio et al 2006 urban sprawl mustafa et al 2018a van vliet et al 2009 land use cover change lucc hewitt and díaz pacheco 2017 hewitt et al 2014 white and engelen 1997 and more though cellular automata can handle very complex spatial situations for modelling environmental phenomena their conceptual basis is straightforward a cellular automaton consists of a large number of cells which can change their state according to specific rules in many applications such as modelling fire spread i e bushfire or forest fire urban sprawl modelling and specially lucc simulation a set of neighbourhood and suitability values are defined reflecting the influence of external factors affecting the state transitions for each cell finally there is a set of rules defining transition potential of a cell from one state to another in terms of lucc models the transition demand and the transition potential are typically the two main requirements white and engelen 1993 for model implementation using cellular automata first historical rates of land use change are calculated which are used to calibrate the total amount of land use change occurring in each time step this is termed transition demand second the spatially explicit probabilities of land use change or transition potential are calculated transition potential represents the behavioural propensities of the actors determining land use change and is defined based on the inferred logic from a set of transition rules transition rules are general structures that offer an easily understandable and transparent way to find the most reliable land use class allocation russell et al 2003 in practice transition rules capture the relationships between land use and a suite of independent predictor variables the effectiveness of cellular automata land use models in informing land use planning depends upon the efficient extraction of reliable and transparent transition rules han et al 2015 hewitt et al 2014 numerous machine learning and statistical methods have been used to calculate land use transition rules and map transition potential for use in cellular automata land use models basse et al 2014 berberoğlu et al 2016 clarke et al 1997 ku 2016 liu et al 2014 mustafa et al 2017 2018a 2018b methods frequently applied include association rule learning al kheder et al 2008 liu and jiang 2011 artificial neural network basse et al 2014 li et al 2013 maximum margin rienow and goetzke 2015 yang et al 2008 instance based castilla and blas 2008 li et al 2015 regression ku 2016 long et al 2014 decision tree ballestores jr and qiu 2012 basse et al 2016 and probabilistic arsanjani et al 2011 vaz et al 2015 methods others such as evolutionary deep learning reinforcement learning dimensionality reduction bayesian and regularisation methods have been used less frequently kamusoko and gamba 2015 li et al 2015 verstegen et al 2014 zhang et al 2008 each of these methods employs structurally different numerical formulations which affect the accuracy and transparency of automata based lucc models few of these methods facilitate the transparent extraction of transition rules and their corresponding uncertainty transparent transition rules enable both an enhanced ability to appreciate the relationships between land use change or other similar environmental phenomena and predictor variables this is necessary to understand model structure hence the inferred logic of model structure captured in transition rules can be visualised dissected and deciphered and can be generalized and applied to address other similar problems this can be invaluable for informing error checking and enabling model validation the explicit identification of transition rules can also help understand the nature of the major land use transitions tayyebi et al 2014 incomplete information about transition rules and their uncertainty may impede the understanding of land use change processes koomen and borsboom van beurden 2012 pozoukidou 2005 however many approaches have been subject to limitations in their ability to clearly identify transition rules and have been criticised as being black boxes islam et al 2018 li and yeh 2002 qiu and jensen 2004 waddell 2002 for instance kamusoko and gamba 2015 compared cellular automata calibrated using random forest support vector machine and logistic regression the higher performance of the random forest model was attributed to the relatively accurate transition potential maps however apart from logistic regression which is capable of revealing the relative global contribution of each variable to the land use change process none of these methods is capable of implementing accessible and transparent sets of transition rules at the pixel level transparent approaches are more desirable than black box approaches for transition rule detection even if this preference trades off some performance tseng et al 2008 uzuner et al 2009 this includes but is not limited to lucc plans where it is helpful to provide insight into the internal decision making process of algorithms for a better interpretation of the result similar requirements also exist for other environmental applications aimed at improving the quality of management plans in the context of natural hazards lai et al 2016 royston et al 2012 shadman roodposhti et al 2016 water treatment gibert et al 2010 soil erosion adinarayana et al 1999 and farming systems moore et al 2014 simulated change and persistence in land use patterns need to be interpreted and validated via a better understanding of uncertainty both at the rule level and spatially while a few studies have successfully mapped the spatial distribution of classification uncertainty bryan et al 2009 khatami et al 2017 providing estimates of classification uncertainty at rule level i e for each rule may also provide complementary insights into land use change processes however rather than providing uncertainty estimates land use modelling studies typically report on the accuracy of lucc analyses using global methods such as confusion matrices and the kappa index congalton 1991 confusion matrices are usually calculated to allow for global measures of accuracy i e overall accuracy to be generated but which lack the ability to quantify the spatial distribution of classification accuracy tsutsumida and comber 2015 similarly the kappa index does not consider the disagreement in classification accuracy pontius jr and millones 2011 stein et al 2005 global measures of accuracy require ground truth data however it is a time consuming process to prepare land use ground truth maps for every simulated map of land use in addition there is no ground truth for future land use scenarios and therefore these methods provide no basis for quantifying confidence in simulated future land use maps explicit rule level uncertainty estimates can assist land use planners and policymakers to understand the uncertainties associated with major land use transitions the mapping of these uncertainties can identify locations where simulated land use allocation occurs with high confidence or areas of low confidence which can both be useful in land use planning here we develop a new algorithm dotrules dictionary of trusted rules for modelling land use transition potential for application in automata based lucc models which features the transparent identification of transition rules and quantifies their uncertainty it also enables the mapping of corresponding land use transition uncertainties in dotrules the uncertainty of transition rules is quantified using shannon entropy dissecting transition rules and their corresponding uncertainty enables the better understanding of the core rules governing major land use cover dynamics which is useful for informing land use planning we also show that the uncertainty values can be applied as an approximation of simulation accuracy we describe the dotrules algorithm and demonstrate its application to the ahvaz region iran we quantify the uncertainty of lucc simulation calibrated using dotrules to calculate land use transition rules and compare the results with simulations based on random forest transition rule detection we discuss the advantages and disadvantages of the new approach for lucc modelling more generally and the application of dotrules in the calculation of transition potential maps for lucc models to broader environmental processes where it is necessary to understand the direction and magnitude of state transitions 2 description of dotrules dotrules is a moderate speed rule based algorithm for calculating transition potential in lucc according to a dictionary of trusted rules where categorical discrete data are involved it is similar to the random forest algorithm breiman 1996 2001 insofar as rule sets are used to select the mode response i e most frequent land use class among every available potential response variable however instead of generating random trees dotrules operates by constructing many transition rules within various rule sets derived from a training dataset with land use assigned to the most frequently occurring class the rule construction process is fulfilled using concatenation of discrete predictor variables and the entropy of each rule is then calculated as an estimate of accuracy the dotrules procedure was implemented in r r core team 2017 and consists of the following six steps 2 1 step 1 assembling the data training data is represented by a set of grid cells i i 1 i 2 i m each grid cell i in i has a value x ij for each of the independent predictor variables or criteria j j 1 j 2 j n where in this study there are nine independent predictor variables table 1 criteria are discretised predictor variables which can be derived from either native categorical data e g land use class or classified continuous data e g distance to road thus for each criterion x ij can adopt one of a fixed set of possible classes specific to that criterion which we represent as the set h for every j in j table 1 note that each criterion j will have a different set of classes h but for clarity here we do not index h by j each grid cell i has a corresponding land use class l i which are also discrete semantic attributes from the set of five land uses l u urban a agriculture b bare lands r roads and w water bodies 2 2 step 2 calculating shannon entropy and prioritising criteria for each criterion j in j we calculated the frequency of grid cells i within each criterion class h in h occurring within each land use class l in l represented as p l h j 1 p l h j i i x i j h l i l i i x i j h for l in l h in h and j in j note that are iverson brackets where q quantity equals 1 if true and 0 if false the term i i x i j h is the number of grid cells in criterion class h in information theory entropy is the quantitative measure of system disorder instability and uncertainty shannon 2001 the shannon entropy is the quantitative measure of uncertainty in this study here we calculate the entropy of land use class occurrences within each criteria class h across all criteria j 2 e h j l l p l h j ln p l h j the entropy of each criterion was then calculated as the average entropy of its classes e h j weighted by the proportion of cells in each class 3 e j h h e h j i i x i j h i where i is the set of grid cells in the training dataset the criteria were then ranked and prioritised according to their average entropy e j with higher priority criteria being those with the lower entropy represented by the ordered set of criteria priority j 2 3 step 3 creating a rule set we then concatenate grid cell criteria values x ij as per criteria priority j in order to form a rule set d the concatenation of two or more characters is the string formed by them in a series i e the concatenation of 12 a7 and 5 is 12a75 equation 4 illustrates the grid cell values for criteria ranked in order of priority i e lowest entropy concatenated for each grid cell row i thereby creating a unique rule for each grid cell in the training dataset 4 d x 11 x 21 x i 1 x i 1 x 12 x 22 x i 2 x i 2 x 1 j x 2 j x i j x i j x 1 j x 2 j x i j x i j x 11 x 21 x i 1 x i 1 x 12 x 22 x i 2 x i 2 x 1 j x 2 j x i j x i j x 1 j x 2 j x i j x i j d 1 d 2 d i d i note that following the concatenation and extraction of rules every rule within the dictionary has maintained its single land use class l i l we then aggregate duplicate rules where grid cells have exactly the same values for all criteria leaving a parsimonious new rule set of unique rules d derived by aggregating d the frequency of occurrence of all potential land use classes l in l is then calculated for each unique rule d in d 5 l 1 l 2 l d f l 1 u f l 1 a f l 1 b f l 1 r f l 1 w f l 2 u f l 2 a f l 2 b f l 2 r f l 2 w f l d u f l d a f l d b f l d r f l d w the land use class i e u a b r w from set l with the highest frequency i e the mode is then assigned to each corresponding unique rule d 2 4 step 4 calculating and mapping the uncertainty of land use prediction considering every unique rule d from our rule set d a shannon entropy value is then calculated based on the frequencies of each possible land use class equation 5 using equation 2 this can inform both the spatial distribution of uncertainty in land use predictions and provides transparent transition rules for informing land use planning the spatial distribution of uncertainty is quantified and mapped by the entropy of each unique rule back to the grid cells corresponding to each rule each grid cell is then allocated to the land use class with the highest frequency for its corresponding rule 2 5 step 5 classify land use of test dataset according to the dictionary of trusted rules above we describe the process of creating the dictionary of trusted rules and allocating the most likely land use class for each rule based on frequency the land use class can now be predicted for the rest of the study area dataset i e the test dataset to do this we follow the same procedure to set up a rule set for the test dataset we then match each test data rule with its equivalent in the dictionary of trusted rules using many to one matching i e matching many rules concatenated from test grid cells to individual trusted rules calculated using the training dataset and allocate the most likely land use to each test data rule this can then be mapped back to the grid cell level as each rule in the test dataset corresponds to a grid cell 2 6 step 6 handling null values finally as there is always a possibility of encountering null values using the dotrules approach where new cells in the test dataset present combinations of criteria states not encountered in the training data here using the same training and test sample we will sequentially exclude the least informative i e highest entropy and lowest ranked independent predictor variables or criteria j from our analysis and re execute dotrules step 3 to 5 this generates a second rule set i e a sub rule set which contains fewer unique rules d in the corresponding d and null records as it contains fewer criteria classes we then repeat in developing some new sub rule sets until all null values in our primary rule set are covered by some corresponding sub rules among secondary sub rule sets the best sub rule to replace a null matching rule in our test rule set is the one with the lowest entropy while those rules with higher entropy values higher than the specified threshold are eliminated 3 methods 3 1 study area the study area was the ahvaz region of south west iran ahvaz city is the capital and largest city of khuzestan province fig 1 the population of ahvaz increased from 334 399 to 1 338 126 from 1976 to 2015 with attendant growth in urban areas the karun river 850 km long and iran s largest splits the city into western and eastern parts then joins the arvand rood river and continues toward the persian gulf the climate is semi arid with a mean annual precipitation of 252 mm and an average annual temperature of 26 9 c june is the driest and warmest month and january is wettest and coolest the major land use transition trends in the study area included the rapid growth of built up urban areas transformed from bare lands and agricultural lands and agricultural lands transformed from bare lands from 1985 up to 2015 a minor transition from agricultural lands to bare lands was also observed urban growth was driven by rapid population growth agricultural production has increased to meet increasing local demand supported by the abundance of water and fertile soils various agricultural commodities are produced such as wheat barley oilseeds rice sugar cane medicinal herbs as well as orchard crops such as palm citrus and olives rangzan et al 2008 3 2 lucc simulation process overview lucc simulation was implemented in five phases fig 2 phase 1 involved data collection preparation and pre processing including land use cover classification undertaken using geospatial analysis software envi and arcgis neighbourhood analysis cost distance layer preparation and data discretisation was then done using the raster package hijmans and van etten 2014 in r phase 2 involved calculating rates of land use change between 1985 and 2000 and identifying major land use change transitions for specifying land use change demand islam et al 2018 kamusoko and gamba 2015 phase 3 involved the use of dotrules and random forest rf algorithms to calculate lucc transition potential maps using a training sample of randomly selected grid cells in phase 4 the land use map of 1985 transition potential maps and the estimated rates of land use change for the primary land use classes were integrated into a cellular automata model in r the ca model was calibrated to apply 30 annual iterations one for each year 1985 2015 finally in phase 5 the predictive accuracy of the simulated land use maps for 2015 was validated against the classified land use map for the same year using 100 000 random points for three simulated land use classes i e urban agriculture and bare lands finally we then compared the accuracy of cellular automata land use change models calibrated using dotrules and rf algorithms 3 3 lucc modelling variables and data sources to analyse the trend of change and calculate transition potential maps which are required for simulating future lucc landsat images of the years 1985 2000 and 2015 were used the two earlier landsat images i e 1985 2000 were used for land use change analysis and the calculation of land use transition potential maps while the landsat image for 2015 was used for validation five groups of variables including cell state cs neighbourhood variables nv suitability variables sv a target variable and validation data were extracted from the main data sources for lucc simulation table 1 note that for the nv as neighbourhood configuration is known to affect cellular automata simulation fuglsang et al 2013 lauf et al 2012 verstegen et al 2014 white and engelen 1993 the optimal kernel size k 8 for neighbourhood analysis was selected based on initial cross validation all variables were resampled to the 30 m grid cell resolution of the landsat data totalling 734 328 cells across the study area the derivation and use of these variables is described in detail below 3 3 1 landsat archive and image classification image pre processing involved normalization for the region of interest land use cover maps of the study area were then classified using a support vector machine classifier with geospatial analysis software envi and arcgis achieving an overall accuracy of 85 during this process all grid cells were allocated to one of five land use cover classes urban area agricultural land bare land roads and water bodies for the 1985 2000 and 2015 images fig 3 3 4 land use change analysis to compute transition demand the rate of simulated land use change or transition demand in cellular automata models needs to be calibrated to observed rates by quantifying the historical amount of change for each land use type hewitt and díaz pacheco 2017 kamusoko et al 2009 kamusoko and gamba 2015 pastor et al 1991 we calculated transition demand based on the landsat derived land use cover maps for 1985 and 2000 fig 3 the type and frequency of land use change between 1985 and 2000 were cross tabulated the time interval used for calibration for the 1985 2000 transition matrix was 15 years land use transition probabilities were calculated as average annual rates of change following previous studies hewitt and díaz pacheco 2017 in order to take account of annual change demand for each cellular automata iteration 3 5 computation of land use cover transition potential maps in constructing a training dataset for calculating transition potential maps we selected 300 000 grid cells randomly from the major 1985 2000 land use change categories of bare lands to urban agriculture to urban bare lands to agriculture agriculture to bare lands and no change that is areas which remained unchanged as urban agriculture bare lands and water bodies independent predictor variables were then calculated for the initial year t 1985 including the cell state i e land use map along with neighbourhood and suitability variables table 1 we then computed the transition potential map for 1985 using dotrules and the random forest algorithm as implemented in the randomforest package liaw and wiener 2002 available in r cs nv and sv were recalculated each year based on the simulated land use and used to update the land use transition potential map each year 3 6 ca based land use change simulation in traditional cellular automata models the evolution of the future cell state is determined by the following formula al shalabi et al 2013 martinez et al 2012 wu 1998 6 s x y t 1 f s x y t ρ x y t ω x y t where s x y t represents the land use state for a cell at location x y at time t ρ x y t is composed of a set of suitability measures for the cell at time t ω x y t is the state of neighbouring cells at time t and f is a transition function three datasets 1 the initial land use cover map 1985 2 the transition potential maps 1985 2000 and 3 the transition demand were used to simulate land use cover up to 2015 using a cellular automata hewitt et al 2013 transition demand calculated via the land use change analysis determined the amount of land use change in each simulated year while the land use transition potential determined the location and type of change yang et al 2016 for each simulation year land use was allocated by finding the grid cell with the maximum transition potential if the new land use was less than that demanded then the change was made the cell with the next highest transition potential was then found and the change made if the new land use was less than that demanded this process was repeated until all land use demands were met for that year yang et al 2016 this whole process was repeated for 30 annual iterations to simulate land use from 1985 to 2015 where a new land use map is produced at the end of each iteration 3 7 comparing dotrules with random forests to quantify dotrules performance in calculating land use transition potential we implemented the same ca based lucc simulation scheme but using the rf algorithm kamusoko and gamba 2015 to calculate transition rules the rf algorithm provides an appropriate benchmark for assessing the performance of the dotrules scheme because of the high performance typically found in predictive modelling caruana and niculescu mizil 2006 rf is also computationally efficient and suitable for large training data mahapatra 2014 we compared the overall accuracy of both dotrules and rf based cellular automata simulation of lucc for the ahvaz study area 4 results 4 1 variable importance and transition rules a major product of dotrules is calculated shannon entropy values which are used for prioritising criteria before assembling transition rules criteria are ranked and prioritised according to their average entropy e j equation 3 with higher priority criteria being those with the lower entropy table 2 in our case study 24 437 transition rules were assembled and applied for the purpose of lucc simulation here every single rule sub rule is defined by a unique string which represents criteria values along with a frequency distribution of potential land use class labels rule exclusive hit ratio rule exclusive entropy value and mode land use label fig 4 the transparency of dotrules opens up information contained in the transition rules for critical observation or examination for example retained information in the following rules of fig 4 can be dissected and retrieved once the variable priority is clarified here both rules have the same value for cell state neighbouring agricultural cells distance to road distance to drainage and neighbouring water and road cells however the rules have different values for neighbouring bare land cells distance from urban neighbouring urban cells and slope the two rules have the same mode label but different uncertainties and hit ratios 4 2 simulation performance following the simulation procedure based on the transition potential maps as calculated by dotrules and rf the results are mapped and then validated by a 100 000 validation test points of 2015 land use data map fig 5 this comparison is done only for simulated land use classes including urban agriculture and bare lands considering the fact that land use map of 2015 was not involved in the preparation of transition potential maps the overall accuracy of the lucc simulation using dotrules 75 4 was very similar to that based on rf 75 8 although both algorithms demonstrate broad spatial similarities lucc simulation results of the ahvaz study area for the target year of 2015 also display localised differences land use simulation of both dotrules and rf were promising in identifying retaining and preserving the spatial details of the 2015 land use cover maps 4 3 uncertainty of major land use transitions a key advantage of dotrules is that all major transitions can be identified and dissected enabling the analysis of major trends of change persistence along with their corresponding information such as uncertainty and frequency table 3 in terms of land use change persistence transitions and their uncertainty values bare lands to bare lands persistence had the lowest transition rule uncertainty while agriculture to urban change was the most uncertain land use transition transition rules indicating land use persistence tended to have lower uncertainty than did rules indicating a change from one land use to another 4 4 rule level spatial uncertainty the dotrules spatial uncertainty product can facilitate the better understanding of uncertainty of transition rules in the mapped land use predictions as there will be one uncertainty map for every simulation year it helps to understand where lucc simulation results are less or more reliable for each iteration considering the results of lucc simulation using dotrules the mean uncertainty maps for 30 iterations demonstrates a large extent of low l very low vl and extremely low el uncertainty classes i e class labels characterised by a low uncertainty estimate may be observed within the inner boundaries where lucc is less active patches of high h very high vh and extremely high eh uncertainty occurred where lucc is more active particularly those areas located at the interface between urban and agricultural lands fig 6 4 5 uncertainty and accuracy in this study 24 437 unique rules were detected for the primary rule set where their relevant hit ratio is measured using available test data of 2015 uncertainty was closely related to hit ratio i e percent of correctly assigned land use labels for a rule of transition rules where the hit ratio of transitions exponentially decreases with increasing uncertainty r2 0 89 thus low uncertainty transitions are associated with a higher hit ratio fig 7 lower uncertainty means there is an obvious land use class mode land use class label for a rule while high uncertainty reflects that there are several candidate land use classes for that rule which results in less accurate land use prediction 5 discussion 5 1 dotrules for lucc simulation the method used to calculate transition potential maps greatly affects the performance of automata based lucc models charif et al 2012 mas et al 2014 van vliet et al 2016 transparency and uncertainty of the transition rules are important for providing a better understanding of the model structure and of the nature of land use transitions here we introduced and applied a competent and transparent algorithm for the calculation of lucc transition potential maps for use in cellular automata modelling of land use change as a predictive algorithm dotrules represents a new way to extract transition rules and map transition potential for use in lucc simulation models while also enabling the mapping of uncertainty values at both pixel and rule levels this makes uncertainty explicit and opens up the information contained in the transition rules to better model scrutiny and to better informed land use planning and policymaking the potential of dotrules is demonstrated here for the ahvaz study area and more applications and experiments are now required in different geographic contexts to fully explore the general applicability of dotrules as a land use transition rule detection algorithm below we discuss expand on the advantages and limitations of using dotrules for cellular automata based lucc simulation 5 1 1 transparency of transition rules depending on the method used to extract transition rules some rule sets may be omitted which impairs the quality of the transition potential map and subsequent lucc simulation for instance in applying black box algorithms such as rf to calculate lucc transition potential it is difficult if not impossible to interpret the derived relationships between future land use and predictor variables such as cell state neighbourhood and suitability variables as a result we cannot gain a clear understanding of the problem at hand due to the lack of an explanatory capability to provide insight into the characteristics of the target dataset qiu and jensen 2004 the aim of developing and applying dotrules for lucc simulation was to improve the quality of urban planning by identifying reliable and accessible land use transition rules dotrules provides an opportunity to uncover model structure by adding more clarity to implemented transition rules and revealing information such as rules component priority order and values frequency distribution of potential matching land use labels for every rule and rule exclusive hit ratio and uncertainty fig 4 it can also identify simplified sub rule sets compromising only the more informative variables for instance considering major types of land use change persistence dotrules identifies the rules governing the most common change persistence patterns within the study area land use planners and policy makers can explore alternative possibilities for land use if past transparent and well understood land use transition rules continue into the future a transparent set of land use transition rules can also help uncover model structure hidden by other black box algorithms and aid model verification and error identification bauer and steinnocher 2001 tseng et al 2008 although most lucc modellers are aware of the shortcomings and the limited accuracy of their input data very little is known about the propagation of such errors in lucc models dong et al 2015 5 1 2 uncertainty of transition rules we have demonstrated dotrules ability to quantify the frequency and uncertainty of land use transition rules table 3 fig 4 it is beneficial to gain reliable information corresponding to the functionality of those rules including their uncertainty and their corresponding hit ratio fig 4 regardless of the fact that a rule is indicating the change or persistence of land use in a grid cell the rule level uncertainty values provide a strong indication of prediction hit ratio and can be applied as a filter to remove low accuracy portions of a lucc map derived from unreliable transitions the uncertainty of rules belonging to primary land use transitions table 3 is also useful for the prior exploration of potential accuracy values i e accuracy estimate and can reduce or eliminate the need for post hoc validation this can be done for the primary rule set or a sub rule set extracted from training data focusing on some specific land use variables forming land use transition potential maps dotrules rule level uncertainty product contains detailed information about the multiple specific land use transitions in a study area which provide landscape and urban planners with the opportunity to better understand the nature of future land use transitions and the level of confidence in their prediction for instance considering the results of uncertainty assessment for lucc simulation fig 6 and table 3 the land use transition between urban and agriculture bare lands is more uncertain than simulated land use transitions between agricultural lands and bare lands by specifying uncertainty thresholds land use transitions in which we are most confident or least confident can be identified and these can be made explicit in mapped planning outputs to guide decision making thereby uncertainty thresholds may be applied to any type of transitions for reducing the rule population to include only the most trusted i e those below a specific uncertainty value the high degree of correspondence between uncertainty and hit ratio means that planners can use dotrules uncertainty products as an indicator of land use simulation accuracy for example if the uncertainty threshold of 0 2 is applied then planners can be confident that the hit ratio of all the remaining rules should be high i e above 90 fig 7 5 1 3 mapping uncertainty in addressing the limitations of widely used accuracy measures and indices using the dotrules algorithm we can also map and apply localised uncertainty values within different classification stages as an estimate of accuracy thus a strength of dotrules in lucc modelling is its demonstrated ability to quantify the uncertainty of simulated land use patterns at pixel level regardless of test data availability uncertainty maps can be produced as an estimate of prediction accuracy even for future land use change scenarios which lack ground truth data this enables the most recent data to be used in model building rather than model validation which should produce more reliable land use simulation further into the future yet still providing an estimate of uncertainty spatially explicit uncertainty also assists landscape and urban planners to foresee the degree of susceptibility to prediction error for specific localities rule level uncertainty maps are created for every lucc iteration 30 iterations in the present study as every pixel corresponds to one transition rule for every iteration the mean uncertainty of these transition rules allocated to a single grid cell may be calculated mapped and analysed for one or many iterations uncertainty values vary over time for lucc simulation for each pixel and this can be graphed over time this helps landscape and urban planners to keep track of lucc modelling uncertainty and or hit ratio at pixel level at different time steps of a lucc model other indicators of uncertainty may also be produced including the maximum minimum or median each of which may be useful depending on the planning application 5 2 broader application to environmental modelling dotrules quantifies the level of correspondence between each predictor variable and response variable table 1 through the calculation of entropy values equation 3 in addition it also extracts the transparent rules each with a quantified frequency and entropy which provides insight into the observed dynamics table 3 in this paper we have applied dotrules for calculating transition potential in cellular automata models of land use cover change nonetheless the conceptual framework of dotrules can be applied to other applications of automata models such as plant population dynamics forest bush fire spreading slope failure debris flow and urban sprawl where transition potential mapping is required and where it is necessary to understand the direction and magnitude of state transitions with greater transparency for instance in terms of bushfire simulation using cellular automata a same set of transition rules should be determined to form a transition potential map quartieri et al 2010 while different set of predictor variables such as bush density flammability land height and wind li and magill 2001 simultaneously the application of rule base methods such as dotrules for a better understanding of dynamic environmental phenomena is not limited to automata models amini et al 2010 luo et al 2017 moore et al 2014 and it involves a broader range of environmental models the authors are currently trialling dotrules in other critical applications such as image classification and landslide susceptibility zonation 5 3 limitations of dotrules the major limitation of dotrules corresponds with the calculated entropy value for low frequency rules where in calculating shannon entropy values some rules may be attributed a low uncertainty value by chance due to a very low frequency value for instance if the entropy value of two different rules is equal they would be attributed the same uncertainty class nonetheless they may be different in terms of frequency values fig 8 in this regard among the rules with the same entropy values those with higher frequency would be more reliable as they occurred more often a subjective thresholding approach may be beneficial to deal with this problem thus those low frequency rules should be ignored if they belong to a frequency value lower than a threshold defined by the user 6 conclusion cellular automata have long been used to capture the complex dynamics of lucc processes we have presented a new and innovative algorithm called dotrules dictionary of trusted rules for calculation of transition rules and transition potential maps for use in ca based simulation models we have then presented an application of the proposed approach in the context of lucc simulation where cellular automata models are increasingly popular dotrules enables land use allocation to be implemented with a new level of transparency and transition rule characteristics are accessible and can be monitored dotrules also enables the spatial exploration of lucc prediction uncertainty these estimates can assist urban planners in avoiding risky land use predictions where rules are not reliable assessing rule information also enables the detection of trends and understanding processes of land use change in a study area the performance of automata simulation based on dotrules transition potential calculation was very similar to simulation based on the state of the art random forest method hence we conclude that dotrules is a promising approach for extraction of transition rules and providing transition potential maps for ca based simulation due to its predictive ability transparency and ability to produce multiple uncertainty products these capabilities can enhance the utility of automata based simulation as a tool for end users and improve the quality of the resultant decision making process more experiments in different environmental applications with a different set of variables are now required to verify the broader utility of dotrules as a ca calibration tool the application of dotrules as a transparent rule based approach to other environmental modelling applications should also be explored 6 1 software availability the following software has been used in this study for spatial data preparation and processing land use simulation and map creation dotrules script is implemented in r v 3 5 0 envi 5 2 harris geospatial broomfield colorado united states r v 3 5 0 r foundation for statistical computing vienna austria arcgis v 10 2 esri inc redlands usa note no specific software component has been developed for this study acknowledgements this research was supported by csiro australian sustainable agriculture scholarship asas as a top up scholarship to majid shadman a phd scholar in the discipline of geography and spatial sciences school of technology environments and design at the university of tasmania rt109121 bab was supported by deakin university we thank three anonymous reviewers for their suggestions for improving the manuscript 
26269,many of the world s greatest challenges are complex socio environmental problems often framed in terms of integrated assessment resilience or sustainability to resolve any of these challenges it is essential to elicit and integrate knowledge across a range of systems informing the design of solutions that take into account the complex and uncertain nature of the individual systems and their interrelationships to meet this scientific challenge we propose a tiered system of systems modeling framework with these elements a component based software framework that couples a wide range of relevant systems using a modular system of systems structure a tiered structure with different levels of abstraction that spans bottom up and top down approaches the ability to inform robust decisions in the face of deep uncertainty and the systematic integration of multiple knowledge domains and disciplines we illustrate the application of the framework and identify research and education initiatives that are needed to facilitate its development and implementation graphical abstract image 1 keywords deep uncertainty integrated assessment resilience stakeholders sustainability systemic 1 introduction many of the world s greatest challenges including those associated with climate change environmental contamination groundwater depletion biodiversity loss biological invasions disease outbreaks the food energy water nexus coastal and inland flooding interdependent infrastructure systems disaster management and urban planning are complex and socio environmental in the nature of their drivers interactions and impacts concepts that are often used to frame and study these problems include integrated assessment hamilton et al 2015 resilience hosseini et al 2016 righi et al 2015 and sustainability bettencourt and kaur 2011 bond and morrison saunders 2011 little et al 2016 miller et al 2014 to briefly illustrate the collective nature of these challenges a simple representation of a socio environmental system is provided in fig 1 showing how environmental economic and social systems form a system of interdependent systems examples of such real systems include watersheds land use coastal systems ecosystems agriculture forestry fisheries climate energy transportation communication as well as economic legal and other social systems in this article we focus on sustainability as the ultimate goal in treating these complex problems because assessing and enhancing sustainability requires integration across a wide range of environmental economic and social systems the other somewhat narrower but still complex socio environmental problems listed above are not as comprehensive as the goal of sustainability requiring integration across fewer systems in general however to resolve any socio environmental problems we need to elicit and integrate knowledge and explicit assumptions across a range of systems informing the design of solutions that take into account the complex and uncertain nature of the individual systems themselves and their interrelationships in sections 1 1 through 1 6 we discuss the five primary challenges that prevent us from surmounting this collective scientific challenge we then sketch the four elements of a proposed tiered system of systems modeling framework that addresses the collective challenge key concepts and terms are defined in table 1 1 1 primary challenge 1 the need for a comprehensive and consistent characterization of socio environmental policy goals the objective of any progressive examination of a specific socio environmental policy issue is achieving a better balance over time and space among socio economic and environmental outcomes we use the goal of sustainability in the following to motivate the need to explictly characterize and mechanize the goals of any challenging socio environmental problem because we see the approach to characterizing sustainability as analogous to the other more tractable but still extremely challenging policy problems for an overview of the field of sustainability readers are referred to a recent conceptual review little et al 2016 which identifies the collective limitations of the existing approaches for characterizing sustainability and briefly motivates the need for the proposed framework many authors have pointed out the difficulty of defining sustainability and sustainable development e g bare 2014 bond and morrison saunders 2011 bossel 1999 costanza and patten 1995 graedel and klee 2002 griggs et al 2014 kuhlman and farrington 2010 lélé 1991 pope et al 2004 in practice the definition of sustainability tends to be determined by the specific assessment approach being used and these vary widely little et al 2016 to characterize the sustainability of a socio environmental system the definition of sustainability must be applied comprehensively and consistently bossel proposed a way of doing this bossel 1996 1999 2007 suggesting that the sustainability of autonomous self organizing systems which include both ecosystems and human systems is determined by several basic orientors including existence effectiveness security adaptability and coexistence for both humans and ecosystems and freedom of action and psychological needs for humans as summarized in table 1 in little et al 2016 according to bossel 1999 the set of basic orientors is complete and covers all essential aspects of the supreme orientor which in this case is sustainability and each basic orientor is unique and cannot be replaced by the other basic orientors because the basic orientors are abstract in nature they need to be translated into a broad range of concrete operational orientors that can be more easily quantified bossel 1996 1999 2007 sustainability is then assessed as shown in fig 2 by comparing the operational orientors which reflect the desired state of the system to associated indicators which reflect the actual state of the system and evaluating the extent of orientor satisfaction in addition some orientors and indicators are inherently more important than others bossel 1999 2007 and relative weights are assigned fraser et al 2006 paracchini et al 2011 in a procedure which is guided by stakeholders voinov and bousquet 2010 voinov et al 2016 assigning weights does however require value judgements glynn et al 2017 voinov et al 2014 which may differ from person to person and from culture to culture making the process in which a group of stakeholders must reach agreement more difficult on the other hand this orientor based approach provides a flexible and systematic method that can be expanded and adjusted with stakeholder input as systems are added to the system of systems there is also the common assumption that sustainability can be achieved by simply identifying an appropriately comprehensive set of indicators although this in itself is difficult which overlooks the fact that most indicators are integrated within a system of complex interdependent systems as shown in fig 1 and are often context dependent as an example we refer to the 17 sustainable development goals which together with 169 targets and 230 indicators allen et al 2016 sridhar 2016 were recently identified by the united nations un 2015 concerted efforts to achieve these goals targets indicators without taking into account their interdependencies while entirely laudable will surely lead to unintended consequences sterman 2001 2012 because so many of them are causally related for instance some approaches to increasing food security may negatively affect the global climate system putting food security itself at risk griggs et al 2014 in fact it might well be argued that it is futile to make adjustments to the indicators in a complex system and expect positive outcomes without an understanding of the causal relationships that connect them because many indicators are integrated within a wide range of interdependent systems there needs to be a clear connection between the orientors and associated indicators which are used to quantify the supreme orientor sustainability or resilience for example and the conceptual framework defined in table 1 which is used to account for the drivers and causal relationships that govern the behavior of the system of systems defined in table 1 1 2 primary challenge 2 the need for a system of systems structure while there is growing recognition of the need for a systems approach to effectively characterize sustainability e g an and lópez carr 2012 fiksel 2012 hadian and madani 2015 housh et al 2014 little et al 2016 liu et al 2015 ramaswami et al 2012 and other socio environmental problems there is no agreement on what a systems approach entails usefully a recent review arnold and wade 2015 clarified the meaning of a systems approach by identifying eight key elements recognizing interconnections identifying and understanding feedback understanding system structure differentiating types of stocks flows and variables identifying and understanding non linear relationships understanding dynamic behavior understanding systems at different scales and reducing complexity by modeling systems conceptually while all eight elements are needed the use of models is especially important when attempting to understand and represent complex systems because the complexity of such systems overwhelms our ability to understand them sterman 2001 2012 this phenomenon which is sometimes referred to as policy resistance arises because complex systems are constantly changing tightly coupled governed by feedbacks nonlinear history dependent self organizing adaptive and evolving characterized by trade offs and counterintuitive sterman 2012 as a result many seemingly obvious solutions to problems fail or actually worsen the situation sterman 2012 causing what are more commonly known as unintended consequences mathematical models are thus the primary tools available for understanding complex systems and are essential ingredients for adaptive management defined in table 1 of interconnected complex systems models are therefore needed for each of the systems that are included in the system of systems with a large number of systems that are potentially involved and a need for systems integration liu et al 2015 a conceptual framework which is based on a system of systems delaurentis and crossley 2005 keating et al 2003 maier 1996 nielsen et al 2015 notion is required here we define a system of systems as a collection of independent constituent systems in which each fulfills its own purpose while acting jointly towards a common goal given the spatial scale of the various socio environmental systems that need to be coupled when focusing on the specific supreme orientor of interest sustainability resilience human health or ecosystem health for example it makes sense to begin at a regional scale keeping in mind the socioeconomic and policy drivers and impacts as the systems level conceptual approach is established we would map out the component systems in more detail keeping in mind that some of the relevant systems may be at a sub regional scale once the initial application is successful it could be applied across many regions with a need for repeated applications of models of similar systems the coupling of many models within a system of systems would be greatly facilitated with a component based software framework also defined in table 1 1 3 primary challenge 3 the need for a tiered structure the third challenge is the vast gap between bottom up with a high level of detail and top down with a lower level of detail approaches little et al 2016 we again use sustainability to motivate the need to ensure that bottom up and top down approaches are consistent for example top down sustainable development goals may be useful as a diagnostic test for assessing the state of nations but what detailed bottom up actions do we take if the indicators inform us that change is needed how do we know that the detailed actions we do take to improve sustainability will not result in unintended consequences to emphasize the point there have been relatively few attempts to synthesize multiple bottom up indicators to attain a more holistic perspective hester and little 2013 huber et al 2015 to connect bottom up and top down approaches in complex socio environmental problems we need a tiered structure with different levels of abstraction borshchev 2013 borshchev and filippov 2004 assuming only two levels for now one can envision more detailed process models at the process level and less detailed system models at the systems level with a consistent way of scaling the models from the process level to the systems level in addition to the tiered structure with different levels of abstraction we also need a way of connecting the tiers albeit loosely so that critical information can be passed among them little et al 2016 a further justification for the tiered structure pertains to the large number of systems that are potentially involved in any socio environmental problem if we only have a small number of process models of not inordinate complexity to couple we may be able to couple them directly at the process level but if we have many complex process level models that are highly resolved both spatially and temporally there is simply too much detail especially if we are coupling them with other models that have much lower resolution in these cases reduced order models also known as meta models or emulators castelletti et al 2012 ratto et al 2012 will be needed finally we note that all socio environmental problems involve policy decisions and that a tiered structure with a coupled system of systems model at the systems level would be especially advantageous because most policy decisions need to be implemented at the systems level and not at the process level of course options assessed at each level need to be clearly linked to actionable measures in the real systems so that suggested improvements can be translated into actions 1 4 primary challenge 4 the problem of deep uncertainty with the preceding discussion in mind sustainability resilience and other complex socio environmental problems generally qualify as wicked problems churchman 1967 rittel and webber 1973 xiang 2013 because the problems are embedded within a wide range of complex interdependent systems there is no optimal solution uncertainty is pervasive bammer 2008 and the stakes are contested a closely related concept with increasing currency is that of deep uncertainty defined in table 1 which exists when there is fundamental disagreement about the driving forces that will shape the future the probability distributions used to represent uncertainty and key variables and how to value alternative outcomes lempert 2002 lempert et al 2003 although uncertainty assessment is essential for making robust decisions in the face of deep uncertainty a qualitative grasp of uncertainties may suffice in some cases such as when stakeholders are engaged in social learning defined in table 1 so that future decisions can be made on a more informed and less contested basis on the other hand when quantitative assessment of uncertainties is needed for decision making novel computational experiments that sample the range of uncertainties and analyse the results will be required to address the problem of keeping the sampling efficient yet produce model outputs that cover the response behavior space of the model s this will be essential when models have long runtimes for any given parameter sample in some cases as mentioned in section 1 3 model emulators castelletti et al 2012 ratto et al 2012 may be built to achieve a faster running model under certain acceptable conditions 1 5 primary challenge 5 the fragmented nature of the disciplinary landscape when dealing with complex socio environmental problems the disciplinary landscape is fragmented with a wide range of professional and academic groups pursuing related goals but without effective interdisciplinary coordination in the case of sustainability little et al 2016 for example many disciplines have added elements of sustainability to their historical approaches resulting for example in green accounting green chemistry and green engineering professional societies are also engaged in initiatives that broaden their sphere of influence to include sustainability but here too the initiatives tend to begin with the traditional focus area of the society water or energy or transportation or ecology or economics for example and then extend to include other aspects of sustainability although these initiatives can only be applauded they collectively guarantee a fragmentation in approaches to assessing and enhancing sustainability integrated assessment defined in table 1 is a well established approach for evaluating environmental science technology and policy problems hamilton et al 2015 jakeman and letcher 2003 laniak et al 2013a rotmans and van asselt 2001 schneider 1997 the approach was designed as a meta discipline to overcome fragmentation and has been used extensively for climate change morgan and dowlatabadi 1996 patt et al 2010 schneider 1997 integrated water resources management jakeman and letcher 2003 and more recently for sustainability hacking and guthrie 2008 a methodology for the design and development of integrated assessment decision support systems mcintosh et al 2011 focuses on the overall iterative development process and includes stakeholders and policy makers scientists and engineers it specialists and the architect s of the decision support system van delden et al 2011 although integrated assessment emphasizes for example the integration of socio economic and ecological models jakeman and letcher 2003 to address policy questions that are derived from engagement with interest groups a more systematic approach that combines a system of systems framework with the integrated assessment methodology would be of considerable benefit for the fields of sustainability and resilience in particular and for resolving complex socio environmental problems in general 1 6 this article surmounting the collective scientific challenge in section 2 we sketch the outline of a tiered system of systems modeling framework for resolving complex socio environmental problems with four key elements a component based systems level software framework that can couple a wide range of relevant systems using a modular system of systems structure a tiered structure with different levels of abstraction that spans bottom up and top down approaches and establishes a systematic connection among the tiers the ability to inform robust decisions in the face of deep uncertainty and the systematic integration of multiple knowledge domains and disciplines the first two elements describe the structure of the framework while the last two elements describe ways in which the framework is implemented in section 3 we briefly illustrate the application of the framework arguing that while sustainability represents the most comprehensive socio environmental problem the other narrower socio environmental problems for example food energy and water systems and interdependent infrastructure systems are essentially subsets of the more comprehensive problem by initially focusing on the narrower problems but keeping the more comprehensive longer term goal in mind we can build confidence in the proposed framework in section 4 we identify research and education initiatives that are needed to facilitate the development and implementation of the framework and in section 5 we identify various barriers and enablers that will impede or expedite the implementation of the framework we conclude by acknowledging that the proposed framework represents a daunting challenge especially in the case of sustainability but argue that progress could be accelerated by initially focusing on the narrower socio environmental problems in a concerted and systematic fashion 2 outline of a tiered system of systems modeling framework in this section we describe the four key elements of the proposed framework 2 1 a component based modeling and software framework when developing a system of systems based on mathematical models within a conceptual framework we need to distinguish between the modeling approach e g system dynamics agent based models or mere linking of any style of computational models as in kelly et al 2013 and the software framework itself in this section we first provide further justification for the system of systems modeling framework and the range of available modeling approaches and then discuss the development of the component based systems level software framework 2 1 1 a system of systems modeling framework a simplified systems approach is already being implemented by the climate change community with integrated assessment models that include key features of human systems such as demography energy use technology the economy agriculture forestry and land use moss et al 2010 these models incorporate simplified representations of the climate system ecosystems and in some cases climate impacts and are calibrated against more complex climate and impact models the models are used to develop emissions scenarios simulate feedbacks estimate the potential economic impacts of climate change evaluate the costs and benefits of mitigation and evaluate uncertainties moss et al 2010 the development of these integrated assessment models involves two conceptual steps the first being the creation of reduced order models from more complex ones also known as up scaling and the second being the coupling of the up scaled components using a common computational structure thus we envisage a process level with process models that have a lower degree of abstraction and more detail as well as a systems level with system models that have a higher degree of abstraction and less detail we note here that the preference would often be for mechanistic models but that machine learning algorithms or expert systems krueger et al 2012 or even empirical relationships could suffice at least initially or when mechanistic understanding and or data are too limited the primary distinction is that the process models operate at a finer level of detail than the system models the integration of the system of systems involves several important challenges the models operate naturally at different temporal and spatial scales and individual models have different mathematical underpinnings although the systems are coupled through information exchange their models may have different inputs and outputs which must be logically connected and scaled there is also new emergent behavior of the coupled models due to interconnectivity which can exercise the individual models in new regimes vespignani 2010 to bridge the difference in spatial and temporal scales among the models and to harmonize the inputs and outputs scale issues would need to be addressed for each individual model such that models have similar spatial and temporal scales at their points of interaction and have compatible inputs and outputs there are many approaches used for modeling complex systems kelly et al 2013 reviewed five of these including system dynamics bayesian networks coupled component models which may also be thought of as hybrid models because they are assembled from a variety of different components agent based models and knowledge based models also known as expert systems kelly et al characterized the contexts in which each may be preferred others have been more definitive and restrictive in their regard as to appropriate approaches thus borshchev 2013 argues that system dynamics agent based models and discrete event models which employ a detailed process based approach are the three essential modeling approaches for simulating complex systems mobus and kalton 2015 identified system dynamics agent based models operations research or optimization and evolutionary models maier et al 2014 as the primary approaches for modeling complex systems concluding that a hybrid version of these approaches which they refer to as complex adaptive and evolvable systems is ultimately needed indeed hybrid or mixed method approaches are gaining in popularity e g howick et al 2016 morgan et al 2017 vincenot et al 2016 the required characteristics of the component based systems level conceptual framework must be considered at an early stage in the development of the overall framework system dynamics and agent based models would clearly be strong candidates as both of these approaches are currently employed in simulating socio environmental systems system dynamics models elsawah et al 2017 sterman 2012 are already being used extensively in evaluating the sustainability of natural social and engineered systems little et al 2016 agent based models are also used for natural social and engineered systems wilensky and rand 2015 and appear especially promising for ecosystems grimm and berger 2016 railsback and grimm 2011 economics farmer and foley 2009 and quantitative social science byrne and callaghan 2014 macy and willer 2002 primarily because of their emergent properties 2 1 2 a component based systems level software framework component based approaches compartmentalize each model that represents a system of interest into individual components the approach is widely adopted in the environmental modeling arena to facilitate the incorporation of existing models while making the development and coupling of new models more efficient de kok et al 2015 for example malard et al 2017 couple system dynamics with physically based models using a wrapper approach to represent a socio economic system and the effects on soil salinity in a farming system such wrappers compartmentalize and separate a model while handling the inter model data exchange processes laniak et al 2013a likewise a systems level software framework that implements such an approach across systems would greatly facilitate the development and implementation of the framework software frameworks as described by lloyd et al 2011 provide a reusable design which guides software developers in partitioning functionality into components and specify how components communicate and manage the order of execution generic frameworks provide support for general software elements such as database access enterprise services graphical interface design and transaction management while domain specific frameworks provide reusable design and functionality for specific knowledge domains frameworks themselves support model development by providing tools for example libraries of components as well as steps and processes that guide modelers for example the most recent version v3 of the object modeling system oms david et al 2013 includes a non invasive lightweight framework design supporting component based model development the use of domain specific language design patterns and provides a cloud based foundation for computational scalability as the framework is non invasive little to no change is required of a target model for its use within the framework once implemented components can be reused in other models coded to the same framework with little migration effort lloyd et al 2011 the community surface dynamics modeling system csdms approaches model integration in a similar manner through the basic modeling interface bmi standard peckham et al 2013 yet another example is the open modeling interface openmi standard gregersen et al 2007 moore and tindall 2005 which was originally developed for the hydrological sciences but has since been expanded to better represent processes in other domains recent advances in model integration frameworks and interoperability standards have lowered the technical barriers to achieving model integration the frameworks are largely method and programming language agnostic although interoperability between computational languages remains an issue the situation is improving the envisioned software framework would allow modelers to quickly develop component based models facilitating common activities in the development process these include component interaction and communication spatial temporal stepping and iteration up downscaling of spatial data multi threading multiprocessor support cross language interoperability and reusable tools for data integration analysis and visualization such a framework could for example be developed with python a general purpose programming language widely used in the computational sciences and implemented in one or more of the existing standards for inter model data exchange and communication in this manner existing integration frameworks and standards could be leveraged for these purposes interoperability among the frameworks may also be desirable and indeed a future possibility this would allow for example models developed with bmi to almost seamlessly interact with openmi wrapped models goodall and peckham 2016 these frameworks and standards aim to offer a consistent yet flexible approach to achieving model integration that said further investment may be needed to support an integrated system of systems framework including capabilities for meta model generation see section 2 2 data integration and other visualization capabilities as well as uncertainty analysis see section 2 3 could facilitate progress towards the proposed goal 2 2 a tiered structure to connect bottom up and top down approaches at both high and low levels of detail we need a tiered structure with different levels of abstraction borshchev 2013 borshchev and filippov 2004 having established this tiered structure we need to ensure that the coupled models at the systems level capture the essential features of the process level a consistent way of up scaling from the process level to the systems level is therefore needed as well as a way of connecting the tiers or levels so that critical information can be passed among them a variety of up scaling approaches also known as emulation modeling or meta modeling have been suggested for objectively identifying such dominant processes or influences ratto et al 2012 dynamic emulation modeling can be achieved in two ways castelletti et al 2012 structure based methods where the mathematical structure of the original model is manipulated to a simpler more computationally efficient form and data based methods where the emulator is identified from a data set generated via numerical experiments conducted on the large simulation model the two methods can be combined to improve accuracy while maintaining efficiency finally we note that emulation modeling is closely related to multiscale modeling hoekstra et al 2014 karabasov et al 2014 which usually involves one way or two way coupling between the scales in what we are proposing however the process level and systems level models are not intimately coupled when applying this up scaling approach to process models the indicators must also be up scaled indicators at the systems level will be by definition broader and more comprehensive than those at the process level with pure absolute or mid point indicators more common at the process level and integrated relative or end point indicators more common at the systems level for a brief review of absolute versus relative and mid point versus end point indicators see hester and little 2013 similar to the up scaling of mechanisms from the process models up scaling of indicators will invariably result in some loss of information it is therefore critical to identify which indicators or types of indicators dominate the characterization of the socio environmental problem at the process level and prioritize these for inclusion at the systems level it may well be the case that some of the indicators are not integrated within the models that form the system of systems these could initially be connected using a simple knowledge based system depending on their relative importance efforts could subsequently be made to include additional models that allow these indicators to be causally connected within the system of systems the up scaling of indicators will be an iterative process that is interconnected with and inseparable from the up scaling of the process models up scaling of process models and indicators will result in the creation of a series of system models that are coupled at the systems level if up scaled indicators at the systems level reveal that mechanisms represented at the process level are either missing or incomplete the process level models may also be adjusted this approach may similarly reveal the need for new or additional indicators at the process level consistent with the process of deriving indicators from the basic orientors bossel 2007 a simple representation of the proposed framework is shown in fig 3 in the tiered system of systems modeling framework important mechanisms are rooted in the process level and propagate upward while a comprehensive definition of the supreme orientor is rooted at the systems level ultimately in the basic orientors and propagates downwards the framework can be applied to resolve a wide range of socio environmental policy issues primarily by defining appropriate basic orientors for the supreme orientor of interest sustainability resilience human health or ecosystem health for example and by selecting the appropriate set of socio environmental systems to include in the system of systems the development and implementation of the framework would proceed in an iterative and modular fashion starting with process models that are already available and using the integrated assessment methodology as a reminder see definition in table 1 and the coupled models at the systems level however the iterative procedure may in time lead to the identification of a more appropriately scaled set of orientors and indicators for each process level model and it may then be possible to use the individual process models themselves to resolve socio environmental problems at the process level 2 3 the ability to inform decisions in the face of deep uncertainty the identification and management of uncertainty is a crucial task for resolving complex socio environmental problems while there has been considerable development of integrated environmental and socio economic models laniak et al 2013b there has been much less but growing generation of information about uncertainty and sensitivity in those models very little frank reporting of uncertainties and very little trust in or explicit use of uncertainty information by users of models jakeman et al 2006 as summarized in table 2 the sources of uncertainty that need to be considered and managed derive from data future forcing conditions parameters and or initial boundary conditions prior knowledge formal and informal model formulation assumptions model parameters and objective functions the value of the verification validation process and the uncertainty communication process model components of problems subject to deep uncertainty are uncertain for most if not all of these reasons uncertainties propagate among components and propagation is not effectively dealt with in an analytic framework due to there typically being a mix of model types uncertainties also arise in situations where results are contested due to the fact that multiple outputs may need to be weighted in different ways to try and recognise stakeholders with different values above all uncertainties and the process of deriving them need to be reported as a matter of course and in a more complete and hence transparent way there are useful existing methods and concepts that can be invoked in the quest for enhancing the treatment of uncertainty existing methods such as monte carlo based techniques including formal bayesian methods are well developed and can be used but the treatment of uncertainty might also vary depending on the objectives of the modeling exercise if it is to increase social learning about a problem among stakeholders then uncertainty can be handled more easily as it may only require participants to understand qualitative relationships and their implications however if the objective is prediction then a more sophisticated treatment of uncertainty would be required if however the objective is to discriminate among management alternatives then outcomes can be evaluated based on the difference or change in outcomes of interest for a given scenario relative to some benchmark scenario as shown by reichert and borsuk 2005 uncertainties in such differentials are lower than with absolutes concepts such as risk and vulnerability are also often relevant for focusing the task of managing uncertainty they can be invoked to simplify the questions being asked of a modeling problem and thereby reduce the demands on uncertainty characterization thus one may attempt to assess the risk of a bad outcome rather than trying to obtain a picture of all outcomes under all conditions most of the quantitative methods for characterizing model uncertainty focus on the prediction objective and involve running the model many times the most utilised is monte carlo sampling of model parameter space where each sample provides a prediction of outcomes so that multiple runs generate a distribution of outcomes invoking this in a probabilistic bayesian setting is becoming popular though many assumptions about prior parameter values and errors may be required and convergence can be a problem there are also more brute force techniques often non probabilistic that aim to stress test a model to assess under what conditions acceptable and non acceptable outcomes are predicted a qualitative approach in complex modeling situations should not be under valued indeed it may be sufficient or at least be a useful adjunct in some cases one way of approaching this is through quality assurance of the modeling process refsgaard et al 2007 and its constituent assumptions while another is to include qualitative judgements about the information and how it is produced van der sluijs et al 2005 we also note that adaptive management which views policies as if they were experiments with results from one generation of study informing subsequent decisions is a useful tool when dealing with uncertainty and which can enhance social learning data integration visualization and analysis tools are also needed to provide intuitive descriptions of complex and large scale simulation data in the face of deep uncertainty the form and scale of model output should be more carefully considered many models are non identifiable and analysis tools are needed to expose critical parameters and uncertainties so that improved identifiable and likely simpler model formulations can be obtained simpler formulations which include emulation models that perform at least as well in prediction have several advantages when undertaking an uncertainty analysis as a final note problem framing and stakeholder engagement are now generally regarded as crucial when the problem has deep uncertainty in order to ensure the right problem is being addressed crucial knowledge and perspectives are identified and trust is generated 2 4 new educational and organizational support structures as already mentioned combining a system of systems modeling framework with the integrated assessment methodology could be of considerable benefit for resolving several families of complex socio environmental problems because of the vast scale of the collective scientific challenge and the large number of disciplines and knowledge domains involved we envision a new category of specially trained systems scientists and engineers they would be familiar with models at both the process and the systems level and their role would be to couple the system models in the component based systems level software framework the solid horizontal arrows in fig 3 these systems scientists and engineers would orchestrate the exchange of information among the systems and would act as facilitators for communication between the two levels the dashed vertical arrows in fig 3 when trying to couple a wide range of different knowledge domains it is neither possible nor desirable for all scientists and engineers working on the problem to be actively engaged in systems integration in fact the vast majority of scientists and engineers would be experts in their own disciplines or domains we refer to them as process experts but would need to have some familiarity with systems integration and the component based software framework a small fraction of the total would be experts in systems integration and the component based software framework we refer to them as systems experts but would also have training in one or more of the specific disciplines or domains as new models are acquired or developed at the systems level they can be coupled to the existing system models and the success of that integration checked bennett et al 2013 jakeman et al 2006 the advantage of having a tiered structure that maintains the process based models is that access to the more detailed mechanistic predictions that are possible with process models is maintained even if these relationships are not hard wired in this way process experts can continue to develop and validate their existing process models and detailed expertise is maintained in those fields increasing communication between those working at the systems level and those working at the process level will ensure that new insights are passed between the two levels so that knowledge and understanding can be simultaneously improved at both levels we note here that this tiered educational structure is reminiscent of the education of t shaped professionals heinemann 2009 mcintosh and taylor 2013 uhlenbrook and de jong 2012 but in our case we need the systems experts to also have detailed knowledge of their own discipline or domain so that they can be responsible for developing the emulation models from the more detailed process models in their discipline or domain in addition they need to use this knowledge when coupling their systems level models with systems level models from other disciplines or domains 3 illustrative examples here we provide a brief illustration of how one could combine the integrated assessment methodology as a reminder see definition in table 1 with the proposed tiered system of systems framework to address two socio environmental issues the food energy water nexus and interdependent infrastructure systems we select sustainability as the supreme orientor for food energy and water systems and resilience as the supreme orientor for evaluating the impact of coastal flooding on interdependent infrastructure systems as shown in figs 4 and 5 respectively in each case we begin with available models of a few systems but implement the framework in a way that allows additional systems to be added in a modular fashion wherever possible we would take advantage of existing modeling infrastructure and scientific expertise by using component models and knowledge that have accrued from intensive studies over a long period 3 1 sustainable food energy and water systems to assess and enhance the sustainability of food energy and water systems in the chesapeake bay watershed we would build upon the integrated assessment methodology this would produce a stakeholder driven voinov and bousquet 2010 voinov et al 2016 conceptual system of systems model that as part of the joint problem framing identifies operational orientors indicators and specific process models to be included and that recognizes the context for decision making and how the various uncertainties are to be prioritized and managed as shown in fig 4 we could take advantage of the existing chesapeake bay model which comprises a suite of process level models including watershed and estuary models cbp 2012 shenk et al 2012 voinov and cerco 2010 modifying them for our purpose and identifying any process model gaps in terms of the requirements of the overall conceptual model to represent food and energy we could initially choose a spatially resolved version of an economic model duchin 2005 duchin and levine 2012 with agriculture seafood and energy as three of many sectors and with water represented by the process level watershed and estuary models we would then up scale the process models to the systems level as described in section 2 2 and having the needs of the overall conceptual model in mind indeed the dynamics of the process level models could be extracted in a way that best suits the purpose of the models at the systems level however the spatial resolution of the economic model is at the county scale and it would therefore be directly coupled at the systems level as shown in fig 4 the three system models would form a system of systems using the component based software framework the basic orientors would be used to derive appropriate operational orientors suitable for these specific systems as well as identify associated indicators and these would then be used to assess and enhance sustainability recognizing that several systems would only be added at a later stage once the initial set of systems were being successfully simulated we could begin to include other systems relevant to the chesapeake bay watershed for example we could include agriculture fisheries and energy models at the process level and then up scale these to the systems level creating additional systems level models and removing the representation of agriculture fisheries and energy from the economic model as already mentioned some of the initial indicators may not be causally integrated within the models these could initially be connected using a knowledge based system krueger et al 2012 and depending on their relative importance could subsequently be included in additional systems that allow these indicators to be causally integrated within the system of systems as new systems were added additional orientors and indicators would be added that are relevant to the new systems making the assessment of sustainability increasingly comprehensive some of the knowledge domains and disciplines do not yet have appropriate models especially social systems but initial versions of these would need to be developed in this way we would build complexity as we develop confidence in the modeling framework for example see jakeman et al 1994 keeping in mind the overall goal for the system of systems model once sustainability is being assessed and enhanced at the regional scale the tiered framework could then be applied to regions of similar scale in other areas if the approach proves successful for several regions across the globe the potential to link across regions nations continents and oceans could be considered methods and approaches for modeling the anthropocene are increasingly being implemented verburg et al 2016 and a modular system of systems framework would be of great benefit in these situations as well to connect all regions we would likely have to up scale see section 2 2 the system models at the regional scale to the national or continental scale creating an additional systems level with this new set of systems again coupled using a component based software framework a repository of re useable components could be made available and applied across regions nations continents and oceans at that point we would have a global model that could be used together with down scaled orientors and up scaled indicators to assess and enhance global sustainability but this would clearly be a major undertaking that would need to be coordinated by an organization like the united nations 3 2 resilience of infrastructure systems to coastal flooding as modern societies become more complex critical interdependent infrastructure systems are more likely to fail under stress unless they are designed to be resilient ellingwood et al 2016 nan and sansavini 2017 zio 2016 resilient infrastructure systems maintain the flow of goods and services in the face of a broad range of natural and anthropogenic hazards as shown in fig 5 we could use the exact same procedures outlined in section 3 1 for sustainable food energy and water to enhance resilience in coastal louisiana which has experienced the catastrophic effects of several land falling hurricanes in recent years although widespread agreement has not been reached on a definition cutter et al 2014 undp 2014 a recent report undp 2014 defines resilience as the capacity to anticipate prevent recover from and transform in the aftermath of shocks stresses and changes the report recommends that measurements of resilience need to be linked to clear targets and that a multi scale generic and multi dimensional approach for resilience that encompasses many dimensions including physical technical economic human social political institutional ecological and environmental should be adopted conceptually these requirements are similar to those for sustainability with a need for a wide range of orientors to be compared to indicators as shown in fig 2 that are integrated within a broad range of socio environmental systems as shown in fig 1 resilience would therefore replace sustainability as the supreme orientor in fig 2 with several basic orientors that capture the abstract essence of resilience and many more concrete operational orientors reflecting desired targets that are compared to the actual values of associated indicators in the system of systems in choosing the relevant systems for a conceptual stakeholder driven system of systems model we could initially focus on a surge and inundation model bilskie et al 2014 a flood protection model duncan et al 2008 and all major infrastructure sectors using a spatially resolved infrastructure and economic model haimes et al 2005 okuyama and santos 2014 as shown in fig 5 the surge and inundation model and the flood protection model would both be implemented at the process level and then up scaled to the systems level while the economic model would be less spatially resolved and would be implemented directly at the systems level once the initial set of systems were being successfully simulated we could begin to include other systems relevant to coastal louisiana for example we could include models for various infrastructure systems at the process level and then up scale these to the systems level creating additional systems level models and removing the representation of these infrastructure sectors from the combined infrastructure and economic model in time to make the assessment of resilience increasingly comprehensive we could extend to environmental and social systems cutter et al 2003 2014 magis 2010 keeping in mind from the outset their role in the conceptual system of systems model 4 next steps in research education and practice in this section we briefly discuss initiatives that are needed for the component based modeling and software framework the tiered structure and scaling procedures decision making under deep uncertainty new educational and organizational support structures and specifications for the proposed framework 4 1 a component based modeling and software framework an omnipresent problem associated with coupling complex models is the vast difference in temporal and spatial scales among models for example climate models span global to regional spatial scales and seasonal to decadal temporal scales while watershed models are spatially explicit at the regional and local scale and temporally explicit over timescales from hours to years the mismatch in scales can be resolved to some extent with judicious aggregation during up scaling from the process to the systems level but large gaps will no doubt remain in some regions this may require that extensive sets of new data be collected and or that methods and expert opinion be used to fill knowledge gaps in this regard the convergence of pervasive sensing with location aware and social media technologies along with infrastructure based sensors is leading to the production and collection of big data in many areas rao et al 2015 zaslavsky et al 2012 and it may be possible to capitalize on this proliferation to help fill in the gaps in spatial and temporal phenomena further challenges include the potential for nonlinear systems to exhibit unpredictable phase change behavior monasson et al 1999 solé et al 1996 the formation of alternate stable states beisner et al 2003 scheffer et al 2001 schröder et al 2005 suding et al 2004 as well as panarchy which describes the conditions that control cycles of growth accumulation restructuring and renewal in coupled human and natural systems garmestani et al 2009 holling 1973 2001 we need to decide on which complex systems should be included for a specific region the level and scope of detail that is necessary for the purpose of the overall conceptual model and how the specific systems should be arranged and organized the last refers to the model composition and structure both semantic will the model composition output useful results and syntactic how the component models are coupled in a technical sense the nuts and bolts for example do we start with a geographic basis emphasizing demographics and then layer additional systems on top of that what about the problem of reconciling natural features watersheds and airsheds with economic and political zones cities and regions progress is also needed in identifying the scope and generic features of the specific systems that need to be included in the system of socio environmental systems although there is some overlap in the systems listed in section 1 a modular approach with a repository of system models that can be used repeatedly in different regions and in different combinations means that system models that are already coupled may need to be decomposed nevertheless a systematic approach with a library of system models for a wide range of real world systems would be extremely valuable 4 2 tiered structure and scaling procedures clearly the necessary fundamental knowledge mechanistic understanding and data are not available for all systems of interest but we can surely make useful progress if we start building on what we have in addition it may be that some models are initially only available at the systems level the tiered framework would therefore be simultaneously developed at both the process level and the systems level procedures for the identification of appropriate sets of basic orientors operational orientors and indicators as well as the consistent up scaling of indicators and process level models and down scaling of orientors and systems level models are needed another problem is the difficulty of up scaling systems with emergent properties from the process level to the systems level and the analogous lack of reductionism for some systems at the systems level finally as discussed in section 3 1 it may be possible to use additional systems level tiers in the framework so that the sustainability or resilience of large sectors of society nations and continents could eventually be characterized ultimately leading to a more realistic assessment of global health 4 3 decision making under deep uncertainty a component based modeling approach will necessarily result in integrated models of high complexity uncertainty assessment of the upper tier system of systems model must first proceed with uncertainty assessment and understanding of the components in that tier as well as uncertainty assessment in each of the lower tier process models generally this is not practised as integrated models have tended to be assessed as a whole and not examined for uncertainties in their component models and how they propagate through the model linkages a new mindset is therefore required to address the challenge of analyzing component models and understanding and assessing their uncertainties as well as capturing how they propagate through the system of systems there are however promising new exploratory modeling and analysis techniques and software that analyse integrated models as a whole in an exploratory sense e g hadka et al 2015 rather than in a predictive sense these techniques are aimed at exploring the effects of policy options under uncertainties in future conditions and model assumptions which are sometimes framed in an effort to assist robust decision making and with outputs that produce pareto fronts that illustrate tradeoffs among the outcomes of interest kasprzyk et al 2013 watson and kasprzyk 2017 the most basic step in uncertainty assessment is to ensure that the modeling addresses the questions being asked this applies not just to the high level objective prediction adaptive management social learning discriminating among management alternatives but also to characterization of specific functions of the quantities of interest homing in on these functions may also serve to simplify the demands of the modeling task for example for a hydrological problem one may not be concerned with the whole time series of certain fluxes but perhaps some integral of those over time and space thereby focusing and reducing the uncertainty requirements indeed it must follow that one allows for the expense of essential analysis of model uncertainty and ascertains what uncertainties are crucial for the specific functions of the quantities of interest and concentrate on them attention to problem framing and stakeholder engagement is crucial when the problem has deep uncertainty in order to manage various aspects of it such as getting the problem framing right the quantities of interest set and soft and hard prior knowledge incorporated managing these uncertainties has several elements including initially identifying and ranking the importance of their sources so that it can be reduced where needed and possible and generally appreciated 4 4 new educational and organizational support structures the disciplinary landscape is fragmented with a wide range of professional and academic groups pursuing related goals but without much formal coordination existing scientific and professional organizations that are focused on integrated assessment resilience or sustainability could re align some of their activities or new organizations may emerge to take on the challenge these organizations could provide independent institutional oversight guiding the consistent development and implementation of the proposed framework the strong disciplinary structure of higher education is of great value to society because it produces much needed disciplinary experts unfortunately it also severely constrains interdisciplinary interaction consequently to ensure the success of the proposed framework we need increasingly novel approaches to education that more strongly integrate across the social physical and life sciences and engineering to create a new generation of systems scientists and engineers in civil engineering for example most undergraduate programs require core competence in civil engineering plus an area of specialization such as construction environmental geotechnical materials structural transportation or water resources engineering we therefore propose adding a new area of specialization to educational programs that is focused on systems then as shown in fig 3 trained civil systems engineers would integrate models at the systems level working in collaboration with systems experts from other disciplines represented by the horizontal solid arrows the civil systems engineers would also facilitate two way communication between the systems level and the process level represented by the vertical dashed arrows the civil engineers at the process level would be introduced to the systems approach while developing new engineering knowledge at the process level two way communication between the systems level and the process level coupled with interdisciplinary communication among all disciplines and knowledge domains at the systems level will be crucial for the successful implementation of the proposed framework we conclude this section by noting the similarity between what we are proposing and general systems theory gst von bertalanffy 1950 1972 which aims to provide a foundational theory of universal principles applying to systems in general rousseau 2015 although the ongoing fragmentation of the systems community casts a long shadow over the vision of discovering and developing gst contemporary work suggests that gst is a realistic prospect that has the potential to support interdisciplinary communication and cooperation facilitate scientific discoveries promote the unity of knowledge and provide a disciplined way to build a systematically healthy world rousseau 2015 thus systems experts from the systems discipline could play a central role in building the proposed framework and in helping to design the curriculum associated with the training of systems experts in all other disciplines and knowledge domains 4 5 specifications for the proposed framework the design principles of the new framework need to be specified this encompasses the individual systems the system of systems the way the systems interact and exchange information the tiered structure and procedures for scaling among the tiers and the orientors and indicators this is not to say that there are no frameworks currently available which address some of these concerns for system model coupling and integration there are in fact many each with their own design philosophies and implementation approaches belete et al 2017 although a full review of available frameworks is inappropriate here no single framework appears to have received majority support while a common aim is to ease the technical burden of coupling models a steep learning curve still exists for example in the context of applying the openmi framework it has been recommended knapen et al 2013 that model developers improve their understanding of software development principles before attempting model integration the proposed framework would ideally be made accessible for model developers within interdisciplinary teams to cater for the diversity in technical ability this may mean making the framework extensible preferably in its native programming language or providing a scripting language and facilitating direct interfaces with existing tools and models which may be written in other programming languages including a graphical interface to enable ease of use would be an additional desirable feature malard et al 2017 a good graphical interface should enable users to more easily interact with the framework facilitating the handover process to end users in cases where the users are not the developers and overcome barriers to adoption of the framework crout et al 2008 5 looking ahead a tiered system of systems modeling framework for resolving complex socio environmental policy issues we have sketched the outline of a generic framework for resolving complex socio environmental problems in this section we briefly identify some barriers and enablers that will impede or accelerate the implementation of the framework and conclude with a cautionary perspective on systems thinking complexity and comprehensiveness potential barriers to building a system of systems model for such socio environmental issues include the challenge of increasing system understanding and informing robust decisions in the face of deep uncertainty and the lack of reliable models for many of the social systems that are needed potential enablers include the current focus of research on deep uncertainty and the adoption of integrated uncertainty assessment approaches which consider the various sources of uncertainty throughout the modeling process and their implications for the modeling purpose maier et al 2016 another enabler is current research focused on consolidating and synthesizing knowledge about coupling environmental and social models schlüter et al 2017 beyond single case studies to generate lessons learned and illuminate promising lines of inquiry www sesmo org an additional barrier exists because in most cases specific socio environmental problems are tackled separately without acknowledging or understanding similarities across problem domains however as shown in section 3 the approach to characterizing the sustainability of food energy and water systems is analogous to the approach to characterizing the resilience of interdependent infrastructure systems to coastal flooding in addition only a few models are usually coupled without much thought given to the possibility of extending to include additional systems the proposed framework could serve as an enabler in these cases providing a way to more effectively elicit and integrate knowledge across a wide range of systems and across several families of socio environmental problems implementing the proposed framework represents a daunting challenge especially in the case of sustainability but even recognition of the need for a framework that can be commonly applied across knowledge domains and disciplines to resolve families of socio environmental problems is a critical first step by initially focusing on somewhat narrower socio environmental problems but keeping the more comprehensive longer term sustainability goal in mind we can build confidence in the proposed framework to facilitate the implementation of the framework we envision a transformation in our approach to science and engineering that spans research education and practice as described above we propose new educational initiatives for training the next generation of process and systems scientists and engineers this transformation can build upon the current science and engineering enterprise in an inclusive way such that many relevant disciplines would be engaged in the development of the framework finally we acknowledge that the field of systems thinking is rich in schools of thought with different epistemological and ontological stances jackson 2010 midgley 2000 the same is true for environmental modeling with its epistemological pluralism macmynowski 2007 the proposed framework is driven from the systems engineering field which largely operates from a positivistic or functionalist paradigm based on ontological assumptions that systems causes and events along with mechanisms and processes operate more or less independently of the observer indeed the concept of post structuralism scheele et al 2018 warrants recognition with choices and assumptions in modeling made as transparent as possible as already emphasized we propose to begin with narrower socio environmental problems but work towards the more comprehensive goal of sustainability in doing so we acknowledge the question posed by ulrich how can we deal critically with the fact that our thinking and hence our knowledge designs and actions cannot possibly be comprehensive in the sense that we never comprehend all that ought to be understood before we pass to judgment and action ulrich 1993 acknowledgements partial financial support was received from the global change center and the fralin life science institute at virginia tech as well as the national socio environmental synthesis center sesync at the university of maryland 
26269,many of the world s greatest challenges are complex socio environmental problems often framed in terms of integrated assessment resilience or sustainability to resolve any of these challenges it is essential to elicit and integrate knowledge across a range of systems informing the design of solutions that take into account the complex and uncertain nature of the individual systems and their interrelationships to meet this scientific challenge we propose a tiered system of systems modeling framework with these elements a component based software framework that couples a wide range of relevant systems using a modular system of systems structure a tiered structure with different levels of abstraction that spans bottom up and top down approaches the ability to inform robust decisions in the face of deep uncertainty and the systematic integration of multiple knowledge domains and disciplines we illustrate the application of the framework and identify research and education initiatives that are needed to facilitate its development and implementation graphical abstract image 1 keywords deep uncertainty integrated assessment resilience stakeholders sustainability systemic 1 introduction many of the world s greatest challenges including those associated with climate change environmental contamination groundwater depletion biodiversity loss biological invasions disease outbreaks the food energy water nexus coastal and inland flooding interdependent infrastructure systems disaster management and urban planning are complex and socio environmental in the nature of their drivers interactions and impacts concepts that are often used to frame and study these problems include integrated assessment hamilton et al 2015 resilience hosseini et al 2016 righi et al 2015 and sustainability bettencourt and kaur 2011 bond and morrison saunders 2011 little et al 2016 miller et al 2014 to briefly illustrate the collective nature of these challenges a simple representation of a socio environmental system is provided in fig 1 showing how environmental economic and social systems form a system of interdependent systems examples of such real systems include watersheds land use coastal systems ecosystems agriculture forestry fisheries climate energy transportation communication as well as economic legal and other social systems in this article we focus on sustainability as the ultimate goal in treating these complex problems because assessing and enhancing sustainability requires integration across a wide range of environmental economic and social systems the other somewhat narrower but still complex socio environmental problems listed above are not as comprehensive as the goal of sustainability requiring integration across fewer systems in general however to resolve any socio environmental problems we need to elicit and integrate knowledge and explicit assumptions across a range of systems informing the design of solutions that take into account the complex and uncertain nature of the individual systems themselves and their interrelationships in sections 1 1 through 1 6 we discuss the five primary challenges that prevent us from surmounting this collective scientific challenge we then sketch the four elements of a proposed tiered system of systems modeling framework that addresses the collective challenge key concepts and terms are defined in table 1 1 1 primary challenge 1 the need for a comprehensive and consistent characterization of socio environmental policy goals the objective of any progressive examination of a specific socio environmental policy issue is achieving a better balance over time and space among socio economic and environmental outcomes we use the goal of sustainability in the following to motivate the need to explictly characterize and mechanize the goals of any challenging socio environmental problem because we see the approach to characterizing sustainability as analogous to the other more tractable but still extremely challenging policy problems for an overview of the field of sustainability readers are referred to a recent conceptual review little et al 2016 which identifies the collective limitations of the existing approaches for characterizing sustainability and briefly motivates the need for the proposed framework many authors have pointed out the difficulty of defining sustainability and sustainable development e g bare 2014 bond and morrison saunders 2011 bossel 1999 costanza and patten 1995 graedel and klee 2002 griggs et al 2014 kuhlman and farrington 2010 lélé 1991 pope et al 2004 in practice the definition of sustainability tends to be determined by the specific assessment approach being used and these vary widely little et al 2016 to characterize the sustainability of a socio environmental system the definition of sustainability must be applied comprehensively and consistently bossel proposed a way of doing this bossel 1996 1999 2007 suggesting that the sustainability of autonomous self organizing systems which include both ecosystems and human systems is determined by several basic orientors including existence effectiveness security adaptability and coexistence for both humans and ecosystems and freedom of action and psychological needs for humans as summarized in table 1 in little et al 2016 according to bossel 1999 the set of basic orientors is complete and covers all essential aspects of the supreme orientor which in this case is sustainability and each basic orientor is unique and cannot be replaced by the other basic orientors because the basic orientors are abstract in nature they need to be translated into a broad range of concrete operational orientors that can be more easily quantified bossel 1996 1999 2007 sustainability is then assessed as shown in fig 2 by comparing the operational orientors which reflect the desired state of the system to associated indicators which reflect the actual state of the system and evaluating the extent of orientor satisfaction in addition some orientors and indicators are inherently more important than others bossel 1999 2007 and relative weights are assigned fraser et al 2006 paracchini et al 2011 in a procedure which is guided by stakeholders voinov and bousquet 2010 voinov et al 2016 assigning weights does however require value judgements glynn et al 2017 voinov et al 2014 which may differ from person to person and from culture to culture making the process in which a group of stakeholders must reach agreement more difficult on the other hand this orientor based approach provides a flexible and systematic method that can be expanded and adjusted with stakeholder input as systems are added to the system of systems there is also the common assumption that sustainability can be achieved by simply identifying an appropriately comprehensive set of indicators although this in itself is difficult which overlooks the fact that most indicators are integrated within a system of complex interdependent systems as shown in fig 1 and are often context dependent as an example we refer to the 17 sustainable development goals which together with 169 targets and 230 indicators allen et al 2016 sridhar 2016 were recently identified by the united nations un 2015 concerted efforts to achieve these goals targets indicators without taking into account their interdependencies while entirely laudable will surely lead to unintended consequences sterman 2001 2012 because so many of them are causally related for instance some approaches to increasing food security may negatively affect the global climate system putting food security itself at risk griggs et al 2014 in fact it might well be argued that it is futile to make adjustments to the indicators in a complex system and expect positive outcomes without an understanding of the causal relationships that connect them because many indicators are integrated within a wide range of interdependent systems there needs to be a clear connection between the orientors and associated indicators which are used to quantify the supreme orientor sustainability or resilience for example and the conceptual framework defined in table 1 which is used to account for the drivers and causal relationships that govern the behavior of the system of systems defined in table 1 1 2 primary challenge 2 the need for a system of systems structure while there is growing recognition of the need for a systems approach to effectively characterize sustainability e g an and lópez carr 2012 fiksel 2012 hadian and madani 2015 housh et al 2014 little et al 2016 liu et al 2015 ramaswami et al 2012 and other socio environmental problems there is no agreement on what a systems approach entails usefully a recent review arnold and wade 2015 clarified the meaning of a systems approach by identifying eight key elements recognizing interconnections identifying and understanding feedback understanding system structure differentiating types of stocks flows and variables identifying and understanding non linear relationships understanding dynamic behavior understanding systems at different scales and reducing complexity by modeling systems conceptually while all eight elements are needed the use of models is especially important when attempting to understand and represent complex systems because the complexity of such systems overwhelms our ability to understand them sterman 2001 2012 this phenomenon which is sometimes referred to as policy resistance arises because complex systems are constantly changing tightly coupled governed by feedbacks nonlinear history dependent self organizing adaptive and evolving characterized by trade offs and counterintuitive sterman 2012 as a result many seemingly obvious solutions to problems fail or actually worsen the situation sterman 2012 causing what are more commonly known as unintended consequences mathematical models are thus the primary tools available for understanding complex systems and are essential ingredients for adaptive management defined in table 1 of interconnected complex systems models are therefore needed for each of the systems that are included in the system of systems with a large number of systems that are potentially involved and a need for systems integration liu et al 2015 a conceptual framework which is based on a system of systems delaurentis and crossley 2005 keating et al 2003 maier 1996 nielsen et al 2015 notion is required here we define a system of systems as a collection of independent constituent systems in which each fulfills its own purpose while acting jointly towards a common goal given the spatial scale of the various socio environmental systems that need to be coupled when focusing on the specific supreme orientor of interest sustainability resilience human health or ecosystem health for example it makes sense to begin at a regional scale keeping in mind the socioeconomic and policy drivers and impacts as the systems level conceptual approach is established we would map out the component systems in more detail keeping in mind that some of the relevant systems may be at a sub regional scale once the initial application is successful it could be applied across many regions with a need for repeated applications of models of similar systems the coupling of many models within a system of systems would be greatly facilitated with a component based software framework also defined in table 1 1 3 primary challenge 3 the need for a tiered structure the third challenge is the vast gap between bottom up with a high level of detail and top down with a lower level of detail approaches little et al 2016 we again use sustainability to motivate the need to ensure that bottom up and top down approaches are consistent for example top down sustainable development goals may be useful as a diagnostic test for assessing the state of nations but what detailed bottom up actions do we take if the indicators inform us that change is needed how do we know that the detailed actions we do take to improve sustainability will not result in unintended consequences to emphasize the point there have been relatively few attempts to synthesize multiple bottom up indicators to attain a more holistic perspective hester and little 2013 huber et al 2015 to connect bottom up and top down approaches in complex socio environmental problems we need a tiered structure with different levels of abstraction borshchev 2013 borshchev and filippov 2004 assuming only two levels for now one can envision more detailed process models at the process level and less detailed system models at the systems level with a consistent way of scaling the models from the process level to the systems level in addition to the tiered structure with different levels of abstraction we also need a way of connecting the tiers albeit loosely so that critical information can be passed among them little et al 2016 a further justification for the tiered structure pertains to the large number of systems that are potentially involved in any socio environmental problem if we only have a small number of process models of not inordinate complexity to couple we may be able to couple them directly at the process level but if we have many complex process level models that are highly resolved both spatially and temporally there is simply too much detail especially if we are coupling them with other models that have much lower resolution in these cases reduced order models also known as meta models or emulators castelletti et al 2012 ratto et al 2012 will be needed finally we note that all socio environmental problems involve policy decisions and that a tiered structure with a coupled system of systems model at the systems level would be especially advantageous because most policy decisions need to be implemented at the systems level and not at the process level of course options assessed at each level need to be clearly linked to actionable measures in the real systems so that suggested improvements can be translated into actions 1 4 primary challenge 4 the problem of deep uncertainty with the preceding discussion in mind sustainability resilience and other complex socio environmental problems generally qualify as wicked problems churchman 1967 rittel and webber 1973 xiang 2013 because the problems are embedded within a wide range of complex interdependent systems there is no optimal solution uncertainty is pervasive bammer 2008 and the stakes are contested a closely related concept with increasing currency is that of deep uncertainty defined in table 1 which exists when there is fundamental disagreement about the driving forces that will shape the future the probability distributions used to represent uncertainty and key variables and how to value alternative outcomes lempert 2002 lempert et al 2003 although uncertainty assessment is essential for making robust decisions in the face of deep uncertainty a qualitative grasp of uncertainties may suffice in some cases such as when stakeholders are engaged in social learning defined in table 1 so that future decisions can be made on a more informed and less contested basis on the other hand when quantitative assessment of uncertainties is needed for decision making novel computational experiments that sample the range of uncertainties and analyse the results will be required to address the problem of keeping the sampling efficient yet produce model outputs that cover the response behavior space of the model s this will be essential when models have long runtimes for any given parameter sample in some cases as mentioned in section 1 3 model emulators castelletti et al 2012 ratto et al 2012 may be built to achieve a faster running model under certain acceptable conditions 1 5 primary challenge 5 the fragmented nature of the disciplinary landscape when dealing with complex socio environmental problems the disciplinary landscape is fragmented with a wide range of professional and academic groups pursuing related goals but without effective interdisciplinary coordination in the case of sustainability little et al 2016 for example many disciplines have added elements of sustainability to their historical approaches resulting for example in green accounting green chemistry and green engineering professional societies are also engaged in initiatives that broaden their sphere of influence to include sustainability but here too the initiatives tend to begin with the traditional focus area of the society water or energy or transportation or ecology or economics for example and then extend to include other aspects of sustainability although these initiatives can only be applauded they collectively guarantee a fragmentation in approaches to assessing and enhancing sustainability integrated assessment defined in table 1 is a well established approach for evaluating environmental science technology and policy problems hamilton et al 2015 jakeman and letcher 2003 laniak et al 2013a rotmans and van asselt 2001 schneider 1997 the approach was designed as a meta discipline to overcome fragmentation and has been used extensively for climate change morgan and dowlatabadi 1996 patt et al 2010 schneider 1997 integrated water resources management jakeman and letcher 2003 and more recently for sustainability hacking and guthrie 2008 a methodology for the design and development of integrated assessment decision support systems mcintosh et al 2011 focuses on the overall iterative development process and includes stakeholders and policy makers scientists and engineers it specialists and the architect s of the decision support system van delden et al 2011 although integrated assessment emphasizes for example the integration of socio economic and ecological models jakeman and letcher 2003 to address policy questions that are derived from engagement with interest groups a more systematic approach that combines a system of systems framework with the integrated assessment methodology would be of considerable benefit for the fields of sustainability and resilience in particular and for resolving complex socio environmental problems in general 1 6 this article surmounting the collective scientific challenge in section 2 we sketch the outline of a tiered system of systems modeling framework for resolving complex socio environmental problems with four key elements a component based systems level software framework that can couple a wide range of relevant systems using a modular system of systems structure a tiered structure with different levels of abstraction that spans bottom up and top down approaches and establishes a systematic connection among the tiers the ability to inform robust decisions in the face of deep uncertainty and the systematic integration of multiple knowledge domains and disciplines the first two elements describe the structure of the framework while the last two elements describe ways in which the framework is implemented in section 3 we briefly illustrate the application of the framework arguing that while sustainability represents the most comprehensive socio environmental problem the other narrower socio environmental problems for example food energy and water systems and interdependent infrastructure systems are essentially subsets of the more comprehensive problem by initially focusing on the narrower problems but keeping the more comprehensive longer term goal in mind we can build confidence in the proposed framework in section 4 we identify research and education initiatives that are needed to facilitate the development and implementation of the framework and in section 5 we identify various barriers and enablers that will impede or expedite the implementation of the framework we conclude by acknowledging that the proposed framework represents a daunting challenge especially in the case of sustainability but argue that progress could be accelerated by initially focusing on the narrower socio environmental problems in a concerted and systematic fashion 2 outline of a tiered system of systems modeling framework in this section we describe the four key elements of the proposed framework 2 1 a component based modeling and software framework when developing a system of systems based on mathematical models within a conceptual framework we need to distinguish between the modeling approach e g system dynamics agent based models or mere linking of any style of computational models as in kelly et al 2013 and the software framework itself in this section we first provide further justification for the system of systems modeling framework and the range of available modeling approaches and then discuss the development of the component based systems level software framework 2 1 1 a system of systems modeling framework a simplified systems approach is already being implemented by the climate change community with integrated assessment models that include key features of human systems such as demography energy use technology the economy agriculture forestry and land use moss et al 2010 these models incorporate simplified representations of the climate system ecosystems and in some cases climate impacts and are calibrated against more complex climate and impact models the models are used to develop emissions scenarios simulate feedbacks estimate the potential economic impacts of climate change evaluate the costs and benefits of mitigation and evaluate uncertainties moss et al 2010 the development of these integrated assessment models involves two conceptual steps the first being the creation of reduced order models from more complex ones also known as up scaling and the second being the coupling of the up scaled components using a common computational structure thus we envisage a process level with process models that have a lower degree of abstraction and more detail as well as a systems level with system models that have a higher degree of abstraction and less detail we note here that the preference would often be for mechanistic models but that machine learning algorithms or expert systems krueger et al 2012 or even empirical relationships could suffice at least initially or when mechanistic understanding and or data are too limited the primary distinction is that the process models operate at a finer level of detail than the system models the integration of the system of systems involves several important challenges the models operate naturally at different temporal and spatial scales and individual models have different mathematical underpinnings although the systems are coupled through information exchange their models may have different inputs and outputs which must be logically connected and scaled there is also new emergent behavior of the coupled models due to interconnectivity which can exercise the individual models in new regimes vespignani 2010 to bridge the difference in spatial and temporal scales among the models and to harmonize the inputs and outputs scale issues would need to be addressed for each individual model such that models have similar spatial and temporal scales at their points of interaction and have compatible inputs and outputs there are many approaches used for modeling complex systems kelly et al 2013 reviewed five of these including system dynamics bayesian networks coupled component models which may also be thought of as hybrid models because they are assembled from a variety of different components agent based models and knowledge based models also known as expert systems kelly et al characterized the contexts in which each may be preferred others have been more definitive and restrictive in their regard as to appropriate approaches thus borshchev 2013 argues that system dynamics agent based models and discrete event models which employ a detailed process based approach are the three essential modeling approaches for simulating complex systems mobus and kalton 2015 identified system dynamics agent based models operations research or optimization and evolutionary models maier et al 2014 as the primary approaches for modeling complex systems concluding that a hybrid version of these approaches which they refer to as complex adaptive and evolvable systems is ultimately needed indeed hybrid or mixed method approaches are gaining in popularity e g howick et al 2016 morgan et al 2017 vincenot et al 2016 the required characteristics of the component based systems level conceptual framework must be considered at an early stage in the development of the overall framework system dynamics and agent based models would clearly be strong candidates as both of these approaches are currently employed in simulating socio environmental systems system dynamics models elsawah et al 2017 sterman 2012 are already being used extensively in evaluating the sustainability of natural social and engineered systems little et al 2016 agent based models are also used for natural social and engineered systems wilensky and rand 2015 and appear especially promising for ecosystems grimm and berger 2016 railsback and grimm 2011 economics farmer and foley 2009 and quantitative social science byrne and callaghan 2014 macy and willer 2002 primarily because of their emergent properties 2 1 2 a component based systems level software framework component based approaches compartmentalize each model that represents a system of interest into individual components the approach is widely adopted in the environmental modeling arena to facilitate the incorporation of existing models while making the development and coupling of new models more efficient de kok et al 2015 for example malard et al 2017 couple system dynamics with physically based models using a wrapper approach to represent a socio economic system and the effects on soil salinity in a farming system such wrappers compartmentalize and separate a model while handling the inter model data exchange processes laniak et al 2013a likewise a systems level software framework that implements such an approach across systems would greatly facilitate the development and implementation of the framework software frameworks as described by lloyd et al 2011 provide a reusable design which guides software developers in partitioning functionality into components and specify how components communicate and manage the order of execution generic frameworks provide support for general software elements such as database access enterprise services graphical interface design and transaction management while domain specific frameworks provide reusable design and functionality for specific knowledge domains frameworks themselves support model development by providing tools for example libraries of components as well as steps and processes that guide modelers for example the most recent version v3 of the object modeling system oms david et al 2013 includes a non invasive lightweight framework design supporting component based model development the use of domain specific language design patterns and provides a cloud based foundation for computational scalability as the framework is non invasive little to no change is required of a target model for its use within the framework once implemented components can be reused in other models coded to the same framework with little migration effort lloyd et al 2011 the community surface dynamics modeling system csdms approaches model integration in a similar manner through the basic modeling interface bmi standard peckham et al 2013 yet another example is the open modeling interface openmi standard gregersen et al 2007 moore and tindall 2005 which was originally developed for the hydrological sciences but has since been expanded to better represent processes in other domains recent advances in model integration frameworks and interoperability standards have lowered the technical barriers to achieving model integration the frameworks are largely method and programming language agnostic although interoperability between computational languages remains an issue the situation is improving the envisioned software framework would allow modelers to quickly develop component based models facilitating common activities in the development process these include component interaction and communication spatial temporal stepping and iteration up downscaling of spatial data multi threading multiprocessor support cross language interoperability and reusable tools for data integration analysis and visualization such a framework could for example be developed with python a general purpose programming language widely used in the computational sciences and implemented in one or more of the existing standards for inter model data exchange and communication in this manner existing integration frameworks and standards could be leveraged for these purposes interoperability among the frameworks may also be desirable and indeed a future possibility this would allow for example models developed with bmi to almost seamlessly interact with openmi wrapped models goodall and peckham 2016 these frameworks and standards aim to offer a consistent yet flexible approach to achieving model integration that said further investment may be needed to support an integrated system of systems framework including capabilities for meta model generation see section 2 2 data integration and other visualization capabilities as well as uncertainty analysis see section 2 3 could facilitate progress towards the proposed goal 2 2 a tiered structure to connect bottom up and top down approaches at both high and low levels of detail we need a tiered structure with different levels of abstraction borshchev 2013 borshchev and filippov 2004 having established this tiered structure we need to ensure that the coupled models at the systems level capture the essential features of the process level a consistent way of up scaling from the process level to the systems level is therefore needed as well as a way of connecting the tiers or levels so that critical information can be passed among them a variety of up scaling approaches also known as emulation modeling or meta modeling have been suggested for objectively identifying such dominant processes or influences ratto et al 2012 dynamic emulation modeling can be achieved in two ways castelletti et al 2012 structure based methods where the mathematical structure of the original model is manipulated to a simpler more computationally efficient form and data based methods where the emulator is identified from a data set generated via numerical experiments conducted on the large simulation model the two methods can be combined to improve accuracy while maintaining efficiency finally we note that emulation modeling is closely related to multiscale modeling hoekstra et al 2014 karabasov et al 2014 which usually involves one way or two way coupling between the scales in what we are proposing however the process level and systems level models are not intimately coupled when applying this up scaling approach to process models the indicators must also be up scaled indicators at the systems level will be by definition broader and more comprehensive than those at the process level with pure absolute or mid point indicators more common at the process level and integrated relative or end point indicators more common at the systems level for a brief review of absolute versus relative and mid point versus end point indicators see hester and little 2013 similar to the up scaling of mechanisms from the process models up scaling of indicators will invariably result in some loss of information it is therefore critical to identify which indicators or types of indicators dominate the characterization of the socio environmental problem at the process level and prioritize these for inclusion at the systems level it may well be the case that some of the indicators are not integrated within the models that form the system of systems these could initially be connected using a simple knowledge based system depending on their relative importance efforts could subsequently be made to include additional models that allow these indicators to be causally connected within the system of systems the up scaling of indicators will be an iterative process that is interconnected with and inseparable from the up scaling of the process models up scaling of process models and indicators will result in the creation of a series of system models that are coupled at the systems level if up scaled indicators at the systems level reveal that mechanisms represented at the process level are either missing or incomplete the process level models may also be adjusted this approach may similarly reveal the need for new or additional indicators at the process level consistent with the process of deriving indicators from the basic orientors bossel 2007 a simple representation of the proposed framework is shown in fig 3 in the tiered system of systems modeling framework important mechanisms are rooted in the process level and propagate upward while a comprehensive definition of the supreme orientor is rooted at the systems level ultimately in the basic orientors and propagates downwards the framework can be applied to resolve a wide range of socio environmental policy issues primarily by defining appropriate basic orientors for the supreme orientor of interest sustainability resilience human health or ecosystem health for example and by selecting the appropriate set of socio environmental systems to include in the system of systems the development and implementation of the framework would proceed in an iterative and modular fashion starting with process models that are already available and using the integrated assessment methodology as a reminder see definition in table 1 and the coupled models at the systems level however the iterative procedure may in time lead to the identification of a more appropriately scaled set of orientors and indicators for each process level model and it may then be possible to use the individual process models themselves to resolve socio environmental problems at the process level 2 3 the ability to inform decisions in the face of deep uncertainty the identification and management of uncertainty is a crucial task for resolving complex socio environmental problems while there has been considerable development of integrated environmental and socio economic models laniak et al 2013b there has been much less but growing generation of information about uncertainty and sensitivity in those models very little frank reporting of uncertainties and very little trust in or explicit use of uncertainty information by users of models jakeman et al 2006 as summarized in table 2 the sources of uncertainty that need to be considered and managed derive from data future forcing conditions parameters and or initial boundary conditions prior knowledge formal and informal model formulation assumptions model parameters and objective functions the value of the verification validation process and the uncertainty communication process model components of problems subject to deep uncertainty are uncertain for most if not all of these reasons uncertainties propagate among components and propagation is not effectively dealt with in an analytic framework due to there typically being a mix of model types uncertainties also arise in situations where results are contested due to the fact that multiple outputs may need to be weighted in different ways to try and recognise stakeholders with different values above all uncertainties and the process of deriving them need to be reported as a matter of course and in a more complete and hence transparent way there are useful existing methods and concepts that can be invoked in the quest for enhancing the treatment of uncertainty existing methods such as monte carlo based techniques including formal bayesian methods are well developed and can be used but the treatment of uncertainty might also vary depending on the objectives of the modeling exercise if it is to increase social learning about a problem among stakeholders then uncertainty can be handled more easily as it may only require participants to understand qualitative relationships and their implications however if the objective is prediction then a more sophisticated treatment of uncertainty would be required if however the objective is to discriminate among management alternatives then outcomes can be evaluated based on the difference or change in outcomes of interest for a given scenario relative to some benchmark scenario as shown by reichert and borsuk 2005 uncertainties in such differentials are lower than with absolutes concepts such as risk and vulnerability are also often relevant for focusing the task of managing uncertainty they can be invoked to simplify the questions being asked of a modeling problem and thereby reduce the demands on uncertainty characterization thus one may attempt to assess the risk of a bad outcome rather than trying to obtain a picture of all outcomes under all conditions most of the quantitative methods for characterizing model uncertainty focus on the prediction objective and involve running the model many times the most utilised is monte carlo sampling of model parameter space where each sample provides a prediction of outcomes so that multiple runs generate a distribution of outcomes invoking this in a probabilistic bayesian setting is becoming popular though many assumptions about prior parameter values and errors may be required and convergence can be a problem there are also more brute force techniques often non probabilistic that aim to stress test a model to assess under what conditions acceptable and non acceptable outcomes are predicted a qualitative approach in complex modeling situations should not be under valued indeed it may be sufficient or at least be a useful adjunct in some cases one way of approaching this is through quality assurance of the modeling process refsgaard et al 2007 and its constituent assumptions while another is to include qualitative judgements about the information and how it is produced van der sluijs et al 2005 we also note that adaptive management which views policies as if they were experiments with results from one generation of study informing subsequent decisions is a useful tool when dealing with uncertainty and which can enhance social learning data integration visualization and analysis tools are also needed to provide intuitive descriptions of complex and large scale simulation data in the face of deep uncertainty the form and scale of model output should be more carefully considered many models are non identifiable and analysis tools are needed to expose critical parameters and uncertainties so that improved identifiable and likely simpler model formulations can be obtained simpler formulations which include emulation models that perform at least as well in prediction have several advantages when undertaking an uncertainty analysis as a final note problem framing and stakeholder engagement are now generally regarded as crucial when the problem has deep uncertainty in order to ensure the right problem is being addressed crucial knowledge and perspectives are identified and trust is generated 2 4 new educational and organizational support structures as already mentioned combining a system of systems modeling framework with the integrated assessment methodology could be of considerable benefit for resolving several families of complex socio environmental problems because of the vast scale of the collective scientific challenge and the large number of disciplines and knowledge domains involved we envision a new category of specially trained systems scientists and engineers they would be familiar with models at both the process and the systems level and their role would be to couple the system models in the component based systems level software framework the solid horizontal arrows in fig 3 these systems scientists and engineers would orchestrate the exchange of information among the systems and would act as facilitators for communication between the two levels the dashed vertical arrows in fig 3 when trying to couple a wide range of different knowledge domains it is neither possible nor desirable for all scientists and engineers working on the problem to be actively engaged in systems integration in fact the vast majority of scientists and engineers would be experts in their own disciplines or domains we refer to them as process experts but would need to have some familiarity with systems integration and the component based software framework a small fraction of the total would be experts in systems integration and the component based software framework we refer to them as systems experts but would also have training in one or more of the specific disciplines or domains as new models are acquired or developed at the systems level they can be coupled to the existing system models and the success of that integration checked bennett et al 2013 jakeman et al 2006 the advantage of having a tiered structure that maintains the process based models is that access to the more detailed mechanistic predictions that are possible with process models is maintained even if these relationships are not hard wired in this way process experts can continue to develop and validate their existing process models and detailed expertise is maintained in those fields increasing communication between those working at the systems level and those working at the process level will ensure that new insights are passed between the two levels so that knowledge and understanding can be simultaneously improved at both levels we note here that this tiered educational structure is reminiscent of the education of t shaped professionals heinemann 2009 mcintosh and taylor 2013 uhlenbrook and de jong 2012 but in our case we need the systems experts to also have detailed knowledge of their own discipline or domain so that they can be responsible for developing the emulation models from the more detailed process models in their discipline or domain in addition they need to use this knowledge when coupling their systems level models with systems level models from other disciplines or domains 3 illustrative examples here we provide a brief illustration of how one could combine the integrated assessment methodology as a reminder see definition in table 1 with the proposed tiered system of systems framework to address two socio environmental issues the food energy water nexus and interdependent infrastructure systems we select sustainability as the supreme orientor for food energy and water systems and resilience as the supreme orientor for evaluating the impact of coastal flooding on interdependent infrastructure systems as shown in figs 4 and 5 respectively in each case we begin with available models of a few systems but implement the framework in a way that allows additional systems to be added in a modular fashion wherever possible we would take advantage of existing modeling infrastructure and scientific expertise by using component models and knowledge that have accrued from intensive studies over a long period 3 1 sustainable food energy and water systems to assess and enhance the sustainability of food energy and water systems in the chesapeake bay watershed we would build upon the integrated assessment methodology this would produce a stakeholder driven voinov and bousquet 2010 voinov et al 2016 conceptual system of systems model that as part of the joint problem framing identifies operational orientors indicators and specific process models to be included and that recognizes the context for decision making and how the various uncertainties are to be prioritized and managed as shown in fig 4 we could take advantage of the existing chesapeake bay model which comprises a suite of process level models including watershed and estuary models cbp 2012 shenk et al 2012 voinov and cerco 2010 modifying them for our purpose and identifying any process model gaps in terms of the requirements of the overall conceptual model to represent food and energy we could initially choose a spatially resolved version of an economic model duchin 2005 duchin and levine 2012 with agriculture seafood and energy as three of many sectors and with water represented by the process level watershed and estuary models we would then up scale the process models to the systems level as described in section 2 2 and having the needs of the overall conceptual model in mind indeed the dynamics of the process level models could be extracted in a way that best suits the purpose of the models at the systems level however the spatial resolution of the economic model is at the county scale and it would therefore be directly coupled at the systems level as shown in fig 4 the three system models would form a system of systems using the component based software framework the basic orientors would be used to derive appropriate operational orientors suitable for these specific systems as well as identify associated indicators and these would then be used to assess and enhance sustainability recognizing that several systems would only be added at a later stage once the initial set of systems were being successfully simulated we could begin to include other systems relevant to the chesapeake bay watershed for example we could include agriculture fisheries and energy models at the process level and then up scale these to the systems level creating additional systems level models and removing the representation of agriculture fisheries and energy from the economic model as already mentioned some of the initial indicators may not be causally integrated within the models these could initially be connected using a knowledge based system krueger et al 2012 and depending on their relative importance could subsequently be included in additional systems that allow these indicators to be causally integrated within the system of systems as new systems were added additional orientors and indicators would be added that are relevant to the new systems making the assessment of sustainability increasingly comprehensive some of the knowledge domains and disciplines do not yet have appropriate models especially social systems but initial versions of these would need to be developed in this way we would build complexity as we develop confidence in the modeling framework for example see jakeman et al 1994 keeping in mind the overall goal for the system of systems model once sustainability is being assessed and enhanced at the regional scale the tiered framework could then be applied to regions of similar scale in other areas if the approach proves successful for several regions across the globe the potential to link across regions nations continents and oceans could be considered methods and approaches for modeling the anthropocene are increasingly being implemented verburg et al 2016 and a modular system of systems framework would be of great benefit in these situations as well to connect all regions we would likely have to up scale see section 2 2 the system models at the regional scale to the national or continental scale creating an additional systems level with this new set of systems again coupled using a component based software framework a repository of re useable components could be made available and applied across regions nations continents and oceans at that point we would have a global model that could be used together with down scaled orientors and up scaled indicators to assess and enhance global sustainability but this would clearly be a major undertaking that would need to be coordinated by an organization like the united nations 3 2 resilience of infrastructure systems to coastal flooding as modern societies become more complex critical interdependent infrastructure systems are more likely to fail under stress unless they are designed to be resilient ellingwood et al 2016 nan and sansavini 2017 zio 2016 resilient infrastructure systems maintain the flow of goods and services in the face of a broad range of natural and anthropogenic hazards as shown in fig 5 we could use the exact same procedures outlined in section 3 1 for sustainable food energy and water to enhance resilience in coastal louisiana which has experienced the catastrophic effects of several land falling hurricanes in recent years although widespread agreement has not been reached on a definition cutter et al 2014 undp 2014 a recent report undp 2014 defines resilience as the capacity to anticipate prevent recover from and transform in the aftermath of shocks stresses and changes the report recommends that measurements of resilience need to be linked to clear targets and that a multi scale generic and multi dimensional approach for resilience that encompasses many dimensions including physical technical economic human social political institutional ecological and environmental should be adopted conceptually these requirements are similar to those for sustainability with a need for a wide range of orientors to be compared to indicators as shown in fig 2 that are integrated within a broad range of socio environmental systems as shown in fig 1 resilience would therefore replace sustainability as the supreme orientor in fig 2 with several basic orientors that capture the abstract essence of resilience and many more concrete operational orientors reflecting desired targets that are compared to the actual values of associated indicators in the system of systems in choosing the relevant systems for a conceptual stakeholder driven system of systems model we could initially focus on a surge and inundation model bilskie et al 2014 a flood protection model duncan et al 2008 and all major infrastructure sectors using a spatially resolved infrastructure and economic model haimes et al 2005 okuyama and santos 2014 as shown in fig 5 the surge and inundation model and the flood protection model would both be implemented at the process level and then up scaled to the systems level while the economic model would be less spatially resolved and would be implemented directly at the systems level once the initial set of systems were being successfully simulated we could begin to include other systems relevant to coastal louisiana for example we could include models for various infrastructure systems at the process level and then up scale these to the systems level creating additional systems level models and removing the representation of these infrastructure sectors from the combined infrastructure and economic model in time to make the assessment of resilience increasingly comprehensive we could extend to environmental and social systems cutter et al 2003 2014 magis 2010 keeping in mind from the outset their role in the conceptual system of systems model 4 next steps in research education and practice in this section we briefly discuss initiatives that are needed for the component based modeling and software framework the tiered structure and scaling procedures decision making under deep uncertainty new educational and organizational support structures and specifications for the proposed framework 4 1 a component based modeling and software framework an omnipresent problem associated with coupling complex models is the vast difference in temporal and spatial scales among models for example climate models span global to regional spatial scales and seasonal to decadal temporal scales while watershed models are spatially explicit at the regional and local scale and temporally explicit over timescales from hours to years the mismatch in scales can be resolved to some extent with judicious aggregation during up scaling from the process to the systems level but large gaps will no doubt remain in some regions this may require that extensive sets of new data be collected and or that methods and expert opinion be used to fill knowledge gaps in this regard the convergence of pervasive sensing with location aware and social media technologies along with infrastructure based sensors is leading to the production and collection of big data in many areas rao et al 2015 zaslavsky et al 2012 and it may be possible to capitalize on this proliferation to help fill in the gaps in spatial and temporal phenomena further challenges include the potential for nonlinear systems to exhibit unpredictable phase change behavior monasson et al 1999 solé et al 1996 the formation of alternate stable states beisner et al 2003 scheffer et al 2001 schröder et al 2005 suding et al 2004 as well as panarchy which describes the conditions that control cycles of growth accumulation restructuring and renewal in coupled human and natural systems garmestani et al 2009 holling 1973 2001 we need to decide on which complex systems should be included for a specific region the level and scope of detail that is necessary for the purpose of the overall conceptual model and how the specific systems should be arranged and organized the last refers to the model composition and structure both semantic will the model composition output useful results and syntactic how the component models are coupled in a technical sense the nuts and bolts for example do we start with a geographic basis emphasizing demographics and then layer additional systems on top of that what about the problem of reconciling natural features watersheds and airsheds with economic and political zones cities and regions progress is also needed in identifying the scope and generic features of the specific systems that need to be included in the system of socio environmental systems although there is some overlap in the systems listed in section 1 a modular approach with a repository of system models that can be used repeatedly in different regions and in different combinations means that system models that are already coupled may need to be decomposed nevertheless a systematic approach with a library of system models for a wide range of real world systems would be extremely valuable 4 2 tiered structure and scaling procedures clearly the necessary fundamental knowledge mechanistic understanding and data are not available for all systems of interest but we can surely make useful progress if we start building on what we have in addition it may be that some models are initially only available at the systems level the tiered framework would therefore be simultaneously developed at both the process level and the systems level procedures for the identification of appropriate sets of basic orientors operational orientors and indicators as well as the consistent up scaling of indicators and process level models and down scaling of orientors and systems level models are needed another problem is the difficulty of up scaling systems with emergent properties from the process level to the systems level and the analogous lack of reductionism for some systems at the systems level finally as discussed in section 3 1 it may be possible to use additional systems level tiers in the framework so that the sustainability or resilience of large sectors of society nations and continents could eventually be characterized ultimately leading to a more realistic assessment of global health 4 3 decision making under deep uncertainty a component based modeling approach will necessarily result in integrated models of high complexity uncertainty assessment of the upper tier system of systems model must first proceed with uncertainty assessment and understanding of the components in that tier as well as uncertainty assessment in each of the lower tier process models generally this is not practised as integrated models have tended to be assessed as a whole and not examined for uncertainties in their component models and how they propagate through the model linkages a new mindset is therefore required to address the challenge of analyzing component models and understanding and assessing their uncertainties as well as capturing how they propagate through the system of systems there are however promising new exploratory modeling and analysis techniques and software that analyse integrated models as a whole in an exploratory sense e g hadka et al 2015 rather than in a predictive sense these techniques are aimed at exploring the effects of policy options under uncertainties in future conditions and model assumptions which are sometimes framed in an effort to assist robust decision making and with outputs that produce pareto fronts that illustrate tradeoffs among the outcomes of interest kasprzyk et al 2013 watson and kasprzyk 2017 the most basic step in uncertainty assessment is to ensure that the modeling addresses the questions being asked this applies not just to the high level objective prediction adaptive management social learning discriminating among management alternatives but also to characterization of specific functions of the quantities of interest homing in on these functions may also serve to simplify the demands of the modeling task for example for a hydrological problem one may not be concerned with the whole time series of certain fluxes but perhaps some integral of those over time and space thereby focusing and reducing the uncertainty requirements indeed it must follow that one allows for the expense of essential analysis of model uncertainty and ascertains what uncertainties are crucial for the specific functions of the quantities of interest and concentrate on them attention to problem framing and stakeholder engagement is crucial when the problem has deep uncertainty in order to manage various aspects of it such as getting the problem framing right the quantities of interest set and soft and hard prior knowledge incorporated managing these uncertainties has several elements including initially identifying and ranking the importance of their sources so that it can be reduced where needed and possible and generally appreciated 4 4 new educational and organizational support structures the disciplinary landscape is fragmented with a wide range of professional and academic groups pursuing related goals but without much formal coordination existing scientific and professional organizations that are focused on integrated assessment resilience or sustainability could re align some of their activities or new organizations may emerge to take on the challenge these organizations could provide independent institutional oversight guiding the consistent development and implementation of the proposed framework the strong disciplinary structure of higher education is of great value to society because it produces much needed disciplinary experts unfortunately it also severely constrains interdisciplinary interaction consequently to ensure the success of the proposed framework we need increasingly novel approaches to education that more strongly integrate across the social physical and life sciences and engineering to create a new generation of systems scientists and engineers in civil engineering for example most undergraduate programs require core competence in civil engineering plus an area of specialization such as construction environmental geotechnical materials structural transportation or water resources engineering we therefore propose adding a new area of specialization to educational programs that is focused on systems then as shown in fig 3 trained civil systems engineers would integrate models at the systems level working in collaboration with systems experts from other disciplines represented by the horizontal solid arrows the civil systems engineers would also facilitate two way communication between the systems level and the process level represented by the vertical dashed arrows the civil engineers at the process level would be introduced to the systems approach while developing new engineering knowledge at the process level two way communication between the systems level and the process level coupled with interdisciplinary communication among all disciplines and knowledge domains at the systems level will be crucial for the successful implementation of the proposed framework we conclude this section by noting the similarity between what we are proposing and general systems theory gst von bertalanffy 1950 1972 which aims to provide a foundational theory of universal principles applying to systems in general rousseau 2015 although the ongoing fragmentation of the systems community casts a long shadow over the vision of discovering and developing gst contemporary work suggests that gst is a realistic prospect that has the potential to support interdisciplinary communication and cooperation facilitate scientific discoveries promote the unity of knowledge and provide a disciplined way to build a systematically healthy world rousseau 2015 thus systems experts from the systems discipline could play a central role in building the proposed framework and in helping to design the curriculum associated with the training of systems experts in all other disciplines and knowledge domains 4 5 specifications for the proposed framework the design principles of the new framework need to be specified this encompasses the individual systems the system of systems the way the systems interact and exchange information the tiered structure and procedures for scaling among the tiers and the orientors and indicators this is not to say that there are no frameworks currently available which address some of these concerns for system model coupling and integration there are in fact many each with their own design philosophies and implementation approaches belete et al 2017 although a full review of available frameworks is inappropriate here no single framework appears to have received majority support while a common aim is to ease the technical burden of coupling models a steep learning curve still exists for example in the context of applying the openmi framework it has been recommended knapen et al 2013 that model developers improve their understanding of software development principles before attempting model integration the proposed framework would ideally be made accessible for model developers within interdisciplinary teams to cater for the diversity in technical ability this may mean making the framework extensible preferably in its native programming language or providing a scripting language and facilitating direct interfaces with existing tools and models which may be written in other programming languages including a graphical interface to enable ease of use would be an additional desirable feature malard et al 2017 a good graphical interface should enable users to more easily interact with the framework facilitating the handover process to end users in cases where the users are not the developers and overcome barriers to adoption of the framework crout et al 2008 5 looking ahead a tiered system of systems modeling framework for resolving complex socio environmental policy issues we have sketched the outline of a generic framework for resolving complex socio environmental problems in this section we briefly identify some barriers and enablers that will impede or accelerate the implementation of the framework and conclude with a cautionary perspective on systems thinking complexity and comprehensiveness potential barriers to building a system of systems model for such socio environmental issues include the challenge of increasing system understanding and informing robust decisions in the face of deep uncertainty and the lack of reliable models for many of the social systems that are needed potential enablers include the current focus of research on deep uncertainty and the adoption of integrated uncertainty assessment approaches which consider the various sources of uncertainty throughout the modeling process and their implications for the modeling purpose maier et al 2016 another enabler is current research focused on consolidating and synthesizing knowledge about coupling environmental and social models schlüter et al 2017 beyond single case studies to generate lessons learned and illuminate promising lines of inquiry www sesmo org an additional barrier exists because in most cases specific socio environmental problems are tackled separately without acknowledging or understanding similarities across problem domains however as shown in section 3 the approach to characterizing the sustainability of food energy and water systems is analogous to the approach to characterizing the resilience of interdependent infrastructure systems to coastal flooding in addition only a few models are usually coupled without much thought given to the possibility of extending to include additional systems the proposed framework could serve as an enabler in these cases providing a way to more effectively elicit and integrate knowledge across a wide range of systems and across several families of socio environmental problems implementing the proposed framework represents a daunting challenge especially in the case of sustainability but even recognition of the need for a framework that can be commonly applied across knowledge domains and disciplines to resolve families of socio environmental problems is a critical first step by initially focusing on somewhat narrower socio environmental problems but keeping the more comprehensive longer term sustainability goal in mind we can build confidence in the proposed framework to facilitate the implementation of the framework we envision a transformation in our approach to science and engineering that spans research education and practice as described above we propose new educational initiatives for training the next generation of process and systems scientists and engineers this transformation can build upon the current science and engineering enterprise in an inclusive way such that many relevant disciplines would be engaged in the development of the framework finally we acknowledge that the field of systems thinking is rich in schools of thought with different epistemological and ontological stances jackson 2010 midgley 2000 the same is true for environmental modeling with its epistemological pluralism macmynowski 2007 the proposed framework is driven from the systems engineering field which largely operates from a positivistic or functionalist paradigm based on ontological assumptions that systems causes and events along with mechanisms and processes operate more or less independently of the observer indeed the concept of post structuralism scheele et al 2018 warrants recognition with choices and assumptions in modeling made as transparent as possible as already emphasized we propose to begin with narrower socio environmental problems but work towards the more comprehensive goal of sustainability in doing so we acknowledge the question posed by ulrich how can we deal critically with the fact that our thinking and hence our knowledge designs and actions cannot possibly be comprehensive in the sense that we never comprehend all that ought to be understood before we pass to judgment and action ulrich 1993 acknowledgements partial financial support was received from the global change center and the fralin life science institute at virginia tech as well as the national socio environmental synthesis center sesync at the university of maryland 
