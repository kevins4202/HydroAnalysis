index,text
26450,mechanistic models rely on specification of parameters representing biophysical traits and process rates such as phytoplankton zooplankton and seagrass growth and respiration rates organism sizes stoichiometry light temperature and nutrient responses nutrient specific excretion rates and detrital stoichiometry and decay rates choosing suitable values for these parameters is difficult current practise is problematic this paper presents a resource designed to facilitate an evidence based approach to parameterisation of aquatic ecosystem models an online tool is provided which collates relevant published biological trait and biogeochemical rate observations from many sources and allows users to explore filter and convert these data in a consistent reproducible way to find parameter values and calculate probability distributions using this information within a traditional or bayesian paradigm should provide improved understanding of the uncertainty and predictive capacity of aquatic ecosystem models and provide insight into current sources of structural error in models keywords parameterisation biogeochemical rates biological traits aquatic ecosystems parameter priors seagrass phytoplankton zooplankton remineralisation software and data availability software and data described in this paper are available from the parameter library exploration interface at http shiny csiro au cdm parameterlibrary latest 1 introduction aquatic ecosystem modelling has a long history of use to shed light on ecosystem function e g scheffer et al 2001 and provide input to policy and management decisions to improve water quality limit the impact of pollutants and toxicants specify fish catches inform management of catchments and riparian vegetation and control eutrophication e g wang et al 2012 although there are several approaches to simulation of environmental systems robson 2014 process based models including complex mechanistic models have been enduringly popular these models aim to mimic the roles of individual processes through mathematical descriptions with realistic parameterisations the premise is that mathematical representations of individual processes combine to collectively offer a faithful depiction of ecosystem dynamics the appeal of this approach is that by simulating systems in terms of real ideally measurable physical geochemical and biological processes mechanistic models offer the prospect of greater insight into system function than other types of models as well as the promise of defensible extrapolation beyond historical conditions to predict future changes the degree to which a mechanistic model can deliver on these promises depends not only on the accuracy of the conceptual understanding of the system represented by the equations used to build the model but also on the accuracy of its parameterisation 2 current state of the art parameterisation of aquatic ecosystem models has historically been a difficult and labour intensive task parameter values are typically required for a range of model components defining traits and responses of phytoplankton zooplankton and other aquatic biota characteristics of sediments and detrital material physical and chemical process rates and more recently bacterial traits and processes although mathematical methods have long been available to automate parameter estimation and calibration these have not been rapidly or universally adopted in this field due to computational constraints or data limitations shimoda and arhonditsis 2016 traditional approaches to setting parameter values have been either to combine where available local measurements of flux rates and component responses for the study site with literature values for parameters that have not been measured locally e g mao et al 2015 murray and parslow 1999 robson and hamilton 2004 or to begin with the values that have been set during a previous application of the model or a similar model and to calibrate from there to achieve an acceptable model data match e g skerratt et al 2013 issues with these approaches are it is often difficult to find relevant parameter values in the literature while phytoplankton growth rates are commonly measured there are relatively few available measurements relevant to some other biogeochemical processes e g zooplankton mortality it can be difficult to ascertain how relevant a particular literature value might be for instance is a published maximum growth rate for a microcystis aeruginosa strain isolated from a temperate lake relevant to a simulation of microcystis aeruginosa in a subtropical estuary it can be difficult or time consuming to convert from experimental results published in the literature to the specific parameters required under the required conditions and in the required units the process is usually poorly documented so that the evidence from which a parameter value was derived cannot readily be traced and the strength of that evidence cannot be assessed parameter uncertainty is poorly understood while a sensitivity analysis might tell us the degree to which variation in the parameter value affects model results we retain little information about the degree of confidence we have in the correct value of that parameter if calibration of the model results in a shift away from an initially hypothesised parameter value it is difficult to assess whether this shift is reasonable or whether the calibration is compensating for an error in another parameter or a structural error in the model many of these issues arise from the fact that ecosystem models are usually representing aggregate processes but the ultimate determinants of these processes are the traits of individual organisms models may not represent how these traits change as the environment causes changes in community structure in practise there is considerable inconsistency in the parameter values used to represent process rates in similar models applied across systems or even within the same system a characteristic example can be found in the lake erie modelling work reviewed by kim et al 2014 and shimoda and arhonditsis 2016 where equivalent phytoplankton functional groups have been characterised as having low 2 day 1 or high 3 day 1 maximum growth rates depending on the model considered there are a range of reasons for this variability in parameter values in the case of phytoplankton growth rates for example we may consider the maximum growth rate of a phytoplankton group typically aims to characterise the average patterns of diverse assemblages of functionally similar phytoplankton species and strains the actual composition of these assemblages will vary in time and space different ways of clustering species into functional phylogenetic or morphological groups might result in different model parameters even with the same initial information the maximum growth rate of a particular phytoplankton strain depends on its history and the conditions to which it is adapted the concept of a phytoplankton growth rate is a simplification of several physiological processes each of which may vary separately these include carbon fixation photosynthesis chlorophyll production and cell division some of the variability in the maximum growth rate parameter will be due to actual variability in carbon to chlorophyll ratios different approaches to calibration of models and different goal metrics will yield different optimal parameter values some models are over parameterised relative to the available data so that no single optimal calibrated parameter set exists different models embody different conceptualisations of the system and every model has some degree of structural error variations in assigned values may compensate for structural errors for example a low maximum phytoplankton growth rate may compensate for a missing loss term such as viral lysis while a high maximum growth rate may compensate for a missing source term such as germination of akinetes hence some of the variability in parameter values is appropriate in terms of our physical and physiological understanding of the system while some of the variability may be due to errors in the structure or calibration of models insofar as we implicitly acknowledge this when we develop a model we treat parameters not strictly as measurable physiological properties but rather as mathematical properties of convenience while treating parameters in this way may produce good model predictions in some circumstances it reduces the connection between models and the processes they claim to represent the predictive capability of the model and our ability to anticipate the circumstances in which our models may fail kruk et al 2011 ramin et al 2011 3 collation of parameter values there have been several efforts to collate and tabulate parameter values from across the literature some of these have been broadly focused on the most commonly used parameters in aquatic ecosystem models e g bowie et al 1985 or eco chemistry jorgensen et al 2000 while others have comprehensively reviewed the literature for a narrower range of parameters relating to a particular component or process of an aquatic ecosystem e g phytoplankton edwards et al 2015 eppley 1985 zooplankton hébert et al 2016 phytoplankton hoogenhout and amesz 1965 seagrass lee et al 2007 coral madin et al 2016 in some cases synthesising this information to derive insight into allometric and ecological functional relationships droop et al 1982 edwards et al 2012 2015 enriquez et al 1993 hansen et al 1997 hebert et al 2016 kruk et al 2010 litchman et al 2007 these sources have been and remain a very valuable resource for modellers a natural progression from the disparate reporting of parameter values is to collate them into a probabilistic form collated process rates or parameter values from diverse studies may be summarised in the form of a probability distribution whereby some parametric distribution is fitted to the reported values if a bayesian nomenclature is adopted these distributions are typically referred to as prior probability distributions e g arhonditsis et al 2008 jones et al 2010 zhang and arhonditsis 2009 bruggeman 2011 considers phytoplankton traits in a probabilistic form predicting unknown values from known values for related species and phylogenetic relationships between species in this paper we build on this legacy by bringing together parameter values from a wide range of sources including past review papers as well as individual peer reviewed experimental and observational research papers introducing an online tool http shiny csiro au cdm parameterlibrary latest to facilitate exploration and visualisation of these data allowing the data to be downloaded filtered combined and manipulated in various common ways presenting parameter data in a form that facilitates its use within a bayesian framework for instance allowing the user to easily fit probability distributions and calculate key statistics such as the 5th 50th and 95th percentiles introducing a community initiative to maintain and expand this database so that it will continue to grow and become more useful in a wider range of modelling contexts over time presenting some guidelines for a more evidence based approach to parameter specification in aquatic ecosystem models while the database is not yet comprehensive the current version comprises 4834 observations relating to traits and responses of marine and freshwater phytoplankton zooplankton detritus seagrasses coral and microphytobenthos such a data synthesis effort not only identifies some of the recurrent knowledge gaps in the literature but it is our hope that it will also encourage aquatic researchers to contribute to this collaborative project by disclosing and sharing measurements as such we have focused on several key parameters that are relevant to both marine and freshwater modelling selecting parameters for inclusion on the basis that they are directly relevant to current modelling projects for each parameter we have found measured values in the literature drawing on previous reviews where appropriate but also checking original sources where possible to minimise propagation of errors from reviews and bringing in new data from more recent studies we have included only parameter estimates derived from peer reviewed experimental or observational studies not parameter values derived from calibration of models although a review of posterior parameter values actually used in modelling would be an interesting topic for another study introducing calibrated model values to this database might bias the distributions away from the empirical evidence base 4 online interface we have implemented a preliminary online user interface to allow further exploration of the parameter library which can be found at http shiny csiro au cdm parameterlibrary latest a screenshot is presented as fig 1 in addition to calculating and visualising probability distributions gamma normal log normal or uniform fitted to each parameter the interface allows users to filter datasets where relevant information exists in the database in most cases there is an option to restrict the dataset to include only data points for which we have independently checked that the value given can be found in the original data source in other cases we have relied on a third party such as a review article the user can view or download a plot and statistical description of the parameter distribution as well as the relevant raw data including the full reference and doi of the source publication and the review article if any in which it was found and the name and contact details of the person who contributed each data point to the database histograms show the actual distribution of observations the number of bins used in each histogram depends on the total number of observational values included wider bins are used when fewer measurements are available to provide a visual indication of the strength of evidence the online tool also provides an option labelled reduce to median of each that allows the user to consolidate replicated measurements to avoid over weighting results for species or other filter options which are over represented in the data set for instance a user may choose to consolidate by species in which case a single median value is used for each species for which there is more than one data point the user can choose the assumptions that are made about the temperature response of biogeochemical rates choosing between a no temperature correction b a q10 or arrhenius response curve or c the response curve implemented in caedym robson and hamilton 2004 after griffin et al 2001 which allows inhibition at higher temperatures the arrhenius and q10 response curves are equivalent in the arrhenius option k t k 20 v t t 20 where k t is the value of a parameter at a specified temperature t c k 20 is the value of that parameter at 20 c and v is a specified constant goldman and carpenter 1974 in the q10 option e g volta et al 2014 q is specified as a multiplier applied to a parameter value for each 10 c increase in temperature in its current beta form the online database contains a relatively small selection of the data available in the literature and in some cases the initial sources are skewed towards particular contexts relevant to the current projects of the authors with community contributions the depth and breadth of parameter coverage will be improved over time this may potentially include physical parameters such as particle sizes and roughness lengths and aggregate biogeochemical process rates such as sediment oxygen demand and oxygen transfer rates as well as the type of biological rates and traits considered so far in addition to making use of the available filter options we encourage users of the online tool to explore the original data sources to get a feel for the conditions under which measurements were made and form their own judgement about the relevance of these conditions to their own studies in some circumstances it might be appropriate to recalculate prior distributions giving more weight to measurements made using more reliable techniques or in field versus laboratory conditions or on the basis of other considerations 5 summary of parameter data table 1 summarises the data included in the parameter library as at may 2017 we anticipate that we will add to this resource over time drawing on community contributions 5 1 detritus our initial detritus dataset currently includes measured values for detrital composition c n and p c n and c p remineralisation decay rates and decay rates of labile and refractory fractions where rates presented in observational papers were calculated for two fractions a key resource was the work of enriquez et al 1993 who collated many of the pre 1993 data several filters are offered including detritus source aquatic or terrestrial material plant type of source material amphibious plant freshwater angiosperm grass macroalgae mangrove phytoplankton seagrass sedge or shrub genus species plant fraction many categories including leaf stem or roots conditions e g buried high intertidal low intertidal subtidal water climate e g coastal mediterranean temperate tropical wet and dry geographic location and type of water body e g lake river wetland bay estuary sea 5 2 zooplankton and other grazers the database currently includes measured zooplankton growth rates and individual animal volumes and lengths growth efficiency ingestion clearance and respiration rates and nutrient specific excretion artes from a range of freshwater marine and estuarine studies key resources included hébert et al 2016 for crustacean sizes and gsell et al 2016 for rotatorians for zooplankton respiration rates 78 measured values were extracted from ikeda 1974 and the remainder of the respiration data i e 714 observations comes from the data compilation of hébert et al 2016 other key review articles and data collations for zooplankton included hansen et al 1991 1997 hebert et al 2016 in addition to zooplankton the dataset currently includes parameter values to define coral uptake rates of 6 distinct particle types including heterotrophic bacteria picophytoplankton synechococcus picoeukaryotes nanoeukaryotes microphytoplankton and particulate organic nitrogen these data were taken from values measured by ribes et al 2003 the database does not yet include other animals such as benthic macroinvertebrates this is an area for future expansion and we would welcome assistance 5 3 phytoplankton and benthic microalgae for phytoplankton maximum growth rates we draw upon several important synthesis papers and data papers on this topic including edwards et al 2015 hoogenhout and amesz 1965 kruk et al 2010 these can be filtered by morphologically based functional group as defined by kruk et al 2010 by type of water fresh euryhaline estuarine marine hypersaline source of strain e g lake river estuary ocean sewage treatment lagoon laboratory culture phylogenetic domain class genus or species we also have data for a range of parameters relating to phytoplankton nutrient uptake nutrient use and carbon content per cell derived largely from edwards et al 2015 for phytoplankton respiration we have 30 data points from 4 literature sources phytoplankton photosynthesis irradiance p vs i curve parameters have been measured across diverse oceanographic conditions freshwater photosynthesis irradiance data have not yet been collated data are taken so far from 22 distinct oceanographic voyages covering regions of the southern ocean the tasman sea great barrier reef equatorial waters and australian coastal waters the relatively high number of data points for each parameter is due to the multiple depths sampled for each studied location microphytobenthos benthic microalgae parameter data are currently limited to a few sources including growth rates for 31 species of microphytobenthos in 5 taxonomic groups 5 4 seagrasses for seagrasses the database currently contains parameters to describe photosynthesis irradiance curves and parameters to describe optimal temperatures for seagrass growth the majority of these are sourced from the review presented by lee et al 2007 6 making effective use of parameter prior distributions the simplest approach to using the information presented via this online resource may be to select the subsample of data most relevant to the application at hand plot a distribution and calibrate within the range suggested by the 5th and 95th percentiles after calibration the final parameter set can be compared with the expected prior distributions if more than a few calibrated parameter values fall close to the outer limits of the expected distribution this may warrant further investigation as it may be suggestive of a structural error in the model or a systematic error in the observational data fig 2 if multiple models applied to different systems consistently produce calibrated values outside or near the outer limits of the expected distributions this may indicate either a common problem in model conceptualisation or a problem in the way that the relevant biophysical metric or rate has been measured or reported in the literature if the problem is a common structural error in models then a focus on improving this component of our models may yield substantial improvements in predictive performance conversely if the problem lies in the biophysical rate data it may indicate either that more measurements are required to properly characterise this parameter perhaps using a wider variety of species from a wider variety of aquatic systems or systems more relevant to the contexts of model application or that better or more consistent measurement protocols are needed to ensure that measurements are relevant to real world conditions for example fig 3 compares the distribution of observed maximum growth rates of freshwater diatoms with the distribution of calibrated values found in 72 published models from the literature shimoda and arhonditsis 2016 although all 72 models use growth rates that are within the range of observed values values at either extreme are either too high or too low to be likely further the preponderance of values used in models are above the median of observed rates this suggest that either there is a systematic bias in measured rates or a systematic bias in the models for example it may be that most of these models over estimate a loss term such as settling to sediments or are missing an important source term for diatoms so an elevated growth rate is compensating for this error both possibilities present opportunities for further investigation which might lead to an improved next generation of models a more powerful approach is to use the parameter priors within a bayesian modelling framework the field of physical statistical modelling berliner 2003 parslow et al 2013 describes the combination of mechanistic models with statistical methods the bayesian hierarchical modelling bhm framework see cressie and wikle 2011 for a thorough introduction to the subject can be used to account for multiple sources uncertainty using conditional probability distributions to simultaneously provide estimates of parameter values and estimates of uncertainty associated with those values and with the resulting model predictions according to bayes law the posterior distribution for a bhm is given by 1 x θ y y x θ x θ θ where x are the state variables of the model θ are the parameters and y are the observations the square brackets θ denotes the probability distribution of θ and x θ is the conditional probability of x given θ the key elements of eqn 1 are θ the prior model x θ the process model and y x θ the data model in many cases there is no analytical solution to the posterior and therefore sampling based methods such as bayesian monte carlo markov chain mcmc analysis must be used andrieu et al 2010 jones et al 2010 murray et al 2013 saloranta et al 2008 the formal inclusion of prior knowledge of parameters within a bhm occurs via the prescription of θ in the absence of adequate information the prior over θ may take a very uninformative form in the most extreme case θ could be sampled from a uniform distribution spanning however even subjective information using expert opinion has been shown to reduce uncertainty by placing finite bounds from the results presented in this paper there is substantial information available to place parametric distributions on some parameters rendering them in many cases to be quite informative to date there have been a handful of studies that have exploited the various components of the bhm framework within aquatic and marine biogeochemical modelling dowd et al 2014 reviewed recent developments applications with the exception of parslow et al 2013 very few studies have included a rigorous derivation of prior distributions parslow et al 2013 derive a number of model dependent prior distributions however there is a need to extend this to model independent parameterisations that cover a broad range of parameterisation and environments 7 future development the beta version of the parameter database presented here and currently available online is a static resource manually updated it is our intention subject to funding to maintain and update this to provide a modern data service this will include not only expansion of the database itself and improvement of the online user interface but also improvement of the underlying data structure consideration of issues of semantics inter operability provenance and data delivery and provision of an api that will allow third parties to access the data in novel ways this will for instance allow modellers to embed calls to the database within automated calibration and data assimilation routines or to develop tools to extract tailored datasets apply specified transformations to the data and automatically produce parameter files in the format needed by a specific model it is anticipated that a future version of the online interface will allow registered users to create accounts save selected filters or transformations share generated prior distributions by generating unique addresses to the results and most importantly contribute by uploading their own new data points or datasets in the meantime we encourage anyone interested in contributing to contact the authors future work will also include an analysis of the distribution of parameter values actually used in models and posterior distributions obtained by applying bayesian hierarchical methods in combination with the prior distributions made available through the online tool this should provide insight into potential structural issues in the current generation of models as well as priorities for additional observational measurements the current version of the database considers point values from various sources but does not take into account uncertainty estimates given by some sources this is not straightforward due to the variability in the type of ranges given by primary sources confidence intervals standard errors standard deviations and absolute ranges but is something that should be addressed in future to minimise loss of information 8 conclusion by presenting physiological trait and biogeochemical process rate information through an online tool that not only synthesises observations from multiple sources but also supports consistent and reproducable processing of these data for instance by applying a consistent temperature correction function to observed metabolic rates we facilitate more evidence based bayesian parameterisation of aquatic systems models as well as improved model evaluation and development processes acknowledgements this work was funded in part through the ereefs project a public private collaboration between australia s leading operational and scientific research agencies government and corporate australia jb and lg contributed to this work during internships at csiro land and water completed as part of their studies thanks to philip balding for assisting with phytoplankton data entry author contributions bjr led the conceptualisation and development of the parameter library planning and writing of the paper and wrote the r code for the currently beta online resource meb kwa mm and as contributed to the planning and design of the parameter library bjr gba ej xsk mph and meb wrote the manuscript xsk drafted a section on use of the parameter library in a modelling workflow and prepared fig 2 gba provided the analysis behind fig 3 kfe collated data relating to phytoplankton nutrient utilisation ck jhs jb and bjr collated data relating to phytoplankton growth nutrient utilisation and cell sizes mph collated analysed and formatted data related to zooplankton size and metabolic rates jb and bjr collated other zooplankton data stt and lg collated parameters relating to seagrass and detritus including decay rates vdv collated data relating to phytoplankton photosynthesis irradiance curves performed a qa qc check of each variable related to these data and prepared table 1 ys collated and prepared metadata appendix a supplementary data the following are the supplementary data related to this article lol metadata lol metadata data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 018 
26450,mechanistic models rely on specification of parameters representing biophysical traits and process rates such as phytoplankton zooplankton and seagrass growth and respiration rates organism sizes stoichiometry light temperature and nutrient responses nutrient specific excretion rates and detrital stoichiometry and decay rates choosing suitable values for these parameters is difficult current practise is problematic this paper presents a resource designed to facilitate an evidence based approach to parameterisation of aquatic ecosystem models an online tool is provided which collates relevant published biological trait and biogeochemical rate observations from many sources and allows users to explore filter and convert these data in a consistent reproducible way to find parameter values and calculate probability distributions using this information within a traditional or bayesian paradigm should provide improved understanding of the uncertainty and predictive capacity of aquatic ecosystem models and provide insight into current sources of structural error in models keywords parameterisation biogeochemical rates biological traits aquatic ecosystems parameter priors seagrass phytoplankton zooplankton remineralisation software and data availability software and data described in this paper are available from the parameter library exploration interface at http shiny csiro au cdm parameterlibrary latest 1 introduction aquatic ecosystem modelling has a long history of use to shed light on ecosystem function e g scheffer et al 2001 and provide input to policy and management decisions to improve water quality limit the impact of pollutants and toxicants specify fish catches inform management of catchments and riparian vegetation and control eutrophication e g wang et al 2012 although there are several approaches to simulation of environmental systems robson 2014 process based models including complex mechanistic models have been enduringly popular these models aim to mimic the roles of individual processes through mathematical descriptions with realistic parameterisations the premise is that mathematical representations of individual processes combine to collectively offer a faithful depiction of ecosystem dynamics the appeal of this approach is that by simulating systems in terms of real ideally measurable physical geochemical and biological processes mechanistic models offer the prospect of greater insight into system function than other types of models as well as the promise of defensible extrapolation beyond historical conditions to predict future changes the degree to which a mechanistic model can deliver on these promises depends not only on the accuracy of the conceptual understanding of the system represented by the equations used to build the model but also on the accuracy of its parameterisation 2 current state of the art parameterisation of aquatic ecosystem models has historically been a difficult and labour intensive task parameter values are typically required for a range of model components defining traits and responses of phytoplankton zooplankton and other aquatic biota characteristics of sediments and detrital material physical and chemical process rates and more recently bacterial traits and processes although mathematical methods have long been available to automate parameter estimation and calibration these have not been rapidly or universally adopted in this field due to computational constraints or data limitations shimoda and arhonditsis 2016 traditional approaches to setting parameter values have been either to combine where available local measurements of flux rates and component responses for the study site with literature values for parameters that have not been measured locally e g mao et al 2015 murray and parslow 1999 robson and hamilton 2004 or to begin with the values that have been set during a previous application of the model or a similar model and to calibrate from there to achieve an acceptable model data match e g skerratt et al 2013 issues with these approaches are it is often difficult to find relevant parameter values in the literature while phytoplankton growth rates are commonly measured there are relatively few available measurements relevant to some other biogeochemical processes e g zooplankton mortality it can be difficult to ascertain how relevant a particular literature value might be for instance is a published maximum growth rate for a microcystis aeruginosa strain isolated from a temperate lake relevant to a simulation of microcystis aeruginosa in a subtropical estuary it can be difficult or time consuming to convert from experimental results published in the literature to the specific parameters required under the required conditions and in the required units the process is usually poorly documented so that the evidence from which a parameter value was derived cannot readily be traced and the strength of that evidence cannot be assessed parameter uncertainty is poorly understood while a sensitivity analysis might tell us the degree to which variation in the parameter value affects model results we retain little information about the degree of confidence we have in the correct value of that parameter if calibration of the model results in a shift away from an initially hypothesised parameter value it is difficult to assess whether this shift is reasonable or whether the calibration is compensating for an error in another parameter or a structural error in the model many of these issues arise from the fact that ecosystem models are usually representing aggregate processes but the ultimate determinants of these processes are the traits of individual organisms models may not represent how these traits change as the environment causes changes in community structure in practise there is considerable inconsistency in the parameter values used to represent process rates in similar models applied across systems or even within the same system a characteristic example can be found in the lake erie modelling work reviewed by kim et al 2014 and shimoda and arhonditsis 2016 where equivalent phytoplankton functional groups have been characterised as having low 2 day 1 or high 3 day 1 maximum growth rates depending on the model considered there are a range of reasons for this variability in parameter values in the case of phytoplankton growth rates for example we may consider the maximum growth rate of a phytoplankton group typically aims to characterise the average patterns of diverse assemblages of functionally similar phytoplankton species and strains the actual composition of these assemblages will vary in time and space different ways of clustering species into functional phylogenetic or morphological groups might result in different model parameters even with the same initial information the maximum growth rate of a particular phytoplankton strain depends on its history and the conditions to which it is adapted the concept of a phytoplankton growth rate is a simplification of several physiological processes each of which may vary separately these include carbon fixation photosynthesis chlorophyll production and cell division some of the variability in the maximum growth rate parameter will be due to actual variability in carbon to chlorophyll ratios different approaches to calibration of models and different goal metrics will yield different optimal parameter values some models are over parameterised relative to the available data so that no single optimal calibrated parameter set exists different models embody different conceptualisations of the system and every model has some degree of structural error variations in assigned values may compensate for structural errors for example a low maximum phytoplankton growth rate may compensate for a missing loss term such as viral lysis while a high maximum growth rate may compensate for a missing source term such as germination of akinetes hence some of the variability in parameter values is appropriate in terms of our physical and physiological understanding of the system while some of the variability may be due to errors in the structure or calibration of models insofar as we implicitly acknowledge this when we develop a model we treat parameters not strictly as measurable physiological properties but rather as mathematical properties of convenience while treating parameters in this way may produce good model predictions in some circumstances it reduces the connection between models and the processes they claim to represent the predictive capability of the model and our ability to anticipate the circumstances in which our models may fail kruk et al 2011 ramin et al 2011 3 collation of parameter values there have been several efforts to collate and tabulate parameter values from across the literature some of these have been broadly focused on the most commonly used parameters in aquatic ecosystem models e g bowie et al 1985 or eco chemistry jorgensen et al 2000 while others have comprehensively reviewed the literature for a narrower range of parameters relating to a particular component or process of an aquatic ecosystem e g phytoplankton edwards et al 2015 eppley 1985 zooplankton hébert et al 2016 phytoplankton hoogenhout and amesz 1965 seagrass lee et al 2007 coral madin et al 2016 in some cases synthesising this information to derive insight into allometric and ecological functional relationships droop et al 1982 edwards et al 2012 2015 enriquez et al 1993 hansen et al 1997 hebert et al 2016 kruk et al 2010 litchman et al 2007 these sources have been and remain a very valuable resource for modellers a natural progression from the disparate reporting of parameter values is to collate them into a probabilistic form collated process rates or parameter values from diverse studies may be summarised in the form of a probability distribution whereby some parametric distribution is fitted to the reported values if a bayesian nomenclature is adopted these distributions are typically referred to as prior probability distributions e g arhonditsis et al 2008 jones et al 2010 zhang and arhonditsis 2009 bruggeman 2011 considers phytoplankton traits in a probabilistic form predicting unknown values from known values for related species and phylogenetic relationships between species in this paper we build on this legacy by bringing together parameter values from a wide range of sources including past review papers as well as individual peer reviewed experimental and observational research papers introducing an online tool http shiny csiro au cdm parameterlibrary latest to facilitate exploration and visualisation of these data allowing the data to be downloaded filtered combined and manipulated in various common ways presenting parameter data in a form that facilitates its use within a bayesian framework for instance allowing the user to easily fit probability distributions and calculate key statistics such as the 5th 50th and 95th percentiles introducing a community initiative to maintain and expand this database so that it will continue to grow and become more useful in a wider range of modelling contexts over time presenting some guidelines for a more evidence based approach to parameter specification in aquatic ecosystem models while the database is not yet comprehensive the current version comprises 4834 observations relating to traits and responses of marine and freshwater phytoplankton zooplankton detritus seagrasses coral and microphytobenthos such a data synthesis effort not only identifies some of the recurrent knowledge gaps in the literature but it is our hope that it will also encourage aquatic researchers to contribute to this collaborative project by disclosing and sharing measurements as such we have focused on several key parameters that are relevant to both marine and freshwater modelling selecting parameters for inclusion on the basis that they are directly relevant to current modelling projects for each parameter we have found measured values in the literature drawing on previous reviews where appropriate but also checking original sources where possible to minimise propagation of errors from reviews and bringing in new data from more recent studies we have included only parameter estimates derived from peer reviewed experimental or observational studies not parameter values derived from calibration of models although a review of posterior parameter values actually used in modelling would be an interesting topic for another study introducing calibrated model values to this database might bias the distributions away from the empirical evidence base 4 online interface we have implemented a preliminary online user interface to allow further exploration of the parameter library which can be found at http shiny csiro au cdm parameterlibrary latest a screenshot is presented as fig 1 in addition to calculating and visualising probability distributions gamma normal log normal or uniform fitted to each parameter the interface allows users to filter datasets where relevant information exists in the database in most cases there is an option to restrict the dataset to include only data points for which we have independently checked that the value given can be found in the original data source in other cases we have relied on a third party such as a review article the user can view or download a plot and statistical description of the parameter distribution as well as the relevant raw data including the full reference and doi of the source publication and the review article if any in which it was found and the name and contact details of the person who contributed each data point to the database histograms show the actual distribution of observations the number of bins used in each histogram depends on the total number of observational values included wider bins are used when fewer measurements are available to provide a visual indication of the strength of evidence the online tool also provides an option labelled reduce to median of each that allows the user to consolidate replicated measurements to avoid over weighting results for species or other filter options which are over represented in the data set for instance a user may choose to consolidate by species in which case a single median value is used for each species for which there is more than one data point the user can choose the assumptions that are made about the temperature response of biogeochemical rates choosing between a no temperature correction b a q10 or arrhenius response curve or c the response curve implemented in caedym robson and hamilton 2004 after griffin et al 2001 which allows inhibition at higher temperatures the arrhenius and q10 response curves are equivalent in the arrhenius option k t k 20 v t t 20 where k t is the value of a parameter at a specified temperature t c k 20 is the value of that parameter at 20 c and v is a specified constant goldman and carpenter 1974 in the q10 option e g volta et al 2014 q is specified as a multiplier applied to a parameter value for each 10 c increase in temperature in its current beta form the online database contains a relatively small selection of the data available in the literature and in some cases the initial sources are skewed towards particular contexts relevant to the current projects of the authors with community contributions the depth and breadth of parameter coverage will be improved over time this may potentially include physical parameters such as particle sizes and roughness lengths and aggregate biogeochemical process rates such as sediment oxygen demand and oxygen transfer rates as well as the type of biological rates and traits considered so far in addition to making use of the available filter options we encourage users of the online tool to explore the original data sources to get a feel for the conditions under which measurements were made and form their own judgement about the relevance of these conditions to their own studies in some circumstances it might be appropriate to recalculate prior distributions giving more weight to measurements made using more reliable techniques or in field versus laboratory conditions or on the basis of other considerations 5 summary of parameter data table 1 summarises the data included in the parameter library as at may 2017 we anticipate that we will add to this resource over time drawing on community contributions 5 1 detritus our initial detritus dataset currently includes measured values for detrital composition c n and p c n and c p remineralisation decay rates and decay rates of labile and refractory fractions where rates presented in observational papers were calculated for two fractions a key resource was the work of enriquez et al 1993 who collated many of the pre 1993 data several filters are offered including detritus source aquatic or terrestrial material plant type of source material amphibious plant freshwater angiosperm grass macroalgae mangrove phytoplankton seagrass sedge or shrub genus species plant fraction many categories including leaf stem or roots conditions e g buried high intertidal low intertidal subtidal water climate e g coastal mediterranean temperate tropical wet and dry geographic location and type of water body e g lake river wetland bay estuary sea 5 2 zooplankton and other grazers the database currently includes measured zooplankton growth rates and individual animal volumes and lengths growth efficiency ingestion clearance and respiration rates and nutrient specific excretion artes from a range of freshwater marine and estuarine studies key resources included hébert et al 2016 for crustacean sizes and gsell et al 2016 for rotatorians for zooplankton respiration rates 78 measured values were extracted from ikeda 1974 and the remainder of the respiration data i e 714 observations comes from the data compilation of hébert et al 2016 other key review articles and data collations for zooplankton included hansen et al 1991 1997 hebert et al 2016 in addition to zooplankton the dataset currently includes parameter values to define coral uptake rates of 6 distinct particle types including heterotrophic bacteria picophytoplankton synechococcus picoeukaryotes nanoeukaryotes microphytoplankton and particulate organic nitrogen these data were taken from values measured by ribes et al 2003 the database does not yet include other animals such as benthic macroinvertebrates this is an area for future expansion and we would welcome assistance 5 3 phytoplankton and benthic microalgae for phytoplankton maximum growth rates we draw upon several important synthesis papers and data papers on this topic including edwards et al 2015 hoogenhout and amesz 1965 kruk et al 2010 these can be filtered by morphologically based functional group as defined by kruk et al 2010 by type of water fresh euryhaline estuarine marine hypersaline source of strain e g lake river estuary ocean sewage treatment lagoon laboratory culture phylogenetic domain class genus or species we also have data for a range of parameters relating to phytoplankton nutrient uptake nutrient use and carbon content per cell derived largely from edwards et al 2015 for phytoplankton respiration we have 30 data points from 4 literature sources phytoplankton photosynthesis irradiance p vs i curve parameters have been measured across diverse oceanographic conditions freshwater photosynthesis irradiance data have not yet been collated data are taken so far from 22 distinct oceanographic voyages covering regions of the southern ocean the tasman sea great barrier reef equatorial waters and australian coastal waters the relatively high number of data points for each parameter is due to the multiple depths sampled for each studied location microphytobenthos benthic microalgae parameter data are currently limited to a few sources including growth rates for 31 species of microphytobenthos in 5 taxonomic groups 5 4 seagrasses for seagrasses the database currently contains parameters to describe photosynthesis irradiance curves and parameters to describe optimal temperatures for seagrass growth the majority of these are sourced from the review presented by lee et al 2007 6 making effective use of parameter prior distributions the simplest approach to using the information presented via this online resource may be to select the subsample of data most relevant to the application at hand plot a distribution and calibrate within the range suggested by the 5th and 95th percentiles after calibration the final parameter set can be compared with the expected prior distributions if more than a few calibrated parameter values fall close to the outer limits of the expected distribution this may warrant further investigation as it may be suggestive of a structural error in the model or a systematic error in the observational data fig 2 if multiple models applied to different systems consistently produce calibrated values outside or near the outer limits of the expected distributions this may indicate either a common problem in model conceptualisation or a problem in the way that the relevant biophysical metric or rate has been measured or reported in the literature if the problem is a common structural error in models then a focus on improving this component of our models may yield substantial improvements in predictive performance conversely if the problem lies in the biophysical rate data it may indicate either that more measurements are required to properly characterise this parameter perhaps using a wider variety of species from a wider variety of aquatic systems or systems more relevant to the contexts of model application or that better or more consistent measurement protocols are needed to ensure that measurements are relevant to real world conditions for example fig 3 compares the distribution of observed maximum growth rates of freshwater diatoms with the distribution of calibrated values found in 72 published models from the literature shimoda and arhonditsis 2016 although all 72 models use growth rates that are within the range of observed values values at either extreme are either too high or too low to be likely further the preponderance of values used in models are above the median of observed rates this suggest that either there is a systematic bias in measured rates or a systematic bias in the models for example it may be that most of these models over estimate a loss term such as settling to sediments or are missing an important source term for diatoms so an elevated growth rate is compensating for this error both possibilities present opportunities for further investigation which might lead to an improved next generation of models a more powerful approach is to use the parameter priors within a bayesian modelling framework the field of physical statistical modelling berliner 2003 parslow et al 2013 describes the combination of mechanistic models with statistical methods the bayesian hierarchical modelling bhm framework see cressie and wikle 2011 for a thorough introduction to the subject can be used to account for multiple sources uncertainty using conditional probability distributions to simultaneously provide estimates of parameter values and estimates of uncertainty associated with those values and with the resulting model predictions according to bayes law the posterior distribution for a bhm is given by 1 x θ y y x θ x θ θ where x are the state variables of the model θ are the parameters and y are the observations the square brackets θ denotes the probability distribution of θ and x θ is the conditional probability of x given θ the key elements of eqn 1 are θ the prior model x θ the process model and y x θ the data model in many cases there is no analytical solution to the posterior and therefore sampling based methods such as bayesian monte carlo markov chain mcmc analysis must be used andrieu et al 2010 jones et al 2010 murray et al 2013 saloranta et al 2008 the formal inclusion of prior knowledge of parameters within a bhm occurs via the prescription of θ in the absence of adequate information the prior over θ may take a very uninformative form in the most extreme case θ could be sampled from a uniform distribution spanning however even subjective information using expert opinion has been shown to reduce uncertainty by placing finite bounds from the results presented in this paper there is substantial information available to place parametric distributions on some parameters rendering them in many cases to be quite informative to date there have been a handful of studies that have exploited the various components of the bhm framework within aquatic and marine biogeochemical modelling dowd et al 2014 reviewed recent developments applications with the exception of parslow et al 2013 very few studies have included a rigorous derivation of prior distributions parslow et al 2013 derive a number of model dependent prior distributions however there is a need to extend this to model independent parameterisations that cover a broad range of parameterisation and environments 7 future development the beta version of the parameter database presented here and currently available online is a static resource manually updated it is our intention subject to funding to maintain and update this to provide a modern data service this will include not only expansion of the database itself and improvement of the online user interface but also improvement of the underlying data structure consideration of issues of semantics inter operability provenance and data delivery and provision of an api that will allow third parties to access the data in novel ways this will for instance allow modellers to embed calls to the database within automated calibration and data assimilation routines or to develop tools to extract tailored datasets apply specified transformations to the data and automatically produce parameter files in the format needed by a specific model it is anticipated that a future version of the online interface will allow registered users to create accounts save selected filters or transformations share generated prior distributions by generating unique addresses to the results and most importantly contribute by uploading their own new data points or datasets in the meantime we encourage anyone interested in contributing to contact the authors future work will also include an analysis of the distribution of parameter values actually used in models and posterior distributions obtained by applying bayesian hierarchical methods in combination with the prior distributions made available through the online tool this should provide insight into potential structural issues in the current generation of models as well as priorities for additional observational measurements the current version of the database considers point values from various sources but does not take into account uncertainty estimates given by some sources this is not straightforward due to the variability in the type of ranges given by primary sources confidence intervals standard errors standard deviations and absolute ranges but is something that should be addressed in future to minimise loss of information 8 conclusion by presenting physiological trait and biogeochemical process rate information through an online tool that not only synthesises observations from multiple sources but also supports consistent and reproducable processing of these data for instance by applying a consistent temperature correction function to observed metabolic rates we facilitate more evidence based bayesian parameterisation of aquatic systems models as well as improved model evaluation and development processes acknowledgements this work was funded in part through the ereefs project a public private collaboration between australia s leading operational and scientific research agencies government and corporate australia jb and lg contributed to this work during internships at csiro land and water completed as part of their studies thanks to philip balding for assisting with phytoplankton data entry author contributions bjr led the conceptualisation and development of the parameter library planning and writing of the paper and wrote the r code for the currently beta online resource meb kwa mm and as contributed to the planning and design of the parameter library bjr gba ej xsk mph and meb wrote the manuscript xsk drafted a section on use of the parameter library in a modelling workflow and prepared fig 2 gba provided the analysis behind fig 3 kfe collated data relating to phytoplankton nutrient utilisation ck jhs jb and bjr collated data relating to phytoplankton growth nutrient utilisation and cell sizes mph collated analysed and formatted data related to zooplankton size and metabolic rates jb and bjr collated other zooplankton data stt and lg collated parameters relating to seagrass and detritus including decay rates vdv collated data relating to phytoplankton photosynthesis irradiance curves performed a qa qc check of each variable related to these data and prepared table 1 ys collated and prepared metadata appendix a supplementary data the following are the supplementary data related to this article lol metadata lol metadata data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 018 
26451,this study sheds light on the coupling of potential flood risk and drainage infrastructure resilience of low lying areas of a coastal urban watershed to evaluate flood hazards and their possible driving forces copulas analyses with the aid of joint probability of simultaneous occurrence help characterize the complexity for hazard classification based on subsequent exposure to inundation under varying levels of adaptive capacity adaptive measures of consideration include traditional flood proofing structures and low impact development facilities for a coastal urban watershed the cross bayou watershed near tampa bay florida findings indicate that coupling flood risk and infrastructure resilience is achievable through the careful formulation of flood risk associated with a resilience metric which is a function of the predicted hazards vulnerability and adaptive capacity the results also give insights into improving existing methodologies for municipalities in flood management practices such as incorporating a multi criteria flood impact assessment that couples risk and resilience in a common evaluation framework keywords flood impact risk analysis resilience assessment coastal sustainability 1 introduction 1 1 background in may 2015 the florida legislature passed and the governor signed into law sb 1094 https www flsenate gov session bill 2015 1094 which regards the consideration of future flood impacts in florida comprehensive plans particularly from a coastal management perspective these new requirements which concern development and redevelopment efforts to reduce flood risk include natural hazards such as high tide events and sea level rise risk in this context can be described as the likelihood of a flood hazard occurring with an associated loss or negative impact the likelihood of associated loss or negative impact is dependent on several factors such as the flood hazard considered and the level of vulnerability to flooding the concepts of hazard and vulnerability can be thought of as the physical manifestations or occurrences of adverse events and the propensity or predisposition to be adversely affected or susceptible to harm ipcc 2014 respectively both of which influence flood exposure simultaneously flood exposure is dependent upon the spread of hazardous effects given vulnerability such as proximity to waterbodies and or condition of drainage outfalls the level of risk however can be influenced by the level of resilience through the connection to the adaptive capacity in a region such as a low lying coastal area the concept of resilience has expanded from its origins in material science and engineering to ecological resilience holling 1973 and eventually to other disciplines such as the social sciences social resilience and psychology psychological resilience when considering infrastructure systems such as drainage under flooding engineering resilience which is highlighted in this study is the ability of such systems to absorb disturbance i e flooding and recover after a disturbance has occurred or the ability to continue functionality under adverse conditions omer 2013 while resilience is typically seen as an outcome it should be viewed as a process which involves adaptation anticipation and improvement in basic functions of a considered system bahadur et al 2010 coupling flood risk and engineering resilience is by no means an easy task de bruijn 2005 defined resilience in terms of flood risk management as the ability of a system to recover from floods quantitatively this can be represented via several indicators such as the amplitude or magnitude of the reaction to disturbances the graduality of reaction s under increasing disturbances and recovery rate de bruijn 2005 a resilient system results in a lower amplitude of reaction to disturbances low graduality of reaction to increasing disturbances and a higher recovery rate analogously this can be tied to three types of capacity of resilience proposed by francis and bekera 2014 which include absorptive capacity adaptive capacity and restorative capacity the absorptive capacity allows for adequate buffering to absorb or contain hazard effects while adaptive capacity is the ability to adjust or provide the necessary changes in response to adverse impacts such as when absorptive capacity has been exceeded restorative capacity is the ability to return to normal function or improved level of performance after a disturbance as with many systems however the absorptive capacity can fluctuate with changes in hazards as is the case when considering future flood risk thus adaptive capacity can be seen as a bridge to restorative capacity and eventually resilience when absorptive capacity has been exceeded adaptive capacity can be understood as the capacity to cope and adapt to adverse effects or from a systems approach the extent to which a system can modify its circumstances to move to a less vulnerable condition luers et al 2003 adaptive capacity also encompasses the ability to plan prepare for facilitate and implement adaptation options klein et al 2003 which first depend upon the nature of the disturbances or potential disturbances subsequently additional factors such as scale of adaptation individual to systemic policy and constraints must also be considered klein et al 2003 has argued for the use of adaptive capacity as an umbrella concept that includes the ability to prepare and plan for hazards as well as to implement technical measures before during and after a hazard event all the while the strategy for adaptive capacity must be flexible with respect to both risk and resilience de bruijn 2005 in order to reduce rigidity in case of disruptive events park et al 2013 while adsorptive capacity can provide an initial gauge of resilience failure is imminent when the adsorptive capacity is exceeded unless adaptive measures are taken this is particularly concerning for system design based upon a particular risk event as opposed to system design adaptive to various levels of risk essentially as park et al 2013 argued the risk based approach considers developing resistance to identified threats as opposed to resilience based approaches which embrace uncertainty and failure due to possible threats via anticipation and adaptation however in this regard risk and resilience cannot be applied individually but must work together risk provides a starting point for identifying potential problems or threats at hand however resilience considers how the progression can be maintained in the face of potential problems or threats 1 2 review of methods when considering flooding in risk analysis and resilience assessment in particular flooding can be caused by any combination of hazards which would impact both risk and resilience this is particularly important for coastal communities which are typically low lying and can face heavy rainfall high tide events and sea level rise within the same time period subsequently there exists a level of uncertainty of any combination of hazards occurring with corresponding consequence s joint probability analysis is useful in this regard for determining the probability of potential flooding hazards occurring simultaneously rather than in isolation a univariate analysis alone cannot provide a complete assessment of the occurrence probability of potential flooding hazards or scenarios particularly if they are interdependent chebana and ouarda 2011 however with typical multivariate analyses one condition is for the variables in question to be independent from one another wahl et al 2012 a univariate analysis also lacks consideration of flooding under multivariate hazards particularly for coastal communities when worst case flooding can occur under combined heavy rainfall and high tide events xu et al 2014 the choice of multivariate analysis must take into consideration that the variables in question could be interdependent may not be under the same family of marginal distributions and are not normally distributed both bayesian networks and copulas have been utilized for analyzing multivariate problems cleophas and zwinderman 2013 nelsen 2006 however bayesian networks require the need for prior information or knowledge for defining conditional probability distributions and the structure of the network depending on the level of detail needed to build such networks the computational demand can be quite large uusitalo 2007 compared to copulas for this reason copulas can be particularly useful while copulas have wide applications across several disciplines such as finance and insurance the application of copulas within hydrology in particular is important since hydrological processes are typically multidimensional in nature and indicate certain levels of interdependence de michele et al 2007 several applications of copulas in hydrology table 1 consisted of analyzing the joint behavior of several hydrological variables during storm events while capturing important statistical dependences de michele and salvadori 2003 salvadori and de michele 2004 balistrocchi and bacchi 2011 modeling multivariate hydrological extremes favre et al 2004 zhang et al 2011 rainfall frequency analysis zhang and singh 2007 flood frequency analysis wang et al 2009 and hydraulic structural design for flooding de michele et al 2005 particularly for inland coastal areas copulas have been useful in analyzing coastal hazards table 2 with underlying hydrological and hydrodynamic processes de michele et al 2007 wahl et al 2012 corbella and stretch 2013 xu et al 2014 trepanier et al 2014 as shown in the previously listed studies copulas can be used to highlight interdependence and the multi dimensional nature of flooding and climate processes however they highlight only one aspect of overall flood risk without considering resilience to these interdependent and multi dimensional events overall flood risk cannot be assessed quantifying flood resilience depends on the interconnection of the urban space and the natural space this interconnection can be represented by the concept of networked systems or networked infrastructure systems when considering infrastructure omer 2013 with regard to flood risk and resilience natural and man made systems such as rivers canals stormwater drainage channels and pipes are seen as the first system s that natural flooding hazards interact with before effects are felt within surrounding systems such as residential communities given the level of resilience of such systems as a result the adaptive capacity of natural and man made systems becomes important to the overall flood risk and resilience due to the cascade effect of interconnected systems omer 2013 park et al 2013 a useful real world example for consideration of both flood risk and infrastructure resilience is the cross bayou watershed located within pinellas county near the tampa bay region of west central florida low lying areas within the cross bayou watershed have been historically prone to flooding driven by rainfall runoff and or high tide events over the years storm events and subsequent flooding have taken a toll on the drainage infrastructure particularly for the undersized conveyance systems found throughout the watershed which are not equipped to handle increased runoff from surrounding urbanization tidal flooding has also impacted low lying areas near a tidal canal which dissects the watershed connecting neighboring bays for which inadequate protection exists water within the canal can flow in either direction depending upon tidal conditions flooding occurs periodically in several low lying communities with strong interactions between the surface water and the groundwater systems in dealing with such a complex system the interconnected pond and channel routing icpr catchment model streamline technologies inc 2015 was applied to the study region for coupling risk and resilience in support of multi criteria flood impact assessment the objectives of this study are to 1 determine the dependence structure of potential flooding risk in a low lying area within the cross bayou watershed via a copula analysis 2 link flood risk and engineering resilience via implementing a risk formulation which includes a resilience metric that is dependent upon the hazard vulnerability and exposure of an area of concern and 3 conduct a multi criteria flood impact assessment for decision analysis such efforts may answer the following scientific questions 1 can the copula analysis fully support the risk analysis 2 how can potential flood risk be offset by modeling adaptive measures for increasing drainage infrastructure resilience with the aid of icpr and 3 can the well coupled flood risk and engineering resilience lead to better decision making via a multi criteria flood impact assessment results of this study will have implications for policy makers such as those in pinellas county who are seeking new ways of reducing the flood insurance rates of their constituents by considering new flood management strategies this paper serves as a companion study of joyce et al 2017 2 study area the cross bayou watershed of pinellas county fig 1 florida was selected as a case study because of its vulnerability to coastal flooding and pinellas county s efforts to implement improved stormwater management to increase the area s adaptive capacity to future hazards the cross bayou watershed encompasses approximately 31 km 2 7697 acres primarily comprised of high density residential industrial and commercial areas an important feature of the watershed is a 16 9 km 10 5 mile long constructed tidal canal the cross bayou canal fig 1 which dissects the watershed and connects tampa bay and boca ciega bay on its northeastern and southwestern ends respectively the cross bayou canal also intersects the pinebrook canal to the southwest fig 1 water within the canal can flow in either direction depending on tidal conditions this feature while useful for overall watershed drainage is potentially hazardous to low lying communities during high tide events particularly when considering the ongoing threat of sea level rise noaa 2016 near the tampa bay region some areas in the watershed are consistently more vulnerable and have a decreased adaptive capacity to flooding the mariners cove residential community fig 2 in particular is known for significant flooding from storm events flooding in the mariners cove community is primarily caused by heavy rains and high tide events of the adjacent cross bayou canal 3 methodology there are many relevant definitions of risk and resilience in the literature the methods outlined in this section focus on the essence of rational choice and the actualization for coupling of risk and resilience 3 1 risk formulation risk in a generalized formulation can be represented as follows 1 risk f likelihood or probability of consequences occurring and consequences risk as a function of likelihood of consequences is related to decision theory such that risk can be represented as an expected value as follows 2 risk expected value likelihood or probability of consequences occurring consequences 3 likelihood or probability of consequences occurring f hazard vulnerability resilience 4 consequences f exposure f hazard vulnerability the likelihood or probability of consequences occurring is a function of hazard vulnerability and resilience the consequences are a function of exposure which is also a function of hazard and vulnerability literature can provide some guidance with regard to how the elements of hazard vulnerability resilience and exposure are related mathematically table 3 details the essence of this issue in an attempt to provide a mathematical formulation of risk in the aforementioned risk formulations sensitivity is the degradation in performance during continuous effects from hazards from a physical system perspective johansen 2010 aside from the generalized formulations presented in table 3 mathematically the formulations have advantages and disadvantages and will be presented on a case by case basis below 5 case i risk hazard vulnerability for this case the risk formulation is general and not specific in scope such that the application of this risk formulation assumes that hazard and vulnerability are only considered without other elements such as exposure or resilience unless defined further by the user of such formulation 6 case ii risk hazard exposure sensitivity resilience for this case the risk formulation is expounded upon by breaking down the vulnerability term as a product of exposure sensitivity and resilience this formulation is less simplistic than in case i however this formulation can only be applied carefully depending on how the resilience term is defined 7 case iii risk hazard vulnerability adaptive capacity case iii applies a quotient adaptive capacity is also one aspect of resilience as defined in literature such as francis and bekera 2014 however the quotient term presents challenges given how adaptive capacity is defined or formulated such that adaptive capacity could be large or small in the case of very small numbers for adaptive capacity the risk can be considerably large conceptually this makes sense however quantitatively this presents challenges for interpretation the success of this formulation depends on how the quotient term adaptive capacity or resilience is defined 3 2 resilience formulation the resilience term throughout the literature does not have a consistent form and varies given the system and assumed response for infrastructure or engineering systems yodo and wang 2016 have outlined how resilience metrics are developed based on three categories or approaches as summarized in table 4 with respect to the first category approach from table 4 defining a quantitative resilience metric based on theoretical resilience curves may present problems since resilience curves could be non linear in form and may not follow a defined pattern given variation in hazard or disruption defining a quantitative resilience metric based on 1 pre and post disruptions performances and 2 reliability and restoration may be more useful for this study francis and bekera 2014 proposed a resilience metric that can account for both pre and post disruptions along with reliability and restoration in the following formulation 8 resilience s p f r f o f d f o where s p speed recovery factor t δ t r e a t r t r for t r t r t δ t r otherwise f r system recovery state f o original system state f d system state following disruption f r f o adaptive capacity f d f o absorptive capacity t δ slack time or the max time during post disruption that is accepted before recovery begins t r time to final recovery i e new equilibrium state t r time to complete initial recovery actions a decay in resilience parameter representing time to new equilibrium state from the aforementioned resilience metric the decay factor a is represented such that if the initial recovery takes longer than the slack time the resilience metric decreases however this metric as proposed by francis and bekera 2014 presented a challenge regarding what value to assign the decay parameter in addition the slack time variable is subjective depending on the system of concern and decision maker lastly when considering flooding the variable representing the original system state f o would be assumed zero since the system i e drainage is at a dormant or no activity state resulting in the ratio becoming undefined in this specific case a potentially useful metric should be modified by considering the difference between the initial recovery time i e initial reduction in inundation depth after maximum inundation area and the final recovery time i e no inundation or no exposure 9 relative change in time of exposure t f t i t i t i initial recovery time time in which inundation depths are initially reduced from maximum inundation depths i e max exposure t f final recovery time time in which inundation depths are non existent following maximum inundation depths i e max exposure a resilience metric can be created that is the reciprocal of the relative change in time of exposure and is represented as follows 10 resilience 1 t f t i t i visually the resilience term can be represented by fig 3 the goal of the resilience metric is to minimize the difference in the numerator t f t i such that the system in question can achieve recovery in a shorter period of time i e t f t i is small in value achieving shorter recovery times highlights greater resilience such that when considering concepts proposed by francis and bekera 2014 absorptive capacity adaptive capacity and restorative capacity of the system are greater the goal subsequently would be to implement a system that achieves greater absorptive capacity adaptive capacity and restorative capacity see fig 4 3 3 the proposed risk formulation framework given the proposed resilience metric the case iii risk formulation is more appropriate to utilize in this study and can be represented in the following generalized formulation 11 risk hazard vulnerability exposure resilience 1 risk expected value of negative impact given the product of hazard vulnerability exposure and resilience components increases in hazard vulnerability and exposure could increase risk however by minimizing the overall recovery time represented by the resilience metric risk can be reduced 12 risk hazard weight vulnerability weight l i k e l i h o o d exposure weight c o n s e q u e n c e s resilience 2 hazard product of joint probabilities of combinations of variables that could contribute to flood hazard via normalized archimedean copula pdf plots see appendix b 3 vulnerability product of applied weights normalized between 0 and 1 with 1 being the highest to a given area of concern based upon several factors such as elevation distance to waterbodies and drainage capacity 4 exposure inundation depth value for an area of concern normalized from 0 to 1 5 resilience resilience 1 t f t i t i t i initial recovery time time in which inundation depths are initially reduced from maximum inundation depths i e max exposure t f final recovery time time in which inundation depths are non existent following maximum inundation depths i e max exposure minimizing the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i results in reduction of risk due to faster recovery 3 4 hazard variables copulas have emerged particularly in hydrology as a useful approach for analyzing multivariate processes or events such as floods for low lying coastal areas in particular such as the cross bayou watershed flooding can occur in two cases 1 with respect to storm tide and or rainfall from a tropical storm event or 2 high tide and or rainfall from a non tropical storm event flooding does not occur in isolation and is dependent on several variables within nature in this study the potential interdependence of daily stage levels in the cross bayou canal daily rainfall daily average wind speed daily barometric pressure and moon phasing fraction of moon illumination fig 5 from observed stations fig 6 are sought to characterize flood hazard potential tidal stage within the canal could be potentially affected by factors such as the following 1 rainfall runoff which drains into the canal from upstream areas 2 high winds from tropical storms which can contribute to storm surges 3 barometric pressure which could increase tidal stage with decreasing pressure and 4 moon phasing such that tides can rise higher and fall lower during new and full moons fraction of moon illumination values of 0 and 1 respectively while rising and falling moderately during first and third quarter moon phases values near 0 25 and 0 75 respectively as evident in fig 5 weak to no correlation is present between the following c tidal stage and wind direction d tidal stage and barometric pressure and e tidal stage and fraction of moon illumination these combinations will not be evaluated by the proposed copula analyses in this study since wave height is not continuous for the same period of record as rainfall and wind speed wave height will be the limiting factor for the period of analysis the year 2012 however is a worthwhile period for copula analysis with associated daily rainfall and daily tidal stage at their maximums compared to the entire period of record 2002 2014 wave height data is also available for the year 2012 3 5 copula functions the copula has its origins in sklar s theorem nelsen 2006 which states that given a joint distribution function h with marginal distributions f 1 and f 2 there exists a copula function c for all real values of x and y 13 h x y c f 1 x f 2 y sklar s theorem can also be applied to n dimensions such that with a distribution function h of n dimensions with marginal distributions f 1 f 2 f n there exists a copula c of n dimensions for all real values of x 14 h x 1 x 1 x n c f 1 x f 2 x f n x the choice in copula is important based upon its ability to capture the dependency structure of the variables considered archimedean copulas are used in a wide range of applications because they are easily constructed nelsen 2006 and are capable of capturing wide ranges of dependence archimedean copulas include the one parameter families gumbel 1960 clayton 1978 ali et al 1978 frank 1979 joe 1993 and the bivariate two parameter bb1 bb3 and bb6 bb9 families joe 1997 an archimedean copula of d dimension s can be represented in the following form 15 c x 1 x d ψ ψ 1 x 1 ψ 1 x d where ψ is a continuous generator function that satisfies the following conditions 1 ψ 1 0 2 ψ 0 3 ψ t 0 and 4 ψ t 0 for all values of t 0 1 widely used archimedean copulas include the gumbel hougaard clayton and frank copulas given d dimension s the gumbel hougaard copula clayton copula and frank copula are represented in table 5 where θ is a dependence parameter their pdfs of archimedean copulas utilized in this study are listed in table 6 the frank copula allows for both positive and negative dependence while the gumbel hougaard copula allows for more positive dependence and the clayton copula allows for more negative dependence however before the choice in copula can be made for determination of joint hazard probability a separate methodology fig 7 consisting of optimization techniques must be developed as such before the identification of the best fit copula can be made appropriate parameters must be estimated with a corresponding likelihood value the best fitting of the copula is best determined by parameter and likelihood estimation the maximum likelihood estimation method can be utilized as a first step toward determining the best fit archimedean copula due to its inherent versatility for varying models and data types khadka 2008 the following steps fig 7 are used to outline the determination of maximum log likelihood using archimedean copula parameters 1 given a d dimensional copula of the form c x 1 x d f f 1 1 x 1 f n 1 x d the corresponding copula density function can be expressed as 16 c x 1 x d 2 c x 1 x d x 1 x d f f 1 1 x 1 f n 1 x d x 1 x d 2 assuming parameters for the copula c and marginal cdfs f i f d as θ and θ θ 1 θ k α 1 α y α k α y respectively with k 1 d where d represents the number of dimensions and y is the number of parameters for a respective marginal distribution can be represented by the following density function 17 f x 1 x d θ θ c f 1 x 1 θ 1 f d x d θ d θ k 1 d f k x k θ 3 defining a likelihood function l l θ x i i 1 n f x i θ such that the likelihood of some parameter s are a certain value given the data x i x n of n observations is similar to the probability of observing the data given some parameter s given the log likelihood is ln l θ x i i 1 n ln f x i θ the log likelihood of eqn 17 this can be represented as 18 ln l θ θ x 1 x n i 1 n ln f x i k x n d θ θ i 1 n c f 1 x i 1 θ k f d x i d θ d θ i 1 n k 1 d ln f k x i k θ for k 1 d where d number of dimensions 4 the negative log likelihood can be determined by adding the negative of eqn 18 as represented 19 ln l θ θ x 1 x n i 1 n ln f x i k x n d θ θ i 1 n c f 1 x i 1 θ f d x i d θ d θ i 1 n k 1 d ln f k x i k θ with the goal of minimizing the negative log likelihood which is equivalent to maximizing the log likelihood the negative log likelihood is found using copula based matlab algorithms adapted for patton 2004 but with changes to account for optimization functions to maximize the log likelihood once the maximum log likelihood of each copula with an associated dependence parameter is determined see appendix b additional criteria is needed to determine the best fit copula for the data the akaike information criterion aic akaike 1974 is typically applied in the selection of semiparametric and parametric copula models however the copula information criterion has been recently developed to provide criteria for copulas specifically with the drawback of increased computational cost jordanger and tjostheim 2014 as such the aic will be a recommended criterion for this study and is determined as follows 20 a i c 2 k 2 ln l l where k is the number of parameters estimated and ll is the log likelihood given a set of candidate models for the data the preferred model is the one with the minimum aic value for the given maximum likelihood the aic value reflects the goodness of fit but it also includes a penalty with each increase in the number of estimated parameters to discourage overfitting 3 6 vulnerability the mariners cove community was selected as a test site for classifying vulnerability the vulnerability component of the risk formulation can be quantitatively defined using an applied weighting system based upon the sum of several criteria table 7 the criteria are as follows 1 the distance to a major water body 2 slope 3 elevation from a digital elevation map dem 4 soil condition and 5 percent imperviousness fig 8 each criteria is assigned a weight from zero least vulnerable to one most vulnerable application of weights to each criterion was conducted using the fuzzy membership tool of arcgis spatial analyst fig 9 showcases the applied weighting within the watershed for each criterion along with the sum of normalized criteria 3 7 exposure the exposure component of the risk formulation is an applied weight which is representative of the level of inundation due to the hazards considered tropical storm debby in late june 2012 was chosen as a test case for determining exposure due to its associated heavy rainfall high tides and waves the level of inundation is determined via a watershed model the interconnected channel and pond routing version 4 software icprv 4 the icprv 4 model streamline technologies inc 2015 is a comprehensive hydrodynamic stormwater and hydrologic model that integrates terrain data hydrologic data hydraulic data and climate data via a layering and data management system icprv 4 was utilized to construct a detailed model of the cross bayou watershed which includes an integrated surface and groundwater interface the icprv 4 software can also determine potential flood inundation via 2d overland flow algorithms for more detail on icpr software please see joyce et al 2017 3 8 adaptive measures four measures table 8 will be considered for defining adaptive capacity each measure will fall within three categories blue grey and green with their respective locations fig 10 from the three measures considered each will be grouped under varying adaptive scenarios table 9 4 results discussion 4 1 exposure exposure of the mariners cove community is presented in figs 11 and 12 as a relation to inundation depth considering the scenarios presented in table 9 the inundation depth is higher with no adaptive measure as expected however incorporation of lid and dredging measures without combined tidal wall and stormwater inlets only offered minor reductions in inundation depths this can be attributed to each adaptive measure offering a different level of resilience against disturbances such as flooding amongst the combination of adaptive measures the incorporation of dredging and the tidal wall with stormwater inlets provides the greatest contribution to reducing the exposure magnitude or inundation depth fig 12c d there are minor changes in spatial exposure when incorporating adaptive measures without the tidal wall and stormwater inlets with the incorporation of the tidal wall and stormwater inlets changes in spatial exposure are more pronounced with an unexpected result such that areas near the tidal wall and stormwater inlets are slightly more exposed spatially however exposure magnitudes are still considerably lower compared to scenarios when no adaptive action was considered exposure only explains one aspect of risk that can be explained further when considering resilience since the incorporation of resilience can essentially determine how long the exposure is felt within the area of concern for instance for what time period will the area of concern be exposed or inundated and in what time period will the flood water begin to recede answers to these questions can be provided by the results of the resilience metric 4 2 resilience the goal of the resilience metric is to minimize the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i such that the system in question can achieve recovery in a shorter period of time such that t f t i is small in value as evident in table 10 the combination of dredging and the tidal wall resulted in minimizing the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i such that this combination resulted in faster overall recovery or greater resilience to flood waters 4 3 risk given the eight scenarios considered the hazard and vulnerability components were the same the primary components that influenced changes in risk were exposure and resilience which are tied to the adaptive measures implemented according to eq 12 the expected value of risk change decreases considerably for adaptive measures incorporating the tidal wall fig 13 reduction in risk magnitudes overall fig 13a h with the incorporation of adaptive measures such as lid dredging and the tidal wall can be attributed to an increase in flood resilience irrespective of changes to exposure magnitudes resilience remains the greatest influence to risk such that increases in flood resilience i e decreases in the time for water to recede from the area via incorporation of the adaptive measures presented in table 10 help to offset risk magnitudes as evident in fig 13 spatially risk does not change much across adaptive measures with the exception of the southwestern corner of the mariners cove area and the eastern boundary of the mariners cove area fig 14 the changes in risk spatially near the southwestern corner and eastern boundary of mariners cove are attributable to incorporation of the tidal wall and stormwater inlets in general the closer to the southern mariners cove boundary the higher the risk overall each adaptive measure offers a different level of resilience against flood disturbances and subsequently offers differing changes in risk more so by magnitude than spatially when conducting risk analysis for future hazards there are sources of uncertainty which could be aleatory or epistemic in nature der kiureghian and ditlevsen 2009 such that uncertainty arises from the process itself or intrinsic uncertainty aleatory and uncertainty from lack of knowledge or data in modelling the process epistemic 4 4 decision analysis decision makers often rely on criteria and weighing possible outcomes before choosing the most beneficial plan of action this particularly concerns municipalities evaluating potential measures for improving infrastructure for their constituents to rely on this is particularly evident in areas that are prone to flooding and often rely on adequate drainage infrastructure to minimize damage to property this is important from the vantage point of national policies related to flood risk and insurance the national flood insurance program nfip aims to reduce the impact of flooding on private and public property by providing affordable insurance to property owners the community rating system crs of the nfip is a voluntary program that encourages communities to adopt and enforce flood management practices which exceed nfip requirements as an incentive for reducing flood insurance premiums recommended flood management practices under crs include flood protection measures such as structural projects along with drainage system maintenance and improving flood risk mapping the adaptive measures considered in the study such as lid the tidal wall with stormwater inlets and dredging are examples of such recommended flood management practices with respect to decision analyses weighting criteria can be a useful approach toward choosing a beneficial plan of action both tables table 11 and table 12 showcase the non weighted criteria values and weighted criteria values for 5 criteria including initial recovery time final recovery time capital investment effort areal average risk and areal average exposure the initial and final recovery times have been previously defined as related to the resilience metric the capital investment effort is the capital investment required to implement the proposed adaptive measure and is assigned a value from 0 to 3 with 0 indicating no capital investment and 3 indicating large capital investment the areal average risk and areal average exposure are the areal means of the risk value and exposure or inundation depth respectively over the entire area of concern fig 15 provides a visual representation of table 12 for decision makers as evidenced by the information in table 12 and fig 15 either the alternative with dredging and the tidal wall or the alternative with lid dredging and the tidal wall should be chosen as the most beneficial plan of action for decision makers both adaptive measures provide the lowest areal average risk the lowest areal average exposure and minimal initial and final recovery time although capital investment costs would be higher 5 conclusion assessing flood risk for decision making requires identifying components of risk and quantifying these components by an integrative approach components associated with risk include hazard vulnerability exposure and resilience in the form of adaptive capacity vulnerability exposure and resilience are dependent on the hazard s considered while vulnerability is dependent on adaptive capacity which is tied to resilience hence risk can vary primarily due to hazard s considered and the associated level of resilience for such hazard s specifically for infrastructure resilience is tied to the level of recovery given the hazard s considered which could be interdependent this has implications for decision makers such as municipalities who may rely on risk being fixed and do not consider interdependent hazards adaptive measures and resilience as a function of adaptive measures and hazards as such this study addresses approaches in considering resilience in overall flood risk management analysis and determines if coupling flood risk and engineering resilience via adaptive measures could improve flood impact assessment as a result this study notes this approach has implications for decision makers such as municipalities and their constituents on a policy level when considering existing flood insurance mapping methodologies incorporating resilience within risk framework as it pertains to drainage infrastructure systems is inherently important for such systems to reduce flood risk particularly for engineered drainage infrastructure systems with adaptive capacity such as lid and flood proofing structure risk is typically considered for a low probable damaging event for design purposes in this study risk is no longer fixed for an entire area but varies spatially which could vary with hazards considered and adaptive measures adopted with this advancement resilience becomes an important factor for determining the performance of drainage infrastructure and flood protection during a major flood event the resilience term was determined from observing time of water receding i e time of recovery via the system the time between the initial and final full water receding from an area of concern is a useful parameter for determining resilience of drainage infrastructure systems toward flooding the shorter the time period for water to fully recede during flooding the more resilient the system and vice versa this study indicates that either the alternative with dredging and the tidal wall or the alternative with lid dredging and the tidal wall should be chosen as the most beneficial plan of action for the community considered enacting a system for which flood waters can recede within a shorter time frame can reduce exposure and subsequently reduce damage and overall risk to flooding our case study has fully confirmed this suite of new concepts within the context of a coupled risk and resilience framework future work may be extended to tackle different types of flooding events for inland cities as well nevertheless key limitations of this study include the relation of vulnerability to resilience within the risk formulation for the purposes of this study the vulnerability metric does not change with space and time however implementation of adaptive measures see table 8 represents a link between vulnerability and resilience allowing changes to the vulnerability metric in space and time dynamically was not a motivation of this study and was instead accounted for by incorporation of the adaptive measures which can be used to offset existing vulnerability and provide a path toward resilience software availability name of software interconnected channel and pond routing icpr version 4 developers streamline technologies inc hardware required 8 gb of ram or higher intel core i5 processor or higher operating system required microsoft windows availability contact streamline technologies inc 1900 town plaza court winter springs florida 32708 or by email sales streamnologies com name of software matlab developers math works hardware required 1 gb of gpu memory or more operating system required windows mac or linux availability contact mathworks https www mathworks com company aboutus contact us contact sales html code scripts contact university of central florida ucf stormwater academy for matlab script of copula analyses acknowledgements the authors are grateful for the financial support of the florida sea grant college program a partnership between the florida board of education the national oceanic and atmospheric administration and florida s citizens and governments under project r cs 58 with assistance from the pinellas county government and to southwest florida water management district for making rainfall data available the authors are thankful for the constructive and copious comments received from anonymous reviewers who helped improve the quality of the paper appendix a curve fitting goodness of fit test table a 1 marginal distribution for each variable for target year 2012 table a 1 variable fitted distribution parameter s location scale shape tidal stage generalized extreme value 0 1836 0 1209 0 4494 rainfall generalized extreme value 2 766 6 261e 04 2 190e 04 wind speed generalized extreme value 0 1232 2 204 6 932 wave height generalized extreme value 0 2545 0 1488 0 2576 table a 2 goodness of fit tests for target year 2012 table a 2 variable of data points null hypothesis p value chi squared k s tidal stage 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis does not reject null hypothesis at 5 significance level rainfall 366 data are consistent with proposed statistical distribution in table 7 0 05 does not reject null hypothesis at 5 significance level rejects null hypothesis wind speed 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis rejects null hypothesis wave height 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis does not reject null hypothesis at 5 significance level appendix b copula analysis for target year 2012 table b 1 copula analysis for hazard variables table b 1 copula family tidal stage vs rainfall tidal stage vs wind speed tidal stage vs wave height max log likelihood value dependence parameter θ aic max log likelihood value dependence parameter θ aic max log likelihood value dependence parameter θ aic gumbel 8 55e 14 1 00 72 2 1 405e 14 1 00 75 8 6 28i 4 47e 15 1 00 78 1 6 28i clayton 577 7 0 100 0 7182 67 06 0 100 3 59 6 28i 9 67e 02 1 093e 01i 0 675 1 748 0 0226i frank 1 417e 05 0 100 11 7 6 26i 42 22 0 100 4 51 6 28i 142 7 0 100 2 08 fig b 1 a frank pdf plot of rainfall and tidal stage 3d view with b rainfall and tidal stage top view for target year 2012 c clayton wind speed and tidal stage 3d view with d wind speed and tidal stage top view for target year 2012 and e clayton pdf plot of wave height and tidal stage 3d view with f wave height and tidal stage top view for target year 2012 fig b 1 
26451,this study sheds light on the coupling of potential flood risk and drainage infrastructure resilience of low lying areas of a coastal urban watershed to evaluate flood hazards and their possible driving forces copulas analyses with the aid of joint probability of simultaneous occurrence help characterize the complexity for hazard classification based on subsequent exposure to inundation under varying levels of adaptive capacity adaptive measures of consideration include traditional flood proofing structures and low impact development facilities for a coastal urban watershed the cross bayou watershed near tampa bay florida findings indicate that coupling flood risk and infrastructure resilience is achievable through the careful formulation of flood risk associated with a resilience metric which is a function of the predicted hazards vulnerability and adaptive capacity the results also give insights into improving existing methodologies for municipalities in flood management practices such as incorporating a multi criteria flood impact assessment that couples risk and resilience in a common evaluation framework keywords flood impact risk analysis resilience assessment coastal sustainability 1 introduction 1 1 background in may 2015 the florida legislature passed and the governor signed into law sb 1094 https www flsenate gov session bill 2015 1094 which regards the consideration of future flood impacts in florida comprehensive plans particularly from a coastal management perspective these new requirements which concern development and redevelopment efforts to reduce flood risk include natural hazards such as high tide events and sea level rise risk in this context can be described as the likelihood of a flood hazard occurring with an associated loss or negative impact the likelihood of associated loss or negative impact is dependent on several factors such as the flood hazard considered and the level of vulnerability to flooding the concepts of hazard and vulnerability can be thought of as the physical manifestations or occurrences of adverse events and the propensity or predisposition to be adversely affected or susceptible to harm ipcc 2014 respectively both of which influence flood exposure simultaneously flood exposure is dependent upon the spread of hazardous effects given vulnerability such as proximity to waterbodies and or condition of drainage outfalls the level of risk however can be influenced by the level of resilience through the connection to the adaptive capacity in a region such as a low lying coastal area the concept of resilience has expanded from its origins in material science and engineering to ecological resilience holling 1973 and eventually to other disciplines such as the social sciences social resilience and psychology psychological resilience when considering infrastructure systems such as drainage under flooding engineering resilience which is highlighted in this study is the ability of such systems to absorb disturbance i e flooding and recover after a disturbance has occurred or the ability to continue functionality under adverse conditions omer 2013 while resilience is typically seen as an outcome it should be viewed as a process which involves adaptation anticipation and improvement in basic functions of a considered system bahadur et al 2010 coupling flood risk and engineering resilience is by no means an easy task de bruijn 2005 defined resilience in terms of flood risk management as the ability of a system to recover from floods quantitatively this can be represented via several indicators such as the amplitude or magnitude of the reaction to disturbances the graduality of reaction s under increasing disturbances and recovery rate de bruijn 2005 a resilient system results in a lower amplitude of reaction to disturbances low graduality of reaction to increasing disturbances and a higher recovery rate analogously this can be tied to three types of capacity of resilience proposed by francis and bekera 2014 which include absorptive capacity adaptive capacity and restorative capacity the absorptive capacity allows for adequate buffering to absorb or contain hazard effects while adaptive capacity is the ability to adjust or provide the necessary changes in response to adverse impacts such as when absorptive capacity has been exceeded restorative capacity is the ability to return to normal function or improved level of performance after a disturbance as with many systems however the absorptive capacity can fluctuate with changes in hazards as is the case when considering future flood risk thus adaptive capacity can be seen as a bridge to restorative capacity and eventually resilience when absorptive capacity has been exceeded adaptive capacity can be understood as the capacity to cope and adapt to adverse effects or from a systems approach the extent to which a system can modify its circumstances to move to a less vulnerable condition luers et al 2003 adaptive capacity also encompasses the ability to plan prepare for facilitate and implement adaptation options klein et al 2003 which first depend upon the nature of the disturbances or potential disturbances subsequently additional factors such as scale of adaptation individual to systemic policy and constraints must also be considered klein et al 2003 has argued for the use of adaptive capacity as an umbrella concept that includes the ability to prepare and plan for hazards as well as to implement technical measures before during and after a hazard event all the while the strategy for adaptive capacity must be flexible with respect to both risk and resilience de bruijn 2005 in order to reduce rigidity in case of disruptive events park et al 2013 while adsorptive capacity can provide an initial gauge of resilience failure is imminent when the adsorptive capacity is exceeded unless adaptive measures are taken this is particularly concerning for system design based upon a particular risk event as opposed to system design adaptive to various levels of risk essentially as park et al 2013 argued the risk based approach considers developing resistance to identified threats as opposed to resilience based approaches which embrace uncertainty and failure due to possible threats via anticipation and adaptation however in this regard risk and resilience cannot be applied individually but must work together risk provides a starting point for identifying potential problems or threats at hand however resilience considers how the progression can be maintained in the face of potential problems or threats 1 2 review of methods when considering flooding in risk analysis and resilience assessment in particular flooding can be caused by any combination of hazards which would impact both risk and resilience this is particularly important for coastal communities which are typically low lying and can face heavy rainfall high tide events and sea level rise within the same time period subsequently there exists a level of uncertainty of any combination of hazards occurring with corresponding consequence s joint probability analysis is useful in this regard for determining the probability of potential flooding hazards occurring simultaneously rather than in isolation a univariate analysis alone cannot provide a complete assessment of the occurrence probability of potential flooding hazards or scenarios particularly if they are interdependent chebana and ouarda 2011 however with typical multivariate analyses one condition is for the variables in question to be independent from one another wahl et al 2012 a univariate analysis also lacks consideration of flooding under multivariate hazards particularly for coastal communities when worst case flooding can occur under combined heavy rainfall and high tide events xu et al 2014 the choice of multivariate analysis must take into consideration that the variables in question could be interdependent may not be under the same family of marginal distributions and are not normally distributed both bayesian networks and copulas have been utilized for analyzing multivariate problems cleophas and zwinderman 2013 nelsen 2006 however bayesian networks require the need for prior information or knowledge for defining conditional probability distributions and the structure of the network depending on the level of detail needed to build such networks the computational demand can be quite large uusitalo 2007 compared to copulas for this reason copulas can be particularly useful while copulas have wide applications across several disciplines such as finance and insurance the application of copulas within hydrology in particular is important since hydrological processes are typically multidimensional in nature and indicate certain levels of interdependence de michele et al 2007 several applications of copulas in hydrology table 1 consisted of analyzing the joint behavior of several hydrological variables during storm events while capturing important statistical dependences de michele and salvadori 2003 salvadori and de michele 2004 balistrocchi and bacchi 2011 modeling multivariate hydrological extremes favre et al 2004 zhang et al 2011 rainfall frequency analysis zhang and singh 2007 flood frequency analysis wang et al 2009 and hydraulic structural design for flooding de michele et al 2005 particularly for inland coastal areas copulas have been useful in analyzing coastal hazards table 2 with underlying hydrological and hydrodynamic processes de michele et al 2007 wahl et al 2012 corbella and stretch 2013 xu et al 2014 trepanier et al 2014 as shown in the previously listed studies copulas can be used to highlight interdependence and the multi dimensional nature of flooding and climate processes however they highlight only one aspect of overall flood risk without considering resilience to these interdependent and multi dimensional events overall flood risk cannot be assessed quantifying flood resilience depends on the interconnection of the urban space and the natural space this interconnection can be represented by the concept of networked systems or networked infrastructure systems when considering infrastructure omer 2013 with regard to flood risk and resilience natural and man made systems such as rivers canals stormwater drainage channels and pipes are seen as the first system s that natural flooding hazards interact with before effects are felt within surrounding systems such as residential communities given the level of resilience of such systems as a result the adaptive capacity of natural and man made systems becomes important to the overall flood risk and resilience due to the cascade effect of interconnected systems omer 2013 park et al 2013 a useful real world example for consideration of both flood risk and infrastructure resilience is the cross bayou watershed located within pinellas county near the tampa bay region of west central florida low lying areas within the cross bayou watershed have been historically prone to flooding driven by rainfall runoff and or high tide events over the years storm events and subsequent flooding have taken a toll on the drainage infrastructure particularly for the undersized conveyance systems found throughout the watershed which are not equipped to handle increased runoff from surrounding urbanization tidal flooding has also impacted low lying areas near a tidal canal which dissects the watershed connecting neighboring bays for which inadequate protection exists water within the canal can flow in either direction depending upon tidal conditions flooding occurs periodically in several low lying communities with strong interactions between the surface water and the groundwater systems in dealing with such a complex system the interconnected pond and channel routing icpr catchment model streamline technologies inc 2015 was applied to the study region for coupling risk and resilience in support of multi criteria flood impact assessment the objectives of this study are to 1 determine the dependence structure of potential flooding risk in a low lying area within the cross bayou watershed via a copula analysis 2 link flood risk and engineering resilience via implementing a risk formulation which includes a resilience metric that is dependent upon the hazard vulnerability and exposure of an area of concern and 3 conduct a multi criteria flood impact assessment for decision analysis such efforts may answer the following scientific questions 1 can the copula analysis fully support the risk analysis 2 how can potential flood risk be offset by modeling adaptive measures for increasing drainage infrastructure resilience with the aid of icpr and 3 can the well coupled flood risk and engineering resilience lead to better decision making via a multi criteria flood impact assessment results of this study will have implications for policy makers such as those in pinellas county who are seeking new ways of reducing the flood insurance rates of their constituents by considering new flood management strategies this paper serves as a companion study of joyce et al 2017 2 study area the cross bayou watershed of pinellas county fig 1 florida was selected as a case study because of its vulnerability to coastal flooding and pinellas county s efforts to implement improved stormwater management to increase the area s adaptive capacity to future hazards the cross bayou watershed encompasses approximately 31 km 2 7697 acres primarily comprised of high density residential industrial and commercial areas an important feature of the watershed is a 16 9 km 10 5 mile long constructed tidal canal the cross bayou canal fig 1 which dissects the watershed and connects tampa bay and boca ciega bay on its northeastern and southwestern ends respectively the cross bayou canal also intersects the pinebrook canal to the southwest fig 1 water within the canal can flow in either direction depending on tidal conditions this feature while useful for overall watershed drainage is potentially hazardous to low lying communities during high tide events particularly when considering the ongoing threat of sea level rise noaa 2016 near the tampa bay region some areas in the watershed are consistently more vulnerable and have a decreased adaptive capacity to flooding the mariners cove residential community fig 2 in particular is known for significant flooding from storm events flooding in the mariners cove community is primarily caused by heavy rains and high tide events of the adjacent cross bayou canal 3 methodology there are many relevant definitions of risk and resilience in the literature the methods outlined in this section focus on the essence of rational choice and the actualization for coupling of risk and resilience 3 1 risk formulation risk in a generalized formulation can be represented as follows 1 risk f likelihood or probability of consequences occurring and consequences risk as a function of likelihood of consequences is related to decision theory such that risk can be represented as an expected value as follows 2 risk expected value likelihood or probability of consequences occurring consequences 3 likelihood or probability of consequences occurring f hazard vulnerability resilience 4 consequences f exposure f hazard vulnerability the likelihood or probability of consequences occurring is a function of hazard vulnerability and resilience the consequences are a function of exposure which is also a function of hazard and vulnerability literature can provide some guidance with regard to how the elements of hazard vulnerability resilience and exposure are related mathematically table 3 details the essence of this issue in an attempt to provide a mathematical formulation of risk in the aforementioned risk formulations sensitivity is the degradation in performance during continuous effects from hazards from a physical system perspective johansen 2010 aside from the generalized formulations presented in table 3 mathematically the formulations have advantages and disadvantages and will be presented on a case by case basis below 5 case i risk hazard vulnerability for this case the risk formulation is general and not specific in scope such that the application of this risk formulation assumes that hazard and vulnerability are only considered without other elements such as exposure or resilience unless defined further by the user of such formulation 6 case ii risk hazard exposure sensitivity resilience for this case the risk formulation is expounded upon by breaking down the vulnerability term as a product of exposure sensitivity and resilience this formulation is less simplistic than in case i however this formulation can only be applied carefully depending on how the resilience term is defined 7 case iii risk hazard vulnerability adaptive capacity case iii applies a quotient adaptive capacity is also one aspect of resilience as defined in literature such as francis and bekera 2014 however the quotient term presents challenges given how adaptive capacity is defined or formulated such that adaptive capacity could be large or small in the case of very small numbers for adaptive capacity the risk can be considerably large conceptually this makes sense however quantitatively this presents challenges for interpretation the success of this formulation depends on how the quotient term adaptive capacity or resilience is defined 3 2 resilience formulation the resilience term throughout the literature does not have a consistent form and varies given the system and assumed response for infrastructure or engineering systems yodo and wang 2016 have outlined how resilience metrics are developed based on three categories or approaches as summarized in table 4 with respect to the first category approach from table 4 defining a quantitative resilience metric based on theoretical resilience curves may present problems since resilience curves could be non linear in form and may not follow a defined pattern given variation in hazard or disruption defining a quantitative resilience metric based on 1 pre and post disruptions performances and 2 reliability and restoration may be more useful for this study francis and bekera 2014 proposed a resilience metric that can account for both pre and post disruptions along with reliability and restoration in the following formulation 8 resilience s p f r f o f d f o where s p speed recovery factor t δ t r e a t r t r for t r t r t δ t r otherwise f r system recovery state f o original system state f d system state following disruption f r f o adaptive capacity f d f o absorptive capacity t δ slack time or the max time during post disruption that is accepted before recovery begins t r time to final recovery i e new equilibrium state t r time to complete initial recovery actions a decay in resilience parameter representing time to new equilibrium state from the aforementioned resilience metric the decay factor a is represented such that if the initial recovery takes longer than the slack time the resilience metric decreases however this metric as proposed by francis and bekera 2014 presented a challenge regarding what value to assign the decay parameter in addition the slack time variable is subjective depending on the system of concern and decision maker lastly when considering flooding the variable representing the original system state f o would be assumed zero since the system i e drainage is at a dormant or no activity state resulting in the ratio becoming undefined in this specific case a potentially useful metric should be modified by considering the difference between the initial recovery time i e initial reduction in inundation depth after maximum inundation area and the final recovery time i e no inundation or no exposure 9 relative change in time of exposure t f t i t i t i initial recovery time time in which inundation depths are initially reduced from maximum inundation depths i e max exposure t f final recovery time time in which inundation depths are non existent following maximum inundation depths i e max exposure a resilience metric can be created that is the reciprocal of the relative change in time of exposure and is represented as follows 10 resilience 1 t f t i t i visually the resilience term can be represented by fig 3 the goal of the resilience metric is to minimize the difference in the numerator t f t i such that the system in question can achieve recovery in a shorter period of time i e t f t i is small in value achieving shorter recovery times highlights greater resilience such that when considering concepts proposed by francis and bekera 2014 absorptive capacity adaptive capacity and restorative capacity of the system are greater the goal subsequently would be to implement a system that achieves greater absorptive capacity adaptive capacity and restorative capacity see fig 4 3 3 the proposed risk formulation framework given the proposed resilience metric the case iii risk formulation is more appropriate to utilize in this study and can be represented in the following generalized formulation 11 risk hazard vulnerability exposure resilience 1 risk expected value of negative impact given the product of hazard vulnerability exposure and resilience components increases in hazard vulnerability and exposure could increase risk however by minimizing the overall recovery time represented by the resilience metric risk can be reduced 12 risk hazard weight vulnerability weight l i k e l i h o o d exposure weight c o n s e q u e n c e s resilience 2 hazard product of joint probabilities of combinations of variables that could contribute to flood hazard via normalized archimedean copula pdf plots see appendix b 3 vulnerability product of applied weights normalized between 0 and 1 with 1 being the highest to a given area of concern based upon several factors such as elevation distance to waterbodies and drainage capacity 4 exposure inundation depth value for an area of concern normalized from 0 to 1 5 resilience resilience 1 t f t i t i t i initial recovery time time in which inundation depths are initially reduced from maximum inundation depths i e max exposure t f final recovery time time in which inundation depths are non existent following maximum inundation depths i e max exposure minimizing the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i results in reduction of risk due to faster recovery 3 4 hazard variables copulas have emerged particularly in hydrology as a useful approach for analyzing multivariate processes or events such as floods for low lying coastal areas in particular such as the cross bayou watershed flooding can occur in two cases 1 with respect to storm tide and or rainfall from a tropical storm event or 2 high tide and or rainfall from a non tropical storm event flooding does not occur in isolation and is dependent on several variables within nature in this study the potential interdependence of daily stage levels in the cross bayou canal daily rainfall daily average wind speed daily barometric pressure and moon phasing fraction of moon illumination fig 5 from observed stations fig 6 are sought to characterize flood hazard potential tidal stage within the canal could be potentially affected by factors such as the following 1 rainfall runoff which drains into the canal from upstream areas 2 high winds from tropical storms which can contribute to storm surges 3 barometric pressure which could increase tidal stage with decreasing pressure and 4 moon phasing such that tides can rise higher and fall lower during new and full moons fraction of moon illumination values of 0 and 1 respectively while rising and falling moderately during first and third quarter moon phases values near 0 25 and 0 75 respectively as evident in fig 5 weak to no correlation is present between the following c tidal stage and wind direction d tidal stage and barometric pressure and e tidal stage and fraction of moon illumination these combinations will not be evaluated by the proposed copula analyses in this study since wave height is not continuous for the same period of record as rainfall and wind speed wave height will be the limiting factor for the period of analysis the year 2012 however is a worthwhile period for copula analysis with associated daily rainfall and daily tidal stage at their maximums compared to the entire period of record 2002 2014 wave height data is also available for the year 2012 3 5 copula functions the copula has its origins in sklar s theorem nelsen 2006 which states that given a joint distribution function h with marginal distributions f 1 and f 2 there exists a copula function c for all real values of x and y 13 h x y c f 1 x f 2 y sklar s theorem can also be applied to n dimensions such that with a distribution function h of n dimensions with marginal distributions f 1 f 2 f n there exists a copula c of n dimensions for all real values of x 14 h x 1 x 1 x n c f 1 x f 2 x f n x the choice in copula is important based upon its ability to capture the dependency structure of the variables considered archimedean copulas are used in a wide range of applications because they are easily constructed nelsen 2006 and are capable of capturing wide ranges of dependence archimedean copulas include the one parameter families gumbel 1960 clayton 1978 ali et al 1978 frank 1979 joe 1993 and the bivariate two parameter bb1 bb3 and bb6 bb9 families joe 1997 an archimedean copula of d dimension s can be represented in the following form 15 c x 1 x d ψ ψ 1 x 1 ψ 1 x d where ψ is a continuous generator function that satisfies the following conditions 1 ψ 1 0 2 ψ 0 3 ψ t 0 and 4 ψ t 0 for all values of t 0 1 widely used archimedean copulas include the gumbel hougaard clayton and frank copulas given d dimension s the gumbel hougaard copula clayton copula and frank copula are represented in table 5 where θ is a dependence parameter their pdfs of archimedean copulas utilized in this study are listed in table 6 the frank copula allows for both positive and negative dependence while the gumbel hougaard copula allows for more positive dependence and the clayton copula allows for more negative dependence however before the choice in copula can be made for determination of joint hazard probability a separate methodology fig 7 consisting of optimization techniques must be developed as such before the identification of the best fit copula can be made appropriate parameters must be estimated with a corresponding likelihood value the best fitting of the copula is best determined by parameter and likelihood estimation the maximum likelihood estimation method can be utilized as a first step toward determining the best fit archimedean copula due to its inherent versatility for varying models and data types khadka 2008 the following steps fig 7 are used to outline the determination of maximum log likelihood using archimedean copula parameters 1 given a d dimensional copula of the form c x 1 x d f f 1 1 x 1 f n 1 x d the corresponding copula density function can be expressed as 16 c x 1 x d 2 c x 1 x d x 1 x d f f 1 1 x 1 f n 1 x d x 1 x d 2 assuming parameters for the copula c and marginal cdfs f i f d as θ and θ θ 1 θ k α 1 α y α k α y respectively with k 1 d where d represents the number of dimensions and y is the number of parameters for a respective marginal distribution can be represented by the following density function 17 f x 1 x d θ θ c f 1 x 1 θ 1 f d x d θ d θ k 1 d f k x k θ 3 defining a likelihood function l l θ x i i 1 n f x i θ such that the likelihood of some parameter s are a certain value given the data x i x n of n observations is similar to the probability of observing the data given some parameter s given the log likelihood is ln l θ x i i 1 n ln f x i θ the log likelihood of eqn 17 this can be represented as 18 ln l θ θ x 1 x n i 1 n ln f x i k x n d θ θ i 1 n c f 1 x i 1 θ k f d x i d θ d θ i 1 n k 1 d ln f k x i k θ for k 1 d where d number of dimensions 4 the negative log likelihood can be determined by adding the negative of eqn 18 as represented 19 ln l θ θ x 1 x n i 1 n ln f x i k x n d θ θ i 1 n c f 1 x i 1 θ f d x i d θ d θ i 1 n k 1 d ln f k x i k θ with the goal of minimizing the negative log likelihood which is equivalent to maximizing the log likelihood the negative log likelihood is found using copula based matlab algorithms adapted for patton 2004 but with changes to account for optimization functions to maximize the log likelihood once the maximum log likelihood of each copula with an associated dependence parameter is determined see appendix b additional criteria is needed to determine the best fit copula for the data the akaike information criterion aic akaike 1974 is typically applied in the selection of semiparametric and parametric copula models however the copula information criterion has been recently developed to provide criteria for copulas specifically with the drawback of increased computational cost jordanger and tjostheim 2014 as such the aic will be a recommended criterion for this study and is determined as follows 20 a i c 2 k 2 ln l l where k is the number of parameters estimated and ll is the log likelihood given a set of candidate models for the data the preferred model is the one with the minimum aic value for the given maximum likelihood the aic value reflects the goodness of fit but it also includes a penalty with each increase in the number of estimated parameters to discourage overfitting 3 6 vulnerability the mariners cove community was selected as a test site for classifying vulnerability the vulnerability component of the risk formulation can be quantitatively defined using an applied weighting system based upon the sum of several criteria table 7 the criteria are as follows 1 the distance to a major water body 2 slope 3 elevation from a digital elevation map dem 4 soil condition and 5 percent imperviousness fig 8 each criteria is assigned a weight from zero least vulnerable to one most vulnerable application of weights to each criterion was conducted using the fuzzy membership tool of arcgis spatial analyst fig 9 showcases the applied weighting within the watershed for each criterion along with the sum of normalized criteria 3 7 exposure the exposure component of the risk formulation is an applied weight which is representative of the level of inundation due to the hazards considered tropical storm debby in late june 2012 was chosen as a test case for determining exposure due to its associated heavy rainfall high tides and waves the level of inundation is determined via a watershed model the interconnected channel and pond routing version 4 software icprv 4 the icprv 4 model streamline technologies inc 2015 is a comprehensive hydrodynamic stormwater and hydrologic model that integrates terrain data hydrologic data hydraulic data and climate data via a layering and data management system icprv 4 was utilized to construct a detailed model of the cross bayou watershed which includes an integrated surface and groundwater interface the icprv 4 software can also determine potential flood inundation via 2d overland flow algorithms for more detail on icpr software please see joyce et al 2017 3 8 adaptive measures four measures table 8 will be considered for defining adaptive capacity each measure will fall within three categories blue grey and green with their respective locations fig 10 from the three measures considered each will be grouped under varying adaptive scenarios table 9 4 results discussion 4 1 exposure exposure of the mariners cove community is presented in figs 11 and 12 as a relation to inundation depth considering the scenarios presented in table 9 the inundation depth is higher with no adaptive measure as expected however incorporation of lid and dredging measures without combined tidal wall and stormwater inlets only offered minor reductions in inundation depths this can be attributed to each adaptive measure offering a different level of resilience against disturbances such as flooding amongst the combination of adaptive measures the incorporation of dredging and the tidal wall with stormwater inlets provides the greatest contribution to reducing the exposure magnitude or inundation depth fig 12c d there are minor changes in spatial exposure when incorporating adaptive measures without the tidal wall and stormwater inlets with the incorporation of the tidal wall and stormwater inlets changes in spatial exposure are more pronounced with an unexpected result such that areas near the tidal wall and stormwater inlets are slightly more exposed spatially however exposure magnitudes are still considerably lower compared to scenarios when no adaptive action was considered exposure only explains one aspect of risk that can be explained further when considering resilience since the incorporation of resilience can essentially determine how long the exposure is felt within the area of concern for instance for what time period will the area of concern be exposed or inundated and in what time period will the flood water begin to recede answers to these questions can be provided by the results of the resilience metric 4 2 resilience the goal of the resilience metric is to minimize the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i such that the system in question can achieve recovery in a shorter period of time such that t f t i is small in value as evident in table 10 the combination of dredging and the tidal wall resulted in minimizing the difference between the initial recovery time t i and the final recovery time i e the numerator t f t i such that this combination resulted in faster overall recovery or greater resilience to flood waters 4 3 risk given the eight scenarios considered the hazard and vulnerability components were the same the primary components that influenced changes in risk were exposure and resilience which are tied to the adaptive measures implemented according to eq 12 the expected value of risk change decreases considerably for adaptive measures incorporating the tidal wall fig 13 reduction in risk magnitudes overall fig 13a h with the incorporation of adaptive measures such as lid dredging and the tidal wall can be attributed to an increase in flood resilience irrespective of changes to exposure magnitudes resilience remains the greatest influence to risk such that increases in flood resilience i e decreases in the time for water to recede from the area via incorporation of the adaptive measures presented in table 10 help to offset risk magnitudes as evident in fig 13 spatially risk does not change much across adaptive measures with the exception of the southwestern corner of the mariners cove area and the eastern boundary of the mariners cove area fig 14 the changes in risk spatially near the southwestern corner and eastern boundary of mariners cove are attributable to incorporation of the tidal wall and stormwater inlets in general the closer to the southern mariners cove boundary the higher the risk overall each adaptive measure offers a different level of resilience against flood disturbances and subsequently offers differing changes in risk more so by magnitude than spatially when conducting risk analysis for future hazards there are sources of uncertainty which could be aleatory or epistemic in nature der kiureghian and ditlevsen 2009 such that uncertainty arises from the process itself or intrinsic uncertainty aleatory and uncertainty from lack of knowledge or data in modelling the process epistemic 4 4 decision analysis decision makers often rely on criteria and weighing possible outcomes before choosing the most beneficial plan of action this particularly concerns municipalities evaluating potential measures for improving infrastructure for their constituents to rely on this is particularly evident in areas that are prone to flooding and often rely on adequate drainage infrastructure to minimize damage to property this is important from the vantage point of national policies related to flood risk and insurance the national flood insurance program nfip aims to reduce the impact of flooding on private and public property by providing affordable insurance to property owners the community rating system crs of the nfip is a voluntary program that encourages communities to adopt and enforce flood management practices which exceed nfip requirements as an incentive for reducing flood insurance premiums recommended flood management practices under crs include flood protection measures such as structural projects along with drainage system maintenance and improving flood risk mapping the adaptive measures considered in the study such as lid the tidal wall with stormwater inlets and dredging are examples of such recommended flood management practices with respect to decision analyses weighting criteria can be a useful approach toward choosing a beneficial plan of action both tables table 11 and table 12 showcase the non weighted criteria values and weighted criteria values for 5 criteria including initial recovery time final recovery time capital investment effort areal average risk and areal average exposure the initial and final recovery times have been previously defined as related to the resilience metric the capital investment effort is the capital investment required to implement the proposed adaptive measure and is assigned a value from 0 to 3 with 0 indicating no capital investment and 3 indicating large capital investment the areal average risk and areal average exposure are the areal means of the risk value and exposure or inundation depth respectively over the entire area of concern fig 15 provides a visual representation of table 12 for decision makers as evidenced by the information in table 12 and fig 15 either the alternative with dredging and the tidal wall or the alternative with lid dredging and the tidal wall should be chosen as the most beneficial plan of action for decision makers both adaptive measures provide the lowest areal average risk the lowest areal average exposure and minimal initial and final recovery time although capital investment costs would be higher 5 conclusion assessing flood risk for decision making requires identifying components of risk and quantifying these components by an integrative approach components associated with risk include hazard vulnerability exposure and resilience in the form of adaptive capacity vulnerability exposure and resilience are dependent on the hazard s considered while vulnerability is dependent on adaptive capacity which is tied to resilience hence risk can vary primarily due to hazard s considered and the associated level of resilience for such hazard s specifically for infrastructure resilience is tied to the level of recovery given the hazard s considered which could be interdependent this has implications for decision makers such as municipalities who may rely on risk being fixed and do not consider interdependent hazards adaptive measures and resilience as a function of adaptive measures and hazards as such this study addresses approaches in considering resilience in overall flood risk management analysis and determines if coupling flood risk and engineering resilience via adaptive measures could improve flood impact assessment as a result this study notes this approach has implications for decision makers such as municipalities and their constituents on a policy level when considering existing flood insurance mapping methodologies incorporating resilience within risk framework as it pertains to drainage infrastructure systems is inherently important for such systems to reduce flood risk particularly for engineered drainage infrastructure systems with adaptive capacity such as lid and flood proofing structure risk is typically considered for a low probable damaging event for design purposes in this study risk is no longer fixed for an entire area but varies spatially which could vary with hazards considered and adaptive measures adopted with this advancement resilience becomes an important factor for determining the performance of drainage infrastructure and flood protection during a major flood event the resilience term was determined from observing time of water receding i e time of recovery via the system the time between the initial and final full water receding from an area of concern is a useful parameter for determining resilience of drainage infrastructure systems toward flooding the shorter the time period for water to fully recede during flooding the more resilient the system and vice versa this study indicates that either the alternative with dredging and the tidal wall or the alternative with lid dredging and the tidal wall should be chosen as the most beneficial plan of action for the community considered enacting a system for which flood waters can recede within a shorter time frame can reduce exposure and subsequently reduce damage and overall risk to flooding our case study has fully confirmed this suite of new concepts within the context of a coupled risk and resilience framework future work may be extended to tackle different types of flooding events for inland cities as well nevertheless key limitations of this study include the relation of vulnerability to resilience within the risk formulation for the purposes of this study the vulnerability metric does not change with space and time however implementation of adaptive measures see table 8 represents a link between vulnerability and resilience allowing changes to the vulnerability metric in space and time dynamically was not a motivation of this study and was instead accounted for by incorporation of the adaptive measures which can be used to offset existing vulnerability and provide a path toward resilience software availability name of software interconnected channel and pond routing icpr version 4 developers streamline technologies inc hardware required 8 gb of ram or higher intel core i5 processor or higher operating system required microsoft windows availability contact streamline technologies inc 1900 town plaza court winter springs florida 32708 or by email sales streamnologies com name of software matlab developers math works hardware required 1 gb of gpu memory or more operating system required windows mac or linux availability contact mathworks https www mathworks com company aboutus contact us contact sales html code scripts contact university of central florida ucf stormwater academy for matlab script of copula analyses acknowledgements the authors are grateful for the financial support of the florida sea grant college program a partnership between the florida board of education the national oceanic and atmospheric administration and florida s citizens and governments under project r cs 58 with assistance from the pinellas county government and to southwest florida water management district for making rainfall data available the authors are thankful for the constructive and copious comments received from anonymous reviewers who helped improve the quality of the paper appendix a curve fitting goodness of fit test table a 1 marginal distribution for each variable for target year 2012 table a 1 variable fitted distribution parameter s location scale shape tidal stage generalized extreme value 0 1836 0 1209 0 4494 rainfall generalized extreme value 2 766 6 261e 04 2 190e 04 wind speed generalized extreme value 0 1232 2 204 6 932 wave height generalized extreme value 0 2545 0 1488 0 2576 table a 2 goodness of fit tests for target year 2012 table a 2 variable of data points null hypothesis p value chi squared k s tidal stage 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis does not reject null hypothesis at 5 significance level rainfall 366 data are consistent with proposed statistical distribution in table 7 0 05 does not reject null hypothesis at 5 significance level rejects null hypothesis wind speed 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis rejects null hypothesis wave height 366 data are consistent with proposed statistical distribution in table 7 0 05 rejects null hypothesis does not reject null hypothesis at 5 significance level appendix b copula analysis for target year 2012 table b 1 copula analysis for hazard variables table b 1 copula family tidal stage vs rainfall tidal stage vs wind speed tidal stage vs wave height max log likelihood value dependence parameter θ aic max log likelihood value dependence parameter θ aic max log likelihood value dependence parameter θ aic gumbel 8 55e 14 1 00 72 2 1 405e 14 1 00 75 8 6 28i 4 47e 15 1 00 78 1 6 28i clayton 577 7 0 100 0 7182 67 06 0 100 3 59 6 28i 9 67e 02 1 093e 01i 0 675 1 748 0 0226i frank 1 417e 05 0 100 11 7 6 26i 42 22 0 100 4 51 6 28i 142 7 0 100 2 08 fig b 1 a frank pdf plot of rainfall and tidal stage 3d view with b rainfall and tidal stage top view for target year 2012 c clayton wind speed and tidal stage 3d view with d wind speed and tidal stage top view for target year 2012 and e clayton pdf plot of wave height and tidal stage 3d view with f wave height and tidal stage top view for target year 2012 fig b 1 
26452,a promising approach to environmental assessment and decision making analyses is based on integrating multicriteria decision analysis mcda and gis tools integration of gis and mcda tools can be potentially achieved through interoperability where these tools can exchange relevant information to tackle a particular environmental problem however the problem of semantic heterogeneity caused by different meanings of data terminologies and models used in gis and mcda has been recognized as an obstacle in the interoperability of these tools conventionally exchange of data between gis and mcda systems for environmental applications relies on prior knowledge to mediate meanings between the two components this paper proposes an ontology enabled framework for semantic interoperability of gis and mcda web services in particular this study has made significant contribution to environmental decision making by providing an interoperable framework to exchange environmental data with intended and unambiguous meanings between gis and mcda services keywords environmental assessment and decision making gis mcda semantic interoperability ontology 1 introduction environmental assessment and decision making eadm applications involve a wide variety of analyses in order to evaluate land use suitability impacts of plans projects and policies on environment environmental vulnerability pollution hazard human health and so on kiker et al 2005 argue that decision making in the context of natural resource utilization and environmental problems can be complex and apparently intractable mainly because of the inherent trade offs between sociopolitical environmental ecological and economic factors the evaluation of remedial and abatement plans for contaminated sites potential environmental consequences of a range of human activities environmental policies and plans and regulatory processes often involves multiple conflicting spatial and non spatial criteria responding to such assessments decisions requires appropriate decision support tools mcintosh et al 2011 one of the most beneficial and applied tools for eadm analyses is gis based multicriteria decision analysis gis mcda most of the eadm problems can be appropriately operationalized in terms of the gis mcda e g kiker et al 2005 jeong et al 2013 perpiña et al 2013 chen et al 2001 zhu and dale 2001 dragan et al 2003 thirumalaivasan et al 2003 hill et al 2005 schlüter and rüger 2007 li et al 2009 aydin et al 2010 chen et al 2010 2011 crossman et al 2011 zerger et al 2011 chen and paydar 2012 chen et al 2013 labiosa et al 2013 kiavarz and jelokhani niaraki 2017 the gis mcda is a procedure that integrates the capabilities of gis and mcda for tackling spatial decision making and land use assessment problems the gis mcda techniques have the potential to support the process of solving a wide variety of environmental decision and assessment problems according to multiple criteria and decision makers preferences e g wang et al 2010 chen et al 2011 crossman et al 2011 zerger et al 2011 chen and paydar 2012 chen et al 2013 labiosa et al 2013 liao et al 2013 nino ruiz et al 2013 vaskan et al 2013 ruiz padillo et al 2016 sadeghi niaraki et al 2010 2011 yu et al 2011a yu and wen 2016 the main rationale behind the efforts to integrate gis and mcda for eadm analyses is that these two distinct areas of techniques can complement each other during the analysis steps jelokhani niaraki and malczewski 2015b malczewski and rinner 2015 each of the gis and mcda components address a different part of eadm processes while gis can be used as a powerful and integrated tool with unique capabilities for storing manipulating analyzing and visualizing geographically referenced environmental data mcda provides a rich collection of procedures and algorithms for structuring eadm problems designing evaluating and prioritizing alternative plans huang et al 2011 reviewed environmental applications of mcda and found that over 300 papers published between 2000 and 2009 reporting mcda applications in the environmental field they argue that mcda can be employed as a formal and valuable methodology in environmental decision making see also munda et al 1994 kiker et al 2005 presented a review of the available literature and provide recommendations for applying mcda techniques in environmental projects they proposed a generalized framework for decision analysis to highlight the fundamental ingredients for more structured and tractable environmental decision making consequently it is in the setting of synergetic characteristics and hybrid heritage of gis and mcda that the importance of gis based eadm becomes obvious a variety of theoretical and methodological perspectives on gis mcda have been suggested over the last 20 years or so the gis mcda approaches have been applied in a wide variety of eadm situations including ecological approaches for defining land suitability habitant for animal and plant species store and kangas 2001 macmillan et al 2016 suitability of land for agricultural activities zabihi et al 2015 zhang et al 2015 montgomery et al 2016 yalew et al 2016 mokarram and hojati 2017 environmental sensitivity and vulnerability navas et al 2011 taramelli et al 2015 leman et al 2016 rebolledo et al 2016 evaluation of the sites for ecotourism bunruamkaew and murayam 2011 gigović et al 2016 ecological planning xie et al 2015 comino et al 2016 evaluation of lands for renewable energies gigović et al 2016 jangid et al 2016 noorollahi et al 2016 rainwater harvesting suitability kahinda et al 2008 jha et al 2014 singh et al 2017 soil water and forest conservation dragan et al 2003 phua and minowa 2005 schlüter and rüger 2007 yu et al 2011b walke et al 2012 krois and schulte 2014 natural hazards chen et al 2001 fernández and lutz 2010 aye et al 2016 selecting the best site for the public and private sector facilities de feo and de gisi 2014 rikalovic et al 2014 abudeif et al 2015 jelokhani niaraki and malczewski 2015a waste management tavares et al 2011 demesouka et al 2013 gbanie et al 2013 and pollution analysis giupponi et al 1999 zhang and huang 2011 the approaches combine a variety of gis and mcda components using different models and platforms e g desktop web and mobile to conduct eadm analyses a number of mechanisms for integrating gis and mcda capabilities in eadm analysis applications have been suggested goodchild et al 1992 lidouh 2013 malczewski and rinner 2015 in general two strategies can be distinguished tight and loose coupling jankowski 1995 malczewski and rinner 2015 the tight coupling means that all components mcda and gis modules are integrated into a monolithic system this strategy integrates mcda capabilities into gis using a shared database and a common user interface the same data source and a single interface is used by both tools the loose coupling approach facilitates the integration of gis and mcda systems using a file exchange mechanism such that a system uses environmental data from the other system as the input data there are however some limitations to the integration of gis and mcda capabilities using tight and loose coupling strategies for environmental assessment the main problem with using tight coupling strategy is that it requires a high level of knowledge of gis mcda and considerable programming efforts to integrate gis and mcda components into a single environmental analysis system in addition due to the monolithic and application specific nature of these strategies they establish a single closed and non interoperable architecture that support only one or a few certain environmental analysis applications application specific the use of loose coupling strategy can reduce these problems by using modular gis and mcda components this strategy allows for having gis and mcda components talk to each other simply by the interchange of files that each can read malczewski 1999 suggests that the integration of gis and mcda should move from the current paradigm in which specific gis and or mcda applications are loosely tightly coupled to their internal data models and structures to interoperable gis mcda platforms according to denzer et al 2005 the presence of an interoperable network of geographic information gi and gis in the environmental domain is going to become more and more important as organizations at different levels are aiming at defining common frameworks and rules for environmental protection in an interoperable gis mcda system gis and mcda tools are able to interoperate and exchange relevant data for tackling a spatial decision problem ideally interoperable gis mcda systems allow users to integrate any of modular spatial analysis and multi criteria functionalities ad hoc at any given time during an eadm analysis process in other words interoperability provides a dynamic integration of gis and mcda tools which allows for a flexible moving of information back and forth between the gis and mcda modules according to the eadm analysis needs however the interoperability of the modular gis and mcda components has been hampered by the heterogeneities that exist between the components the heterogeneities between the two systems can occur at different levels including syntactic and semantic levels syntactic heterogeneity is the difference in data formats and syntaxes such as the difference in file formats used by gis and mcda the ability of gis and mcda tools to exchange information and to use the information that has been exchanged depends on data interchange standards and formats the interoperability of gis and mcda is concerned not just with the packaging of data syntax but with the ability to semantically interpret and understand the meaning of environmental data that is being exchanged beyond the ability of gis and mcda to syntactically exchange data they also need the ability to automatically and meaningfully interpret the data exchanged in order to produce useful eadm results as defined by the two systems however the problem of semantic heterogeneity caused by the differences in the meanings of data terminologies and models used in gis and mcda has been recognized as a substantial obstacle for achieving such a level of interoperability in a note malczewski 2010 argues that the hybrid heritage of gis mcda brings about as a series of terminological contradictions and inconsistencies that cause major challenges in advancing research on interoperability between gis and mcda typically the semantics of environmental procedures and data in both mcda and gis tools are rarely explicit they are in the form of informal and implicit agreements among developers and users in other words the assumptions on the meaning of gis and mcda concepts are implicitly embedded in the tools conventionally decision makers need to mediate the meanings semantics between gis and mcda elements based on their knowledge and then integrate the gis and corresponding mcda elements in most cases they need to manually transfer data derived from one system to another due to the heterogeneous nature such as data and operations heterogeneity of mcda and gis for example when a user imports a file e g criteria weights for a specific eadm problem from mcda software into gis software it specifically requires the user to provide interpretations of the terms used in mcda software and the terms used within the gis software this approach is inefficient and error prone since it requires the user to have knowledge about the underlying models and assumptions in both systems peachavanish et al 2006 without semantic interoperability a user who is only expert in gis not mcda performing environmental analysis tasks that require mcda knowledge needs to consult a mcda analyst to appropriately perform the mcda task in other words the burden of semantic interpretations and mediations between the two domains of gis and mcda is on the shoulders of users the problem of semantic interoperability calls for an ontology based approach for integrating gis and mcda malczewski 2010 malczewski and jelokhani niaraki 2012 recently there have been a number of studies using ontologies in spatial decision support systems e g sadeghi niaraki and kim 2009 blomqvist 2012 jelokhani niaraki and malczewski 2012b li et al 2012 effati and sadeghi niaraki 2015 hasani et al 2015 however to our knowledge none of the studies have specifically focused on the semantic interoperability of gis and mcda therefore the objective of this paper is to adopt an ontological framework to facilitate the semantic interoperability of gis and mcda components e g web services agents for eadm 2 semantic interoperability of gis and mcda concept standards and tools 2 1 concept the integration of gis and mcda components are two classes of computer systems for which the concept of interoperability has a great potential in this context interoperability implies the sharing or exchanging of data and information between gis and mcda systems where the data and information residing in gis may be used effectively by mcda and vice versa the aim is to allow the seamless interaction of gis and mcda systems that were not initially developed for this purpose fig 1 a and b illustrate the concept of interoperability between gis and mcda components using two different scenarios these scenarios present the interoperability depending on the way in which a spatial decision analysis task mostly involves the use of gis or mcda components gis dominant vs mcda dominant in the first scenario user mainly adopts gis components to perform the decision analysis task the gis components create feasible alternative locations determine spatial criteria values perform standardization function for normalizing criteria maps and carry out an overlay analysis to combine criterion maps for computing the overall scores or rankings of alternatives in this case the gis components are the core and mostly run the spatial decision analysis processes and merely interoperate with a mcda component to request the numerical weights computed based on the user inputs see fig 1a contrary to the first scenario in which decision tasks are mostly performed by gis functionalities in the second scenario the mcda components tend to be used more for solving the decision problem in this scenario the user mainly adopts the mcda components to structure the decision hierarchy e g analytic hierarchy process convert the user preferences to numerical weights integrate the weights and criteria values for computing the overall scores or rankings of alternatives based on a particular mcda technique e g ordered weighted averaging and generate sensitivity analysis graphs see fig 1b these components interoperate with the gis components to only request the spatial criterion values to compute the overall scores and return the evaluation results to gis part for representing on the map the interoperability of gis and mcda components can mainly occur at the syntactic and semantic levels syntactic interoperability means that both the gis and mcda components have agreed upon syntax and data format for the exchange of decision information whereas syntactic interoperability focuses on the syntax or structure of data semantic interoperability enables the automatic recognition of the data exchanged semantic interoperability is the ability of gis and mcda components to transmit data with intended and unambiguous meanings while preserving source and receiver software i e gis and mcda autonomy 2 2 standards interoperability of gis and mcda systems requires the development and use of uniform standards that provide the bridge language needed for the exchange of information with the objective of improving interoperability at the syntactic and semantic levels communities in the gis and mcda fields have been developing standards 2 2 1 gis standards in the gis community ogc open geospatial consortium has developed extensive standards for geospatial applications formally providing the terms definitions and information models in the gis domain see ogc 2014 ogc is the international organization dedicated to develop geospatial implementation standards consistent with the standards of international organization for standardization iso the federal geographic data committee fgdc national spatial data infrastructure nsdi spatial data transfer standard sdts and international committee for information technology standards incits digital geographic information working group dgiwg and other abstract or content standards see albrecht 1999 open and interoperable geoprocessing services or the ability to share heterogeneous geodata and geo processing resources is the main goal of ogc the ogc aims at achieving its task of geographic standardization through specifications e g abstract and implementation specifications these specifications have been developed in the geospatial community to address interoperability challenges 2 2 2 mcda standards major efforts in the standardization of concepts within the mcda community are by decision deck ewg mcda euro working group on multicriteria decision aiding and sds spatial decision support consortiums decision deck a consortium of european researchers has defined data standards in the mcda domain called umcda ml universal multi criteria decision analysis modeling language and xmcda a standardized xml based mcda standards which can be adopted by various mcda applications to make them interoperable meyer and bigaret 2012 umcda ml is one of the scientific initiatives inside decision deck to support a universal modeling language to express mcda concepts and generic decision aid processes xmcda is an instance of umcda ml which provides a standardized xml specification to represent objects and data structures in the field of mcda the goals of xmcda standards are to provide a common and unified data structure in mcda resolve the heterogeneity of the available mcda tools ease the interoperability of different mcda web services and tools using a common terminology chaining of various mcda algorithms and web services and the visual representation of mcda concepts and data structures via standard tools meyer and bigaret 2012 cailloux et al 2013 ewg mcda is a working group whose objective is to promote original research and contribute to the development in the field of mcda in a newsletter by ewg mcda roy 2000 has published a standard decision aiding glossary paoli et al 2011 suggest that the terms represented in this glossary can be used for the construction of a standard mcda ontology in another attempt sds spatial decision support consortium a network of professionals involved in spatial decision applications has developed a sds knowledge portal as a source of information and a common vocabulary ontology for sds researchers and practitioners see sds consortium 2014 this portal partially covers the essential concepts in mcda including terms methods techniques tools and models data sources etc as a part of the broad body of knowledge in the sds field 2 3 tools web services and ontology 2 3 1 web services currently there are many interoperable gis and mcda web services freely available for use by decision makers to perform gis and mcda functionalities respectively particularly the ogc standards compliant services such as web feature service wfs web map service wms web coverage service wcs and web processing service wps see ogc 2014 are being used in different gis applications similarly the xmcda based web services by decision deck consortium allow an easy access to mcda tools decision deck offers an architecture dedicated to web services whose primary goal is to offer a place where mcda computational resources methods etc can be hosted and made freely available to anyone for details see http www decision deck org ws webservices html from an algorithmic point of view the xmcda web services conduct elementary mcda processes which if properly chained can rebuild a mcda application cailloux et al 2013 xmcda web services are able to interoperate based on xmcda standards ros 2011 meyer and bigaret 2012 with the availability of distributed gis and or mcda web services the web services can be used and chained to solve the decision problem at hand 2 3 2 ontology despite many efforts to address issues of standards in gis and mcda communities interoperability especially with respect to semantics amongst gis and mcda components is scant semantic interoperability all relies on a common understanding and agreements about the meanings of concepts in the two domains it is achieved only when components agree on the meaning of the data they exchange ontologies are considered as an enabling technology for the semantic web and methods and tools for ontology definition are being studied for interoperability an ontology can facilitate semantic interoperability between the gis and mcda components through providing a formal semantic and explicit specification of a shared understanding and conceptualization in the two domains gruber 1993 guarino et al 2009 3 ontological framework for semantic interoperability of gis and mcda the methodology used in this study involves developing an interoperable environmental gis mcda framework based on a set of ontologies and gis mcda web services see fig 2 the ontologies include gis and mcda domain ontologies gis and mcda task ontologies gis data ontology and gis mcda mapping ontology the gis and mcda domain ontologies organize and represent concepts that are used for the purpose of data retrieval and analysis in gis and mcda context respectively gis and mcda task ontologies contain knowledge about the usage of gis and mcda domain ontologies respectively this framework allows users to define the gis mcda elements e g spatial constraint and property analyses decision rule etc of a particular eadm analysis these gis mcda elements can be defined by choosing a pre defined gis or mcda task template from gis mcda task ontologies the information needed to support spatial multicriteria decision making for the application at hand requires geographical data typically geographic data are used to support decision makers in determining the set of geographic alternatives constraints and criteria associated with the eadm analysis at hand to this end the data ontology organizes information relevant to the application the system automatically translates and decomposes the user specified gis and mcda tasks into a set of relevant atomic operations based on the task workflow knowledge represented by swrl rules in the gis and mcda task ontologies respectively it can use the appropriate web services corresponding to each of the operations the individual services of each domain e g gis can be used in the order represented by swrl rules and then chained either manually by users or automatically next the individual services e g gis and or mcda services can semantically exchange their output data i e messages with each other based on the terminology used in the domain ontologies e g gis and or mcda domain ontology in other words for gis and or mcda web services to be able to semantically communicate input and output data of services must be committed to gis mcda domain ontologies after the completion of for example a particular gis task using a set of gis web services source services the outputs of the gis services need to be exchanged with the other party i e mcda web services target services the mediator web service uses the mapping ontology to mediate map the meaning of outputs produced by gis services and inputs required by mcda services for appropriately transferring the data between them 3 1 domain ontologies 3 1 1 gis domain ontology it is required to provide formal representation of concepts that are used for the purpose of environmental data retrieval and analysis in the gis domain the gis domain ontology should describe the vocabulary related to gis domain the concepts used in the gis domain ontology can be mainly derived from previous studies e g kolas et al 2005 budak arpinar et al 2006 peachavanish et al 2006 hudelot et al 2008 bittner et al 2009 batsakis and petrakis 2010 fonseca 2001 frank 2003 kavouras and kokla 2007 top level ontologies and ogc specifications or iso standards see fig 3 for instance battle and kolas 2012 suggest that according to the ogc geosparql standard a basic gis ontology includes feature a thing that can have a spatial location geometry a representation of a spatial location and spatial object a superclass of both features and geometries the property hasgeometry links features a thing to their geometry their location jung et al 2013 suggest that this basic ontology can be extended based on the ogc specifications for example the ogc abstract specification topic 5 feature stipulates that a geospatial feature should contain a particular type of geometry e g point polyline or polygon with an srs spatial reference system and attribute in addition to above concepts the gis domain ontology should include the notion of role for spatial features objects see peachavanish et al 2006 fonseca et al 2002 the notion of roles can be used for representing whether a feature plays the role of being source or target of a spatial property see fig 4 typically external spatial properties including mereological topological and metric relations describe relations between two objects one of which plays the role of a source object and the other one plays the role of a target object the notion of roles is important in the gis domain ontology because they convey a specific intent user intentions on the information required in other words spatial property analyses generate different outputs information depending on which of the two features is source or target for example the outputs of spatial property analysis find average distance between a parking candidate site and airport stations that considers parking candidate site as source object and airport stations as target objects differs from that for find average distance between an airport station and parking candidate sites that considers airport station as source object and parking candidate sites as target objects 3 1 2 mcda domain ontology the mcda domain ontology organizes and represents concepts that are used for the purpose of environmental data retrieval and analysis in mcda see mahmoudi and muller schloer 2009 sadeghi niaraki and kim 2009 malczewski and jelokhani niaraki 2012 šaša bastinos and krisper 2013 this ontology can also be derived from standardized sources such as xmcda standards by decision deck the mcda vocabulary by sds consortium and french english decision aiding glossary by ewg community see fig 5 it structures and encodes the knowledge terms concepts and properties of mcda data models such as decision goal decision alternatives decision constraints evaluation criteria objectives and attributes criteria weights decision matrix alternative evaluation outcomes etc see fig 5 contrary to gis domain that there are already many researches focused on representing gis concepts using ontologies there have been little research efforts focusing on representing mcda components into an ontological structure in the 5th and 8th workshops by decision deck consortium the construction of a global mcda ontology has been proposed paoli et al 2011 3 2 task ontologies 3 2 1 gis task ontology gis task ontology formulates the spatial analysis steps and their sequences for performing eadm analysis tasks in terms of the terminology defined in the gis domain ontology see fig 6 e g sadeghi niaraki and kim 2009 jelokhani niaraki and malczewski 2012a timpf 2001 argues that giscience needs task ontologies in addition to domain ontologies to enable knowledge sharing semantic interoperability and web service reuse for interoperable gis in previous studies ontologies have been identified as ways to standardize and conceptualize spatial analysis steps as the workflows of geospatial analysis tasks e g peachavanish et al 2006 yue et al 2009 accordingly in this study the gis task ontology captures the workflow knowledge for solving gis analysis tasks where the required gis functions gis web services and their sequences for a variety of gis tasks are presented as the workflows in the ontology the web services that perform gis functions are considered as individuals in the ontology table 1 shows examples of gis analysis tasks their types and required gis functions for instance the gis analysis task determine average distance between source and target features within a radius from source features is a type of spatial property analysis and can be semantically modeled using the classes of gis functions e g buffer overlay and distance with the gis web services as the individuals of these classes see also fig 7 describing gis workflow knowledge of eadm analyses in the ontology follows the approach proposed by jung et al 2013 who use swrl rules to describe the required gis functions in the workflows by using swrl one can represent knowledge on what gis functions and their sequences are needed for a particular task swrl rules can be expressed in terms of ontology classes properties individuals and data values to present the workflows for gis analysis tasks the right hand side of the rules represent a generic gis task class containing specific gis tasks as instances and the left hand side of the rule represents the relevant gis function classes their instances and sequences to perform the specific gis task tables 2 and 3 show examples of swrl rules for gis analysis tasks including spatial property and constraint analysis tasks respectively for instance rule 1 in the table delineates the spatial property analysis task class determine distance between source and target features requires the gis function classes of buffer overlay and distance using the hasgisoperation property with its sequences through the hasnextgisoperation property the individuals i e task instances belonging to this task class has the corresponding gis functions belonging to the classes of buffer overlay and distance for example the instances of this task class including determine nearest distance between source and target features within a radius from source features determine average distance between source and target features between two radiuses from source features and determine nearest distance between source and target features outside a given radius from source features require different sets of the gis functions gis web services of buffer full side overlay intersect and distance farthest buffer ring overlay intersect and distance average and buffer full side overlay clip and distance nearest as instances of classes buffer overlay and distance respectively 3 2 2 mcda task ontology similar to the gis task ontology mcda task ontology formulates the mcda steps and their sequences for eadm analysis tasks i e the computational and operational elements of mcda see fig 8 sadeghi niaraki and kim 2009 jelokhani niaraki and malczewski 2012a specifically the mcda task ontology used in this study structures the mcda workflow knowledge of eadm analyses by combining various elementary calculation mcda components see fig 9 instead of considering only monolithic mcda models or software tools the xmcda standard aims to enable algorithmic mcda workflows that is the design of components e g xmcda web services performing independent computational steps used in mcda methods meyer and bigaret 2012 cailloux et al 2013 meyer and bigaret 2012 suggest that the mcda methods should not be used as static and immutable black boxes but rather as dynamic workflows which can be changed and adapted for the current purpose table 4 shows an example swrl rule for the mcda task of evaluate alternatives using a decision rule for instance this mcda task class requires the classes of mcda functions of preference elicitation preference to weight conversion decision rule and evaluate using the hasmcdaoperation property with its sequences through the hasnextmcdaoperation property the individuals i e task instances belonging to the mcda task class has the corresponding mcda functions i e mcda web services belonging to the above classes for example the instances of the task class evaluate alternatives using a decision rule including find rankings of alternatives in descending order using an ahp owa and find scores of alternatives using an rank order wlc require different sets of the mcda functions or web services of pair wise comparison ahp owa and ranking in descending order and rank order rank sum rs wlc and score as instances of classes preference elicitation preference to weight conversion decision rule and evaluate respectively 3 3 mapping ontology the semantic interoperability between the gis and mcda components for a particular eadm analysis requires mediation or mapping between the concepts in the two gis and mcda domain ontologies in this study a semantic mapping bridge ontology has been used to meaningfully relate the concepts in the two ontologies arch int arch int 2013 developing such a mapping ontology requires knowledge from both gis and mcda domain experts in order to make the correct semantic correspondences and specify all mappings between the gis and mcda concepts table 5 shows an example of mcda terminology and the corresponding vocabulary used in gis as well as their semantic relatedness for example the mcda term alternative and the gis term source feature object can be related in such a way that an alternative entity can be instantiated as a source feature instantiation of an alternative as source feature means that an alternative represents a particular class of spatial features in the decision space the term weight in mcda is semantically different from the spatial weight in gis where it is defined as the relative importance of the criteria objectives attributes in the mcda terminology while in the gis terminology weight may be conceived as a file containing the spatial relationships between spatial entities thus they are two different non overlapping concepts fig 10 demonstrates the process of semantic mapping using an example for example the terms source feature spatial constraint spatial property linear unit in the gis ontology can be related with the terms alternative constraint attribute and ratio scale in the mcda ontology using the semantic relations of is instance of is is kind of and is a respectively this process allows the instances of the gis ontology classes to be appropriately transformed as the data instances of the mcda ontology classes 3 4 gis data ontology gis data ontology at the lowest level of ontological hierarchy is built as a means to better organize data relevant to eadm analyses enable ontology assisted queries based on the semantic relationships and assist users and or search engines in semantically retrieving relevant information sadeghi niaraki and kim 2009 the concept feature presents an abstract top level concept in the ontology depending on the application at hand e g landfill site selection the concept is further broken into a number of sub concepts which represent general urban classes such as transportation waste type landfill etc see fig 11 the gis data ontology is connected to the gis domain ontology by making the top class feature in the data ontology a subclass of feature in the gis domain ontology meaning that instances of the class can point to src geometries attributes etc with the has src has geometry and has attribute properties see battle and kolas 2012 4 prototype implementation a prototype gis mcda system was developed to demonstrate the semantic interoperability of gis and mcda components in an eadm analysis context the prototype involves a set of gis and mcda web services ogc and xmcda standard compliant web services interoperating to solve a landfill site selection suitability ranking as one of the eadm analysis problems see fig 12 both the ontologies and swrl rules have been formalized and implemented using owl ontology web language in protégé environment fig 13 shows the gui of gis mcda application for solving the landfill site suitability problem the mcda elements associated with landfill site selection including the spatial constraints objectives attributes etc were specified through the gui the constraints were defined as the sub classes of gis domain ontology based on the appropriate constraint task templates which are linked to the corresponding constraint analysis tasks in the gis task ontology for example defining the constraint a candidate landfill site must be within 100 m of residential area requires choosing the template a source feature must be within a given distance of target features which is in turn linked to the constraint analysis task determine if source features are within a given distance of target features when users choose the constraint task template a dialog window specific to that template appears and allows for specifying landfill as source feature i e alternative residential area as target feature and the value 100 m to derive the feasible source features that are located within a given distance of target features see fig 14 the system allows users to access the names of source i e alternatives and target features in the dialog window through loading from the gis data ontology in this prototype the data ontology is linked to an oracle spatial database containing gis data associated with the landfill site selection including the data related with the specified source i e alternative in mcda domain and target features the spatial data are stored in the oracle database based on the ogc standards the use of oracle spatial allows for performing ontology assisted queries based on the semantic relationship between the data it enables to use semantic operators to meaningfully search source and target features specified in the dialog window referring to the gis data ontology let s say the user specifies for example transportation station as the target feature in the dialog window the user needs to consider the transportation station of any types as target feature in the decision analysis however a typical query of the gis database involving syntactic match will not return any rows because no table rows contain the exact value transportation station for example the following query will not return any rows select type from feature where station type transportation station however many rows in the gis database are relevant because they are a type of transportation station for instance the features such as bus stops terminals etc are all types of transportation stations the sem related operator in oracle spatial instead of lexical equality enables to semantically retrieve all the relevant rows from the gis database with the aid of gis data ontology sem related operator is able to infer over the gis data ontology and find all transportation stations including subway stations bus stops airports terminals and railway stations as these features have been semantically defined as the sub or equivalent classes of transportation station in the ontology the objective and attribute concepts can be similarly created through gui each of the objectives is derived from attributes for example the objective maximize accessibility could be derived from two attributes nearest distance between landfill candidate sites and residential areas and average distance between landfill candidate sites and residential areas similar to constraints attributes can be constructed in terms of spatial property templates which are linked to the corresponding spatial property analysis tasks in gis task ontology for instance attributes nearest distance between the landfill candidate sites and residential areas can be defined based on the template nearest distance between source and target features within a radius from source features which requires the task determine nearest distance between source and target features within a radius from source features see fig 15 similar to gis analysis tasks i e spatial constraint and property tasks users can choose their appropriate mcda tasks such as evaluate the alternatives using a decision rule for eadm analyses for example the user can select determine scores of alternatives using rank order wlc or determine rankings of alternatives in a descending order using ahp owa as one of the instances of mcda task class evaluate the alternatives using a decision rule if users selects determine scores of alternatives using rank order wlc the inference engine identifies the corresponding mcda operations i e mcda web services based on the swrl rule in mcda task ontology once the problem specific alternatives source features constrains criteria i e objectives and attributes preferences mcda task etc have been specified the system invokes the appropriate web services to perform the specified tasks specifically the gis web services can be invoked according to spatial constraint and property templates tasks defined in the gis task ontology to compute the constraint values feasible candidate sites and spatial property values specified similarly mcda services could be invoked to perform the mcda operations including preference elicitation preference conversion to weight and decision analysis rule methods the task workflow knowledge represented by swrl rules in the gis and or mcda task ontologies guide how to combine the relevant atomic web services they describe dependencies regarding the execution order of web services depending on the task the inference engine appropriately identifies the web services using the swrl rules composition of the atomic gis and or mcda web services is based on the sequences of peer to peer messages exchanged among them for gis and or mcda web services to engage in knowledge level communication they must be able to i parse the messages at the syntactic level i e services should parse or decode messages to its parts such as message content language sender etc and ii understand the messages at the semantic level the parsed symbols knowledge terms must be understood in the same way the content of message is unambiguously defined where what is sent is the same as what is understood after the completion of a particular gis task using the set of gis web services source services the outputs of the task need to be exchanged with the other party i e mcda web services target services fig 16 shows a simple example of mapping procedure used to transform the data instances from gis to mcda domain and vice versa first the mediator web service receives the message from the source web service and parse it at both syntactic and semantic levels to extract the semantic data including instances classes properties and property values next the mediator web service reasons if there is a corresponding class property for the extracted classes properties in the mapping ontology for example using reasoning over mapping ontology and extracted semantic data messages received from source services it is possible to understand and infer that if a source feature s is instance of the class of source feature and this class is equivalent with the class of alternative then s is instance of the class alternative after finding the corresponding classes mediator web service transfers the data to the classes and creates a message based on them for sending to the target web service let s illustrate the exchange of messages between gis and mcda web services using a scenario according to the scenario the gis service determines the feasibility of alternatives e g the constraint value of 1 for the source feature e g alternative landfill site a1 and compute the spatial property value i e nearest distance between landfill candidate sites and streets e g 100 m for the feasible source feature the service sends the source feature constraint and spatial property outputs through the soap messages to the mediator web service for instance see fig 17 the mediator web service translates these gis data outputs to the corresponding mcda elements based on the semantic mapping ontology and in turn sends them to the mcda web service 5 discussion this work is considered as an initial step toward a semantic interoperability of gis and mcda for eadm analyses however we consider several aspects to be improved as future lines of work the lack of adequate standards or ontologies can be considered as the major obstacle in making progress with respect to building an interoperable gis mcda environment it has been recognized that the key to the success of ontology based gis mcda systems for the purpose of interoperability is a set of agreed upon standards at different levels including data operations and terminology at the moment the main problem is that there does not exist unified domain and task ontologies in the communities especially in mcda community although in recent years ontologies have been widely used and become one of the top areas of interest in gis community it has been only recently that some considerations have been given to the use of ontologies in mcda community further efforts should be made in mcda community to appropriately develop ontologies as well as standard specifications for mcda elements moreover an effective interoperability between gis and mcda systems requires that experts of the two communities agree on the terminologies and precisely define mapping between gis and mcda elements in the semantic mapping ontology extensive efforts have been made in gis community for standardizing catalogue services e g opengis catalogue service implementation specification ogc catalogue services standard as well as developing interoperable services most notably the ogc standards compliant services such as web feature service wfs web map service wms web coverage service wcs and web processing service wps see ogc 2014 however the efforts to develop interoperable mcda services and standardized registry catalogues for mcda services has been rather limited this paper mainly focused on semantic interoperability integration between gis and mcda but does not consider the advanced matter of automatic discovery of gis and or mcda services the study used swrl rules to represent the gis and mcda task knowledge for chaining the web services future study could adopt reasoning algorithms to semantically discover the required conceptual workflows and gis services for performing the gis and mcda tasks and facilitate the web service composition process while the present study used a couple of gis and mcda tasks in the prototype system to illustrate the potential of proposed approach for semantic interoperability of gis and mcda future research might consider using additional and more complex tasks 6 conclusion this paper has presented an ontology based approach for semantic interoperability of gis and mcda components for eadm problems our basic premise is to provide a semantic interoperable gis mcda framework where the set of existing gis and mcda web services could interoperate exchange their data and meaningfully interpret and understand the meaning of data that is being exchanged for the eadm purposes the proposed approach involves the use of a set of ontologies to address the challenges of semantic interoperability between gis and mcda the ontologies include gis and mcda domain ontologies gis and mcda task ontologies gis mcda mapping ontology and gis data ontology the ontologies serve as the foundation on which a semantic interoperable gis mcda framework is built specifically they serves as i a knowledge base containing all necessary information to be used manipulated reasoned and processed by the web services ii a semantic knowledge model upon which the gis and or mcda web services can effectively communicate and exchange messages this enables the services to understand messages from other services and semantically exchange and interpret their knowledge iii a semantic mediator to mediate the meaning of terms between gis and mcda and iv a gis mcda workflow knowledge base that provides a service chaining knowledge for decision making a prototype implementation of eadm analysis was developed based on the ontologies to demonstrate the semantic interoperability of gis and mcda components the prototype involves a set of gis and mcda web services semantically interoperating to solve a landfill site suitability problem the gis and mcda web services could semantically interact and interoperate with each other by means of ontologies consequently the study facilitates syntactic and semantic interoperability between the two different set of gis and mcda web services for solving environmental problems this allows users to integrate any of modular spatial analysis and multi criteria tools i e web services ad hoc at any given time during a particular eadm process it opens up the opportunities for advancing research on integrating gis and mcda tools in environmental decision making processes specifically it could shift the paradigm of environmental decision analyses from application specific and monolithic to a semantic interoperable system acknowledgment this research was supported in part by korean msit ministry of science and ict under the itrc support program iitp 2017 2016 0 00312 supervised by the iitp and in part by the nrf grant 2014r1a2a1a11053135 
26452,a promising approach to environmental assessment and decision making analyses is based on integrating multicriteria decision analysis mcda and gis tools integration of gis and mcda tools can be potentially achieved through interoperability where these tools can exchange relevant information to tackle a particular environmental problem however the problem of semantic heterogeneity caused by different meanings of data terminologies and models used in gis and mcda has been recognized as an obstacle in the interoperability of these tools conventionally exchange of data between gis and mcda systems for environmental applications relies on prior knowledge to mediate meanings between the two components this paper proposes an ontology enabled framework for semantic interoperability of gis and mcda web services in particular this study has made significant contribution to environmental decision making by providing an interoperable framework to exchange environmental data with intended and unambiguous meanings between gis and mcda services keywords environmental assessment and decision making gis mcda semantic interoperability ontology 1 introduction environmental assessment and decision making eadm applications involve a wide variety of analyses in order to evaluate land use suitability impacts of plans projects and policies on environment environmental vulnerability pollution hazard human health and so on kiker et al 2005 argue that decision making in the context of natural resource utilization and environmental problems can be complex and apparently intractable mainly because of the inherent trade offs between sociopolitical environmental ecological and economic factors the evaluation of remedial and abatement plans for contaminated sites potential environmental consequences of a range of human activities environmental policies and plans and regulatory processes often involves multiple conflicting spatial and non spatial criteria responding to such assessments decisions requires appropriate decision support tools mcintosh et al 2011 one of the most beneficial and applied tools for eadm analyses is gis based multicriteria decision analysis gis mcda most of the eadm problems can be appropriately operationalized in terms of the gis mcda e g kiker et al 2005 jeong et al 2013 perpiña et al 2013 chen et al 2001 zhu and dale 2001 dragan et al 2003 thirumalaivasan et al 2003 hill et al 2005 schlüter and rüger 2007 li et al 2009 aydin et al 2010 chen et al 2010 2011 crossman et al 2011 zerger et al 2011 chen and paydar 2012 chen et al 2013 labiosa et al 2013 kiavarz and jelokhani niaraki 2017 the gis mcda is a procedure that integrates the capabilities of gis and mcda for tackling spatial decision making and land use assessment problems the gis mcda techniques have the potential to support the process of solving a wide variety of environmental decision and assessment problems according to multiple criteria and decision makers preferences e g wang et al 2010 chen et al 2011 crossman et al 2011 zerger et al 2011 chen and paydar 2012 chen et al 2013 labiosa et al 2013 liao et al 2013 nino ruiz et al 2013 vaskan et al 2013 ruiz padillo et al 2016 sadeghi niaraki et al 2010 2011 yu et al 2011a yu and wen 2016 the main rationale behind the efforts to integrate gis and mcda for eadm analyses is that these two distinct areas of techniques can complement each other during the analysis steps jelokhani niaraki and malczewski 2015b malczewski and rinner 2015 each of the gis and mcda components address a different part of eadm processes while gis can be used as a powerful and integrated tool with unique capabilities for storing manipulating analyzing and visualizing geographically referenced environmental data mcda provides a rich collection of procedures and algorithms for structuring eadm problems designing evaluating and prioritizing alternative plans huang et al 2011 reviewed environmental applications of mcda and found that over 300 papers published between 2000 and 2009 reporting mcda applications in the environmental field they argue that mcda can be employed as a formal and valuable methodology in environmental decision making see also munda et al 1994 kiker et al 2005 presented a review of the available literature and provide recommendations for applying mcda techniques in environmental projects they proposed a generalized framework for decision analysis to highlight the fundamental ingredients for more structured and tractable environmental decision making consequently it is in the setting of synergetic characteristics and hybrid heritage of gis and mcda that the importance of gis based eadm becomes obvious a variety of theoretical and methodological perspectives on gis mcda have been suggested over the last 20 years or so the gis mcda approaches have been applied in a wide variety of eadm situations including ecological approaches for defining land suitability habitant for animal and plant species store and kangas 2001 macmillan et al 2016 suitability of land for agricultural activities zabihi et al 2015 zhang et al 2015 montgomery et al 2016 yalew et al 2016 mokarram and hojati 2017 environmental sensitivity and vulnerability navas et al 2011 taramelli et al 2015 leman et al 2016 rebolledo et al 2016 evaluation of the sites for ecotourism bunruamkaew and murayam 2011 gigović et al 2016 ecological planning xie et al 2015 comino et al 2016 evaluation of lands for renewable energies gigović et al 2016 jangid et al 2016 noorollahi et al 2016 rainwater harvesting suitability kahinda et al 2008 jha et al 2014 singh et al 2017 soil water and forest conservation dragan et al 2003 phua and minowa 2005 schlüter and rüger 2007 yu et al 2011b walke et al 2012 krois and schulte 2014 natural hazards chen et al 2001 fernández and lutz 2010 aye et al 2016 selecting the best site for the public and private sector facilities de feo and de gisi 2014 rikalovic et al 2014 abudeif et al 2015 jelokhani niaraki and malczewski 2015a waste management tavares et al 2011 demesouka et al 2013 gbanie et al 2013 and pollution analysis giupponi et al 1999 zhang and huang 2011 the approaches combine a variety of gis and mcda components using different models and platforms e g desktop web and mobile to conduct eadm analyses a number of mechanisms for integrating gis and mcda capabilities in eadm analysis applications have been suggested goodchild et al 1992 lidouh 2013 malczewski and rinner 2015 in general two strategies can be distinguished tight and loose coupling jankowski 1995 malczewski and rinner 2015 the tight coupling means that all components mcda and gis modules are integrated into a monolithic system this strategy integrates mcda capabilities into gis using a shared database and a common user interface the same data source and a single interface is used by both tools the loose coupling approach facilitates the integration of gis and mcda systems using a file exchange mechanism such that a system uses environmental data from the other system as the input data there are however some limitations to the integration of gis and mcda capabilities using tight and loose coupling strategies for environmental assessment the main problem with using tight coupling strategy is that it requires a high level of knowledge of gis mcda and considerable programming efforts to integrate gis and mcda components into a single environmental analysis system in addition due to the monolithic and application specific nature of these strategies they establish a single closed and non interoperable architecture that support only one or a few certain environmental analysis applications application specific the use of loose coupling strategy can reduce these problems by using modular gis and mcda components this strategy allows for having gis and mcda components talk to each other simply by the interchange of files that each can read malczewski 1999 suggests that the integration of gis and mcda should move from the current paradigm in which specific gis and or mcda applications are loosely tightly coupled to their internal data models and structures to interoperable gis mcda platforms according to denzer et al 2005 the presence of an interoperable network of geographic information gi and gis in the environmental domain is going to become more and more important as organizations at different levels are aiming at defining common frameworks and rules for environmental protection in an interoperable gis mcda system gis and mcda tools are able to interoperate and exchange relevant data for tackling a spatial decision problem ideally interoperable gis mcda systems allow users to integrate any of modular spatial analysis and multi criteria functionalities ad hoc at any given time during an eadm analysis process in other words interoperability provides a dynamic integration of gis and mcda tools which allows for a flexible moving of information back and forth between the gis and mcda modules according to the eadm analysis needs however the interoperability of the modular gis and mcda components has been hampered by the heterogeneities that exist between the components the heterogeneities between the two systems can occur at different levels including syntactic and semantic levels syntactic heterogeneity is the difference in data formats and syntaxes such as the difference in file formats used by gis and mcda the ability of gis and mcda tools to exchange information and to use the information that has been exchanged depends on data interchange standards and formats the interoperability of gis and mcda is concerned not just with the packaging of data syntax but with the ability to semantically interpret and understand the meaning of environmental data that is being exchanged beyond the ability of gis and mcda to syntactically exchange data they also need the ability to automatically and meaningfully interpret the data exchanged in order to produce useful eadm results as defined by the two systems however the problem of semantic heterogeneity caused by the differences in the meanings of data terminologies and models used in gis and mcda has been recognized as a substantial obstacle for achieving such a level of interoperability in a note malczewski 2010 argues that the hybrid heritage of gis mcda brings about as a series of terminological contradictions and inconsistencies that cause major challenges in advancing research on interoperability between gis and mcda typically the semantics of environmental procedures and data in both mcda and gis tools are rarely explicit they are in the form of informal and implicit agreements among developers and users in other words the assumptions on the meaning of gis and mcda concepts are implicitly embedded in the tools conventionally decision makers need to mediate the meanings semantics between gis and mcda elements based on their knowledge and then integrate the gis and corresponding mcda elements in most cases they need to manually transfer data derived from one system to another due to the heterogeneous nature such as data and operations heterogeneity of mcda and gis for example when a user imports a file e g criteria weights for a specific eadm problem from mcda software into gis software it specifically requires the user to provide interpretations of the terms used in mcda software and the terms used within the gis software this approach is inefficient and error prone since it requires the user to have knowledge about the underlying models and assumptions in both systems peachavanish et al 2006 without semantic interoperability a user who is only expert in gis not mcda performing environmental analysis tasks that require mcda knowledge needs to consult a mcda analyst to appropriately perform the mcda task in other words the burden of semantic interpretations and mediations between the two domains of gis and mcda is on the shoulders of users the problem of semantic interoperability calls for an ontology based approach for integrating gis and mcda malczewski 2010 malczewski and jelokhani niaraki 2012 recently there have been a number of studies using ontologies in spatial decision support systems e g sadeghi niaraki and kim 2009 blomqvist 2012 jelokhani niaraki and malczewski 2012b li et al 2012 effati and sadeghi niaraki 2015 hasani et al 2015 however to our knowledge none of the studies have specifically focused on the semantic interoperability of gis and mcda therefore the objective of this paper is to adopt an ontological framework to facilitate the semantic interoperability of gis and mcda components e g web services agents for eadm 2 semantic interoperability of gis and mcda concept standards and tools 2 1 concept the integration of gis and mcda components are two classes of computer systems for which the concept of interoperability has a great potential in this context interoperability implies the sharing or exchanging of data and information between gis and mcda systems where the data and information residing in gis may be used effectively by mcda and vice versa the aim is to allow the seamless interaction of gis and mcda systems that were not initially developed for this purpose fig 1 a and b illustrate the concept of interoperability between gis and mcda components using two different scenarios these scenarios present the interoperability depending on the way in which a spatial decision analysis task mostly involves the use of gis or mcda components gis dominant vs mcda dominant in the first scenario user mainly adopts gis components to perform the decision analysis task the gis components create feasible alternative locations determine spatial criteria values perform standardization function for normalizing criteria maps and carry out an overlay analysis to combine criterion maps for computing the overall scores or rankings of alternatives in this case the gis components are the core and mostly run the spatial decision analysis processes and merely interoperate with a mcda component to request the numerical weights computed based on the user inputs see fig 1a contrary to the first scenario in which decision tasks are mostly performed by gis functionalities in the second scenario the mcda components tend to be used more for solving the decision problem in this scenario the user mainly adopts the mcda components to structure the decision hierarchy e g analytic hierarchy process convert the user preferences to numerical weights integrate the weights and criteria values for computing the overall scores or rankings of alternatives based on a particular mcda technique e g ordered weighted averaging and generate sensitivity analysis graphs see fig 1b these components interoperate with the gis components to only request the spatial criterion values to compute the overall scores and return the evaluation results to gis part for representing on the map the interoperability of gis and mcda components can mainly occur at the syntactic and semantic levels syntactic interoperability means that both the gis and mcda components have agreed upon syntax and data format for the exchange of decision information whereas syntactic interoperability focuses on the syntax or structure of data semantic interoperability enables the automatic recognition of the data exchanged semantic interoperability is the ability of gis and mcda components to transmit data with intended and unambiguous meanings while preserving source and receiver software i e gis and mcda autonomy 2 2 standards interoperability of gis and mcda systems requires the development and use of uniform standards that provide the bridge language needed for the exchange of information with the objective of improving interoperability at the syntactic and semantic levels communities in the gis and mcda fields have been developing standards 2 2 1 gis standards in the gis community ogc open geospatial consortium has developed extensive standards for geospatial applications formally providing the terms definitions and information models in the gis domain see ogc 2014 ogc is the international organization dedicated to develop geospatial implementation standards consistent with the standards of international organization for standardization iso the federal geographic data committee fgdc national spatial data infrastructure nsdi spatial data transfer standard sdts and international committee for information technology standards incits digital geographic information working group dgiwg and other abstract or content standards see albrecht 1999 open and interoperable geoprocessing services or the ability to share heterogeneous geodata and geo processing resources is the main goal of ogc the ogc aims at achieving its task of geographic standardization through specifications e g abstract and implementation specifications these specifications have been developed in the geospatial community to address interoperability challenges 2 2 2 mcda standards major efforts in the standardization of concepts within the mcda community are by decision deck ewg mcda euro working group on multicriteria decision aiding and sds spatial decision support consortiums decision deck a consortium of european researchers has defined data standards in the mcda domain called umcda ml universal multi criteria decision analysis modeling language and xmcda a standardized xml based mcda standards which can be adopted by various mcda applications to make them interoperable meyer and bigaret 2012 umcda ml is one of the scientific initiatives inside decision deck to support a universal modeling language to express mcda concepts and generic decision aid processes xmcda is an instance of umcda ml which provides a standardized xml specification to represent objects and data structures in the field of mcda the goals of xmcda standards are to provide a common and unified data structure in mcda resolve the heterogeneity of the available mcda tools ease the interoperability of different mcda web services and tools using a common terminology chaining of various mcda algorithms and web services and the visual representation of mcda concepts and data structures via standard tools meyer and bigaret 2012 cailloux et al 2013 ewg mcda is a working group whose objective is to promote original research and contribute to the development in the field of mcda in a newsletter by ewg mcda roy 2000 has published a standard decision aiding glossary paoli et al 2011 suggest that the terms represented in this glossary can be used for the construction of a standard mcda ontology in another attempt sds spatial decision support consortium a network of professionals involved in spatial decision applications has developed a sds knowledge portal as a source of information and a common vocabulary ontology for sds researchers and practitioners see sds consortium 2014 this portal partially covers the essential concepts in mcda including terms methods techniques tools and models data sources etc as a part of the broad body of knowledge in the sds field 2 3 tools web services and ontology 2 3 1 web services currently there are many interoperable gis and mcda web services freely available for use by decision makers to perform gis and mcda functionalities respectively particularly the ogc standards compliant services such as web feature service wfs web map service wms web coverage service wcs and web processing service wps see ogc 2014 are being used in different gis applications similarly the xmcda based web services by decision deck consortium allow an easy access to mcda tools decision deck offers an architecture dedicated to web services whose primary goal is to offer a place where mcda computational resources methods etc can be hosted and made freely available to anyone for details see http www decision deck org ws webservices html from an algorithmic point of view the xmcda web services conduct elementary mcda processes which if properly chained can rebuild a mcda application cailloux et al 2013 xmcda web services are able to interoperate based on xmcda standards ros 2011 meyer and bigaret 2012 with the availability of distributed gis and or mcda web services the web services can be used and chained to solve the decision problem at hand 2 3 2 ontology despite many efforts to address issues of standards in gis and mcda communities interoperability especially with respect to semantics amongst gis and mcda components is scant semantic interoperability all relies on a common understanding and agreements about the meanings of concepts in the two domains it is achieved only when components agree on the meaning of the data they exchange ontologies are considered as an enabling technology for the semantic web and methods and tools for ontology definition are being studied for interoperability an ontology can facilitate semantic interoperability between the gis and mcda components through providing a formal semantic and explicit specification of a shared understanding and conceptualization in the two domains gruber 1993 guarino et al 2009 3 ontological framework for semantic interoperability of gis and mcda the methodology used in this study involves developing an interoperable environmental gis mcda framework based on a set of ontologies and gis mcda web services see fig 2 the ontologies include gis and mcda domain ontologies gis and mcda task ontologies gis data ontology and gis mcda mapping ontology the gis and mcda domain ontologies organize and represent concepts that are used for the purpose of data retrieval and analysis in gis and mcda context respectively gis and mcda task ontologies contain knowledge about the usage of gis and mcda domain ontologies respectively this framework allows users to define the gis mcda elements e g spatial constraint and property analyses decision rule etc of a particular eadm analysis these gis mcda elements can be defined by choosing a pre defined gis or mcda task template from gis mcda task ontologies the information needed to support spatial multicriteria decision making for the application at hand requires geographical data typically geographic data are used to support decision makers in determining the set of geographic alternatives constraints and criteria associated with the eadm analysis at hand to this end the data ontology organizes information relevant to the application the system automatically translates and decomposes the user specified gis and mcda tasks into a set of relevant atomic operations based on the task workflow knowledge represented by swrl rules in the gis and mcda task ontologies respectively it can use the appropriate web services corresponding to each of the operations the individual services of each domain e g gis can be used in the order represented by swrl rules and then chained either manually by users or automatically next the individual services e g gis and or mcda services can semantically exchange their output data i e messages with each other based on the terminology used in the domain ontologies e g gis and or mcda domain ontology in other words for gis and or mcda web services to be able to semantically communicate input and output data of services must be committed to gis mcda domain ontologies after the completion of for example a particular gis task using a set of gis web services source services the outputs of the gis services need to be exchanged with the other party i e mcda web services target services the mediator web service uses the mapping ontology to mediate map the meaning of outputs produced by gis services and inputs required by mcda services for appropriately transferring the data between them 3 1 domain ontologies 3 1 1 gis domain ontology it is required to provide formal representation of concepts that are used for the purpose of environmental data retrieval and analysis in the gis domain the gis domain ontology should describe the vocabulary related to gis domain the concepts used in the gis domain ontology can be mainly derived from previous studies e g kolas et al 2005 budak arpinar et al 2006 peachavanish et al 2006 hudelot et al 2008 bittner et al 2009 batsakis and petrakis 2010 fonseca 2001 frank 2003 kavouras and kokla 2007 top level ontologies and ogc specifications or iso standards see fig 3 for instance battle and kolas 2012 suggest that according to the ogc geosparql standard a basic gis ontology includes feature a thing that can have a spatial location geometry a representation of a spatial location and spatial object a superclass of both features and geometries the property hasgeometry links features a thing to their geometry their location jung et al 2013 suggest that this basic ontology can be extended based on the ogc specifications for example the ogc abstract specification topic 5 feature stipulates that a geospatial feature should contain a particular type of geometry e g point polyline or polygon with an srs spatial reference system and attribute in addition to above concepts the gis domain ontology should include the notion of role for spatial features objects see peachavanish et al 2006 fonseca et al 2002 the notion of roles can be used for representing whether a feature plays the role of being source or target of a spatial property see fig 4 typically external spatial properties including mereological topological and metric relations describe relations between two objects one of which plays the role of a source object and the other one plays the role of a target object the notion of roles is important in the gis domain ontology because they convey a specific intent user intentions on the information required in other words spatial property analyses generate different outputs information depending on which of the two features is source or target for example the outputs of spatial property analysis find average distance between a parking candidate site and airport stations that considers parking candidate site as source object and airport stations as target objects differs from that for find average distance between an airport station and parking candidate sites that considers airport station as source object and parking candidate sites as target objects 3 1 2 mcda domain ontology the mcda domain ontology organizes and represents concepts that are used for the purpose of environmental data retrieval and analysis in mcda see mahmoudi and muller schloer 2009 sadeghi niaraki and kim 2009 malczewski and jelokhani niaraki 2012 šaša bastinos and krisper 2013 this ontology can also be derived from standardized sources such as xmcda standards by decision deck the mcda vocabulary by sds consortium and french english decision aiding glossary by ewg community see fig 5 it structures and encodes the knowledge terms concepts and properties of mcda data models such as decision goal decision alternatives decision constraints evaluation criteria objectives and attributes criteria weights decision matrix alternative evaluation outcomes etc see fig 5 contrary to gis domain that there are already many researches focused on representing gis concepts using ontologies there have been little research efforts focusing on representing mcda components into an ontological structure in the 5th and 8th workshops by decision deck consortium the construction of a global mcda ontology has been proposed paoli et al 2011 3 2 task ontologies 3 2 1 gis task ontology gis task ontology formulates the spatial analysis steps and their sequences for performing eadm analysis tasks in terms of the terminology defined in the gis domain ontology see fig 6 e g sadeghi niaraki and kim 2009 jelokhani niaraki and malczewski 2012a timpf 2001 argues that giscience needs task ontologies in addition to domain ontologies to enable knowledge sharing semantic interoperability and web service reuse for interoperable gis in previous studies ontologies have been identified as ways to standardize and conceptualize spatial analysis steps as the workflows of geospatial analysis tasks e g peachavanish et al 2006 yue et al 2009 accordingly in this study the gis task ontology captures the workflow knowledge for solving gis analysis tasks where the required gis functions gis web services and their sequences for a variety of gis tasks are presented as the workflows in the ontology the web services that perform gis functions are considered as individuals in the ontology table 1 shows examples of gis analysis tasks their types and required gis functions for instance the gis analysis task determine average distance between source and target features within a radius from source features is a type of spatial property analysis and can be semantically modeled using the classes of gis functions e g buffer overlay and distance with the gis web services as the individuals of these classes see also fig 7 describing gis workflow knowledge of eadm analyses in the ontology follows the approach proposed by jung et al 2013 who use swrl rules to describe the required gis functions in the workflows by using swrl one can represent knowledge on what gis functions and their sequences are needed for a particular task swrl rules can be expressed in terms of ontology classes properties individuals and data values to present the workflows for gis analysis tasks the right hand side of the rules represent a generic gis task class containing specific gis tasks as instances and the left hand side of the rule represents the relevant gis function classes their instances and sequences to perform the specific gis task tables 2 and 3 show examples of swrl rules for gis analysis tasks including spatial property and constraint analysis tasks respectively for instance rule 1 in the table delineates the spatial property analysis task class determine distance between source and target features requires the gis function classes of buffer overlay and distance using the hasgisoperation property with its sequences through the hasnextgisoperation property the individuals i e task instances belonging to this task class has the corresponding gis functions belonging to the classes of buffer overlay and distance for example the instances of this task class including determine nearest distance between source and target features within a radius from source features determine average distance between source and target features between two radiuses from source features and determine nearest distance between source and target features outside a given radius from source features require different sets of the gis functions gis web services of buffer full side overlay intersect and distance farthest buffer ring overlay intersect and distance average and buffer full side overlay clip and distance nearest as instances of classes buffer overlay and distance respectively 3 2 2 mcda task ontology similar to the gis task ontology mcda task ontology formulates the mcda steps and their sequences for eadm analysis tasks i e the computational and operational elements of mcda see fig 8 sadeghi niaraki and kim 2009 jelokhani niaraki and malczewski 2012a specifically the mcda task ontology used in this study structures the mcda workflow knowledge of eadm analyses by combining various elementary calculation mcda components see fig 9 instead of considering only monolithic mcda models or software tools the xmcda standard aims to enable algorithmic mcda workflows that is the design of components e g xmcda web services performing independent computational steps used in mcda methods meyer and bigaret 2012 cailloux et al 2013 meyer and bigaret 2012 suggest that the mcda methods should not be used as static and immutable black boxes but rather as dynamic workflows which can be changed and adapted for the current purpose table 4 shows an example swrl rule for the mcda task of evaluate alternatives using a decision rule for instance this mcda task class requires the classes of mcda functions of preference elicitation preference to weight conversion decision rule and evaluate using the hasmcdaoperation property with its sequences through the hasnextmcdaoperation property the individuals i e task instances belonging to the mcda task class has the corresponding mcda functions i e mcda web services belonging to the above classes for example the instances of the task class evaluate alternatives using a decision rule including find rankings of alternatives in descending order using an ahp owa and find scores of alternatives using an rank order wlc require different sets of the mcda functions or web services of pair wise comparison ahp owa and ranking in descending order and rank order rank sum rs wlc and score as instances of classes preference elicitation preference to weight conversion decision rule and evaluate respectively 3 3 mapping ontology the semantic interoperability between the gis and mcda components for a particular eadm analysis requires mediation or mapping between the concepts in the two gis and mcda domain ontologies in this study a semantic mapping bridge ontology has been used to meaningfully relate the concepts in the two ontologies arch int arch int 2013 developing such a mapping ontology requires knowledge from both gis and mcda domain experts in order to make the correct semantic correspondences and specify all mappings between the gis and mcda concepts table 5 shows an example of mcda terminology and the corresponding vocabulary used in gis as well as their semantic relatedness for example the mcda term alternative and the gis term source feature object can be related in such a way that an alternative entity can be instantiated as a source feature instantiation of an alternative as source feature means that an alternative represents a particular class of spatial features in the decision space the term weight in mcda is semantically different from the spatial weight in gis where it is defined as the relative importance of the criteria objectives attributes in the mcda terminology while in the gis terminology weight may be conceived as a file containing the spatial relationships between spatial entities thus they are two different non overlapping concepts fig 10 demonstrates the process of semantic mapping using an example for example the terms source feature spatial constraint spatial property linear unit in the gis ontology can be related with the terms alternative constraint attribute and ratio scale in the mcda ontology using the semantic relations of is instance of is is kind of and is a respectively this process allows the instances of the gis ontology classes to be appropriately transformed as the data instances of the mcda ontology classes 3 4 gis data ontology gis data ontology at the lowest level of ontological hierarchy is built as a means to better organize data relevant to eadm analyses enable ontology assisted queries based on the semantic relationships and assist users and or search engines in semantically retrieving relevant information sadeghi niaraki and kim 2009 the concept feature presents an abstract top level concept in the ontology depending on the application at hand e g landfill site selection the concept is further broken into a number of sub concepts which represent general urban classes such as transportation waste type landfill etc see fig 11 the gis data ontology is connected to the gis domain ontology by making the top class feature in the data ontology a subclass of feature in the gis domain ontology meaning that instances of the class can point to src geometries attributes etc with the has src has geometry and has attribute properties see battle and kolas 2012 4 prototype implementation a prototype gis mcda system was developed to demonstrate the semantic interoperability of gis and mcda components in an eadm analysis context the prototype involves a set of gis and mcda web services ogc and xmcda standard compliant web services interoperating to solve a landfill site selection suitability ranking as one of the eadm analysis problems see fig 12 both the ontologies and swrl rules have been formalized and implemented using owl ontology web language in protégé environment fig 13 shows the gui of gis mcda application for solving the landfill site suitability problem the mcda elements associated with landfill site selection including the spatial constraints objectives attributes etc were specified through the gui the constraints were defined as the sub classes of gis domain ontology based on the appropriate constraint task templates which are linked to the corresponding constraint analysis tasks in the gis task ontology for example defining the constraint a candidate landfill site must be within 100 m of residential area requires choosing the template a source feature must be within a given distance of target features which is in turn linked to the constraint analysis task determine if source features are within a given distance of target features when users choose the constraint task template a dialog window specific to that template appears and allows for specifying landfill as source feature i e alternative residential area as target feature and the value 100 m to derive the feasible source features that are located within a given distance of target features see fig 14 the system allows users to access the names of source i e alternatives and target features in the dialog window through loading from the gis data ontology in this prototype the data ontology is linked to an oracle spatial database containing gis data associated with the landfill site selection including the data related with the specified source i e alternative in mcda domain and target features the spatial data are stored in the oracle database based on the ogc standards the use of oracle spatial allows for performing ontology assisted queries based on the semantic relationship between the data it enables to use semantic operators to meaningfully search source and target features specified in the dialog window referring to the gis data ontology let s say the user specifies for example transportation station as the target feature in the dialog window the user needs to consider the transportation station of any types as target feature in the decision analysis however a typical query of the gis database involving syntactic match will not return any rows because no table rows contain the exact value transportation station for example the following query will not return any rows select type from feature where station type transportation station however many rows in the gis database are relevant because they are a type of transportation station for instance the features such as bus stops terminals etc are all types of transportation stations the sem related operator in oracle spatial instead of lexical equality enables to semantically retrieve all the relevant rows from the gis database with the aid of gis data ontology sem related operator is able to infer over the gis data ontology and find all transportation stations including subway stations bus stops airports terminals and railway stations as these features have been semantically defined as the sub or equivalent classes of transportation station in the ontology the objective and attribute concepts can be similarly created through gui each of the objectives is derived from attributes for example the objective maximize accessibility could be derived from two attributes nearest distance between landfill candidate sites and residential areas and average distance between landfill candidate sites and residential areas similar to constraints attributes can be constructed in terms of spatial property templates which are linked to the corresponding spatial property analysis tasks in gis task ontology for instance attributes nearest distance between the landfill candidate sites and residential areas can be defined based on the template nearest distance between source and target features within a radius from source features which requires the task determine nearest distance between source and target features within a radius from source features see fig 15 similar to gis analysis tasks i e spatial constraint and property tasks users can choose their appropriate mcda tasks such as evaluate the alternatives using a decision rule for eadm analyses for example the user can select determine scores of alternatives using rank order wlc or determine rankings of alternatives in a descending order using ahp owa as one of the instances of mcda task class evaluate the alternatives using a decision rule if users selects determine scores of alternatives using rank order wlc the inference engine identifies the corresponding mcda operations i e mcda web services based on the swrl rule in mcda task ontology once the problem specific alternatives source features constrains criteria i e objectives and attributes preferences mcda task etc have been specified the system invokes the appropriate web services to perform the specified tasks specifically the gis web services can be invoked according to spatial constraint and property templates tasks defined in the gis task ontology to compute the constraint values feasible candidate sites and spatial property values specified similarly mcda services could be invoked to perform the mcda operations including preference elicitation preference conversion to weight and decision analysis rule methods the task workflow knowledge represented by swrl rules in the gis and or mcda task ontologies guide how to combine the relevant atomic web services they describe dependencies regarding the execution order of web services depending on the task the inference engine appropriately identifies the web services using the swrl rules composition of the atomic gis and or mcda web services is based on the sequences of peer to peer messages exchanged among them for gis and or mcda web services to engage in knowledge level communication they must be able to i parse the messages at the syntactic level i e services should parse or decode messages to its parts such as message content language sender etc and ii understand the messages at the semantic level the parsed symbols knowledge terms must be understood in the same way the content of message is unambiguously defined where what is sent is the same as what is understood after the completion of a particular gis task using the set of gis web services source services the outputs of the task need to be exchanged with the other party i e mcda web services target services fig 16 shows a simple example of mapping procedure used to transform the data instances from gis to mcda domain and vice versa first the mediator web service receives the message from the source web service and parse it at both syntactic and semantic levels to extract the semantic data including instances classes properties and property values next the mediator web service reasons if there is a corresponding class property for the extracted classes properties in the mapping ontology for example using reasoning over mapping ontology and extracted semantic data messages received from source services it is possible to understand and infer that if a source feature s is instance of the class of source feature and this class is equivalent with the class of alternative then s is instance of the class alternative after finding the corresponding classes mediator web service transfers the data to the classes and creates a message based on them for sending to the target web service let s illustrate the exchange of messages between gis and mcda web services using a scenario according to the scenario the gis service determines the feasibility of alternatives e g the constraint value of 1 for the source feature e g alternative landfill site a1 and compute the spatial property value i e nearest distance between landfill candidate sites and streets e g 100 m for the feasible source feature the service sends the source feature constraint and spatial property outputs through the soap messages to the mediator web service for instance see fig 17 the mediator web service translates these gis data outputs to the corresponding mcda elements based on the semantic mapping ontology and in turn sends them to the mcda web service 5 discussion this work is considered as an initial step toward a semantic interoperability of gis and mcda for eadm analyses however we consider several aspects to be improved as future lines of work the lack of adequate standards or ontologies can be considered as the major obstacle in making progress with respect to building an interoperable gis mcda environment it has been recognized that the key to the success of ontology based gis mcda systems for the purpose of interoperability is a set of agreed upon standards at different levels including data operations and terminology at the moment the main problem is that there does not exist unified domain and task ontologies in the communities especially in mcda community although in recent years ontologies have been widely used and become one of the top areas of interest in gis community it has been only recently that some considerations have been given to the use of ontologies in mcda community further efforts should be made in mcda community to appropriately develop ontologies as well as standard specifications for mcda elements moreover an effective interoperability between gis and mcda systems requires that experts of the two communities agree on the terminologies and precisely define mapping between gis and mcda elements in the semantic mapping ontology extensive efforts have been made in gis community for standardizing catalogue services e g opengis catalogue service implementation specification ogc catalogue services standard as well as developing interoperable services most notably the ogc standards compliant services such as web feature service wfs web map service wms web coverage service wcs and web processing service wps see ogc 2014 however the efforts to develop interoperable mcda services and standardized registry catalogues for mcda services has been rather limited this paper mainly focused on semantic interoperability integration between gis and mcda but does not consider the advanced matter of automatic discovery of gis and or mcda services the study used swrl rules to represent the gis and mcda task knowledge for chaining the web services future study could adopt reasoning algorithms to semantically discover the required conceptual workflows and gis services for performing the gis and mcda tasks and facilitate the web service composition process while the present study used a couple of gis and mcda tasks in the prototype system to illustrate the potential of proposed approach for semantic interoperability of gis and mcda future research might consider using additional and more complex tasks 6 conclusion this paper has presented an ontology based approach for semantic interoperability of gis and mcda components for eadm problems our basic premise is to provide a semantic interoperable gis mcda framework where the set of existing gis and mcda web services could interoperate exchange their data and meaningfully interpret and understand the meaning of data that is being exchanged for the eadm purposes the proposed approach involves the use of a set of ontologies to address the challenges of semantic interoperability between gis and mcda the ontologies include gis and mcda domain ontologies gis and mcda task ontologies gis mcda mapping ontology and gis data ontology the ontologies serve as the foundation on which a semantic interoperable gis mcda framework is built specifically they serves as i a knowledge base containing all necessary information to be used manipulated reasoned and processed by the web services ii a semantic knowledge model upon which the gis and or mcda web services can effectively communicate and exchange messages this enables the services to understand messages from other services and semantically exchange and interpret their knowledge iii a semantic mediator to mediate the meaning of terms between gis and mcda and iv a gis mcda workflow knowledge base that provides a service chaining knowledge for decision making a prototype implementation of eadm analysis was developed based on the ontologies to demonstrate the semantic interoperability of gis and mcda components the prototype involves a set of gis and mcda web services semantically interoperating to solve a landfill site suitability problem the gis and mcda web services could semantically interact and interoperate with each other by means of ontologies consequently the study facilitates syntactic and semantic interoperability between the two different set of gis and mcda web services for solving environmental problems this allows users to integrate any of modular spatial analysis and multi criteria tools i e web services ad hoc at any given time during a particular eadm process it opens up the opportunities for advancing research on integrating gis and mcda tools in environmental decision making processes specifically it could shift the paradigm of environmental decision analyses from application specific and monolithic to a semantic interoperable system acknowledgment this research was supported in part by korean msit ministry of science and ict under the itrc support program iitp 2017 2016 0 00312 supervised by the iitp and in part by the nrf grant 2014r1a2a1a11053135 
26453,farming systems are complex and have several dimensions that interact in a dynamic and continuous manner depending on farmers management strategies this complexity peaks in indian semi arid regions where small farms encounter a highly competitive environment for markets and resources especially unreliable access to water from rainfall and irrigation namaste a dynamic computer model for water management at the farm level was developed to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies the most relevant and novel aspects are i system based representation of farming systems ii description of dynamic processes via management flexibility and adaptation iii representation of farmers decision making processes at multiple temporal and spatial scales iv management of shared resources namaste s ability to simulate farmers adaptive decision making processes is illustrated by simulating a virtual indian village composed of two virtual farms with access to groundwater keywords farmer s decision making process adaptation modeling climate change water management policy 1 introduction agriculture faces many challenges regarding its productivity revenue and environmental and health impacts challenges that must be considered within the known context of climate change agriculture also faces demands to increase the quantity quality accessibility and availability of production to secure food production and improve product quality to address needs of the world s growing population meynard et al 2012 hertel 2015 mckenzie and williams 2015 agricultural productivity must increase within a framework of environmental and health concerns to do so agriculture should decrease its environmental impacts on water air soil and aquatic environments and consider the scarcity of resources such as water phosphorus and fossil fuels especially for production of nitrogen fertilizers fao 2011 brown et al 2015 under climate change warmer temperatures changes in rainfall patterns and increased frequency of extreme weather are expected to occur consequently it has direct biophysical effects on agricultural production and can negatively affect crop yields and livestock nelson et al 2014 rising sea level will increase risks of flooding of agricultural land in coastal regions while changes in rainfall patterns could increase growth of weeds pests and diseases de lapeyre de bellaire et al 2016 on the deccan plateau in india the countryside has witnessed the proliferation of individual electrical pump driven borewells that extract water from underground aquifers sekhar et al 2006 javeed et al 2009 the low productivity of the aquifer dewandel et al 2010 perrin et al 2011 and a rapid decline in the water table level has decreased borewell yields ruiz et al 2015 indicating that groundwater irrigated agriculture still largely depends on rainfall for a region that depends largely on monsoon and winter rainfall to maintain agricultural production any shift in climate would have a severe impact on natural resources and the economy drilling borewells to gain control over water access is crucial to maintain household sustainability however it also entails the risk of failed borewells and intractable debts taylor 2013 modeling and quantifying spatio temporal variability in water resources and interactions among groundwater agricultural practices and crop growth is an essential component of integrated and comprehensive water resource management ruiz et al 2015 simulating scenarios of climate change and water management policies is an essential tool to identify mechanisms that farmers can use and policies that can be implemented to address these challenges white et al 2015 in these modeling and simulation approaches farmers decision making processes should be considered to assess how agricultural production systems change and adapt to external changes and opportunities farm management requires farmers to make a set of interconnected and successive decisions over time and at multiple spatial scales risbey et al 1999 le gal et al 2011 in the long term farmers decide on possible investments and marketing strategies to select or adapt to best fulfill their objectives decisions about cropping systems also impact the farm decisions about crop rotation and allocation are considered at the whole farm level detlefsen and jensen 2007 castellazzi et al 2008 dury et al 2010 and can be investigated in the long term and or adapted for shorter periods once a crop is chosen farmers must make intra annual decisions to choose crop management techniques and the varieties to sow in the coming year this decision can be made before cultivation and adapted if necessary tactical decisions generally this decision concerns the whole farm to ensure that practices are consistent or to maintain a minimum of crop diversity on the farm however tactical decision descriptions are not sufficient to trigger daily operational management therefore farmers must define specific ways to execute their tactical plans farmers decide on crop operations and resource management and even change the purpose of their crops when conditions are not conducive to the initial plan a farm decision making model should include sequential aspects of the decision making process and farmers abilities to adapt and react akplogan 2013 according to a review of modeling adaptive processes in farmers decision making robert et al 2016a 70 of the articles reviewed focused on only one stage of the decision adaptation at the strategic level for the entire farm or at the tactical level for the farm or plot we suggest reconsidering farm management as a decision making process in which decisions and adaptations are made continuously and sequentially over time the 3d approach strategic decisions tactical decisions operational decisions to simulate reality more closely these considerations prompted the development of a simulation model able to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies this article presents this farming system model and an example of its application to a semi arid region in karnataka state southwestern india we first introduce the conceptual model and the modeling and simulation platform we then describe the model namaste in detail and illustrate its capabilities by applying it to a case study in southern india finally we discuss the key modeling choices and present several insights on how to upscale the model from the farm level to watershed regional and national levels 2 materials and methods 2 1 conceptual modeling we divided the systemic representation of the farming system into three interactive systems i decision system which describes farmers continuous and sequential decision process ii operating system technical system which translates decisions ordered by the decision system into instructions to execute tasks which is an action to perform on a biophysical object or location e g sowing operation and iii biophysical system which describes crop and soil dynamics and their interactions especially relations between groundwater soil and plant development using a crop model clouaire and rellier 2009 le gal et al 2010 dury 2011 akplogan 2013 robert et al 2016b fig 1 for the decision model we consider farmers as cognitive agents able to think memorize analyze predict and learn to manage future events and plan their actions le bars et al 2005 in artificial intelligence and cognitive sciences agents are commonly represented as belief desire intention bdi agents bratman 1987 rao and georgeff 1991 the bdi framework is founded on the well known theory of rational action in humans bdi agents are considered to have an incomplete view of their environment simon 1950 cyert and march 1963 the concept of belief represents a farmer s knowledge of the farming system and its environment desires are a farmer s objectives goals that meet production or management goals intentions are action plans that achieve a farmer s objectives desires farmers are represented as bdi agents at several levels of the conceptual model of the farming system farmers beliefs and desires are the basis of the production processes in the farming systems farmers manage their farms based on their knowledge and objectives farmers have different types of knowledge about their farms structural i e farming structure and organization procedural i e know how of farming practices and observable i e observations about their environment observing social and economic environments is important to be able to quickly respond to changes and uncertainties in the production context the climate prices of crops and inputs and availability of external resources such as groundwater labor or shared equipment are common uncontrollable data farmers use to make decisions they also adapt their practices based on recent outputs of production systems such as yields decision models provide the plans that farmers will execute in their production systems based on their observations and objectives which translates into actions invest perform a crop operation etc that correspond to intentions of the bdi agent contrary to these actions which are direct outputs from the farmer decision making model other outputs are consequences of these actions on the biophysical system such as impact on groundwater water consumption due to the volume of water pumped and drainage due to the natural return of excess water from rainfall and irrigation for more details see robert et al 2016b 2 2 record a modeling and simulation computer platform 2 2 1 overview the record platform is a modeling and simulation computer platform devoted to the study of agro ecosystems bergez et al 2013 record facilitates design of simple single atomic or hierarchical complex coupled models and enables using different temporal and spatial scales within models it is based on the virtual laboratory environment vle a free and open source multi modeling and simulation platform based on the discrete event system specification devs formalism that derives from the theory defined by zeigler et al 2000 on modeling and simulation for dynamic systems with discrete events vle provides a simulation engine modeling tools software libraries and an integrated development environment to the record platform specific extensions have been developed in record to bridge the gap between the generic vle and the framework adapted to the domain of agro ecosystems these extensions help modelers in developing their models in the formalism they are used or which is the most suitable by providing generic templates of codes and input parameters see bergez et al 2013 for an extended description of the platform in this study two vle extensions decision and differenceequation were used to build the model and a specific package rvle for the statistical software r r core team 2016 was used to help perform simulations 2 2 2 vle extension decision to represent farmers operational decision making processes the decision extension bergez et al 2016 implements the decision portion of the model in the decision system operating system biophysical system approach during simulations the decision model identifies the state of the environment e g weather plant soil resource availability and sends orders to the connected models e g a biophysical crop model according to a flexible work plan of activities i e the tasks to be achieved the work plan of activities contains the following i a knowledge base composed of information about the system used to reach a decision the information includes dynamics of biophysical processes availabilities of human and material resources and spatial information about farm structure they are organized as a set of variables whose values evolve during the simulation to update the knowledge base at each time step ii tasks to be executed and associated conditions predicates rules and time windows iii temporal relations between tasks for details of the decision extension see bergez et al 2016 recently resource constraints were added to the conditions associated to the tasks a set of available discrete resources is defined and structured by categories within the knowledge base resource constraints are defined for each task by a needed quantity possible alternatives and priorities during simulation the resource allocation is sequentially managed depending on resource availability and task priorities 2 2 3 vle extension differenceequation the vle extension differenceequation extension formulates time discrete models that calculate values of real variables at time t as a function of the value of variables in the system at time t δ t 2δ etc e g varx t 1 f varx t the expected parameters for an atomic model using differenceequation are i the simulation time step δt which must be the same for all equations ii the mode either name of all external variables and iii the state variables and there initial values 2 2 4 rvle a user friendly tool in the record platform rvle www vle project org is an r package that calls vle s application programming interface from r it can open packages and read model structure vpz files assign experimental conditions to models call the simulator build experimental frames and turn simulation results into an r object such as a matrix or dataframe it is especially useful for analyzing simulation results and performing statistics it also allows users to manipulate models from the r environment 3 description of the farming system model 3 1 models used to build the farming system model 3 1 1 the decision system 3 1 1 1 3d three integrated decision models the novelty of the decision system is that three integrated decision models were built to represent farmers strategic tactical and operational decisions and adaptations the strategic model simulates farmers strategic decisions which include decisions about investment and cropping systems the tactical model simulates farmers tactical decisions especially adaptation of cropping systems the operational model simulates farmers operational and crop management decisions we used three modeling formalisms to describe farmers decisions throughout the decision process decisions about investment and cropping systems that are influenced by economic return maximizing profit were expressed using dynamic stochastic programming decisions about establishing cropping systems that are influenced by motivations besides economic return e g proximity to a market equipment were implemented via a decision rule modeling approach using a specific descriptive language whose syntax is based on formal if then else rules written as a boolean condition if indicator operator threshold then action1 else action2 decisions about crop management were described as an activity graph supported by the decision extension in the record platform and using a knowledge base the knowledge base collects information that the farmer obtains from the biophysical subsystem when monitoring and observing the environment the activity graph represents the farmer s work plan and relies on the knowledge base to activate or disable technical operations an activity denotes a task rules control the start of the activity by checking whether conditions necessary to perform the operation exist for details of this formalism see bergez et al 2016 3 1 1 2 modeling resource management in the operational decision model from a modeling viewpoint two types of resources were distinguished in the farming system i conditional discrete and returnable resources which are necessary to execute an operation and can be used and then returned once the operation is finished e g labor tractor and ii unconditional and consumable resources which are not necessary to execute an operation and are consumed and not returned after use in an operation e g irrigation water these resources are managed differently in the model the operating system manages the unconditional and consumable resources for example following an order to execute irrigation the decision system returns each day the amount of water needed to irrigate the farm the operating system compares the water needed to the water available and executes the order to irrigate by transferring the lesser of the two values water needed or water available to the biophysical system conflicts between activities requiring the same conditional discrete and retrunables resources at the same time are dynamically managed using rules to allocate resources and determine the order in which activities are executed prioritization is managed by rules that temporally rank activities that can be executed simultaneously ranks can be reviewed and updated by other rules 3 1 2 the operating system the operating system translates decision orders into executable and timed actions it calculates the duration of each activity based on the quantity and type of resources that an operation uses and the speed with which each resource executes an operation entered in the experimental conditions of the simulation the operating system can also transform certain data transferred from the decision system so that data units correspond to those expected by the receiving model the operating system is implemented using difference equations in record 3 1 3 the biophysical model the stics model which represents the crop and soil system simulates dynamics of a crop soil system over one or more crop cycles at a daily time step brisson et al 1998 we selected stics for its adaptability to many crop types robustness in a wide range of soil and climate conditions and modularity brisson et al 2003 it has been successfully used in spatially explicit applications and coupled with hydrological models at the watershed level beaujouan et al 2001 stics receives the crop operations and parameters applied to the plot from the operating system which executes orders provided by the decision system stics returns information about crop stage yield soil characteristics water use and drainage the fortran code in stics was wrapped into an atomic model using difference equations in record bergez et al 2014 3 2 model structure the model is composed of one decision system and one operating system the latter of which interacts with one biophysical system fig 1 the biophysical system can be made up of several crop models for example an independent stics model represents the crop and soil of each plot of the modeled farm for each plot the decision system has a work plan with specific activities listed for the plot s crop several work plans may run in parallel when the modeled farm has several plots the resource manager must manage conditional discrete and returnable resources both among activities in a given work plan and among work plans in this case prioritization is used to rank all activities temporally unconditional and consumable resources are distributed by an intermediary model that for example allocates irrigation water to plots when the decision system sends several irrigation orders on the same day available water is distributed according to priorities assigned to work plans to simulate the cropping system over several years several work plans for the same plot must be run sequentially the next one is loaded when the last activity of a given work plan is completed 3 3 dynamic functioning in systemic modeling i e modeling a complex system as sub systems interacting with each other models must be able to interact with each other to provide feedback and other types of interactions two types of variables are identified in models state variables which are managed by the model itself and external variables which are managed by other models external events e g rainfall market prices electricity availability and availability of resources e g labor equipment irrigation water are summarized in the inputs model fig 1 at the beginning of the year the strategic model receives information from inputs so that strategic decisions about investment and long term cropping systems are based on farmers knowledge and objectives these two types of strategic decisions are forwarded to the tactical model at the beginning of the cropping season the tactical model receives information from inputs so that farmers knowledge is updated this new information prompts the tactical model to update the cropping system and forward the adapted cropping system to the operational model at the operational level decisions are based on the appropriate agronomic context and the farmers updated knowledge once both types of information meet the requirements for executing an operation and are forwarded to the operational model an operation order is transferred to the operating system which translates it into a task execution so that the operation is performed in the plot one day after the decision is made at the end of the operation the crop model returns the agronomic context to the operational model after harvest the operational model informs the tactical model that the cropping season is over when the tactical model automatically wakes up on day 250 official end of the season it receives information about the successful harvest and updated knowledge about the system processes and updates the cropping system for the second season and forwards this updated cropping system to the operational model at the end of the second season the same process occurs and soon after the second year begins 4 application case the namaste simulation model namaste simulates farmers adaptations to uncertain events such as climate change water table depletion the economic environment and agricultural reforms we applied namaste to a case study located in karnataka state india in the berambadi watershed the cropping system is organized around three seasons i the rainy season kharif when most crops are grown ii the winter season rabi when mainly irrigated crops are grown and iii the dry season when little cultivation occurs summer monsoon rainfall is a key determinant of crop choice farmers make three types of decisions i whether to invest in an irrigation system ii crop selection and iii crop management and operations all farmers in the watershed pump irrigation water from the same aquifer and those from the same village share labor and equipment to illustrate namaste s ability to simulate farmers adaptive decision making processes under conditions of both limited and shared resources we ran a baseline scenario over a 10 year planning horizon in which the parameters that describe the climate crop market conditions prices and costs and water pumping conditions i e hours of electricity available each day cost of pumping were based on those obtained from farmer surveys 4 1 coupling the farming system model to the hydrological model rainfall market prices and availability of electricity labor equipment and irrigation water are inputs to the farming system they are modeled as subsystems of an external system that limits the farming system fig 2 4 1 1 hydrological subsystem ambhas tomer 2012 is a spatially explicit groundwater model that simulates dynamics of daily groundwater level based on equations from mcdonald and harbaugh 1988 it predicts daily groundwater level actual net recharge and discharge for a 1 ha cell the net recharge is calculated as the difference between the amount of water drained below the soil profile and the required water amount for crop irrigation predicted by stics as for the stics model a wrapper was used to integrate the ambhas code into record avoiding heavy recoding this wrapper was based on difference equations formalism 4 1 2 climate market and power supply subsystems the weather model simulates expected and actual rainfall each day the market model simulates expected and actual crop market prices the electricity model simulates the number of hours of electricity available each day these sub models are implemented in an atomic model using difference equations in record 4 2 namaste simulation when resources are shared interactions are important to an individual farmer s decision making process to integrate resource constraints into the farming system model our experimental model simulates a virtual village composed of two virtual farms that have access to groundwater in the same ambhas cell of 3 ha fig 2 the first farmer manages 1 ha of land organized into two crop plots and owns one bullock for cropping operations the second manages 2 ha of land organized into two crop plots and owns two bullocks neither farmer owns a tractor on each farm both the farmer and his wife work both farms can hire labor and rent equipment from the village i e 50 female laborers 40 male laborers 4 pairs of bullocks 1 tractor because a borewell can be drilled on each plot this namaste simulation consisted of four borewell models during simulation the strategic decision model may change the parameters for pump horsepower and well depth the net recharge water drained minus water pumped returned to the ambhas cell is calculated by an intermediate model that calculates the difference between total drainage and pumping flows of all plots namaste simulation of two farms in one village proceeds as follows each farm is simulated by a decision system which includes strategic tactical and operational decision models namaste considers farm characteristics e g number of plots soil type amount of labor and equipment crop management files and initial farm status as initial conditions the operational decision model manages the village s labor and equipment that is used for crop production it functions as a resource manager and attributes resources to the first enquirer the operational model interacts with the operating system while the biophysical system has as many stics models as there are plots in the village the external system i e weather market and electricity models constrains both farms in the same way and we assume that farmers individual or combined decisions do not influence it the same ambhas cell simulates the irrigation water available to both farms at the beginning of the year in the strategic model each farmer makes decisions about investment and the cropping system for the next ten years independent of other farm decisions investment in a borewell determines parameters of the borewell model for each plot at the beginning of the cropping season in the tactical model each farmer independently updates the cropping system based on new information on prices rainfall and groundwater level this updated cropping system is then entered into the operational model which defines the crop rotation and the management of each crop the operational model determines when conditions necessary for crop operations are met and requests resources from its resource manager farmers practices interact at this level which means that one farmer s crop operations may be restricted by the other s when both need labor and equipment at the same time irrigation water may also be a source of conflict between farmers since the water that one farmer withdraws from the aquifer is no longer available to the other fig 2 4 3 calibration and validation the model was calibrated with data from two farm surveys conducted in the berambadi watershed farmers on the watershed were surveyed in 2014 and 2015 the first survey targeted 27 farmers to obtain detailed data about their practices especially their decisions and how they adapted them the second survey targeted 684 farmers to obtain broad data on farm characteristics and the social economic and agronomic environments this survey enabled creation of a typology of farmers on the watershed based on biophysical factors e g farm location soil type groundwater accessibility economic factors e g farm size labor equipment and social factors e g castes family structure education off farm job for more details see robert et al 2017a we surveyed seed retailers and village leaders panchayats to learn about recommended crop management practices and village organization additionally 52 experimental plots were monitored over three years which provided empirical data on crop production and management these data helped supplement the verbal information that farmers provided during surveys meteorological data were obtained from a meteorological station and water gauges installed on the watershed prices and costs were obtained from farmers and official district data from the indian ministry of agriculture and cooperation directorate of economics and statistics and the national informatics center agricultural census division the model was validated using computerized model verification and operational validation computerized model verification checks whether the computer implementation corresponds to the conceptual model representation whitner and balci 1986 sargent 2013 it verifies that the computer code has no coding errors or computer bugs and that the simulation language is implemented well we used two main approaches for computerized model verification i static which tests the main script at multiple points allowing for local checks during encoding and ii dynamic which executes the script with several sets of data and experimental conditions to verify the accuracy of outputs in contrast operational validation checks whether the behavior of the final simulation model is accurate enough to fulfill the research objectives the model was validated mainly by subjectively exploring its behavior sargent 2013 and graphically comparing model predictions to observed data verifying that predicted yields lay within the ranges of observed yields in the watershed details not shown the crop model stics was validated using plot data sreelash et al 2017 the ambhas model was validated on watershed data tomer 2012 we also qualitatively analyzed model behavior verifying that predicted variables moved in the same directions as observed variables for example we verified that crop choice management and yield correctly responded to climate variations that investment in irrigation corresponded to economic and climate environments and that the groundwater table correctly responded to rainfall and irrigation 4 4 simulation results 4 4 1 sequential decision making and adaptation at different temporal and spatial scales to illustrate decision processes and adaptations we describe the sequential decision making processes the first farmer followed to cultivate 1 ha of land in long term decision making the farmer s objective is to select the sequence of investment in irrigation equipment and season specific crops that maximizes the discounted stream of future revenue across the 10 year planning horizon such selection is based on expected rainfall and crop prices and available water for irrigation at the beginning of the 10 year planning horizon the farmer has expectations for the future climate i e percentage chances that a year will have good 2 4 above average 22 average 46 below average 22 or poor rainfall 7 3 and decides in the sequence of investment in irrigation equipment and season specific crops he should do to maximize his future revenue across the 10 year planning horizon running the strategic decision model at the beginning of the 10 year planning horizon results in one investment in irrigation equipment dig a borewell the first year and a 10 year cropping system described in the first column of fig 3 as time passed the farmer is receiving new information on his environment and is observing changes in the weather conditions in fact his expectations on climate may evolve as time passed and he may review and update his weather expectation at the beginning of each year or even each season compared to his expectations at the beginning of the planning horizon for instance comparing weather expectations used in the strategic decision model and those used in each run of the tactical model results in going from percentage chances that a year will have good 2 4 above average 22 average 46 below average 22 or poor rainfall 7 3 to the yearly belief that the first year will be average rain the second year will be average rain the third year will be above average rain the fourth year will be below average rain the fifth year will be average rain the sixth year will be above average rain the seventh year will be average rain the eighth year will be average rain the ninth year will be poor rain and the tenth year will be below average rain in the computations and the simulations rainfall selection within the tactical decision model is made by using probability of occurrence used in the strategic decision model updated accordingly with daily rainfall provided by the weather model in medium term decision making the farmer s objective is to review the season specific crops that maximizes the future revenue that should be obtained at the end of the year running the tactical decision model at the beginning of each year 10 runs results in yearly updated cropping systems described in the second column of fig 3 at the tactical level the farmer knows what kind of climate to expect i e average rainfall but is uncertain about the seasonal distribution of rainfall throughout the year by starting daily crop operations operational decisions the farmer can adapt crops and or practices when conditions are not suitable thus the crop that will finally be harvested at the end of the season by the farmer third column of fig 3 may be different from the one selected at the beginning of the year tactical decisions second column of fig 3 and at the beginning of the planning horizon strategic decisions first column of fig 3 changes in rainfall expectation and cropping system choice led the farmers to review his expectations on the groundwater level and profit fig 4 expected profit in long and medium term are computed based on crop prices expected yields irrigation costs and other crop specific costs investment in irrigation equipment are included in the long term profit expectation computation details are provided in robert et al 2017b expectations on groundwater level are necessary in particular for the long term decision since the decision on investment in irrigation will impact on future access to irrigation water expectations on groundwater level are also necessary on medium and short term decisions since pumping water and irrigating a crop will impact on the future availability of irrigation water long and medium term groundwater level expectations depend on borewell recharge from rainfall borewell discharge water pumped for irrigation the past period and the capital stock depreciation rate of the equipment details are provided in robert et al 2017b note that short term groundwater expectations are replaced in the computations and the simulations by observed groundwater level from the groundwater model tactical and operational adaptations influenced the groundwater level and the farmer s profit fig 4 due to generally adequate rainfall during the planning horizon except the 4th 9th and 10th years pumping was lower and recharge was higher than predicted at the strategic level thus the groundwater level did not decrease as much as expected and the borewell did not go dry during the planning horizon concerning profit rainfall induced economically disastrous years i e 3rd and 9th years by preventing the farmer s initial cropping system plan and requiring maize to be sown or highly profitable years i e the 6th year by being above average and well distributed during crop growth in at least one year i e the 10th irrigation was able to compensate for the below average rainfall and provide a profit the fourth year illustrates crop choice adaptation at the tactical level at the beginning of year 4th the farmer is convinced the season will be below average rainfall the farmer knowing that low rainfall during the cropping season impacts rainfed crops in particular and observing a groundwater level much higher 12 4 m higher than expected at the beginning of the planning horizon fig 4 changed 0 1 ha of rainfed marigold to irrigated sunflower to go with the 0 9 ha of irrigated turmeric fig 3 the third year illustrates crop choice adaptation at the operational level the farmer planned to grow 0 9 ha of turmeric and 0 1 ha of marigold both crops have similar sowing windows 30 march 1 may and conditions rules soil moisture must be low enough to bear loads rule 75 of field capacity but high enough for seeds to germinate rule 60 of field capacity and rain must not be forecast for two consecutive days since sowing can last two days rule total expected rainfall for two consecutive days must not exceed 5 mm five days in april 6 7 8 9 and 11 had acceptable soil moisture 60 75 of field capacity but april s high rainfall 147 mm prevented sowing because sowing turmeric and marigold was not possible the farmer had to review the cropping plan and sow only maize which has a wider sowing window april june the sixth year illustrates practices adaptation at the operational level unlike in the third year turmeric and sunflower could be sown in april however frequent rainfall events after sowing decreased the number of irrigation events planned for turmeric four planned irrigation events were cancelled because irrigation rules recommend irrigating when total rainfall during the past three days is 50 mm and 5 mm of rainfall is expected in the next two days 4 4 2 resource management between scarcity and sharing namaste considers situations in which resources may be limited because they are scarce and limited and or used by another farmer in these conditions farmers adapt their daily practices delaying crop operations until resources become available competition for resources can be internal when operations occur at the same time in two plots of the same farm or external when competition is due to another farmer s practices to illustrate resource management in namaste we describe management of village resources 50 female laborers 40 male laborers 4 pairs of bullocks 1 tractor in the first year of the planning horizon fig 5 the first farmer planned to grow turmeric on 0 9 ha with 16 irrigation events and rainfed marigold on 0 1 ha the second farmer planned to grow turmeric on 1 6 ha with 12 irrigation events and rainfed marigold on 0 4 ha three types of resource conflicts were observed table 1 and fig 5 4 4 2 1 type 1 conflicts over use of the tractor to plow land turmeric and marigold plots must be plowed during the same window 1 23 march and according to the same conditions rules since turmeric has a higher priority index than marigold i e the farmer prefers economically to grow turmeric than marigold the resource manager allocates the tractor to the turmeric plots first e g the first tractor plowing occurs on 10th 11th march for turmeric plots and 15th 16th march for marigold plots between turmeric plots the resource is randomly attributed for example for turmeric the first tractor plowing occurs on march 10th on farm 1 and march 11th on farm 2 and the second tractor plowing occurs on march 20th on farm 1 and on march 21st on farm 2 4 4 2 2 type 2 conflicts over use of female labor to weed turmeric since both turmeric plots were sown on the 13th of april they had the same time windows for weeding events 10 35 55 65 85 95 and 115 125 days after sowing for each weeding event the first farmer needed 28 male and 21 female laborers and the second farmer needed 14 male and 37 female laborers since the village had only 50 female laborers both farmers could not weed their plots at the same time the resource is randomly attributed 4 4 2 3 type 3 conflicts over use of both female and male labor to harvest turmeric the first farmer needed 20 male and 23 female laborers and the second farmer needed 35 male and 42 female laborers to harvest their turmeric plots since both female and male labor was limited at 50 and 40 laborers respectively one of the farmers had to delay harvest 5 discussion understanding farmers decision making processes and relationships with the biophysical system is necessary to understand farming system complexity at multiple scales namaste is a model able to simulate decision making processes and interactions between strategic tactic and operational decisions investment cropping system and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies the model proposes two main innovations i its decision model simulates farmers decision making processes by describing dynamic sequential decisions via adaptation to the biophysical environment and ii it couples decision economic biophysical and hydrological systems through an integrative approach to predict effects and spillover of human decisions on natural systems research increasingly considers farm management as a flexible and dynamic process in recent agricultural literature however consequences on long and short term farm organization are rarely considered even though they appear to influence farmers decision making greatly daydé et al 2014 indeed 70 of the articles reviewed in robert et al 2016a focused on only one stage of the decision adaptation at the strategic level for the entire farm or at the tactical level for the farm or plot some authors combined strategic and tactical decisions to consider the entire decision making process and adaptation of farmers dynamic programming is a dynamic model that allows the adaptation of strategic decisions according to adaptations made to tactical decisions it has been used to address strategic investment decisions reynaud 2009 duffy and taylor 1993 and to address tactic decisions about cropping systems hyytiäinen et al 2011 thomas 2003 other authors combined reactive formalisms with a recursive approach and static discrete stochastic programming approaches to describe the sequential decision making process from strategic decisions and adaptations to tactical decisions and adaptations mosnier et al 2009 belhouchette et al 2004 lescot et al 2011 we used the basic definition of le gal et al 2011 which divides a decision into a set of interconnected decisions made over time and at multiple spatial scales sequential and dynamic representation is particularly useful and appropriate for modeling entire processes for making strategic tactical and operational decisions risbey et al 1999 le gal et al 2011 we proposed in this paper to combine several formalisms within an integrated model in which strategic and tactical adaptations and decisions influence each other to model adaptive behavior within farmers decision making processes to model three stages of decision we combined economic decision rule and activity based models the integrative approach of coupling decision economic biophysical and hydrological models was necessary to model and quantify spatio temporal variability in water resources and interactions among groundwater agricultural practices and crop growth one difficulty in modeling these processes is combining independent models that were originally developed for specific purposes at different spatial and temporal scales kraucunas et al 2015 the biophysical model used in namaste was developed to simulate fixed practices such as sowing irrigation and harvest on one plot for one cropping season see brisson et al 1998 for more details on the stics model the hydrological model simulates groundwater dynamics of a large territory see tomer 2012 for more details on the ambhas model the decision model describes farmers decisions and practices on their farms meaningful model integration requires calibration and validation of the global model and consistency in the underlying system boundaries assumptions and scale of analysis of these diverse models in the namaste model as kling et al 2016 suggested it is necessary to develop bridge models that convert outputs of one model into inputs of another this approach enables atomic models to be connected at different scales and to operate together by downscaling outputs from watershed to field models as an example the borewell model is considered as a bridge model to convert the groundwater level which is a watershed output from the ambhas model to the available water for irrigation at the plot scale which is a field data used as an input for the field model stics and by upscaling outputs from field to watershed models e g outputs on drained water from the atomic stics models and pumped water from the atomic borewell models are used by the bridge model net recharge to compute the daily net recharge at the watershed scale that will be used as input by the watershed model ambhas this can ensure that component models are manageable and provide outputs at both field and watershed scales hibbard and janetos 2013 work on hard linked integration is still on going soft linking is a necessary starting point to test different modelling and linking approaches holz et al 2016 it ensures practicality transparency and learning with low initial investments in computer programming future work on hard linking will ensure efficiency scalability and control with automatized exchanges of information between models in this study operational validation was performed mainly by subjectively and qualitatively exploring model behavior sargent 2013 we developed a simulation model able to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies nonetheless quantitative simulation results for the berambadi case study still have high uncertainty due to for instance simplification of some biophysical processes simplification due to the farmers typology or simplification due to the grid of climate calibrating and validating the namaste model is an important and time consuming step that is still underway 94 parameters are directly accessible in the coupled model and ambhas and stics have many internal parameters that require tedious calibration in our experimental model we simulated a virtual village composed we simulated a virtual village composed of two virtual farms that have access to groundwater in the same ambhas cell the watershed scale of the ambhas model was not used and considering only one cell of this model resulted in simplifying the watershed model by ignoring lateral water fluxes thus our model is still considered as a dynamic model for water management at the farm level and cannot be considered yet as an agent based model in future work we will consider the upscaling issue from the perspective of modeling the consequences of farmers adaptations to changes in climate and groundwater upscaling from the farm level to watershed regional and national levels is a common approach for studying system behavior and dynamics such as farm adaptations to climate change gibbons et al 2010 land use and land cover changes in response to climate change rounsevell et al 2014 and ecosystem changes in response to biotic and abiotic processes nash et al 2014 peters et al 2007 identified three types of scales fine one individual intermediate groups of individuals and broad large spatial extents such as the landscape region and planet the appropriate scale is defined by the research question or hypothesis and often requires upscaling or downscaling existing models gibbons et al 2010 the challenge of aggregation or upscaling is to determine which fine scale details matter most at intermediate or broad scales research questions differ according to the scale at the farm scale we focused on farmers decision making processes and their adaptation to uncertain changes e g climate and resource availability the watershed level requires exploring the influence of decision making on the groundwater table rather than the process of decision making itself and to consider interactions between individuals for shared resources at the watershed level relative trends are more important than absolute values for example at the watershed level we are more interested in the total amount of groundwater used for irrigation in the watershed than on individual farms in the agricultural literature there are few models dealing with both watershed and individual farm scale representations the shadoc model barreteau and bousquet 2000 the bali model lansing and et kremer 1994 and the sinuse model feuillette et al 2003 simulate water management at the watershed scale but are limited to a single scheme scale or to management units that are not the individual farmer but farmers irrigation groups or a villages interacting in use and management of the resource with decision making processes based on maximisation functions of farmers or groups or villages incomes according to their available resources the multi agent maelia s gaudou and sibertin blanc 2013 and catchscape s becu et al 2003 architectures aimed at being integrative spatially distributed and individual based in order to cope with complex and adaptive issues at the watershed scale in future work our upscaled model will provide a detail description of the farmer cognitive agents with decision making processes and adaptive behavior our model provides tools to analyze evaluate and optimize agronomic environmental and economic criteria we tested the model with a baseline scenario to simulate current farming practices in the berambadi watershed and predict influence of the latter on groundwater level in a virtual village composed of two farms modeling agricultural production scenarios can help stakeholders make decisions about regulations and resource restrictions and encourage new practices to recommend to farmers 6 conclusion we developed an original simulation model of a farming system that combines relevant principles highlighted in the scientific literature the model was initially developed to address critical issues of groundwater depletion and farming practices in a watershed in southwestern india its structure frameworks and formalisms can be applied to other agricultural contexts our application focused on water management in semi arid agricultural systems but the model can also be applied to other farming systems to confirm the reusability and robustness of the framework acknowledgement this study was funded by the indo french centre for the promotion of advanced research cefipra the inra flagship program on adaptation of agriculture and forest to climate change accaf di inter 2012 2 and accaf di 2013 3 the doctoral school of the university of toulouse edt and the observatory for environment research experimental tropical watersheds ore bvet http bvet obs mip fr we are grateful to all the experts farmers survey takers and trainees who helped implement the method we give special thanks to tony raveneau for his help with the computer implementation 
26453,farming systems are complex and have several dimensions that interact in a dynamic and continuous manner depending on farmers management strategies this complexity peaks in indian semi arid regions where small farms encounter a highly competitive environment for markets and resources especially unreliable access to water from rainfall and irrigation namaste a dynamic computer model for water management at the farm level was developed to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies the most relevant and novel aspects are i system based representation of farming systems ii description of dynamic processes via management flexibility and adaptation iii representation of farmers decision making processes at multiple temporal and spatial scales iv management of shared resources namaste s ability to simulate farmers adaptive decision making processes is illustrated by simulating a virtual indian village composed of two virtual farms with access to groundwater keywords farmer s decision making process adaptation modeling climate change water management policy 1 introduction agriculture faces many challenges regarding its productivity revenue and environmental and health impacts challenges that must be considered within the known context of climate change agriculture also faces demands to increase the quantity quality accessibility and availability of production to secure food production and improve product quality to address needs of the world s growing population meynard et al 2012 hertel 2015 mckenzie and williams 2015 agricultural productivity must increase within a framework of environmental and health concerns to do so agriculture should decrease its environmental impacts on water air soil and aquatic environments and consider the scarcity of resources such as water phosphorus and fossil fuels especially for production of nitrogen fertilizers fao 2011 brown et al 2015 under climate change warmer temperatures changes in rainfall patterns and increased frequency of extreme weather are expected to occur consequently it has direct biophysical effects on agricultural production and can negatively affect crop yields and livestock nelson et al 2014 rising sea level will increase risks of flooding of agricultural land in coastal regions while changes in rainfall patterns could increase growth of weeds pests and diseases de lapeyre de bellaire et al 2016 on the deccan plateau in india the countryside has witnessed the proliferation of individual electrical pump driven borewells that extract water from underground aquifers sekhar et al 2006 javeed et al 2009 the low productivity of the aquifer dewandel et al 2010 perrin et al 2011 and a rapid decline in the water table level has decreased borewell yields ruiz et al 2015 indicating that groundwater irrigated agriculture still largely depends on rainfall for a region that depends largely on monsoon and winter rainfall to maintain agricultural production any shift in climate would have a severe impact on natural resources and the economy drilling borewells to gain control over water access is crucial to maintain household sustainability however it also entails the risk of failed borewells and intractable debts taylor 2013 modeling and quantifying spatio temporal variability in water resources and interactions among groundwater agricultural practices and crop growth is an essential component of integrated and comprehensive water resource management ruiz et al 2015 simulating scenarios of climate change and water management policies is an essential tool to identify mechanisms that farmers can use and policies that can be implemented to address these challenges white et al 2015 in these modeling and simulation approaches farmers decision making processes should be considered to assess how agricultural production systems change and adapt to external changes and opportunities farm management requires farmers to make a set of interconnected and successive decisions over time and at multiple spatial scales risbey et al 1999 le gal et al 2011 in the long term farmers decide on possible investments and marketing strategies to select or adapt to best fulfill their objectives decisions about cropping systems also impact the farm decisions about crop rotation and allocation are considered at the whole farm level detlefsen and jensen 2007 castellazzi et al 2008 dury et al 2010 and can be investigated in the long term and or adapted for shorter periods once a crop is chosen farmers must make intra annual decisions to choose crop management techniques and the varieties to sow in the coming year this decision can be made before cultivation and adapted if necessary tactical decisions generally this decision concerns the whole farm to ensure that practices are consistent or to maintain a minimum of crop diversity on the farm however tactical decision descriptions are not sufficient to trigger daily operational management therefore farmers must define specific ways to execute their tactical plans farmers decide on crop operations and resource management and even change the purpose of their crops when conditions are not conducive to the initial plan a farm decision making model should include sequential aspects of the decision making process and farmers abilities to adapt and react akplogan 2013 according to a review of modeling adaptive processes in farmers decision making robert et al 2016a 70 of the articles reviewed focused on only one stage of the decision adaptation at the strategic level for the entire farm or at the tactical level for the farm or plot we suggest reconsidering farm management as a decision making process in which decisions and adaptations are made continuously and sequentially over time the 3d approach strategic decisions tactical decisions operational decisions to simulate reality more closely these considerations prompted the development of a simulation model able to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies this article presents this farming system model and an example of its application to a semi arid region in karnataka state southwestern india we first introduce the conceptual model and the modeling and simulation platform we then describe the model namaste in detail and illustrate its capabilities by applying it to a case study in southern india finally we discuss the key modeling choices and present several insights on how to upscale the model from the farm level to watershed regional and national levels 2 materials and methods 2 1 conceptual modeling we divided the systemic representation of the farming system into three interactive systems i decision system which describes farmers continuous and sequential decision process ii operating system technical system which translates decisions ordered by the decision system into instructions to execute tasks which is an action to perform on a biophysical object or location e g sowing operation and iii biophysical system which describes crop and soil dynamics and their interactions especially relations between groundwater soil and plant development using a crop model clouaire and rellier 2009 le gal et al 2010 dury 2011 akplogan 2013 robert et al 2016b fig 1 for the decision model we consider farmers as cognitive agents able to think memorize analyze predict and learn to manage future events and plan their actions le bars et al 2005 in artificial intelligence and cognitive sciences agents are commonly represented as belief desire intention bdi agents bratman 1987 rao and georgeff 1991 the bdi framework is founded on the well known theory of rational action in humans bdi agents are considered to have an incomplete view of their environment simon 1950 cyert and march 1963 the concept of belief represents a farmer s knowledge of the farming system and its environment desires are a farmer s objectives goals that meet production or management goals intentions are action plans that achieve a farmer s objectives desires farmers are represented as bdi agents at several levels of the conceptual model of the farming system farmers beliefs and desires are the basis of the production processes in the farming systems farmers manage their farms based on their knowledge and objectives farmers have different types of knowledge about their farms structural i e farming structure and organization procedural i e know how of farming practices and observable i e observations about their environment observing social and economic environments is important to be able to quickly respond to changes and uncertainties in the production context the climate prices of crops and inputs and availability of external resources such as groundwater labor or shared equipment are common uncontrollable data farmers use to make decisions they also adapt their practices based on recent outputs of production systems such as yields decision models provide the plans that farmers will execute in their production systems based on their observations and objectives which translates into actions invest perform a crop operation etc that correspond to intentions of the bdi agent contrary to these actions which are direct outputs from the farmer decision making model other outputs are consequences of these actions on the biophysical system such as impact on groundwater water consumption due to the volume of water pumped and drainage due to the natural return of excess water from rainfall and irrigation for more details see robert et al 2016b 2 2 record a modeling and simulation computer platform 2 2 1 overview the record platform is a modeling and simulation computer platform devoted to the study of agro ecosystems bergez et al 2013 record facilitates design of simple single atomic or hierarchical complex coupled models and enables using different temporal and spatial scales within models it is based on the virtual laboratory environment vle a free and open source multi modeling and simulation platform based on the discrete event system specification devs formalism that derives from the theory defined by zeigler et al 2000 on modeling and simulation for dynamic systems with discrete events vle provides a simulation engine modeling tools software libraries and an integrated development environment to the record platform specific extensions have been developed in record to bridge the gap between the generic vle and the framework adapted to the domain of agro ecosystems these extensions help modelers in developing their models in the formalism they are used or which is the most suitable by providing generic templates of codes and input parameters see bergez et al 2013 for an extended description of the platform in this study two vle extensions decision and differenceequation were used to build the model and a specific package rvle for the statistical software r r core team 2016 was used to help perform simulations 2 2 2 vle extension decision to represent farmers operational decision making processes the decision extension bergez et al 2016 implements the decision portion of the model in the decision system operating system biophysical system approach during simulations the decision model identifies the state of the environment e g weather plant soil resource availability and sends orders to the connected models e g a biophysical crop model according to a flexible work plan of activities i e the tasks to be achieved the work plan of activities contains the following i a knowledge base composed of information about the system used to reach a decision the information includes dynamics of biophysical processes availabilities of human and material resources and spatial information about farm structure they are organized as a set of variables whose values evolve during the simulation to update the knowledge base at each time step ii tasks to be executed and associated conditions predicates rules and time windows iii temporal relations between tasks for details of the decision extension see bergez et al 2016 recently resource constraints were added to the conditions associated to the tasks a set of available discrete resources is defined and structured by categories within the knowledge base resource constraints are defined for each task by a needed quantity possible alternatives and priorities during simulation the resource allocation is sequentially managed depending on resource availability and task priorities 2 2 3 vle extension differenceequation the vle extension differenceequation extension formulates time discrete models that calculate values of real variables at time t as a function of the value of variables in the system at time t δ t 2δ etc e g varx t 1 f varx t the expected parameters for an atomic model using differenceequation are i the simulation time step δt which must be the same for all equations ii the mode either name of all external variables and iii the state variables and there initial values 2 2 4 rvle a user friendly tool in the record platform rvle www vle project org is an r package that calls vle s application programming interface from r it can open packages and read model structure vpz files assign experimental conditions to models call the simulator build experimental frames and turn simulation results into an r object such as a matrix or dataframe it is especially useful for analyzing simulation results and performing statistics it also allows users to manipulate models from the r environment 3 description of the farming system model 3 1 models used to build the farming system model 3 1 1 the decision system 3 1 1 1 3d three integrated decision models the novelty of the decision system is that three integrated decision models were built to represent farmers strategic tactical and operational decisions and adaptations the strategic model simulates farmers strategic decisions which include decisions about investment and cropping systems the tactical model simulates farmers tactical decisions especially adaptation of cropping systems the operational model simulates farmers operational and crop management decisions we used three modeling formalisms to describe farmers decisions throughout the decision process decisions about investment and cropping systems that are influenced by economic return maximizing profit were expressed using dynamic stochastic programming decisions about establishing cropping systems that are influenced by motivations besides economic return e g proximity to a market equipment were implemented via a decision rule modeling approach using a specific descriptive language whose syntax is based on formal if then else rules written as a boolean condition if indicator operator threshold then action1 else action2 decisions about crop management were described as an activity graph supported by the decision extension in the record platform and using a knowledge base the knowledge base collects information that the farmer obtains from the biophysical subsystem when monitoring and observing the environment the activity graph represents the farmer s work plan and relies on the knowledge base to activate or disable technical operations an activity denotes a task rules control the start of the activity by checking whether conditions necessary to perform the operation exist for details of this formalism see bergez et al 2016 3 1 1 2 modeling resource management in the operational decision model from a modeling viewpoint two types of resources were distinguished in the farming system i conditional discrete and returnable resources which are necessary to execute an operation and can be used and then returned once the operation is finished e g labor tractor and ii unconditional and consumable resources which are not necessary to execute an operation and are consumed and not returned after use in an operation e g irrigation water these resources are managed differently in the model the operating system manages the unconditional and consumable resources for example following an order to execute irrigation the decision system returns each day the amount of water needed to irrigate the farm the operating system compares the water needed to the water available and executes the order to irrigate by transferring the lesser of the two values water needed or water available to the biophysical system conflicts between activities requiring the same conditional discrete and retrunables resources at the same time are dynamically managed using rules to allocate resources and determine the order in which activities are executed prioritization is managed by rules that temporally rank activities that can be executed simultaneously ranks can be reviewed and updated by other rules 3 1 2 the operating system the operating system translates decision orders into executable and timed actions it calculates the duration of each activity based on the quantity and type of resources that an operation uses and the speed with which each resource executes an operation entered in the experimental conditions of the simulation the operating system can also transform certain data transferred from the decision system so that data units correspond to those expected by the receiving model the operating system is implemented using difference equations in record 3 1 3 the biophysical model the stics model which represents the crop and soil system simulates dynamics of a crop soil system over one or more crop cycles at a daily time step brisson et al 1998 we selected stics for its adaptability to many crop types robustness in a wide range of soil and climate conditions and modularity brisson et al 2003 it has been successfully used in spatially explicit applications and coupled with hydrological models at the watershed level beaujouan et al 2001 stics receives the crop operations and parameters applied to the plot from the operating system which executes orders provided by the decision system stics returns information about crop stage yield soil characteristics water use and drainage the fortran code in stics was wrapped into an atomic model using difference equations in record bergez et al 2014 3 2 model structure the model is composed of one decision system and one operating system the latter of which interacts with one biophysical system fig 1 the biophysical system can be made up of several crop models for example an independent stics model represents the crop and soil of each plot of the modeled farm for each plot the decision system has a work plan with specific activities listed for the plot s crop several work plans may run in parallel when the modeled farm has several plots the resource manager must manage conditional discrete and returnable resources both among activities in a given work plan and among work plans in this case prioritization is used to rank all activities temporally unconditional and consumable resources are distributed by an intermediary model that for example allocates irrigation water to plots when the decision system sends several irrigation orders on the same day available water is distributed according to priorities assigned to work plans to simulate the cropping system over several years several work plans for the same plot must be run sequentially the next one is loaded when the last activity of a given work plan is completed 3 3 dynamic functioning in systemic modeling i e modeling a complex system as sub systems interacting with each other models must be able to interact with each other to provide feedback and other types of interactions two types of variables are identified in models state variables which are managed by the model itself and external variables which are managed by other models external events e g rainfall market prices electricity availability and availability of resources e g labor equipment irrigation water are summarized in the inputs model fig 1 at the beginning of the year the strategic model receives information from inputs so that strategic decisions about investment and long term cropping systems are based on farmers knowledge and objectives these two types of strategic decisions are forwarded to the tactical model at the beginning of the cropping season the tactical model receives information from inputs so that farmers knowledge is updated this new information prompts the tactical model to update the cropping system and forward the adapted cropping system to the operational model at the operational level decisions are based on the appropriate agronomic context and the farmers updated knowledge once both types of information meet the requirements for executing an operation and are forwarded to the operational model an operation order is transferred to the operating system which translates it into a task execution so that the operation is performed in the plot one day after the decision is made at the end of the operation the crop model returns the agronomic context to the operational model after harvest the operational model informs the tactical model that the cropping season is over when the tactical model automatically wakes up on day 250 official end of the season it receives information about the successful harvest and updated knowledge about the system processes and updates the cropping system for the second season and forwards this updated cropping system to the operational model at the end of the second season the same process occurs and soon after the second year begins 4 application case the namaste simulation model namaste simulates farmers adaptations to uncertain events such as climate change water table depletion the economic environment and agricultural reforms we applied namaste to a case study located in karnataka state india in the berambadi watershed the cropping system is organized around three seasons i the rainy season kharif when most crops are grown ii the winter season rabi when mainly irrigated crops are grown and iii the dry season when little cultivation occurs summer monsoon rainfall is a key determinant of crop choice farmers make three types of decisions i whether to invest in an irrigation system ii crop selection and iii crop management and operations all farmers in the watershed pump irrigation water from the same aquifer and those from the same village share labor and equipment to illustrate namaste s ability to simulate farmers adaptive decision making processes under conditions of both limited and shared resources we ran a baseline scenario over a 10 year planning horizon in which the parameters that describe the climate crop market conditions prices and costs and water pumping conditions i e hours of electricity available each day cost of pumping were based on those obtained from farmer surveys 4 1 coupling the farming system model to the hydrological model rainfall market prices and availability of electricity labor equipment and irrigation water are inputs to the farming system they are modeled as subsystems of an external system that limits the farming system fig 2 4 1 1 hydrological subsystem ambhas tomer 2012 is a spatially explicit groundwater model that simulates dynamics of daily groundwater level based on equations from mcdonald and harbaugh 1988 it predicts daily groundwater level actual net recharge and discharge for a 1 ha cell the net recharge is calculated as the difference between the amount of water drained below the soil profile and the required water amount for crop irrigation predicted by stics as for the stics model a wrapper was used to integrate the ambhas code into record avoiding heavy recoding this wrapper was based on difference equations formalism 4 1 2 climate market and power supply subsystems the weather model simulates expected and actual rainfall each day the market model simulates expected and actual crop market prices the electricity model simulates the number of hours of electricity available each day these sub models are implemented in an atomic model using difference equations in record 4 2 namaste simulation when resources are shared interactions are important to an individual farmer s decision making process to integrate resource constraints into the farming system model our experimental model simulates a virtual village composed of two virtual farms that have access to groundwater in the same ambhas cell of 3 ha fig 2 the first farmer manages 1 ha of land organized into two crop plots and owns one bullock for cropping operations the second manages 2 ha of land organized into two crop plots and owns two bullocks neither farmer owns a tractor on each farm both the farmer and his wife work both farms can hire labor and rent equipment from the village i e 50 female laborers 40 male laborers 4 pairs of bullocks 1 tractor because a borewell can be drilled on each plot this namaste simulation consisted of four borewell models during simulation the strategic decision model may change the parameters for pump horsepower and well depth the net recharge water drained minus water pumped returned to the ambhas cell is calculated by an intermediate model that calculates the difference between total drainage and pumping flows of all plots namaste simulation of two farms in one village proceeds as follows each farm is simulated by a decision system which includes strategic tactical and operational decision models namaste considers farm characteristics e g number of plots soil type amount of labor and equipment crop management files and initial farm status as initial conditions the operational decision model manages the village s labor and equipment that is used for crop production it functions as a resource manager and attributes resources to the first enquirer the operational model interacts with the operating system while the biophysical system has as many stics models as there are plots in the village the external system i e weather market and electricity models constrains both farms in the same way and we assume that farmers individual or combined decisions do not influence it the same ambhas cell simulates the irrigation water available to both farms at the beginning of the year in the strategic model each farmer makes decisions about investment and the cropping system for the next ten years independent of other farm decisions investment in a borewell determines parameters of the borewell model for each plot at the beginning of the cropping season in the tactical model each farmer independently updates the cropping system based on new information on prices rainfall and groundwater level this updated cropping system is then entered into the operational model which defines the crop rotation and the management of each crop the operational model determines when conditions necessary for crop operations are met and requests resources from its resource manager farmers practices interact at this level which means that one farmer s crop operations may be restricted by the other s when both need labor and equipment at the same time irrigation water may also be a source of conflict between farmers since the water that one farmer withdraws from the aquifer is no longer available to the other fig 2 4 3 calibration and validation the model was calibrated with data from two farm surveys conducted in the berambadi watershed farmers on the watershed were surveyed in 2014 and 2015 the first survey targeted 27 farmers to obtain detailed data about their practices especially their decisions and how they adapted them the second survey targeted 684 farmers to obtain broad data on farm characteristics and the social economic and agronomic environments this survey enabled creation of a typology of farmers on the watershed based on biophysical factors e g farm location soil type groundwater accessibility economic factors e g farm size labor equipment and social factors e g castes family structure education off farm job for more details see robert et al 2017a we surveyed seed retailers and village leaders panchayats to learn about recommended crop management practices and village organization additionally 52 experimental plots were monitored over three years which provided empirical data on crop production and management these data helped supplement the verbal information that farmers provided during surveys meteorological data were obtained from a meteorological station and water gauges installed on the watershed prices and costs were obtained from farmers and official district data from the indian ministry of agriculture and cooperation directorate of economics and statistics and the national informatics center agricultural census division the model was validated using computerized model verification and operational validation computerized model verification checks whether the computer implementation corresponds to the conceptual model representation whitner and balci 1986 sargent 2013 it verifies that the computer code has no coding errors or computer bugs and that the simulation language is implemented well we used two main approaches for computerized model verification i static which tests the main script at multiple points allowing for local checks during encoding and ii dynamic which executes the script with several sets of data and experimental conditions to verify the accuracy of outputs in contrast operational validation checks whether the behavior of the final simulation model is accurate enough to fulfill the research objectives the model was validated mainly by subjectively exploring its behavior sargent 2013 and graphically comparing model predictions to observed data verifying that predicted yields lay within the ranges of observed yields in the watershed details not shown the crop model stics was validated using plot data sreelash et al 2017 the ambhas model was validated on watershed data tomer 2012 we also qualitatively analyzed model behavior verifying that predicted variables moved in the same directions as observed variables for example we verified that crop choice management and yield correctly responded to climate variations that investment in irrigation corresponded to economic and climate environments and that the groundwater table correctly responded to rainfall and irrigation 4 4 simulation results 4 4 1 sequential decision making and adaptation at different temporal and spatial scales to illustrate decision processes and adaptations we describe the sequential decision making processes the first farmer followed to cultivate 1 ha of land in long term decision making the farmer s objective is to select the sequence of investment in irrigation equipment and season specific crops that maximizes the discounted stream of future revenue across the 10 year planning horizon such selection is based on expected rainfall and crop prices and available water for irrigation at the beginning of the 10 year planning horizon the farmer has expectations for the future climate i e percentage chances that a year will have good 2 4 above average 22 average 46 below average 22 or poor rainfall 7 3 and decides in the sequence of investment in irrigation equipment and season specific crops he should do to maximize his future revenue across the 10 year planning horizon running the strategic decision model at the beginning of the 10 year planning horizon results in one investment in irrigation equipment dig a borewell the first year and a 10 year cropping system described in the first column of fig 3 as time passed the farmer is receiving new information on his environment and is observing changes in the weather conditions in fact his expectations on climate may evolve as time passed and he may review and update his weather expectation at the beginning of each year or even each season compared to his expectations at the beginning of the planning horizon for instance comparing weather expectations used in the strategic decision model and those used in each run of the tactical model results in going from percentage chances that a year will have good 2 4 above average 22 average 46 below average 22 or poor rainfall 7 3 to the yearly belief that the first year will be average rain the second year will be average rain the third year will be above average rain the fourth year will be below average rain the fifth year will be average rain the sixth year will be above average rain the seventh year will be average rain the eighth year will be average rain the ninth year will be poor rain and the tenth year will be below average rain in the computations and the simulations rainfall selection within the tactical decision model is made by using probability of occurrence used in the strategic decision model updated accordingly with daily rainfall provided by the weather model in medium term decision making the farmer s objective is to review the season specific crops that maximizes the future revenue that should be obtained at the end of the year running the tactical decision model at the beginning of each year 10 runs results in yearly updated cropping systems described in the second column of fig 3 at the tactical level the farmer knows what kind of climate to expect i e average rainfall but is uncertain about the seasonal distribution of rainfall throughout the year by starting daily crop operations operational decisions the farmer can adapt crops and or practices when conditions are not suitable thus the crop that will finally be harvested at the end of the season by the farmer third column of fig 3 may be different from the one selected at the beginning of the year tactical decisions second column of fig 3 and at the beginning of the planning horizon strategic decisions first column of fig 3 changes in rainfall expectation and cropping system choice led the farmers to review his expectations on the groundwater level and profit fig 4 expected profit in long and medium term are computed based on crop prices expected yields irrigation costs and other crop specific costs investment in irrigation equipment are included in the long term profit expectation computation details are provided in robert et al 2017b expectations on groundwater level are necessary in particular for the long term decision since the decision on investment in irrigation will impact on future access to irrigation water expectations on groundwater level are also necessary on medium and short term decisions since pumping water and irrigating a crop will impact on the future availability of irrigation water long and medium term groundwater level expectations depend on borewell recharge from rainfall borewell discharge water pumped for irrigation the past period and the capital stock depreciation rate of the equipment details are provided in robert et al 2017b note that short term groundwater expectations are replaced in the computations and the simulations by observed groundwater level from the groundwater model tactical and operational adaptations influenced the groundwater level and the farmer s profit fig 4 due to generally adequate rainfall during the planning horizon except the 4th 9th and 10th years pumping was lower and recharge was higher than predicted at the strategic level thus the groundwater level did not decrease as much as expected and the borewell did not go dry during the planning horizon concerning profit rainfall induced economically disastrous years i e 3rd and 9th years by preventing the farmer s initial cropping system plan and requiring maize to be sown or highly profitable years i e the 6th year by being above average and well distributed during crop growth in at least one year i e the 10th irrigation was able to compensate for the below average rainfall and provide a profit the fourth year illustrates crop choice adaptation at the tactical level at the beginning of year 4th the farmer is convinced the season will be below average rainfall the farmer knowing that low rainfall during the cropping season impacts rainfed crops in particular and observing a groundwater level much higher 12 4 m higher than expected at the beginning of the planning horizon fig 4 changed 0 1 ha of rainfed marigold to irrigated sunflower to go with the 0 9 ha of irrigated turmeric fig 3 the third year illustrates crop choice adaptation at the operational level the farmer planned to grow 0 9 ha of turmeric and 0 1 ha of marigold both crops have similar sowing windows 30 march 1 may and conditions rules soil moisture must be low enough to bear loads rule 75 of field capacity but high enough for seeds to germinate rule 60 of field capacity and rain must not be forecast for two consecutive days since sowing can last two days rule total expected rainfall for two consecutive days must not exceed 5 mm five days in april 6 7 8 9 and 11 had acceptable soil moisture 60 75 of field capacity but april s high rainfall 147 mm prevented sowing because sowing turmeric and marigold was not possible the farmer had to review the cropping plan and sow only maize which has a wider sowing window april june the sixth year illustrates practices adaptation at the operational level unlike in the third year turmeric and sunflower could be sown in april however frequent rainfall events after sowing decreased the number of irrigation events planned for turmeric four planned irrigation events were cancelled because irrigation rules recommend irrigating when total rainfall during the past three days is 50 mm and 5 mm of rainfall is expected in the next two days 4 4 2 resource management between scarcity and sharing namaste considers situations in which resources may be limited because they are scarce and limited and or used by another farmer in these conditions farmers adapt their daily practices delaying crop operations until resources become available competition for resources can be internal when operations occur at the same time in two plots of the same farm or external when competition is due to another farmer s practices to illustrate resource management in namaste we describe management of village resources 50 female laborers 40 male laborers 4 pairs of bullocks 1 tractor in the first year of the planning horizon fig 5 the first farmer planned to grow turmeric on 0 9 ha with 16 irrigation events and rainfed marigold on 0 1 ha the second farmer planned to grow turmeric on 1 6 ha with 12 irrigation events and rainfed marigold on 0 4 ha three types of resource conflicts were observed table 1 and fig 5 4 4 2 1 type 1 conflicts over use of the tractor to plow land turmeric and marigold plots must be plowed during the same window 1 23 march and according to the same conditions rules since turmeric has a higher priority index than marigold i e the farmer prefers economically to grow turmeric than marigold the resource manager allocates the tractor to the turmeric plots first e g the first tractor plowing occurs on 10th 11th march for turmeric plots and 15th 16th march for marigold plots between turmeric plots the resource is randomly attributed for example for turmeric the first tractor plowing occurs on march 10th on farm 1 and march 11th on farm 2 and the second tractor plowing occurs on march 20th on farm 1 and on march 21st on farm 2 4 4 2 2 type 2 conflicts over use of female labor to weed turmeric since both turmeric plots were sown on the 13th of april they had the same time windows for weeding events 10 35 55 65 85 95 and 115 125 days after sowing for each weeding event the first farmer needed 28 male and 21 female laborers and the second farmer needed 14 male and 37 female laborers since the village had only 50 female laborers both farmers could not weed their plots at the same time the resource is randomly attributed 4 4 2 3 type 3 conflicts over use of both female and male labor to harvest turmeric the first farmer needed 20 male and 23 female laborers and the second farmer needed 35 male and 42 female laborers to harvest their turmeric plots since both female and male labor was limited at 50 and 40 laborers respectively one of the farmers had to delay harvest 5 discussion understanding farmers decision making processes and relationships with the biophysical system is necessary to understand farming system complexity at multiple scales namaste is a model able to simulate decision making processes and interactions between strategic tactic and operational decisions investment cropping system and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies the model proposes two main innovations i its decision model simulates farmers decision making processes by describing dynamic sequential decisions via adaptation to the biophysical environment and ii it couples decision economic biophysical and hydrological systems through an integrative approach to predict effects and spillover of human decisions on natural systems research increasingly considers farm management as a flexible and dynamic process in recent agricultural literature however consequences on long and short term farm organization are rarely considered even though they appear to influence farmers decision making greatly daydé et al 2014 indeed 70 of the articles reviewed in robert et al 2016a focused on only one stage of the decision adaptation at the strategic level for the entire farm or at the tactical level for the farm or plot some authors combined strategic and tactical decisions to consider the entire decision making process and adaptation of farmers dynamic programming is a dynamic model that allows the adaptation of strategic decisions according to adaptations made to tactical decisions it has been used to address strategic investment decisions reynaud 2009 duffy and taylor 1993 and to address tactic decisions about cropping systems hyytiäinen et al 2011 thomas 2003 other authors combined reactive formalisms with a recursive approach and static discrete stochastic programming approaches to describe the sequential decision making process from strategic decisions and adaptations to tactical decisions and adaptations mosnier et al 2009 belhouchette et al 2004 lescot et al 2011 we used the basic definition of le gal et al 2011 which divides a decision into a set of interconnected decisions made over time and at multiple spatial scales sequential and dynamic representation is particularly useful and appropriate for modeling entire processes for making strategic tactical and operational decisions risbey et al 1999 le gal et al 2011 we proposed in this paper to combine several formalisms within an integrated model in which strategic and tactical adaptations and decisions influence each other to model adaptive behavior within farmers decision making processes to model three stages of decision we combined economic decision rule and activity based models the integrative approach of coupling decision economic biophysical and hydrological models was necessary to model and quantify spatio temporal variability in water resources and interactions among groundwater agricultural practices and crop growth one difficulty in modeling these processes is combining independent models that were originally developed for specific purposes at different spatial and temporal scales kraucunas et al 2015 the biophysical model used in namaste was developed to simulate fixed practices such as sowing irrigation and harvest on one plot for one cropping season see brisson et al 1998 for more details on the stics model the hydrological model simulates groundwater dynamics of a large territory see tomer 2012 for more details on the ambhas model the decision model describes farmers decisions and practices on their farms meaningful model integration requires calibration and validation of the global model and consistency in the underlying system boundaries assumptions and scale of analysis of these diverse models in the namaste model as kling et al 2016 suggested it is necessary to develop bridge models that convert outputs of one model into inputs of another this approach enables atomic models to be connected at different scales and to operate together by downscaling outputs from watershed to field models as an example the borewell model is considered as a bridge model to convert the groundwater level which is a watershed output from the ambhas model to the available water for irrigation at the plot scale which is a field data used as an input for the field model stics and by upscaling outputs from field to watershed models e g outputs on drained water from the atomic stics models and pumped water from the atomic borewell models are used by the bridge model net recharge to compute the daily net recharge at the watershed scale that will be used as input by the watershed model ambhas this can ensure that component models are manageable and provide outputs at both field and watershed scales hibbard and janetos 2013 work on hard linked integration is still on going soft linking is a necessary starting point to test different modelling and linking approaches holz et al 2016 it ensures practicality transparency and learning with low initial investments in computer programming future work on hard linking will ensure efficiency scalability and control with automatized exchanges of information between models in this study operational validation was performed mainly by subjectively and qualitatively exploring model behavior sargent 2013 we developed a simulation model able to reproduce interactions between decisions investment and technical and processes resource management and biophysical under scenarios of climate change socio economic and water management policies nonetheless quantitative simulation results for the berambadi case study still have high uncertainty due to for instance simplification of some biophysical processes simplification due to the farmers typology or simplification due to the grid of climate calibrating and validating the namaste model is an important and time consuming step that is still underway 94 parameters are directly accessible in the coupled model and ambhas and stics have many internal parameters that require tedious calibration in our experimental model we simulated a virtual village composed we simulated a virtual village composed of two virtual farms that have access to groundwater in the same ambhas cell the watershed scale of the ambhas model was not used and considering only one cell of this model resulted in simplifying the watershed model by ignoring lateral water fluxes thus our model is still considered as a dynamic model for water management at the farm level and cannot be considered yet as an agent based model in future work we will consider the upscaling issue from the perspective of modeling the consequences of farmers adaptations to changes in climate and groundwater upscaling from the farm level to watershed regional and national levels is a common approach for studying system behavior and dynamics such as farm adaptations to climate change gibbons et al 2010 land use and land cover changes in response to climate change rounsevell et al 2014 and ecosystem changes in response to biotic and abiotic processes nash et al 2014 peters et al 2007 identified three types of scales fine one individual intermediate groups of individuals and broad large spatial extents such as the landscape region and planet the appropriate scale is defined by the research question or hypothesis and often requires upscaling or downscaling existing models gibbons et al 2010 the challenge of aggregation or upscaling is to determine which fine scale details matter most at intermediate or broad scales research questions differ according to the scale at the farm scale we focused on farmers decision making processes and their adaptation to uncertain changes e g climate and resource availability the watershed level requires exploring the influence of decision making on the groundwater table rather than the process of decision making itself and to consider interactions between individuals for shared resources at the watershed level relative trends are more important than absolute values for example at the watershed level we are more interested in the total amount of groundwater used for irrigation in the watershed than on individual farms in the agricultural literature there are few models dealing with both watershed and individual farm scale representations the shadoc model barreteau and bousquet 2000 the bali model lansing and et kremer 1994 and the sinuse model feuillette et al 2003 simulate water management at the watershed scale but are limited to a single scheme scale or to management units that are not the individual farmer but farmers irrigation groups or a villages interacting in use and management of the resource with decision making processes based on maximisation functions of farmers or groups or villages incomes according to their available resources the multi agent maelia s gaudou and sibertin blanc 2013 and catchscape s becu et al 2003 architectures aimed at being integrative spatially distributed and individual based in order to cope with complex and adaptive issues at the watershed scale in future work our upscaled model will provide a detail description of the farmer cognitive agents with decision making processes and adaptive behavior our model provides tools to analyze evaluate and optimize agronomic environmental and economic criteria we tested the model with a baseline scenario to simulate current farming practices in the berambadi watershed and predict influence of the latter on groundwater level in a virtual village composed of two farms modeling agricultural production scenarios can help stakeholders make decisions about regulations and resource restrictions and encourage new practices to recommend to farmers 6 conclusion we developed an original simulation model of a farming system that combines relevant principles highlighted in the scientific literature the model was initially developed to address critical issues of groundwater depletion and farming practices in a watershed in southwestern india its structure frameworks and formalisms can be applied to other agricultural contexts our application focused on water management in semi arid agricultural systems but the model can also be applied to other farming systems to confirm the reusability and robustness of the framework acknowledgement this study was funded by the indo french centre for the promotion of advanced research cefipra the inra flagship program on adaptation of agriculture and forest to climate change accaf di inter 2012 2 and accaf di 2013 3 the doctoral school of the university of toulouse edt and the observatory for environment research experimental tropical watersheds ore bvet http bvet obs mip fr we are grateful to all the experts farmers survey takers and trainees who helped implement the method we give special thanks to tony raveneau for his help with the computer implementation 
26454,species distribution modelling sdm was integrated in version 2 0 of the biodiversityr package released in 2012 ensemble habitat suitability is calculated as the weighted average of suitabilities predicted by different algorithms advanced options for sdm in the current version 2 8 4 of the package include tuning the best combination of the number and weights of models contributing to the ensemble suitability and calculating the absence presence threshold as the average or minimum of recommended threshold values algorithm specific suitability values can be transformed via generalized linear models with probit link so that they become more similar in range other options include reducing spatial sorting bias by selecting background locations in circular neighbourhoods and generating suitability maps that show the number of contributing models that predict species presence the approaches are illustrated for two species with open access point location data sets bradypus variegatus and thryothorus ludovicianus keywords species distribution model ensemble model spatial sorting bias r statistical language and environment ecological software biodiversityr package 1 introduction species distribution models sdms other names for these models include ecological niche models and habitat suitability models are widely used in ecological studies for an overview of the sdm framework see guisan and zimmermann 2000 guisan and thuiller 2005 elith and leathwick 2009 miller 2010 thuiller and munkemuller 2010 hijmans and elith 2016 recent developments in the calibration of sdms such as the application of machine learning algorithms elith et al 2006 wisz et al 2008 and the use of ensemble consensus procedures araújo and new 2007 marmion et al 2009 thuiller et al 2009 buisson et al 2010 luedeling et al 2014 trolle et al 2014 allow for the creation of more reliable habitat suitability maps when new potentially superior algorithms for sdm become available such as using zero inflated random variables in hybrid bayesian networks maldonado et al 2016 or maximizing the likelihood of species occurrence probability royle et al 2012 these can be easily integrated in the ensemble modelling framework since these powerful sdm methods have become available challenges in sdm have shifted to areas such the availability of point location data boakes et al 2010 feeley and silman 2011 duputié et al 2014 dealing with errors and bias in point location data sets hortal et al 2008 loiselle et al 2008 platts et al 2008 phillips et al 2009 lobo and tognelli 2011 syfert et al 2013 beck et al 2014 varela et al 2014 robertson et al 2016 and projecting across space and time dormann 2007 elith et al 2010 stanton et al 2012 braunisch et al 2013 baker et al 2016 werkowska et al 2016 the biodiversityr package kindt 2017 was initially developed to accompany a manual on the statistical analysis of biodiversity and community ecology data kindt and coe 2005 the package currently provides a graphical user interface for ordination cluster and diversity analysis using the vegan package oksanen et al 2017 ensemble approaches for sdm have been incorporated into biodiversityr version 2 0 that was released in december 2012 initially to make model calibration procedures more explicit than in the biomod package model formulae are available as arguments in the main function that calibrates the ensemble model and the contributing models whereas default formulae can be generated with function biodiversityr ensemble formulae a second reason was to build on functions offered by the dismo package hijmans et al 2015 such as the dismo threshold allowing a wider range of methods of transforming suitability into absence presence than were available in biomod dismo maxent dismo domain and dismo mahal functions these functions fit suitability based on the maximum entropy phillips et al 2006 domain carpenter et al 1993 and mahalanobis 1936 algorithms that were not available in biomod a third motivation was to address the challenge of developing r functions that create habitat suitability maps with limited user input during short training exercises or to familiarize users with model outputs a final reason was to provide support to doctoral and post doctoral sdm research ranjitkar et al 2014a 2016a 2014b van breugel et al 2015b 2015a 2016b since the introduction of sdm methods in biodiversityr the development of sdm methods has developed independently in biomod and biodiversityr whereas recently a new r package for sdm sdm was released naimi and araújo 2016 biodiversityr offers various unique methods of sdm whereas application of these methods is straightforward through a graphical user interface or by using functions with default argument settings 2 methods and features in a similar way to ensemble modelling approaches implemented in the biomod and sdm packages sdm functions in biodiversityr calculate ensemble suitability s e as a weighted average of suitabilities predicted by contributing models s i s e i w i s i i w i previous studies have shown that the consensus method based on weighted averages may significantly increase the accuracy of sdm marmion et al 2009 the current version of biodiversityr 2 8 4 released in 2017 allows the calibration of 23 candidate models that could contribute to the calculation of s e including maximum entropy models available via argument maxent main r calibration function of dismo maxent phillips et al 2006 elith et al 2011 hijmans et al 2015 maximum likelihood models maxlike maxlike maxlike chandler and royle 2013 royle et al 2012 two different implementations of boosted regression trees gbm and gbmstep gbm gbm and dismo gbm step friedman 2001 friedman et al 2001 elith et al 2008 ridgeway 2015 random forests rf randomforest randomforest breiman 2001 liaw and wiener 2012 stepwise generalized linear regression models glm and glmstep stats glm and mass stepaic mccullagh and nelder 1989 venables et al 2002 venables and ripley 2013 stepwise generalized additive models gam and gamstep gam gam and gam step gam hastie and tibshirani 1990 hastie 2013 generalized additive models with integrated smoothness estimation mgcv and mgcvfix mgcv gam wood 2011 2013 multivariate adaptive regression spline models earth earth earth friedman 1991 leathwick et al 2005 milborrow 2014 recursive partitioning and regression trees rpart rpart rpart breiman et al 1984 therneau et al 2014 artificial neural networks nnet nnet nnet venables et al 2002 ripley and venables 2013 flexible discriminant analysis fda mda fda hastie et al 1994 leisch et al 2013 two different implementations of support vector machine models svm and svme kernlab ksvm and e1071 svm karatzoglou et al 2013 meyer et al 2014 lasso or elastic net regularized generalized linear models glmnet glmnet glmnet friedman et al 2016 2010 two different implementations of the bioclim algorithm bioclim and bioclim o whereby bioclim o follows the original methodology more closely in predicting suitability as 0 0 5 or 1 0 dismo bioclim and biodiversityr ensemble bioclim nix 1986 booth et al 2014 hijmans et al 2015 the domain algorithm domain dismo domain carpenter et al 1993 and two different implementations of the mahalanobis algorithm mahal and mahal01 dismo mahal mahalanobis 1936 with default settings of the ensemble batch function table 1 the only inputs required from users to generate suitability maps are presence point locations and a rasterstack object with raster layers representing explanatory variables in the first step of the sdm procedure ensemble weights for the models are obtained by a 4 fold cross validation procedure whereby each of the four models are calibrated and tested with data not used for calibration hijmans 2012 van breugel et al 2015a ranjitkar et al 2016b and the ensemble weight is calculated as the average auc the area under the receiver operator curve a statistic commonly used to evaluate sd models bradley 1997 hijmans 2012 wisz et al 2008 jiménez valverde 2012 varela et al 2014 over the four cross validations with default settings presence and absence locations are randomly assigned to the four cross validation bins an alternative procedure argument get block is available whereby presence and absence locations are divided in four blocks created by lines of latitude and longitude that divide the locations as equally as possible a procedure that is expected to reduce spatial correlation between training and testing locations important for evaluating models that will transfer suitability across space or time muscarella et al 2014 although the auc is the most commonly used statistic to evaluate sdms hijmans 2012 various authors have criticised its use jiménez valverde 2012 documented strong correlation between the auc and the absence presence threshold that makes sensitivity the proportion of correctly predicted presence locations equal to specificity the proportion of correctly predicted absence locations as a consequence the auc may be equivalent to using a threshold that discriminates between predicted absence unsuitable habitat and predicted presence suitable habitat whereas the feature of avoiding the use of such absence presence threshold is the main argument in favour of the auc when the realized distribution of the species does not represent the full potential distribution of a species models with lower auc can produce habitat suitability maps that better represent the potential distribution therefore the auc is appropriate mainly for models of realized distributions jiménez valverde 2012 the auc will be larger for species with more restricted distributions therefore the statistic is expected to inflate when background locations are sampled from larger areas lobo et al 2008 hijmans 2012 despite these shortcomings the auc remains a valid measure of relative model performance for the same species assuming that presence locations are representative of suitable habitat and the same study area wisz et al 2008 as such auc values can be used to compare different models that are candidates to contribute to s e in the second step of the procedure models with auc values larger than 0 7 an auc threshold that is often used to identify good models hijmans 2012 are calibrated with the full set of presence and background point locations instead of the 75 subsets used for the 4 fold cross validations in the final step suitability maps are generated it is possible to modify default settings such as the number of cross validation steps argument k splits or how models with lower auc values are down weighed setting argument ensemble exponent results in weights calculated as aucensemble exponent a procedure suggested by hijmans and elith 2016 users can also substitute objects for contributing models opt not to use default formulae to calibrate contributing models or use other weights for the final step suitabilities predicted by the contributing models can be transformed by generalized linear models with probit link argument probit thus ensuring that all suitabilities are probability values the bioclim domain mahal and mahal01 algorithms do not provide probability values since species presence 1 or absence 0 are used as binary response variables for the transformations the transformation is expected to result in the ranges of suitability values rsv becoming more similar across contributing models a pattern that is investigated in the case studies differences in rsv introduce an effect that is similar to weights for example in situations where a model predicts suitability values that are on average half the suitability values predicted by a second model the actual weight of the first model in the calculated ensemble suitability s e will be considerably smaller as this effect is not necessarily linked to contributing model performance obtaining more similar rsv is a desirable feature for ensemble modelling another application of the probit transformation is to detect probable model overfitting in situations where the majority of training presence locations have transformed suitabilities equal to one and the majority of training absence locations have transformed suitabilities equal to zero these situations are similar to situations where an absence presence threshold results in perfect or near perfect separation of presence and absence suitabilities before the transformation was applied appendix c another unique feature ensemble strategy is that it is possible to obtain weights that correspond to the combination of parameter settings of ensemble best the number of models contributing to the ensemble ensemble min the minimum auc of models contributing to the ensemble and ensemble exponent weights are calculated as aucensemble exponent that results in the highest auc for the ensemble model argument vif max and function ensemble vif allow to select a subset of explanatory variables where no explanatory variable has a variance inflation factor vif fox and monette 1992 larger than a user defined threshold default 10 the subset is selected by an iterative procedure that removes the explanatory variable with highest vif at each iteration step yet another unique feature is an option to determine the threshold between absence and threshold suitabilities as the mean or minimum the latter results in a larger area of suitable habitat of recommended threshold values liu et al 2013 2005 argument ssb reduce is a unique feature whereby spatial sorting bias hijmans 2012 can be reduced by selecting background point locations for model evaluations in circular neighbourhoods of presence point locations the user can set the radius of neighbourhoods with argument circles d interpretation of evaluation strips elith et al 2005 is facilitated by the inclusion of boxplots that show the distribution of observed presence point locations within the range investigated by the evaluation strips with argument kml out suitability maps are created that can be opened in the google earth software in addition to suitability maps generated by the raster writeraster function it is possible to repeat the entire ensemble calibration and mapping procedure several times starting with a random allocation of presence and absence locations to the k subsets of the k fold cross validation procedure with a parameter setting for argument n ensembles that is larger than one in that case s e called consensus suitability in biodiversityr is calculated as the average of s e obtained in each ensemble run biodiversityr creates count suitability maps csms that show the number of contributing models that predict species presence count suitability models are based on model specific suitability maps that are transformed in predicted absence presence maps through absence presence thresholds liu et al 2013 2005 estimated for each contributing model by interpreting these csms users can identify areas of suitable habitat where agreement between contributing models is lowest and other areas where it is highest as predicted absence presence maps take into account the differences in rsv they are not sensitive to these differences however contributions of each model to the count suitability are equal and predictions by models with lower auc are thus not down weighed given possible undesired effects of differences in rsv interpretation of sdm results can be more reliable when users check where maps of s e and csms agree with each other suitability maps can be obtained relatively easily with r scripts modified from examples provided in the documentation of biodiversityr functions and in appendix a the easiest way that users can calibrate sdm and plot suitability maps is through the graphical user interface gui available from the biodiversityr menu of ensemble suitability modelling the gui interface of biodiversityr is generated by the r commander fox et al 2017 four submenus are available i the stacks of environmental layers menu to create and modify the rasterstack with explanatory variables ii the presence data set menu to create or select the presence point locations iii the absence data set menu to create or select the absence or background point locations and iv the ensemble suitability modelling menu to calibrate sdm plot suitability maps and plot evaluation strips the main gui of the ensemble suitability modelling submenu allows to set options for sdm calibration such as the selection of candidate models to contribute to the ensemble suitability the number of ensembles for the consensus suitability the maximum vif of explanatory variables and whether circular neighbourhoods should be created to potentially reduce spatial sorting bias 3 case study 1 ensemble suitability modelling of bradypus variegatus the dismo species distribution modeling package hijmans et al 2015 includes 116 presence point locations for bradypus variegatus a lowland sloth species phillips et al 2006 and eight bioclimatic booth et al 2014 nix 1986 raster layers at 30 arc minutes resolution for north and south america the subset of bioclimatic variables including bio5 the maximum temperature of the warmest month bio6 the minimum temperature of the coldest month bio16 the precipitation of the wettest quarter and bio17 the precipitation of the driest quarter were selected as explanatory variables for species distribution modelling bio16 had the highest vif of 2 52 maps of presence point locations are provided in appendix b ensemble models were fitted with the ensemble batch function part of the script used for this example is included in appendix a with argument settings of k splits 4 determine ensemble weights by a 4 fold cross validation procedure vif max 10 ensure that no explanatory variable has a variance inflation factor larger than 10 threshold method spec sens use the threshold between absence and presence suitabilities that corresponds to the highest sum of the true positive and true negative rates ensemble best 10 select the 10 models with highest auc and ensemble exponent c 1 3 select the ensemble model with highest auc from a set of ensembles where weights for contributing models are calculated as auc auc2 or auc3 argument settings of get block true or false and probit true or false were changed to create 10 ensembles argument n ensembles for each combination of k fold cross validation method and transformation method i random cross validation with no probit transformation rn ii random cross validation with probit transformation rp iii block cross validation with no probit transformation bn and iv block cross validation with probit transformation bn table 2 auc statistics multiplied by 100 and rank obtained by sorting models from highest to lowest average auc in cross validations obtained in different combinations of selecting subsets of k fold cross validations and transformation of suitability values results are based on 10 ensembles for each combination consensus maps depicting average s e showed that the choice of transformation or k fold cross validation method did not have a significant influence on the areas predicted to be suitable for bradypus variegatus although probit transformations resulted in larger areas with higher suitability values fig 1 the average auc of contributing and ensemble models the mean of auc values in four cross validation tests each for 10 ensembles were all above the threshold of 0 7 for good models hijmans 2012 table 2 the ensemble model either had the highest average auc rn and rp or an auc value that was less than 0 002 of the best contributing model bn and bp final ensemble models calibrated with all available presence and background point locations were above or just below the threshold of 0 9 for very good models thuiller 2003 heubes et al 2011 the minimum auc values were considerably smaller 0 7 for bn and bp suggesting unique environmental conditions in some blocks given that the ensemble model had among the highest auc values and generated similar suitability maps for rn rp bn and bp it appears therefore that the ensemble modelling framework provides a reliable suitability modelling approach that is robust against some differences in rsv and weights of contributing models the random forest model rf only ranked high for rn was included only once for the rp and never included for bn and bp the extremely high auc values 0 99 for the final rf and for calibration data suggest that this model overfitted the data probit transformations resulted in suitability values to become more similar in range fig 2 only the svm model had a median value below 0 5 with the probit transformations fig 2 however probit transformations did not completely remove differences in rsv median values were still lower for contributing models that had median values below 0 5 prior to the transformation the probit transformation suggested overfitting by rf as probability values were separated in values close to one for presence locations and values close to zero for background locations csms showed an influence of the cross validation and transformation method on the area where all 10 models agreed that the species was predicted to be present fig 3 rn had the smallest area where 10 models predicted presence whereas bn and bp had the largest areas for all csms areas where at least six models predicted presence corresponded well to areas predicted to be suitable in consensus maps the smallest areas for rn are a result from rf only predicting presence close to known presence point locations maps for rp bn and bp of 10 models agreeing on predicted species presence depict areas predicted to be suitable by gbm and gbmstep models that had final auc values above 0 94 as areas predicted to be suitable by gbm and gbmstep are in relatively close proximity of known presence locations it is possible that gbm and gbmstep overfit data to some degree albeit not as strongly as rf 4 case study 2 ensemble suitability modelling of thryothorus ludovicianus the maxlike package chandler and royle 2013 includes 4615 presence point locations derived from the north american breeding survey for thryothorus ludovicianus the carolina wren royle et al 2012 and four land cover variables at resolution of 25 km2 that represent the cover of mixed forest deciduous forest coniferous forest and grasslands all variables had a variance inflation factor that was smaller than 1 2 maps of presence point locations are provided in appendix b 40 ensemble models were fitted with the ensemble batch function part of the script used for this example is included in appendix a with the same argument settings as the first case study table 3 auc statistics multiplied by 100 and rank obtained by sorting models from highest to lowest average auc in cross validations obtained in different combinations of selecting subsets of k fold cross validations and transformation of suitability values results are based on 10 ensembles for each combination na indicates that final auc statistics were not available as the model did not contribute to the final ensemble the average auc of contributing and ensemble models the mean of auc values in four cross validation tests each for 10 ensembles were above the threshold of 0 7 for the random k fold method and below this threshold for the block method table 2 an indication that some blocks had different environmental conditions ensembles with block selection of k fold cross validation data sets resulted in the highest average auc for the ensemble model for random selection of k fold cross validation data sets ensemble models ranked third or fourth for average auc but were also less than 0 003 than the best contributing models consensus maps depicting average s e and csms all showed that the choice of transformation or k fold cross validation method did not have a significant influence on the areas predicted to be suitable figs 4 and 5 most of the areas suitable in s e maps corresponded to areas where all 10 models agreed that the species could be present the suitable areas in s e maps corresponded best to areas in csm maps where at least seven models predicted presence the effect of the probit transformation was that more similar rsv were obtained fig 6 5 conclusions biodiversityr provides a powerful and easy to use framework for sdm applications including several unique features such as tuning the best combination of the number and weights of models contributing to the s e transforming contributing model predictions by a glm with probit link and creating suitability maps that show the number of contributing models that predict species presence the graphical user interface facilitates user input the case studies showed that ensemble models either had the highest auc values or had auc values that were very close to those of the best models since selecting s e as the suitability predicted by the best candidate model is a valid ensemble modelling approach as well the results support the hypothesis that the ensemble framework generates better suitability maps results for the block cross validation method indicated that the framework will perform poorly with data sets that do not include locations in part of the suitable range however although probit transformations resulted in more similar rsv values in practice similar s e maps were obtained with or without probit transformations it appears therefore that the ensemble modelling framework could be robust against some differences in rsv the justification for the probit transformations could therefore be more theoretical than practical however it also appears that the probit transformation has the ability to filter out contributing models that seem to strongly overfit the data as the possible effects of differences in rsv are not known for new data sets best practice would either be to compare results obtained with and without probit transformations or to apply the probit transformation to reduce some possible effects of rsv applied to candidate models that do not generate probability values the probit transformation further ensures that s e can be interpreted as a weighted average of probability values csms are suitability maps that are based on absence presence transformations of contributing models csms provide an additional method of inferring suitability that is not sensitive to differences in rsv given that similar areas that are suitable for a species can be identified on maps a combination of s e maps and csms can increase the confidence in predictions of suitable areas where there are differences between areas where all models predict species presence and areas predicted to be suitable based on the s e stratified field studies could determine whether there are actual differences in the probability of encountering the target species outside and inside the zone where all models agree this article is the suggested citation for reporting sdm results obtained with the biodiversityr package software and data availability this manuscript provides a description of species distribution modelling options that are available in the biodiversityr package kindt 2017 available from https cran r project org package biodiversityr with file sizes ranging between 339 and 607 kb the package was developed for the r statistical environment that can be installed in linux mac os x and windows platforms r core team 2017 data sets point location data and raster data sets with explanatory variables are available from the dismo package hijmans et al 2015 and the maxlike package chandler and royle 2013 acknowledgements incorporation of sdm functions in biodiversityr was funded by various donors including donors contributing to the cgiar fund and the austrian development cooperation through the project titled threats to priority food tree species in burkina faso drivers of resource losses and mitigation measures project reference no 2011 01 inputs from colleagues from the fta research program are greatly appreciated especially suggestions from eike luedeling evert thomas paulo van breugel and maarten van zonneveld appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 009 
26454,species distribution modelling sdm was integrated in version 2 0 of the biodiversityr package released in 2012 ensemble habitat suitability is calculated as the weighted average of suitabilities predicted by different algorithms advanced options for sdm in the current version 2 8 4 of the package include tuning the best combination of the number and weights of models contributing to the ensemble suitability and calculating the absence presence threshold as the average or minimum of recommended threshold values algorithm specific suitability values can be transformed via generalized linear models with probit link so that they become more similar in range other options include reducing spatial sorting bias by selecting background locations in circular neighbourhoods and generating suitability maps that show the number of contributing models that predict species presence the approaches are illustrated for two species with open access point location data sets bradypus variegatus and thryothorus ludovicianus keywords species distribution model ensemble model spatial sorting bias r statistical language and environment ecological software biodiversityr package 1 introduction species distribution models sdms other names for these models include ecological niche models and habitat suitability models are widely used in ecological studies for an overview of the sdm framework see guisan and zimmermann 2000 guisan and thuiller 2005 elith and leathwick 2009 miller 2010 thuiller and munkemuller 2010 hijmans and elith 2016 recent developments in the calibration of sdms such as the application of machine learning algorithms elith et al 2006 wisz et al 2008 and the use of ensemble consensus procedures araújo and new 2007 marmion et al 2009 thuiller et al 2009 buisson et al 2010 luedeling et al 2014 trolle et al 2014 allow for the creation of more reliable habitat suitability maps when new potentially superior algorithms for sdm become available such as using zero inflated random variables in hybrid bayesian networks maldonado et al 2016 or maximizing the likelihood of species occurrence probability royle et al 2012 these can be easily integrated in the ensemble modelling framework since these powerful sdm methods have become available challenges in sdm have shifted to areas such the availability of point location data boakes et al 2010 feeley and silman 2011 duputié et al 2014 dealing with errors and bias in point location data sets hortal et al 2008 loiselle et al 2008 platts et al 2008 phillips et al 2009 lobo and tognelli 2011 syfert et al 2013 beck et al 2014 varela et al 2014 robertson et al 2016 and projecting across space and time dormann 2007 elith et al 2010 stanton et al 2012 braunisch et al 2013 baker et al 2016 werkowska et al 2016 the biodiversityr package kindt 2017 was initially developed to accompany a manual on the statistical analysis of biodiversity and community ecology data kindt and coe 2005 the package currently provides a graphical user interface for ordination cluster and diversity analysis using the vegan package oksanen et al 2017 ensemble approaches for sdm have been incorporated into biodiversityr version 2 0 that was released in december 2012 initially to make model calibration procedures more explicit than in the biomod package model formulae are available as arguments in the main function that calibrates the ensemble model and the contributing models whereas default formulae can be generated with function biodiversityr ensemble formulae a second reason was to build on functions offered by the dismo package hijmans et al 2015 such as the dismo threshold allowing a wider range of methods of transforming suitability into absence presence than were available in biomod dismo maxent dismo domain and dismo mahal functions these functions fit suitability based on the maximum entropy phillips et al 2006 domain carpenter et al 1993 and mahalanobis 1936 algorithms that were not available in biomod a third motivation was to address the challenge of developing r functions that create habitat suitability maps with limited user input during short training exercises or to familiarize users with model outputs a final reason was to provide support to doctoral and post doctoral sdm research ranjitkar et al 2014a 2016a 2014b van breugel et al 2015b 2015a 2016b since the introduction of sdm methods in biodiversityr the development of sdm methods has developed independently in biomod and biodiversityr whereas recently a new r package for sdm sdm was released naimi and araújo 2016 biodiversityr offers various unique methods of sdm whereas application of these methods is straightforward through a graphical user interface or by using functions with default argument settings 2 methods and features in a similar way to ensemble modelling approaches implemented in the biomod and sdm packages sdm functions in biodiversityr calculate ensemble suitability s e as a weighted average of suitabilities predicted by contributing models s i s e i w i s i i w i previous studies have shown that the consensus method based on weighted averages may significantly increase the accuracy of sdm marmion et al 2009 the current version of biodiversityr 2 8 4 released in 2017 allows the calibration of 23 candidate models that could contribute to the calculation of s e including maximum entropy models available via argument maxent main r calibration function of dismo maxent phillips et al 2006 elith et al 2011 hijmans et al 2015 maximum likelihood models maxlike maxlike maxlike chandler and royle 2013 royle et al 2012 two different implementations of boosted regression trees gbm and gbmstep gbm gbm and dismo gbm step friedman 2001 friedman et al 2001 elith et al 2008 ridgeway 2015 random forests rf randomforest randomforest breiman 2001 liaw and wiener 2012 stepwise generalized linear regression models glm and glmstep stats glm and mass stepaic mccullagh and nelder 1989 venables et al 2002 venables and ripley 2013 stepwise generalized additive models gam and gamstep gam gam and gam step gam hastie and tibshirani 1990 hastie 2013 generalized additive models with integrated smoothness estimation mgcv and mgcvfix mgcv gam wood 2011 2013 multivariate adaptive regression spline models earth earth earth friedman 1991 leathwick et al 2005 milborrow 2014 recursive partitioning and regression trees rpart rpart rpart breiman et al 1984 therneau et al 2014 artificial neural networks nnet nnet nnet venables et al 2002 ripley and venables 2013 flexible discriminant analysis fda mda fda hastie et al 1994 leisch et al 2013 two different implementations of support vector machine models svm and svme kernlab ksvm and e1071 svm karatzoglou et al 2013 meyer et al 2014 lasso or elastic net regularized generalized linear models glmnet glmnet glmnet friedman et al 2016 2010 two different implementations of the bioclim algorithm bioclim and bioclim o whereby bioclim o follows the original methodology more closely in predicting suitability as 0 0 5 or 1 0 dismo bioclim and biodiversityr ensemble bioclim nix 1986 booth et al 2014 hijmans et al 2015 the domain algorithm domain dismo domain carpenter et al 1993 and two different implementations of the mahalanobis algorithm mahal and mahal01 dismo mahal mahalanobis 1936 with default settings of the ensemble batch function table 1 the only inputs required from users to generate suitability maps are presence point locations and a rasterstack object with raster layers representing explanatory variables in the first step of the sdm procedure ensemble weights for the models are obtained by a 4 fold cross validation procedure whereby each of the four models are calibrated and tested with data not used for calibration hijmans 2012 van breugel et al 2015a ranjitkar et al 2016b and the ensemble weight is calculated as the average auc the area under the receiver operator curve a statistic commonly used to evaluate sd models bradley 1997 hijmans 2012 wisz et al 2008 jiménez valverde 2012 varela et al 2014 over the four cross validations with default settings presence and absence locations are randomly assigned to the four cross validation bins an alternative procedure argument get block is available whereby presence and absence locations are divided in four blocks created by lines of latitude and longitude that divide the locations as equally as possible a procedure that is expected to reduce spatial correlation between training and testing locations important for evaluating models that will transfer suitability across space or time muscarella et al 2014 although the auc is the most commonly used statistic to evaluate sdms hijmans 2012 various authors have criticised its use jiménez valverde 2012 documented strong correlation between the auc and the absence presence threshold that makes sensitivity the proportion of correctly predicted presence locations equal to specificity the proportion of correctly predicted absence locations as a consequence the auc may be equivalent to using a threshold that discriminates between predicted absence unsuitable habitat and predicted presence suitable habitat whereas the feature of avoiding the use of such absence presence threshold is the main argument in favour of the auc when the realized distribution of the species does not represent the full potential distribution of a species models with lower auc can produce habitat suitability maps that better represent the potential distribution therefore the auc is appropriate mainly for models of realized distributions jiménez valverde 2012 the auc will be larger for species with more restricted distributions therefore the statistic is expected to inflate when background locations are sampled from larger areas lobo et al 2008 hijmans 2012 despite these shortcomings the auc remains a valid measure of relative model performance for the same species assuming that presence locations are representative of suitable habitat and the same study area wisz et al 2008 as such auc values can be used to compare different models that are candidates to contribute to s e in the second step of the procedure models with auc values larger than 0 7 an auc threshold that is often used to identify good models hijmans 2012 are calibrated with the full set of presence and background point locations instead of the 75 subsets used for the 4 fold cross validations in the final step suitability maps are generated it is possible to modify default settings such as the number of cross validation steps argument k splits or how models with lower auc values are down weighed setting argument ensemble exponent results in weights calculated as aucensemble exponent a procedure suggested by hijmans and elith 2016 users can also substitute objects for contributing models opt not to use default formulae to calibrate contributing models or use other weights for the final step suitabilities predicted by the contributing models can be transformed by generalized linear models with probit link argument probit thus ensuring that all suitabilities are probability values the bioclim domain mahal and mahal01 algorithms do not provide probability values since species presence 1 or absence 0 are used as binary response variables for the transformations the transformation is expected to result in the ranges of suitability values rsv becoming more similar across contributing models a pattern that is investigated in the case studies differences in rsv introduce an effect that is similar to weights for example in situations where a model predicts suitability values that are on average half the suitability values predicted by a second model the actual weight of the first model in the calculated ensemble suitability s e will be considerably smaller as this effect is not necessarily linked to contributing model performance obtaining more similar rsv is a desirable feature for ensemble modelling another application of the probit transformation is to detect probable model overfitting in situations where the majority of training presence locations have transformed suitabilities equal to one and the majority of training absence locations have transformed suitabilities equal to zero these situations are similar to situations where an absence presence threshold results in perfect or near perfect separation of presence and absence suitabilities before the transformation was applied appendix c another unique feature ensemble strategy is that it is possible to obtain weights that correspond to the combination of parameter settings of ensemble best the number of models contributing to the ensemble ensemble min the minimum auc of models contributing to the ensemble and ensemble exponent weights are calculated as aucensemble exponent that results in the highest auc for the ensemble model argument vif max and function ensemble vif allow to select a subset of explanatory variables where no explanatory variable has a variance inflation factor vif fox and monette 1992 larger than a user defined threshold default 10 the subset is selected by an iterative procedure that removes the explanatory variable with highest vif at each iteration step yet another unique feature is an option to determine the threshold between absence and threshold suitabilities as the mean or minimum the latter results in a larger area of suitable habitat of recommended threshold values liu et al 2013 2005 argument ssb reduce is a unique feature whereby spatial sorting bias hijmans 2012 can be reduced by selecting background point locations for model evaluations in circular neighbourhoods of presence point locations the user can set the radius of neighbourhoods with argument circles d interpretation of evaluation strips elith et al 2005 is facilitated by the inclusion of boxplots that show the distribution of observed presence point locations within the range investigated by the evaluation strips with argument kml out suitability maps are created that can be opened in the google earth software in addition to suitability maps generated by the raster writeraster function it is possible to repeat the entire ensemble calibration and mapping procedure several times starting with a random allocation of presence and absence locations to the k subsets of the k fold cross validation procedure with a parameter setting for argument n ensembles that is larger than one in that case s e called consensus suitability in biodiversityr is calculated as the average of s e obtained in each ensemble run biodiversityr creates count suitability maps csms that show the number of contributing models that predict species presence count suitability models are based on model specific suitability maps that are transformed in predicted absence presence maps through absence presence thresholds liu et al 2013 2005 estimated for each contributing model by interpreting these csms users can identify areas of suitable habitat where agreement between contributing models is lowest and other areas where it is highest as predicted absence presence maps take into account the differences in rsv they are not sensitive to these differences however contributions of each model to the count suitability are equal and predictions by models with lower auc are thus not down weighed given possible undesired effects of differences in rsv interpretation of sdm results can be more reliable when users check where maps of s e and csms agree with each other suitability maps can be obtained relatively easily with r scripts modified from examples provided in the documentation of biodiversityr functions and in appendix a the easiest way that users can calibrate sdm and plot suitability maps is through the graphical user interface gui available from the biodiversityr menu of ensemble suitability modelling the gui interface of biodiversityr is generated by the r commander fox et al 2017 four submenus are available i the stacks of environmental layers menu to create and modify the rasterstack with explanatory variables ii the presence data set menu to create or select the presence point locations iii the absence data set menu to create or select the absence or background point locations and iv the ensemble suitability modelling menu to calibrate sdm plot suitability maps and plot evaluation strips the main gui of the ensemble suitability modelling submenu allows to set options for sdm calibration such as the selection of candidate models to contribute to the ensemble suitability the number of ensembles for the consensus suitability the maximum vif of explanatory variables and whether circular neighbourhoods should be created to potentially reduce spatial sorting bias 3 case study 1 ensemble suitability modelling of bradypus variegatus the dismo species distribution modeling package hijmans et al 2015 includes 116 presence point locations for bradypus variegatus a lowland sloth species phillips et al 2006 and eight bioclimatic booth et al 2014 nix 1986 raster layers at 30 arc minutes resolution for north and south america the subset of bioclimatic variables including bio5 the maximum temperature of the warmest month bio6 the minimum temperature of the coldest month bio16 the precipitation of the wettest quarter and bio17 the precipitation of the driest quarter were selected as explanatory variables for species distribution modelling bio16 had the highest vif of 2 52 maps of presence point locations are provided in appendix b ensemble models were fitted with the ensemble batch function part of the script used for this example is included in appendix a with argument settings of k splits 4 determine ensemble weights by a 4 fold cross validation procedure vif max 10 ensure that no explanatory variable has a variance inflation factor larger than 10 threshold method spec sens use the threshold between absence and presence suitabilities that corresponds to the highest sum of the true positive and true negative rates ensemble best 10 select the 10 models with highest auc and ensemble exponent c 1 3 select the ensemble model with highest auc from a set of ensembles where weights for contributing models are calculated as auc auc2 or auc3 argument settings of get block true or false and probit true or false were changed to create 10 ensembles argument n ensembles for each combination of k fold cross validation method and transformation method i random cross validation with no probit transformation rn ii random cross validation with probit transformation rp iii block cross validation with no probit transformation bn and iv block cross validation with probit transformation bn table 2 auc statistics multiplied by 100 and rank obtained by sorting models from highest to lowest average auc in cross validations obtained in different combinations of selecting subsets of k fold cross validations and transformation of suitability values results are based on 10 ensembles for each combination consensus maps depicting average s e showed that the choice of transformation or k fold cross validation method did not have a significant influence on the areas predicted to be suitable for bradypus variegatus although probit transformations resulted in larger areas with higher suitability values fig 1 the average auc of contributing and ensemble models the mean of auc values in four cross validation tests each for 10 ensembles were all above the threshold of 0 7 for good models hijmans 2012 table 2 the ensemble model either had the highest average auc rn and rp or an auc value that was less than 0 002 of the best contributing model bn and bp final ensemble models calibrated with all available presence and background point locations were above or just below the threshold of 0 9 for very good models thuiller 2003 heubes et al 2011 the minimum auc values were considerably smaller 0 7 for bn and bp suggesting unique environmental conditions in some blocks given that the ensemble model had among the highest auc values and generated similar suitability maps for rn rp bn and bp it appears therefore that the ensemble modelling framework provides a reliable suitability modelling approach that is robust against some differences in rsv and weights of contributing models the random forest model rf only ranked high for rn was included only once for the rp and never included for bn and bp the extremely high auc values 0 99 for the final rf and for calibration data suggest that this model overfitted the data probit transformations resulted in suitability values to become more similar in range fig 2 only the svm model had a median value below 0 5 with the probit transformations fig 2 however probit transformations did not completely remove differences in rsv median values were still lower for contributing models that had median values below 0 5 prior to the transformation the probit transformation suggested overfitting by rf as probability values were separated in values close to one for presence locations and values close to zero for background locations csms showed an influence of the cross validation and transformation method on the area where all 10 models agreed that the species was predicted to be present fig 3 rn had the smallest area where 10 models predicted presence whereas bn and bp had the largest areas for all csms areas where at least six models predicted presence corresponded well to areas predicted to be suitable in consensus maps the smallest areas for rn are a result from rf only predicting presence close to known presence point locations maps for rp bn and bp of 10 models agreeing on predicted species presence depict areas predicted to be suitable by gbm and gbmstep models that had final auc values above 0 94 as areas predicted to be suitable by gbm and gbmstep are in relatively close proximity of known presence locations it is possible that gbm and gbmstep overfit data to some degree albeit not as strongly as rf 4 case study 2 ensemble suitability modelling of thryothorus ludovicianus the maxlike package chandler and royle 2013 includes 4615 presence point locations derived from the north american breeding survey for thryothorus ludovicianus the carolina wren royle et al 2012 and four land cover variables at resolution of 25 km2 that represent the cover of mixed forest deciduous forest coniferous forest and grasslands all variables had a variance inflation factor that was smaller than 1 2 maps of presence point locations are provided in appendix b 40 ensemble models were fitted with the ensemble batch function part of the script used for this example is included in appendix a with the same argument settings as the first case study table 3 auc statistics multiplied by 100 and rank obtained by sorting models from highest to lowest average auc in cross validations obtained in different combinations of selecting subsets of k fold cross validations and transformation of suitability values results are based on 10 ensembles for each combination na indicates that final auc statistics were not available as the model did not contribute to the final ensemble the average auc of contributing and ensemble models the mean of auc values in four cross validation tests each for 10 ensembles were above the threshold of 0 7 for the random k fold method and below this threshold for the block method table 2 an indication that some blocks had different environmental conditions ensembles with block selection of k fold cross validation data sets resulted in the highest average auc for the ensemble model for random selection of k fold cross validation data sets ensemble models ranked third or fourth for average auc but were also less than 0 003 than the best contributing models consensus maps depicting average s e and csms all showed that the choice of transformation or k fold cross validation method did not have a significant influence on the areas predicted to be suitable figs 4 and 5 most of the areas suitable in s e maps corresponded to areas where all 10 models agreed that the species could be present the suitable areas in s e maps corresponded best to areas in csm maps where at least seven models predicted presence the effect of the probit transformation was that more similar rsv were obtained fig 6 5 conclusions biodiversityr provides a powerful and easy to use framework for sdm applications including several unique features such as tuning the best combination of the number and weights of models contributing to the s e transforming contributing model predictions by a glm with probit link and creating suitability maps that show the number of contributing models that predict species presence the graphical user interface facilitates user input the case studies showed that ensemble models either had the highest auc values or had auc values that were very close to those of the best models since selecting s e as the suitability predicted by the best candidate model is a valid ensemble modelling approach as well the results support the hypothesis that the ensemble framework generates better suitability maps results for the block cross validation method indicated that the framework will perform poorly with data sets that do not include locations in part of the suitable range however although probit transformations resulted in more similar rsv values in practice similar s e maps were obtained with or without probit transformations it appears therefore that the ensemble modelling framework could be robust against some differences in rsv the justification for the probit transformations could therefore be more theoretical than practical however it also appears that the probit transformation has the ability to filter out contributing models that seem to strongly overfit the data as the possible effects of differences in rsv are not known for new data sets best practice would either be to compare results obtained with and without probit transformations or to apply the probit transformation to reduce some possible effects of rsv applied to candidate models that do not generate probability values the probit transformation further ensures that s e can be interpreted as a weighted average of probability values csms are suitability maps that are based on absence presence transformations of contributing models csms provide an additional method of inferring suitability that is not sensitive to differences in rsv given that similar areas that are suitable for a species can be identified on maps a combination of s e maps and csms can increase the confidence in predictions of suitable areas where there are differences between areas where all models predict species presence and areas predicted to be suitable based on the s e stratified field studies could determine whether there are actual differences in the probability of encountering the target species outside and inside the zone where all models agree this article is the suggested citation for reporting sdm results obtained with the biodiversityr package software and data availability this manuscript provides a description of species distribution modelling options that are available in the biodiversityr package kindt 2017 available from https cran r project org package biodiversityr with file sizes ranging between 339 and 607 kb the package was developed for the r statistical environment that can be installed in linux mac os x and windows platforms r core team 2017 data sets point location data and raster data sets with explanatory variables are available from the dismo package hijmans et al 2015 and the maxlike package chandler and royle 2013 acknowledgements incorporation of sdm functions in biodiversityr was funded by various donors including donors contributing to the cgiar fund and the austrian development cooperation through the project titled threats to priority food tree species in burkina faso drivers of resource losses and mitigation measures project reference no 2011 01 inputs from colleagues from the fta research program are greatly appreciated especially suggestions from eike luedeling evert thomas paulo van breugel and maarten van zonneveld appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 009 
