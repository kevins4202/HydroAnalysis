index,text
26445,news media plays an important role in shaping public opinions and attitudes about the environment tracking and analyzing media coverage can provide insight into public exposure to narratives that impact resource consumption environmental behavior and emergency response which can help to inform model development or provide additional model inputs this paper presents articulate an open source flexible tool for discovering compiling and quantifying newspaper coverage on a user specified topic articulate is written in python and interfaces with google custom search engine api we demonstrate the tool s application and validate its performance on two case studies of news media coverage in the new york times about drought in california and flooding in houston texas in recent years our results show that articulate can generate data similar to or better than proprietary databases thus articulate can help researchers and environmental managers gain important insights to better understand and quantify changing socio environmental dynamics keywords news media coverage python google custom search engine api search tool software availability name of software articulate developers nicholas roby and newsha ajami contact address 473 via ortega room 204 stanford ca 94306 telephone 650 724 8162 email address nickroby12 gmail com and newsha stanford edu year first available 2017 hardware required personal computer internet access software required python 2 7 availability https github com stanford urban water policy innovation articulate cost 0 program language python program size 12 kb 1 introduction there is increasing recognition that human behavior plays an integral role in environmental planning decisions lund 2015 sovacool et al 2015 behavior has thus emerged as a central feature of many new environmental modeling methods and applications for example in recent years scientists and engineers have attempted to model human actions and reactions within hydrologic systems garcia et al 2016 gonzales and ajami 2017 noël and cai 2017 when predicting energy demand ma et al 2009 wilson and dowlatabadi 2007 as inputs into land use change projections lauf et al 2012 and for developing emergency response scenarios during extreme events such as floods and wildfires giordano et al 2017 nara et al 2017 assumptions about human behavior can have significant effects on the accuracy and results of models that aim to couple human and natural systems noël and cai 2017 sun et al 2016 and as a result many different approaches for modeling human environmental interactions have emerged one method for understanding human behavior is to measure issue salience public awareness and education are important drivers of environmental resource consumption and conservation stern 1976 emergency response behavior du et al 2017 mccaffrey et al 2017 and attitudes and beliefs king et al 2017 both social media du et al 2017 and news media treuer et al 2017 troy et al 2015 have been recognized as vehicles for measuring community exposure and sensitivity to environmental topics and events in this paper we focus on news media one important method of influence ball rokeach and defleur 1976 boykoff and boykoff 2007 sampei and aoyagi usui 2009 that has been relatively unexplored in the modeling of environmental scenarios quantifying and analyzing coverage of environmental topics such as droughts floods wildfire clean energy climate change or other themes can provide deep insight into awareness levels of the public which can then impact decision making analyzing news media coverage can also be used to retroactively evaluate the most salient events surrounding a topic boykoff and boykoff 2007 as environmental news coverage is often driven by specific events bolsen 2011 which are believed to reflect periods of public awareness this approach of creating news media time series and analyzing coverage has been used in previous research to provide insight into public exposure to environmental topics akerlof et al 2012 crow et al 2016 ruiz sinoga and león gross 2013 treuer et al 2017 quantitatively researchers have examined news media to develop sustainability indicators rivera et al 2014 validate flooding models smith et al 2012 and as inputs into water demand models quesnel and ajami 2017 these types of multidisciplinary modeling approaches are likely to be used more frequently in the future current tools for evaluating and quantifying news media coverage include proprietary databases such as proquest and lexisnexis bolsen 2011 wei et al 2015 these database tools fit with a graphical user interface gui allow the user to query from a selection of news media sources during specific time periods and for specific keyword s using query language these tools then extract classify represent and store various pieces of information from each source however database tools can be inflexible and in some cases may lack the temporal and source coverage required for a full and proper analysis most limiting is the propriety nature of these databases that require expensive subscriptions while free alternatives are available by performing manual searches in a specific newspaper web repository or by using existing newspaper specific api tools such data collection methods can be labor intensive time consuming and limited to specific news sources that may not be representative enough for a comprehensive analysis these tools which were built for different needs and objectives can also produce results in a format that is not exportable and therefore incompatible with quantitative analyses and modeling applications thus we created a new instrument articulate as a compliment to current news media discovery tools articulate is written in python and interfaces with the google custom search engine cse api to provide a free flexible customizable tool with functional outputs additionally articulate can offer equal and sometimes greater data coverage than produced by comparable proprietary databases this tool can be used to gain insight into public exposure to a topic of interest such as environmental issues as well as to evaluate the most important events related to that topic in this paper we first describe the functionality and methodological details of the software package then we validate the tool using two case studies of news media coverage of drought in california and flooding in houston texas to demonstrate the algorithm s performance and applicability 2 articulate software package 2 1 basic functionality the articulate software package uses existing open source application program interface api tools to search through online content and news article databases collect content of interest tally search results and generate a database of articles with pertinent information for further qualitative and quantitative analysis the general process of the algorithm is shown in fig 1 the articulate algorithm is written in python and uses the google cse api to submit queries web searches programmatically into the google search bar step through multiple pages of results and scrape only relevant news articles from defined news sources for specific keywords over a user defined time window in the process articulate filters out ads sidebars commentaries and images typically found in news websites thus eliminating sources of confounding information and returning only relevant results the tool allows users more flexibility and control over the queries and resulting outputs than existing proprietary applications while some familiarity with the python programming language and the google cse api is required for initial setup the articulate package includes a gui to facilitate its use in the following sections we present articulate as an accessible algorithm that lowers many of the barriers a user could face when trying to implement google cse api applications at socially and environmentally relevant scales articulate is open source and is available as a github repository at https github com stanford urban water policy innovation articulate 2 2 software environment articulate uses the python programing language a freely available language with extensive capabilities that runs on both windows and unix like platforms python software foundation 2016 articulate is written in python version 2 7 with the following modules csv py time py datetime py googleapiclient discovery py ast py numpy py pandas py sys py dateutil py cookiejar py urllib py urllib2 py and tkinter py google cse api client requires internet access for use and the user must first create an account with google cse api client to obtain an api developer key while also setting up their custom search engine with the parameters relevant to their needs see https developers google com custom search docs overview for more information 2 3 google cse api client the backbone of articulate is the google cse api client a web searching tool developed by google that allows the user to submit any google search programmatically google 2017 it performs like google s search bar returning the same page of results ten at a time that would be achieved had the search been executed within google s web search bar the information within each result contains items which can include title of the result media type i e video article etc author short excerpt content keywords date of publication and various other attributes the information is extracted and stored within the database specified by the user in the same way that the user steps through various pages of results when searching within google s search bar the user must also step through various pages when using the google cse api tool the flexible and dynamic nature of the cse tool allows for an array of input parameters the user has the capability to search various websites by date by quoted content by an exclusive query and other functions the functionality of this tool allows the user to step through ten pages of the same search up to the 100th result each query submission can retrieve up to 10 results and each developer key gets 100 queries a day for free after that the google developer apis are subject to a fee for each 1 000 queries used in a day and the user can submit up to 10 000 queries a day with this method we address these search retrieval limitations in our algorithm by developing a time step method described in section 2 6 below to cycle through the queries and results 2 4 software inputs articulate requires a set of comma separated values csv and string inputs there is one distinct csv input a csv file that contains the specific news websites to be searched and the structure of the dictionary used to access each website specific metadata string inputs include 1 the name of the csv file in which the output database should be stored 2 the reference date which defines the start point in the time period of the search inquiry 3 the date step size the time interval to be used in the representation of results 4 the number of developer keys needed 5 the developer key s 6 search term s 7 or term s defined by google as additional search terms to check for in a document where each document in the search results must contain at least one of the additional search terms each search term can have multiple or terms and or terms are defined separately for each search term and 8 include term s defined by google as exactterms which must be found in each result in addition to the initial search term it should be noted that all terms in the query are case sensitive search or and include terms 2 5 software outputs articulate produces two types of spreadsheet outputs the first kind of output are dataframes which contain the number of articles published by each source for each search these dataframes report tallied results counting the number of desired articles in each time interval e g number of articles each day month or year one dataframe is produced for each search term the second output is a database file which contains identifying information for each article with the following headers site search title day year media type and further info the database file contains both hits and misses hits are articles meeting all query criteria which are counted in the tally database misses should be manually removed from the database and are identified by incomplete information or reported errors in the output file one database is produced for each query and all search term results are included in the single database 2 6 software algorithm articulate uses a time step approach to address the size limitations of google cse queries while capturing all possible search results the algorithm searches within sub windows of a pre defined time period cycling through each sub window until the entire time period of interest has been analyzed the user inputs a reference date which defines the time range of the analysis e g a start date ten years ago until the present and a time step size in days months or years the step size is determined through trial and error a non trivial task as the step size must be of a fine enough resolution to find relevant articles while also not being excessively small that it makes inefficient use of limited queries generally a step size that produces between 10 and 100 results in every step is preferred once the time step and reference date have been established the user inputs the developer key s the csv input file and the query terms the search include and or terms a python pandas series is created which contains dates that span the entire date range the program then follows multiple loops to step through each search term for each media source and for each step window within the loop articulate configures the time series to include each sub window in the total time frame thereby creating a timeline of all the sub windows for the entire period that will be analyzed each query submission returns 10 results if the query submission retrieves no results the program moves to the next step window if the total query results for the specified sub window are greater than 100 the program notifies the user that some results were missed in the time step designation and attempts to adjust to catch the missing results however this is not a robust method of gaining results as the google cse api may skip dates when delivering the result to mitigate against this error the user should attempt to create a step size that captures all results desired by performing a trial and error run on a subset of the timeframe until a time step is found that produces the ideal 10 100 results as the program loops through each result obtained in a query it extracts associated information including date title newspaper source type of result etc in the process the results are filtered to determine if the result type result content and date are consistent with user specifications one important contribution of articulate is that it filters all banners comments advertisements images and other non article results produced by google cse api as well as duplicates and irrelevant results to do this the algorithm performs four different checks on each result 1 check for article type articulate attempts to obtain the result type which could include article banner comment advertisement image video and more for this process articulate calls on the site specific metadata to read the query submission results and return its type label if the extraction methods are unsuccessful and the article type cannot be verified articulate will report an error in the output database 2 check for content relevance if the result type is confirmed as an article articulate attempts to extract the url from the results the url is then used to read the html file representing the news article if the query the combination of search or and include terms is found in the article html then the article is deemed relevant and tallied if not then relevance cannot be confirmed and articulate will report an error in the output database in the case that the url cannot be found the article is both stored and tallied but relevance cannot be confirmed and is reported as unchecked it is important to note that this process only checks whether relevant keywords are present in the article and cannot assure that those keywords are used in a relevant context 3 check for duplicate titles as articulate tallies relevant articles it extracts the article titles from the results if the article title cannot be found in the dictionary structure of the result articulate reports it as an error article titles that can be extracted are stored for each new article found the algorithm checks the database of stored titles to confirm the result is not a duplicate 4 check for time frame relevance articulate extracts the article date from the results converts it to a standard python datetime format and confirms whether the date falls within the user defined time frame if the date cannot be extracted or is in an unrecognized format the result cannot be tallied because the date is unknown conversely if the date is successfully extracted the result is tallied in its respective time sub window and stored in the database if all four checks are satisfactory the results are stored and counted in the tally file if one or more of the checks is unsatisfactory the article will not be tallied in the dataframe organized by media source search query and date and will be reported as an error in the database because articulate goes through a mechanistic process of looking for relevant information in the form of specific keywords and specific dates in news articles there may be cases where results are reported despite being irrelevant e g a sports team in a winning drought a flood of support for charity users should check articulate output results for relevance as needed users querying proprietary databases can also face this challenge to address this obstacle some researchers have used sampling and coding methods treuer et al 2017 wei et al 2015 while others have manually checked all articles for relevance crow et al 2016 quesnel and ajami 2017 3 example applications 3 1 case study descriptions we tested articulate using two case studies of extreme environmental events 1 drought in california and 2 flooding in houston texas these case studies were chosen as they represent different kinds of events where researchers and environmental managers could benefit from a better understanding of public awareness and news media exposure droughts and floods are both extreme events with important societal and environmental implications but they have different spatial and temporal characteristics that may drive news coverage dynamics community response and adaptation measures di baldassarre et al 2017 1 california drought we used articulate to search for news articles containing the term california drought or a combination of california drought and water related terms such as water snow or rain over the period january 2005 through august 2016 this time period is useful for analysis of water use behavior changes related to media coverage as it encompasses two distinct drought periods 2007 2009 and the historic drought from 2012 2016 as well as a reference pre drought period 2005 2007 2 houston flooding for our second case study we used articulate to extract newspaper articles containing the terms houston flood and water related terms such as water rain or rainfall from january 2013 through october 2017 this is an interesting period of analysis because it captures both a historic flooding event due to hurricane harvey in august 2017 as well as previous smaller but still extreme flooding incidents in recent years to ensure that we could capture these previous flood events for comparison we did not include the terms harvey or hurricane in our search 3 2 approach for validation purposes we show the results of performing our case study searches in the new york times using various tools while articulate can access any online newspaper we chose the new york times because it is a highly circulated newspaper with a comprehensive science section clark 2006 that also has its own api we validated the results obtained using articulate by comparing the software outputs to similar searches using proquest lexisnexis the new york times api see http developer nytimes com for more information and a manual search in the new york times online news repository we used three different methods to evaluate search results first we examined search results from all five methods described above for a sample of four semi randomly chosen months within each case study second we compared time series of search results between articulate and proquest for the entire period of interest looking especially at trends and peaks third we looked at the distributions of search results from articulate and proquest for the entire time frames of interest for each topic 4 results and discussion fig 2 shows a comparison of the tallied results for a sample of semi randomly chosen months over the periods of analysis in each case study these months were chosen to represent the temporal characteristics of our two case studies and to demonstrate how the results obtained with articulate compare to other news search tools in periods of low and high media coverage as shown in this comparison the outputs of the articulate search compare well to similar searches with proquest and lexisnexis and do not seem to consistently under or over predict results when compared to the new york times directly in addition we performed a manual comparison between the results obtained by each of the five search methods to provide insights on the nature of the differences between articulate and other tools in the case study of drought in california fig 2a the most significant differences are in april 2015 a period of high media coverage where the number of hits captured by articulate is comparable to proquest but considerably lower than lexisnexis the new york times api and a manual search looking at the titles of articles captured by these different methods we found that the larger volume reported in the new york times website corresponds to some relevant articles that the other search tools missed but also several irrelevant articles or different media types such as a your weekly news quiz photo stories of the drought and a few sports articles reporting on winning droughts lexisnexis results which are closer to the new york times results in this period also captured some of these different media types and irrelevant results similarly in the case study of floods in houston fig 2b the main differences between results are in the period of highest media coverage in september 2017 unlike the first case study in these results articulate tallied more articles than any of the other tools a manual assessment of these results indicated that all of the articles captured by articulate in this period were indeed relevant news articles with titles related to hurricane harvey or containing the keywords flood s ing and houston and unique it is unclear why some of these articles captured by articulate were not captured by the other tools overall our algorithm proved to be general enough to pick up articles about the subject of interest without being too broad such that articles are returned which use the relevant keywords but are not actually about the topic thus the outputs from articulate prove to be a valid programmatic way of collecting relevant news media while facilitating the search process comparing the functionality of articulate to the other tools one distinct advantage of the articulate software package is that unlike performing time consuming manual searches or relying on the availability of a source specific api this tool can be used to collect news articles from multiple different sources in addition the articulate software allows for the automatic collection of news article information in a usable csv database format for further analysis while lexisnexis and manual searches do not facilitate such functionality without additional time consuming and labor intensive steps furthermore while both proquest and lexisnexis have a database for the new york times they do not have a comprehensive database for all newspapers whereas articulate can access any online newspaper of the users choosing additionally the new york times is one of only a few newspapers with an api for easy data gathering providing added motivation for using articulate when a more comprehensive analysis warrants the examination of multiple sources we then zoom in to more closely examine the comparison between outputs from articulate and proquest over the entire periods of interest for each case study january 2005 through august 2016 and january 2013 through october 2017 respectively in figs 3 and 4 we chose to look at proquest more closely as it is one of the current leading and most widely used proprietary databases with similar functionality to articulate examining fig 3a and b we see that in the case of drought in california news media extends over several months surrounding each of the two drought events in the period of study with greater coverage following the second drought declaration in 2014 whereas news media about flood in houston is largely concentrated on august and september 2017 during and following the historic damage caused by hurricane harvey in both cases there are several distinct peaks of media coverage captured by both articulate and proquest during the recent historic drought in california we see that peaks from both sources generally correspond to drought related political events fig 3a for example january 2014 was when the governor declared a state of emergency due to the drought summer 2014 was when the state called for voluntary outdoor water conservation and april 2015 was when the first ever mandatory statewide water use restrictions were announced california state water resources control board 2015 examining the time series for flooding in houston we see a different pattern but leading us to the same conclusions both articulate and proquest produce similar results with one small peak in may of 2015 corresponding to the historic memorial day flooding event ramirez 2017 and one large peak in august and september 2017 corresponding to the hurricane harvey event chokshi and astor 2017 instead of heightened media coverage during political events like during the recent historic california drought we see peaks during the flood events themselves demonstrating that despite the different characteristics of droughts and floods such as duration and severity news media coverage still provides insights into issue salience finally fig 4a b and c show that the distribution of outputs for each month over the period of analysis is very similar between articulate and proquest search tools for the california drought the mean number of articles per month in articulate and proquest were 3 16 and 3 10 respectively with a similar spread of results for floods in houston the mean number of monthly articles were 5 07 and 3 02 furthermore fig 4c shows the difference between the results output by articulate and those output by proquest where the mean difference is 0 06 for california drought and 2 05 for houston flooding based on insights from the time series plots the larger difference between the two sources for houston flooding is likely due to the difference in the 2017 spike where articulate produced more results than proquest as previously mentioned a manual assessment of these results indicated that the news articles captured by both tools are largely relevant to the topic and it is unclear why some articles were identified by articulate but not by proquest although the presented case studies demonstrate the benefits of articulate and potential environmental applications users should consider some of the challenges and limitations that arise from the nature of this tool in particular this software relies on google s cse api a free online tool to retrieve information similar to the way a user would perform a google search because this method requires news media to be present in the internet in a recognizable online format articulate is inherently limited to applications in recent decades and may be insufficient when the purpose of the study is to assess multi decadal trends expanding prior to the internet era this is coupled with the limitation that articulate searches from a certain date to the present thus it is most practical to search for articles within a relatively recent time period other tools like proquest or lexisnexis which provide access to historical repositories may be more appropriate for analyses requiring more extensive temporal coverage articulate is a tool for the 21st century 5 conclusion this work develops articulate a new open source tool that facilitates the search and collection of news media from a variety of sources while providing more flexibility and data accessibility than existing tools articulate uses the google cse api engine to perform online news searches that allow for 1 access to more sources than currently available in similar proprietary tools 2 access to wider time frames and more personalized queries than typical newspaper specific apis 3 customizable search parameters that parallel those of a user friendly google search and 4 a resulting flexible csv database for further qualitative and quantitative analysis this novel tool opens opportunities for further analysis of growing socio environmental issues and can be easily manipulated to suit variety of research needs including generating information to inform scenario development and to create model inputs given the importance of understanding the interaction between news media political actions and influence on public behavior we argue that access to an increased number of data sources as well as easy data manipulation present significant benefits to researchers and environmental managers alike public engagement can play a major role in human response to extreme environmental events including droughts and floods di baldassarre et al 2013 quesnel and ajami 2017 or more generally in human environment interactions and being able to effectively and accurately quantify media coverage using a tool like articulate presents many opportunities to further understand public behavior under changing political and environmental conditions in this way articulate provides a novel tool to enhance the development of modeling applications that effectively couple human and natural systems acknowledgements the authors wish to thank the editors and two anonymous reviewers for their thoughtful feedback and comments which greatly improved the clarity and content of the manuscript the authors acknowledge financing support from the bay area water supply and conservation agency sonoma county water agency the national science foundation engineering research center for reinventing the nation s urban water infrastructure renuwit award no eec 1028968 a national science foundation graduate research fellowship awarded to p g and star fellowship assistance agreement no fp 91778101 0 awarded to k j q by the u s environmental protection agency epa this work has not been formally reviewed by epa the views expressed are solely those of the authors and epa does not endorse any products or commercial services mentioned in this manuscript 
26445,news media plays an important role in shaping public opinions and attitudes about the environment tracking and analyzing media coverage can provide insight into public exposure to narratives that impact resource consumption environmental behavior and emergency response which can help to inform model development or provide additional model inputs this paper presents articulate an open source flexible tool for discovering compiling and quantifying newspaper coverage on a user specified topic articulate is written in python and interfaces with google custom search engine api we demonstrate the tool s application and validate its performance on two case studies of news media coverage in the new york times about drought in california and flooding in houston texas in recent years our results show that articulate can generate data similar to or better than proprietary databases thus articulate can help researchers and environmental managers gain important insights to better understand and quantify changing socio environmental dynamics keywords news media coverage python google custom search engine api search tool software availability name of software articulate developers nicholas roby and newsha ajami contact address 473 via ortega room 204 stanford ca 94306 telephone 650 724 8162 email address nickroby12 gmail com and newsha stanford edu year first available 2017 hardware required personal computer internet access software required python 2 7 availability https github com stanford urban water policy innovation articulate cost 0 program language python program size 12 kb 1 introduction there is increasing recognition that human behavior plays an integral role in environmental planning decisions lund 2015 sovacool et al 2015 behavior has thus emerged as a central feature of many new environmental modeling methods and applications for example in recent years scientists and engineers have attempted to model human actions and reactions within hydrologic systems garcia et al 2016 gonzales and ajami 2017 noël and cai 2017 when predicting energy demand ma et al 2009 wilson and dowlatabadi 2007 as inputs into land use change projections lauf et al 2012 and for developing emergency response scenarios during extreme events such as floods and wildfires giordano et al 2017 nara et al 2017 assumptions about human behavior can have significant effects on the accuracy and results of models that aim to couple human and natural systems noël and cai 2017 sun et al 2016 and as a result many different approaches for modeling human environmental interactions have emerged one method for understanding human behavior is to measure issue salience public awareness and education are important drivers of environmental resource consumption and conservation stern 1976 emergency response behavior du et al 2017 mccaffrey et al 2017 and attitudes and beliefs king et al 2017 both social media du et al 2017 and news media treuer et al 2017 troy et al 2015 have been recognized as vehicles for measuring community exposure and sensitivity to environmental topics and events in this paper we focus on news media one important method of influence ball rokeach and defleur 1976 boykoff and boykoff 2007 sampei and aoyagi usui 2009 that has been relatively unexplored in the modeling of environmental scenarios quantifying and analyzing coverage of environmental topics such as droughts floods wildfire clean energy climate change or other themes can provide deep insight into awareness levels of the public which can then impact decision making analyzing news media coverage can also be used to retroactively evaluate the most salient events surrounding a topic boykoff and boykoff 2007 as environmental news coverage is often driven by specific events bolsen 2011 which are believed to reflect periods of public awareness this approach of creating news media time series and analyzing coverage has been used in previous research to provide insight into public exposure to environmental topics akerlof et al 2012 crow et al 2016 ruiz sinoga and león gross 2013 treuer et al 2017 quantitatively researchers have examined news media to develop sustainability indicators rivera et al 2014 validate flooding models smith et al 2012 and as inputs into water demand models quesnel and ajami 2017 these types of multidisciplinary modeling approaches are likely to be used more frequently in the future current tools for evaluating and quantifying news media coverage include proprietary databases such as proquest and lexisnexis bolsen 2011 wei et al 2015 these database tools fit with a graphical user interface gui allow the user to query from a selection of news media sources during specific time periods and for specific keyword s using query language these tools then extract classify represent and store various pieces of information from each source however database tools can be inflexible and in some cases may lack the temporal and source coverage required for a full and proper analysis most limiting is the propriety nature of these databases that require expensive subscriptions while free alternatives are available by performing manual searches in a specific newspaper web repository or by using existing newspaper specific api tools such data collection methods can be labor intensive time consuming and limited to specific news sources that may not be representative enough for a comprehensive analysis these tools which were built for different needs and objectives can also produce results in a format that is not exportable and therefore incompatible with quantitative analyses and modeling applications thus we created a new instrument articulate as a compliment to current news media discovery tools articulate is written in python and interfaces with the google custom search engine cse api to provide a free flexible customizable tool with functional outputs additionally articulate can offer equal and sometimes greater data coverage than produced by comparable proprietary databases this tool can be used to gain insight into public exposure to a topic of interest such as environmental issues as well as to evaluate the most important events related to that topic in this paper we first describe the functionality and methodological details of the software package then we validate the tool using two case studies of news media coverage of drought in california and flooding in houston texas to demonstrate the algorithm s performance and applicability 2 articulate software package 2 1 basic functionality the articulate software package uses existing open source application program interface api tools to search through online content and news article databases collect content of interest tally search results and generate a database of articles with pertinent information for further qualitative and quantitative analysis the general process of the algorithm is shown in fig 1 the articulate algorithm is written in python and uses the google cse api to submit queries web searches programmatically into the google search bar step through multiple pages of results and scrape only relevant news articles from defined news sources for specific keywords over a user defined time window in the process articulate filters out ads sidebars commentaries and images typically found in news websites thus eliminating sources of confounding information and returning only relevant results the tool allows users more flexibility and control over the queries and resulting outputs than existing proprietary applications while some familiarity with the python programming language and the google cse api is required for initial setup the articulate package includes a gui to facilitate its use in the following sections we present articulate as an accessible algorithm that lowers many of the barriers a user could face when trying to implement google cse api applications at socially and environmentally relevant scales articulate is open source and is available as a github repository at https github com stanford urban water policy innovation articulate 2 2 software environment articulate uses the python programing language a freely available language with extensive capabilities that runs on both windows and unix like platforms python software foundation 2016 articulate is written in python version 2 7 with the following modules csv py time py datetime py googleapiclient discovery py ast py numpy py pandas py sys py dateutil py cookiejar py urllib py urllib2 py and tkinter py google cse api client requires internet access for use and the user must first create an account with google cse api client to obtain an api developer key while also setting up their custom search engine with the parameters relevant to their needs see https developers google com custom search docs overview for more information 2 3 google cse api client the backbone of articulate is the google cse api client a web searching tool developed by google that allows the user to submit any google search programmatically google 2017 it performs like google s search bar returning the same page of results ten at a time that would be achieved had the search been executed within google s web search bar the information within each result contains items which can include title of the result media type i e video article etc author short excerpt content keywords date of publication and various other attributes the information is extracted and stored within the database specified by the user in the same way that the user steps through various pages of results when searching within google s search bar the user must also step through various pages when using the google cse api tool the flexible and dynamic nature of the cse tool allows for an array of input parameters the user has the capability to search various websites by date by quoted content by an exclusive query and other functions the functionality of this tool allows the user to step through ten pages of the same search up to the 100th result each query submission can retrieve up to 10 results and each developer key gets 100 queries a day for free after that the google developer apis are subject to a fee for each 1 000 queries used in a day and the user can submit up to 10 000 queries a day with this method we address these search retrieval limitations in our algorithm by developing a time step method described in section 2 6 below to cycle through the queries and results 2 4 software inputs articulate requires a set of comma separated values csv and string inputs there is one distinct csv input a csv file that contains the specific news websites to be searched and the structure of the dictionary used to access each website specific metadata string inputs include 1 the name of the csv file in which the output database should be stored 2 the reference date which defines the start point in the time period of the search inquiry 3 the date step size the time interval to be used in the representation of results 4 the number of developer keys needed 5 the developer key s 6 search term s 7 or term s defined by google as additional search terms to check for in a document where each document in the search results must contain at least one of the additional search terms each search term can have multiple or terms and or terms are defined separately for each search term and 8 include term s defined by google as exactterms which must be found in each result in addition to the initial search term it should be noted that all terms in the query are case sensitive search or and include terms 2 5 software outputs articulate produces two types of spreadsheet outputs the first kind of output are dataframes which contain the number of articles published by each source for each search these dataframes report tallied results counting the number of desired articles in each time interval e g number of articles each day month or year one dataframe is produced for each search term the second output is a database file which contains identifying information for each article with the following headers site search title day year media type and further info the database file contains both hits and misses hits are articles meeting all query criteria which are counted in the tally database misses should be manually removed from the database and are identified by incomplete information or reported errors in the output file one database is produced for each query and all search term results are included in the single database 2 6 software algorithm articulate uses a time step approach to address the size limitations of google cse queries while capturing all possible search results the algorithm searches within sub windows of a pre defined time period cycling through each sub window until the entire time period of interest has been analyzed the user inputs a reference date which defines the time range of the analysis e g a start date ten years ago until the present and a time step size in days months or years the step size is determined through trial and error a non trivial task as the step size must be of a fine enough resolution to find relevant articles while also not being excessively small that it makes inefficient use of limited queries generally a step size that produces between 10 and 100 results in every step is preferred once the time step and reference date have been established the user inputs the developer key s the csv input file and the query terms the search include and or terms a python pandas series is created which contains dates that span the entire date range the program then follows multiple loops to step through each search term for each media source and for each step window within the loop articulate configures the time series to include each sub window in the total time frame thereby creating a timeline of all the sub windows for the entire period that will be analyzed each query submission returns 10 results if the query submission retrieves no results the program moves to the next step window if the total query results for the specified sub window are greater than 100 the program notifies the user that some results were missed in the time step designation and attempts to adjust to catch the missing results however this is not a robust method of gaining results as the google cse api may skip dates when delivering the result to mitigate against this error the user should attempt to create a step size that captures all results desired by performing a trial and error run on a subset of the timeframe until a time step is found that produces the ideal 10 100 results as the program loops through each result obtained in a query it extracts associated information including date title newspaper source type of result etc in the process the results are filtered to determine if the result type result content and date are consistent with user specifications one important contribution of articulate is that it filters all banners comments advertisements images and other non article results produced by google cse api as well as duplicates and irrelevant results to do this the algorithm performs four different checks on each result 1 check for article type articulate attempts to obtain the result type which could include article banner comment advertisement image video and more for this process articulate calls on the site specific metadata to read the query submission results and return its type label if the extraction methods are unsuccessful and the article type cannot be verified articulate will report an error in the output database 2 check for content relevance if the result type is confirmed as an article articulate attempts to extract the url from the results the url is then used to read the html file representing the news article if the query the combination of search or and include terms is found in the article html then the article is deemed relevant and tallied if not then relevance cannot be confirmed and articulate will report an error in the output database in the case that the url cannot be found the article is both stored and tallied but relevance cannot be confirmed and is reported as unchecked it is important to note that this process only checks whether relevant keywords are present in the article and cannot assure that those keywords are used in a relevant context 3 check for duplicate titles as articulate tallies relevant articles it extracts the article titles from the results if the article title cannot be found in the dictionary structure of the result articulate reports it as an error article titles that can be extracted are stored for each new article found the algorithm checks the database of stored titles to confirm the result is not a duplicate 4 check for time frame relevance articulate extracts the article date from the results converts it to a standard python datetime format and confirms whether the date falls within the user defined time frame if the date cannot be extracted or is in an unrecognized format the result cannot be tallied because the date is unknown conversely if the date is successfully extracted the result is tallied in its respective time sub window and stored in the database if all four checks are satisfactory the results are stored and counted in the tally file if one or more of the checks is unsatisfactory the article will not be tallied in the dataframe organized by media source search query and date and will be reported as an error in the database because articulate goes through a mechanistic process of looking for relevant information in the form of specific keywords and specific dates in news articles there may be cases where results are reported despite being irrelevant e g a sports team in a winning drought a flood of support for charity users should check articulate output results for relevance as needed users querying proprietary databases can also face this challenge to address this obstacle some researchers have used sampling and coding methods treuer et al 2017 wei et al 2015 while others have manually checked all articles for relevance crow et al 2016 quesnel and ajami 2017 3 example applications 3 1 case study descriptions we tested articulate using two case studies of extreme environmental events 1 drought in california and 2 flooding in houston texas these case studies were chosen as they represent different kinds of events where researchers and environmental managers could benefit from a better understanding of public awareness and news media exposure droughts and floods are both extreme events with important societal and environmental implications but they have different spatial and temporal characteristics that may drive news coverage dynamics community response and adaptation measures di baldassarre et al 2017 1 california drought we used articulate to search for news articles containing the term california drought or a combination of california drought and water related terms such as water snow or rain over the period january 2005 through august 2016 this time period is useful for analysis of water use behavior changes related to media coverage as it encompasses two distinct drought periods 2007 2009 and the historic drought from 2012 2016 as well as a reference pre drought period 2005 2007 2 houston flooding for our second case study we used articulate to extract newspaper articles containing the terms houston flood and water related terms such as water rain or rainfall from january 2013 through october 2017 this is an interesting period of analysis because it captures both a historic flooding event due to hurricane harvey in august 2017 as well as previous smaller but still extreme flooding incidents in recent years to ensure that we could capture these previous flood events for comparison we did not include the terms harvey or hurricane in our search 3 2 approach for validation purposes we show the results of performing our case study searches in the new york times using various tools while articulate can access any online newspaper we chose the new york times because it is a highly circulated newspaper with a comprehensive science section clark 2006 that also has its own api we validated the results obtained using articulate by comparing the software outputs to similar searches using proquest lexisnexis the new york times api see http developer nytimes com for more information and a manual search in the new york times online news repository we used three different methods to evaluate search results first we examined search results from all five methods described above for a sample of four semi randomly chosen months within each case study second we compared time series of search results between articulate and proquest for the entire period of interest looking especially at trends and peaks third we looked at the distributions of search results from articulate and proquest for the entire time frames of interest for each topic 4 results and discussion fig 2 shows a comparison of the tallied results for a sample of semi randomly chosen months over the periods of analysis in each case study these months were chosen to represent the temporal characteristics of our two case studies and to demonstrate how the results obtained with articulate compare to other news search tools in periods of low and high media coverage as shown in this comparison the outputs of the articulate search compare well to similar searches with proquest and lexisnexis and do not seem to consistently under or over predict results when compared to the new york times directly in addition we performed a manual comparison between the results obtained by each of the five search methods to provide insights on the nature of the differences between articulate and other tools in the case study of drought in california fig 2a the most significant differences are in april 2015 a period of high media coverage where the number of hits captured by articulate is comparable to proquest but considerably lower than lexisnexis the new york times api and a manual search looking at the titles of articles captured by these different methods we found that the larger volume reported in the new york times website corresponds to some relevant articles that the other search tools missed but also several irrelevant articles or different media types such as a your weekly news quiz photo stories of the drought and a few sports articles reporting on winning droughts lexisnexis results which are closer to the new york times results in this period also captured some of these different media types and irrelevant results similarly in the case study of floods in houston fig 2b the main differences between results are in the period of highest media coverage in september 2017 unlike the first case study in these results articulate tallied more articles than any of the other tools a manual assessment of these results indicated that all of the articles captured by articulate in this period were indeed relevant news articles with titles related to hurricane harvey or containing the keywords flood s ing and houston and unique it is unclear why some of these articles captured by articulate were not captured by the other tools overall our algorithm proved to be general enough to pick up articles about the subject of interest without being too broad such that articles are returned which use the relevant keywords but are not actually about the topic thus the outputs from articulate prove to be a valid programmatic way of collecting relevant news media while facilitating the search process comparing the functionality of articulate to the other tools one distinct advantage of the articulate software package is that unlike performing time consuming manual searches or relying on the availability of a source specific api this tool can be used to collect news articles from multiple different sources in addition the articulate software allows for the automatic collection of news article information in a usable csv database format for further analysis while lexisnexis and manual searches do not facilitate such functionality without additional time consuming and labor intensive steps furthermore while both proquest and lexisnexis have a database for the new york times they do not have a comprehensive database for all newspapers whereas articulate can access any online newspaper of the users choosing additionally the new york times is one of only a few newspapers with an api for easy data gathering providing added motivation for using articulate when a more comprehensive analysis warrants the examination of multiple sources we then zoom in to more closely examine the comparison between outputs from articulate and proquest over the entire periods of interest for each case study january 2005 through august 2016 and january 2013 through october 2017 respectively in figs 3 and 4 we chose to look at proquest more closely as it is one of the current leading and most widely used proprietary databases with similar functionality to articulate examining fig 3a and b we see that in the case of drought in california news media extends over several months surrounding each of the two drought events in the period of study with greater coverage following the second drought declaration in 2014 whereas news media about flood in houston is largely concentrated on august and september 2017 during and following the historic damage caused by hurricane harvey in both cases there are several distinct peaks of media coverage captured by both articulate and proquest during the recent historic drought in california we see that peaks from both sources generally correspond to drought related political events fig 3a for example january 2014 was when the governor declared a state of emergency due to the drought summer 2014 was when the state called for voluntary outdoor water conservation and april 2015 was when the first ever mandatory statewide water use restrictions were announced california state water resources control board 2015 examining the time series for flooding in houston we see a different pattern but leading us to the same conclusions both articulate and proquest produce similar results with one small peak in may of 2015 corresponding to the historic memorial day flooding event ramirez 2017 and one large peak in august and september 2017 corresponding to the hurricane harvey event chokshi and astor 2017 instead of heightened media coverage during political events like during the recent historic california drought we see peaks during the flood events themselves demonstrating that despite the different characteristics of droughts and floods such as duration and severity news media coverage still provides insights into issue salience finally fig 4a b and c show that the distribution of outputs for each month over the period of analysis is very similar between articulate and proquest search tools for the california drought the mean number of articles per month in articulate and proquest were 3 16 and 3 10 respectively with a similar spread of results for floods in houston the mean number of monthly articles were 5 07 and 3 02 furthermore fig 4c shows the difference between the results output by articulate and those output by proquest where the mean difference is 0 06 for california drought and 2 05 for houston flooding based on insights from the time series plots the larger difference between the two sources for houston flooding is likely due to the difference in the 2017 spike where articulate produced more results than proquest as previously mentioned a manual assessment of these results indicated that the news articles captured by both tools are largely relevant to the topic and it is unclear why some articles were identified by articulate but not by proquest although the presented case studies demonstrate the benefits of articulate and potential environmental applications users should consider some of the challenges and limitations that arise from the nature of this tool in particular this software relies on google s cse api a free online tool to retrieve information similar to the way a user would perform a google search because this method requires news media to be present in the internet in a recognizable online format articulate is inherently limited to applications in recent decades and may be insufficient when the purpose of the study is to assess multi decadal trends expanding prior to the internet era this is coupled with the limitation that articulate searches from a certain date to the present thus it is most practical to search for articles within a relatively recent time period other tools like proquest or lexisnexis which provide access to historical repositories may be more appropriate for analyses requiring more extensive temporal coverage articulate is a tool for the 21st century 5 conclusion this work develops articulate a new open source tool that facilitates the search and collection of news media from a variety of sources while providing more flexibility and data accessibility than existing tools articulate uses the google cse api engine to perform online news searches that allow for 1 access to more sources than currently available in similar proprietary tools 2 access to wider time frames and more personalized queries than typical newspaper specific apis 3 customizable search parameters that parallel those of a user friendly google search and 4 a resulting flexible csv database for further qualitative and quantitative analysis this novel tool opens opportunities for further analysis of growing socio environmental issues and can be easily manipulated to suit variety of research needs including generating information to inform scenario development and to create model inputs given the importance of understanding the interaction between news media political actions and influence on public behavior we argue that access to an increased number of data sources as well as easy data manipulation present significant benefits to researchers and environmental managers alike public engagement can play a major role in human response to extreme environmental events including droughts and floods di baldassarre et al 2013 quesnel and ajami 2017 or more generally in human environment interactions and being able to effectively and accurately quantify media coverage using a tool like articulate presents many opportunities to further understand public behavior under changing political and environmental conditions in this way articulate provides a novel tool to enhance the development of modeling applications that effectively couple human and natural systems acknowledgements the authors wish to thank the editors and two anonymous reviewers for their thoughtful feedback and comments which greatly improved the clarity and content of the manuscript the authors acknowledge financing support from the bay area water supply and conservation agency sonoma county water agency the national science foundation engineering research center for reinventing the nation s urban water infrastructure renuwit award no eec 1028968 a national science foundation graduate research fellowship awarded to p g and star fellowship assistance agreement no fp 91778101 0 awarded to k j q by the u s environmental protection agency epa this work has not been formally reviewed by epa the views expressed are solely those of the authors and epa does not endorse any products or commercial services mentioned in this manuscript 
26446,in the smart cities context real time knowledge of residential water consumption has become increasingly important especially given the fast evolution of sensors ict and the production of big high resolution data coming from the urban environment a variety of reasons often leads to the creation of continuity gaps in these data series thus making the need for a methodology that produces reliable and realistic synthetic data urgent in this article we present a methodology that generates synthetic household water consumption data we showcase it in two case studies skiathos greece and sosnowiec poland which exhibit significant differences in water consumption patterns the methodology captures the stochasticity of daily residential water use algorithm validation is implemented through the comparison of various metrics for actual and generated data this way we show that the suggested approach is capable of adequately simulating water consumption in both micro and macro time scale keywords synthetic water consumption data generation pulse models missing data water consumption patterns data availability the data used in this article are water consumption data collected by a total of 16 households for a period of 13 months starting from february 1st 2015 in two locations skiathos greece 10 sampling points faucets each one corresponding to a different household and sosnowiec poland 9 sampling points faucets and appliances in 6 households the water consumption monitoring system was installed in a diverse group of households that were specifically chosen in order to provide needed data to help comprehend human behavior and water consumption patterns by different users in a household in various socio economic settings the criterion for the selection of the households was the availability and promptness of the housekeepers additionally the households were chosen so that they were diverse regarding their location in the network and number of occupants wireless sensors were installed in various sampling points in the households i e faucets washers and showers 30 second step records were transmitted to a remote central server in real time technical details on the water consumption monitoring system are provided in chen et al 2015 the data used are available online at validation issewatus eu data re use this work was undertaken for the ec fp7 project iss ewatus 2016 1 introduction key to a smart city concept is the idea of measurement of instrumenting the urban landscape and associated activity and monitoring their state and behavior in a way that leads to technological governmental and societal advances it has been said that you can t manage what you can t measure which greatly applies to a city of the future in which near real time measurements enable stakeholder awareness engagement and quick response to new conditions thus leading to a new model of civic behavior and involvement this new paradigm is based on almost individualized planning on one hand and near real time information on another lim et al 2010 a recent study cominola et al 2015 reviews water smart metering projects taking place in the last decades worldwide according to this work these projects which focus on real time water use monitoring at high spatial and temporal granularity stimulate modeling approaches and behavior adaptive urban water management strategies consumer awareness campaigns have been documented in the literature in the last decade russel and fielding 2010 novak et al 2016 perren and yang 2015 shan et al 2015 while latest advances include the development of gaming platforms wang and capiluppi 2015 for water management and the involvement of social media for citizen engagement in water saving practices the european commission has funded a series of research projects that developed a series of diverse case studies that all showed how building consumer awareness could limit water consumption all these projects are grouped under the ict4water cluster http ict4water eu a number of water utilities increasingly attempt to influence the behavior of consumers towards improving water consumption by using communication tools to give information back to users and display their consumption or customized feedbacks or water saving tips at the same time various companies have been established lately that specialize solely on transforming the way customers think about their household water consumption as well as the way utilities engage with their customers such companies combine machine learning ml and other data science tools with cloud computing and behavioral science to develop software as a service solution to customer engagement and efficiency issues faced by utilities subsequently the need for the collection and management of large quantities of temporal and spatial high resolution data emerges as the core of urban planning while at the same time the radical evolution in the technological sector of sensors information and communication technologies icts social network data analysis and data mining dm techniques reveals new potentials for more efficient planning laspidou 2014 laspidou et al 2015 yang et al 2017 in the urban water domain due to fast urbanization increasing demands climate change and high pressure on water resources research activity increasingly focuses on monitoring understanding and better managing urban water activities detailed monitoring of household water consumption can reveal useful information about citizen behavioral patterns not only related to their water use per se but also concerning a range of socio economic factors directly or indirectly related to water such as circadian rhythms working hours daily habits house amenities familial structure and profile etc furthermore the spatiotemporal analysis of household water use can help make water consumption a key indicator of human behavior thus helping authorities and relevant stakeholders identify changes in city living conditions such as local development migration epidemics or it can disclose population shifts due to events such as terrorist attacks natural disasters large scale organized meetings or tournaments etc besides the wealth of information potentially extracted by monitoring water consumption channeling this data back to the consumers will contribute to an increased awareness that will lead to a smaller household water footprint lanzarone and zanzi 2010 perren and yang 2015 al hoqani and yang 2015 the effectiveness of similar schemes regarding energy consumption through energy metering billing and direct display methodologies has already been documented darby 2006 concluding that feedback to consumers is an important element of an energy savings scheme for consumers numerous relative examples are reported in ehrhardt martinez et al 2010 and fischer 2008 works indicatively in the staats et al 2004 study an energy savings increase of approximately 3 in 16 months in netherland is reported and in wilhite and ling 1995 study an increase of 2 4 in energy saving in one year for a norway case study is presented in fields such as dm ml and knowledge discovery from databases kdd a commonly emerging issue which is the main focus of this paper is that of missing values or missing data numerous reasons can lead to such a problem equipment malfunctions refusal of respondents to fill in questionnaires and gathering of erroneous data etc schafer and graham 2002 batista and monard 2002 demand management initiatives rely on good comprehension of water usage practices as well as of factors influencing water demand white et al 2003 the emerging data driven demand management has been supported by cloud based data platforms and represents a new critical element to improve decision making in today s water industry utility managers can achieve the sustainability and affordability objectives they desire through the practical application of data analytics fielding et al 2012 in this context the implication of data gaps is really important since the decision making process relies on continuous data sets such continuous data sets improve the resilience of new decision making schemes based on the reason why a gap is created missing data is categorized into three classes depending on the level of randomness of the incident missing completely at random mcar missing at random mar and not missing at random nmar are commonly used classes that imply that the incident either does not depend on the missing value or depends on a related to the value attribute or directly depends on the value respectively little and rubin 1987 little 1988 an example for a mcar would be the interruption of functioning of a sensor that would create a gap no matter what the measurements would be an example for mar would be the absence of answer in a questionnaire about an attribute that is indirectly related to the gender of the respondent an example for nmar would be the case of a sensor not recording a value because it lies outside its measuring capacity range thus a missing or erroneous value would imply that it is out of this range the level of randomness is conclusive of the method that the missing data are treated depending on the class that the data gap belongs to a different methodology for treating the missing data is selected another criterion for choosing the method to treat an incident of missing value is the nature of the attribute specifically if the attribute were a time series the treatment would involve analysis of components such as trend and seasonality moreover if the missing attribute value were correlated to another known attribute then the method of treatment would be selected based on this correlation which would imply the implementation of multivariate analysis as opposed to univariate lastly a criterion is the length of the missing part this can vary from a single missing value to a larger gap of data the aforementioned criteria are decisive of the treatment of an incident variable methods are applied for this purpose some commonly applied tactics include ignoring and discarding the incident case substitution mean or mode imputation hot deck and cold deck method applying a predictive model and others lakshminarayan et al 1999 grzymala busse and hu 2001 batista and monard 2003 the imputation of a missing value is generally classified into deterministic or stochastic rao 1996 other than filling missing data gaps the production of data that mimic the properties of a data set synthetic data can be essential in situations in which available real data are limited and longer data sets are required for evaluation validation and or testing of models platforms algorithms or decision support systems dsss barse et al 2003 define synthetic data as generated data by simulated users in a simulated system performing simulated actions a typical example of need for synthetic data is the case when privacy constraints block the direct use of original sets in other words water utilities may not agree to provide actual water consumption data being concerned about violating the privacy of their customers even if data is anonymized synthetically generated data overcome problems related to data privacy cominola et al 2016 in such cases the use of a tool that provides synthetic water consumption data will serve well the needs of water utilities including decision making platforms used in data driven demand management schemes another example is the training and adapting of a fraud detection system fds on a synthetic data set testing its properties by injecting synthetic frauds or comparing the performance of different fdss barse et al 2003 past research works have focused on investigating whether urban water consumption time series can be simulated in multiple temporal and spatial scales pulse models developed for creating such artificial time series generally consider two variables the duration and intensity of a consumption event these models can be divided into two categories the ones that simulate pulses of specific end use fixtures and the ones that are parameterized to simulate overall household water demand creaco et al 2017 buchberger and wu 1995 used the poisson rectangular pulse prp methodology to simulate the behavior of the water consumer considering actual monitored household consumption alvisi et al 2003 introduced a cluster neyman scott stochastic process to simulate residential water demand respectfully to the cyclical behaviour observed during a typical working day blokker et al 2009 developed a methodology for simulating water consumption at residence level using 8 end uses bathtub dishwasher etc patterns based on survey data and technical characteristics of the appliances cominola et al 2016 used data from 300 households in 9 u s cities and developed a stochastic simulation model for the generation of residential water end uses based on the assumption that each end use is characterized by a unique signature creaco et al 2015 proved that taking into account dependence between duration and intensity variables can improve the pulse approach performance creaco et al 2016 focused on parameterizing the values of the aforementioned model which were associated with the model variables respectfully to the water balance characteristics in multiple time scales they concluded that high accuracy in smart metering relates to better performance of the model in the 2017 study of di palma et al the overall pulse model was introduced to describe aggregated water consumption this model does not generate single end use pulses but the water consumption of a whole household as recorded at a water meter a methodology on filling a gap of water consumption data with meaningful values is presented in this article the suggested methodology is in accordance to the established residential water demand pulse models since it is based on simulating the user behavior while considering characteristic variables such as intensity of flow and duration the methodology differs from other established approaches in the fact that it firstly captures consumption patterns throughout the day and then introduces a novel algorithm that simulates the duration of the incident the suggested innovative algorithm for simulating the duration attribute gives a valuable degree of freedom that allows dealing with the consumption pattern beforehand thus the methodology overall facilitates capturing precisely the pattern the water consumption data concern household water consumption and are collected for the purpose of investigating the effect of real time monitoring and informing consumers about their own water consumption on their water use behavior through an ict supported consumer awareness process the methodology is developed validated and applied to the created gap in order to retrieve required data sets to be used for the development of a dss platform developed in the context of the ec funded project iss ewatus 2016 in this article the two study areas and the data characteristics are firstly described next the methodology is presented in a step by step format for the base case study of skiathos households as well as its modification for sosnowiec households the method validation process and results are presented and discussed the article finishes with a series of conclusions and implications of the new data generation tool for the urban water sector 2 materials and methods 2 1 case study in the context of the iss ewatus project sensors were installed in faucets showers and appliances in multiple households in skiathos island greece and sosnowiec poland for the purpose of this article data sets from 16 households were used 10 and 6 with 10 and 9 sampling points respectively faucets at skiathos and faucets and appliances at sosnowiec the monitoring period was initiated on february 1st 2015 while data collection is still ongoing as of november 2016 technical issues during the installation in skiathos island delayed the initiation of data recording which officially began on april 14 and created a data gap of 72 days from february 1st to april 14 2015 for the greek case study the initial motivation for the synthetic data generating sdg methodology presented in this article was to fill the data gap that was created in the greek case study once this goal was reached we extended the methodology to a generic sdg tool capable of producing synthetic data based on historical water consumptions to ensure that the applicability of the developed sdg tool is not limited to the water consumption profiles of skiathos for which it was developed we further tested its robustness by generating data for the polish case study that exhibited different water consumption profiles the two case studies are very different in terms of socio economics demographics climate and geography factors that are all expected to influence water consumption patterns skiathos is an island with high seasonal touristic activity and seasonal weather variability kofinas et al 2014 2016 mellios et al 2015 however for the skiathos case study water sensors are installed at typical non touristic households where the impact of tourism and weather variables is not that significant it should be noted that in skiathos the sensors are installed at kitchen faucets a factor that diminishes the seasonal weather impact on water consumption in contrast to water consumption in the bathroom which is related to showers and baths influenced by weather a third factor to conclusively define the pattern of water consumption is that water in skiathos is announced to have high concentrations of mercury thus not potable water is used mostly for washing and cleaning furthermore skiathos is a small village of about 6000 people with small family owned businesses without large companies and corporations this corresponds to a traditional lifestyle with extended families living together and stay at home parents or grandparents as a result the water use pattern differs from one of a large city with urban lifestyle and exhibits no detectable weekday weekend water consumption variability water use in the skiathos case study shows no seasonality or trend for the aforementioned reasons finally pilot households in skiathos are located in the old town where houses are often over 100 years old and amenities are limited usually houses are equipped with a total of 3 or 4 faucets mostly kitchen and bathroom with dishwashers not being a typical appliance the significance of this is that the total household water consumption is split among these few faucets and the water consumption in the kitchen where the sensors were installed is a significant part of the total household consumption sosnowiec is a city in the katowice urban area with 2 7 million people with strong urban dynamics it is a typical industrial city with heavy industries companies and other associated economic activity as far as the water consumption case study is concerned sosnowiec reveals an urban lifestyle and the corresponding water use mode is characterized by variability in water consumption between weekdays and weekends the sensors installed at sosnowiec are in showers bathtubs kitchens appliances balconies etc thus the patterns of use are expected to be more variable offering a challenging case study for testing the developed sdg methodology moreover the fact that the sosnowiec pilots have multiple water supply points in the household especially when compared to those in the skiathos pilots the total average daily consumption in each sosnowiec water supply point is overall lower than those in skiathos 2 2 data description the data sets refer to water flowrate values that are recorded every 30 sec of water consumption once there is flow in the monitored faucet appliance the sensor generates a record with the corresponding timestamp at the end of the 30 sec period it records the water consumption in liters min during the 30 sec period if the faucet appliance is still on the next timestamp is recorded and then the corresponding consumption and so on every 30 sec the sensor checks for flow and when the faucet appliance is off no record is produced thus finalizing the creation of a water consumption incident record when the next water consumption incident starts the procedure repeats itself for the period in between the two incidents no record is produced all incidents themselves have a 30 sec time step but the starting time of an incident might be for example 45 s after the previous one this means that sensor produced data are not in the form of a single time series with a 30 sec time step but are recorded in the form of numerous clusters each one representing a small time series of the equivalent incident in reality in order to distinguish between incidents one detects when the time distance between two consecutive records is greater than 30 s as shown in fig 1 a the number of records per incident is used to calculate its duration 2 2 1 method description an algorithm that would generate flowrate records for a household water supply point should simulate all meaningful qualitative and quantitative characteristics of the actual records the number of incidents per day the duration of each incident the time of the day most likely for an incident to occur and the flowrate of the event an additional criterion to such a simulation is that when summing all simulated consumptions the total water consumed should follow closely the actual total water consumption both during the day and during the year as a whole components of the overall time series such as trend and seasonality if any should be traced as well the methodology developed for building the flowrate time series table follows a stochastic approach agnostic to behavioral or exogenous factors that satisfies each one of the aforementioned characteristics it is extensively described throughout the following steps of phase 1 and phase 2 steps 1 to 4 correspond to the calibration and distribution fitting phase 1 step 1 preprocessing the procedure starts with the transformation of raw data into a time series for the time series mode a 30 sec time step is kept constantly time continuity is not interrupted when there is no record on the contrary time periods when the faucet is off are denoted with the phrase no record in order to keep the same time frame for all incidents the starting point of each incident is moved to the next 30 sec time step thus transforming all time stamps to time steps in the produced time series fig 1 step 2 creating incident tables tables are created for each day of the recorded data in which time steps with a recorded flow rate value are denoted with 1 and time steps with no records are denoted with 0 this means that 1s stand for water taps being on and 0s for water taps being off fig 2 step 3 estimating binomial distribution the binomial distribution of 1 0 is estimated for each time step across all 400 recorded days this way the probability of occurrence of an incident at each 30 sec time step is defined fig 3 at this point it is noted that the probability of occurrence throughout the day is indicative of the consumption pattern for each household faucet which means that rush hours in terms of water use are expected to behold higher probabilities of a water incident consistent to the combined day routines of the householders in fig 4 a and b two consumption profiles in two households are presented the specific consumption profiles are chosen to be presented among all because they indicate distinctive differences the first profile shows 1 peak in the afternoon around 2 pm while the second shows two peaks one in the afternoon and one in the evening moreover the second profile shows throughout the whole day higher level of probability of an incident faucet use this does not necessarily mean that the water consumption is higher in the second profile since the flowrates for the given incidents might be significantly higher in the first case it only means that the faucet in the second householder is used more often than the first householder step 4 investigating the flowrate values distribution the distribution of the flowrate variable is investigated by using the steps described in the kolmogorov smirnov k s methodology hollander and wolf 1973 popular distributions namely gauss gamma exponential and beta are tested in order to conclude on the most suitable distribution that simulates the values of flowrate that occur in each water supply point in order to define the distribution of recorded flowrate values all recorded values available of every incident are set from lower to higher value and are divided into 20 classes highest suggested number of classes following to the typical procedure for building a pearson histogram dean and illowsky 2009 the frequency of occurrence for each class is estimated fig 5 equivalent frequencies of a hypothetical sample which keeps the same mean and standard deviation with the real flowrate values are estimated for the 4 tested popular distributions gauss gamma exponential and beta the maximum vertical distances d between the cumulative frequencies at the center of each class of actual data distribution and the ones tested are estimated the minimum of these distances is considered to give the distribution to better mimic the actual distribution fig 6 the following steps concern the generation of synthetic data for a 24 hr period phase 2 step 5 generating table of incidents a table of 1s and 0s is produced through a random generator following the binomial distribution estimated for each time step step 3 this way a number of 30 sec records is produced consistent to the probability of incident occurrence for a household water supply point naturally more 1s are expected to be generated during the rush hour and more 0s when faucet use is low step 6 filling incidents with flowrate values the time steps with 1s step 5 are now filled in with values records generated by a random generator following the flow rate distribution as it was derived in step 4 this way average level standard deviation and distribution of simulated flowrates are kept consistent with actual flowrates since they are based on all flowrates in the sample the same is true for the number of 30 sec records that the faucet was on step 5 and 6 could actually be merged into a single step by using a joint probability distribution of the two variables incident occurrence and flowrate level however this is not applied in the present work keeping the steps separate helps having a more detailed overlook of the procedure and specify its weaknesses and strengths in parts at this point the simulated total average daily consumption is also kept consistent with actual data the problem that remains is that generated 30 sec records are mostly isolated and widely spread throughout the day and are not properly clustered to simulate the observed water consumption incidents this is addressed in the next step of the algorithm step 7 clustering incident values the way the algorithm is set up steps 1 through 6 it randomly produces 30 sec records throughout the day these records are mostly isolated and not clustered in continuous events water consumption incidents that last a few minutes for example it is possible although unlikely that the algorithm will randomly produce two or more 30 sec records in a row creating a simulated incident our intention is to reproduce not only the cumulative water consumed in a day but also to reproduce the number of water consumption incidents in a sense the number of times the faucet is used during the day to achieve this we created a clustering step step 7 that clusters generated isolated records into realistic multiple time step incidents we use the law of inverse square distance newton 1999 in order to produce these clusters specifically each incident either an isolated record or multiple continuous records generated is treated as a particle with attribute level equal to its flowrate particle will be considered an incident that is generated and is followed by a no record while its attribute will be the sum of all flowrates if it comprises more than one record the number of time steps between incidents is considered to be the distance between the particles for every pair of consequent incidents of m1 flowrate1 and m2 flowrate2 which have a distance of d number of time steps the attractive forces are calculated for all neighboring particles throughout a 24 hr period according to the inverse distance square law equation 1 the highest force drives the first particle movement stacking together the two neighboring particles on which the highest force is exerted and creates a new particle cluster keeping the number of original time steps the same the newly created particle is placed in the timeline at a location that lies in between their initial positions this location is defined by the fraction of their flowrates so that the resulting cluster is placed closer to the larger particle according to equation 2 the clustering procedure shown schematically in fig 7 iterates for the generation of a day s data until the number of clusters reaches a desired value that value comes out by a random generator consistent to the gaussian distribution of cluster numbers per day for the whole sample of each household water supply point so that the mean value and standard deviation of the number of clusters of a day are preserved 1 f m 1 m 2 d 2 f l o w r a t e 1 f l o w r a t e 2 n u m b e r o f t i m e s t e p s 2 2 m 1 m 2 r 2 r 1 2 where r1 and r2 are the distances number of timesteps from m1 and m2 equivalently of the point the two particles will meet a schematical flowchart of the developed methodology its phases and steps is presented in fig 8 2 2 2 method modification for sosnowiec data we initially suspected that water consumption in sosnowiec exhibits weekly seasonality since the area is strictly urban residential and the working and lifestyle routines result into different water consumption patterns from weekdays to weekends a statement also supported in the literature arampatzis et al 2014 in fig 9 we show a sample of water consumption patterns for a household in sosnowiec where we see that the two probability of having a record functions are different further investigation into this is presented in the results and discussion section where we show that indeed water consumption shows weekly seasonality accordingly the algorithm was modified to reproduce results consistent with this variable pattern initially data were separated into weekday and weekend observations steps 1 and 2 were kept the same while steps 3 and 5 were modified to implement separately weekday and weekend binomial distributions as a result separate data series for weekdays and weekends are generated by different incident binomial distributions step 4 does not change since we assume that the user will not modify faucet use he she will not turn the faucet on to a higher setting for example depending on the day of the week therefore we used the same flowrate level distributions for all days of the week just like the greek case study the approach with a separate dataset for each day of the week can be adopted once longer time series of data will be available the weekday weekend pattern approach is supported against the basic no pattern approach by a trial and error process this means that data were generated with both approaches and the weekday weekend approach gave better fitting according to the validation procedures as described next distinguishing seasonality by observing the week and weekend profiles of the households consumption is not always conclusive since in most of the cases the mean and its high and low intervals are the same but the diffence may lie within the distribution of the same consumption timely 2 3 validation methodology for validating the methodology data sets for one year of water consumption are generated for each water supply point for both case studies for these data sets we produced curves of the probability of occurrence of an incident for both simulated and actual consumptions this way we validated the incident occurrence variable next the generated flowrate values are validated by checking the fitting in simulating actual average flowrates an important criterion for validating the methodology is based on the difference between total water consumptions generated by the sdg algorithm and actual water consumptions recorded for each pilot the generated data should neither overestimate nor underestimate significantly the total water consumption this difference in cumulative consumptions is calculated for different time scales so it is not only calculated for the whole simulating period additionally we divide the day into 4 quarters early morning 24 00 06 00 morning 06 00 12 00 afternoon 12 00 18 00 and evening 18 00 24 00 and calculated cumulative consumptions for these quarters this way we group consumptions and check the algorithm ability to capture consumption patterns throughout the day the fraction of average quarterly consumption relative to the average daily consumption is also considered as a critical indicator for capturing the water consumption distribution throughout the pilots 24 h periods methodology validation steps and metrics used are presented in table 1 3 results and discussion before the actual generation of consumption data and in order to perform step 4 of the algorithm the distribution of the flowrate variable is investigated gaussian exponential gamma and beta distributions are examined in order to identify the one that describes best the probability distribution of water consumption values for each data set one of the four distributions emerges as closest to the actual one in table 2 it is shown that almost for all pilots and for both cities the exponential distribution is the closest one only or few water supply points namely skiathos 1 6 and 10 gamma distribution was proved to be the most suitable however improvement over the exponential distribution was only marginal therefore in order to keep the algorithm uniform we selected the exponential distribution for generating actual flowrate values for both cities however we do not suggest generalizing that the probability distribution of any faucet flowrate would be exponential since different kinds of fixtures such as the ones operated by foot or users with different habits mentality or physiology might produce values of another distribution the flowrate value depends on the way the faucet is operated some users turn it on all the way to the highest setting some let it drip slowly while different fixtures could affect this value as well sink or bath sets with separate levers for hot and cold water will probably have different flowrate profiles than mixer faucets this kind of distribution investigation is suggested as a pre processing step for dealing with a new household faucet tap before implementing the data generation algorithm we use of the sdg algorithm to generate water consumption data for about one year for each of the 10 skiathos and 9 sosnowiec pilots in figs 10 and 11 comparative curves of average flowrate for actual denoted with red and generated denoted with grey data are presented it is apparent that household consumption patterns are quite variable with some of them exhibiting one or two distinct peaks throughout the day and some showing no distinct peak at all it is also apparent that the suggested methodology can simulate quite successfully the water consumption pattern of each household not only does the method capture the peaks of the household pattern and the usual early morning very low or even zero consumption but it also captures slight variations in microscale granularity for a quantitative comparison of actual and generated data we calculate for both actual and simulated values for the whole data set i the probability of having a record at each 30 sec time step and ii the average flowrate for each 30 sec time step including the zero values that correspond to a no record we then produce scatter plots of generated and simulated data for i and ii and we calculate the r2 values all scatter plots of flowrate values are shown in fig 12 while a summary table including the r2 of the probability of having a record variable is also provided table 3 it should be noted that the r2 values are indicative of a very detailed resemblance a 30 sec granularity resemblance thus it is a quite strict metric for the validation of the method skiathos r2 probability of having a record values are very high most of them over 0 80 and two of them even reaching 1 00 two values are relatively low 0 37 for pilot 2 and 0 30 for pilot 8 even if the corresponding comparative diagrams do not show any particular inconsistency pattern between actual and simulated this is due to the fact that consumption values for those houses are lower than those of the other households in the greek case study for sosnowiec the performance of the model in capturing the record probability or faucet appliance use during the day is high with r2 values higher than 0 70 except for pilot 7 which has a lower r2 equal to 0 43 generally however the method for sosnowiec performs slightly worse than for skiathos this is because the method performance seems to be related to the frequency of incident occurrence as described in section 2 1 sosnowiec households have more water taps and as a result total household consumption is divided among more water supply points than in skiathos as a result each water tap is used less frequently when compared to skiathos resulting in less data overall in table 3 this relevance can be denoted by comparing the incident frequency in incidents month for skiathos and sosnowiec it is also quite reasonable that in skiathos pilots the ones performing relatively poorly are the ones with very low use frequency namely pilot 2 with 129 incidents per month and pilot 8 with 153 incidents per month lower but still high performance results are derived from the flowrate r2 values table 3 most of the r2 values 13 out of 19 are higher than 0 5 and go as high as 0 99 or 1 00 for the case of skiathos pilots 1 and 7 lower performance for this variable is reasonable since its performance is contingent on the performance of the probability of having a record in order to capture the exact average flowrate value one needs to first capture the probability of having a record and then produce a realistic flowrate value if the algorithm predicts a low probability of having a record at a time step then we expect it to produce multiple 0 values no records which will bring the average flowrate value down to lower values thus reducing the performance of the flowrate variable as expected the pilots that performed more weakly for the probability of having a record variable are the ones that also performed relatively weak for the flowrate variable the sdg algorithm generally performs slightly worse for sosnowiec than for skiathos pilots regarding the r2 values an important reason for this difference is the weekday weekend routine which although necessary it creates the issue of less data being available for the algorithm since it splits the sample in two statistical measures that describe the incident binomial and flowrate exponential distributions come out of a lower number of samples overall additionally and for the same reason it is observed that rarely used water taps make nosier flowrate curves we expect that higher use frequency of a faucet will lead to more data being collected for that pilot thus leading to smoother average flowrate curves finally skiathos consumption profiles depict more characteristic signatures easier for the algorithm to capture in table 4 the percentage differences of generated minus actual water volumes for all simulated days are presented this comparison is done for validating the ability of the model to simulate cumulative water consumptions for each pilot for most of the pilots the difference is kept really low lower than 3 we can see that this percentage shows sometimes slight overestimation and sometimes slight underestimation of total water consumption which means that the method in total is not showing some biased tendency to go in either direction the higher differences are shown for skiathos water pilot 3 17 76 and 6 23 39 and for sosnowiec water pilot 9 12 6 while very low differences less than 1 are achieved for 5 out of 19 pilots the overall better performance of sosnowiec pilots regarding percentage differences is more deceptive than indicative of the actual comparative performance this is because the flowrate average levels in sosnowiec pilots are generally significantly lower than those of skiathos pilots thus resulting to higher apparent percentage differences that however correspond to lower actual volumes of water in table 5 the quarter percentages of actual and generated data are compared here we show which fraction of daily water consumption is realized in each quarter of the 24 hr period so for skiathos pilot 1 0 41 of the daily water consumption happens from midnight to 6 am while 39 52 of it happens from 6 am to noon in the following row the same percentages are calculated for the generated data while in the row below the differences between actual and generated fractions are calculated as it is expected the pilots that performed highly in the r2 values of incident probability and flowrate variables perform even better in this less strict performance indicator we also see that pilots that performed the poorest in terms of r2 for both incident probability and flowrate skiathos pilots 2 and 8 and sosnowiec pilot 7 actually perform quite well at the quarter scale this might be a less detailed view on the method validation however it is more meaningful since it is more indicative of the method s ability to capture the usage pattern rather than each single per 30 sec flowrate value in an overall evaluation of the method it is deduced that the proposed sdg algorithm has the ability to perform adequately simulating with satisfying precision the water use consumption patterns in the household the stochastic structure of the model allows to agnostically simulate consumption regardless of behavioral or exogenous factors two factors apparently linked to the performance of the methodology are the density of recorded incidents number of records and the time length of the recorded data the method can also accommodate previously identified regular variations such as seasonal weekday weekend features annual seasonality if any could also be treated either by producing days from equivalent days recorded data or by estimating the seasonality curve producing the residual data from residual recorded data recorded data minus estimated seasonality and then adding the seasonality to the generated residuals the former approach is expected to be more accurate but would need much more data to be implemented than the latter approach a possible detection of a trend component could also be dealt with the latter approach specifically by abstracting trend by the raw data generating de trended residuals and then adding trend to locate the generated consumption data to time wanted with further investigation of the applicability of the algorithm to the de seasonalizing and de trending techniques the potential of using the proposed sdg method as a forecasting tool is enhanced especially when combined with advanced water consumer clustering techniques such as self organizing maps som laspidou et al 2015 a user might be able to generate tap water data by feeding the data generator with known or easy to find parameters even nominal such as the size or age of the household or the working status and ages of the residents of the household etc another possible use of the algorithm could be that of implementing a monte carlo type simulation of daily water consumption of all individual households in an agglomeration or district metered area dma this way we can identify extreme values lowest and highest possible for water consumption of all households this could be a useful planning tool that can help the water provider identify maximum possible consumptions and corresponding times of the day that they can occur thus quantifying the risk of failing to meet water demand such information can be useful in identifying weak links in a water supply network which could be related to reservoir or groundwater levels network capacity meeting required pressure levels in the network etc such a direction could give a useful tool to the water security domain one might wonder could this tool be used to model water consumption in a whole city and in that case how many households would we need to have data from in order to be able to reliably draw conclusions about the whole city the answer is that the parameters of population n and tolerated error e of simulated households in a city should be used to define the size of the sample that is representative of the population särndal et al 2003 i e the number of households with data needed obviously such a tool allows the user to use only a limited number of households in order to draw conclusions for a much larger sample further to that stratified sampling nassiuma 2001 can be used this technique suggests that a sample can be more representative when the population is divided into strata of common defined characteristics clustering algorithms can be considered pre cursors of stratified sampling laspidou et al 2015 applied the som algorithm to cluster water meter data of a small town into different categories such as households hotels and small enterprises using 3 month water consumption data the aforementioned imply that the scalability of the model can be enhanced by the use of clustering algorithms which would define the number of representative pilots needed respectively to the number of taxa suggested there is clearly potential in the use of such an algorithm especially when combined with clustering techniques given enough representative households with water consumption data of fine granularity the possibility of expanding its use to the whole city is to be considered 4 conclusions a method for generating meaningful water consumption data is introduced the sdg algorithm differs from past algorithms within the pulse model group because it introduces early on the probability of having a water consumption incident at each time step as a critical variable to the simulation that variable is modeled with the use of the binomial distribution this way it ensures that the consumption pattern is captured the flowrate values are modeled with the use of the estimated frequency distributions a reverse square distance based subroutine is introduced for the reproduction of the duration variable of each event by clustering the reproduced incidents the challenges of such a task are to mimic the characteristic patterns of each enduse to simulate the water flowrate levels and durations of water consumption events and to preserve the water volume consumed at a large time scale such as that of 400 days the methodology is implemented and tested for two case studies with different characteristics namely skiathos greece and sosnowiec poland pilots in skiathos a small greek island with rural life style exhibit higher consumptions that show no seasonality while pilots in sosnowiec an industrial urban center are characterized by lower consumptions and a weekday weekend seasonality pattern a number of qualitative and quantitative tests are implemented to check whether the objectives of the study are satisfied the suggested methodology can successfully reproduce data that mimic the source consumption patterns following the actual peaks and lags recorded and consumption characteristics such as the number and length of water use incidents per day the method successfully captures the cumulative yearly water balance for each pilot the algorithm becomes more robust when more recorded data is available and when more frequent water use incidents are recorded the method seems to adjust well when the weekday weekend pattern is evident the tool developed can provide meaningful water consumption data for reproducing gaps of data and missing values the suggested approach can constitute a potential basis for building up a tool which will support tasks demanding long water consumption data series such as implementing and testing dsss supporting the development of models of user behavior risk assessment of critical water consumption design parameters or developing water use fraud detection systems finally the algorithm can be used to reproduce water consumption data when privacy constraints hinder the use of actual time series acknowledgements this work was supported by the project iss ewatus integrated support system for efficient water usage and resources management which is implemented in the framework of the eu 7th framework programme specific programme cooperation information and communication technologies grant agreement number 619228 
26446,in the smart cities context real time knowledge of residential water consumption has become increasingly important especially given the fast evolution of sensors ict and the production of big high resolution data coming from the urban environment a variety of reasons often leads to the creation of continuity gaps in these data series thus making the need for a methodology that produces reliable and realistic synthetic data urgent in this article we present a methodology that generates synthetic household water consumption data we showcase it in two case studies skiathos greece and sosnowiec poland which exhibit significant differences in water consumption patterns the methodology captures the stochasticity of daily residential water use algorithm validation is implemented through the comparison of various metrics for actual and generated data this way we show that the suggested approach is capable of adequately simulating water consumption in both micro and macro time scale keywords synthetic water consumption data generation pulse models missing data water consumption patterns data availability the data used in this article are water consumption data collected by a total of 16 households for a period of 13 months starting from february 1st 2015 in two locations skiathos greece 10 sampling points faucets each one corresponding to a different household and sosnowiec poland 9 sampling points faucets and appliances in 6 households the water consumption monitoring system was installed in a diverse group of households that were specifically chosen in order to provide needed data to help comprehend human behavior and water consumption patterns by different users in a household in various socio economic settings the criterion for the selection of the households was the availability and promptness of the housekeepers additionally the households were chosen so that they were diverse regarding their location in the network and number of occupants wireless sensors were installed in various sampling points in the households i e faucets washers and showers 30 second step records were transmitted to a remote central server in real time technical details on the water consumption monitoring system are provided in chen et al 2015 the data used are available online at validation issewatus eu data re use this work was undertaken for the ec fp7 project iss ewatus 2016 1 introduction key to a smart city concept is the idea of measurement of instrumenting the urban landscape and associated activity and monitoring their state and behavior in a way that leads to technological governmental and societal advances it has been said that you can t manage what you can t measure which greatly applies to a city of the future in which near real time measurements enable stakeholder awareness engagement and quick response to new conditions thus leading to a new model of civic behavior and involvement this new paradigm is based on almost individualized planning on one hand and near real time information on another lim et al 2010 a recent study cominola et al 2015 reviews water smart metering projects taking place in the last decades worldwide according to this work these projects which focus on real time water use monitoring at high spatial and temporal granularity stimulate modeling approaches and behavior adaptive urban water management strategies consumer awareness campaigns have been documented in the literature in the last decade russel and fielding 2010 novak et al 2016 perren and yang 2015 shan et al 2015 while latest advances include the development of gaming platforms wang and capiluppi 2015 for water management and the involvement of social media for citizen engagement in water saving practices the european commission has funded a series of research projects that developed a series of diverse case studies that all showed how building consumer awareness could limit water consumption all these projects are grouped under the ict4water cluster http ict4water eu a number of water utilities increasingly attempt to influence the behavior of consumers towards improving water consumption by using communication tools to give information back to users and display their consumption or customized feedbacks or water saving tips at the same time various companies have been established lately that specialize solely on transforming the way customers think about their household water consumption as well as the way utilities engage with their customers such companies combine machine learning ml and other data science tools with cloud computing and behavioral science to develop software as a service solution to customer engagement and efficiency issues faced by utilities subsequently the need for the collection and management of large quantities of temporal and spatial high resolution data emerges as the core of urban planning while at the same time the radical evolution in the technological sector of sensors information and communication technologies icts social network data analysis and data mining dm techniques reveals new potentials for more efficient planning laspidou 2014 laspidou et al 2015 yang et al 2017 in the urban water domain due to fast urbanization increasing demands climate change and high pressure on water resources research activity increasingly focuses on monitoring understanding and better managing urban water activities detailed monitoring of household water consumption can reveal useful information about citizen behavioral patterns not only related to their water use per se but also concerning a range of socio economic factors directly or indirectly related to water such as circadian rhythms working hours daily habits house amenities familial structure and profile etc furthermore the spatiotemporal analysis of household water use can help make water consumption a key indicator of human behavior thus helping authorities and relevant stakeholders identify changes in city living conditions such as local development migration epidemics or it can disclose population shifts due to events such as terrorist attacks natural disasters large scale organized meetings or tournaments etc besides the wealth of information potentially extracted by monitoring water consumption channeling this data back to the consumers will contribute to an increased awareness that will lead to a smaller household water footprint lanzarone and zanzi 2010 perren and yang 2015 al hoqani and yang 2015 the effectiveness of similar schemes regarding energy consumption through energy metering billing and direct display methodologies has already been documented darby 2006 concluding that feedback to consumers is an important element of an energy savings scheme for consumers numerous relative examples are reported in ehrhardt martinez et al 2010 and fischer 2008 works indicatively in the staats et al 2004 study an energy savings increase of approximately 3 in 16 months in netherland is reported and in wilhite and ling 1995 study an increase of 2 4 in energy saving in one year for a norway case study is presented in fields such as dm ml and knowledge discovery from databases kdd a commonly emerging issue which is the main focus of this paper is that of missing values or missing data numerous reasons can lead to such a problem equipment malfunctions refusal of respondents to fill in questionnaires and gathering of erroneous data etc schafer and graham 2002 batista and monard 2002 demand management initiatives rely on good comprehension of water usage practices as well as of factors influencing water demand white et al 2003 the emerging data driven demand management has been supported by cloud based data platforms and represents a new critical element to improve decision making in today s water industry utility managers can achieve the sustainability and affordability objectives they desire through the practical application of data analytics fielding et al 2012 in this context the implication of data gaps is really important since the decision making process relies on continuous data sets such continuous data sets improve the resilience of new decision making schemes based on the reason why a gap is created missing data is categorized into three classes depending on the level of randomness of the incident missing completely at random mcar missing at random mar and not missing at random nmar are commonly used classes that imply that the incident either does not depend on the missing value or depends on a related to the value attribute or directly depends on the value respectively little and rubin 1987 little 1988 an example for a mcar would be the interruption of functioning of a sensor that would create a gap no matter what the measurements would be an example for mar would be the absence of answer in a questionnaire about an attribute that is indirectly related to the gender of the respondent an example for nmar would be the case of a sensor not recording a value because it lies outside its measuring capacity range thus a missing or erroneous value would imply that it is out of this range the level of randomness is conclusive of the method that the missing data are treated depending on the class that the data gap belongs to a different methodology for treating the missing data is selected another criterion for choosing the method to treat an incident of missing value is the nature of the attribute specifically if the attribute were a time series the treatment would involve analysis of components such as trend and seasonality moreover if the missing attribute value were correlated to another known attribute then the method of treatment would be selected based on this correlation which would imply the implementation of multivariate analysis as opposed to univariate lastly a criterion is the length of the missing part this can vary from a single missing value to a larger gap of data the aforementioned criteria are decisive of the treatment of an incident variable methods are applied for this purpose some commonly applied tactics include ignoring and discarding the incident case substitution mean or mode imputation hot deck and cold deck method applying a predictive model and others lakshminarayan et al 1999 grzymala busse and hu 2001 batista and monard 2003 the imputation of a missing value is generally classified into deterministic or stochastic rao 1996 other than filling missing data gaps the production of data that mimic the properties of a data set synthetic data can be essential in situations in which available real data are limited and longer data sets are required for evaluation validation and or testing of models platforms algorithms or decision support systems dsss barse et al 2003 define synthetic data as generated data by simulated users in a simulated system performing simulated actions a typical example of need for synthetic data is the case when privacy constraints block the direct use of original sets in other words water utilities may not agree to provide actual water consumption data being concerned about violating the privacy of their customers even if data is anonymized synthetically generated data overcome problems related to data privacy cominola et al 2016 in such cases the use of a tool that provides synthetic water consumption data will serve well the needs of water utilities including decision making platforms used in data driven demand management schemes another example is the training and adapting of a fraud detection system fds on a synthetic data set testing its properties by injecting synthetic frauds or comparing the performance of different fdss barse et al 2003 past research works have focused on investigating whether urban water consumption time series can be simulated in multiple temporal and spatial scales pulse models developed for creating such artificial time series generally consider two variables the duration and intensity of a consumption event these models can be divided into two categories the ones that simulate pulses of specific end use fixtures and the ones that are parameterized to simulate overall household water demand creaco et al 2017 buchberger and wu 1995 used the poisson rectangular pulse prp methodology to simulate the behavior of the water consumer considering actual monitored household consumption alvisi et al 2003 introduced a cluster neyman scott stochastic process to simulate residential water demand respectfully to the cyclical behaviour observed during a typical working day blokker et al 2009 developed a methodology for simulating water consumption at residence level using 8 end uses bathtub dishwasher etc patterns based on survey data and technical characteristics of the appliances cominola et al 2016 used data from 300 households in 9 u s cities and developed a stochastic simulation model for the generation of residential water end uses based on the assumption that each end use is characterized by a unique signature creaco et al 2015 proved that taking into account dependence between duration and intensity variables can improve the pulse approach performance creaco et al 2016 focused on parameterizing the values of the aforementioned model which were associated with the model variables respectfully to the water balance characteristics in multiple time scales they concluded that high accuracy in smart metering relates to better performance of the model in the 2017 study of di palma et al the overall pulse model was introduced to describe aggregated water consumption this model does not generate single end use pulses but the water consumption of a whole household as recorded at a water meter a methodology on filling a gap of water consumption data with meaningful values is presented in this article the suggested methodology is in accordance to the established residential water demand pulse models since it is based on simulating the user behavior while considering characteristic variables such as intensity of flow and duration the methodology differs from other established approaches in the fact that it firstly captures consumption patterns throughout the day and then introduces a novel algorithm that simulates the duration of the incident the suggested innovative algorithm for simulating the duration attribute gives a valuable degree of freedom that allows dealing with the consumption pattern beforehand thus the methodology overall facilitates capturing precisely the pattern the water consumption data concern household water consumption and are collected for the purpose of investigating the effect of real time monitoring and informing consumers about their own water consumption on their water use behavior through an ict supported consumer awareness process the methodology is developed validated and applied to the created gap in order to retrieve required data sets to be used for the development of a dss platform developed in the context of the ec funded project iss ewatus 2016 in this article the two study areas and the data characteristics are firstly described next the methodology is presented in a step by step format for the base case study of skiathos households as well as its modification for sosnowiec households the method validation process and results are presented and discussed the article finishes with a series of conclusions and implications of the new data generation tool for the urban water sector 2 materials and methods 2 1 case study in the context of the iss ewatus project sensors were installed in faucets showers and appliances in multiple households in skiathos island greece and sosnowiec poland for the purpose of this article data sets from 16 households were used 10 and 6 with 10 and 9 sampling points respectively faucets at skiathos and faucets and appliances at sosnowiec the monitoring period was initiated on february 1st 2015 while data collection is still ongoing as of november 2016 technical issues during the installation in skiathos island delayed the initiation of data recording which officially began on april 14 and created a data gap of 72 days from february 1st to april 14 2015 for the greek case study the initial motivation for the synthetic data generating sdg methodology presented in this article was to fill the data gap that was created in the greek case study once this goal was reached we extended the methodology to a generic sdg tool capable of producing synthetic data based on historical water consumptions to ensure that the applicability of the developed sdg tool is not limited to the water consumption profiles of skiathos for which it was developed we further tested its robustness by generating data for the polish case study that exhibited different water consumption profiles the two case studies are very different in terms of socio economics demographics climate and geography factors that are all expected to influence water consumption patterns skiathos is an island with high seasonal touristic activity and seasonal weather variability kofinas et al 2014 2016 mellios et al 2015 however for the skiathos case study water sensors are installed at typical non touristic households where the impact of tourism and weather variables is not that significant it should be noted that in skiathos the sensors are installed at kitchen faucets a factor that diminishes the seasonal weather impact on water consumption in contrast to water consumption in the bathroom which is related to showers and baths influenced by weather a third factor to conclusively define the pattern of water consumption is that water in skiathos is announced to have high concentrations of mercury thus not potable water is used mostly for washing and cleaning furthermore skiathos is a small village of about 6000 people with small family owned businesses without large companies and corporations this corresponds to a traditional lifestyle with extended families living together and stay at home parents or grandparents as a result the water use pattern differs from one of a large city with urban lifestyle and exhibits no detectable weekday weekend water consumption variability water use in the skiathos case study shows no seasonality or trend for the aforementioned reasons finally pilot households in skiathos are located in the old town where houses are often over 100 years old and amenities are limited usually houses are equipped with a total of 3 or 4 faucets mostly kitchen and bathroom with dishwashers not being a typical appliance the significance of this is that the total household water consumption is split among these few faucets and the water consumption in the kitchen where the sensors were installed is a significant part of the total household consumption sosnowiec is a city in the katowice urban area with 2 7 million people with strong urban dynamics it is a typical industrial city with heavy industries companies and other associated economic activity as far as the water consumption case study is concerned sosnowiec reveals an urban lifestyle and the corresponding water use mode is characterized by variability in water consumption between weekdays and weekends the sensors installed at sosnowiec are in showers bathtubs kitchens appliances balconies etc thus the patterns of use are expected to be more variable offering a challenging case study for testing the developed sdg methodology moreover the fact that the sosnowiec pilots have multiple water supply points in the household especially when compared to those in the skiathos pilots the total average daily consumption in each sosnowiec water supply point is overall lower than those in skiathos 2 2 data description the data sets refer to water flowrate values that are recorded every 30 sec of water consumption once there is flow in the monitored faucet appliance the sensor generates a record with the corresponding timestamp at the end of the 30 sec period it records the water consumption in liters min during the 30 sec period if the faucet appliance is still on the next timestamp is recorded and then the corresponding consumption and so on every 30 sec the sensor checks for flow and when the faucet appliance is off no record is produced thus finalizing the creation of a water consumption incident record when the next water consumption incident starts the procedure repeats itself for the period in between the two incidents no record is produced all incidents themselves have a 30 sec time step but the starting time of an incident might be for example 45 s after the previous one this means that sensor produced data are not in the form of a single time series with a 30 sec time step but are recorded in the form of numerous clusters each one representing a small time series of the equivalent incident in reality in order to distinguish between incidents one detects when the time distance between two consecutive records is greater than 30 s as shown in fig 1 a the number of records per incident is used to calculate its duration 2 2 1 method description an algorithm that would generate flowrate records for a household water supply point should simulate all meaningful qualitative and quantitative characteristics of the actual records the number of incidents per day the duration of each incident the time of the day most likely for an incident to occur and the flowrate of the event an additional criterion to such a simulation is that when summing all simulated consumptions the total water consumed should follow closely the actual total water consumption both during the day and during the year as a whole components of the overall time series such as trend and seasonality if any should be traced as well the methodology developed for building the flowrate time series table follows a stochastic approach agnostic to behavioral or exogenous factors that satisfies each one of the aforementioned characteristics it is extensively described throughout the following steps of phase 1 and phase 2 steps 1 to 4 correspond to the calibration and distribution fitting phase 1 step 1 preprocessing the procedure starts with the transformation of raw data into a time series for the time series mode a 30 sec time step is kept constantly time continuity is not interrupted when there is no record on the contrary time periods when the faucet is off are denoted with the phrase no record in order to keep the same time frame for all incidents the starting point of each incident is moved to the next 30 sec time step thus transforming all time stamps to time steps in the produced time series fig 1 step 2 creating incident tables tables are created for each day of the recorded data in which time steps with a recorded flow rate value are denoted with 1 and time steps with no records are denoted with 0 this means that 1s stand for water taps being on and 0s for water taps being off fig 2 step 3 estimating binomial distribution the binomial distribution of 1 0 is estimated for each time step across all 400 recorded days this way the probability of occurrence of an incident at each 30 sec time step is defined fig 3 at this point it is noted that the probability of occurrence throughout the day is indicative of the consumption pattern for each household faucet which means that rush hours in terms of water use are expected to behold higher probabilities of a water incident consistent to the combined day routines of the householders in fig 4 a and b two consumption profiles in two households are presented the specific consumption profiles are chosen to be presented among all because they indicate distinctive differences the first profile shows 1 peak in the afternoon around 2 pm while the second shows two peaks one in the afternoon and one in the evening moreover the second profile shows throughout the whole day higher level of probability of an incident faucet use this does not necessarily mean that the water consumption is higher in the second profile since the flowrates for the given incidents might be significantly higher in the first case it only means that the faucet in the second householder is used more often than the first householder step 4 investigating the flowrate values distribution the distribution of the flowrate variable is investigated by using the steps described in the kolmogorov smirnov k s methodology hollander and wolf 1973 popular distributions namely gauss gamma exponential and beta are tested in order to conclude on the most suitable distribution that simulates the values of flowrate that occur in each water supply point in order to define the distribution of recorded flowrate values all recorded values available of every incident are set from lower to higher value and are divided into 20 classes highest suggested number of classes following to the typical procedure for building a pearson histogram dean and illowsky 2009 the frequency of occurrence for each class is estimated fig 5 equivalent frequencies of a hypothetical sample which keeps the same mean and standard deviation with the real flowrate values are estimated for the 4 tested popular distributions gauss gamma exponential and beta the maximum vertical distances d between the cumulative frequencies at the center of each class of actual data distribution and the ones tested are estimated the minimum of these distances is considered to give the distribution to better mimic the actual distribution fig 6 the following steps concern the generation of synthetic data for a 24 hr period phase 2 step 5 generating table of incidents a table of 1s and 0s is produced through a random generator following the binomial distribution estimated for each time step step 3 this way a number of 30 sec records is produced consistent to the probability of incident occurrence for a household water supply point naturally more 1s are expected to be generated during the rush hour and more 0s when faucet use is low step 6 filling incidents with flowrate values the time steps with 1s step 5 are now filled in with values records generated by a random generator following the flow rate distribution as it was derived in step 4 this way average level standard deviation and distribution of simulated flowrates are kept consistent with actual flowrates since they are based on all flowrates in the sample the same is true for the number of 30 sec records that the faucet was on step 5 and 6 could actually be merged into a single step by using a joint probability distribution of the two variables incident occurrence and flowrate level however this is not applied in the present work keeping the steps separate helps having a more detailed overlook of the procedure and specify its weaknesses and strengths in parts at this point the simulated total average daily consumption is also kept consistent with actual data the problem that remains is that generated 30 sec records are mostly isolated and widely spread throughout the day and are not properly clustered to simulate the observed water consumption incidents this is addressed in the next step of the algorithm step 7 clustering incident values the way the algorithm is set up steps 1 through 6 it randomly produces 30 sec records throughout the day these records are mostly isolated and not clustered in continuous events water consumption incidents that last a few minutes for example it is possible although unlikely that the algorithm will randomly produce two or more 30 sec records in a row creating a simulated incident our intention is to reproduce not only the cumulative water consumed in a day but also to reproduce the number of water consumption incidents in a sense the number of times the faucet is used during the day to achieve this we created a clustering step step 7 that clusters generated isolated records into realistic multiple time step incidents we use the law of inverse square distance newton 1999 in order to produce these clusters specifically each incident either an isolated record or multiple continuous records generated is treated as a particle with attribute level equal to its flowrate particle will be considered an incident that is generated and is followed by a no record while its attribute will be the sum of all flowrates if it comprises more than one record the number of time steps between incidents is considered to be the distance between the particles for every pair of consequent incidents of m1 flowrate1 and m2 flowrate2 which have a distance of d number of time steps the attractive forces are calculated for all neighboring particles throughout a 24 hr period according to the inverse distance square law equation 1 the highest force drives the first particle movement stacking together the two neighboring particles on which the highest force is exerted and creates a new particle cluster keeping the number of original time steps the same the newly created particle is placed in the timeline at a location that lies in between their initial positions this location is defined by the fraction of their flowrates so that the resulting cluster is placed closer to the larger particle according to equation 2 the clustering procedure shown schematically in fig 7 iterates for the generation of a day s data until the number of clusters reaches a desired value that value comes out by a random generator consistent to the gaussian distribution of cluster numbers per day for the whole sample of each household water supply point so that the mean value and standard deviation of the number of clusters of a day are preserved 1 f m 1 m 2 d 2 f l o w r a t e 1 f l o w r a t e 2 n u m b e r o f t i m e s t e p s 2 2 m 1 m 2 r 2 r 1 2 where r1 and r2 are the distances number of timesteps from m1 and m2 equivalently of the point the two particles will meet a schematical flowchart of the developed methodology its phases and steps is presented in fig 8 2 2 2 method modification for sosnowiec data we initially suspected that water consumption in sosnowiec exhibits weekly seasonality since the area is strictly urban residential and the working and lifestyle routines result into different water consumption patterns from weekdays to weekends a statement also supported in the literature arampatzis et al 2014 in fig 9 we show a sample of water consumption patterns for a household in sosnowiec where we see that the two probability of having a record functions are different further investigation into this is presented in the results and discussion section where we show that indeed water consumption shows weekly seasonality accordingly the algorithm was modified to reproduce results consistent with this variable pattern initially data were separated into weekday and weekend observations steps 1 and 2 were kept the same while steps 3 and 5 were modified to implement separately weekday and weekend binomial distributions as a result separate data series for weekdays and weekends are generated by different incident binomial distributions step 4 does not change since we assume that the user will not modify faucet use he she will not turn the faucet on to a higher setting for example depending on the day of the week therefore we used the same flowrate level distributions for all days of the week just like the greek case study the approach with a separate dataset for each day of the week can be adopted once longer time series of data will be available the weekday weekend pattern approach is supported against the basic no pattern approach by a trial and error process this means that data were generated with both approaches and the weekday weekend approach gave better fitting according to the validation procedures as described next distinguishing seasonality by observing the week and weekend profiles of the households consumption is not always conclusive since in most of the cases the mean and its high and low intervals are the same but the diffence may lie within the distribution of the same consumption timely 2 3 validation methodology for validating the methodology data sets for one year of water consumption are generated for each water supply point for both case studies for these data sets we produced curves of the probability of occurrence of an incident for both simulated and actual consumptions this way we validated the incident occurrence variable next the generated flowrate values are validated by checking the fitting in simulating actual average flowrates an important criterion for validating the methodology is based on the difference between total water consumptions generated by the sdg algorithm and actual water consumptions recorded for each pilot the generated data should neither overestimate nor underestimate significantly the total water consumption this difference in cumulative consumptions is calculated for different time scales so it is not only calculated for the whole simulating period additionally we divide the day into 4 quarters early morning 24 00 06 00 morning 06 00 12 00 afternoon 12 00 18 00 and evening 18 00 24 00 and calculated cumulative consumptions for these quarters this way we group consumptions and check the algorithm ability to capture consumption patterns throughout the day the fraction of average quarterly consumption relative to the average daily consumption is also considered as a critical indicator for capturing the water consumption distribution throughout the pilots 24 h periods methodology validation steps and metrics used are presented in table 1 3 results and discussion before the actual generation of consumption data and in order to perform step 4 of the algorithm the distribution of the flowrate variable is investigated gaussian exponential gamma and beta distributions are examined in order to identify the one that describes best the probability distribution of water consumption values for each data set one of the four distributions emerges as closest to the actual one in table 2 it is shown that almost for all pilots and for both cities the exponential distribution is the closest one only or few water supply points namely skiathos 1 6 and 10 gamma distribution was proved to be the most suitable however improvement over the exponential distribution was only marginal therefore in order to keep the algorithm uniform we selected the exponential distribution for generating actual flowrate values for both cities however we do not suggest generalizing that the probability distribution of any faucet flowrate would be exponential since different kinds of fixtures such as the ones operated by foot or users with different habits mentality or physiology might produce values of another distribution the flowrate value depends on the way the faucet is operated some users turn it on all the way to the highest setting some let it drip slowly while different fixtures could affect this value as well sink or bath sets with separate levers for hot and cold water will probably have different flowrate profiles than mixer faucets this kind of distribution investigation is suggested as a pre processing step for dealing with a new household faucet tap before implementing the data generation algorithm we use of the sdg algorithm to generate water consumption data for about one year for each of the 10 skiathos and 9 sosnowiec pilots in figs 10 and 11 comparative curves of average flowrate for actual denoted with red and generated denoted with grey data are presented it is apparent that household consumption patterns are quite variable with some of them exhibiting one or two distinct peaks throughout the day and some showing no distinct peak at all it is also apparent that the suggested methodology can simulate quite successfully the water consumption pattern of each household not only does the method capture the peaks of the household pattern and the usual early morning very low or even zero consumption but it also captures slight variations in microscale granularity for a quantitative comparison of actual and generated data we calculate for both actual and simulated values for the whole data set i the probability of having a record at each 30 sec time step and ii the average flowrate for each 30 sec time step including the zero values that correspond to a no record we then produce scatter plots of generated and simulated data for i and ii and we calculate the r2 values all scatter plots of flowrate values are shown in fig 12 while a summary table including the r2 of the probability of having a record variable is also provided table 3 it should be noted that the r2 values are indicative of a very detailed resemblance a 30 sec granularity resemblance thus it is a quite strict metric for the validation of the method skiathos r2 probability of having a record values are very high most of them over 0 80 and two of them even reaching 1 00 two values are relatively low 0 37 for pilot 2 and 0 30 for pilot 8 even if the corresponding comparative diagrams do not show any particular inconsistency pattern between actual and simulated this is due to the fact that consumption values for those houses are lower than those of the other households in the greek case study for sosnowiec the performance of the model in capturing the record probability or faucet appliance use during the day is high with r2 values higher than 0 70 except for pilot 7 which has a lower r2 equal to 0 43 generally however the method for sosnowiec performs slightly worse than for skiathos this is because the method performance seems to be related to the frequency of incident occurrence as described in section 2 1 sosnowiec households have more water taps and as a result total household consumption is divided among more water supply points than in skiathos as a result each water tap is used less frequently when compared to skiathos resulting in less data overall in table 3 this relevance can be denoted by comparing the incident frequency in incidents month for skiathos and sosnowiec it is also quite reasonable that in skiathos pilots the ones performing relatively poorly are the ones with very low use frequency namely pilot 2 with 129 incidents per month and pilot 8 with 153 incidents per month lower but still high performance results are derived from the flowrate r2 values table 3 most of the r2 values 13 out of 19 are higher than 0 5 and go as high as 0 99 or 1 00 for the case of skiathos pilots 1 and 7 lower performance for this variable is reasonable since its performance is contingent on the performance of the probability of having a record in order to capture the exact average flowrate value one needs to first capture the probability of having a record and then produce a realistic flowrate value if the algorithm predicts a low probability of having a record at a time step then we expect it to produce multiple 0 values no records which will bring the average flowrate value down to lower values thus reducing the performance of the flowrate variable as expected the pilots that performed more weakly for the probability of having a record variable are the ones that also performed relatively weak for the flowrate variable the sdg algorithm generally performs slightly worse for sosnowiec than for skiathos pilots regarding the r2 values an important reason for this difference is the weekday weekend routine which although necessary it creates the issue of less data being available for the algorithm since it splits the sample in two statistical measures that describe the incident binomial and flowrate exponential distributions come out of a lower number of samples overall additionally and for the same reason it is observed that rarely used water taps make nosier flowrate curves we expect that higher use frequency of a faucet will lead to more data being collected for that pilot thus leading to smoother average flowrate curves finally skiathos consumption profiles depict more characteristic signatures easier for the algorithm to capture in table 4 the percentage differences of generated minus actual water volumes for all simulated days are presented this comparison is done for validating the ability of the model to simulate cumulative water consumptions for each pilot for most of the pilots the difference is kept really low lower than 3 we can see that this percentage shows sometimes slight overestimation and sometimes slight underestimation of total water consumption which means that the method in total is not showing some biased tendency to go in either direction the higher differences are shown for skiathos water pilot 3 17 76 and 6 23 39 and for sosnowiec water pilot 9 12 6 while very low differences less than 1 are achieved for 5 out of 19 pilots the overall better performance of sosnowiec pilots regarding percentage differences is more deceptive than indicative of the actual comparative performance this is because the flowrate average levels in sosnowiec pilots are generally significantly lower than those of skiathos pilots thus resulting to higher apparent percentage differences that however correspond to lower actual volumes of water in table 5 the quarter percentages of actual and generated data are compared here we show which fraction of daily water consumption is realized in each quarter of the 24 hr period so for skiathos pilot 1 0 41 of the daily water consumption happens from midnight to 6 am while 39 52 of it happens from 6 am to noon in the following row the same percentages are calculated for the generated data while in the row below the differences between actual and generated fractions are calculated as it is expected the pilots that performed highly in the r2 values of incident probability and flowrate variables perform even better in this less strict performance indicator we also see that pilots that performed the poorest in terms of r2 for both incident probability and flowrate skiathos pilots 2 and 8 and sosnowiec pilot 7 actually perform quite well at the quarter scale this might be a less detailed view on the method validation however it is more meaningful since it is more indicative of the method s ability to capture the usage pattern rather than each single per 30 sec flowrate value in an overall evaluation of the method it is deduced that the proposed sdg algorithm has the ability to perform adequately simulating with satisfying precision the water use consumption patterns in the household the stochastic structure of the model allows to agnostically simulate consumption regardless of behavioral or exogenous factors two factors apparently linked to the performance of the methodology are the density of recorded incidents number of records and the time length of the recorded data the method can also accommodate previously identified regular variations such as seasonal weekday weekend features annual seasonality if any could also be treated either by producing days from equivalent days recorded data or by estimating the seasonality curve producing the residual data from residual recorded data recorded data minus estimated seasonality and then adding the seasonality to the generated residuals the former approach is expected to be more accurate but would need much more data to be implemented than the latter approach a possible detection of a trend component could also be dealt with the latter approach specifically by abstracting trend by the raw data generating de trended residuals and then adding trend to locate the generated consumption data to time wanted with further investigation of the applicability of the algorithm to the de seasonalizing and de trending techniques the potential of using the proposed sdg method as a forecasting tool is enhanced especially when combined with advanced water consumer clustering techniques such as self organizing maps som laspidou et al 2015 a user might be able to generate tap water data by feeding the data generator with known or easy to find parameters even nominal such as the size or age of the household or the working status and ages of the residents of the household etc another possible use of the algorithm could be that of implementing a monte carlo type simulation of daily water consumption of all individual households in an agglomeration or district metered area dma this way we can identify extreme values lowest and highest possible for water consumption of all households this could be a useful planning tool that can help the water provider identify maximum possible consumptions and corresponding times of the day that they can occur thus quantifying the risk of failing to meet water demand such information can be useful in identifying weak links in a water supply network which could be related to reservoir or groundwater levels network capacity meeting required pressure levels in the network etc such a direction could give a useful tool to the water security domain one might wonder could this tool be used to model water consumption in a whole city and in that case how many households would we need to have data from in order to be able to reliably draw conclusions about the whole city the answer is that the parameters of population n and tolerated error e of simulated households in a city should be used to define the size of the sample that is representative of the population särndal et al 2003 i e the number of households with data needed obviously such a tool allows the user to use only a limited number of households in order to draw conclusions for a much larger sample further to that stratified sampling nassiuma 2001 can be used this technique suggests that a sample can be more representative when the population is divided into strata of common defined characteristics clustering algorithms can be considered pre cursors of stratified sampling laspidou et al 2015 applied the som algorithm to cluster water meter data of a small town into different categories such as households hotels and small enterprises using 3 month water consumption data the aforementioned imply that the scalability of the model can be enhanced by the use of clustering algorithms which would define the number of representative pilots needed respectively to the number of taxa suggested there is clearly potential in the use of such an algorithm especially when combined with clustering techniques given enough representative households with water consumption data of fine granularity the possibility of expanding its use to the whole city is to be considered 4 conclusions a method for generating meaningful water consumption data is introduced the sdg algorithm differs from past algorithms within the pulse model group because it introduces early on the probability of having a water consumption incident at each time step as a critical variable to the simulation that variable is modeled with the use of the binomial distribution this way it ensures that the consumption pattern is captured the flowrate values are modeled with the use of the estimated frequency distributions a reverse square distance based subroutine is introduced for the reproduction of the duration variable of each event by clustering the reproduced incidents the challenges of such a task are to mimic the characteristic patterns of each enduse to simulate the water flowrate levels and durations of water consumption events and to preserve the water volume consumed at a large time scale such as that of 400 days the methodology is implemented and tested for two case studies with different characteristics namely skiathos greece and sosnowiec poland pilots in skiathos a small greek island with rural life style exhibit higher consumptions that show no seasonality while pilots in sosnowiec an industrial urban center are characterized by lower consumptions and a weekday weekend seasonality pattern a number of qualitative and quantitative tests are implemented to check whether the objectives of the study are satisfied the suggested methodology can successfully reproduce data that mimic the source consumption patterns following the actual peaks and lags recorded and consumption characteristics such as the number and length of water use incidents per day the method successfully captures the cumulative yearly water balance for each pilot the algorithm becomes more robust when more recorded data is available and when more frequent water use incidents are recorded the method seems to adjust well when the weekday weekend pattern is evident the tool developed can provide meaningful water consumption data for reproducing gaps of data and missing values the suggested approach can constitute a potential basis for building up a tool which will support tasks demanding long water consumption data series such as implementing and testing dsss supporting the development of models of user behavior risk assessment of critical water consumption design parameters or developing water use fraud detection systems finally the algorithm can be used to reproduce water consumption data when privacy constraints hinder the use of actual time series acknowledgements this work was supported by the project iss ewatus integrated support system for efficient water usage and resources management which is implemented in the framework of the eu 7th framework programme specific programme cooperation information and communication technologies grant agreement number 619228 
26447,smart water meters can help businesses save water but achieving this goal requires trusted algorithms for processing the data and intuitive interactive software systems to support end users in decision making this paper presents an algorithm and a web based software system to detect and visualise anomalous water use the algorithm calculates an anomaly score for each day together with a rationale describing the symptoms of unusual water use the score for each day is based on ten features of daily demand and its historical context the score and its rationale are posted to users to help them track down the underlying physical causes of anomalies using data from two aquatic leisure centres we demonstrate that anomaly scores give better coverage than traditional threshold based systems that end users are able to utilise the timely feedback to save water and that the algorithm is reasonably robust to parameter settings keywords water demand management anomaly detection decision support smart meter software availability name of software smart water meter anomaly detection system developers samitha patabendige rui wang rachel cardell oliver contact samitha maddumapatabendige research uwa edu au year first available 2017 hardware required general purpose web server software required linux server os apache 2 server jre 8 php 7 r 3 3 3 languages java javascript php r availability contact authors case study http datascience csp uwa edu au anomalysite support languages english licensing gnu general public license 1 introduction reducing water consumption is a problem of growing importance because of the pressure on existing infrastructure caused by increasing urban populations weather extremes and the rising costs of maintenance cominola et al 2015 for example one city water utility found that leaks occupy 28 of the total consumptions in commercial office buildings 18 in shopping centres and 22 in aquatic leisure centres best practice guidelines 2007 best practice guidelines 2011 for businesses with high water use such as aquatic leisure centres a key goal is to identify and address unnecessary water use best practice guidelines 2011 this paper addresses the problem of detecting unusual water use events called anomalies using smart water meters timely user oriented alerts about anomalous water use help users to manage their demand more effectively common practices for monitoring water use are manual meter readings or using automated monitoring systems waterwise council program 2017 best practice guidelines 2007 best practice guidelines 2011 manually reading and recording meters is a laborious task and so systems that automate data collection and presentation are preferable currently available automated systems can be classified into commercial or research systems commercial systems are designed to be general purpose to suit different businesses the majority of commercial systems provide basic statistics on consumption but do not detect more complex patterns of anomalous water use additionally systems such as outpost 1 1 http www outpostcentral com and greensense view 2 2 http www greensense com au have an alarm notification feature for example outpost requires the user to choose thresholds and then an alert is sent automatically whenever consumption levels exceed the pre defined thresholds although these systems eliminate the need for physical access to read water meters they still require the involvement of experts who understand the business and are capable of setting thresholds and also re tuning the thresholds for different contexts such as the change of seasons monitoring systems built upon research outcomes are typically open source and cost free similar to commercial systems most offer basic statistics on consumption research systems are scientifically oriented so usually designed for specific purposes such as evaluating a particular algorithm nezhad et al jarrah nezhad et al 2014 present smartd a smart dashboard that specifically aims to help data analysts visualise the data from smart meters liu et al 2015 introduce smas a smart meter data analytics system that focuses on analysing daily consumption profiles and how external factors e g temperature affect consumptions liu et al 2016 also propose a software architecture lambda designed for real time anomaly detection on large data sets where the performance of the system is critical lambda requires special purpose hardware i e clusters of servers janetzko et al 2014 present an anomaly detection system with the focus on visualisation the system provides different visualisations of multi variate time series data for energy consumption but it does not report the rationale for its decisions anomaly detection is a general approach in which machine learning algorithms are used to identify any unusual patterns in water meter readings not just threshold alarms automatic anomaly detection is however a challenging task and existing methods suffer from a number of problems that hamper their application in the water domain in the supervised machine learning approach an algorithm is trained using a large amount of historical data that has been labelled by an expert however such labelling is extremely time consuming and the labelling has to be re done for each new user nguyen et al 2013 unsupervised machine learning algorithms for anomaly detection are available rayana and leman 2016 but their results may have low accuracy in practice because anomalies are rare by definition unexpected and also dependent on the context in which they occur such as summer or winter weekday or weekend another lack in existing anomaly detection methods is that they cannot explain transparently why demand is anomalous and hence their results may not be trustworthy for business use finally few anomaly detection methods are designed to integrate into existing management workflows and so they are not applied in practice a distinctive feature of water used by businesses is its dependence on calendar contexts such as the season of the year day of the week opening hours and maintenance schedules anomaly detection systems need to be calendar context aware of the relevant context for each day in order to avoid incorrect reports existing threshold alarm systems need to be manually re tuned they are not context aware we propose a method for identifying business relevant calendar contexts and for integrating these contexts into our anomaly detection algorithm most previous smart metering studies focus on residential water users rather than the business users considered in this paper but an important consideration for both types of users is how to integrate anomaly detection into the normal workflow of the user we describe such an integrated system as a workflow aware system in a recent review cominola et al 2015 conclude that although there have been many studies on user oriented residential demand management strategies there has been limited integration between their specialized methodologies for the four phases of data gathering characterising end users user modelling and personalised demand management systems they conclude that there is a clear need to shift research efforts from the development of specialized methodologies within each step of the procedure toward a more integrated approach that covers all the four phases cominola et al 2015 this paper addresses that gap the goal of the system presented in this paper is to develop and test a software system that provides automated calendar context aware workflow aware anomaly detection for water using businesses such as leisure centres the main contributions of this paper for achieving that goal are 1 an automated machine learning algorithm that generates an anomaly score for each day of water use with a rationale for that score the algorithm is calendar context aware which automatically take calendar contexts of businesses into consideration eliminating the involvement from human experts for timely re tuning thresholds 2 a web based software system that implements an active communication mechanism which automatically reports detected anomalies to users via emails and allows users to post feedback on the anomalies the lightweight communication channel quickly establishes trustworthiness in users enabling the system to be easily integrated into existing management work flows we have evaluated the system for two aquatic leisure centres with multiple swimming pools 2 anomaly detection algorithm this section describes our method for calculating an anomaly score for each day of water use anomalies are data objects that deviate significantly from the rest of the data objects as they were generated by a different mechanism hawkins 1980 in this study the data are smart meter readings which are typically presented as time sequence of consumption rates e g average flow rate during a fixed period for each day for example in a leisure centre the water demand for a day is recorded as a vector of 96 values measuring the average flow rate every 15 min intuitively a smart water meter reading may be considered as anomalous when it is exceptionally high or low in volume compared to other readings however identifying a single reading is not of much interest to stakeholders since it does not take into account its context where the context is a set of data points that are expected to have similar volumes so instead of analysing each single data point of a day in this study we aim to identify whether a day of water use is an anomalous based on the context of the time of day and relevant historical days of use knowing why a given day has a high anomaly score helps non residential water users to trace the underlying cause therefore in addition to calculating an anomaly score of a day our system gives a rationale for the score for example a day has an anomaly score of 0 75 because of an unusual flows between 6 00a m and 9 00a m rationales summarise the symptoms of anomalous water use which guides users in tracing physical causes firstly we identify the influential data points using calendar contexts for example water consumption is periodic which changes by the seasons hence using the past three month data of a given day as the context is more meaningful than using all historical data secondly we identify features such as the minimum and maximum flow of a day or the flows during a certain period of the day thirdly we employ an unsupervised algorithm to detect anomalies from each feature values of the context finally the algorithm aggregates the top anomaly scores to generate an overall anomaly score for each day the following subsections describe these steps in detail formally let n be the number of days in the dataset o be the number of observations per day m be the number of days in a context where m n and p be the number of features per day table 1 summarises the concepts and notations and algorithm 1 describes the algorithm in pseudo code 2 1 data model we model the input data from a smart water meter as a database d of observations d is an n o matrix with n rows each corresponding a day of use and o columns representing the readings on that day for example a smart water meter that records average water flow in every 15 min for one year will create matrix d with o 96 columns and n 365 rows the data of context days for d is a matrix c d calculated from d to identify whether a given day d is anomalous the inputs to our anomaly detection algorithm are d c d and d the algorithm outputs an anomaly score s d indicating the probably of being an anomalous day 2 2 features of daily water use features are measurable characteristics can be derived from the daily smart meter readings to help distinguish a normal day from an anomalous one features help to characterise different types of anomalies for example analysing minimum daily consumptions can detect leaks analysing the maximums can help to identify exceptional events and analysing water use from specific time intervals of a day can identify disruptions to normal routines table 2 lists the details of all features used in this paper the first three features day flow minimum flow and maximum flow summarise the use for a whole day these features supply an overview of the daily consumption which helps to identify anomalous water use from a day scope the other features focus on periodic water uses of a day consisting of two non business hour time intervals e g 6 00p m 12 00a m and four business hour time intervals e g 6 00a m 9 00a m these features are used to generate scores for anomalous water use at different times of the day and for stating the rationale for daily anomaly scores smart water meters can be configured with different reporting periods such as 15 min or 1 h some data mining algorithms such as disaggregation nguyen et al 2013 are strongly dependent on the resolution of the raw meter data however in our algorithm the raw smart water meter readings are converted to features that are statistical summaries min max or mean of the flows from longer periods from a few hours to a whole day for example the mean flow during 6a m 9a m or the maximum flow of the day using these aggregated features makes our algorithm independent of the resolution of the raw meter data given database d with the dimension of n o and a target day d the proposed algorithm applies feature functions f 1 f p where each function maps the vector of a day into a single feature value that is a given day is summarised as a vector with p values where each value maps a feature listed in table 2 we write a feature value v j calculatefeaturevalue f j d d calculating all feature values for every day in d gives a feature matrix f n p hence a row from f represents all the features from table 2 of a single day and a column represents a single feature in a series of days 2 3 anomaly scores as aforementioned anomalies are data objects that deviate significantly from the majority in other words anomalies are points that lie far from the closest neighbours which also known as distance based anomaly knorr and ng 1997 in this study we employ the k nn algorithm because it is a robust method for identifying distance based anomalies ramaswamy et al 2000 the k nn algorithm ranks the anomalousness of a data point using its distance to the k t h nearest neighbour relative to other distances in the database for example the higher the distance to the k t h nearest neighbour the higher the anomalousness of the point distance to the kth neighbour rather than nearest neighbour gives the algorithm robustness to minor fluctuations in the data hewahi and saad 2007 the choice of k is domain dependent since k is the minimum number of times similar observations should occur to be considered as normal for a given observation if there are at least k similar observations then the distance to the k t h nearest neighbour will be small and the observation will be considered normal on the other hand if there are fewer than k similar observations then the distance to the kth nearest neighbour will be larger and the observation is potentially anomalous the larger the distance the higher the anomaly score in consultation with domain experts we identified that k 10 m is a suitable value for k when m days are being considered since features present different characteristics of a day the anomaly distance of one feature from a particular day is not directly comparable with others furthermore the distance from one observation to its k th neighbour varies depending on the particular set of values it is being compared with our algorithm applies the anomaly unification algorithm of kriegel et al 2011 to address this problem by converting raw k nn distance scores into probability values once we have a probability of anomalousness instead of a distance then we can compare anomalies of different features and using different sets of observations kriegel et al 2011 showed that normalisation processes improves anomaly scores significantly compared to existing methods the method detailed in kriegel et al 2011 translates an outlier score such as a k nn distance into a value in the range 0 1 that can be interpreted as the probability of a data object of being an outlier there are three steps first the values output by an anomaly detection algorithm must be regularised that is inliers normal values have near 0 scores and outliers have non zero values the distances returned by k n n algorithm are already regular second the regular scores should be normalised to the range 0 1 the simplest way of bringing outlier scores onto a normalised scale is to apply a linear transformation that maps the minimum maximum scores onto 0 1 third given the mean and the standard deviation of the set of the normalised regular scores the gaussian error function can be used to transform the outlier score into a probability value 2 4 calendar contexts for water use so far we have assumed that anomaly scores for a day d are calculated with reference to the whole database d however this is not advisable because in reality a distinctive feature of water used by businesses is its dependence on its calendar contexts such as the season of the year day of the week opening hours and maintenance schedules so we should take calendar contexts into account to avoid providing misleading feedback on anomalies for example fig 1 shows two sets of anomaly scores one is calculated using only weekdays and the other using only weekends for values of this feature such as 100 l min indicated on the graph the anomaly score can vary from 0 9 indicating a serious anomaly to 0 01 indicating no anomaly depending on the calendar context ignoring the day of week context for this observation could result in false positives incorrectly reporting an anomaly or a false negative failure to report an anomaly we evaluate each day s anomaly score in terms of the calendar context of that day a calendar context for a day d is simply a subset of days from the database d that would be expected to have similar water use as d for example if d is a saturday then its context could be a subset of days from d consisting of weekend days the function getcontextdays d d takes a date d and a database d of dimension n o and returns an m o matrix c of related days c always includes the observations for day d each business has a set of relevant calendar contexts and every day in the database belongs to exactly one of these contexts given a particular day we can look up its context and select all days from the database within the context contexts are specified by constraints on features of calendar dates similar to the context spaces approach of padovitz padovitz et al 2004 contexts such as holiday closures or maintenance dates can be enumerated explicitly closures d d d 2016 12 25 d 2016 04 25 recurring patterns such as australian summer and weekend days are defined using logical constraints d ndate d d a y s a t s u n d m o n t h d e c j a n f e b where ndate denotes all normal dates that are not associated with special categories such as public holidays or maintenance days how are relevant calendar contexts for each business application identified firstly a list of candidate contexts are generated candidate contexts are generated using generic calendar categories such as days of the week seasons of the year together with business specific contexts such as maintenance schedules and closure days in some cases business specific dates can be extracted automatically from the building management system database or suggested by domain experts this context generation step results in a list of contexts c 1 c k each of which denotes a subset of the observation database calculating the features of each day and applying the anomaly probability process gives an anomaly probability matrix k associated with context c i small contexts are merged with their most similar neighbours to meet the requirements of the anomaly probability algorithm described in section 2 3 similarity values for merging are modelled by the kolmogrov smirnov ks statistic the largest difference between anomaly probabilities for the distributions d k 1 k 2 f j sup x 0 1 p r a n o m a l y s c o r e x k j 1 p r a n o m a l y s c o r e x k j 2 for example in fig 1 the ks statistic would be 0 89 0 9 0 01 indicating that these contexts are significantly different on the other hand monday to friday have similar anomaly score distributions with ks around 0 2 so individual contexts for monday to friday can be merged into a single context of weekdays this process applied to historical data is used to determine relevant and stable contexts for a given data set 2 5 combining individual scores this section explains how to combine these per feature anomaly probabilities into a meaningful outlier score for each day the ensemble score is defined as the average score of the top q anomaly probabilities over all features this approach strikes a balance between agreement and diversity of the features rayana and leman 2016 the pseudo code reference for ensemble score is ensemblescore p q in algorithm 1 where p is a p tuple of anomaly probability scores and q is a parameter for the number of contributory features ensemblescore p q σ j w j p j weights w j for each feature are assigned based on r j the rank of the score of each feature value equally scored values are ranked in the order of their feature identifiers where the value of 1 indicates the maximum valued feature in a tuple 2 is the next and so on overall weights for the ensemble are defined by 1 w i 1 q if r i 1 q 0 otherwise 2 6 algorithm image 1 the pseudo code is shown in algorithm 1 the first step of the algorithm is to select c the set of m context days that are relevant for a target day d from the smart meter time series d then we calculate an m p feature matrix by applying the feature functions f 1 f p to each of the m days i e rows in c from line 7 to 11 we take one feature and one day at a time and calculate its k nn anomaly distance and then the probability score for each feature in the target day d line 11 anomaly probabilities are stored in the p tuple p the anomaly score for day d is calculated by averaging the top q probability scores in p line 13 in addition a string giving the rationale for the anomaly score in terms of the anomaly probabilities for each feature is generated line 14 the algorithm returns the ensemble score and its rationale for day d line 15 3 interacting with experts anomaly detection aims to identify unusual data points by looking for outliers in businesses however not all outliers are relevant as anomalies and vice versa for example a scheduled action may consume a large amount of water but it is not anomalous hence understanding businesses models by eliciting interchanging and integrating domain specific knowledge is important for improving the performance of anomaly detection models several studies have attempted to incorporate feedback from human experts in order to improve the performance of systems das et al 2016 present an active anomaly discovery approach where the algorithm firstly makes independent predictions and then asks human experts to label the predicted anomalies and the system subsequently trains itself using the labelled data to improve the performance similarly horn and willett 2011 present a study that uses feedback requested from an expert system to improve the performance of their algorithm unlike previous studies that focus on investigating the actual improvement achieved by incorporating feedback from human experts in this paper we present a fully functioning system that implements a two way communication channel for the knowledge acquisition process the communication channel consists of a reporting mechanism that sends anomaly notifications to users and a web portal where users can leave feedback on the detected anomaly 3 1 daily email report anomaly reports are sent by emails on daily basis the daily report contains an anomaly score for the day a graph illustrating the hourly consumptions of the day and a text description of the rational of the detected anomaly sample messages are shown in table 4 in addition a hyper link button is embedded in the email where the url links to the anomaly page for the day on our web portal unlike many existing systems that implement passive communications i e users have to login to retrieve information our active communication strategy encourages human experts being proactive in interchanging their domain specific knowledge users can view the report and leave feedback via any devices connecting to the internet such as smart phones 3 2 user feedback users can post feedback on the system detected anomalies using the web portal the format of a feedback message has three levels daily anomaly classification fine grain feature labelling and textual comments each level responds to a specific question what happened how it happened and why it happened the daily anomaly classification mechanism not only minimises potential conflicts in the scores assigned by different users but also implements a high level knowledge acquisition process the labels collected can be used as the ground truth for further improving the anomaly detection algorithm a day is classified into three categories immediate investigation under observation and normal day the fine grain feature labelling allows users to select the features in table 2 that may indicate the anomaly for example a user may classify a day as immediate investigation because of unusual consumption in the early morning such fine grain feature labels record the actual time intervals and volumes that indicates anomalies which can be used for fine tuning the algorithm textual comments offer opportunities for users to report the physical cause for the anomaly once identified for example a user may want to note that the unusual consumption for the day is caused by a scheduled action that may indicate a new context for consideration 4 software 4 1 system architecture the proposed software system consists of three modules data acquisition and pre processing dap anomaly detection ad and user interface ui as shown in fig 2 the dap module fetches and validates the raw data from the outpost smart water meter data server and stores the clean data into the database the ad module is the core part of the system which is responsible for predicting anomalies by retrieving historical data from the database storing anomaly scores and sending email notifications to users advising whether an anomaly is detected the ui module is the interface between end users and the database where users can query and view historical water consumptions and criticise the detected anomalies by sending feedback to the database 4 2 data acquisition and pre processing daily water consumption is recorded by outpost smart water meters these devices monitor flow rates on a predefined m minute time interval and record p values with their timestamps per day by each device for example if the time interval is set to 15 min then 96 values are recorded per day these values are uploaded from the device to an outpost data server each day via cellular networks our system fetches raw data from the outpost server using the application programming interfaces api provided by outpost the dap module includes data validation normalisation and unix time stamping processes the data validation ensures the integrity of raw data i e p values per day the water meter occasionally records more than p values thus any extra data between m minute intervals are removed by the validation step daily data having fewer than p values often indicates the failure of a device in such case a notification will be sent by our system and no further processing is performed each device may record water usage in different volumes e g the flow rate in litres per minute or the total consumption for m minutes in kilolitres thus the normalisation step converts the data into an unified format finally each value is tagged with a unix timestamp and stored in our database 4 3 anomaly detection and notification the ad module computes anomaly scores the data processing sequences are 1 fetching the clean data from the database produced by the dap module 2 calculating the anomaly score and generating textual descriptions of the score 3 storing the outputs into the database and 3 sending the daily anomaly report to users by emails 4 4 data visualisation historical daily water consumption with its corresponding anomaly score is visualised in two heat maps as shown in fig 3 each heat map shows the last 15 months historical data or since the earliest available data in fig 3 each column presents weekdays each row presents days of the week and each cell presents a particular day these heat maps provide an overview of water consumptions that allows users to quickly identify common patterns as well as compare the usage of a particular day to its corresponding anomaly score fig 3 shows examples in addition to the historical data heat maps the system also visualises the hourly consumption of each day in a day in context chart as shown in fig 4 where the red line indicates the hourly consumption of the day and the blue dashed line shows the median of context days users can query a particular day s hourly consumption rate by clicking the cell in any of the heat maps to spot any suspicious activity that may shed light on the physical cause of the anomaly 4 5 implementation and deployment the system is implemented using different programming languages the dap module is implemented using java the ad module is written in r and the ui is implemented using javascript and php each module is designed for robustness and re usability and hence they work independently i e no direct data translation between modules all data is stored in sqlite databases the feedback from users is stored in one database and the water consumption and anomaly data is stored in a separate database that is read only from external access preventing any unauthorised write access the dap and ad modules are deployed as scheduled tasks which run on daily basis to fetch the new data and perform the anomaly detection and notification process the ui is a website hosted by the uwa data science server 5 case study 5 1 aquatic leisure centres the studies were conducted for two urban aquatic leisure centres cannington and riverton leisureplexes in western australia although the anomaly detection system was deployed in march 2017 for both centres our evaluation of anomaly detection accuracy is largely based on historical data table 3 summarises the water capacity and consumption information of the two centres 5 2 system configuration the aquatic centres have common operating conditions though the capacities are different therefore we used the same parameter values to configure both systems p 10 features of daily water use as in table 2 the system used a sliding window approach to select context days where the context for a given day is defined as the previous m 84 days we used k 10 m 8 for k nn the number of contributory features for the ensemble score is q 3 and avoids bias 5 3 anomaly detection accuracy we evaluate the effectiveness of our anomaly detection system by comparing its performance with an existing industry standard for anomaly detection minimum night flow mnf alarms note that it is not feasible to evaluate the accuracy of our system using standard techniques such as least squares residuals because there is no ground truth for what the true score of a day should be instead we categorise the anomalies found using existing practice and with our algorithm the established practice at both aquatic centres was to use a minimum night flow mnf rule to set alarm thresholds in the outpost system mnf is the average flow recorded between 0 a m midnight to 4 a m this time is chosen because flow rates are expected to be at a minimum during these hours based on the operational schedule of the centres both centres configured the system to send an email alert when the mnf exceeds 10l min the reason for monitoring mnf is that a high mnf is a symptom of a continuous flow or leak so we check how often mnf alarms are the result of cfs or not historical data comprising 360 days from the riverton leisureplex was analysed four properties of each day were calculated whether a mnf alarm is raised true of false where there is a continuous non zero flow cf on that day true or false the score assigned by ads for that day the rationales given by ads for that day we do not simplify ads into binary categories since we have the more meaningful score values a true positive tp alarm occurs when both the mnf and cf conditions are satisfied if mnf is true but cf false that is a false alarm called a false positive fp on the other hand cf is true but mnf false is a missed alarm called a false negative fn however the fn situation can not occur for our settings since both mnf and cf use the same flow limit and so if the minimum flow for the whole day cf is 10 l min then the minimum night flow will also be 10 l min most of the days in the sample are true negatives tn when there are no anomalies from either the mnf or cf for each day other types of anomalous activity may also occur such as unusual flow during the day time when the ads scores alert the users to these cases or when the ads helps to identify fp or fn we say that ads adds value fig 5 shows three days with different types of anomalies the box plot in fig 5 shows the range of ads scores for all the days in each category the selected days represent three categories of reports as follows a mnf true cf true ads 99 this mnf alarm is a true positive for this day ads added value with a score of 99 and rationale that identifies daily and afternoon anomalies as well as the continuous flow that triggered the mnf alarm b mnf true cf false ads 92 this mnf alarm is a false positive because there is not a continuous flow however for this day ads adds value with a score of 92 and rationale that identifies anomalies in the afternoon and evening c mnf false cf false ads 82 this mnf alarm is a true negative because there is no alarm and no continuous flow however for this day ads adds value with a score of 82 and rationale that identifies anomalies in the morning and in the overall daily consumption as can be seen in the box plots of fig 6 there are a few mnf true positives with low ads scores these arise when the minimum night flow is the single anomalous feature of the day the choice of q 3 leads to an ensemble score average about 0 3 for this situation section 5 5 discusses the trade offs to be made in selecting a suitable q value 5 4 gaining user trust our system sends an email message each morning to inform users of the anomaly score and rationale for the previous day s water use this feedback could also be delivered as text messages to phones the daily email contains a summary message that provides the overall anomaly score and the rationale for that the score it also contains a figure which can be used for more detailed insight into the day s water consumption users can post a response giving the physical causes for an anomaly once these have been tracked down during our discussion with the users we found that this system gained their trust in three ways first it develops user engagement by supplying a two way communication channel in which the system reports anomaly scores and their rationale and the users report underlying physical causes second the system is easy to use because it follows the existing operational procedures of the management staff third the reported rationale for each anomaly score helped users to track down the physical causes of anomalies table 4 shows how users have been able to associate reported anomalies with their causes and posted responses to our system after the system had been in use for one month the users had sufficient trust in the system to request emails only when there is a high anomaly score 5 5 sensitivity our anomaly detection algorithm depends on several parameters in this section we investigate how sensitive the results are to the choice of parameter values we say the algorithm is robust if changes in parameter values do not significantly change the anomaly scores 5 5 1 sensitivity of the algorithm to the parameter q fig 6a shows the cumulative distribution function for anomaly probability scores by varying the number of features q used for the ensemble in the riverton data we kept the number of neighbours k at 8 for this experiment for high q values few days receive high anomaly scores for example when q is 10 fewer than 5 of the days have an anomaly score above 0 75 days with only one or two anomalous features will not be examined because they have low scores so there will be false negatives thus selecting a high q is not suitable for low q values too many days have high anomaly scores for example when q is 1 more than 30 of the days have an anomaly score above 0 75 this is undesirable because we believe that the true rate of anomalies is lower so there will be false positives days with only one anomalous feature have a high score and will need to be examined when selecting a value for q we wanted to strike a balance between these extremes so we selected q 3 which represents a central value range as shown in fig 6a 5 5 2 sensitivity of the algorithm to the parameter k fig 6b shows the cumulative distribution function for the anomaly probability scores by varying the number of neighbours k used for the k nn algorithm for each feature in riverton data we kept the number of features q at 3 for this experiment the algorithm is not overly sensitive for low k values of the number of days in a context in particular the low k graphs are very close in fig 6b showing that the algorithm is robust in this range we selected 10 for k as a central value from the stable range 6 conclusions and future work in this paper we have presented a software system for detecting anomalous water use from smart water data the system automatically generates an anomaly score for each day together with a rationale for that score to achieve this we proposed a calendar context aware anomaly detection algorithm which understands the relevant context for each day the system features a two way communication channel the system reports anomaly scores and rationale to users via emails and it has a web based ui for users to report the physical cause for an anomaly our lightweight and active communication mechanism is easily integrated into existing business management workflows we claim that the process and lessons learnt from this integrated algorithm driven software development are readily applicable to other consumption data monitoring and processing tasks including but not limited to utilities such as electricity and gas and other types of businesses or residential water users although the current system features a feedback mechanism that allows users to post comments on the detected anomalies it lacks a process that automatically integrates such feedback into future rationale messages in future work we plan to extend the anomaly detection algorithm to a semi supervised approach that will continually learn from the feedback acknowledgements the authors would like to thank nick wilkinson kelly sanders ryan scally shana mckay and michael gosatti of the city of canning western australia and kel medbury and nathan harper of the water corporation of western australia for access to their smart meter data and for their valuable feedback in the development of this system this research is funded by the cooperative research centre for water sensitive cities crcwsc under intelligent urban water systems project c5 1 author s p is also supported by an australian government research training program scholarship at the university of western australia the authors thank colleagues across the crcwsc programs for their feedback on this project 
26447,smart water meters can help businesses save water but achieving this goal requires trusted algorithms for processing the data and intuitive interactive software systems to support end users in decision making this paper presents an algorithm and a web based software system to detect and visualise anomalous water use the algorithm calculates an anomaly score for each day together with a rationale describing the symptoms of unusual water use the score for each day is based on ten features of daily demand and its historical context the score and its rationale are posted to users to help them track down the underlying physical causes of anomalies using data from two aquatic leisure centres we demonstrate that anomaly scores give better coverage than traditional threshold based systems that end users are able to utilise the timely feedback to save water and that the algorithm is reasonably robust to parameter settings keywords water demand management anomaly detection decision support smart meter software availability name of software smart water meter anomaly detection system developers samitha patabendige rui wang rachel cardell oliver contact samitha maddumapatabendige research uwa edu au year first available 2017 hardware required general purpose web server software required linux server os apache 2 server jre 8 php 7 r 3 3 3 languages java javascript php r availability contact authors case study http datascience csp uwa edu au anomalysite support languages english licensing gnu general public license 1 introduction reducing water consumption is a problem of growing importance because of the pressure on existing infrastructure caused by increasing urban populations weather extremes and the rising costs of maintenance cominola et al 2015 for example one city water utility found that leaks occupy 28 of the total consumptions in commercial office buildings 18 in shopping centres and 22 in aquatic leisure centres best practice guidelines 2007 best practice guidelines 2011 for businesses with high water use such as aquatic leisure centres a key goal is to identify and address unnecessary water use best practice guidelines 2011 this paper addresses the problem of detecting unusual water use events called anomalies using smart water meters timely user oriented alerts about anomalous water use help users to manage their demand more effectively common practices for monitoring water use are manual meter readings or using automated monitoring systems waterwise council program 2017 best practice guidelines 2007 best practice guidelines 2011 manually reading and recording meters is a laborious task and so systems that automate data collection and presentation are preferable currently available automated systems can be classified into commercial or research systems commercial systems are designed to be general purpose to suit different businesses the majority of commercial systems provide basic statistics on consumption but do not detect more complex patterns of anomalous water use additionally systems such as outpost 1 1 http www outpostcentral com and greensense view 2 2 http www greensense com au have an alarm notification feature for example outpost requires the user to choose thresholds and then an alert is sent automatically whenever consumption levels exceed the pre defined thresholds although these systems eliminate the need for physical access to read water meters they still require the involvement of experts who understand the business and are capable of setting thresholds and also re tuning the thresholds for different contexts such as the change of seasons monitoring systems built upon research outcomes are typically open source and cost free similar to commercial systems most offer basic statistics on consumption research systems are scientifically oriented so usually designed for specific purposes such as evaluating a particular algorithm nezhad et al jarrah nezhad et al 2014 present smartd a smart dashboard that specifically aims to help data analysts visualise the data from smart meters liu et al 2015 introduce smas a smart meter data analytics system that focuses on analysing daily consumption profiles and how external factors e g temperature affect consumptions liu et al 2016 also propose a software architecture lambda designed for real time anomaly detection on large data sets where the performance of the system is critical lambda requires special purpose hardware i e clusters of servers janetzko et al 2014 present an anomaly detection system with the focus on visualisation the system provides different visualisations of multi variate time series data for energy consumption but it does not report the rationale for its decisions anomaly detection is a general approach in which machine learning algorithms are used to identify any unusual patterns in water meter readings not just threshold alarms automatic anomaly detection is however a challenging task and existing methods suffer from a number of problems that hamper their application in the water domain in the supervised machine learning approach an algorithm is trained using a large amount of historical data that has been labelled by an expert however such labelling is extremely time consuming and the labelling has to be re done for each new user nguyen et al 2013 unsupervised machine learning algorithms for anomaly detection are available rayana and leman 2016 but their results may have low accuracy in practice because anomalies are rare by definition unexpected and also dependent on the context in which they occur such as summer or winter weekday or weekend another lack in existing anomaly detection methods is that they cannot explain transparently why demand is anomalous and hence their results may not be trustworthy for business use finally few anomaly detection methods are designed to integrate into existing management workflows and so they are not applied in practice a distinctive feature of water used by businesses is its dependence on calendar contexts such as the season of the year day of the week opening hours and maintenance schedules anomaly detection systems need to be calendar context aware of the relevant context for each day in order to avoid incorrect reports existing threshold alarm systems need to be manually re tuned they are not context aware we propose a method for identifying business relevant calendar contexts and for integrating these contexts into our anomaly detection algorithm most previous smart metering studies focus on residential water users rather than the business users considered in this paper but an important consideration for both types of users is how to integrate anomaly detection into the normal workflow of the user we describe such an integrated system as a workflow aware system in a recent review cominola et al 2015 conclude that although there have been many studies on user oriented residential demand management strategies there has been limited integration between their specialized methodologies for the four phases of data gathering characterising end users user modelling and personalised demand management systems they conclude that there is a clear need to shift research efforts from the development of specialized methodologies within each step of the procedure toward a more integrated approach that covers all the four phases cominola et al 2015 this paper addresses that gap the goal of the system presented in this paper is to develop and test a software system that provides automated calendar context aware workflow aware anomaly detection for water using businesses such as leisure centres the main contributions of this paper for achieving that goal are 1 an automated machine learning algorithm that generates an anomaly score for each day of water use with a rationale for that score the algorithm is calendar context aware which automatically take calendar contexts of businesses into consideration eliminating the involvement from human experts for timely re tuning thresholds 2 a web based software system that implements an active communication mechanism which automatically reports detected anomalies to users via emails and allows users to post feedback on the anomalies the lightweight communication channel quickly establishes trustworthiness in users enabling the system to be easily integrated into existing management work flows we have evaluated the system for two aquatic leisure centres with multiple swimming pools 2 anomaly detection algorithm this section describes our method for calculating an anomaly score for each day of water use anomalies are data objects that deviate significantly from the rest of the data objects as they were generated by a different mechanism hawkins 1980 in this study the data are smart meter readings which are typically presented as time sequence of consumption rates e g average flow rate during a fixed period for each day for example in a leisure centre the water demand for a day is recorded as a vector of 96 values measuring the average flow rate every 15 min intuitively a smart water meter reading may be considered as anomalous when it is exceptionally high or low in volume compared to other readings however identifying a single reading is not of much interest to stakeholders since it does not take into account its context where the context is a set of data points that are expected to have similar volumes so instead of analysing each single data point of a day in this study we aim to identify whether a day of water use is an anomalous based on the context of the time of day and relevant historical days of use knowing why a given day has a high anomaly score helps non residential water users to trace the underlying cause therefore in addition to calculating an anomaly score of a day our system gives a rationale for the score for example a day has an anomaly score of 0 75 because of an unusual flows between 6 00a m and 9 00a m rationales summarise the symptoms of anomalous water use which guides users in tracing physical causes firstly we identify the influential data points using calendar contexts for example water consumption is periodic which changes by the seasons hence using the past three month data of a given day as the context is more meaningful than using all historical data secondly we identify features such as the minimum and maximum flow of a day or the flows during a certain period of the day thirdly we employ an unsupervised algorithm to detect anomalies from each feature values of the context finally the algorithm aggregates the top anomaly scores to generate an overall anomaly score for each day the following subsections describe these steps in detail formally let n be the number of days in the dataset o be the number of observations per day m be the number of days in a context where m n and p be the number of features per day table 1 summarises the concepts and notations and algorithm 1 describes the algorithm in pseudo code 2 1 data model we model the input data from a smart water meter as a database d of observations d is an n o matrix with n rows each corresponding a day of use and o columns representing the readings on that day for example a smart water meter that records average water flow in every 15 min for one year will create matrix d with o 96 columns and n 365 rows the data of context days for d is a matrix c d calculated from d to identify whether a given day d is anomalous the inputs to our anomaly detection algorithm are d c d and d the algorithm outputs an anomaly score s d indicating the probably of being an anomalous day 2 2 features of daily water use features are measurable characteristics can be derived from the daily smart meter readings to help distinguish a normal day from an anomalous one features help to characterise different types of anomalies for example analysing minimum daily consumptions can detect leaks analysing the maximums can help to identify exceptional events and analysing water use from specific time intervals of a day can identify disruptions to normal routines table 2 lists the details of all features used in this paper the first three features day flow minimum flow and maximum flow summarise the use for a whole day these features supply an overview of the daily consumption which helps to identify anomalous water use from a day scope the other features focus on periodic water uses of a day consisting of two non business hour time intervals e g 6 00p m 12 00a m and four business hour time intervals e g 6 00a m 9 00a m these features are used to generate scores for anomalous water use at different times of the day and for stating the rationale for daily anomaly scores smart water meters can be configured with different reporting periods such as 15 min or 1 h some data mining algorithms such as disaggregation nguyen et al 2013 are strongly dependent on the resolution of the raw meter data however in our algorithm the raw smart water meter readings are converted to features that are statistical summaries min max or mean of the flows from longer periods from a few hours to a whole day for example the mean flow during 6a m 9a m or the maximum flow of the day using these aggregated features makes our algorithm independent of the resolution of the raw meter data given database d with the dimension of n o and a target day d the proposed algorithm applies feature functions f 1 f p where each function maps the vector of a day into a single feature value that is a given day is summarised as a vector with p values where each value maps a feature listed in table 2 we write a feature value v j calculatefeaturevalue f j d d calculating all feature values for every day in d gives a feature matrix f n p hence a row from f represents all the features from table 2 of a single day and a column represents a single feature in a series of days 2 3 anomaly scores as aforementioned anomalies are data objects that deviate significantly from the majority in other words anomalies are points that lie far from the closest neighbours which also known as distance based anomaly knorr and ng 1997 in this study we employ the k nn algorithm because it is a robust method for identifying distance based anomalies ramaswamy et al 2000 the k nn algorithm ranks the anomalousness of a data point using its distance to the k t h nearest neighbour relative to other distances in the database for example the higher the distance to the k t h nearest neighbour the higher the anomalousness of the point distance to the kth neighbour rather than nearest neighbour gives the algorithm robustness to minor fluctuations in the data hewahi and saad 2007 the choice of k is domain dependent since k is the minimum number of times similar observations should occur to be considered as normal for a given observation if there are at least k similar observations then the distance to the k t h nearest neighbour will be small and the observation will be considered normal on the other hand if there are fewer than k similar observations then the distance to the kth nearest neighbour will be larger and the observation is potentially anomalous the larger the distance the higher the anomaly score in consultation with domain experts we identified that k 10 m is a suitable value for k when m days are being considered since features present different characteristics of a day the anomaly distance of one feature from a particular day is not directly comparable with others furthermore the distance from one observation to its k th neighbour varies depending on the particular set of values it is being compared with our algorithm applies the anomaly unification algorithm of kriegel et al 2011 to address this problem by converting raw k nn distance scores into probability values once we have a probability of anomalousness instead of a distance then we can compare anomalies of different features and using different sets of observations kriegel et al 2011 showed that normalisation processes improves anomaly scores significantly compared to existing methods the method detailed in kriegel et al 2011 translates an outlier score such as a k nn distance into a value in the range 0 1 that can be interpreted as the probability of a data object of being an outlier there are three steps first the values output by an anomaly detection algorithm must be regularised that is inliers normal values have near 0 scores and outliers have non zero values the distances returned by k n n algorithm are already regular second the regular scores should be normalised to the range 0 1 the simplest way of bringing outlier scores onto a normalised scale is to apply a linear transformation that maps the minimum maximum scores onto 0 1 third given the mean and the standard deviation of the set of the normalised regular scores the gaussian error function can be used to transform the outlier score into a probability value 2 4 calendar contexts for water use so far we have assumed that anomaly scores for a day d are calculated with reference to the whole database d however this is not advisable because in reality a distinctive feature of water used by businesses is its dependence on its calendar contexts such as the season of the year day of the week opening hours and maintenance schedules so we should take calendar contexts into account to avoid providing misleading feedback on anomalies for example fig 1 shows two sets of anomaly scores one is calculated using only weekdays and the other using only weekends for values of this feature such as 100 l min indicated on the graph the anomaly score can vary from 0 9 indicating a serious anomaly to 0 01 indicating no anomaly depending on the calendar context ignoring the day of week context for this observation could result in false positives incorrectly reporting an anomaly or a false negative failure to report an anomaly we evaluate each day s anomaly score in terms of the calendar context of that day a calendar context for a day d is simply a subset of days from the database d that would be expected to have similar water use as d for example if d is a saturday then its context could be a subset of days from d consisting of weekend days the function getcontextdays d d takes a date d and a database d of dimension n o and returns an m o matrix c of related days c always includes the observations for day d each business has a set of relevant calendar contexts and every day in the database belongs to exactly one of these contexts given a particular day we can look up its context and select all days from the database within the context contexts are specified by constraints on features of calendar dates similar to the context spaces approach of padovitz padovitz et al 2004 contexts such as holiday closures or maintenance dates can be enumerated explicitly closures d d d 2016 12 25 d 2016 04 25 recurring patterns such as australian summer and weekend days are defined using logical constraints d ndate d d a y s a t s u n d m o n t h d e c j a n f e b where ndate denotes all normal dates that are not associated with special categories such as public holidays or maintenance days how are relevant calendar contexts for each business application identified firstly a list of candidate contexts are generated candidate contexts are generated using generic calendar categories such as days of the week seasons of the year together with business specific contexts such as maintenance schedules and closure days in some cases business specific dates can be extracted automatically from the building management system database or suggested by domain experts this context generation step results in a list of contexts c 1 c k each of which denotes a subset of the observation database calculating the features of each day and applying the anomaly probability process gives an anomaly probability matrix k associated with context c i small contexts are merged with their most similar neighbours to meet the requirements of the anomaly probability algorithm described in section 2 3 similarity values for merging are modelled by the kolmogrov smirnov ks statistic the largest difference between anomaly probabilities for the distributions d k 1 k 2 f j sup x 0 1 p r a n o m a l y s c o r e x k j 1 p r a n o m a l y s c o r e x k j 2 for example in fig 1 the ks statistic would be 0 89 0 9 0 01 indicating that these contexts are significantly different on the other hand monday to friday have similar anomaly score distributions with ks around 0 2 so individual contexts for monday to friday can be merged into a single context of weekdays this process applied to historical data is used to determine relevant and stable contexts for a given data set 2 5 combining individual scores this section explains how to combine these per feature anomaly probabilities into a meaningful outlier score for each day the ensemble score is defined as the average score of the top q anomaly probabilities over all features this approach strikes a balance between agreement and diversity of the features rayana and leman 2016 the pseudo code reference for ensemble score is ensemblescore p q in algorithm 1 where p is a p tuple of anomaly probability scores and q is a parameter for the number of contributory features ensemblescore p q σ j w j p j weights w j for each feature are assigned based on r j the rank of the score of each feature value equally scored values are ranked in the order of their feature identifiers where the value of 1 indicates the maximum valued feature in a tuple 2 is the next and so on overall weights for the ensemble are defined by 1 w i 1 q if r i 1 q 0 otherwise 2 6 algorithm image 1 the pseudo code is shown in algorithm 1 the first step of the algorithm is to select c the set of m context days that are relevant for a target day d from the smart meter time series d then we calculate an m p feature matrix by applying the feature functions f 1 f p to each of the m days i e rows in c from line 7 to 11 we take one feature and one day at a time and calculate its k nn anomaly distance and then the probability score for each feature in the target day d line 11 anomaly probabilities are stored in the p tuple p the anomaly score for day d is calculated by averaging the top q probability scores in p line 13 in addition a string giving the rationale for the anomaly score in terms of the anomaly probabilities for each feature is generated line 14 the algorithm returns the ensemble score and its rationale for day d line 15 3 interacting with experts anomaly detection aims to identify unusual data points by looking for outliers in businesses however not all outliers are relevant as anomalies and vice versa for example a scheduled action may consume a large amount of water but it is not anomalous hence understanding businesses models by eliciting interchanging and integrating domain specific knowledge is important for improving the performance of anomaly detection models several studies have attempted to incorporate feedback from human experts in order to improve the performance of systems das et al 2016 present an active anomaly discovery approach where the algorithm firstly makes independent predictions and then asks human experts to label the predicted anomalies and the system subsequently trains itself using the labelled data to improve the performance similarly horn and willett 2011 present a study that uses feedback requested from an expert system to improve the performance of their algorithm unlike previous studies that focus on investigating the actual improvement achieved by incorporating feedback from human experts in this paper we present a fully functioning system that implements a two way communication channel for the knowledge acquisition process the communication channel consists of a reporting mechanism that sends anomaly notifications to users and a web portal where users can leave feedback on the detected anomaly 3 1 daily email report anomaly reports are sent by emails on daily basis the daily report contains an anomaly score for the day a graph illustrating the hourly consumptions of the day and a text description of the rational of the detected anomaly sample messages are shown in table 4 in addition a hyper link button is embedded in the email where the url links to the anomaly page for the day on our web portal unlike many existing systems that implement passive communications i e users have to login to retrieve information our active communication strategy encourages human experts being proactive in interchanging their domain specific knowledge users can view the report and leave feedback via any devices connecting to the internet such as smart phones 3 2 user feedback users can post feedback on the system detected anomalies using the web portal the format of a feedback message has three levels daily anomaly classification fine grain feature labelling and textual comments each level responds to a specific question what happened how it happened and why it happened the daily anomaly classification mechanism not only minimises potential conflicts in the scores assigned by different users but also implements a high level knowledge acquisition process the labels collected can be used as the ground truth for further improving the anomaly detection algorithm a day is classified into three categories immediate investigation under observation and normal day the fine grain feature labelling allows users to select the features in table 2 that may indicate the anomaly for example a user may classify a day as immediate investigation because of unusual consumption in the early morning such fine grain feature labels record the actual time intervals and volumes that indicates anomalies which can be used for fine tuning the algorithm textual comments offer opportunities for users to report the physical cause for the anomaly once identified for example a user may want to note that the unusual consumption for the day is caused by a scheduled action that may indicate a new context for consideration 4 software 4 1 system architecture the proposed software system consists of three modules data acquisition and pre processing dap anomaly detection ad and user interface ui as shown in fig 2 the dap module fetches and validates the raw data from the outpost smart water meter data server and stores the clean data into the database the ad module is the core part of the system which is responsible for predicting anomalies by retrieving historical data from the database storing anomaly scores and sending email notifications to users advising whether an anomaly is detected the ui module is the interface between end users and the database where users can query and view historical water consumptions and criticise the detected anomalies by sending feedback to the database 4 2 data acquisition and pre processing daily water consumption is recorded by outpost smart water meters these devices monitor flow rates on a predefined m minute time interval and record p values with their timestamps per day by each device for example if the time interval is set to 15 min then 96 values are recorded per day these values are uploaded from the device to an outpost data server each day via cellular networks our system fetches raw data from the outpost server using the application programming interfaces api provided by outpost the dap module includes data validation normalisation and unix time stamping processes the data validation ensures the integrity of raw data i e p values per day the water meter occasionally records more than p values thus any extra data between m minute intervals are removed by the validation step daily data having fewer than p values often indicates the failure of a device in such case a notification will be sent by our system and no further processing is performed each device may record water usage in different volumes e g the flow rate in litres per minute or the total consumption for m minutes in kilolitres thus the normalisation step converts the data into an unified format finally each value is tagged with a unix timestamp and stored in our database 4 3 anomaly detection and notification the ad module computes anomaly scores the data processing sequences are 1 fetching the clean data from the database produced by the dap module 2 calculating the anomaly score and generating textual descriptions of the score 3 storing the outputs into the database and 3 sending the daily anomaly report to users by emails 4 4 data visualisation historical daily water consumption with its corresponding anomaly score is visualised in two heat maps as shown in fig 3 each heat map shows the last 15 months historical data or since the earliest available data in fig 3 each column presents weekdays each row presents days of the week and each cell presents a particular day these heat maps provide an overview of water consumptions that allows users to quickly identify common patterns as well as compare the usage of a particular day to its corresponding anomaly score fig 3 shows examples in addition to the historical data heat maps the system also visualises the hourly consumption of each day in a day in context chart as shown in fig 4 where the red line indicates the hourly consumption of the day and the blue dashed line shows the median of context days users can query a particular day s hourly consumption rate by clicking the cell in any of the heat maps to spot any suspicious activity that may shed light on the physical cause of the anomaly 4 5 implementation and deployment the system is implemented using different programming languages the dap module is implemented using java the ad module is written in r and the ui is implemented using javascript and php each module is designed for robustness and re usability and hence they work independently i e no direct data translation between modules all data is stored in sqlite databases the feedback from users is stored in one database and the water consumption and anomaly data is stored in a separate database that is read only from external access preventing any unauthorised write access the dap and ad modules are deployed as scheduled tasks which run on daily basis to fetch the new data and perform the anomaly detection and notification process the ui is a website hosted by the uwa data science server 5 case study 5 1 aquatic leisure centres the studies were conducted for two urban aquatic leisure centres cannington and riverton leisureplexes in western australia although the anomaly detection system was deployed in march 2017 for both centres our evaluation of anomaly detection accuracy is largely based on historical data table 3 summarises the water capacity and consumption information of the two centres 5 2 system configuration the aquatic centres have common operating conditions though the capacities are different therefore we used the same parameter values to configure both systems p 10 features of daily water use as in table 2 the system used a sliding window approach to select context days where the context for a given day is defined as the previous m 84 days we used k 10 m 8 for k nn the number of contributory features for the ensemble score is q 3 and avoids bias 5 3 anomaly detection accuracy we evaluate the effectiveness of our anomaly detection system by comparing its performance with an existing industry standard for anomaly detection minimum night flow mnf alarms note that it is not feasible to evaluate the accuracy of our system using standard techniques such as least squares residuals because there is no ground truth for what the true score of a day should be instead we categorise the anomalies found using existing practice and with our algorithm the established practice at both aquatic centres was to use a minimum night flow mnf rule to set alarm thresholds in the outpost system mnf is the average flow recorded between 0 a m midnight to 4 a m this time is chosen because flow rates are expected to be at a minimum during these hours based on the operational schedule of the centres both centres configured the system to send an email alert when the mnf exceeds 10l min the reason for monitoring mnf is that a high mnf is a symptom of a continuous flow or leak so we check how often mnf alarms are the result of cfs or not historical data comprising 360 days from the riverton leisureplex was analysed four properties of each day were calculated whether a mnf alarm is raised true of false where there is a continuous non zero flow cf on that day true or false the score assigned by ads for that day the rationales given by ads for that day we do not simplify ads into binary categories since we have the more meaningful score values a true positive tp alarm occurs when both the mnf and cf conditions are satisfied if mnf is true but cf false that is a false alarm called a false positive fp on the other hand cf is true but mnf false is a missed alarm called a false negative fn however the fn situation can not occur for our settings since both mnf and cf use the same flow limit and so if the minimum flow for the whole day cf is 10 l min then the minimum night flow will also be 10 l min most of the days in the sample are true negatives tn when there are no anomalies from either the mnf or cf for each day other types of anomalous activity may also occur such as unusual flow during the day time when the ads scores alert the users to these cases or when the ads helps to identify fp or fn we say that ads adds value fig 5 shows three days with different types of anomalies the box plot in fig 5 shows the range of ads scores for all the days in each category the selected days represent three categories of reports as follows a mnf true cf true ads 99 this mnf alarm is a true positive for this day ads added value with a score of 99 and rationale that identifies daily and afternoon anomalies as well as the continuous flow that triggered the mnf alarm b mnf true cf false ads 92 this mnf alarm is a false positive because there is not a continuous flow however for this day ads adds value with a score of 92 and rationale that identifies anomalies in the afternoon and evening c mnf false cf false ads 82 this mnf alarm is a true negative because there is no alarm and no continuous flow however for this day ads adds value with a score of 82 and rationale that identifies anomalies in the morning and in the overall daily consumption as can be seen in the box plots of fig 6 there are a few mnf true positives with low ads scores these arise when the minimum night flow is the single anomalous feature of the day the choice of q 3 leads to an ensemble score average about 0 3 for this situation section 5 5 discusses the trade offs to be made in selecting a suitable q value 5 4 gaining user trust our system sends an email message each morning to inform users of the anomaly score and rationale for the previous day s water use this feedback could also be delivered as text messages to phones the daily email contains a summary message that provides the overall anomaly score and the rationale for that the score it also contains a figure which can be used for more detailed insight into the day s water consumption users can post a response giving the physical causes for an anomaly once these have been tracked down during our discussion with the users we found that this system gained their trust in three ways first it develops user engagement by supplying a two way communication channel in which the system reports anomaly scores and their rationale and the users report underlying physical causes second the system is easy to use because it follows the existing operational procedures of the management staff third the reported rationale for each anomaly score helped users to track down the physical causes of anomalies table 4 shows how users have been able to associate reported anomalies with their causes and posted responses to our system after the system had been in use for one month the users had sufficient trust in the system to request emails only when there is a high anomaly score 5 5 sensitivity our anomaly detection algorithm depends on several parameters in this section we investigate how sensitive the results are to the choice of parameter values we say the algorithm is robust if changes in parameter values do not significantly change the anomaly scores 5 5 1 sensitivity of the algorithm to the parameter q fig 6a shows the cumulative distribution function for anomaly probability scores by varying the number of features q used for the ensemble in the riverton data we kept the number of neighbours k at 8 for this experiment for high q values few days receive high anomaly scores for example when q is 10 fewer than 5 of the days have an anomaly score above 0 75 days with only one or two anomalous features will not be examined because they have low scores so there will be false negatives thus selecting a high q is not suitable for low q values too many days have high anomaly scores for example when q is 1 more than 30 of the days have an anomaly score above 0 75 this is undesirable because we believe that the true rate of anomalies is lower so there will be false positives days with only one anomalous feature have a high score and will need to be examined when selecting a value for q we wanted to strike a balance between these extremes so we selected q 3 which represents a central value range as shown in fig 6a 5 5 2 sensitivity of the algorithm to the parameter k fig 6b shows the cumulative distribution function for the anomaly probability scores by varying the number of neighbours k used for the k nn algorithm for each feature in riverton data we kept the number of features q at 3 for this experiment the algorithm is not overly sensitive for low k values of the number of days in a context in particular the low k graphs are very close in fig 6b showing that the algorithm is robust in this range we selected 10 for k as a central value from the stable range 6 conclusions and future work in this paper we have presented a software system for detecting anomalous water use from smart water data the system automatically generates an anomaly score for each day together with a rationale for that score to achieve this we proposed a calendar context aware anomaly detection algorithm which understands the relevant context for each day the system features a two way communication channel the system reports anomaly scores and rationale to users via emails and it has a web based ui for users to report the physical cause for an anomaly our lightweight and active communication mechanism is easily integrated into existing business management workflows we claim that the process and lessons learnt from this integrated algorithm driven software development are readily applicable to other consumption data monitoring and processing tasks including but not limited to utilities such as electricity and gas and other types of businesses or residential water users although the current system features a feedback mechanism that allows users to post comments on the detected anomalies it lacks a process that automatically integrates such feedback into future rationale messages in future work we plan to extend the anomaly detection algorithm to a semi supervised approach that will continually learn from the feedback acknowledgements the authors would like to thank nick wilkinson kelly sanders ryan scally shana mckay and michael gosatti of the city of canning western australia and kel medbury and nathan harper of the water corporation of western australia for access to their smart meter data and for their valuable feedback in the development of this system this research is funded by the cooperative research centre for water sensitive cities crcwsc under intelligent urban water systems project c5 1 author s p is also supported by an australian government research training program scholarship at the university of western australia the authors thank colleagues across the crcwsc programs for their feedback on this project 
26448,this study evaluated the potential for circuit level electricity data to improve performance by a water end use disaggregation tool support vector machine classifiers were employed to categorize observed water events from an extensive dataset published in the literature additional electricity related event features were assigned depending on temporal proximity to recent clothes washer or dishwasher events classifiers were trained on a portion of the dataset with and without the electricity related features then tested on an equally sized portion of the dataset a classifier also categorized events from the testing dataset where event durations were adjusted to match larger sampling intervals from 10s up to 120s specific electricity related features significantly improved classifier performance for clothes washer dishwasher and shower events classifier performance was maintained for longer events as sampling frequency decreased although performance for short duration events decreased overall these results indicate significant potential benefits from integrating electricity related features for water disaggregation tools keywords water end use event water disaggregation tool residential water and electricity data support vector machine confusion matrix 1 introduction as cities embrace smarter management systems for energy and water resources advanced metering infrastructure ami has become a common enabler for water utilities to improve their business models prevent water theft detect leaks and improve customer service hawkins and berthold 2015 due to energy requirements for water processes increased monitoring for end uses of water to prevent losses or encourage conservation can also have non trivial energy impacts carlson and walburger 2007 sanders and webber 2012 these factors along with projected growth for water demand in urban areas davies 2016 bruun et al 2017 motivate efforts to better characterize and understand the time varying nature of urban water demand with the development of sophisticated water meters and high resolution sensors water use from entire homes can be non intrusively metered with water consumption sampled on a sub daily basis cominola et al 2015 with certain meters or data loggers water use can be measured with very high temporal resolution corresponding to sampling intervals of a few seconds deoreo et al 1996 high resolution water data has enabled applications that characterize patterns of appliance or fixture water consumption and use these patterns to categorize whole home water data by end uses cominola et al 2015 an early example of end use disaggregation software tracewizard employed a decision tree algorithm that uses household audits of appliance stock pre defined templates and trained analysts to predict water events mayer and deoreo 1999 deoreo 2011 deoreo et al 2016 other past examples of disaggregation tools include identiflow hydrosense and autoflow cominola et al 2015 identiflow used decision trees to disaggregate water consumption semi autonomously at the household level kowalski and marshallsay 2003 hydrosense used a pressure sensor to identify unique pressure waves then implemented a bayesian approach to identify when individual fixtures were opened and closed froehlich et al 2009 2011 a more recent piece of software autoflow was developed using an echelon of methods and machine learning algorithms such as hidden markov models dynamic time warping gradient vector filtering and artificial neural networks nguyen et al 2013a b 2014 2015 autoflow was developed and validated using recent end use studies conducted in brisbane and melbourne australia beal and stewart 2011 2014 the tracewizard software has been used widely in the past for large end use studies and autofow is being developed for autonomous commercial applications although the methods used by these two approaches are significantly different both operate using water consumption data with very high temporal resolution cominola et al 2015 for example the autoflow algorithm was developed with and validated on five second interval data with a precision of 0 014 l per pulse from the data logger nguyen et al 2013a b 2015 and the tracewizard algorithm uses 10 s interval data with approximately 0 01 gallons per pulse from the data logger deoreo et al 1996 mayer and deoreo 1999 deoreo et al 2016 while high resolution data enables accurate disaggregation the relationship between sampling frequency and classification accuracy has not been widely reported by the existing literature this relationship is related to the nyquist sampling theorem which states that the highest frequency event that can be measured is at best one half of the sampling frequency matthews et al 2008 to capture microscopic features of a signal rules of thumb suggest that up to 20 data points might be required per fundamental period zeifman and roth 2011 a similar blind identification problem has been studied in the electricity sector where non intrusive load monitoring nilm algorithms attempt to identify distinct appliance signatures from aggregate energy data kolter et al 2010 kolter et al 2012 kolter and ferreira 2011 zeifman and roth 2011 zoha et al 2012 cominola et al 2017 in general pattern recognition based approaches are preferred over optimization approaches zoha et al 2012 such algorithms might be relevant to the water event disaggregation problem although their portability has not been assessed cominola et al 2015 when both electricity and water consumption data are collected a combined approach could use established nilm algorithms to identify electrical appliance signatures of water using devices and proceed to use the information about electricity consumption to improve water disaggregation methods unlike appliance level water sensors appliance level electricity data of major appliances i e clothes washer dishwasher electric water heater can be easily metered via individual circuits circuit level electricity data can simplify a combined approach allowing water disaggregation methods to use measured electricity consumption by major appliances rather than predictions from nilm algorithms electricity consumption data has not been incorporated into existing water disaggregation tools potentially due to the unique datasets required for analysis as more homes adopt smart meters for both electricity and water however there is significant potential for synergetic applications of both types of data a recent analysis employed clustering techniques on circuit level clothes washer and dishwasher data collected from homes in central texas to identify distinct cycle patterns mccartney 2016 once the cycles were clustered water consumption for a given cycle was estimated as the median of water use during cycles from that cluster excess water consumption during each cycle was assigned to other end uses using an approximation mccartney 2016 this analysis was preliminary and water consumption could not be validated against a known dataset to date the authors are unaware of a water disaggregation tool that combines data for electricity consumption and water consumption to make predictions against a known dataset this study is a first step in that direction the purpose of this analysis is to evaluate if circuit level electricity data from clothes washer and dishwasher appliances can improve water event classification accuracy and if performance is maintained as the temporal resolution of water data decreases sampling interval increases to minimize intrusiveness data from the residential end uses of water study reuws version 2 deoreo et al 2016 were used to provide ground truth information to train classifiers features related to appliance electricity consumption were defined for each event in the reuws dataset according to their temporal proximity to known clothes washer or dishwasher events support vector machine svm classifiers were then trained on a portion of the original dataset with and without electricity related features to assess if circuit level electricity data can significantly improve classifier performance classifier performance was also evaluated for sampling intervals as large as two minutes to explore the trade off between classifier performance and the temporal resolution of data sampling a flowchart to visualize the components of the study is shown in fig 1 this study is novel because it represents the first attempt to incorporate information about simultaneous electricity consumption into a water end use disaggregation tool the results from this study provide an indication of the potential for electricity data to support classifier performance even when the temporal resolution of water data is diminished in the future water and electricity disaggregation tools might be replaced by low cost direct sensing of individual fixtures or appliances in a manner that integrates seamlessly into a smart home environment however it is unclear if or when direct sensing will become a standard component of new buildings in the interim water disaggregation tools can provide useful information about consumption at the fixture level using household level data from smart meters whose adoption is ongoing as smart home concepts receive more attention this analysis can support normative suggestions for home energy and water data collection and management hardware requirements and applications 2 methods this section briefly describes the dataset feature selection learning algorithm selection classification methodology and performance metrics used to evaluate classifier performance 2 1 dataset the water event log from the residential end uses of water study reuws version 2 was the primary dataset used in this study deoreo et al 2016 the reuws study was funded by the water research foundation wrf to measure classify and analyze end uses of water in the residential sector deoreo et al 2016 data were collected from nine utilities in the united states and canada where each utility supplied billing data for approximately 1000 homes mailed a survey to approximately 1000 homes and conducted end use monitoring of 100 homes of which 10 were also monitored for hot water consumption data collection took place from 2010 to 2013 generating high quality end use data from 762 homes in nine locations end use data were highly resolved with 80 100 pulses per gallon and 10 s increments as part of the study aquacraft inc gathered information about water using appliances in each home where end use data were collected trained analysts used the tracewizard software to disaggregate whole home water data and classify water events by appliance type each water event entry consisted of the following features participant id appliance type start time volume duration peak flow rate and mode flow rate this study focused on classifying water events from the six largest indoor water consuming fixtures toilet shower faucet clothes washer bath and dishwasher in addition to irrigation water events the full dataset consists of 495 931 faucet events 124 685 toilet events 41 319 clothes washer events 17 079 shower events 11 648 dishwasher events 4754 irrigation events and 1742 bath events bath events which are relatively rare in the dataset are grouped in the same class as shower events for the purposes of this study the raw flow trace data from the reuws study were not available due to privacy restrictions so making distinctions between simultaneously occurring water events i e combined events was beyond the scope of this study similar to approaches used in other studies this study relied on the reuws dataset to provide ground truth information about water events to form a dataset for learning algorithm training cominola et al 2015 2 2 feature selection classifier models reported in this study used input data consisting of two features relating to water consumption event volume and event duration these features explain the majority of variation between typical water events in the reuws dataset shown in fig 2 the exception is irrigation events which do not show a discernible clustering on a density plot adding additional features such as time of day or peak flow rate did not significantly improve classification performance so the additional features were excluded to limit the dimensionality of input data a primary goal of this study was to explore the potential for simultaneous circuit level electricity data to improve the classification accuracy for certain categories of water events major appliances like clothes washers dishwashers and electric water heaters often occupy separate circuits in residential homes to mitigate against overloading an example of circuit level electricity data for clothes washers and dishwashers is maintained by pecan street inc mccartney 2016 which is an independent research group and smart grid demonstration project located in austin tx pecan street collects water and electricity consumption data including circuit level electricity data for over 100 homes the majority of these homes are single family units located in austin tx by plotting water and circuit level electricity data together relationships between electricity consumption and water end uses can be inferred examples of water data and coincident electricity data for clothes washers and dishwashers are shown in figs 3 and 4 clothes washers typically work by first filling the tub with water and then agitating or rotating the tub to clean clothes depending on the type of machine and the type of cycle selected by the user this process may repeat several times until the clothes are rinsed a spin cycle then removes excess water in general clothes washers use water first then electricity to power electric motors dishwashers perform different functions but typically follow a similar pattern water consumption that occurs at the beginning or during a clothes washer or dishwasher cycle while the appliance is consuming electricity is often associated with that event however unrelated water consumption can also occur while a clothes washer or dishwasher consumes electricity for example a user might begin a clothes washer cycle and then take a shower and use the toilet shortly thereafter this chain of events might cause both water and electricity consumption patterns to appear distorted to accommodate overlapping or combined water events a method such as gradient vector filtering nguyen et al 2013b could be incorporated to distinguish discrete usages on the electricity side pattern recognition approaches developed for non intrusive load monitoring applications could be implemented to distinguish appliance cycles within the data from background electricity usage solving the challenges posed by pattern distortion from closely spaced events though beyond the scope of the current study is critical to demonstrating the proposed method in future studies with these limitations in mind circuit level electricity data for major appliances cannot absolutely identify categories of water consumption but they might provide useful information to classifiers in some instances to test the hypothesis that circuit level electricity data can facilitate water event classification two additional features were defined for each event in the reuws dataset f l a g c w and f l a g d w for a given water event the f l a g c w feature indicates whether circuit level electricity consumption by the clothes washer would be observed following the water event for a given house h all water events occurring within d t c w following a clothes washer event are assigned f l a g c w 1 clothes washer events are also assigned f l a g c w 1 the f l a g d w feature is similarly defined for dishwashers the value of both d t c w and d t d w was 60 min for this study which is an estimate for the duration of final drying cycles for both appliances an example of f l a g c w and f l a g d w assignment for twelve events from a single house in the reuws dataset is shown in table 1 2 3 training support vector machine classifiers the matlab classification learner matlab 2016 was used to evaluate several supervised learning algorithms to identify suitable methods for classifying water events initial experiments found that support vector machine svm classifiers achieved a relatively high accuracy with moderate speed and memory requirements svm classifiers work by non linearly mapping input vectors to a high dimension feature space where a linear decision surface is constructed cortes and vapnik 1995 points along this surface form the support vectors which produces the greatest margin between classes in two group classification problems matlab 2016 an echelon of svm classifiers can be extended to solve multi group classification problems with functionality from the fitcecoc function in matlab which can fit multi group support vector machine classifiers matlab 2016 among options for the kernalscale parameter a value of 1 2 was selected to facilitate high accuracy while discouraging overfitting both the training dataset and testing dataset included 4000 input vectors from each category except irrigation which consisted of 2000 inputs the datasets were non overlapping and were randomly sampled from each class without replacement ultimately two svm models were trained for use on the testing dataset the first svm model svm 1 classified all water events into groups by using only features related to event volume and duration the second svm model svm 2 performed a similar task but with f l a g c w and f l a g d w included in the input vector features the difference between performance metrics for both classifiers provides an indication of the potential for circuit level electricity data to improve accuracy for water event disaggregation tools 2 4 assessing classifier performance with lower temporal resolution the relationship between data resolution and classifier accuracy has implications for the design and implementation of disaggregation protocols in commercial devices however the tradeoff between sampling interval and disaggregation accuracy has not been widely reported in the literature if classifier performance remains largely unchanged for a decreased sampling rate then computational requirements associated with the method might decrease when the sampling interval increases however calculations for event features such as duration peak flow rate and mode flow rate can be affected for example two water events with equivalent volumes might have significantly different durations if an event with short duration falls in between sampling intervals the event might appear identical to an event that consumed an equal volume of water over a longer period decreased data resolution meaning less frequent sampling will also impact secondary water event features that rely on precise measurements for flow rate magnitude and gradient at the beginning and end of water events and could be a limiting factor for more complex disaggregation methods that relying on additional features or precise flow rate patterns this study included a quantitative examination of svm classifier performance with input data where the sampling interval varied from 10s to 120s the raw data underlying the reuws study were not available so individual flow traces could not be resampled to simulate lowered temporal resolution instead increased sampling interval was simulated by rounding up event durations to become multiples of the sampling interval for example if the modified sampling interval was d t 60 s all events with a duration between 61 and 120 s were rounded to a duration equal to 120 s this approach simulates a longer sampling interval for single water events that are not directly followed by another event otherwise increasing the sampling interval could cause 1 multiple events falling within a single interval to be reported as one event or 2 distinct water events to overlap in the flow trace and appear as combined events the significance of temporal resolution on combined events is a subject for future analysis 2 5 classifier performance metrics the classifiers presented in this study are assessed on two types of performance occurrence accuracy and volume accuracy occurrence accuracy pertains to the fraction of correctly identified inputs based on number of events and has been previously used to assess water event disaggregation nguyen et al 2013a 2015 volume accuracy pertains to the fraction of water consumption that is correctly classified these two performance metrics are related but potentially very different for example a classifier that successfully predicts small frequent water events but incorrectly predicts infrequent large water events will score differently on the basis of the two metrics the classifier s occurrence accuracy would be high indicating good performance while volume accuracy would indicate the opposite a successful end use disaggregation method should balance performance from both objectives these accuracy metrics can be calculated in aggregate or for each category performance by the svm 1 and svm 2 classifiers will be presented via confusion matrices also known as contingency matrices confusion matrices are graphical aids used for communicating classifier performance in supervised learning powers 2007 specifically they show predicted and actual classifications and can reveal how a classifier mislabels or confuses groups with one another kohavi and provost 1998 each column of the matrix represents instances of predicted class membership while each row represents actual instances of class membership powers 2007 an example confusion matrix is shown fig 5 for an illustrative dataset consisting of actual class membership x a b b b c c and predicted class membership x p a b a b c b the overall true positive rate is reported in the bottom right of the figure the confusion matrices are accompanied by reported values for true positive rate tpr false negative rate fnr positive predictive value ppv and false discovery rate fdr true positive rate also called recall measures the fraction of correctly identified positive cases from each class the positive predictive value also called precision or confidence measures the fraction of predicted positive cases that are real positive cases for each class powers 2007 in section 3 3 receiver operator characteristic roc curves are employed to evaluate classifier performance when the temporal resolution of water events decreases sampling interval increases a roc curve is a technique for visualizing and selecting classifiers based on performance fawcett 2006 roc analysis communicates the trade off between hit rates true positives and false alarms false positives as the classifier discriminant or threshold is varied fawcett 2006 an example roc curve is shown in fig 6 for the classification of iris flowers of three different species matlab 2016 a point on the roc graph is considered better than another if it is to the northwest on the graph fawcett 2006 indicating a higher tpr and lower fpr area under a roc curve is another measure of performance which increases as curves move way from the straight diagonal line a curve along the straight diagonal is indicative of a classifier that randomly guesses the true class half of the time fawcett 2006 section 3 3 uses a class reference formulation to handle multiple classes of water events where the ith roc graph plots classifier performance for class c i against all other classes fawcett 2006 the circle represents the optimal operation point on the roc curve where the following optimality criterion is met 1 s 1 p p c f p c t n c f n c t p where p denotes the prevalence of the target class and c t p c f n c f p and c t n relate to rates of true positives false negatives false positives and true negatives matlab 2016 3 results and discussion classification results and performance metrics are presented in sections 3 1 3 3 section 3 1 presents a two dimensional map for classifying events without electricity related features and introduces confusion matrices to assess classifier performance section 3 2 presents results from svm 2 which incorporated additional features related to coincident electricity consumption by clothes washer and dishwasher appliances results in sections 3 1 3 2 are for the testing dataset which was described in section 2 3 in section 3 3 roc curves are used to assess how classifier accuracy is influenced when the sampling interval of recorded data is increased 3 1 svm 1 classifier event map and performance metrics the svm 1 classifier predicts class membership using only two features to describe the water event duration and volume distinct classification regions for each class can be visualized in two dimensions on an event map depicted in fig 7 the event map communicates the class type assigned by svm 1 to an event of given volume and duration irrigation events which are typically long in duration and often consume large amounts of water occupy the periphery of the map while shower events occupy a large continuous region roughly corresponding to areas of high shower density depicted in fig 2 the large size of the shower region indicates these events are characterized by disparate behaviors and habits across the set of users contributing to a wide range of event features clothes washer dishwasher faucet and toilet events are more tightly grouped in small regions on the event map indicating there might be significant overlap between features from these classes within the training dataset confusion matrices that communicate svm 1 classifier performance across the testing dataset are shown in fig 8 the two confusion matrices are calculated on the basis of occurrence accuracy left and volume accuracy right depicting areas of mislabeling among the classes category specific results for true positives false positives positive predictions and false discoveries are also reported in terms of number of events the classifier performs worst when labeling dishwasher clothes washer and irrigation events in particular true positive rates for clothes washer events on the basis of number of events and volume are both low t p r o c c 0 39 t p r v o l 0 59 based on fig 8 small clothes washer events often fall in the toilet or faucet category while a significant fraction of larger events are identified as showers the tpr for dishwashers is third worst based on number of events and volume for faucet events the classifier identifies a relatively high faction based on number of events but those events represent only 37 of the total volume from the faucet category large faucet events are often classified in the dishwasher region lying to the right of the faucet region in fig 7 the opposite effect is observed for irrigation events where 51 of positively identified events make up 96 of volume from the class short duration irrigation events of moderate volume are fairly common but their volume is small compared to the large irrigation events in the dataset overall svm 1 achieves a true positive rate of 71 across events in the testing dataset corresponding to 93 of total volume indicating moderate accuracy is possible with only input features related to event volume and duration however these numbers are heavily influenced by svm 1 performance on faucet and irrigation events which constitute the majority of event occurrences and event volumes respectively with the faucet category excluded accuracy drops to 68 in terms of number of events with the irrigation category excluded accuracy drops to 80 in terms of volume the goal of introducing electricity related features into a new svm model is to increase true positives and decrease false discoveries for clothes washer and dishwasher categories a decrease in false discovery rates for some classes might result in increased true positive rates for other classes 3 2 svm 2 classifier event map and performance metrics a second svm model svm 2 predicts class membership using four features for each input vector event volume event duration and two flags that indicate coincident circuit level electricity consumption by a clothes washer or dishwasher by tagging water events with flags corresponding to electricity use additional features were available to the classifier for identification of clothes washer events and dishwasher events however the tagging process for f l a g c w and f l a g d w also identified unrelated events that occurred within d t 60 min of clothes washer or dishwasher events potentially limiting the ability of svm 2 to distinguish between classes of the approximately 697 000 events in the full dataset approximately 83 000 non clothes washer events were labeled with f l a g c w 1 indicating they occurred within one hour following a clothes washer event similarly approximately 19 000 non dishwasher events were labeled with f l a g d w 1 approximately 6500 events had both flags set to one and roughly half of these were non clothes washer non dishwasher events because svm 2 classifies events in four dimensions a single two dimensional event map could not be produced instead the classifier yields two dimensional event maps for each combination of f l a g c w and f l a g d w event maps of the three most common combinations are shown in fig 9 unlike svm 1 the electricity related features allow svm 2 to assign different classifications to water events with identical volumes and durations when both flags are equal to zero clothes washer and dishwasher regions do not appear on the event map with larger regions assigned to faucet toilet and shower categories when f l a g c w 1 and f l a g d w 0 only clothes washer shower and irrigation regions appear on the map meaning that faucet and toilet events occurring shortly after clothes washer events will be misclassified when f l a g c w 0 and f l a g d w 1 dishwasher and toilet regions dominate for smaller water events again indicating that faucet events occurring shortly after dishwasher events will be misclassified depending on the electricity related flags the maps in fig 9 will categorize an event that consumes two gallons over two minutes as either a toilet clothes washer or dishwasher event confusion matrices for svm 2 model are shown in fig 10 the matrices illustrate several performance advantages for svm 2 over svm 1 notably svm 2 significantly improves tprs for the dishwasher class to 99 on the basis of number of events and volume from the testing dataset additionally the classifier correctly labels 98 of the events and 96 of the volume from the clothes washer class in fig 7 clothes washer and shower events occupy adjoining regions on the event map by adding electricity flags svm 2 increasingly distinguishes between shower and the clothes washer categories contributing to improved performance for both categories similarly electricity flags help differentiate distinct regions for toilet events clothes washer events and dishwasher events on the event maps in fig 9 classifier performance for the shower and toilet category also improves by several percentage points achieving tprs close to 90 reporting false discovery rates and making comparisons to true positive rates is an important component of assessing how the classifier performs on the testing dataset another advantage of svm 2 is a decreased rate of false discoveries which is below 20 for all categories in terms of occurrences and volumes for example svm 2 classified 3574 of 4000 shower events from the testing set correctly but actually predicted a total of 4201 shower events overall when including false discoveries the false discovery rate fdr 15 is larger than the false positive rate fpr 11 suggesting a tendency for the classifier to overpredict shower events of the faucet events in the testing set 3180 of 4000 were labeled correctly in the testing set overall only 3474 faucet events were predicted when including false discoveries suggesting faucet events are slightly underpredicted in terms of number of events in general svm 2 also slightly overpredicts occurrences of clothes washer dishwasher and toilet events while significantly underpredicting the occurrence of irrigation events if true positive and false discovery rates are approximately equal for different classes negative impacts on classifier performance might be somewhat abated by cancellation of error across classes svm 2 actually achieves a slightly lower true positive rate relative to svm 1 when classifying events from the faucet class 80 versus 82 although this decrease is offset by a 9 accuracy increase in terms of volume still svm 2 accuracy in terms of faucet volume is only 46 indicating that large faucet events are commonly misclassified accurate classification of irrigation events is also a limitation of svm 2 although the classifier achieved a 97 tpr in terms of volume the tpr is only slightly increased to 53 in terms of number of events overall svm 2 achieves a true positive rate of 87 in terms of number of events in the testing dataset in terms of total volume the true positive rate is 96 the training and testing datasets used to train support vector machine classifiers are unique to this study so the ability to compare classifier performance to other water disaggregation tools and draw conclusions is limited with those restrictions in mind a limited comparison can be made to the combined approach using hidden markov models hmm and artificial neural networks ann that achieved high accuracy during testing on single water events from a sample of melbourne homes nguyen et al 2015 accuracies were reported for each category in terms of number of events only recognition accuracies reported for the svm 2 classier for showers and toilets fall short of benchmarks established by the hmm ann approach by approximately 5 nguyen et al 2015 in terms of number of events which indicates the advantage gained by implementing more advanced pattern recognition models and including additional water event features in an ann such as peak flow rate mode flow rate magnitude of initial final water event flow rate rise fall and gradient of initial final flow rate change the reported accuracy of svm 2 for clothes washer and dishwasher events 98 and 99 respectively is actually slightly higher than those reported by the hmm ann approach indicating that electricity related features have potential to effectively distinguish mechanical events from non mechanical events this study proposed adding binary features to indicate whether or not coincident circuit level activity would be observed for a given water event it might be possible to define more descriptive electricity related features to assist water disaggregation tools expanding the testing of electricity related features for water event classification is an area of future study results from the svm 2 classifier establish accuracy benchmarks for recognizing end use categories with simple input features and without any human input in the classification both of which are desirable features for disaggregation tools although svm 2 performance is limited particularly in terms of the number of irrigation events and the total volume of faucet use a method that accurately categorizes clothes washer dishwasher shower and toilet events could be a useful tool for water utilities interested in characterizing residential water demand in their service area to improve classifier performance for certain categories svm models could be trained with defined prior probabilities for each category this approach would effectively increase the size of event map regions corresponding to high prior probability categories at the expense of other regions commercial products might benefit from this approach in the field where the frequency of water events by category differs from the training and testing dataset and instead depends on specific household characteristics 3 3 classifier performance with lower resolution data to observe how svm 2 classifier accuracy depends on the data sampling interval roc curves were established for classifier performance on the testing dataset with event durations processed to simulate lower temporal resolution for a given water event decreasing the sampling rate has the effect of making the event appear longer than it actually was on the map in fig 9 the location occupied by the event shifts upwards possibly into a region corresponding to a different category close to the origin there is significant overlap between regions occupied by several categories including faucets toilets dishwashers and clothes washers these categories are more likely to be reclassified when measured with an increased sampling rate in fig 11 a set of roc curves illustrates classifier performance for clothes washer faucet and toilet events as the sampling rate decreases from once every 10 s to once every 60s to once every 120s in each plot a circle indicates the optimal operating point of the classifier that satisfies equation 1 at each level of data resolution for each category in fig 11 increasing the sampling interval moves the optimal operating point down in the roc space indicating fewer true positives and to the left indicating fewer false discoveries for the shower category increasing the sampling interval moves the optimal operating point up and to the right indicating more events occupy areas in the shower region on the event maps in fig 9 for the performance of one classifier to dominate another its location in the roc space should be up and to the left of its competitor fawcett 2006 conservative classifiers with fewer false discoveries are sometimes preferred in real world domains where negative instances are common fawcett 2006 therefore increased sampling rate does not necessarily result in worse performance by svm 2 even though true positive rates decrease if limiting false discoveries is a priority increased sampling rate might actually benefit svm 2 performance on certain categories practical considerations like battery life and data storage requirements might be important considerations for water utilities interested in implementing water disaggregation tools at scale within their networks the curves in fig 11 indicate that increasing the sampling interval to d t 60 s might represent a tradeoff between reasonably high performance in terms of true positives versus decreased sampling requirements if a water meter register is implemented to record store and transmit the water data a decreased sampling rate could facilitate longer battery life and decreased memory requirements increasing the sampling interval would also reduce the quantity of data collected and could decrease the burden of data management for water utilities in a scenario with larger sampling intervals classifiers that rely on basic water event features might have an advantage over more sophisticated algorithms whose accuracy depends on highly resolved flow patterns as sampling interval increases there will be some decrease in true positive rates across categories though the effect might be small enough where water utilities and consumers can still gain valuable insights about water use within individual homes a potential adverse consequence of decreased sample rate is that two or more single water events might be consolidated into a single measurement of water consumption which would adversely impact categories that primarily consist of short duration events such as dishwashers toilets and faucets another consequence of decreased sampling interval is that multiple single events might increasingly appear as a combined event in the data implementing methods to identify combined events as the sampling rate decreases is the subject of future study although it was not directly studied in this section the volume resolution of water data will also influence water event disaggregation accuracy as the volume interval of water data increases water events with a volume less than the resolution of the metering device will not be recorded unless their flow triggers a reading in this scenario classification of small water events like faucet use will degrade rapidly for example 74 of faucet events in the reuws dataset are less than 0 5 gallons and 88 of faucet events are less than one gallon similarly 34 of dishwasher events are less than one gallon with increased volume intervals water event features tied to highly resolved flow patterns might become more difficult to quantify at a volume resolution on one gallon however classification of larger events such as showers or irrigation events might still be possible using an svm classifier or other methods 4 conclusion this section summarizes results from the analysis performed in this study and then concludes with discussion of limitations and opportunities for future work 4 1 summary of results this study proposes that circuit level electricity data for major household appliances such as clothes washers and dishwashers can be used to define features associated with end use water events and that these features can meaningfully improve the accuracy of autonomous water event disaggregation tools to test this hypothesis two support vector machine classifiers were trained on a subset of the residential end uses of water version 2 event database inputs to the first classifier consisted of features related to event volume and duration while inputs to the second classifier also included features indicating coincident clothes washer or dishwasher electricity consumption when electricity related features were excluded the classifier achieved moderate levels of accuracy indicating that disaggregation with simple water features can be useful even in the absence of electricity data including electricity related features significantly improved classifier performance across several categories particularly for mechanical water devices classifier performance was also assessed for data with decreased sampling rate demonstrating a tradeoff between measuring device requirements and true positive rates of classifier predictions 4 2 future work the data available for this study were limited to water use so features related to circuit level electricity consumption were defined by assigning positive values to the features if water events at the household level occurred within one hour following a known clothes washer or dishwasher event in future work a new method to assign features will be devised by analyzing simultaneous water and circuit level electricity data for known water events among participating homes within the pecan street smart grid demonstration project for a given water event f l a g c w f l a g d w will be set depending on its proximity to the nearest observed circuit level electricity consumption by the clothes washer dishwasher because clothes washer and dishwasher durations can vary significantly the new method to assign features will require independent testing to mitigate a potential source of error if circuit level electricity data are not available nilm techniques zeifman and roth 2011 cominola et al 2015 offer a potential solution for defining appliance level features from aggregated whole home data electricity data can potentially provide other useful features for water event disaggregation tools other than indicating whether a water event occurred during a clothes washer or dishwasher cycle for example a water event immediately preceding a clothes washer cycle is likely to be linked to that event conversely a clothes washer does not typically consume water during a spin cycle in homes with electric hot water heaters certain water events like showers might regularly cause the water heater to operate to replenish hot water supplies conversely an irrigation event would not trigger electricity use by the hot water heater in homes with gas hot water heaters a similar relationship between end uses consuming hot water and gas consumption might be observed a subset of homes within the full set of pecan street participants are instrumented to collect water electricity and gas data and future work will explore relationships within the data that can be used to define additional features for water event disaggregation identifying individual water events from a whole home sample of water data is a related problem that is outside the scope of this study water events that occur in isolation are easily identifiable in recorded flow traces however whole home data can mask when multiple water events are occurring simultaneously posing a challenge for accurate event classification in future work a method to separate combined events such as gradient vector filtering nguyen et al 2013b should be incorporated when processing raw data using raw data in future work will allow for a more rigorous examination of classification accuracy for varying levels of data resolution finding an appropriate threshold for sampling rate and volume resolution is an important practical consideration for water utilities interested in implementing water disaggregation tools that preserve battery life of measuring devices reduce memory requirements of transmitting devices and limit the burden of data management leaks were not included as an event category in this study because they do not originate from a single appliance or fixture in future work a classifier will include leaks on the event map where they will most likely occupy regions near the y axis where volume is small leak detection has been demonstrated in commercial advanced metering products to detect ongoing leaks hawkins and berthold 2015 and this functionality would pair nicely with a water disaggregation tool software and data availability the primary software used in this study was matlab r2016b matlab 2016 which is widely used by scientists and engineers the software is commercially available from the mathworks incorporated natick ma and was first developed in 1984 versions of the software are available for windows ios or linux operating systems matlab r2017a matlab 2017 can be purchased online for 2 150 although discounts are available for academic users the mathworks can be contacted at 508 647 7000 there were two primary sources of data used in this study 1 residential end uses of water study version 2 access database point of contact maureen hodgins research manager at the water resources foundation e mail mhodgins waterrf org phone 303 734 3465 address 6666 west quincy avenue denver co 80235 3098 year available 2016 availability and cost freely available with permission software required microsoft access form of repository microsoft access database size of archive 260 mb 2 pecan street dataport point of contact grant fisher water and data director e mail info pecanstreet org phone 512 782 9213 address 3925 west braker lane austin texas 78759 year available 2016 availability and cost freely available with permission form of repository sql database acknowledgements this project was supported by pecan street inc and the texas emerging technology fund osp 201404029001 the authors would like to thank pecan street and the water research foundation for providing data used in this study abbreviations fdr false discovery rate the fraction of events from a given class that are incorrectly classified fnr false negative rate the fraction of assignments made to a given class that are false nilm non intrusive load monitoring processes that monitor household electricity consumption and infer information about operations and consumption by individual appliances ppv positive predictive value the fraction of assignments made to a given class that are true reuws residential end uses of water study version 2 was commissioned by the water research foundation to research water consumption practices by residential consumers in the united states and canada roc receiver operator characteristic a visual illustration of classifier performance as the threshold of the classifier discriminant is varied svm support vector machine a type of machine learning algorithm to make binary classifications tpr true positive rate the fraction of events from a given class that are correctly classified wrf water research foundation a research organization dedicated to advancing the science of water by sponsoring research 
26448,this study evaluated the potential for circuit level electricity data to improve performance by a water end use disaggregation tool support vector machine classifiers were employed to categorize observed water events from an extensive dataset published in the literature additional electricity related event features were assigned depending on temporal proximity to recent clothes washer or dishwasher events classifiers were trained on a portion of the dataset with and without the electricity related features then tested on an equally sized portion of the dataset a classifier also categorized events from the testing dataset where event durations were adjusted to match larger sampling intervals from 10s up to 120s specific electricity related features significantly improved classifier performance for clothes washer dishwasher and shower events classifier performance was maintained for longer events as sampling frequency decreased although performance for short duration events decreased overall these results indicate significant potential benefits from integrating electricity related features for water disaggregation tools keywords water end use event water disaggregation tool residential water and electricity data support vector machine confusion matrix 1 introduction as cities embrace smarter management systems for energy and water resources advanced metering infrastructure ami has become a common enabler for water utilities to improve their business models prevent water theft detect leaks and improve customer service hawkins and berthold 2015 due to energy requirements for water processes increased monitoring for end uses of water to prevent losses or encourage conservation can also have non trivial energy impacts carlson and walburger 2007 sanders and webber 2012 these factors along with projected growth for water demand in urban areas davies 2016 bruun et al 2017 motivate efforts to better characterize and understand the time varying nature of urban water demand with the development of sophisticated water meters and high resolution sensors water use from entire homes can be non intrusively metered with water consumption sampled on a sub daily basis cominola et al 2015 with certain meters or data loggers water use can be measured with very high temporal resolution corresponding to sampling intervals of a few seconds deoreo et al 1996 high resolution water data has enabled applications that characterize patterns of appliance or fixture water consumption and use these patterns to categorize whole home water data by end uses cominola et al 2015 an early example of end use disaggregation software tracewizard employed a decision tree algorithm that uses household audits of appliance stock pre defined templates and trained analysts to predict water events mayer and deoreo 1999 deoreo 2011 deoreo et al 2016 other past examples of disaggregation tools include identiflow hydrosense and autoflow cominola et al 2015 identiflow used decision trees to disaggregate water consumption semi autonomously at the household level kowalski and marshallsay 2003 hydrosense used a pressure sensor to identify unique pressure waves then implemented a bayesian approach to identify when individual fixtures were opened and closed froehlich et al 2009 2011 a more recent piece of software autoflow was developed using an echelon of methods and machine learning algorithms such as hidden markov models dynamic time warping gradient vector filtering and artificial neural networks nguyen et al 2013a b 2014 2015 autoflow was developed and validated using recent end use studies conducted in brisbane and melbourne australia beal and stewart 2011 2014 the tracewizard software has been used widely in the past for large end use studies and autofow is being developed for autonomous commercial applications although the methods used by these two approaches are significantly different both operate using water consumption data with very high temporal resolution cominola et al 2015 for example the autoflow algorithm was developed with and validated on five second interval data with a precision of 0 014 l per pulse from the data logger nguyen et al 2013a b 2015 and the tracewizard algorithm uses 10 s interval data with approximately 0 01 gallons per pulse from the data logger deoreo et al 1996 mayer and deoreo 1999 deoreo et al 2016 while high resolution data enables accurate disaggregation the relationship between sampling frequency and classification accuracy has not been widely reported by the existing literature this relationship is related to the nyquist sampling theorem which states that the highest frequency event that can be measured is at best one half of the sampling frequency matthews et al 2008 to capture microscopic features of a signal rules of thumb suggest that up to 20 data points might be required per fundamental period zeifman and roth 2011 a similar blind identification problem has been studied in the electricity sector where non intrusive load monitoring nilm algorithms attempt to identify distinct appliance signatures from aggregate energy data kolter et al 2010 kolter et al 2012 kolter and ferreira 2011 zeifman and roth 2011 zoha et al 2012 cominola et al 2017 in general pattern recognition based approaches are preferred over optimization approaches zoha et al 2012 such algorithms might be relevant to the water event disaggregation problem although their portability has not been assessed cominola et al 2015 when both electricity and water consumption data are collected a combined approach could use established nilm algorithms to identify electrical appliance signatures of water using devices and proceed to use the information about electricity consumption to improve water disaggregation methods unlike appliance level water sensors appliance level electricity data of major appliances i e clothes washer dishwasher electric water heater can be easily metered via individual circuits circuit level electricity data can simplify a combined approach allowing water disaggregation methods to use measured electricity consumption by major appliances rather than predictions from nilm algorithms electricity consumption data has not been incorporated into existing water disaggregation tools potentially due to the unique datasets required for analysis as more homes adopt smart meters for both electricity and water however there is significant potential for synergetic applications of both types of data a recent analysis employed clustering techniques on circuit level clothes washer and dishwasher data collected from homes in central texas to identify distinct cycle patterns mccartney 2016 once the cycles were clustered water consumption for a given cycle was estimated as the median of water use during cycles from that cluster excess water consumption during each cycle was assigned to other end uses using an approximation mccartney 2016 this analysis was preliminary and water consumption could not be validated against a known dataset to date the authors are unaware of a water disaggregation tool that combines data for electricity consumption and water consumption to make predictions against a known dataset this study is a first step in that direction the purpose of this analysis is to evaluate if circuit level electricity data from clothes washer and dishwasher appliances can improve water event classification accuracy and if performance is maintained as the temporal resolution of water data decreases sampling interval increases to minimize intrusiveness data from the residential end uses of water study reuws version 2 deoreo et al 2016 were used to provide ground truth information to train classifiers features related to appliance electricity consumption were defined for each event in the reuws dataset according to their temporal proximity to known clothes washer or dishwasher events support vector machine svm classifiers were then trained on a portion of the original dataset with and without electricity related features to assess if circuit level electricity data can significantly improve classifier performance classifier performance was also evaluated for sampling intervals as large as two minutes to explore the trade off between classifier performance and the temporal resolution of data sampling a flowchart to visualize the components of the study is shown in fig 1 this study is novel because it represents the first attempt to incorporate information about simultaneous electricity consumption into a water end use disaggregation tool the results from this study provide an indication of the potential for electricity data to support classifier performance even when the temporal resolution of water data is diminished in the future water and electricity disaggregation tools might be replaced by low cost direct sensing of individual fixtures or appliances in a manner that integrates seamlessly into a smart home environment however it is unclear if or when direct sensing will become a standard component of new buildings in the interim water disaggregation tools can provide useful information about consumption at the fixture level using household level data from smart meters whose adoption is ongoing as smart home concepts receive more attention this analysis can support normative suggestions for home energy and water data collection and management hardware requirements and applications 2 methods this section briefly describes the dataset feature selection learning algorithm selection classification methodology and performance metrics used to evaluate classifier performance 2 1 dataset the water event log from the residential end uses of water study reuws version 2 was the primary dataset used in this study deoreo et al 2016 the reuws study was funded by the water research foundation wrf to measure classify and analyze end uses of water in the residential sector deoreo et al 2016 data were collected from nine utilities in the united states and canada where each utility supplied billing data for approximately 1000 homes mailed a survey to approximately 1000 homes and conducted end use monitoring of 100 homes of which 10 were also monitored for hot water consumption data collection took place from 2010 to 2013 generating high quality end use data from 762 homes in nine locations end use data were highly resolved with 80 100 pulses per gallon and 10 s increments as part of the study aquacraft inc gathered information about water using appliances in each home where end use data were collected trained analysts used the tracewizard software to disaggregate whole home water data and classify water events by appliance type each water event entry consisted of the following features participant id appliance type start time volume duration peak flow rate and mode flow rate this study focused on classifying water events from the six largest indoor water consuming fixtures toilet shower faucet clothes washer bath and dishwasher in addition to irrigation water events the full dataset consists of 495 931 faucet events 124 685 toilet events 41 319 clothes washer events 17 079 shower events 11 648 dishwasher events 4754 irrigation events and 1742 bath events bath events which are relatively rare in the dataset are grouped in the same class as shower events for the purposes of this study the raw flow trace data from the reuws study were not available due to privacy restrictions so making distinctions between simultaneously occurring water events i e combined events was beyond the scope of this study similar to approaches used in other studies this study relied on the reuws dataset to provide ground truth information about water events to form a dataset for learning algorithm training cominola et al 2015 2 2 feature selection classifier models reported in this study used input data consisting of two features relating to water consumption event volume and event duration these features explain the majority of variation between typical water events in the reuws dataset shown in fig 2 the exception is irrigation events which do not show a discernible clustering on a density plot adding additional features such as time of day or peak flow rate did not significantly improve classification performance so the additional features were excluded to limit the dimensionality of input data a primary goal of this study was to explore the potential for simultaneous circuit level electricity data to improve the classification accuracy for certain categories of water events major appliances like clothes washers dishwashers and electric water heaters often occupy separate circuits in residential homes to mitigate against overloading an example of circuit level electricity data for clothes washers and dishwashers is maintained by pecan street inc mccartney 2016 which is an independent research group and smart grid demonstration project located in austin tx pecan street collects water and electricity consumption data including circuit level electricity data for over 100 homes the majority of these homes are single family units located in austin tx by plotting water and circuit level electricity data together relationships between electricity consumption and water end uses can be inferred examples of water data and coincident electricity data for clothes washers and dishwashers are shown in figs 3 and 4 clothes washers typically work by first filling the tub with water and then agitating or rotating the tub to clean clothes depending on the type of machine and the type of cycle selected by the user this process may repeat several times until the clothes are rinsed a spin cycle then removes excess water in general clothes washers use water first then electricity to power electric motors dishwashers perform different functions but typically follow a similar pattern water consumption that occurs at the beginning or during a clothes washer or dishwasher cycle while the appliance is consuming electricity is often associated with that event however unrelated water consumption can also occur while a clothes washer or dishwasher consumes electricity for example a user might begin a clothes washer cycle and then take a shower and use the toilet shortly thereafter this chain of events might cause both water and electricity consumption patterns to appear distorted to accommodate overlapping or combined water events a method such as gradient vector filtering nguyen et al 2013b could be incorporated to distinguish discrete usages on the electricity side pattern recognition approaches developed for non intrusive load monitoring applications could be implemented to distinguish appliance cycles within the data from background electricity usage solving the challenges posed by pattern distortion from closely spaced events though beyond the scope of the current study is critical to demonstrating the proposed method in future studies with these limitations in mind circuit level electricity data for major appliances cannot absolutely identify categories of water consumption but they might provide useful information to classifiers in some instances to test the hypothesis that circuit level electricity data can facilitate water event classification two additional features were defined for each event in the reuws dataset f l a g c w and f l a g d w for a given water event the f l a g c w feature indicates whether circuit level electricity consumption by the clothes washer would be observed following the water event for a given house h all water events occurring within d t c w following a clothes washer event are assigned f l a g c w 1 clothes washer events are also assigned f l a g c w 1 the f l a g d w feature is similarly defined for dishwashers the value of both d t c w and d t d w was 60 min for this study which is an estimate for the duration of final drying cycles for both appliances an example of f l a g c w and f l a g d w assignment for twelve events from a single house in the reuws dataset is shown in table 1 2 3 training support vector machine classifiers the matlab classification learner matlab 2016 was used to evaluate several supervised learning algorithms to identify suitable methods for classifying water events initial experiments found that support vector machine svm classifiers achieved a relatively high accuracy with moderate speed and memory requirements svm classifiers work by non linearly mapping input vectors to a high dimension feature space where a linear decision surface is constructed cortes and vapnik 1995 points along this surface form the support vectors which produces the greatest margin between classes in two group classification problems matlab 2016 an echelon of svm classifiers can be extended to solve multi group classification problems with functionality from the fitcecoc function in matlab which can fit multi group support vector machine classifiers matlab 2016 among options for the kernalscale parameter a value of 1 2 was selected to facilitate high accuracy while discouraging overfitting both the training dataset and testing dataset included 4000 input vectors from each category except irrigation which consisted of 2000 inputs the datasets were non overlapping and were randomly sampled from each class without replacement ultimately two svm models were trained for use on the testing dataset the first svm model svm 1 classified all water events into groups by using only features related to event volume and duration the second svm model svm 2 performed a similar task but with f l a g c w and f l a g d w included in the input vector features the difference between performance metrics for both classifiers provides an indication of the potential for circuit level electricity data to improve accuracy for water event disaggregation tools 2 4 assessing classifier performance with lower temporal resolution the relationship between data resolution and classifier accuracy has implications for the design and implementation of disaggregation protocols in commercial devices however the tradeoff between sampling interval and disaggregation accuracy has not been widely reported in the literature if classifier performance remains largely unchanged for a decreased sampling rate then computational requirements associated with the method might decrease when the sampling interval increases however calculations for event features such as duration peak flow rate and mode flow rate can be affected for example two water events with equivalent volumes might have significantly different durations if an event with short duration falls in between sampling intervals the event might appear identical to an event that consumed an equal volume of water over a longer period decreased data resolution meaning less frequent sampling will also impact secondary water event features that rely on precise measurements for flow rate magnitude and gradient at the beginning and end of water events and could be a limiting factor for more complex disaggregation methods that relying on additional features or precise flow rate patterns this study included a quantitative examination of svm classifier performance with input data where the sampling interval varied from 10s to 120s the raw data underlying the reuws study were not available so individual flow traces could not be resampled to simulate lowered temporal resolution instead increased sampling interval was simulated by rounding up event durations to become multiples of the sampling interval for example if the modified sampling interval was d t 60 s all events with a duration between 61 and 120 s were rounded to a duration equal to 120 s this approach simulates a longer sampling interval for single water events that are not directly followed by another event otherwise increasing the sampling interval could cause 1 multiple events falling within a single interval to be reported as one event or 2 distinct water events to overlap in the flow trace and appear as combined events the significance of temporal resolution on combined events is a subject for future analysis 2 5 classifier performance metrics the classifiers presented in this study are assessed on two types of performance occurrence accuracy and volume accuracy occurrence accuracy pertains to the fraction of correctly identified inputs based on number of events and has been previously used to assess water event disaggregation nguyen et al 2013a 2015 volume accuracy pertains to the fraction of water consumption that is correctly classified these two performance metrics are related but potentially very different for example a classifier that successfully predicts small frequent water events but incorrectly predicts infrequent large water events will score differently on the basis of the two metrics the classifier s occurrence accuracy would be high indicating good performance while volume accuracy would indicate the opposite a successful end use disaggregation method should balance performance from both objectives these accuracy metrics can be calculated in aggregate or for each category performance by the svm 1 and svm 2 classifiers will be presented via confusion matrices also known as contingency matrices confusion matrices are graphical aids used for communicating classifier performance in supervised learning powers 2007 specifically they show predicted and actual classifications and can reveal how a classifier mislabels or confuses groups with one another kohavi and provost 1998 each column of the matrix represents instances of predicted class membership while each row represents actual instances of class membership powers 2007 an example confusion matrix is shown fig 5 for an illustrative dataset consisting of actual class membership x a b b b c c and predicted class membership x p a b a b c b the overall true positive rate is reported in the bottom right of the figure the confusion matrices are accompanied by reported values for true positive rate tpr false negative rate fnr positive predictive value ppv and false discovery rate fdr true positive rate also called recall measures the fraction of correctly identified positive cases from each class the positive predictive value also called precision or confidence measures the fraction of predicted positive cases that are real positive cases for each class powers 2007 in section 3 3 receiver operator characteristic roc curves are employed to evaluate classifier performance when the temporal resolution of water events decreases sampling interval increases a roc curve is a technique for visualizing and selecting classifiers based on performance fawcett 2006 roc analysis communicates the trade off between hit rates true positives and false alarms false positives as the classifier discriminant or threshold is varied fawcett 2006 an example roc curve is shown in fig 6 for the classification of iris flowers of three different species matlab 2016 a point on the roc graph is considered better than another if it is to the northwest on the graph fawcett 2006 indicating a higher tpr and lower fpr area under a roc curve is another measure of performance which increases as curves move way from the straight diagonal line a curve along the straight diagonal is indicative of a classifier that randomly guesses the true class half of the time fawcett 2006 section 3 3 uses a class reference formulation to handle multiple classes of water events where the ith roc graph plots classifier performance for class c i against all other classes fawcett 2006 the circle represents the optimal operation point on the roc curve where the following optimality criterion is met 1 s 1 p p c f p c t n c f n c t p where p denotes the prevalence of the target class and c t p c f n c f p and c t n relate to rates of true positives false negatives false positives and true negatives matlab 2016 3 results and discussion classification results and performance metrics are presented in sections 3 1 3 3 section 3 1 presents a two dimensional map for classifying events without electricity related features and introduces confusion matrices to assess classifier performance section 3 2 presents results from svm 2 which incorporated additional features related to coincident electricity consumption by clothes washer and dishwasher appliances results in sections 3 1 3 2 are for the testing dataset which was described in section 2 3 in section 3 3 roc curves are used to assess how classifier accuracy is influenced when the sampling interval of recorded data is increased 3 1 svm 1 classifier event map and performance metrics the svm 1 classifier predicts class membership using only two features to describe the water event duration and volume distinct classification regions for each class can be visualized in two dimensions on an event map depicted in fig 7 the event map communicates the class type assigned by svm 1 to an event of given volume and duration irrigation events which are typically long in duration and often consume large amounts of water occupy the periphery of the map while shower events occupy a large continuous region roughly corresponding to areas of high shower density depicted in fig 2 the large size of the shower region indicates these events are characterized by disparate behaviors and habits across the set of users contributing to a wide range of event features clothes washer dishwasher faucet and toilet events are more tightly grouped in small regions on the event map indicating there might be significant overlap between features from these classes within the training dataset confusion matrices that communicate svm 1 classifier performance across the testing dataset are shown in fig 8 the two confusion matrices are calculated on the basis of occurrence accuracy left and volume accuracy right depicting areas of mislabeling among the classes category specific results for true positives false positives positive predictions and false discoveries are also reported in terms of number of events the classifier performs worst when labeling dishwasher clothes washer and irrigation events in particular true positive rates for clothes washer events on the basis of number of events and volume are both low t p r o c c 0 39 t p r v o l 0 59 based on fig 8 small clothes washer events often fall in the toilet or faucet category while a significant fraction of larger events are identified as showers the tpr for dishwashers is third worst based on number of events and volume for faucet events the classifier identifies a relatively high faction based on number of events but those events represent only 37 of the total volume from the faucet category large faucet events are often classified in the dishwasher region lying to the right of the faucet region in fig 7 the opposite effect is observed for irrigation events where 51 of positively identified events make up 96 of volume from the class short duration irrigation events of moderate volume are fairly common but their volume is small compared to the large irrigation events in the dataset overall svm 1 achieves a true positive rate of 71 across events in the testing dataset corresponding to 93 of total volume indicating moderate accuracy is possible with only input features related to event volume and duration however these numbers are heavily influenced by svm 1 performance on faucet and irrigation events which constitute the majority of event occurrences and event volumes respectively with the faucet category excluded accuracy drops to 68 in terms of number of events with the irrigation category excluded accuracy drops to 80 in terms of volume the goal of introducing electricity related features into a new svm model is to increase true positives and decrease false discoveries for clothes washer and dishwasher categories a decrease in false discovery rates for some classes might result in increased true positive rates for other classes 3 2 svm 2 classifier event map and performance metrics a second svm model svm 2 predicts class membership using four features for each input vector event volume event duration and two flags that indicate coincident circuit level electricity consumption by a clothes washer or dishwasher by tagging water events with flags corresponding to electricity use additional features were available to the classifier for identification of clothes washer events and dishwasher events however the tagging process for f l a g c w and f l a g d w also identified unrelated events that occurred within d t 60 min of clothes washer or dishwasher events potentially limiting the ability of svm 2 to distinguish between classes of the approximately 697 000 events in the full dataset approximately 83 000 non clothes washer events were labeled with f l a g c w 1 indicating they occurred within one hour following a clothes washer event similarly approximately 19 000 non dishwasher events were labeled with f l a g d w 1 approximately 6500 events had both flags set to one and roughly half of these were non clothes washer non dishwasher events because svm 2 classifies events in four dimensions a single two dimensional event map could not be produced instead the classifier yields two dimensional event maps for each combination of f l a g c w and f l a g d w event maps of the three most common combinations are shown in fig 9 unlike svm 1 the electricity related features allow svm 2 to assign different classifications to water events with identical volumes and durations when both flags are equal to zero clothes washer and dishwasher regions do not appear on the event map with larger regions assigned to faucet toilet and shower categories when f l a g c w 1 and f l a g d w 0 only clothes washer shower and irrigation regions appear on the map meaning that faucet and toilet events occurring shortly after clothes washer events will be misclassified when f l a g c w 0 and f l a g d w 1 dishwasher and toilet regions dominate for smaller water events again indicating that faucet events occurring shortly after dishwasher events will be misclassified depending on the electricity related flags the maps in fig 9 will categorize an event that consumes two gallons over two minutes as either a toilet clothes washer or dishwasher event confusion matrices for svm 2 model are shown in fig 10 the matrices illustrate several performance advantages for svm 2 over svm 1 notably svm 2 significantly improves tprs for the dishwasher class to 99 on the basis of number of events and volume from the testing dataset additionally the classifier correctly labels 98 of the events and 96 of the volume from the clothes washer class in fig 7 clothes washer and shower events occupy adjoining regions on the event map by adding electricity flags svm 2 increasingly distinguishes between shower and the clothes washer categories contributing to improved performance for both categories similarly electricity flags help differentiate distinct regions for toilet events clothes washer events and dishwasher events on the event maps in fig 9 classifier performance for the shower and toilet category also improves by several percentage points achieving tprs close to 90 reporting false discovery rates and making comparisons to true positive rates is an important component of assessing how the classifier performs on the testing dataset another advantage of svm 2 is a decreased rate of false discoveries which is below 20 for all categories in terms of occurrences and volumes for example svm 2 classified 3574 of 4000 shower events from the testing set correctly but actually predicted a total of 4201 shower events overall when including false discoveries the false discovery rate fdr 15 is larger than the false positive rate fpr 11 suggesting a tendency for the classifier to overpredict shower events of the faucet events in the testing set 3180 of 4000 were labeled correctly in the testing set overall only 3474 faucet events were predicted when including false discoveries suggesting faucet events are slightly underpredicted in terms of number of events in general svm 2 also slightly overpredicts occurrences of clothes washer dishwasher and toilet events while significantly underpredicting the occurrence of irrigation events if true positive and false discovery rates are approximately equal for different classes negative impacts on classifier performance might be somewhat abated by cancellation of error across classes svm 2 actually achieves a slightly lower true positive rate relative to svm 1 when classifying events from the faucet class 80 versus 82 although this decrease is offset by a 9 accuracy increase in terms of volume still svm 2 accuracy in terms of faucet volume is only 46 indicating that large faucet events are commonly misclassified accurate classification of irrigation events is also a limitation of svm 2 although the classifier achieved a 97 tpr in terms of volume the tpr is only slightly increased to 53 in terms of number of events overall svm 2 achieves a true positive rate of 87 in terms of number of events in the testing dataset in terms of total volume the true positive rate is 96 the training and testing datasets used to train support vector machine classifiers are unique to this study so the ability to compare classifier performance to other water disaggregation tools and draw conclusions is limited with those restrictions in mind a limited comparison can be made to the combined approach using hidden markov models hmm and artificial neural networks ann that achieved high accuracy during testing on single water events from a sample of melbourne homes nguyen et al 2015 accuracies were reported for each category in terms of number of events only recognition accuracies reported for the svm 2 classier for showers and toilets fall short of benchmarks established by the hmm ann approach by approximately 5 nguyen et al 2015 in terms of number of events which indicates the advantage gained by implementing more advanced pattern recognition models and including additional water event features in an ann such as peak flow rate mode flow rate magnitude of initial final water event flow rate rise fall and gradient of initial final flow rate change the reported accuracy of svm 2 for clothes washer and dishwasher events 98 and 99 respectively is actually slightly higher than those reported by the hmm ann approach indicating that electricity related features have potential to effectively distinguish mechanical events from non mechanical events this study proposed adding binary features to indicate whether or not coincident circuit level activity would be observed for a given water event it might be possible to define more descriptive electricity related features to assist water disaggregation tools expanding the testing of electricity related features for water event classification is an area of future study results from the svm 2 classifier establish accuracy benchmarks for recognizing end use categories with simple input features and without any human input in the classification both of which are desirable features for disaggregation tools although svm 2 performance is limited particularly in terms of the number of irrigation events and the total volume of faucet use a method that accurately categorizes clothes washer dishwasher shower and toilet events could be a useful tool for water utilities interested in characterizing residential water demand in their service area to improve classifier performance for certain categories svm models could be trained with defined prior probabilities for each category this approach would effectively increase the size of event map regions corresponding to high prior probability categories at the expense of other regions commercial products might benefit from this approach in the field where the frequency of water events by category differs from the training and testing dataset and instead depends on specific household characteristics 3 3 classifier performance with lower resolution data to observe how svm 2 classifier accuracy depends on the data sampling interval roc curves were established for classifier performance on the testing dataset with event durations processed to simulate lower temporal resolution for a given water event decreasing the sampling rate has the effect of making the event appear longer than it actually was on the map in fig 9 the location occupied by the event shifts upwards possibly into a region corresponding to a different category close to the origin there is significant overlap between regions occupied by several categories including faucets toilets dishwashers and clothes washers these categories are more likely to be reclassified when measured with an increased sampling rate in fig 11 a set of roc curves illustrates classifier performance for clothes washer faucet and toilet events as the sampling rate decreases from once every 10 s to once every 60s to once every 120s in each plot a circle indicates the optimal operating point of the classifier that satisfies equation 1 at each level of data resolution for each category in fig 11 increasing the sampling interval moves the optimal operating point down in the roc space indicating fewer true positives and to the left indicating fewer false discoveries for the shower category increasing the sampling interval moves the optimal operating point up and to the right indicating more events occupy areas in the shower region on the event maps in fig 9 for the performance of one classifier to dominate another its location in the roc space should be up and to the left of its competitor fawcett 2006 conservative classifiers with fewer false discoveries are sometimes preferred in real world domains where negative instances are common fawcett 2006 therefore increased sampling rate does not necessarily result in worse performance by svm 2 even though true positive rates decrease if limiting false discoveries is a priority increased sampling rate might actually benefit svm 2 performance on certain categories practical considerations like battery life and data storage requirements might be important considerations for water utilities interested in implementing water disaggregation tools at scale within their networks the curves in fig 11 indicate that increasing the sampling interval to d t 60 s might represent a tradeoff between reasonably high performance in terms of true positives versus decreased sampling requirements if a water meter register is implemented to record store and transmit the water data a decreased sampling rate could facilitate longer battery life and decreased memory requirements increasing the sampling interval would also reduce the quantity of data collected and could decrease the burden of data management for water utilities in a scenario with larger sampling intervals classifiers that rely on basic water event features might have an advantage over more sophisticated algorithms whose accuracy depends on highly resolved flow patterns as sampling interval increases there will be some decrease in true positive rates across categories though the effect might be small enough where water utilities and consumers can still gain valuable insights about water use within individual homes a potential adverse consequence of decreased sample rate is that two or more single water events might be consolidated into a single measurement of water consumption which would adversely impact categories that primarily consist of short duration events such as dishwashers toilets and faucets another consequence of decreased sampling interval is that multiple single events might increasingly appear as a combined event in the data implementing methods to identify combined events as the sampling rate decreases is the subject of future study although it was not directly studied in this section the volume resolution of water data will also influence water event disaggregation accuracy as the volume interval of water data increases water events with a volume less than the resolution of the metering device will not be recorded unless their flow triggers a reading in this scenario classification of small water events like faucet use will degrade rapidly for example 74 of faucet events in the reuws dataset are less than 0 5 gallons and 88 of faucet events are less than one gallon similarly 34 of dishwasher events are less than one gallon with increased volume intervals water event features tied to highly resolved flow patterns might become more difficult to quantify at a volume resolution on one gallon however classification of larger events such as showers or irrigation events might still be possible using an svm classifier or other methods 4 conclusion this section summarizes results from the analysis performed in this study and then concludes with discussion of limitations and opportunities for future work 4 1 summary of results this study proposes that circuit level electricity data for major household appliances such as clothes washers and dishwashers can be used to define features associated with end use water events and that these features can meaningfully improve the accuracy of autonomous water event disaggregation tools to test this hypothesis two support vector machine classifiers were trained on a subset of the residential end uses of water version 2 event database inputs to the first classifier consisted of features related to event volume and duration while inputs to the second classifier also included features indicating coincident clothes washer or dishwasher electricity consumption when electricity related features were excluded the classifier achieved moderate levels of accuracy indicating that disaggregation with simple water features can be useful even in the absence of electricity data including electricity related features significantly improved classifier performance across several categories particularly for mechanical water devices classifier performance was also assessed for data with decreased sampling rate demonstrating a tradeoff between measuring device requirements and true positive rates of classifier predictions 4 2 future work the data available for this study were limited to water use so features related to circuit level electricity consumption were defined by assigning positive values to the features if water events at the household level occurred within one hour following a known clothes washer or dishwasher event in future work a new method to assign features will be devised by analyzing simultaneous water and circuit level electricity data for known water events among participating homes within the pecan street smart grid demonstration project for a given water event f l a g c w f l a g d w will be set depending on its proximity to the nearest observed circuit level electricity consumption by the clothes washer dishwasher because clothes washer and dishwasher durations can vary significantly the new method to assign features will require independent testing to mitigate a potential source of error if circuit level electricity data are not available nilm techniques zeifman and roth 2011 cominola et al 2015 offer a potential solution for defining appliance level features from aggregated whole home data electricity data can potentially provide other useful features for water event disaggregation tools other than indicating whether a water event occurred during a clothes washer or dishwasher cycle for example a water event immediately preceding a clothes washer cycle is likely to be linked to that event conversely a clothes washer does not typically consume water during a spin cycle in homes with electric hot water heaters certain water events like showers might regularly cause the water heater to operate to replenish hot water supplies conversely an irrigation event would not trigger electricity use by the hot water heater in homes with gas hot water heaters a similar relationship between end uses consuming hot water and gas consumption might be observed a subset of homes within the full set of pecan street participants are instrumented to collect water electricity and gas data and future work will explore relationships within the data that can be used to define additional features for water event disaggregation identifying individual water events from a whole home sample of water data is a related problem that is outside the scope of this study water events that occur in isolation are easily identifiable in recorded flow traces however whole home data can mask when multiple water events are occurring simultaneously posing a challenge for accurate event classification in future work a method to separate combined events such as gradient vector filtering nguyen et al 2013b should be incorporated when processing raw data using raw data in future work will allow for a more rigorous examination of classification accuracy for varying levels of data resolution finding an appropriate threshold for sampling rate and volume resolution is an important practical consideration for water utilities interested in implementing water disaggregation tools that preserve battery life of measuring devices reduce memory requirements of transmitting devices and limit the burden of data management leaks were not included as an event category in this study because they do not originate from a single appliance or fixture in future work a classifier will include leaks on the event map where they will most likely occupy regions near the y axis where volume is small leak detection has been demonstrated in commercial advanced metering products to detect ongoing leaks hawkins and berthold 2015 and this functionality would pair nicely with a water disaggregation tool software and data availability the primary software used in this study was matlab r2016b matlab 2016 which is widely used by scientists and engineers the software is commercially available from the mathworks incorporated natick ma and was first developed in 1984 versions of the software are available for windows ios or linux operating systems matlab r2017a matlab 2017 can be purchased online for 2 150 although discounts are available for academic users the mathworks can be contacted at 508 647 7000 there were two primary sources of data used in this study 1 residential end uses of water study version 2 access database point of contact maureen hodgins research manager at the water resources foundation e mail mhodgins waterrf org phone 303 734 3465 address 6666 west quincy avenue denver co 80235 3098 year available 2016 availability and cost freely available with permission software required microsoft access form of repository microsoft access database size of archive 260 mb 2 pecan street dataport point of contact grant fisher water and data director e mail info pecanstreet org phone 512 782 9213 address 3925 west braker lane austin texas 78759 year available 2016 availability and cost freely available with permission form of repository sql database acknowledgements this project was supported by pecan street inc and the texas emerging technology fund osp 201404029001 the authors would like to thank pecan street and the water research foundation for providing data used in this study abbreviations fdr false discovery rate the fraction of events from a given class that are incorrectly classified fnr false negative rate the fraction of assignments made to a given class that are false nilm non intrusive load monitoring processes that monitor household electricity consumption and infer information about operations and consumption by individual appliances ppv positive predictive value the fraction of assignments made to a given class that are true reuws residential end uses of water study version 2 was commissioned by the water research foundation to research water consumption practices by residential consumers in the united states and canada roc receiver operator characteristic a visual illustration of classifier performance as the threshold of the classifier discriminant is varied svm support vector machine a type of machine learning algorithm to make binary classifications tpr true positive rate the fraction of events from a given class that are correctly classified wrf water research foundation a research organization dedicated to advancing the science of water by sponsoring research 
26449,immersive virtual reality is applied in many human activities this virtual reality can be used as a training tool for thinning operations in forests the aim of this study is to describe the complex solution of thinning trainer that we developed this system uses the sibyla growth model for predicting forest growth development the model based on virtual reality modeling language was chosen as a virtual reality environment the cave automatic virtual environment was implemented as a hardware tool for the thinning trainer offering an immersive virtual reality of the forest for the users mutual connection of hardware virtual reality and the sibyla growth model is established using a software solution called caveman fully automatic system provides users with the possibility to interact and experiment with a virtual forest graphical abstract image 1 keywords cave system sibyla model virtual reality forest simulation thinning training computer technology abbreviations cave cave automatic virtual environment vr virtual reality vrml virtual reality modeling language hmd head mounted display software availability name of the software sibyla latest version triquetra head developer marek fabrika technical university in zvolen t g masaryka 24 zvolen sk 96053 slovakia tel 421455206298 email address fabrika tuzvo sk first available 2008 costs freeware for non commercial use program language rad studio delphi object pascal program size 3 59 gb hardware required personal computer directx compatible graphic card it is also possible to use opengl 3 59 gb of hdd space keyboard mouse and screen with minimum resolution of 1024 768 with 32 bit color depth software required windows operating system windows xp windows vista windows 7 8 8 1 or 10 availability http sibyla tuzvo sk software html 1 introduction visualization through immersive virtual reality is successfully used in the analyses of complex data and their interpretation in the form of visual perception for users the first cave was developed in 1992 as a specialized hardware for the visualization of virtual worlds cruz neira et al 1993a it was applied in a number of various tasks such as architectonic walks space browser and visualization of molecular dynamics of cancer the usage reflected technological possibilities of that time and showed the importance of sophisticated visualization for science the success of the first cave device stimulated other researchers to develop similar equipment the majority of the cave systems are prototypes that meet specific requirements with regard to visualization purposes they are built in various configurations of projection walls defanti et al 2011 they can either consist of one front two sidewalls and a floor as in the case of the first cave system cruz neira et al 1993b or the user can be completely surrounded by the projection as in the 5 wall starcave defanti et al 2009 or the 6 wall cave systems such as the cornea cornea 2017 c 6 in iowa vrac 2008 many cave systems are based on pointing the projector at the back wall although more modern devices employing screens instead of projection walls are also built such a system was constructed by a team of evl scientists and is known as cave2 febretti et al 2013 which indirectly indicates the second generation of the systems the systems also often exist in the form of mobile display panels consisting only of several screens such as nextcave merrill 2009 the number of projection walls is an important parameter for application opportunities of the device in different scientific areas also other types of sophisticated visualization devices can be used for the purposes of visualization those systems are often hmd head mounted display based which can depend on external sensors e g oculus rift oculus 2017 or devices that use internal sensors of the display device such as mobile phone samsung 2017 accelerometers lg 2017 etc those technologies can work optionally in combination with specialized movement devices such as the virtuix omni virtuix 2017 or the cyberith virtualizer cyberith 2017 both of which address the possible uses of natural forms of movement other complex solutions are virtusphere 2017 or cybersphere fernandes et al 2003 each solution provides a different level of immersivity experience affordability space demands and possibilities of cooperation in a virtual environment many applications of immersive forms of virtual reality in different areas have been registered recently for example in medicine virtual reality is intensively exploited for visualization of human organs during the training of young surgeons kral et al 2004 or for 3 d trips through a human body crees 2012 in biology these systems have been used for the visualization of ontogenesis of microorganisms karr and brady 2000 and in micro biology for the visualization of biological macro molecules virtalis 2012 in geoscience immersive visualization was applied in the search for deposits of mineral resources maes and hunter 2006 devices using immersive virtual reality are also suitable for design and architecture purposes palmer 2011 they can also be used to visualize data obtained from terrestrial or aerial laser scanning kreylos et al 2008 data are visualized directly as point clouds or models are created by post processing of point clouds fabio 2003 methods of immersive visualization are also successfully used in the design and testing of device prototypes to be produced in the future in such a way costs can be saved and high efficiency can be achieved even before starting mass production one such example is the automotive industry bell 2013 there are numerous possibilities for the use of immersive visualization systems as evidenced by other examples produced by scientific institutions evl 2017 dealing with applications development within the framework of applied research a specific usage of visualization techniques and systems of virtual reality is in the area of creating and developing trainers in conjunction with appropriate simulation tools these applications create complex devices intended for training different fields of human activity aviation transport astronautics medicine etc development of technologies has also affected the development of forestry e g trainers in harvesting and transport technologies ovaskainen 2005 the use of trainers is still a rare phenomenon in this field however they could be used to solve many more tasks one of them is training in thinning methods this is a practical activity based on the knowledge gained in the educational process and the experience acquired in the field moreover its impact becomes distinct only after a few years or decades therefore training in thinning methods in the field is incomplete it lacks the immediate feedback that would provide the information about the thinning impact on the forest s state structure and stability these effects can be examined using simulators that model forest development pretzsch 2009 weiskittel et al 2011 burkhart and tomé 2012 fabrika and pretzsch 2013 different modeling concepts process based structural or empirical can be applied spatially explicit distance dependent models e g eco physiological tree models grote and pretzsch 2002 parrott and lange 2004 functional structural plant models prusinkiewicz et al 2007 or empirical tree models hasenauer 1994 nagel 1999 pretzsch et al 2002 fabrika 2007 are suitable for these purposes it is also important to ensure the performance efficiency of the final system because the simulation after the performed thinning must be finished in a short time period in a few seconds to a few minutes these tools are not fully fledged thinning trainers they lack the dimension of practical performance i e manual selection of trees some approaches used simple procedures for the selection of trees directly from the list in a database or a text table pretzsch et al 2002 sterba et al 1995 hasenauer 1994 included the selection of trees in the moses growth model from the horizontal projections of tree crowns however neither approach reproduces selection procedures used in a real forest environment this is because they focus only on the horizontal distribution of trees while the vertical structure is in the background thus some authors have attempted to use three dimensional projections of a forest from multiple products we can mention bwinpro software nagel 1999 tragic parrott and lange 2004 or sylvan stand structure with the sylview visualization extension larsen and scott 2010 a similar approach is used in svs the stand visualization system mcgaughey 1997 a specialized tool for forest visualization these products move the interaction closer to reality because they are oriented toward a more complex idea about the structure of the original and remaining populations nevertheless they still do not copy the performance of thinning in a real forest stand this feature was achieved by implementing virtual reality procedures a successful attempt was smartforest orland 1994 with the forest manager interface extension uusitalo et al 1997 and the treeview product from seifert 1998 this product was successfully linked to the silva pretzsch et al 2002 and balance grote and pretzsch 2002 models a more versatile solution was suggested by authors who used vrml language for modeling virtual worlds as for example in virtual forester lanwert 2007 or in the visualization extension of the sibyla model fabrika 2003 described approaches do not imitate interventions to the forest realistically the marteloscope method poore 2013 is one field thinning training based method its concept uses thinning research plots fixed in the field focused on individual trees and their parameters plots are often reproduced in many forms suitable for use by computer programs even growth simulators this connection is useful according to the forest s virtual reality and possibility for evaluating interventions optionally to simulate the growth but it is fixed only to particular plots that are expensive to establish a better way to ensure the flexibility and versatility of a thinning trainer is to replace such plots fixed in the field with their representations in immersive forms of virtual reality cave hmd based etc this process allows the use of any initial forest stand structure a thinning trainer can be defined as a system composed of a mathematical model of a forest computer software and hardware used for training tree selection and simulation of immediate impact of thinning on forest condition production ecological and economic the difference between a simulator and a trainer is in the thinning realization phase simulators usually use algorithms to select trees for thinning but in a trainer the thinning is done manually by the user preferably interactively in the immersive virtual reality experience a trainer attempts to mimic reality as well as possible i e it contains elements of advanced virtual reality with a high degree of immersivity and interactivity due to this requirement its development requires that the integration of three inevitable components must be solved a mathematical forest growth model a software solution for forest visualization and a hardware tool for forest visualization and interaction the objective of this paper is to present a thinning trainer developed at the technical university in zvolen that uses the sibyla spatially explicit distance dependent empirical tree model as a technological platform and the cave system as a platform for computer aided virtual reality we demonstrate the representativeness of virtual reality in the thinning trainer via comparison of a real research plot with its virtual model in the cave this experiment serves to prove the hypothesis of how reliably a thinning trainer we developed can represent reality at the same time we also managed to perform a practical example a defined type of thinning conducted by two independent persons and the sibyla model the experiment proves the hypothesis that our thinning trainer can substitute for field thinning training methods e g the marteloscope method thus serving as a fully functional system to realize thinning training 2 essentials 2 1 sibyla growth model the sibyla growth model fabrika 2007 is the basis of the thinning trainer at present this model is a hybrid model containing empirical pretzsch 2009 process based landsberg and sands 2011 and structural prusinkiewicz and lindenmayer 1990 modeling principles the core of the model is the empirical model which is also the crucial model for the forest trainer it is a spatially explicit distance dependent tree model the model requires input data on individual trees position diameter height crown parameters quality parameters if the data are not available a forest structure generator is used the given or generated forest structure is displayed with a 3d forest structure model from tree parameters and spatial structure the calculation model calculates all important outputs representing production biomass biodiversity revenues and costs the model is directly parameterized for 5 basic tree species common beech pedunculate or sessile oak norway spruce silver fir and scots pine in total it is possible to simulate 26 different tree species but some of them are derived by modifying growth processes of the basic tree species the simulation of forest development is performed with an interval of 1 year and uses mortality disturbance thinning competition and increment models as well as a model of forest regeneration the mortality model focuses on intrinsic and growth dependent mortality fabrika 2007 the disturbance model addresses externally induced mortality of trees which determines tree mortality caused by the influence of external disturbance factors it is based on risk modeling that consists of the probability model of hazard exposure and vulnerability it addresses tree mortality caused by wind snow ice bark beetles and timber borers defoliators wood destroying fungi air pollutants drought fire and illegal cutting fabrika and vaculčiak 2009 different types of intervention can be modelled using the thinning model from below from above neutral thinning the method of crop trees the method of target dimensions the method of target frequency distribution the geometric method and an interactive thinning method fabrika and ďurský 2005 the possibility of interactive thinning is the most important feature for integration with the thinning trainer the list of the trees removed trees and or crop trees represents the basis for thinning intervention the competition model is based on the crown light competition index kkl proposed by pretzsch 1995 the increment model simulates diameter and height increments of trees it is based on reducing their growth potential growth potential is defined on the basis of ecological site classification according to kahn 1994 ecological site classification is based on climate and soil characteristics length of vegetation period average temperature in vegetation period year temperature amplitude total precipitation during vegetation period amount of co2 and n2o in the air soil moisture and soil nutrient supply growth potential is modified with respect to competition pressure and vitality of the trees determined by the crown size the model is age independent if the tree age is unknown it is derived from the growth potential and the current height at the beginning of the growth period the model of forest regeneration generates trees of the new generation it is an ingrowth model merganič and fabrika 2011 composed from the model of the new generation individual s number the model of diameter and height distribution of the new generation and the model of locating the regeneration in the stand 2 2 virtual reality of the forest forest virtual reality was developed using vrml 97 language iso iec 14772 1 1997 it uses simple objects for the tree and stand visualization fabrika 2003 the visualization principle is shown in fig 1 a tree is divided into three parts stem crown and tree foot a stem and tree foot are displayed as overlapping cones cone nodes a crown is displayed using four planes indexedfaceset nodes which are rotated by 45 using the transform node and one horizontal octagon cutting the vertical planes bark texture of the particular tree species is applied using the imagetexture node as a parameter in the appearance node on the stem and tree foot the base of the cone of the tree foot is coated in wood texture representing a cross section which is important in the case of removed trees the illusion of the real crown is achieved by applying the texture of the crown profile to rotated vertical planes and the texture of the crown projection to the horizontal octagon the textures have transparent background png format was used equations for calculation of dimensions for the solids representing stem crown and tree foot are published in fabrika 2003 for visualization 26 prototypes of tree species were used spruce fir pine douglas fir larch beech oak hornbeam aspen birch cherry tree willow poplar rowan field maple sycamore maple norway maple elm acacia ash lime walnut tree plane tree alder chestnut and yew apart from healthy trees two prototypes of dead trees were also created coniferous and deciduous which are used for visualizing the results of the mortality model all prototypes were created by changing the textures of the stem the tree foot and the tree crown trees are placed on the digital terrain model defined using the patch model obtained from a regular grid individual trees are placed within this grid according to their cartesian coordinates the height coordinate is derived using bilinear interpolation based on horizontal tree coordinates and spatial coordinates of the corners of the specific square cell in the grid the terrain is created using the elevationgrid node and is covered with the stand s soil texture using the imagetexture node as a parameter in the appearance node the stand with the terrain is placed on a large plane covered with the same texture as the soil terrain the stand is located in the center of the cube the stand s surroundings are displayed on the inner sides of the cube background node cubic projection is applied the visibility and the light in the stand are defined using the fog node and the pointlight node respectively tree objects were supplemented by interaction possibilities one kind of interaction allows marking of the trees a green strip around the tree perimeter at a height of 1 3 m indicates a future crop tree and a red strip represents a tree marked for thinning this interaction is provided by a 10 cm high invisible hollow cylinder its gravity center is placed on the axis of the trunk at a height of 1 3 m with the cylinder s diameter equal to the tree s thickness at that height the cylinder is bounded with the event in the form of a script which changes its color from green to red to transparent each time the user clicks on object other interaction allows the cutting of the tree this is provided by an event bounded to the tree s foot by clicking on the tree s foot its position is changed from vertical to horizontal an additional click on the tree s foot returns the tree to its vertical position the virtual forest environment also contains other classes prototypes of objects a range pole a crystal ball and a plant the range pole is an object that serves as a link to a web location or a service opened in a web browser action is provided by the node anchor this node can be used to display information regarding for instance forest stand the crystal ball is represented by a stone base with a matte glass ball the ball is connected with the node anchor which allows the virtual forest scene to change by clicking the ball the new virtual forest stand specified by a url address is loaded the url address is associated with the node anchor this functionality offers teleportation from the actual stand to a past or future state the plant object is created by the node bilboard as a vertical plane placed in the terrain size of the plane width and height is equal to the size of the plant a plane covered by the plant s texture with a transparent background is automatically rotated towards the user in total 193 textures of plants were created to enrich the virtual forest stand s visual representation fig 2 shows the object structure of the virtual forest model 2 3 cave system at the technical university in zvolen the developed cave system is used to visualize natural and technological objects in the form of immersive and interactive virtual reality this system has a cubic shape missing one wall where the entrance is situated i e overall it consists of five walls a front a right a left a top and a bottom the dimensions of the sidewalls are 3 2 25 m and the dimensions of the top and bottom walls are 3 3 m the walls are projection panels on which the stereoscopic image is displayed using back projection from a pair of projectors an infitec stereoscopic system infitec 2017 is used the top and the bottom walls of the cave system have a square shape the discrepancy in the shape of the projected image with the shape of the bottom and top walls is solved by covering them with twice the number of projectors the use of the surrounding system space is optimized using projection mirrors the projection system projects onto five walls while two of them are divided into halves hence 14 projectors are necessary the bottom wall is reinforced with the safety glass to allow motion inside the cube the entire construction of the cube is raised therefore stairs are used to access its space the system s design is shown in fig 3 an observer uses 3d glasses that enable him to see the image in 3d the glasses are equipped with a device for detecting the spatial position of the polhemus type polhemus 2017 monitoring the position of the observer s head in the space and the direction of its view is used to recalculate and correct the projected images and the stereoscopic perspective of the observer movements in the virtual world are mediated either by a joystick or a space mouse in the observer s hands interaction with objects from the virtual world is allowed using another control activated by a fictional laser beam the beam s movement in virtual reality is mediated by moving a control therefore the beam is also a part of the system used for detecting the spatial position in the space of the cube a system of speakers 5 1 is placed to achieve a spatial perception of sound projectors are connected to a cluster of computers with visualization software the cluster is located in the adjacent room and is composed of eight computers seven of which render the image for the projection on the individual projection panels in the stereoscopic mode and one of which is a process control computer the computers contain powerful graphics cards that ensure smooth rendering mounted on the construction of the cave is the console computer which communicates with the control computer of the cluster and is used for managing the visualization system and receiving input from the user superengine control console software is installed on the console computer for the complex control of the computers in the cluster the software activates all of the hardware via the network and controls all necessary visualization applications the tool is versatile able to visualize different formats of virtual worlds however it was designed with regard to the visualization options of a virtual forest produced by the sibyla growth simulator thus the tool that was created enables the mediation of user s interventions into a virtual forest visualized with sophisticated equipment which enhances the experience from reality presented in this manner nevertheless the feedback based on the prognosis of further forest development is essential for prediction purposes the growth simulator needs not only the complex data on a forest at a simulation plot but also the information on performed interventions marked trees to ensure this functionality mutual integration is required of the growth simulator sibyla the virtual reality vr produced by the simulator and the visualization system cave with an adequate software solution 3 experiment mutual integration of the forest growth model sibyla along with cave type specialized hardware for visualization and interaction with the virtual forest comprises the thinning trainer that needs to be evaluated immersive virtual reality in cave must represent the real structure of the forest as much as possible to ensure that all the key characteristics of the forest structure from the field plot are present also in its virtual reality form virtual reality representativeness is also in close relation with the desired thinning method for users to be able to perform its realization in a way close to reality for these purposes we have done two experiments 3 1 evaluation of virtual reality representativeness evaluation of virtual reality representativeness needs establishment of a research plot fig 4 a a plot of 1 ha 100 100 m containing a mixture of tree species mainly beech spruce and fir is situated on a slightly sloped terrain individual tree parameters such as position of the tree in a local x y coordinate system diameter height and crown were measured using the field map system field map 2016 to capture the forest s real structure all data were transferred into the sibyla model s database to create a virtual reality representation of this research plot simultaneously hemispherical images right and left hemisphere were taken on 15 positions using a fisheye lens placed around the plot with the intent to represent its variability amount of the images was set statistically based on variability of forest stand structure diameters and heights of trees precision of the estimation to calculate the amount of images was set to 10 right and left hemisphere pairs of photos were processed into one spherical image which can be displayed as a movable and adjustable spherical panorama in any website browser in the form of a flash or javascript based application fig 4b positions where the spherical images were taken are marked by range poles in the plot s virtual reality in the first variant of the experiment these spherical images were shown one by one to users who were tasked with walking around the virtual forest and trying to find the corresponding structures marked by a range pole in the research plot s virtual reality displayed in the cave system fig 4c we examined whether the user was able to match the real forest structure seen in the spherical image with its virtual reality representation and optionally the number of attempts needed for successful identification to evaluate the success of identification we performed this experiment also in the field on the above mentioned research plot it was the second variant where the same assessors were asked to match the same spherical images with the structures of real forest on set of positions marked by range poles in the field number of assessors for both variants of the experiment was set to 10 persons based on initial pre estimation in the field which showed success of identification 1 3 attempt counting at 94 5 required precision was set to 15 and target confidence level at 95 3 2 thinning training realization for the thinning training experiment we created a simulation plot via the module generator in the sibyla model the plot is 0 25 ha 50 50 m generated as pure beech stand aged 80 years with stocking at 0 85 and a site index of 32 according to the slovak classification climatic and soil characteristics were established to correspond to the selected site index thinning was realized using the method of future crop trees the method is based on selecting high quality individuals i e supporting them by removing their competitors the intention was to maintain approximately 100 crop trees per hectare 25 trees per plot and to remove one of the biggest competitors per future crop tree simulation was done for 10 years of forest development after the thinning this experiment was realized by two independent people and automatically using the sibyla model as a reference 4 solution the technical solution of the thinning trainer is based on the mutual integration of software and hardware tools the applied software is the sibyla simulator which predicts forest development after thinning the specialized hardware used for moving in the virtual forest and manipulating trees is of the cave type finally there is a software link connecting the growth model with the virtual reality and the cave hardware sibyla vr cave system which is based on the caveman module 4 1 thinning trainer system of sibyla vr cave the sophisticated approach of forest visualization in the cave system tries to create a virtual representation that is sufficiently reliable to arouse a feeling in an observer that he is actually present in the visualized environment in such a stereoscopic and immersive form of a virtual forest users can obtain an authentic image of the forest structure from actual position and view angle inside the simulation plot they can move without any restrictions and they can obtain detailed information about individual trees and look into their crowns thus they can assess the trees mutual competition in the canopy in this way forest visualization in the cave system aims to provide the user with complex information about the forest and its individual trees through intensive and interactive visual perception all of the above mentioned features provide a user who performs the intervention in the virtual forest with the necessary information that is also available in the real forest the information is necessary when making decisions regarding which trees are to be left in the forest and which are to be removed with regard to the type of the thinning intervention and its objectives in the virtual forest environment a user can freely move using the assistance control devices joystick or space mouse the controller for implementing interactions can be used to obtain information on individual trees by pointing a virtual laser beam at a tree after which the data are displayed at the front wall of the cave system fig 5 a interventions in a forest are performed by marking individual trees fig 5b using the controller to implement interactions with this device the user activates a laser beam the user points the beam at a desired tree at a height of 1 3 m a spray icon appears the user either marks the tree as a target green or to be removed red or leaves the tree unmarked by repeated pressing of the button it is also possible to cut the tree in actual time if the laser beam points at the tree s foot a symbolic icon of a chainsaw is displayed by pressing the button the tree is cut fig 5c if the removed tree was not the correct one it can be returned to its original state in the same way as it was removed after implementing the intervention in a virtual forest the prognosis of its future development with regard to the performed selection starts for these purposes the growth simulator needs the following input data parameters and positions of individual trees their dimensions and the terrain configuration of the simulation plot climatic and soil characteristics of the site specifications for each tree s categories at the simulation plot target to be removed unmarked the acquisition and the transfer of these data are tasks for the caveman module which ensures the communication link between the sibyla model and the cave hardware tool caveman is a part of the modular structure of the growth simulator and is activated by the user in the cave system s main control software s menu called the superengine control console the module includes a set of operations necessary to start forest growth prognosis and calculation of results the principles of module operations are clarified in fig 6 the stand virtual reality in wrl format is the initial condition visualized in the cave system in this virtual forest a user performs interventions using the approach presented above in addition the stand and site information of the particular virtual forest is given in a tool called the stand diagnostic card which is in the form of a dynamic web site based on html language with javascript in html format both files are produced simultaneously by the growth simulator since they represent the same simulation plot in the given growth period their cross connection is indicated with the bond in the scheme the stand diagnostic card contains data about the whole stand such as production parameters ecological and economic indicators as well as climatic and soil characteristics for the simulation plot the first phase of the caveman module activity is the import of necessary data from the virtual reality file wrl of the visualized forest the module obtains the information on individual trees and the simulation plot the information includes the data on tree species sequential tree number tree position given in coordinates of x y on the plot age diameter height crown diameter height to crown base and assortment class and tree damage as well as data on the terrain s configuration and dimensions of the simulation plot from the html file of the stand diagnostic card the caveman module obtains data on climatic and soil conditions of the plot tree status after the virtual reality intervention is recorded in a file which is automatically created by cave s control software after the start up of the caveman module on the console the status of the tree corresponding to the performed intervention in the forest will be transferred from this file the information includes the sequential tree number and its status 0 unmarked tree 1 target tree and 1 tree to be removed at the end of this phase the user sets the length of the prognosis in a dialogue box within the module in the next phase data are transferred into the database structure of the sibyla growth model when the data import is finished the caveman module automatically activates the prophesier module of the sibyla growth simulator which prognosticates the growth per individual tree at the simulation plot during the prognosis the initial forest structure climatic and soil characteristics and performed interventions in the forest are taken into account after the successful completion of the prognosis the prophesier module is terminated from the predicted data representing individual trees production ecological and economic characteristics of the whole stand have to be calculated the caveman module ensures this by running the calculator module this module updates all relevant data for the given forest stand after the calculator module is terminated all necessary data are stored in the database of the growth simulator the data include the results of the growth prognosis that are linked to the initial forest structure the calculated stand characteristics and the data obtained during the data import which did not change after the simulation such data represent the simulation plot and its terrain configuration climatic and site data and marking of the target crop trees at the end the caveman module activates the generator of the virtual stand and the stand diagnostic card the generator creates a new virtual reality in wrl format representing the condition after the prognosis of forest growth the links on the crystal balls in the virtual environment for the user s time teleport to the past or the future and the link for the stand diagnostic card are updated in the forest virtual reality s file a new stand diagnostic card with updated data on the new stand structure is also created in the environment of the cave system the new virtual reality is presented to the user showing the forest condition after the intervention with changed parameters of individual trees updated stand data in the stand diagnostic card can be displayed on the control console this information is important for evaluating the intervention on the basis of stand characteristic changes by clicking on the crystal balls in the stand s virtual reality time teleport is possible between the growth periods at the simulation plot this serves to compare initial stand condition with condition after prognosis via changes in virtual representation of stand structure and by changes in important stand characteristics in the stand diagnostic card at the same time in the new virtual reality a user can again perform interventions in the forest and start the growth prognosis thus the whole cycle of operations of the caveman module is repeated as shown in fig 6 by loops a set of virtual reality cards and stand diagnostic cards this results in a fully automated process with minimum need of user assistance the thinning trainer can be handled by only 1 operator user it is sufficient for the user after implementing an intervention to start the caveman module on the console of the cave system and set the prognosis length the module takes care of everything else time needed for simulation process for standard size of research plot 50 50 m and for 10 years of prognosis varies from 30 s up to 2 min depending on structure of the forest mainly number of trees on the plot scene performance was tested on sample of 20 000 trees what is approximately 50 ha of mature stand the image rendering was smooth and without any issues or freezing upon movement sample area or number of the trees greatly exceeds the needs of thinning trainer 5 results 5 1 virtual reality representativeness in the first variant of the experiment the assessors were asked to match real forest structure captured on 15 spherical images with its virtual reality representation people who underwent the experiment had three attempts to match the structure seen on the spherical image with its virtual reality representation in the cave environment otherwise the identification was considered unsuccessful second variant of the experiment was intended to match the same set of spherical images with the real forest structures from which those images were taken both variants of experiment were performed by the same assessors table 1 shows relative and absolute numbers of success in identification for both variants and per attempts needed 5 2 thinning training experiment the usability of the cave based thinning trainer as an alternative to field thinning training methods was verified by the thinning experiment a defined type of thinning was performed by two independent persons and the sibyla model itself as a reference after thinning we applied growth simulation for 10 years in the future to see the changes in stand parameters tables 2 4 show the results describing stand structure before and after the thinning was performed results show that even starting from the same structure total stand the desired thinning method can be applied in multiple ways by comparing the sibyla model with the both assessors we can conclude that there is a difference in the preferences of tree selection the sibyla model has selected comparable number of trees from quality classes a and b as crop trees while the assessors varied more in total number of crop trees and their preferred crop trees were mainly of a quality class overall results after prognosis of both assessors and sibyla are comparable and well representing the desired thinning method 6 discussion realization of thinning is a notably difficult task and requires considerable skill to perform with respect to the desired forest functions our intention was to build the thinning trainer which will help to understand and teach users to perform thinning correctly it was necessary to build an environment that offers reality like experience but this experience must be shareable between multiple users in the case of virtual reality the aspect of mutual interaction is often important cruz neira et al 1992 this interaction can only be achieved with a tool that allows participation of more than one user in the same environment this feature is definitely important also for the thinning trainer in the real forest stand several people can discuss which trees to mark and this possibility should be preserved also in the virtual stand hmd devices are unable to meet this need while the cave devices are adapted to it in the case of hmd technology the problem is that it is participative only in one way it s basically the one way shared virtual reality people around the user of hmd can see exactly what the hmd user sees on the display of the computer images for both eyes but the feedback in form of nonverbal communication is not possible for example the mentor trainer which trains the student trainee with hmd cannot point on tree its part or position in the space of vr to which the student should get the attention because the student with hmd cannot see him but can only hear his advices verbal navigation in the space is very time consuming and often confusing improvement could be in mutually shared virtual reality in both ways in form of multiplayer where the multiple users use hmd and each of them is represented via avatar thus is able to communicate verbally and non verbally we do not have such solution yet but its development is planned for hmd version of the thinning trainer we think that cave is more suitable to be participative for these purposes and from our experience the cooperation of multiple persons in the cave was performed without any major issues it is true that user which wears tracked 3d glasses is the only one that gets the non deformed image however the image deformation for the other users is not crucial because they are moving in limited space 3 3 m so relatively close to tracked user image deformation can be seen on edges of the screens for example trees may look cranked instead of straight edge of front and upper wall perspective within individual walls is correct image is seen with correct depth and 3d experience other users also wear 3d glasses in this case it is simple to point on desired tree its part or position in the space it is not only verbal but also non verbal communication between the users which is very important for the mentor in the cave other users can see not only space that tracked user can see but also the surrounding area this is when the mentor can draw attention to situation which is not in field of view of tracked user hmd systems are strictly limited by the user s field of view for example oculus rift 110 htc vive 110 lg valve 110 samsung gear vr 101 playstation vr 100 starvr ima 210 google cardboard 90 microsoft hololens 90 110 virtualrealitytimes 2017 on the other hand hmd based technologies are mobile devices and also way more affordable because of their low cost that is still decreasing with the progress in technology they can depend on external sensors oculus 2017 placed around the user what can be partially limiting or the sensors which are a part of the device samsung 2017 lg 2017 what makes them even more mobile quality of the visualized image in hmd can be different if they use one display per eye oculus 2017 with high resolution or screen of mobile phone used as display for both eyes samsung 2017 what reduces the resolution performance of computer powered hmd is also way greater than the possibilities of mobile phone used in vr headset considering all the pros and cons of the above mentioned technologies we decided to build a forest trainer based on the cave technology mainly because of its cooperation mode ability to share the virtual reality of the forest in both ways with the more persons at the same time and place the thinning trainer based on the individual tree growth model and sophisticated visualization system helps the users to achieve an experience similar to that found in the real forest we tried to build it in as realistic a way as possible to represent the forest structure i e to prepare a virtual environment containing all components with bonds between them to be reliable for the users from our observations the level of virtual world fidelity is the key factor for performing the actions which depend on visual interpretation of real system for this reason we carried out the first experiment on virtual reality representativeness the experiment showed that even if the virtual reality of the forest were built from simple geometrical objects covered with appropriate textures forming a virtual forest it could in majority of attempts be matched with the structure shown on spherical images of a real forest assessors who participated in our experiment on virtual reality representativeness matched vr representation of the forest seen in the cave with the forest structures on spherical images variant 1 of the experiment mostly on the 1st or 2nd attempt situation was similar in the second variant where the same assessors tried to match the same set of spherical images but with the real forest structures from which those images were taken comparing both variants we can see that in second variant the percentage of successful identification was higher for 1st and 2nd attempt in the first variant there is higher success in identification on 3rd attempt on the other hand unsuccessful identification percentage was low as shown in the results for both of variants but higher for the first variant results showed that assessors were more successful in matching of the forest structure seen on spherical images with the real forest structure but the difference in percentage of successful identification compared to vr in the cave is small we can conclude that forest virtual reality of the sibyla model in the cave system sufficiently describes i e represents the real forest structure thanks to this representation users can work not only with randomly generated structures of forests but also with captured and visualized real ones commonly performed thinning training experiments such as the marteloscope method poore 2013 are usually based on permanent research plots established in the field where the knowledge and skills are gained by practical marking and discussion with experts achieved experience is definitely valuable the users select trees according to the plan and are confronted with the change of forest structure parameters but they do not have any feedback on what will occur with the forest in the future after the intervention the thinning trainer we built can overcome several disadvantages of field based experiments using such techniques as field gis technologies and terrestrial laser scanning we are able to capture the real forest structure i e structure of permanent research plots and manipulate it as we would the real one therefore the field thinning training experiments can also be performed in the cave system but furthermore they are enriched for simulation process users can apply growth prognosis after thinning automatically via the sibyla model and are able see the estimation of what will occur with the forest in the future thinning training via our system is not necessarily linked to permanent plots in the field because it is possible to create any kind of the forest structure via the sibyla model and perform thinning at any time in any weather conditions this is the case in our second experiment on thinning training the results of this experiment showed that both assessors were able to implement specified thinning concepts and stand characteristics after simulation were similar to reference thinning made by the sibyla model this proves that users are able to perform a thinning concept because they are provided with all the necessary elements to make a decision on which trees have to be selected as future crop trees and which will be removed users can test themselves on whether the thinning they just realized filled the desired goals on future forest structures or not if something went wrong they can return to the initial state and try again the thinning trainer solution also has some disadvantages in the visualization schematization and the high cost of building the cave device the trainer can be further developed and extended enhancing a virtual forest can be achieved by improving the visualization of trees using the refined tree architecture at a level of branches and leaves this enhancement can be ensured by creating a more comprehensive topology of tree organs vrml language and model architecture of forest visualization is prepared to do that because of the hmd mobility advantage we are currently developing the thinning trainer application for hmd based devices what will allow the application to be used at office home etc to train either individually without trainer or with trainer as avatar in shared vr from what has previously been discussed it is clear that the solution of the forest trainer is not only innovative but also open which allows its further development and the extension of new properties and applications 7 conclusions modern methods of simulating developments in the real world and its visualization using immersive systems of virtual reality represent a prospective direction of scientific development and the popularization of scientific results to the public many scientific and practical disciplines are equipped with trainers for training conducted activities the realization of thinning is an area of practical and scientific activity in forestry which deserves attention in the field of trainer development such trainers should be linked to an efficient simulation tool of forest development should contain forest visualization using virtual reality and should use hardware for the immersive visualization of virtual reality despite a great number of applications for immersive visualization from different areas of science research education and practice we could not find any references for an existing forest thinning trainer in available publications and internet sources that would meet our requirements the design solution of our cave system is an innovative device in european conditions regardless of its usage from the perspective of its application as a thinning trainer and the technical link of individual components the system is unique at a global scale we proposed a solution that connects a forest growth model with a forest visualization solution and modern hardware of the cave type it is a fully automated system with frugal maintenance in addition it has an open architecture which allows the improvement of individual components and the addition of new functionalities it is a practical tool that can support decision making in forests and landscape and forestry education our solution can become the basis for similar devices applied in forestry and ecology acknowledgments the latest sub models of the sibyla growth simulator which is a part of the proposed thinning trainer were developed within the scientific project vega 1 0618 12 titled modeling of forest growth processes at high resolution level the link of the cave device used to display the immersive virtual reality to the growth simulator was carried out within the framework of the slovak research and development agency under the project no apvv 0069 12 titled new technology of nature management newton the cave device was constructed within the project opvav 2009 5 1 03 soro of the operational program for research and development from the european regional development fund experiments for this study were supported by project no apvv 15 0265 titled modeling tree species growth in the carpathian forest ecosystems under different climate change scenarios 
26449,immersive virtual reality is applied in many human activities this virtual reality can be used as a training tool for thinning operations in forests the aim of this study is to describe the complex solution of thinning trainer that we developed this system uses the sibyla growth model for predicting forest growth development the model based on virtual reality modeling language was chosen as a virtual reality environment the cave automatic virtual environment was implemented as a hardware tool for the thinning trainer offering an immersive virtual reality of the forest for the users mutual connection of hardware virtual reality and the sibyla growth model is established using a software solution called caveman fully automatic system provides users with the possibility to interact and experiment with a virtual forest graphical abstract image 1 keywords cave system sibyla model virtual reality forest simulation thinning training computer technology abbreviations cave cave automatic virtual environment vr virtual reality vrml virtual reality modeling language hmd head mounted display software availability name of the software sibyla latest version triquetra head developer marek fabrika technical university in zvolen t g masaryka 24 zvolen sk 96053 slovakia tel 421455206298 email address fabrika tuzvo sk first available 2008 costs freeware for non commercial use program language rad studio delphi object pascal program size 3 59 gb hardware required personal computer directx compatible graphic card it is also possible to use opengl 3 59 gb of hdd space keyboard mouse and screen with minimum resolution of 1024 768 with 32 bit color depth software required windows operating system windows xp windows vista windows 7 8 8 1 or 10 availability http sibyla tuzvo sk software html 1 introduction visualization through immersive virtual reality is successfully used in the analyses of complex data and their interpretation in the form of visual perception for users the first cave was developed in 1992 as a specialized hardware for the visualization of virtual worlds cruz neira et al 1993a it was applied in a number of various tasks such as architectonic walks space browser and visualization of molecular dynamics of cancer the usage reflected technological possibilities of that time and showed the importance of sophisticated visualization for science the success of the first cave device stimulated other researchers to develop similar equipment the majority of the cave systems are prototypes that meet specific requirements with regard to visualization purposes they are built in various configurations of projection walls defanti et al 2011 they can either consist of one front two sidewalls and a floor as in the case of the first cave system cruz neira et al 1993b or the user can be completely surrounded by the projection as in the 5 wall starcave defanti et al 2009 or the 6 wall cave systems such as the cornea cornea 2017 c 6 in iowa vrac 2008 many cave systems are based on pointing the projector at the back wall although more modern devices employing screens instead of projection walls are also built such a system was constructed by a team of evl scientists and is known as cave2 febretti et al 2013 which indirectly indicates the second generation of the systems the systems also often exist in the form of mobile display panels consisting only of several screens such as nextcave merrill 2009 the number of projection walls is an important parameter for application opportunities of the device in different scientific areas also other types of sophisticated visualization devices can be used for the purposes of visualization those systems are often hmd head mounted display based which can depend on external sensors e g oculus rift oculus 2017 or devices that use internal sensors of the display device such as mobile phone samsung 2017 accelerometers lg 2017 etc those technologies can work optionally in combination with specialized movement devices such as the virtuix omni virtuix 2017 or the cyberith virtualizer cyberith 2017 both of which address the possible uses of natural forms of movement other complex solutions are virtusphere 2017 or cybersphere fernandes et al 2003 each solution provides a different level of immersivity experience affordability space demands and possibilities of cooperation in a virtual environment many applications of immersive forms of virtual reality in different areas have been registered recently for example in medicine virtual reality is intensively exploited for visualization of human organs during the training of young surgeons kral et al 2004 or for 3 d trips through a human body crees 2012 in biology these systems have been used for the visualization of ontogenesis of microorganisms karr and brady 2000 and in micro biology for the visualization of biological macro molecules virtalis 2012 in geoscience immersive visualization was applied in the search for deposits of mineral resources maes and hunter 2006 devices using immersive virtual reality are also suitable for design and architecture purposes palmer 2011 they can also be used to visualize data obtained from terrestrial or aerial laser scanning kreylos et al 2008 data are visualized directly as point clouds or models are created by post processing of point clouds fabio 2003 methods of immersive visualization are also successfully used in the design and testing of device prototypes to be produced in the future in such a way costs can be saved and high efficiency can be achieved even before starting mass production one such example is the automotive industry bell 2013 there are numerous possibilities for the use of immersive visualization systems as evidenced by other examples produced by scientific institutions evl 2017 dealing with applications development within the framework of applied research a specific usage of visualization techniques and systems of virtual reality is in the area of creating and developing trainers in conjunction with appropriate simulation tools these applications create complex devices intended for training different fields of human activity aviation transport astronautics medicine etc development of technologies has also affected the development of forestry e g trainers in harvesting and transport technologies ovaskainen 2005 the use of trainers is still a rare phenomenon in this field however they could be used to solve many more tasks one of them is training in thinning methods this is a practical activity based on the knowledge gained in the educational process and the experience acquired in the field moreover its impact becomes distinct only after a few years or decades therefore training in thinning methods in the field is incomplete it lacks the immediate feedback that would provide the information about the thinning impact on the forest s state structure and stability these effects can be examined using simulators that model forest development pretzsch 2009 weiskittel et al 2011 burkhart and tomé 2012 fabrika and pretzsch 2013 different modeling concepts process based structural or empirical can be applied spatially explicit distance dependent models e g eco physiological tree models grote and pretzsch 2002 parrott and lange 2004 functional structural plant models prusinkiewicz et al 2007 or empirical tree models hasenauer 1994 nagel 1999 pretzsch et al 2002 fabrika 2007 are suitable for these purposes it is also important to ensure the performance efficiency of the final system because the simulation after the performed thinning must be finished in a short time period in a few seconds to a few minutes these tools are not fully fledged thinning trainers they lack the dimension of practical performance i e manual selection of trees some approaches used simple procedures for the selection of trees directly from the list in a database or a text table pretzsch et al 2002 sterba et al 1995 hasenauer 1994 included the selection of trees in the moses growth model from the horizontal projections of tree crowns however neither approach reproduces selection procedures used in a real forest environment this is because they focus only on the horizontal distribution of trees while the vertical structure is in the background thus some authors have attempted to use three dimensional projections of a forest from multiple products we can mention bwinpro software nagel 1999 tragic parrott and lange 2004 or sylvan stand structure with the sylview visualization extension larsen and scott 2010 a similar approach is used in svs the stand visualization system mcgaughey 1997 a specialized tool for forest visualization these products move the interaction closer to reality because they are oriented toward a more complex idea about the structure of the original and remaining populations nevertheless they still do not copy the performance of thinning in a real forest stand this feature was achieved by implementing virtual reality procedures a successful attempt was smartforest orland 1994 with the forest manager interface extension uusitalo et al 1997 and the treeview product from seifert 1998 this product was successfully linked to the silva pretzsch et al 2002 and balance grote and pretzsch 2002 models a more versatile solution was suggested by authors who used vrml language for modeling virtual worlds as for example in virtual forester lanwert 2007 or in the visualization extension of the sibyla model fabrika 2003 described approaches do not imitate interventions to the forest realistically the marteloscope method poore 2013 is one field thinning training based method its concept uses thinning research plots fixed in the field focused on individual trees and their parameters plots are often reproduced in many forms suitable for use by computer programs even growth simulators this connection is useful according to the forest s virtual reality and possibility for evaluating interventions optionally to simulate the growth but it is fixed only to particular plots that are expensive to establish a better way to ensure the flexibility and versatility of a thinning trainer is to replace such plots fixed in the field with their representations in immersive forms of virtual reality cave hmd based etc this process allows the use of any initial forest stand structure a thinning trainer can be defined as a system composed of a mathematical model of a forest computer software and hardware used for training tree selection and simulation of immediate impact of thinning on forest condition production ecological and economic the difference between a simulator and a trainer is in the thinning realization phase simulators usually use algorithms to select trees for thinning but in a trainer the thinning is done manually by the user preferably interactively in the immersive virtual reality experience a trainer attempts to mimic reality as well as possible i e it contains elements of advanced virtual reality with a high degree of immersivity and interactivity due to this requirement its development requires that the integration of three inevitable components must be solved a mathematical forest growth model a software solution for forest visualization and a hardware tool for forest visualization and interaction the objective of this paper is to present a thinning trainer developed at the technical university in zvolen that uses the sibyla spatially explicit distance dependent empirical tree model as a technological platform and the cave system as a platform for computer aided virtual reality we demonstrate the representativeness of virtual reality in the thinning trainer via comparison of a real research plot with its virtual model in the cave this experiment serves to prove the hypothesis of how reliably a thinning trainer we developed can represent reality at the same time we also managed to perform a practical example a defined type of thinning conducted by two independent persons and the sibyla model the experiment proves the hypothesis that our thinning trainer can substitute for field thinning training methods e g the marteloscope method thus serving as a fully functional system to realize thinning training 2 essentials 2 1 sibyla growth model the sibyla growth model fabrika 2007 is the basis of the thinning trainer at present this model is a hybrid model containing empirical pretzsch 2009 process based landsberg and sands 2011 and structural prusinkiewicz and lindenmayer 1990 modeling principles the core of the model is the empirical model which is also the crucial model for the forest trainer it is a spatially explicit distance dependent tree model the model requires input data on individual trees position diameter height crown parameters quality parameters if the data are not available a forest structure generator is used the given or generated forest structure is displayed with a 3d forest structure model from tree parameters and spatial structure the calculation model calculates all important outputs representing production biomass biodiversity revenues and costs the model is directly parameterized for 5 basic tree species common beech pedunculate or sessile oak norway spruce silver fir and scots pine in total it is possible to simulate 26 different tree species but some of them are derived by modifying growth processes of the basic tree species the simulation of forest development is performed with an interval of 1 year and uses mortality disturbance thinning competition and increment models as well as a model of forest regeneration the mortality model focuses on intrinsic and growth dependent mortality fabrika 2007 the disturbance model addresses externally induced mortality of trees which determines tree mortality caused by the influence of external disturbance factors it is based on risk modeling that consists of the probability model of hazard exposure and vulnerability it addresses tree mortality caused by wind snow ice bark beetles and timber borers defoliators wood destroying fungi air pollutants drought fire and illegal cutting fabrika and vaculčiak 2009 different types of intervention can be modelled using the thinning model from below from above neutral thinning the method of crop trees the method of target dimensions the method of target frequency distribution the geometric method and an interactive thinning method fabrika and ďurský 2005 the possibility of interactive thinning is the most important feature for integration with the thinning trainer the list of the trees removed trees and or crop trees represents the basis for thinning intervention the competition model is based on the crown light competition index kkl proposed by pretzsch 1995 the increment model simulates diameter and height increments of trees it is based on reducing their growth potential growth potential is defined on the basis of ecological site classification according to kahn 1994 ecological site classification is based on climate and soil characteristics length of vegetation period average temperature in vegetation period year temperature amplitude total precipitation during vegetation period amount of co2 and n2o in the air soil moisture and soil nutrient supply growth potential is modified with respect to competition pressure and vitality of the trees determined by the crown size the model is age independent if the tree age is unknown it is derived from the growth potential and the current height at the beginning of the growth period the model of forest regeneration generates trees of the new generation it is an ingrowth model merganič and fabrika 2011 composed from the model of the new generation individual s number the model of diameter and height distribution of the new generation and the model of locating the regeneration in the stand 2 2 virtual reality of the forest forest virtual reality was developed using vrml 97 language iso iec 14772 1 1997 it uses simple objects for the tree and stand visualization fabrika 2003 the visualization principle is shown in fig 1 a tree is divided into three parts stem crown and tree foot a stem and tree foot are displayed as overlapping cones cone nodes a crown is displayed using four planes indexedfaceset nodes which are rotated by 45 using the transform node and one horizontal octagon cutting the vertical planes bark texture of the particular tree species is applied using the imagetexture node as a parameter in the appearance node on the stem and tree foot the base of the cone of the tree foot is coated in wood texture representing a cross section which is important in the case of removed trees the illusion of the real crown is achieved by applying the texture of the crown profile to rotated vertical planes and the texture of the crown projection to the horizontal octagon the textures have transparent background png format was used equations for calculation of dimensions for the solids representing stem crown and tree foot are published in fabrika 2003 for visualization 26 prototypes of tree species were used spruce fir pine douglas fir larch beech oak hornbeam aspen birch cherry tree willow poplar rowan field maple sycamore maple norway maple elm acacia ash lime walnut tree plane tree alder chestnut and yew apart from healthy trees two prototypes of dead trees were also created coniferous and deciduous which are used for visualizing the results of the mortality model all prototypes were created by changing the textures of the stem the tree foot and the tree crown trees are placed on the digital terrain model defined using the patch model obtained from a regular grid individual trees are placed within this grid according to their cartesian coordinates the height coordinate is derived using bilinear interpolation based on horizontal tree coordinates and spatial coordinates of the corners of the specific square cell in the grid the terrain is created using the elevationgrid node and is covered with the stand s soil texture using the imagetexture node as a parameter in the appearance node the stand with the terrain is placed on a large plane covered with the same texture as the soil terrain the stand is located in the center of the cube the stand s surroundings are displayed on the inner sides of the cube background node cubic projection is applied the visibility and the light in the stand are defined using the fog node and the pointlight node respectively tree objects were supplemented by interaction possibilities one kind of interaction allows marking of the trees a green strip around the tree perimeter at a height of 1 3 m indicates a future crop tree and a red strip represents a tree marked for thinning this interaction is provided by a 10 cm high invisible hollow cylinder its gravity center is placed on the axis of the trunk at a height of 1 3 m with the cylinder s diameter equal to the tree s thickness at that height the cylinder is bounded with the event in the form of a script which changes its color from green to red to transparent each time the user clicks on object other interaction allows the cutting of the tree this is provided by an event bounded to the tree s foot by clicking on the tree s foot its position is changed from vertical to horizontal an additional click on the tree s foot returns the tree to its vertical position the virtual forest environment also contains other classes prototypes of objects a range pole a crystal ball and a plant the range pole is an object that serves as a link to a web location or a service opened in a web browser action is provided by the node anchor this node can be used to display information regarding for instance forest stand the crystal ball is represented by a stone base with a matte glass ball the ball is connected with the node anchor which allows the virtual forest scene to change by clicking the ball the new virtual forest stand specified by a url address is loaded the url address is associated with the node anchor this functionality offers teleportation from the actual stand to a past or future state the plant object is created by the node bilboard as a vertical plane placed in the terrain size of the plane width and height is equal to the size of the plant a plane covered by the plant s texture with a transparent background is automatically rotated towards the user in total 193 textures of plants were created to enrich the virtual forest stand s visual representation fig 2 shows the object structure of the virtual forest model 2 3 cave system at the technical university in zvolen the developed cave system is used to visualize natural and technological objects in the form of immersive and interactive virtual reality this system has a cubic shape missing one wall where the entrance is situated i e overall it consists of five walls a front a right a left a top and a bottom the dimensions of the sidewalls are 3 2 25 m and the dimensions of the top and bottom walls are 3 3 m the walls are projection panels on which the stereoscopic image is displayed using back projection from a pair of projectors an infitec stereoscopic system infitec 2017 is used the top and the bottom walls of the cave system have a square shape the discrepancy in the shape of the projected image with the shape of the bottom and top walls is solved by covering them with twice the number of projectors the use of the surrounding system space is optimized using projection mirrors the projection system projects onto five walls while two of them are divided into halves hence 14 projectors are necessary the bottom wall is reinforced with the safety glass to allow motion inside the cube the entire construction of the cube is raised therefore stairs are used to access its space the system s design is shown in fig 3 an observer uses 3d glasses that enable him to see the image in 3d the glasses are equipped with a device for detecting the spatial position of the polhemus type polhemus 2017 monitoring the position of the observer s head in the space and the direction of its view is used to recalculate and correct the projected images and the stereoscopic perspective of the observer movements in the virtual world are mediated either by a joystick or a space mouse in the observer s hands interaction with objects from the virtual world is allowed using another control activated by a fictional laser beam the beam s movement in virtual reality is mediated by moving a control therefore the beam is also a part of the system used for detecting the spatial position in the space of the cube a system of speakers 5 1 is placed to achieve a spatial perception of sound projectors are connected to a cluster of computers with visualization software the cluster is located in the adjacent room and is composed of eight computers seven of which render the image for the projection on the individual projection panels in the stereoscopic mode and one of which is a process control computer the computers contain powerful graphics cards that ensure smooth rendering mounted on the construction of the cave is the console computer which communicates with the control computer of the cluster and is used for managing the visualization system and receiving input from the user superengine control console software is installed on the console computer for the complex control of the computers in the cluster the software activates all of the hardware via the network and controls all necessary visualization applications the tool is versatile able to visualize different formats of virtual worlds however it was designed with regard to the visualization options of a virtual forest produced by the sibyla growth simulator thus the tool that was created enables the mediation of user s interventions into a virtual forest visualized with sophisticated equipment which enhances the experience from reality presented in this manner nevertheless the feedback based on the prognosis of further forest development is essential for prediction purposes the growth simulator needs not only the complex data on a forest at a simulation plot but also the information on performed interventions marked trees to ensure this functionality mutual integration is required of the growth simulator sibyla the virtual reality vr produced by the simulator and the visualization system cave with an adequate software solution 3 experiment mutual integration of the forest growth model sibyla along with cave type specialized hardware for visualization and interaction with the virtual forest comprises the thinning trainer that needs to be evaluated immersive virtual reality in cave must represent the real structure of the forest as much as possible to ensure that all the key characteristics of the forest structure from the field plot are present also in its virtual reality form virtual reality representativeness is also in close relation with the desired thinning method for users to be able to perform its realization in a way close to reality for these purposes we have done two experiments 3 1 evaluation of virtual reality representativeness evaluation of virtual reality representativeness needs establishment of a research plot fig 4 a a plot of 1 ha 100 100 m containing a mixture of tree species mainly beech spruce and fir is situated on a slightly sloped terrain individual tree parameters such as position of the tree in a local x y coordinate system diameter height and crown were measured using the field map system field map 2016 to capture the forest s real structure all data were transferred into the sibyla model s database to create a virtual reality representation of this research plot simultaneously hemispherical images right and left hemisphere were taken on 15 positions using a fisheye lens placed around the plot with the intent to represent its variability amount of the images was set statistically based on variability of forest stand structure diameters and heights of trees precision of the estimation to calculate the amount of images was set to 10 right and left hemisphere pairs of photos were processed into one spherical image which can be displayed as a movable and adjustable spherical panorama in any website browser in the form of a flash or javascript based application fig 4b positions where the spherical images were taken are marked by range poles in the plot s virtual reality in the first variant of the experiment these spherical images were shown one by one to users who were tasked with walking around the virtual forest and trying to find the corresponding structures marked by a range pole in the research plot s virtual reality displayed in the cave system fig 4c we examined whether the user was able to match the real forest structure seen in the spherical image with its virtual reality representation and optionally the number of attempts needed for successful identification to evaluate the success of identification we performed this experiment also in the field on the above mentioned research plot it was the second variant where the same assessors were asked to match the same spherical images with the structures of real forest on set of positions marked by range poles in the field number of assessors for both variants of the experiment was set to 10 persons based on initial pre estimation in the field which showed success of identification 1 3 attempt counting at 94 5 required precision was set to 15 and target confidence level at 95 3 2 thinning training realization for the thinning training experiment we created a simulation plot via the module generator in the sibyla model the plot is 0 25 ha 50 50 m generated as pure beech stand aged 80 years with stocking at 0 85 and a site index of 32 according to the slovak classification climatic and soil characteristics were established to correspond to the selected site index thinning was realized using the method of future crop trees the method is based on selecting high quality individuals i e supporting them by removing their competitors the intention was to maintain approximately 100 crop trees per hectare 25 trees per plot and to remove one of the biggest competitors per future crop tree simulation was done for 10 years of forest development after the thinning this experiment was realized by two independent people and automatically using the sibyla model as a reference 4 solution the technical solution of the thinning trainer is based on the mutual integration of software and hardware tools the applied software is the sibyla simulator which predicts forest development after thinning the specialized hardware used for moving in the virtual forest and manipulating trees is of the cave type finally there is a software link connecting the growth model with the virtual reality and the cave hardware sibyla vr cave system which is based on the caveman module 4 1 thinning trainer system of sibyla vr cave the sophisticated approach of forest visualization in the cave system tries to create a virtual representation that is sufficiently reliable to arouse a feeling in an observer that he is actually present in the visualized environment in such a stereoscopic and immersive form of a virtual forest users can obtain an authentic image of the forest structure from actual position and view angle inside the simulation plot they can move without any restrictions and they can obtain detailed information about individual trees and look into their crowns thus they can assess the trees mutual competition in the canopy in this way forest visualization in the cave system aims to provide the user with complex information about the forest and its individual trees through intensive and interactive visual perception all of the above mentioned features provide a user who performs the intervention in the virtual forest with the necessary information that is also available in the real forest the information is necessary when making decisions regarding which trees are to be left in the forest and which are to be removed with regard to the type of the thinning intervention and its objectives in the virtual forest environment a user can freely move using the assistance control devices joystick or space mouse the controller for implementing interactions can be used to obtain information on individual trees by pointing a virtual laser beam at a tree after which the data are displayed at the front wall of the cave system fig 5 a interventions in a forest are performed by marking individual trees fig 5b using the controller to implement interactions with this device the user activates a laser beam the user points the beam at a desired tree at a height of 1 3 m a spray icon appears the user either marks the tree as a target green or to be removed red or leaves the tree unmarked by repeated pressing of the button it is also possible to cut the tree in actual time if the laser beam points at the tree s foot a symbolic icon of a chainsaw is displayed by pressing the button the tree is cut fig 5c if the removed tree was not the correct one it can be returned to its original state in the same way as it was removed after implementing the intervention in a virtual forest the prognosis of its future development with regard to the performed selection starts for these purposes the growth simulator needs the following input data parameters and positions of individual trees their dimensions and the terrain configuration of the simulation plot climatic and soil characteristics of the site specifications for each tree s categories at the simulation plot target to be removed unmarked the acquisition and the transfer of these data are tasks for the caveman module which ensures the communication link between the sibyla model and the cave hardware tool caveman is a part of the modular structure of the growth simulator and is activated by the user in the cave system s main control software s menu called the superengine control console the module includes a set of operations necessary to start forest growth prognosis and calculation of results the principles of module operations are clarified in fig 6 the stand virtual reality in wrl format is the initial condition visualized in the cave system in this virtual forest a user performs interventions using the approach presented above in addition the stand and site information of the particular virtual forest is given in a tool called the stand diagnostic card which is in the form of a dynamic web site based on html language with javascript in html format both files are produced simultaneously by the growth simulator since they represent the same simulation plot in the given growth period their cross connection is indicated with the bond in the scheme the stand diagnostic card contains data about the whole stand such as production parameters ecological and economic indicators as well as climatic and soil characteristics for the simulation plot the first phase of the caveman module activity is the import of necessary data from the virtual reality file wrl of the visualized forest the module obtains the information on individual trees and the simulation plot the information includes the data on tree species sequential tree number tree position given in coordinates of x y on the plot age diameter height crown diameter height to crown base and assortment class and tree damage as well as data on the terrain s configuration and dimensions of the simulation plot from the html file of the stand diagnostic card the caveman module obtains data on climatic and soil conditions of the plot tree status after the virtual reality intervention is recorded in a file which is automatically created by cave s control software after the start up of the caveman module on the console the status of the tree corresponding to the performed intervention in the forest will be transferred from this file the information includes the sequential tree number and its status 0 unmarked tree 1 target tree and 1 tree to be removed at the end of this phase the user sets the length of the prognosis in a dialogue box within the module in the next phase data are transferred into the database structure of the sibyla growth model when the data import is finished the caveman module automatically activates the prophesier module of the sibyla growth simulator which prognosticates the growth per individual tree at the simulation plot during the prognosis the initial forest structure climatic and soil characteristics and performed interventions in the forest are taken into account after the successful completion of the prognosis the prophesier module is terminated from the predicted data representing individual trees production ecological and economic characteristics of the whole stand have to be calculated the caveman module ensures this by running the calculator module this module updates all relevant data for the given forest stand after the calculator module is terminated all necessary data are stored in the database of the growth simulator the data include the results of the growth prognosis that are linked to the initial forest structure the calculated stand characteristics and the data obtained during the data import which did not change after the simulation such data represent the simulation plot and its terrain configuration climatic and site data and marking of the target crop trees at the end the caveman module activates the generator of the virtual stand and the stand diagnostic card the generator creates a new virtual reality in wrl format representing the condition after the prognosis of forest growth the links on the crystal balls in the virtual environment for the user s time teleport to the past or the future and the link for the stand diagnostic card are updated in the forest virtual reality s file a new stand diagnostic card with updated data on the new stand structure is also created in the environment of the cave system the new virtual reality is presented to the user showing the forest condition after the intervention with changed parameters of individual trees updated stand data in the stand diagnostic card can be displayed on the control console this information is important for evaluating the intervention on the basis of stand characteristic changes by clicking on the crystal balls in the stand s virtual reality time teleport is possible between the growth periods at the simulation plot this serves to compare initial stand condition with condition after prognosis via changes in virtual representation of stand structure and by changes in important stand characteristics in the stand diagnostic card at the same time in the new virtual reality a user can again perform interventions in the forest and start the growth prognosis thus the whole cycle of operations of the caveman module is repeated as shown in fig 6 by loops a set of virtual reality cards and stand diagnostic cards this results in a fully automated process with minimum need of user assistance the thinning trainer can be handled by only 1 operator user it is sufficient for the user after implementing an intervention to start the caveman module on the console of the cave system and set the prognosis length the module takes care of everything else time needed for simulation process for standard size of research plot 50 50 m and for 10 years of prognosis varies from 30 s up to 2 min depending on structure of the forest mainly number of trees on the plot scene performance was tested on sample of 20 000 trees what is approximately 50 ha of mature stand the image rendering was smooth and without any issues or freezing upon movement sample area or number of the trees greatly exceeds the needs of thinning trainer 5 results 5 1 virtual reality representativeness in the first variant of the experiment the assessors were asked to match real forest structure captured on 15 spherical images with its virtual reality representation people who underwent the experiment had three attempts to match the structure seen on the spherical image with its virtual reality representation in the cave environment otherwise the identification was considered unsuccessful second variant of the experiment was intended to match the same set of spherical images with the real forest structures from which those images were taken both variants of experiment were performed by the same assessors table 1 shows relative and absolute numbers of success in identification for both variants and per attempts needed 5 2 thinning training experiment the usability of the cave based thinning trainer as an alternative to field thinning training methods was verified by the thinning experiment a defined type of thinning was performed by two independent persons and the sibyla model itself as a reference after thinning we applied growth simulation for 10 years in the future to see the changes in stand parameters tables 2 4 show the results describing stand structure before and after the thinning was performed results show that even starting from the same structure total stand the desired thinning method can be applied in multiple ways by comparing the sibyla model with the both assessors we can conclude that there is a difference in the preferences of tree selection the sibyla model has selected comparable number of trees from quality classes a and b as crop trees while the assessors varied more in total number of crop trees and their preferred crop trees were mainly of a quality class overall results after prognosis of both assessors and sibyla are comparable and well representing the desired thinning method 6 discussion realization of thinning is a notably difficult task and requires considerable skill to perform with respect to the desired forest functions our intention was to build the thinning trainer which will help to understand and teach users to perform thinning correctly it was necessary to build an environment that offers reality like experience but this experience must be shareable between multiple users in the case of virtual reality the aspect of mutual interaction is often important cruz neira et al 1992 this interaction can only be achieved with a tool that allows participation of more than one user in the same environment this feature is definitely important also for the thinning trainer in the real forest stand several people can discuss which trees to mark and this possibility should be preserved also in the virtual stand hmd devices are unable to meet this need while the cave devices are adapted to it in the case of hmd technology the problem is that it is participative only in one way it s basically the one way shared virtual reality people around the user of hmd can see exactly what the hmd user sees on the display of the computer images for both eyes but the feedback in form of nonverbal communication is not possible for example the mentor trainer which trains the student trainee with hmd cannot point on tree its part or position in the space of vr to which the student should get the attention because the student with hmd cannot see him but can only hear his advices verbal navigation in the space is very time consuming and often confusing improvement could be in mutually shared virtual reality in both ways in form of multiplayer where the multiple users use hmd and each of them is represented via avatar thus is able to communicate verbally and non verbally we do not have such solution yet but its development is planned for hmd version of the thinning trainer we think that cave is more suitable to be participative for these purposes and from our experience the cooperation of multiple persons in the cave was performed without any major issues it is true that user which wears tracked 3d glasses is the only one that gets the non deformed image however the image deformation for the other users is not crucial because they are moving in limited space 3 3 m so relatively close to tracked user image deformation can be seen on edges of the screens for example trees may look cranked instead of straight edge of front and upper wall perspective within individual walls is correct image is seen with correct depth and 3d experience other users also wear 3d glasses in this case it is simple to point on desired tree its part or position in the space it is not only verbal but also non verbal communication between the users which is very important for the mentor in the cave other users can see not only space that tracked user can see but also the surrounding area this is when the mentor can draw attention to situation which is not in field of view of tracked user hmd systems are strictly limited by the user s field of view for example oculus rift 110 htc vive 110 lg valve 110 samsung gear vr 101 playstation vr 100 starvr ima 210 google cardboard 90 microsoft hololens 90 110 virtualrealitytimes 2017 on the other hand hmd based technologies are mobile devices and also way more affordable because of their low cost that is still decreasing with the progress in technology they can depend on external sensors oculus 2017 placed around the user what can be partially limiting or the sensors which are a part of the device samsung 2017 lg 2017 what makes them even more mobile quality of the visualized image in hmd can be different if they use one display per eye oculus 2017 with high resolution or screen of mobile phone used as display for both eyes samsung 2017 what reduces the resolution performance of computer powered hmd is also way greater than the possibilities of mobile phone used in vr headset considering all the pros and cons of the above mentioned technologies we decided to build a forest trainer based on the cave technology mainly because of its cooperation mode ability to share the virtual reality of the forest in both ways with the more persons at the same time and place the thinning trainer based on the individual tree growth model and sophisticated visualization system helps the users to achieve an experience similar to that found in the real forest we tried to build it in as realistic a way as possible to represent the forest structure i e to prepare a virtual environment containing all components with bonds between them to be reliable for the users from our observations the level of virtual world fidelity is the key factor for performing the actions which depend on visual interpretation of real system for this reason we carried out the first experiment on virtual reality representativeness the experiment showed that even if the virtual reality of the forest were built from simple geometrical objects covered with appropriate textures forming a virtual forest it could in majority of attempts be matched with the structure shown on spherical images of a real forest assessors who participated in our experiment on virtual reality representativeness matched vr representation of the forest seen in the cave with the forest structures on spherical images variant 1 of the experiment mostly on the 1st or 2nd attempt situation was similar in the second variant where the same assessors tried to match the same set of spherical images but with the real forest structures from which those images were taken comparing both variants we can see that in second variant the percentage of successful identification was higher for 1st and 2nd attempt in the first variant there is higher success in identification on 3rd attempt on the other hand unsuccessful identification percentage was low as shown in the results for both of variants but higher for the first variant results showed that assessors were more successful in matching of the forest structure seen on spherical images with the real forest structure but the difference in percentage of successful identification compared to vr in the cave is small we can conclude that forest virtual reality of the sibyla model in the cave system sufficiently describes i e represents the real forest structure thanks to this representation users can work not only with randomly generated structures of forests but also with captured and visualized real ones commonly performed thinning training experiments such as the marteloscope method poore 2013 are usually based on permanent research plots established in the field where the knowledge and skills are gained by practical marking and discussion with experts achieved experience is definitely valuable the users select trees according to the plan and are confronted with the change of forest structure parameters but they do not have any feedback on what will occur with the forest in the future after the intervention the thinning trainer we built can overcome several disadvantages of field based experiments using such techniques as field gis technologies and terrestrial laser scanning we are able to capture the real forest structure i e structure of permanent research plots and manipulate it as we would the real one therefore the field thinning training experiments can also be performed in the cave system but furthermore they are enriched for simulation process users can apply growth prognosis after thinning automatically via the sibyla model and are able see the estimation of what will occur with the forest in the future thinning training via our system is not necessarily linked to permanent plots in the field because it is possible to create any kind of the forest structure via the sibyla model and perform thinning at any time in any weather conditions this is the case in our second experiment on thinning training the results of this experiment showed that both assessors were able to implement specified thinning concepts and stand characteristics after simulation were similar to reference thinning made by the sibyla model this proves that users are able to perform a thinning concept because they are provided with all the necessary elements to make a decision on which trees have to be selected as future crop trees and which will be removed users can test themselves on whether the thinning they just realized filled the desired goals on future forest structures or not if something went wrong they can return to the initial state and try again the thinning trainer solution also has some disadvantages in the visualization schematization and the high cost of building the cave device the trainer can be further developed and extended enhancing a virtual forest can be achieved by improving the visualization of trees using the refined tree architecture at a level of branches and leaves this enhancement can be ensured by creating a more comprehensive topology of tree organs vrml language and model architecture of forest visualization is prepared to do that because of the hmd mobility advantage we are currently developing the thinning trainer application for hmd based devices what will allow the application to be used at office home etc to train either individually without trainer or with trainer as avatar in shared vr from what has previously been discussed it is clear that the solution of the forest trainer is not only innovative but also open which allows its further development and the extension of new properties and applications 7 conclusions modern methods of simulating developments in the real world and its visualization using immersive systems of virtual reality represent a prospective direction of scientific development and the popularization of scientific results to the public many scientific and practical disciplines are equipped with trainers for training conducted activities the realization of thinning is an area of practical and scientific activity in forestry which deserves attention in the field of trainer development such trainers should be linked to an efficient simulation tool of forest development should contain forest visualization using virtual reality and should use hardware for the immersive visualization of virtual reality despite a great number of applications for immersive visualization from different areas of science research education and practice we could not find any references for an existing forest thinning trainer in available publications and internet sources that would meet our requirements the design solution of our cave system is an innovative device in european conditions regardless of its usage from the perspective of its application as a thinning trainer and the technical link of individual components the system is unique at a global scale we proposed a solution that connects a forest growth model with a forest visualization solution and modern hardware of the cave type it is a fully automated system with frugal maintenance in addition it has an open architecture which allows the improvement of individual components and the addition of new functionalities it is a practical tool that can support decision making in forests and landscape and forestry education our solution can become the basis for similar devices applied in forestry and ecology acknowledgments the latest sub models of the sibyla growth simulator which is a part of the proposed thinning trainer were developed within the scientific project vega 1 0618 12 titled modeling of forest growth processes at high resolution level the link of the cave device used to display the immersive virtual reality to the growth simulator was carried out within the framework of the slovak research and development agency under the project no apvv 0069 12 titled new technology of nature management newton the cave device was constructed within the project opvav 2009 5 1 03 soro of the operational program for research and development from the european regional development fund experiments for this study were supported by project no apvv 15 0265 titled modeling tree species growth in the carpathian forest ecosystems under different climate change scenarios 
