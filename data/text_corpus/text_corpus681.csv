index,text
3405,crowdsourced personal weather stations pwss adoption has been growing rapidly and provides the potential to fill in hyper local rainfall observation gaps however current adoption patterns exhibit spatial biases that must be understood when using the data for modeling and decision making here we first examine the pws rainfall spatial representation at huc 12 watersheds in twelve metropolitan areas in the u s furthermore by modeling the pws adoption using socio economic and flood related data at census tract level the results suggest current adoption patterns exhibit spatial biases toward wealthier neighborhoods and flood prone regions the findings provide insights to inform how policies could be made to distribute resources to improve the rainfall data collection efforts in pws underrepresented regions as crowdsourced data are increasingly used for decision making by policymakers efforts to close the gap in current non uniform pws spatial adoption will allow crowdsourced rainfall data to be better positioned to support decision makers in their flood resilience efforts keywords crowdsourcing rainfall flooding spatial analysis logistic regression flood resilience 1 introduction flooding causes significant social and economic damage and loss in the united states each year the national academies of science engineering 2019 hurricane harvey is an extreme example of bringing unprecedented rainfall across the city of houston texas leading to catastrophic flooding that impacted more than 100 000 homes and incurred an estimated damage cost of 125 billion van oldenborgh et al 2017 with the increase in frequency and intensity of heavy rainfall due to climate change the impact of flooding is projected to become more severe over time cheng and aghakouchak 2014 fowler et al 2021 in urban watersheds with large portions of impervious surfaces and low lying areas rainfall driven flooding has been causing even more considerable damage affecting greater numbers of people rosenzweig et al 2018 therefore there is a need for improved flood assessments as well as building flood resilience in urban areas to adapt to the increasing severity of flood hazards bertilsson et al 2019 flood assessments at high spatial and temporal resolution remain a challenge an ideal flood assessment will require a spatially and temporally representative dataset as flood modeling involves complex and nonlinear flow and physical processes hu et al 2019 traditionally flood assessments have mainly been efforts by government agencies local government agencies are responsible for identifying flood risks and creating flood forecasting models tyler et al 2019 however flood related data available from government agencies needed for enabling high resolution flood modeling at flood prone areas are usually insufficient or lacking cristiano et al 2017 furthermore increasing attention has been focused on incorporating the public and a wider representation of stakeholders in the decision making process of flood assessment and management white et al 2010 lack of engagement from communities for locally specific flood knowledge will lead to inadequate flood model calibration and validation gebremedhin et al 2020 in such cases flood forecasting models generated by government agencies may not be fully accepted and supported by local stakeholders as they may not reflect adequate representations of floods in certain regions gebremedhin et al 2020 rosenzweig et al 2018 sy et al 2019 lack of engagement of local stakeholders may also worsen the disparities of social economic and environmental resources across communities white et al 2010 crowdsourced data can provide a potential solution crowdsourcing is a method of collecting information from as many involved contributors from the general public muller et al 2015 which offers a way of obtaining large amounts of flooding related data cost efficiently assumpção et al 2018 with the development of inexpensive sensors and communication technology the amount of data collected by the public has been growing rapidly generating a wealth of information muller et al 2015 for example social media such as twitter data produces geotagged information that can be extracted to map the potential extent of flood events wang et al 2018 mobile apps such as google waze enable users to report floods in a convenient and efficient way which greatly supports government efforts to identify the location of floods praharaj et al 2021 in the case of rainfall monitoring the adoption of personal weather stations pwss by the general public fills in the rainfall observation gap in locations where government agency data are unavailable de vos et al 2017 pwss are off the shelf weather stations installed and maintained by individuals and the reported data can be easily shared through websites such as weather underground gharesifard and wehn 2016 in recent years crowdsourced pws adoption has been growing rapidly to supplement agency maintained rain gauges which are usually limited in coverage de vos et al 2017 as well as remote sensing rainfall e g satellite and radar which requires validation from ground rain gauges muller et al 2015 for example in houston texas pws adoption density has grown from 0 06 to 0 24 pws per km2 from 2016 to 2019 such exponential growth suggests that in populated areas in the u s pwss could alone provide sufficient spatial resolutions for rainfall observations needed for urban hydrology in a few years berne et al 2004 chen et al 2021 furthermore by participating in data collection via crowdsourcing the public may also become more proactive in engaging in local decision making processes buytaert et al 2016 in fact the motivation behind adopting pwss and sharing the collected data often starts with owners using the data for personal purposes simultaneously growth in pws adoption has the potential to benefit society at large since the collected data could implicitly create crucial knowledge for local communities gharesifard and wehn 2016 therefore as local stakeholders e g pws owners gain better understanding of the their environments with time empowering them to grow from collecting data toward developing models and decision support systems becomes key almoradie et al 2015 voinov et al 2016 the ongoing trend of increasing crowdsourcing participation e g rainfall data collection by pws adoption from the general public in local communities therefore lays the foundation for generating collective knowledge to support flood resilience efforts paul et al 2018 despite the rapid growth of pws adoption for generating a wealth of rainfall observations current adoption patterns exhibit spatial biases caused by underrepresentation or overrepresentation of certain regions muller et al 2015 using non representative and spatially biased datasets as model inputs could lead to biased modeling results and decision making towe et al 2020 furthermore recent advancements of data driven techniques has enabled modeling flood at high spatial and temporal resolution mosavi et al 2018 sadler et al 2018 shen et al 2019 zahura et al 2020 which has also highlighted the importance of using representative datasets as large datasets are not always comprehensive torralba and efros 2011 therefore it is important to ensure that crowdsourced rainfall datasets are analyzed for spatial and temporal representation and variability and if biases exist such biases are clearly understood and accounted for when using the crowdsourced data in modeling and decision making spatial bias in crowdsourced data has been studied across a variety of fields for example social media usage is mainly concentrated in populous areas leading to certain groups not receiving needed assistance during disasters fan et al 2020 mobile phone applications have the potential to enhance resilience building against disasters but the lack of user centered design often results in low uptakes in marginalized and vulnerable groups who are more vulnerable in disaster situations craig et al 2019 paul et al 2021 crowdsourced bicycle ridership data are biased toward recreational riders who track exercise activity which requires a correction for better representation of the ridership patterns of all riders such as commuter cyclists roy et al 2019 biodiversity data collected from crowdsourcing exhibited spatial biases toward accessible areas or recreational summer homes millar et al 2019 pws adoption and other crowdsourced climate and atmospheric data have spatial biases toward populous areas muller et al 2015 in previous studies regression models using socio economic and demographic factors have been widely used for assessing the factors that affect the spatial bias pattern examples include exploring the effect of socio economic factors and environmental attitudes on rain barrel spatial adoption patterns ando and freitas 2011 the presence of hazardous waste sites affecting life expectancy kiaghadi et al 2021 and spatial models to predict electric vehicle ownership choice behavior chen et al 2015 however limited studies have been focused on the fast growing crowdsourced pws network to provide high resolution rainfall observations and engage communities in supporting flood resilience therefore the research questions that guide this study are 1 are pws providing a spatially representative sample of rainfall data 2 what are the underlying factors that affect the spatial pattern of pws adoption in this study we used a unique pws adoption dataset obtained from weather underground which is one of the largest crowdsourced pws platforms this dataset consists of the location of more than 100 000 pws in the contiguous u s pws rainfall representation in united states geological survey usgs level 12 hydrological unit codes u s geological survey 2021 huc 12 watersheds in 12 selected metropolitan areas were analyzed to quantify to what extent current pws adoption can contribute to rainfall observations we further applied logistic regression models using socio economic and flood related data to identify the factors that influence the spatial bias in pws adoption moreover we measured and analyzed the marginal effects of resulting models to quantify the pws adoption disparities across neighborhoods 2 materials and methods 2 1 pws adoption data acquisition pws adoption dataset used in this study was obtained from weather underground database using their previous version of application programming interface api the analysis dataset contains metadata including the id and the geographic location latitude and longitude of more than 100 000 pwss the geographical location of these pwss were then mapped using arcgis to explore the pws adoption spatial pattern in the contiguous u s note that at the end of 2019 this version of the api has been retired the new version of the api introduced in 2020 requires a weather underground key which can be obtained through connecting a pws to the platform furthermore the number of api calls per day in order to download the data was greatly limited and some of the metadata including pws installation date were no longer available therefore to preserve such details in the dataset this study focused on the pws adoption dataset gathered on april 2019 2 2 pws rainfall representation calculation pws rainfall representation was evaluated based on the pws density in an urban watershed within a metropolitan area ma in this study an urban watershed is defined as huc 12 watersheds that intersected with united states census bureau uscb delineated urban areas within the ma boundary u s census bureau 2021 to compute the pws density in an urban watershed of each ma we used the point density tool in arcgis version 10 6 to convert pws location point data to gridded raster which represents the pws density followed by using the zonal statistics tool to compute the average pws point density in a watershed see fig 1 for the workflow like traditional rain gauges rainfall data recorded from pwss are point observations which can only be representative of the rainfall of a specific area due to the spatial variability of rainfall events cristiano et al 2017 to have a sufficient representation of rainfall spatial variability in terms of rain gauge density in a monitoring network the world meteorological organization recommends at least one rain gauge per 10 20 km2 for urban areas world meteorological organization wmo 2008 therefore we used an average pws point density of 0 1 pws per km2 1 pws per 10 km2 as the threshold to assess pws rainfall representation an underrepresented ur watershed is defined as having a point density lower than the threshold while a non ur watershed is defined as having a point density above the threshold using an one way anova test we further extracted the population density estimates from the worldpop population dataset tatem 2017 to test if a significant mean difference in mean population density exists between ur and non ur watersheds 2 3 pws adoption logistic regression model we used logistic regression to assess the association between the pws adoption and selected socio economic data the built in glm package in r programming language version 3 6 1 was applied to run the regression models census tract ct level socio economic data were obtained from the uscb s 2017 american community survey 5 year estimate including population median household income household density and owner occupied household ratio the number of owner occupied households divided by the total number of households ct was selected as the common geographic unit for analysis because it is the smallest geographic boundary used by the census bureau to build the logistic regression model we aggregated the pws location information into total counts of pws adopted in each ct then classified these counts into binary groups pws adoption 1 for cts that have at least 1 pws 1 pws and pws adoption 0 for cts that have no pws adoption the logistic regression equation is shown as follows 1 p p w s a d o p t i o n e β 0 i 1 n β i q i i n 1 m β i x i 1 e β 0 i 1 n β i q i i n 1 m β i x i where p is the probability of 1 pws adoption in a ct β 0 is the intercept β 1 β 2 β n 1 are the coefficients for the categorical variable q i i 1 2 n 1 and q i are the dummy variables for each category with the value of either 0 or 1 β n β n 1 β m are the coefficients for the continuous variables x i the regression coefficients are estimated by maximum likelihood in the explanatory variables to account for household income differences across mas the median household income mhi was converted into categorical variables using quartile groups m h i q 1 m h i q 2 m h i q 3 and m h i q 4 in each analyzed ma a ct falling within first quartile of ma mhi was assigned to m h i q 1 a ct falling between first and second quartile assigned to m h i q 2 a ct falling between second and third quartile was assigned to m h i q 3 a ct falling above the fourth quartile was assigned to m h i q 4 population pop household density hhd and owner occupied household ratio oohr are used as continuous variables for the logistic regression model 2 4 flood vulnerability and pws adoption two types of flood related datasets at the ct level i the total number of flood claims fc and ii percent housing units in the 100 year flood zone fz were used to assess the association between potential flood risk and pws adoption fc data were obtained from national flood insurance program nfip redacted claims which is a large database containing more than two million claims transactions since the nfip launched federal emergency management agency 2021 fz data were obtained from a dataset published by nyu furman center this dataset was created by combining housing and population data with fema floodplain maps to calculate the percent of housing units intersecting with a fema 100 year floodplain nyu furman center 2021 in this study we used the fc and fz data to represent the flood vulnerability of a ct we assumed that a ct with a higher number of flood insurance claims is more likely to have a higher flood risk and a ct with a higher percentage of housing units in the floodplain also implies that this ct is more vulnerable to flooding for each analyzed ma we classified cts into high low fc groups based on the fc value with the low fc group having fcs below the median value of the cts within the ma representing the lower flood risk cts and high fc group having fcs above the median values representing the higher flood risk cts similarly the fz data were used to classify cts into in fz and not in fz groups with in fz group representing cts that have any percent housing units in the floodplain and not in fz group representing cts that have zero percent in the floodplain 3 results 3 1 pws adoption in the contiguous u s the spatial pattern of pws shows that adoption is concentrated in the metropolitan areas in the east and west coast of the contiguous u s fig 2 a unlike the agency operated rainfall network where rainfall stations are usually uniformly distributed pws is spatially biased toward populous areas at the metropolitan area ma level pws adoption is also highly correlated with population fig 2b with a correlation coefficient of 0 88 for all mas in the contiguous u s to assess the pws representation in the mas in the contiguous u s the top 12 mas by pws adoption fig 2b were selected in this study the analyzed mas are distributed across the contiguous u s with the number of pws adoption ranging from 1 300 to 2 569 units per ma and pws density ranging from 0 03 to 0 27 pws per km2 3 2 pws rainfall representation in huc 12 watersheds in this study watersheds with pws point density 0 1 are considered underrepresented ur otherwise they are considered well represented non ur the results of the pws representation calculation showed that the current pws rainfall representation exhibits three characteristics table 1 first at the ma level pws rainfall representation in huc 12 watersheds varied across the analyzed mas the average pws point density ranges from 0 14 in atlanta to 0 59 in san francisco most of the analyzed mas besides chicago have a maximum pws point density above 0 40 indicating that pws has the potential to contribute to the coverage of rainfall observations for at least every 2 5 km2 in those watersheds among the analyzed mas san francisco has the highest maximum pws point density of 1 34 pws per km2 which could provide a considerable rainfall representation that is even greater than the current resolution of radar derived rainfall e g the next generation weather radar nexrad typically has 1 km by 1 km resolution second though the average pws point density for analyzed 12 mas is well represented at the ma level large disparities occurred at the huc 12 watershed level when uneven representation begins to appear for example in seattle and denver pws adoptions are more uniformly distributed where no ur watersheds were observed however the percentage of ur watersheds is larger in mas such as atlanta 40 and houston 30 third the mapping of the pws representation shows that the pws representation has certain spatial distribution patterns which merit further analysis as shown in fig 2 ur watersheds are generally clustered in a specific region of a ma for example in houston ur watersheds are concentrated on the east portion of the ma where pws adoption in these watersheds is relatively low in chicago and atlanta large portions of ur watersheds are concentrated in the southern part of the ma we hypothesized that population density is explanatory for the pws adoption spatial disparities to test this hypothesis population density estimates for each huc 12 watershed were computed using the worldpop dataset tatem 2017 as shown in table 1 the average population density in non ur watersheds was higher than in ur watersheds however the result of the one way anova test showed that the mean difference of population density in ur and non ur watersheds in most mas was not statistically significant although there is a notable pws adoption difference in ur and non ur watersheds population density did not fully explain the spatial bias in pws adoption as can be seen in the scatterplots in fig 3 most ur watersheds shown in red dots have similar levels of population density compared to non ur watersheds shown in blue dots while lacking pws representation therefore we implemented further analysis at a finer geographic scale census tract to assess other factors affecting the spatial pattern of pws adoption 3 3 pws adoption logistic regression model logistic regression models were built for the 12 analyzed mas 12 regression models to assess the factors affecting the spatial pattern of pws adoption table 2 shows the summary statistics of the response variable pws adoption and the selected socio economic explanatory variables at the census tract ct level multicollinearity diagnostics performed using the variance inflation factor vif indicated that the selected input variables do not pose a concern of collinearity since all the vifs are below 3 0 midi et al 2013 using the pws point density threshold of 0 1 per km2 in the previous section we assumed that the presence of 1 pws could provide the minimum rainfall representation for a ct since the area of a ct in the analyzed mas is mostly below 10 km2 therefore the cts were classified into binary groups of 1 pws adoption and no pws as the response variables in the logistic regression models the summary statistics table 2 showed that the percentage of cts with 1 pws adoption ranged from 21 new york to 77 seattle and is generally lower in mas with a larger number of cts such as new york los angeles and chicago notably the comparison of cts in no pws and 1 pws groups table 2 across mas shows that the median household income mhi and owner occupied household ratio oohr are significantly higher in the cts with 1 pws while household density hhd is mostly higher in cts with no pws the mean population in the two groups was similar since cts by definition are delineated based on the population 3 the exponentiated coefficients odds ratio of the logistic regression models are shown in table 3 among the input variables to the models mhi pop and oohr have positive effects on pws adoption odds ratio greater than 1 while hhd has negative effects odds ratio less than 1 which indicates negative effects on pws adoption the mhi variables were converted into categorical variables using quartile groups based on the coefficients of the mhi for the 12 analyzed mas the odds ratio of the fourth mhi quartile group mhi q4 was the highest followed by the third mhi q3 and second mhi q4 mhi quartile group notably in mas such as dallas and houston the odds ratio of the third and fourth mhi quartile was significantly greater than first mhi quartile group mhi q1 which indicated that pws adoptions are much more likely to occur in wealthier neighborhoods the coefficients of population variables were significant for every analyzed ma in predicting pws adoption suggesting that an increase of 1 000 in population could lead to an increase of 9 6 houston 48 5 boston of the odds ratio that a ct has 1 pws adoption the coefficients of oohr showed that pws adoption is more likely to occur in cts with a larger percentage of owner occupied households suggesting that an increase of 1 in oohr in a ct could lead to an increase of 3 11 of the 1 pws adoption odds ratio however pws adoption is less likely in densely populated cts for an increase of 1000 households per km2 in hhd in a ct the odds ratio of pws adoption could be decreased by 3 5 to 50 5 across analyzed mas 3 4 the effects of median household income on pws adoption marginal effects of mhi were calculated to provide an intuitive comparison of mhi effects on pws adoption across analyzed mas a marginal effect me is defined as the change in the response variable associated with a change in one explanatory variable while holding other variables at a specific value in the pws adoption logistic regression models the mes of mhi demonstrate the discrete change in predicted probability of pws adoption from the reference category mhi q1 to other categories mhi q2 mhi q3 and mhi q4 keeping pop hhd and oohr at their mean values the mes of mhi for the analyzed mas are shown in fig 4 a based on the results of the me common patterns were observed for every analyzed ma the predicted probability of pws adoption is always the lowest in the reference category mhi q1 followed by the second mhi q2 third mhi q3 the predicted probability of pws adoption of the mhi q4 is consistently the highest this indicates that in a hypothetical ct with identical pop hhd and oohr the probability of pws adoption is greater in upper mhi quartile categories for example for houston the me for the mhi q1 category is 19 while in mhi q2 mhi q3 and mhi q4 the mes are 34 58 and 81 respectively despite the common pattern that predicted pws adoption is generally higher in upper mhi quartile groups the level of pws adoption disparities due to mhi varied significantly across analyzed mas as shown in fig 3a seattle generally has a higher probability greater than 66 of pws adoption regardless of the mhi quartile group however the adoption probability varied largely in mas such as houston mhi q1 19 mhi q4 81 and chicago mhi q1 7 and mhi q4 45 the ratio of the average mes of the upper mhi quartile group mhi q2 mhi q3 and mhi q4 to the mhi q1 was computed to quantify the level of disparities as shown in fig 4b in mas such as seattle and denver the level of disparities between mhi quartile group are lower ratio to q1 were 1 3 and 1 5 respectively while in mas such as houston and chicago the level of disparities is much higher ratio to q1 were 3 3 and 5 4 respectively the pws adoption pattern versus mhi quartiles of these example mas are shown in fig 5 3 5 the effects of potential flood risk on pws adoption in this study we further theorized that flood risk may influence pws adoption therefore two types of flood related dataset at the ct level were used to assess the association between potential flood risk and pws adoption i total number of flood claim fc and ii percent housing units in the 100 year flood zone fz a comparison of fz and fc data showed that cts with a lower number of flood claims low fc group are associated with a lower percentage of cts that are in the in fz group while cts with a higher number of flood claims high fc group are associated with a higher percentage of cts that are in the in fz group as can be seen in fig 6 this relationship is consistent for the analyzed mas which indicates that cts that have any percent of occupied housing units in the fema 100 year flood plain are more likely to have a greater number of flood claims for example in new york only 4 6 of the cts in the low fc group are in the in fz group while 59 3 of the cts in the high fc group are also in the in fz group next we look at the relationship between pws adoption and the two flood related variables to investigate whether the vulnerability to flood affects the likelihood of pws adoption logistic regression models were built for the 12 analyzed mas using socio economic explanatory variables along with fc and fz data to assess the potential flood risk affecting the spatial pattern of pws adoption note that these models were built separately from the models in previous section since the fz data have larger percentage of cts with missing data the in analyzed mas 100 27 18 and 17 missing in seattle san francisco washington dc and chicago respectively in addition since the correlation exists between fc and fz variable two separated regression models one adding fc data and another one adding fz data were built the coefficients for the models adding fc and fz data are shown in table 4 and table 5 respectively compared to the reference group of low fc high fc groups for most of the analyzed mas have odds ratio greater than 1 0 indicating positive effects on the pws adoption similarly compared to the reference group of not in fz group the odds ratios for in fz group are greater than 1 0 which also suggesting positive effect on pws adoption we further used marginal effects me to demonstrate the effects of flood vulnerability on pws adoption using the me adjusted for the second quartile mhi group as an example as shown in fig 7 a consistent relationship was found that the mes of the predicted probability of pws adoption in high fc groups were higher than the mes in the low fc groups fig 7a which means that cts that have higher number of flood claims will be more likely to have pws adoption similarly the mes of the fz variable also suggested that cts that intersected the 100 year floodplain have higher probability of pws adoption than those not in the floodplain fig 7b these findings suggest that with the assumption that the number of flood claims and the percentage of households in the 100 year floodplain represent the flood vulnerability of a ct current pws adoption pattern is spatially biased toward flood prone regions 4 discussion in this study we analyzed a large pws adoption dataset to explore the rainfall representation of crowdsourced pws in metropolitan areas mas in the contiguous u s consistent with previous literature pws adoptions are generally concentrated in populous ma muller et al 2015 however our analysis showed that pws adoption exhibited significant spatial biases which may result in overrepresentation and underrepresentation of crowdsourced rainfall observations across huc 12 watersheds within a ma in the 12 analyzed mas the results of logistic regression models revealed that current pws rainfall representation is biased toward wealthier neighborhoods and higher flood risk regions due to adoption disparities potential causes of these biases could be because wealthier families have more resources and leisure to participate in pws adoption since purchasing and maintaining pwss are often regarded as a hobby rather than a necessity in the household gharesifard and wehn 2016 on the other hand continuous urban development might be driving the population growth in the floodplains which potentially explains the higher possibility of pws adoption in floodplains due to increased population qiang 2019 in addition areas with higher number of flood claims are usually associated with homeownership and higher value homes kousky and michel kerjan 2017 which is the proportion of demographics that are more likely to adopt pws however these are speculations based on the correlation and regression model results future studies are needed to test and better understand causality for these correlations policies could be made to direct the distribution of resources of rainfall data collection efforts in pws underrepresented regions unlike the non uniform pws spatial adoption traditional rainfall monitoring networks are often designed as uniformly deployed rainfall stations across the watersheds for example harris county flood control district hcfcd maintains a large rainfall network consisted of 175 rain gauges uniformly distributed across harris county as shown in fig 8 the point density of pws and hcfcd comparison shows that in huc 12 watersheds the point density of hcfcd maintained network ranges from 0 01 to 0 07 whereas the point density of pws widely ranges from 0 01 to 0 48 this observation suggests that to make the best use of the limited public resources city managers and engineers should consider the spatial pattern of pws adoption when designing future rainfall monitoring networks rather than evenly deploying rain gauges relocating resources to set up more rain gauges in pws underrepresented regions would support the current non uniform pws adoption moreover local agencies and organizations could leverage their resources to conduct science technology engineering and mathematics stem related programs and workshop that incentivize the communities in pws underrepresented regions to participate in data driven decision making processes and thus increasing their representation in crowdsourced rainfall data mondschein et al 2019 incentivizing pws adoptions to increase participation in crowdsourced data collection could strengthen the awareness of stakeholders for their local environment in the pws underrepresented regions past examples include the use of volunteered stream monitoring to increase the community s awareness for protecting environmental resources overdevest et al 2004 the application of participative geographic information approaches could strengthen work relations among local actors and authorities to prevent river flooding usón et al 2016 crowdsourced rainfall data collection in the community collaborative rain hail and snow network cocorahs were shown to have educational benefits that improve the climate literacy of the participants reges et al 2016 additionally the increased participation in pws adoption could improve the usefulness of the crowdsourced rainfall data unlike traditional rainfall networks that are maintained and collected by experts with rigorous quality control procedures the utility of crowdsourced pws rainfall network is often compromised by its data quality because of limited quality control processes and lack of trust for data contributed from non experts muller et al 2015 as pws quality control and trustworthiness assessments methods often require a sufficient number of neighboring pwss for optimal performance chen et al 2021 de vos et al 2019 increased number of pws adoption in the pws underrepresented areas can therefore help ensure the quality and trustworthiness of the crowdsourced data and thus improve the usefulness of the crowdsourced rainfall data future work could also focus on the design of crowdsourcing incentivizing programs for pws adoption by coupling the regression model results with agent based modeling to better identify the behavior of crowdsourcing participants and therefore increase adoption yang et al 2019 our research merits further exploration in terms of methodology and underlying datasets which limited the potential of our analytical framework in understanding the spatial biases of crowdsourced environmental data first to explore the pws adoption spatial patterns we only considered socio economic factors of population including median household income household density and owner occupied household ratio incorporating other demographic characteristics such as the proportion of retired people students which potentially form a sizable proportion of the pws owners or marginalized vulnerable groups which may likely to have lower uptake may aid the understanding of the spatial pattern of pws adoption paul et al 2021 second this study focused on analyzing the factors that affect pws rainfall representation in a census tract therefore binary results of pws adoption were used for building regression models to assess the socio economic and flood vulnerability differences in census tracts with adoption or no adoption future work could expand the results by building regression models to predict the number of pwss or per capita pws to understand the factors that influence the pws growth de groote et al 2016 finally this study focused on the pws adoption at the census tract level which may not be adequate to fully capture the factors that affect pws adoption since aggregation of data can inflate estimates of association between variables within a ct future work could be focused on surveying individual pws owners at household level to permit understandings for finer level of pws adoption gharesifard and wehn 2016 another unexplored question in this study is the difference of adoption rates between the analyzed mas for example seattle generally has higher probability 66 of pws adoption regardless of the mhi quartile group while in chicago the probability is lower than 45 for every mhi quartile group this can be caused by some other factors that were not explored in this study for example while pws adoption often occur at the individual household level large number of pwss could be adopted by corporations that are used for the benefits for their business pws adoption could also be adopted by non profit organizations ngos that initiate weather monitoring programs furthermore the difference in cost of living across mas might be explanatory of pws adopt number difference in mas since pws are devices that could be easily purchased online with the similar costs a pws of 200 usd for example may be affordable in mas with higher cost of living while in mas with lower cost of living this price could be relatively costly while most of the past literature on crowdsourced rainfall has focused on the usefulness quality and trustworthiness of the observed data they tended to focus on a single study area a watershed or a city of interest bardossy et al 2021 chen et al 2021 de vos et al 2017 2019 mandement and caumont 2020 this study on the other hand opens a door for spatial pattern analysis of broader crowdsourced data using pws adoption as a case study our study highlights a phenomenon that generates cross disciplinary research opportunities between engineers designers social sciences city managers planners and ngo groups moving forward alongside the rapid growth of the number of crowdsourced data to support flood assessment efforts the methodology could be applied to other types of crowdsourced data to assess the potential spatial biases in crowdsourced data collection enabling crowdsourced networks to be better positioned for decision makers in their flood resilience efforts 5 conclusion in this study we first examined the rainfall spatial representation of pwss at huc 12 watersheds in twelve metropolitan areas in the u s the results show disparities across the analyzed metropolitan areas with the percentage of pws underrepresented watersheds ranging from 0 to 40 furthermore by modeling pws adoption using socio economic and flood related data at census tract level the results suggest that the current pws adoption pattern exhibits spatial biases toward wealthier neighborhoods and flood prone regions the findings provide insights to inform how policies could be made to distribute resources to improve the rainfall data collection efforts in pws underrepresented regions as crowdsourced data are increasingly used for decision making by policymakers efforts to close the gap in current non uniform pws spatial adoption will allow crowdsourced rainfall data to be better positioned to support decision makers in their flood resilience efforts data availability all data used in this study are publicly available personal weather station adoption data was downloaded from the weather underground database using the weather underground api weather underground 2019 huc 12 watershed boundary was downloaded from usgs national water information system u s geological survey 2021 socio economic data was downloaded from the united states census bureau u s census bureau 2021 population raster data was downloaded from worldpop website tatem 2017 nfip redacted flood claim data is available from openfema website federal emergency management agency 2021 and the flood zone data was accessed from nyu furman center floodzonedata us website nyu furman center 2021 author contributions all authors contributed to conceptualization of the study a b c conducted all the analyses and wrote the initial draft z z assisted with data collection and visualization j l g and t d c aided in the study design and interpretation of the results all authors contributed to revising and finalizing the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research is supported by the national science foundation under grant no cbet 1735587 we gratefully acknowledge weather underground united states geological survey united states census bureau worldpop the federal emergency management agency and the nyu furman center for access to their data 
3405,crowdsourced personal weather stations pwss adoption has been growing rapidly and provides the potential to fill in hyper local rainfall observation gaps however current adoption patterns exhibit spatial biases that must be understood when using the data for modeling and decision making here we first examine the pws rainfall spatial representation at huc 12 watersheds in twelve metropolitan areas in the u s furthermore by modeling the pws adoption using socio economic and flood related data at census tract level the results suggest current adoption patterns exhibit spatial biases toward wealthier neighborhoods and flood prone regions the findings provide insights to inform how policies could be made to distribute resources to improve the rainfall data collection efforts in pws underrepresented regions as crowdsourced data are increasingly used for decision making by policymakers efforts to close the gap in current non uniform pws spatial adoption will allow crowdsourced rainfall data to be better positioned to support decision makers in their flood resilience efforts keywords crowdsourcing rainfall flooding spatial analysis logistic regression flood resilience 1 introduction flooding causes significant social and economic damage and loss in the united states each year the national academies of science engineering 2019 hurricane harvey is an extreme example of bringing unprecedented rainfall across the city of houston texas leading to catastrophic flooding that impacted more than 100 000 homes and incurred an estimated damage cost of 125 billion van oldenborgh et al 2017 with the increase in frequency and intensity of heavy rainfall due to climate change the impact of flooding is projected to become more severe over time cheng and aghakouchak 2014 fowler et al 2021 in urban watersheds with large portions of impervious surfaces and low lying areas rainfall driven flooding has been causing even more considerable damage affecting greater numbers of people rosenzweig et al 2018 therefore there is a need for improved flood assessments as well as building flood resilience in urban areas to adapt to the increasing severity of flood hazards bertilsson et al 2019 flood assessments at high spatial and temporal resolution remain a challenge an ideal flood assessment will require a spatially and temporally representative dataset as flood modeling involves complex and nonlinear flow and physical processes hu et al 2019 traditionally flood assessments have mainly been efforts by government agencies local government agencies are responsible for identifying flood risks and creating flood forecasting models tyler et al 2019 however flood related data available from government agencies needed for enabling high resolution flood modeling at flood prone areas are usually insufficient or lacking cristiano et al 2017 furthermore increasing attention has been focused on incorporating the public and a wider representation of stakeholders in the decision making process of flood assessment and management white et al 2010 lack of engagement from communities for locally specific flood knowledge will lead to inadequate flood model calibration and validation gebremedhin et al 2020 in such cases flood forecasting models generated by government agencies may not be fully accepted and supported by local stakeholders as they may not reflect adequate representations of floods in certain regions gebremedhin et al 2020 rosenzweig et al 2018 sy et al 2019 lack of engagement of local stakeholders may also worsen the disparities of social economic and environmental resources across communities white et al 2010 crowdsourced data can provide a potential solution crowdsourcing is a method of collecting information from as many involved contributors from the general public muller et al 2015 which offers a way of obtaining large amounts of flooding related data cost efficiently assumpção et al 2018 with the development of inexpensive sensors and communication technology the amount of data collected by the public has been growing rapidly generating a wealth of information muller et al 2015 for example social media such as twitter data produces geotagged information that can be extracted to map the potential extent of flood events wang et al 2018 mobile apps such as google waze enable users to report floods in a convenient and efficient way which greatly supports government efforts to identify the location of floods praharaj et al 2021 in the case of rainfall monitoring the adoption of personal weather stations pwss by the general public fills in the rainfall observation gap in locations where government agency data are unavailable de vos et al 2017 pwss are off the shelf weather stations installed and maintained by individuals and the reported data can be easily shared through websites such as weather underground gharesifard and wehn 2016 in recent years crowdsourced pws adoption has been growing rapidly to supplement agency maintained rain gauges which are usually limited in coverage de vos et al 2017 as well as remote sensing rainfall e g satellite and radar which requires validation from ground rain gauges muller et al 2015 for example in houston texas pws adoption density has grown from 0 06 to 0 24 pws per km2 from 2016 to 2019 such exponential growth suggests that in populated areas in the u s pwss could alone provide sufficient spatial resolutions for rainfall observations needed for urban hydrology in a few years berne et al 2004 chen et al 2021 furthermore by participating in data collection via crowdsourcing the public may also become more proactive in engaging in local decision making processes buytaert et al 2016 in fact the motivation behind adopting pwss and sharing the collected data often starts with owners using the data for personal purposes simultaneously growth in pws adoption has the potential to benefit society at large since the collected data could implicitly create crucial knowledge for local communities gharesifard and wehn 2016 therefore as local stakeholders e g pws owners gain better understanding of the their environments with time empowering them to grow from collecting data toward developing models and decision support systems becomes key almoradie et al 2015 voinov et al 2016 the ongoing trend of increasing crowdsourcing participation e g rainfall data collection by pws adoption from the general public in local communities therefore lays the foundation for generating collective knowledge to support flood resilience efforts paul et al 2018 despite the rapid growth of pws adoption for generating a wealth of rainfall observations current adoption patterns exhibit spatial biases caused by underrepresentation or overrepresentation of certain regions muller et al 2015 using non representative and spatially biased datasets as model inputs could lead to biased modeling results and decision making towe et al 2020 furthermore recent advancements of data driven techniques has enabled modeling flood at high spatial and temporal resolution mosavi et al 2018 sadler et al 2018 shen et al 2019 zahura et al 2020 which has also highlighted the importance of using representative datasets as large datasets are not always comprehensive torralba and efros 2011 therefore it is important to ensure that crowdsourced rainfall datasets are analyzed for spatial and temporal representation and variability and if biases exist such biases are clearly understood and accounted for when using the crowdsourced data in modeling and decision making spatial bias in crowdsourced data has been studied across a variety of fields for example social media usage is mainly concentrated in populous areas leading to certain groups not receiving needed assistance during disasters fan et al 2020 mobile phone applications have the potential to enhance resilience building against disasters but the lack of user centered design often results in low uptakes in marginalized and vulnerable groups who are more vulnerable in disaster situations craig et al 2019 paul et al 2021 crowdsourced bicycle ridership data are biased toward recreational riders who track exercise activity which requires a correction for better representation of the ridership patterns of all riders such as commuter cyclists roy et al 2019 biodiversity data collected from crowdsourcing exhibited spatial biases toward accessible areas or recreational summer homes millar et al 2019 pws adoption and other crowdsourced climate and atmospheric data have spatial biases toward populous areas muller et al 2015 in previous studies regression models using socio economic and demographic factors have been widely used for assessing the factors that affect the spatial bias pattern examples include exploring the effect of socio economic factors and environmental attitudes on rain barrel spatial adoption patterns ando and freitas 2011 the presence of hazardous waste sites affecting life expectancy kiaghadi et al 2021 and spatial models to predict electric vehicle ownership choice behavior chen et al 2015 however limited studies have been focused on the fast growing crowdsourced pws network to provide high resolution rainfall observations and engage communities in supporting flood resilience therefore the research questions that guide this study are 1 are pws providing a spatially representative sample of rainfall data 2 what are the underlying factors that affect the spatial pattern of pws adoption in this study we used a unique pws adoption dataset obtained from weather underground which is one of the largest crowdsourced pws platforms this dataset consists of the location of more than 100 000 pws in the contiguous u s pws rainfall representation in united states geological survey usgs level 12 hydrological unit codes u s geological survey 2021 huc 12 watersheds in 12 selected metropolitan areas were analyzed to quantify to what extent current pws adoption can contribute to rainfall observations we further applied logistic regression models using socio economic and flood related data to identify the factors that influence the spatial bias in pws adoption moreover we measured and analyzed the marginal effects of resulting models to quantify the pws adoption disparities across neighborhoods 2 materials and methods 2 1 pws adoption data acquisition pws adoption dataset used in this study was obtained from weather underground database using their previous version of application programming interface api the analysis dataset contains metadata including the id and the geographic location latitude and longitude of more than 100 000 pwss the geographical location of these pwss were then mapped using arcgis to explore the pws adoption spatial pattern in the contiguous u s note that at the end of 2019 this version of the api has been retired the new version of the api introduced in 2020 requires a weather underground key which can be obtained through connecting a pws to the platform furthermore the number of api calls per day in order to download the data was greatly limited and some of the metadata including pws installation date were no longer available therefore to preserve such details in the dataset this study focused on the pws adoption dataset gathered on april 2019 2 2 pws rainfall representation calculation pws rainfall representation was evaluated based on the pws density in an urban watershed within a metropolitan area ma in this study an urban watershed is defined as huc 12 watersheds that intersected with united states census bureau uscb delineated urban areas within the ma boundary u s census bureau 2021 to compute the pws density in an urban watershed of each ma we used the point density tool in arcgis version 10 6 to convert pws location point data to gridded raster which represents the pws density followed by using the zonal statistics tool to compute the average pws point density in a watershed see fig 1 for the workflow like traditional rain gauges rainfall data recorded from pwss are point observations which can only be representative of the rainfall of a specific area due to the spatial variability of rainfall events cristiano et al 2017 to have a sufficient representation of rainfall spatial variability in terms of rain gauge density in a monitoring network the world meteorological organization recommends at least one rain gauge per 10 20 km2 for urban areas world meteorological organization wmo 2008 therefore we used an average pws point density of 0 1 pws per km2 1 pws per 10 km2 as the threshold to assess pws rainfall representation an underrepresented ur watershed is defined as having a point density lower than the threshold while a non ur watershed is defined as having a point density above the threshold using an one way anova test we further extracted the population density estimates from the worldpop population dataset tatem 2017 to test if a significant mean difference in mean population density exists between ur and non ur watersheds 2 3 pws adoption logistic regression model we used logistic regression to assess the association between the pws adoption and selected socio economic data the built in glm package in r programming language version 3 6 1 was applied to run the regression models census tract ct level socio economic data were obtained from the uscb s 2017 american community survey 5 year estimate including population median household income household density and owner occupied household ratio the number of owner occupied households divided by the total number of households ct was selected as the common geographic unit for analysis because it is the smallest geographic boundary used by the census bureau to build the logistic regression model we aggregated the pws location information into total counts of pws adopted in each ct then classified these counts into binary groups pws adoption 1 for cts that have at least 1 pws 1 pws and pws adoption 0 for cts that have no pws adoption the logistic regression equation is shown as follows 1 p p w s a d o p t i o n e β 0 i 1 n β i q i i n 1 m β i x i 1 e β 0 i 1 n β i q i i n 1 m β i x i where p is the probability of 1 pws adoption in a ct β 0 is the intercept β 1 β 2 β n 1 are the coefficients for the categorical variable q i i 1 2 n 1 and q i are the dummy variables for each category with the value of either 0 or 1 β n β n 1 β m are the coefficients for the continuous variables x i the regression coefficients are estimated by maximum likelihood in the explanatory variables to account for household income differences across mas the median household income mhi was converted into categorical variables using quartile groups m h i q 1 m h i q 2 m h i q 3 and m h i q 4 in each analyzed ma a ct falling within first quartile of ma mhi was assigned to m h i q 1 a ct falling between first and second quartile assigned to m h i q 2 a ct falling between second and third quartile was assigned to m h i q 3 a ct falling above the fourth quartile was assigned to m h i q 4 population pop household density hhd and owner occupied household ratio oohr are used as continuous variables for the logistic regression model 2 4 flood vulnerability and pws adoption two types of flood related datasets at the ct level i the total number of flood claims fc and ii percent housing units in the 100 year flood zone fz were used to assess the association between potential flood risk and pws adoption fc data were obtained from national flood insurance program nfip redacted claims which is a large database containing more than two million claims transactions since the nfip launched federal emergency management agency 2021 fz data were obtained from a dataset published by nyu furman center this dataset was created by combining housing and population data with fema floodplain maps to calculate the percent of housing units intersecting with a fema 100 year floodplain nyu furman center 2021 in this study we used the fc and fz data to represent the flood vulnerability of a ct we assumed that a ct with a higher number of flood insurance claims is more likely to have a higher flood risk and a ct with a higher percentage of housing units in the floodplain also implies that this ct is more vulnerable to flooding for each analyzed ma we classified cts into high low fc groups based on the fc value with the low fc group having fcs below the median value of the cts within the ma representing the lower flood risk cts and high fc group having fcs above the median values representing the higher flood risk cts similarly the fz data were used to classify cts into in fz and not in fz groups with in fz group representing cts that have any percent housing units in the floodplain and not in fz group representing cts that have zero percent in the floodplain 3 results 3 1 pws adoption in the contiguous u s the spatial pattern of pws shows that adoption is concentrated in the metropolitan areas in the east and west coast of the contiguous u s fig 2 a unlike the agency operated rainfall network where rainfall stations are usually uniformly distributed pws is spatially biased toward populous areas at the metropolitan area ma level pws adoption is also highly correlated with population fig 2b with a correlation coefficient of 0 88 for all mas in the contiguous u s to assess the pws representation in the mas in the contiguous u s the top 12 mas by pws adoption fig 2b were selected in this study the analyzed mas are distributed across the contiguous u s with the number of pws adoption ranging from 1 300 to 2 569 units per ma and pws density ranging from 0 03 to 0 27 pws per km2 3 2 pws rainfall representation in huc 12 watersheds in this study watersheds with pws point density 0 1 are considered underrepresented ur otherwise they are considered well represented non ur the results of the pws representation calculation showed that the current pws rainfall representation exhibits three characteristics table 1 first at the ma level pws rainfall representation in huc 12 watersheds varied across the analyzed mas the average pws point density ranges from 0 14 in atlanta to 0 59 in san francisco most of the analyzed mas besides chicago have a maximum pws point density above 0 40 indicating that pws has the potential to contribute to the coverage of rainfall observations for at least every 2 5 km2 in those watersheds among the analyzed mas san francisco has the highest maximum pws point density of 1 34 pws per km2 which could provide a considerable rainfall representation that is even greater than the current resolution of radar derived rainfall e g the next generation weather radar nexrad typically has 1 km by 1 km resolution second though the average pws point density for analyzed 12 mas is well represented at the ma level large disparities occurred at the huc 12 watershed level when uneven representation begins to appear for example in seattle and denver pws adoptions are more uniformly distributed where no ur watersheds were observed however the percentage of ur watersheds is larger in mas such as atlanta 40 and houston 30 third the mapping of the pws representation shows that the pws representation has certain spatial distribution patterns which merit further analysis as shown in fig 2 ur watersheds are generally clustered in a specific region of a ma for example in houston ur watersheds are concentrated on the east portion of the ma where pws adoption in these watersheds is relatively low in chicago and atlanta large portions of ur watersheds are concentrated in the southern part of the ma we hypothesized that population density is explanatory for the pws adoption spatial disparities to test this hypothesis population density estimates for each huc 12 watershed were computed using the worldpop dataset tatem 2017 as shown in table 1 the average population density in non ur watersheds was higher than in ur watersheds however the result of the one way anova test showed that the mean difference of population density in ur and non ur watersheds in most mas was not statistically significant although there is a notable pws adoption difference in ur and non ur watersheds population density did not fully explain the spatial bias in pws adoption as can be seen in the scatterplots in fig 3 most ur watersheds shown in red dots have similar levels of population density compared to non ur watersheds shown in blue dots while lacking pws representation therefore we implemented further analysis at a finer geographic scale census tract to assess other factors affecting the spatial pattern of pws adoption 3 3 pws adoption logistic regression model logistic regression models were built for the 12 analyzed mas 12 regression models to assess the factors affecting the spatial pattern of pws adoption table 2 shows the summary statistics of the response variable pws adoption and the selected socio economic explanatory variables at the census tract ct level multicollinearity diagnostics performed using the variance inflation factor vif indicated that the selected input variables do not pose a concern of collinearity since all the vifs are below 3 0 midi et al 2013 using the pws point density threshold of 0 1 per km2 in the previous section we assumed that the presence of 1 pws could provide the minimum rainfall representation for a ct since the area of a ct in the analyzed mas is mostly below 10 km2 therefore the cts were classified into binary groups of 1 pws adoption and no pws as the response variables in the logistic regression models the summary statistics table 2 showed that the percentage of cts with 1 pws adoption ranged from 21 new york to 77 seattle and is generally lower in mas with a larger number of cts such as new york los angeles and chicago notably the comparison of cts in no pws and 1 pws groups table 2 across mas shows that the median household income mhi and owner occupied household ratio oohr are significantly higher in the cts with 1 pws while household density hhd is mostly higher in cts with no pws the mean population in the two groups was similar since cts by definition are delineated based on the population 3 the exponentiated coefficients odds ratio of the logistic regression models are shown in table 3 among the input variables to the models mhi pop and oohr have positive effects on pws adoption odds ratio greater than 1 while hhd has negative effects odds ratio less than 1 which indicates negative effects on pws adoption the mhi variables were converted into categorical variables using quartile groups based on the coefficients of the mhi for the 12 analyzed mas the odds ratio of the fourth mhi quartile group mhi q4 was the highest followed by the third mhi q3 and second mhi q4 mhi quartile group notably in mas such as dallas and houston the odds ratio of the third and fourth mhi quartile was significantly greater than first mhi quartile group mhi q1 which indicated that pws adoptions are much more likely to occur in wealthier neighborhoods the coefficients of population variables were significant for every analyzed ma in predicting pws adoption suggesting that an increase of 1 000 in population could lead to an increase of 9 6 houston 48 5 boston of the odds ratio that a ct has 1 pws adoption the coefficients of oohr showed that pws adoption is more likely to occur in cts with a larger percentage of owner occupied households suggesting that an increase of 1 in oohr in a ct could lead to an increase of 3 11 of the 1 pws adoption odds ratio however pws adoption is less likely in densely populated cts for an increase of 1000 households per km2 in hhd in a ct the odds ratio of pws adoption could be decreased by 3 5 to 50 5 across analyzed mas 3 4 the effects of median household income on pws adoption marginal effects of mhi were calculated to provide an intuitive comparison of mhi effects on pws adoption across analyzed mas a marginal effect me is defined as the change in the response variable associated with a change in one explanatory variable while holding other variables at a specific value in the pws adoption logistic regression models the mes of mhi demonstrate the discrete change in predicted probability of pws adoption from the reference category mhi q1 to other categories mhi q2 mhi q3 and mhi q4 keeping pop hhd and oohr at their mean values the mes of mhi for the analyzed mas are shown in fig 4 a based on the results of the me common patterns were observed for every analyzed ma the predicted probability of pws adoption is always the lowest in the reference category mhi q1 followed by the second mhi q2 third mhi q3 the predicted probability of pws adoption of the mhi q4 is consistently the highest this indicates that in a hypothetical ct with identical pop hhd and oohr the probability of pws adoption is greater in upper mhi quartile categories for example for houston the me for the mhi q1 category is 19 while in mhi q2 mhi q3 and mhi q4 the mes are 34 58 and 81 respectively despite the common pattern that predicted pws adoption is generally higher in upper mhi quartile groups the level of pws adoption disparities due to mhi varied significantly across analyzed mas as shown in fig 3a seattle generally has a higher probability greater than 66 of pws adoption regardless of the mhi quartile group however the adoption probability varied largely in mas such as houston mhi q1 19 mhi q4 81 and chicago mhi q1 7 and mhi q4 45 the ratio of the average mes of the upper mhi quartile group mhi q2 mhi q3 and mhi q4 to the mhi q1 was computed to quantify the level of disparities as shown in fig 4b in mas such as seattle and denver the level of disparities between mhi quartile group are lower ratio to q1 were 1 3 and 1 5 respectively while in mas such as houston and chicago the level of disparities is much higher ratio to q1 were 3 3 and 5 4 respectively the pws adoption pattern versus mhi quartiles of these example mas are shown in fig 5 3 5 the effects of potential flood risk on pws adoption in this study we further theorized that flood risk may influence pws adoption therefore two types of flood related dataset at the ct level were used to assess the association between potential flood risk and pws adoption i total number of flood claim fc and ii percent housing units in the 100 year flood zone fz a comparison of fz and fc data showed that cts with a lower number of flood claims low fc group are associated with a lower percentage of cts that are in the in fz group while cts with a higher number of flood claims high fc group are associated with a higher percentage of cts that are in the in fz group as can be seen in fig 6 this relationship is consistent for the analyzed mas which indicates that cts that have any percent of occupied housing units in the fema 100 year flood plain are more likely to have a greater number of flood claims for example in new york only 4 6 of the cts in the low fc group are in the in fz group while 59 3 of the cts in the high fc group are also in the in fz group next we look at the relationship between pws adoption and the two flood related variables to investigate whether the vulnerability to flood affects the likelihood of pws adoption logistic regression models were built for the 12 analyzed mas using socio economic explanatory variables along with fc and fz data to assess the potential flood risk affecting the spatial pattern of pws adoption note that these models were built separately from the models in previous section since the fz data have larger percentage of cts with missing data the in analyzed mas 100 27 18 and 17 missing in seattle san francisco washington dc and chicago respectively in addition since the correlation exists between fc and fz variable two separated regression models one adding fc data and another one adding fz data were built the coefficients for the models adding fc and fz data are shown in table 4 and table 5 respectively compared to the reference group of low fc high fc groups for most of the analyzed mas have odds ratio greater than 1 0 indicating positive effects on the pws adoption similarly compared to the reference group of not in fz group the odds ratios for in fz group are greater than 1 0 which also suggesting positive effect on pws adoption we further used marginal effects me to demonstrate the effects of flood vulnerability on pws adoption using the me adjusted for the second quartile mhi group as an example as shown in fig 7 a consistent relationship was found that the mes of the predicted probability of pws adoption in high fc groups were higher than the mes in the low fc groups fig 7a which means that cts that have higher number of flood claims will be more likely to have pws adoption similarly the mes of the fz variable also suggested that cts that intersected the 100 year floodplain have higher probability of pws adoption than those not in the floodplain fig 7b these findings suggest that with the assumption that the number of flood claims and the percentage of households in the 100 year floodplain represent the flood vulnerability of a ct current pws adoption pattern is spatially biased toward flood prone regions 4 discussion in this study we analyzed a large pws adoption dataset to explore the rainfall representation of crowdsourced pws in metropolitan areas mas in the contiguous u s consistent with previous literature pws adoptions are generally concentrated in populous ma muller et al 2015 however our analysis showed that pws adoption exhibited significant spatial biases which may result in overrepresentation and underrepresentation of crowdsourced rainfall observations across huc 12 watersheds within a ma in the 12 analyzed mas the results of logistic regression models revealed that current pws rainfall representation is biased toward wealthier neighborhoods and higher flood risk regions due to adoption disparities potential causes of these biases could be because wealthier families have more resources and leisure to participate in pws adoption since purchasing and maintaining pwss are often regarded as a hobby rather than a necessity in the household gharesifard and wehn 2016 on the other hand continuous urban development might be driving the population growth in the floodplains which potentially explains the higher possibility of pws adoption in floodplains due to increased population qiang 2019 in addition areas with higher number of flood claims are usually associated with homeownership and higher value homes kousky and michel kerjan 2017 which is the proportion of demographics that are more likely to adopt pws however these are speculations based on the correlation and regression model results future studies are needed to test and better understand causality for these correlations policies could be made to direct the distribution of resources of rainfall data collection efforts in pws underrepresented regions unlike the non uniform pws spatial adoption traditional rainfall monitoring networks are often designed as uniformly deployed rainfall stations across the watersheds for example harris county flood control district hcfcd maintains a large rainfall network consisted of 175 rain gauges uniformly distributed across harris county as shown in fig 8 the point density of pws and hcfcd comparison shows that in huc 12 watersheds the point density of hcfcd maintained network ranges from 0 01 to 0 07 whereas the point density of pws widely ranges from 0 01 to 0 48 this observation suggests that to make the best use of the limited public resources city managers and engineers should consider the spatial pattern of pws adoption when designing future rainfall monitoring networks rather than evenly deploying rain gauges relocating resources to set up more rain gauges in pws underrepresented regions would support the current non uniform pws adoption moreover local agencies and organizations could leverage their resources to conduct science technology engineering and mathematics stem related programs and workshop that incentivize the communities in pws underrepresented regions to participate in data driven decision making processes and thus increasing their representation in crowdsourced rainfall data mondschein et al 2019 incentivizing pws adoptions to increase participation in crowdsourced data collection could strengthen the awareness of stakeholders for their local environment in the pws underrepresented regions past examples include the use of volunteered stream monitoring to increase the community s awareness for protecting environmental resources overdevest et al 2004 the application of participative geographic information approaches could strengthen work relations among local actors and authorities to prevent river flooding usón et al 2016 crowdsourced rainfall data collection in the community collaborative rain hail and snow network cocorahs were shown to have educational benefits that improve the climate literacy of the participants reges et al 2016 additionally the increased participation in pws adoption could improve the usefulness of the crowdsourced rainfall data unlike traditional rainfall networks that are maintained and collected by experts with rigorous quality control procedures the utility of crowdsourced pws rainfall network is often compromised by its data quality because of limited quality control processes and lack of trust for data contributed from non experts muller et al 2015 as pws quality control and trustworthiness assessments methods often require a sufficient number of neighboring pwss for optimal performance chen et al 2021 de vos et al 2019 increased number of pws adoption in the pws underrepresented areas can therefore help ensure the quality and trustworthiness of the crowdsourced data and thus improve the usefulness of the crowdsourced rainfall data future work could also focus on the design of crowdsourcing incentivizing programs for pws adoption by coupling the regression model results with agent based modeling to better identify the behavior of crowdsourcing participants and therefore increase adoption yang et al 2019 our research merits further exploration in terms of methodology and underlying datasets which limited the potential of our analytical framework in understanding the spatial biases of crowdsourced environmental data first to explore the pws adoption spatial patterns we only considered socio economic factors of population including median household income household density and owner occupied household ratio incorporating other demographic characteristics such as the proportion of retired people students which potentially form a sizable proportion of the pws owners or marginalized vulnerable groups which may likely to have lower uptake may aid the understanding of the spatial pattern of pws adoption paul et al 2021 second this study focused on analyzing the factors that affect pws rainfall representation in a census tract therefore binary results of pws adoption were used for building regression models to assess the socio economic and flood vulnerability differences in census tracts with adoption or no adoption future work could expand the results by building regression models to predict the number of pwss or per capita pws to understand the factors that influence the pws growth de groote et al 2016 finally this study focused on the pws adoption at the census tract level which may not be adequate to fully capture the factors that affect pws adoption since aggregation of data can inflate estimates of association between variables within a ct future work could be focused on surveying individual pws owners at household level to permit understandings for finer level of pws adoption gharesifard and wehn 2016 another unexplored question in this study is the difference of adoption rates between the analyzed mas for example seattle generally has higher probability 66 of pws adoption regardless of the mhi quartile group while in chicago the probability is lower than 45 for every mhi quartile group this can be caused by some other factors that were not explored in this study for example while pws adoption often occur at the individual household level large number of pwss could be adopted by corporations that are used for the benefits for their business pws adoption could also be adopted by non profit organizations ngos that initiate weather monitoring programs furthermore the difference in cost of living across mas might be explanatory of pws adopt number difference in mas since pws are devices that could be easily purchased online with the similar costs a pws of 200 usd for example may be affordable in mas with higher cost of living while in mas with lower cost of living this price could be relatively costly while most of the past literature on crowdsourced rainfall has focused on the usefulness quality and trustworthiness of the observed data they tended to focus on a single study area a watershed or a city of interest bardossy et al 2021 chen et al 2021 de vos et al 2017 2019 mandement and caumont 2020 this study on the other hand opens a door for spatial pattern analysis of broader crowdsourced data using pws adoption as a case study our study highlights a phenomenon that generates cross disciplinary research opportunities between engineers designers social sciences city managers planners and ngo groups moving forward alongside the rapid growth of the number of crowdsourced data to support flood assessment efforts the methodology could be applied to other types of crowdsourced data to assess the potential spatial biases in crowdsourced data collection enabling crowdsourced networks to be better positioned for decision makers in their flood resilience efforts 5 conclusion in this study we first examined the rainfall spatial representation of pwss at huc 12 watersheds in twelve metropolitan areas in the u s the results show disparities across the analyzed metropolitan areas with the percentage of pws underrepresented watersheds ranging from 0 to 40 furthermore by modeling pws adoption using socio economic and flood related data at census tract level the results suggest that the current pws adoption pattern exhibits spatial biases toward wealthier neighborhoods and flood prone regions the findings provide insights to inform how policies could be made to distribute resources to improve the rainfall data collection efforts in pws underrepresented regions as crowdsourced data are increasingly used for decision making by policymakers efforts to close the gap in current non uniform pws spatial adoption will allow crowdsourced rainfall data to be better positioned to support decision makers in their flood resilience efforts data availability all data used in this study are publicly available personal weather station adoption data was downloaded from the weather underground database using the weather underground api weather underground 2019 huc 12 watershed boundary was downloaded from usgs national water information system u s geological survey 2021 socio economic data was downloaded from the united states census bureau u s census bureau 2021 population raster data was downloaded from worldpop website tatem 2017 nfip redacted flood claim data is available from openfema website federal emergency management agency 2021 and the flood zone data was accessed from nyu furman center floodzonedata us website nyu furman center 2021 author contributions all authors contributed to conceptualization of the study a b c conducted all the analyses and wrote the initial draft z z assisted with data collection and visualization j l g and t d c aided in the study design and interpretation of the results all authors contributed to revising and finalizing the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research is supported by the national science foundation under grant no cbet 1735587 we gratefully acknowledge weather underground united states geological survey united states census bureau worldpop the federal emergency management agency and the nyu furman center for access to their data 
3406,accurate and timely short term forecasting services of precipitation variable are significant for people s lives and property security the data driven approaches demonstrate promising performance in the extrapolation of precipitation in this paper we proposed a spatiotemporal prediction model namely the spatial temporal long short term memory based on the self attentive mechanism st lstm sa this model enables better aggregation of sequence features inspired by proposed improvements the encapsulated 3d convolution is developed to fully exploit the short term spatiotemporal information and the channel correlation is modeled by self attention mechanism to further improve representations in the long term interaction the effectiveness of which is validated in the ablation study comprehensive experiments have been conducted on the radar echo sequence we successfully predicted future radar reflectivity images for next 3 h with data for previous three hours as inputs in wuhan china three machine learning methods multiple linear regression mlr support vector regression svr and artificial neural networks ann and deep learning model convlstm have been chosen as comparative groups to corroborate the nowcasting availability of this model for radar echo extrapolation moreover the supplementary experiment on minute dataset also illustrated the superiority of st lstm sa the studies analyzed the forecasting performance in terms of image quality and rainfall error the experimental results demonstrated the better versatility and performance of st lstm sa these conclusions and attempts may provide efficient guidance for precipitation nowcasting in urban areas keywords precipitation nowcasting spatiotemporal lstm self attention radar echo image 1 introduction high resolution precipitation nowcasting is a crucial issue in weather forecasts which is especially useful for the prevention and management of droughts and floods disasters xu et al 2018a to reduce socioeconomic impacts prudden et al 2020 precipitation nowcasting is a short term 0 6 h forecasting of local rainfall intensity based on rainfall related data in previous hours such as weather radar or meteorological satellite data lingling 2018 compared with medium and long term weather forecasting nowcasting is instructive for grasping current weather patterns within a relatively shorter forecast time fengru 2020 weather radar observations provide precipitation maps with high spatial 1 km and temporal 5 min resolution reyniers 2008 well suited as the data source for precipitation nowcasting in the past the field of radar based precipitation nowcasting mainly utilized numerical weather predictions nwp and optical flow algorithm and problems resulting from which like scale dependence and certain limitations are difficult to eliminate prudden et al 2020 sun 2014 yano 2018 the applications of statistical machine learning ml and deep learning dl in hydrometeorology xu et al 2020 develop a new idea of data driven forecasting in earth system science hua and simovici 2016 xu et al 2021 xu et al 2018b and provide a promising direction to make optimal use of the high resolution observations prudden et al 2020 the flexibility and powerful representation ability of deep learning can capture the non linearity and randomness of meteorological data as well as improve the timeliness and accuracy of forecasting recent advances in precipitation forecast offer different insights on how to predict future sequences based on radar data in the early stage the neural network models were mostly used with meteorological factors surrounding pixels in radar maps shangzan et al 2017 or rainfall sequences deshpande 2012 as inputs the introduction of convolutional neural networks cnn allowed us to better extract features from large amounts of image data and offer great generalization capability bengio et al 2007 long et al 2017 sadeghi 2019 a multi channel 3d cube successive convolution network 3d csn was constructed zhuang and ding 2016 which leveraged both raw 3d radar and other meteorological data directly to predict specific location of rainfall in addition a 3 d convolutional neural network 3d cnn model was proposed to nowcast extrapolation based precipitation with lead time up to 10 min using unique 3 d radar data kim et al 2021 furthermore precipitation forecasting was treated as an image to image translation problem in u net convolutional neural network agrawal et al 2019 ronneberger et al 2015 rainnet ayzel et al 2020 was designed inspired by u net to achieve nowcasting for up to a lead time of 1 h further studies also confirmed the potential of cnn for radar based precipitation nowcasting choi et al 2021 zhang et al 2017 zheng 2021 to capture long term dependency between distant frames wang 2021 a sequence to sequence sutskever et al 2014 long short term memory lstm model derived from natural language processing nlp sutskever et al 2011 has been applied into meteorology prediction the paper firstly drawing upon the ideas of spatiotemporal forecast in the field of radar based precipitation proposed a convolutional long short term memory convlstm model shi 2015 extending the spatial features based on lstm by adding 2d convolution to improve the location invariant structure in convlstm trajectory gated recurrent unit trajgru algorithm composed of convolution and gru cho et al 2014 was proposed chen et al 2020 which added the learnable convolution to learn spatial variation of images to jointly model information interaction of spatiotemporal correlations wang 2021 predrnn was defined with a spatiotemporal lstm building block to update memory state over both time and space bonnet et al 2020 wang et al 2017 as an improvement method predrnn wang et al 2018b was constructed by causal lstm with cascaded mechanism and gradient highway unit ghu to build spatiotemporal network and alleviate the problem of gradient vanishing by applying differencing operations on the non stationary and approximately stationary properties in spatiotemporal dynamics memory in memory mim networks wang 2019b were proposed and applied to time series forecasting tasks a new curriculum learning strategy wang 2021 was introduced into predrnn to learn long term dynamics which can cover different aspects of spatiotemporal representations in summary combining the merits of convolutional and recurrent architectures underpinned the follow up research work kim et al 2017 tran and song 2019 video prediction algorithms combining computer vision with temporal features have been diversely used for action recognition liu et al 2017 detecting abnormal activities feng 2021 and motion prediction villegas et al 2017a etc essentially radar echo extrapolation as with frame level prediction on videos is employed to perform future frame prediction in fact video prediction algorithms have shown good possibilities wang et al 2018b wang 2019a wang et al 2017 for radar echo extrapolation bonnet et al 2020 devoted to long term and finer prediction video prediction has made impressive progress network with motion and content separation villegas et al 2017a was proposed to model the spatiotemporal dynamics and the hierarchical approach villegas et al 2017b wichers et al 2018 facilitates longer term predictions of future frames novel ideas for precipitation nowcasting may be motivated by recent advances in video prediction self attention sa vaswani 2017 is an attention mechanism that computes weights at different positions based on its own sequence it has been shown to be impactful in machine reading cheng et al 2016 computer vision huang 2019 wang et al 2018a and even the field of point cloud guo et al 2021 etc it infers image information by focusing more on recognizable regions of the image or accomplishes semantic transformation by focusing on specific words in long sentences compared to convolution operation sa module is rather efficient to aggregate historical consecutive sequences convlstm with self attention memory module was designed to fuse features from previous frames lin et al 2020 a recall gate partially motivated by attention mechanism to recall long range memory was introduced into the model called e3d lstm wang 2019a connecting the current state with the previous memories and assigning different weights to memories at different timesteps and a dual attention interaction luo et al 2021 was developed to recall the forgotten information in the long term in this study we present a novel spatiotemporal lstm based on self attention st lstm sa integrating 3d convolution and self attention module 3d convolution deeply into rnns instead of 2d convolution aims to better perceive short term spatiotemporal features for interaction part we adopt the zigzag memory flow path of bi directional hierarchies wang 2021 self attention module is employed in the output state and the channel attention is designed to extract effective features which serves as the inputs for next step our contributions can be summarized as follows 1 we introduce the 3d convolution and attention mechanism to combine the short term spatiotemporal and long term channel information generating refined feature for the input sequence 2 by applying radar based precipitation data comprehensive experiments have been conducted we predict the reflectivity images from radar echo sequences for up to 1 3 h lead times using the developed st lstm sa model a suit of meteorological indicators is reported to measure the nowcasting accuracy at different rainfall threshold 3 ablation study is conducted to further verify the effectiveness of the model and minute level experiment aiming to nowcast the precipitation for future 1 h is also enforced which have certain theoretical support and research significance for the precise and timely precipitation forecast 2 study area and data description 2 1 study area according to the 2020 wuhan statistic yearbook there were 115 rainy days in 2019 with an accumulated rainfall of 335 6 mm in june short term and accurate rainfall forecasting can provide important support for emergency rescue flood prevention and water resources utilization for wuhan city the radar data used in this study is collected by radar station z9270 which is located at wuhan china 30 517 n 114 378 e the scanning radius of radar is 230 km containing radar echo data from 2016 to 2020 our study area covers 464 rows and 464 columns with a grid resolution of 1 km per pixel and the temporal resolution of 6 min the original echo data for each pixel is the combined reflectivity level converted from radar reflectivity at an interval of 5 dbz and the value range of which is 0 15 here we used 16 colors for visualization to distinguish different reflectivity intensities setting strong echoes close to red violet weak echoes close to blue and zero reflectivity to black the rainfall data are derived from ground based station observations of wuhan including station location information and weather observations such as temperature relative humidity and precipitation the distribution of radar sites and rainfall sites is displayed in the fig 1 2 2 data pre processing the radar echo sequences and rainfall data are both obtained from the china meteorological data website the basic radar data have been checked by quality control operations electromagnetic interference clutter ground clutter etc including reflectivity factor doppler velocity and spectral width the rainfall data are recorded every minute and radar echo data are recorded at an interval of 6 min to realize hourly level nowcasting the hourly average reflectivity level and accumulated rainfall are calculated furthermore we convert the combined reflectivity level to pixel level via p level m i n l e v e l max l e v e l m i n l e v e l p denotes the pixel level and level denotes the reflectivity level with a range of 0 1 z r relationship describes the relationship between radar reflectivity z and precipitation r austin 1987 the radar echo data in this paper divide the reflectance into different levels so it is required to establish the mathematical expression first we fit the radar reflectance and the corresponding hourly rainfall to the exponential equation z a r b where r is the rainfall intensity mm h z is the radar reflectivity factor dbz in our study a 22 17 b 0 2 there are 70 rainy days in total selected from the download address and 24 frames per day if data is continuous and non missing we expand the dataset by a sliding window with splicing continuous 6 hour reflectivity image since the three hour extrapolation is based on previous three hour observation the length of window is 6 and the slide step size is 1 besides each input with shape 464 464 1 is patched to the shape 29 29 256 we find this size works best eventually the input of model with the shape 6 29 29 256 is obtained then we divide the dataset randomly into three parts our final dataset has 718 sequences for training 232 sequences for validation and 232 sequences for test as for machine learning the 2 dimensional data is converted into 1 dimension to perform point to point prediction based on the pre processing dataset to validate the performance of st lstm sa on richer data we additionally constructed the minute level dataset each daily sequence is divided into 6 non overlapping frame blocks and we randomly assign 4 blocks for training 1 block for validation and 1 block for testing to get disjoint subsets shi 2015 likewise the data instances are sliced using a 20 frame wide sliding window and slide step is set to 1 consider the case of discontinuous echo data our final dataset contains 4890 training sequences 1235 validation and testing sequences with 20 frames long 10 for input and 10 for prediction 3 methodology 3 1 machine learning and deep learning models for comparison machine learning is an attractive alternative to make predictions on data with low computational cost yu et al 2017 in the study we construct radar derived nowcasting by using an mlr model with reflectivity as independent variable an svr model with recurrent prediction and a multi layer ann model moreover convlstm is used as the primary baseline model mlr is a regression model with one dependent variable and multiple independent variables which represents one of the most well investigated and intelligible technique valverde ramírez et al 2005 mlr is applied here to explore the linear relationship and obtain radar reflectivity on the basis of the regression formula namely 1 y α 0 α 1 x 1 α 1 x 1 α k x k where y refers to the estimate of forecast object the coefficients and constant terms α k before each term x k are required to find the best regression equation that satisfies the prediction y closest to the true value y support vector machines svm vapnik 1998 mainly deal with classification and regression by selecting the optimal hyperplane such that the distance from the sample points to the hyperplane is greater than a certain value many researchers have successfully applied svm to hydrology and meteorology such as radar derived rainfall forecasting yu et al 2017 typhoon flood forecasting lin et al 2013 many studies in which the ann model has been widely applied to practical problems such as hydro meteorological simulation and forecasting toth et al 2000 can be found mislan et al 2015 n et al 1992 the neural network model can be considered as a multiple input multiple output function that assigns computations to neurons which are grouped by layers into an input layer hidden layers and an output layer the output of nodes is obtained by connecting different weights to each node and then scaling them to the appropriate range using the activation function convlstm shi 2017 is a variant of lstm incorporating the convolution operation to extract spatial features into lstm unit that extracts temporal features and replacing the ordinary dot product operation with convolution operation to build the coding forecasting structure it has been broadly adopted in rainfall prediction kim et al 2017 or other sequence prediction lin et al 2019 3 2 spatiotemporal lstm with sa st lstm sa inspired by the self attention s success in vision and nlp tasks we propose a novel spatiotemporal model formally we record the observations periodically shi 2015 a sequence of tensor will be obtained χ χ 1 χ t the tensor χ r t h w c denotes the observations where r denotes the domain of the observed features each dimension indicates temporal depth spatial region and the number of feature map channels wang 2019a respectively for precipitation nowcasting the observation at every timestep is a 2d radar echo map whose pixel is viewed as the measurements to make information flow better deliver knowledge at different levels of recurrent layers the st lstm wang et al 2017 served as the backbone is introduced in which the memory is updated in zigzag direction apart from that 3d convolution is integrated inside the lstm unit in order to characterize local spatiotemporal motions the output state of the unit is the feature at current timestep that integrates two kinds of memories the overall architecture of st lstm sa unit is presented in fig 2 each structure unit has four inputs the initial features x or the output state of upper hidden layer h t k 1 the hidden output from previous timestep h t 1 k the memory state from previous timestep c t 1 k the spatiotemporal memory from previous lstm layer m t k 1 in st lstm sa model shown as fig 2 the model cell can be considered as two parts the red part transfers deeper memories in the horizontal direction i e at different timestep the key equations are computed as follows 2 i t σ w xi χ t w hi h t 1 k b i 3 g t t a n h w xg χ t w hg h t 1 k b g 4 f t σ w xf χ t w hf h t 1 k b f 5 c t k i t g t f t c t 1 k where is the 3d convolution operation is the element wise product and σ is the sigmoid function i t is the update gate to determine updated information compared with previous timestep forget gate f t decides the information to be discarded then the memory state is updated to get c t k at the current timestep the blue part shows the spatiotemporal memory interaction memory m in zigzag transmission wang et al 2017 represents the information flowing across various layers likewise i t is the input gate g t is the input modulation gate f t is the forget gate and o t is the output gate the output state integrates two memories c t k and m t k to perceive the spatiotemporal interactions of image sequences then we apply the 1 1 1 convolution operation for dimensionality reduction here the complete equations are shown 6 i t σ w xi χ t w mi m t k 1 b i 7 g t t a n h w xg χ t w mg m t k 1 b g 8 f t σ w xf χ t w hf m t k 1 b f 9 m t k i t g t f t m t k 1 10 o t σ w xo χ t w ho h t 1 k w mo m t k b o 11 h t k o t t a n h w 1 1 1 c t k m t k the spatiotemporal memory flow architecture is shown as fig 3 the update of current memory is not only related to the current input features and the hidden state but also depends on the memory information from previous timestep and previous layer as the information flows through the current node the layers receive and update values of memory state based on their individual dynamics to further model the long range dependency an attention mechanism is deployed directly on the outputs h b t h w c which is fed into the module and reshaped into size r b t h w c the h matrix captures the representation of input over multiple timestep and represents the aggregation of spatiotemporal features next the attention weight is defined as 12 α s o f t m a x w 2 tanh w 1 h 13 output h α where h is the transpose of h w 1 r b c t h w w 2 r b c c are the weight matrices the softmax function is applied to the matrix product operation α takes a value between 0 and 1 and represents position similarity of different channels in h then α is dotted with h to update spatiotemporal memory eventually we reshape the output into appropriate size as the final result we follow the scheduled sampling strategy bengio et al 2015 in this model which is proposed to alleviate the inconsistency of data distribution in training and testing process intuitively for the i th iteration of training algorithm we use the true frame with probability ε i or the estimate value with probability 1 ε i ε i is decided by a decreasing function such that the inputs of model favor sampling from the previous frames with iteration increasing as expected in testing process 3 3 evaluation methodology as for evaluation we selected the root mean square error rmse the peak signal to noise ratio psnr and the structural similarity index measure ssim shi 2015 for image quality evaluation to compare the forecasts and observations rmse measures the grayscale error psnr describes the severity of image distortion and ssim measures the similarity of two images at the levels of brightness contrast and structure which ranges from 1 to 1 besides the correlation is also used to measure the linear relationship between the predicts and observations given a ground truth image i and a predicted image k of size m n the calculation formulas are as follows 14 rmse 1 mn i 0 m 1 j 0 n 1 i i j k i j 2 15 mse 1 mn i 0 m 1 j 0 n 1 i i j k i j 2 16 psnr 10 l o g ma x i 2 mse where ma x i is the maximum gray value of the image i 17 l x y 2 μ x μ y c 1 μ x 2 μ y 2 c 1 c x y 2 σ x σ y c 2 σ x 2 σ y 2 c 2 s x y σ xy c 3 σ x σ y c 3 18 ssim x y l x y α c x y β s x y γ 19 ssim x y 2 μ x μ y c 1 2 σ xy c 2 μ x 2 μ y 2 c 1 σ x 2 σ y 2 c 2 20 correlation i j i i j k i j i j i i j 2 i j k i j 2 where x y are the two comparison objects l c s represent luminance contrast and structure μ x μ y are the means of x and y as the estimation of brightness σ x 2 σ y 2 are the variances of x y as the estimation of contrast σ xy is the covariance of x and y as the estimation of structure to avoid the denominator to be zero set c 1 c 2 c 3 as the constants α β γ often take the value of 1 meteorological precipitation evaluation indicators are generally derived from binary classification based on the recall precision accuracy and other indicators of binary classification many precipitation evaluation indicators are proposed concretely we selected critical success index csi schaefer 1990 false alarm rate far probability of detection pod and equitable threat score ets hamill 1999 wang 2014 to assess the experimental results the prediction and ground truth are converted to a 0 1 matrix using a certain threshold of reflectivity level these indicators are calculated as follows the values of the scoring indicators range from 0 to 1 the lower far the better and the higher the value of csi pod and ets the better 21 pod hits hits m i s s e s 22 far false a l a r m s hits f a l s e a l a r m s 23 csi hits hits f a l s e a l a r m s m i s s e s 24 dr hits f a l s e a l a r m s h i t s m i s s e s h i t s m i s s e s f a l s e a l a r m s c o r r e c t n e g a t i v e s 25 ets hits d r h i t s m i s s e s f a l s e a l a r m s d r the parameters for different cases are given in the table 1 hits means the predicted event occurred and misses means the occurred event was not predicted false alarm denotes that the predicted event did not occur and the last case denotes that neither the predicted nor the actual event occurred based on the above fitted z r relationship equation the predicted rainfall can be evaluated by converting the predicted reflectivity in radar echo map to rainfall and calculating the absolute rainfall error which is denoted as follows 26 r a i n f a l l m a e 1 n i 1 n r i r i where r i is the actual rainfall at each pixel r i is the predicted rainfall and n is the total number of image pixels rainfall mae represents the overall error of rainfall prediction 4 experimental discuss 4 1 experimental setup in our experiments svr with gauss kernel function is used for constructing the hourly rainfall forecasting model yu et al 2017 moreover the output from previous timestep is added into the input of next timestep to achieve multi step prediction as for ann three input values are connected to a three layer network the loss function is mse and batch size of each iteration is set to 4 early stopping is performed on the validation set to prevent overfitting we have tested different structures of convlstm model and the optimal convlstm stacked layers containing 64 64 64 64 nodes with 5 5 kernel size is finally constructed we use the l1 l2 loss function to obtain lower mse and higher ssim a 3d convolution layer prompting to transform the channel number is deployed at the top layer st lstm sa model is optimized with adam optimizer kingma and ba 2014 to minimize the l1 l2 loss over every pixel in the frame with a starting learning rate of 10 3 the batch size of each iteration is set to 1 the scheduled sampling iteration is set to 50000 and the sampling function is linear with 0 00002 as the decreasing factor the model architecture contains 4 layers of st lstm sas with 64 64 64 64 units and the integrated 3d conv operator is composed of a 2 5 5convolution moreover we also tried other network configurations with the state to state and input to state kernels changed to 3 3 and 9 9 and other loss functions have also been used all experiments are implemented in tensorflow abadi et al 2016 the model is validated at each epoch and the training is terminated by observing the losses and metrics in the validation set to further investigate the effectiveness of the proposed improvement we conduct ablation study in next section 4 2 overall accuracy evaluation the evaluation indexes of predicts are summarized in table 2 the predictive performance of all models is decreasing with the extension of extrapolation time among the three ml methods mlr badarpura et al 2020 has the best ssim and ann toth et al 2000 has the best rmse and psnr unexpectedly mlr performs better than svr hong 2008 the multi step predictions of ann are independent but svr suffered from problem of error accumulation which can explain svr has a larger ssim in the first hour while error increases obviously in the second two hours besides ann gives a good performance in terms of time efficiency which is more noticeable when the data size is larger st lstm sa performs better than other alternatives which shows that an appropriate interaction network structure is crucial to achieving good performance convlstm kim et al 2017 doesn t perform very well on radar echo dataset in this study the predictions presented in table 2 are the eventual and optimal results based on optimizing its parameters although convlstm is more computationally efficient than the complex spatiotemporal model the results are less usable we conduct a series of ablation studies and summarize the results in table 2 the results show the contribution of the proposed components in st lstm sa we can see that the model equipped with both 3d convolution and attention mechanism performs better there is the alternative model st lstm with only convolution module and the performance drop validates the effectiveness the integration of spatiotemporal and channel memories 4 3 spatial predictive accuracy assessment we visualize the predicted radar frames by mapping them into rgb space fig 4 illustrates the predictions for 8 00 10 00 with the inputs for 5 00 7 00 on 25 may 2019 it can be noticed that the predicted image in the first frame is the clearest and closest to the truth while the forecasts seem less spatially accurate in the second and third frames the uncertain and blurring problems are gradually more severe from fig 4 the radar echo contour predicted by convlstm is roughly accurate which smooths the predicts bonnet et al 2020 and has indefinite boundaries of different reflectivity regions the predicted value is obviously smaller especially for forecasting strong reflectivity region the prediction results of the ml methods appear to alleviate the ambiguity problem since they predict individually with pixels as the basic unit and it can be clearly seen that it has a problem of over predict areas of reflectivity ravuri 2021 apart from that for medium intensity radar reflectivity it predicts poorly while comparing with the inputs we find that the radar echo position and shape almost align with the nearest input reducing the nowcasting availability in reality st lstm sa model outperforms other methods the predictions of which match the observations better its predictions are similar to observations blurring problem also has been greatly improved in the first frame of prediction the location and shape of cavity predicted by st lstm sa is much closer to real image which provides forecasts with more location accurate contours of different levels are also closer to observations nevertheless forecasting performance of models deteriorates as shown in fig 4 by the gradual smoothing and blurring of results with increasing lead time and the fineness of details still requires to be improved in the future in fig 5 during this time period the strong echo region gradually moved eastward and the reflectivity increases locally while the reflectivity in the southwest gradually increases in this case it can be observed that convlstm still suffers from serious problems of picture blurring and smaller prediction values shi 2015 the ml methods barely predict the echo movement trend accurately and miss the prediction of strong echoes they look less accurate both on space and value as time goes on st lstm sa is relatively more accurate in predicting the shape and value of radar echo regions in the first frame the model is better able to predict the occurrence of high intensity rainfall however in the second hour st lstm sa fails to capture the features of banded structures with intense reflectivity it also did not predict the location of strong echoes relatively accurately in the third hour consequently ml and convlstm perform less well in prediction than st lstm sa which are observed to falsely deduce the coverage of radar echoes at various timesteps st lstm sa accurately predicts the radar echo motion in the future hours it does not over predict the area of strong echo region and covers the spatial extent of the echoes relatively precisely providing a good basis for practical predictions there also exists a common problem both ml and dl methods underperform in predicting strong echo motion variations over long periods of time ravuri 2021 which is a challenge to struggle in nowcasting 4 4 comparison of precipitation evaluation among different models since the above evaluations were analyzed at the pixel level we projected the results back to radar reflectivity setting reflectivity level 5 20dbz as the critical point for rain corresponding rainfall is 0 5 mm h shi 2017 we calculated the 0 1 matrix by analyzing whether the reflectivity level exceeds the defined threshold as indicated in table 3 the reliability of prediction results decreases as the extrapolation time increases bonnet et al 2020 taking the first hour of the best performance as an example in st lstm sa model 65 of the predicted events actually occurred 72 of the events occurred are correctly predicted the ml methods correctly predict the occurrence of over 45 of rainfall events but with a false alarm rate close to 40 fully demonstrating that it struggles to correctly cover the spatial extent of rainfall event though the accuracy of convlstm for predicting is less than 30 the frequency of false predictions is 36 suggesting that the model has less over predicted the occurrence of rainfall events than ml st lstm sa can still predict close to 60 of rainfall events at the third hour however other models show difficulties in predicting rainfall with prediction accuracy below 30 in summary st lstm sa model achieves the highest csi score pod score ets score and lowest far score giving better performance in order to observe the prediction ability to forecast heavier precipitation of st lstm sa we selected 5 mm h corresponding to radar reflectivity of 30 dbz as the medium rainfall threshold and calculated the corresponding binary indexes comparing table 3 and table 4 it can be found that st lstm sa has a more obvious improvement on the stronger echo region in the evaluation of the first frame st lstm sa successfully predicted 47 of rainfall events with a minimum false alarm rate of 24 the pod and far of convlstm are all close to 0 which reflect the inability of convlstm to nowcast the occurrence of moderate and heavy rainfall as for ml methods mlr s forecasting performance declines more significantly and the better model is the svr the pod is less than 35 and far is more than 40 the ml methods overestimate the distribution area of rainfall in our attempt to calculate meteorological indicators using 10 mm h as a threshold we found that the type of weather system in the samples had an impact on the results assessment since there are both small medium scale and large scale heavy rainfall events in the experimental sample here we performed a separate analysis to judge the model s responsiveness to heavy rainfall prediction during the statistical process st lstm sa has the common fault of dl models which has difficulty in predicting small scale weather patterns ravuri 2021 but for medium or large scales the prediction accuracy can reach more than 40 the ml approaches facilitate the capture of short term variation in small scale weather due to individual pixel by pixel prediction the prediction accuracy is around 40 for samples with coverage area of less than 100 pixels for 10 mm h rainfall for samples with coverage area of more than 100 grids the prediction accuracy is less than 10 convlstm is relatively consistent in these two cases and barely predicts the occurrence of heavy rainfall table 5 displays the average rainfall error of these models the rainfall error of convlstm is apparently the largest with 0 1 mm h more than st lstm sa in the 1st and 2nd hours the ml methods perform comparably in terms of rainfall error and in the third hour their results are on par with st lstm sa which is significantly better than ml in the previous two hours the results of correlation coefficients shi 2015 are highly consistent with ssim and predictions of st lstm sa are much closer to observations the boxplot and histogram of statistical rainfall error are shown in fig 6 apparently convlstm has the largest statistical error the largest error fluctuations and the highest number of outliers it seems that ml not only has more smaller rainfall errors but also many samples with rainfall errors greater than 0 4 mm h st lstm sa has a lower median and mean and is also distributed within a smaller error range overall and with relatively few outliers demonstrating the superiority of st lstm sa in all samples 4 5 experiments on minute datasets in order to better understand the performance of st lstm sa in contrast to convlstm we continue by inspecting a minute level nowcasting at ten different lead time 0 60 min table 6 shows the experimental results on minute dataset these indicators are the averages of ten frames st lstm sa clearly delivers the best predicts making an improvement of 4 over convlstm in terms of ssim the difference of rainfall error also demonstrates the better versatility of st lstm sa for the csi metrics up to the threshold of 5dbz 20dbz and 30dbz the performance decreases with increasing thresholds our model outperforms the baseline at any lead time the superiority of st lstm sa over convlstm appears to be highest for the lead time from 0 min to 30 min the predict performance drops continuously with long term extrapolation to visually compare the predicts of the two models we depict in fig 7 to demonstrate the superiority of spatiotemporal interaction both the two models produce precipitation variability decreasing with increasing lead time ravuri 2021 we can see the evolution of radar echoes at every timestep and st lstm sa is able to capture the highly non stationary process while convlstm only roughly predicts the outline of radar echoes owing to aggressive blurring and miss intensity in this showcase the echoes below aggregate and accumulate to form a strip echo and the upper area of stronger echoes expand gradually convlstm fails to capture the banded echo structure with intense reflectivity and covers a smaller rainfall region in contrast to observations the predicts of st lstm sa reflect the strong echo better at the early timestep but the model fails to predict the local strong echo beyond 30 min furthermore over predicting areas of rain will happen especially for small scale echo structure 5 conclusion instead of physically modeling the complex factors involved in the atmospheric evolution of precipitation agrawal et al 2019 spatiotemporal forecasting models treated precipitation nowcasting as a data driven problem demonstrating good performance the several ml models built in our study obtain relatively better prediction results ml models can predict over 40 of rainfall events and forecast well on small scale intense rainfall meanwhile it triggers more false alarms shi 2015 and is less available in practice due to the reliance on each pixel location individually and ignore the strong spatiotemporal correlation ml methods are suitable for small scale and short term prediction but fail to model the fast and large scale echo movement convlstm shi 2017 on hourly and minute datasets performs poorly with serious drawbacks of blurred images and small predicted values it is inferior to ml in both meteorological metrics and rainfall evaluation st lstm sa performs optimally in all indicators and is more consistent with observations moreover the ablation study shows the effectiveness of proposed modules spatiotemporal patterns can be learned by 3d convolution self attention and spatiotemporal memory interaction in the evaluation of meteorological indicators st lstm sa can successfully predict more than 60 of rainfall events and more than 40 of moderate rainfall events for heavier precipitation st lstm sa responds better to large scale patterns and provides accurate short term predictions at hourly lead times yet there remains challenges for our model to forecast heavy precipitation at long lead time rainfall prediction is a sparse problem mukhopadhyay et al 2019 we selected rainy days firstly to construct the training and validating datasets in our study but an unbalanced distribution of rainfall data at different scales will also result in the inaccurate evaluation the robust quantitative measurements aligned with operational utility are required ravuri 2021 to reduce the influence of data distribution to sum up this paper presented a st lstm model with sa mechanism and predicted radar images up to 3 hour successfully based on radar echo images of wuhan station other comparative experiments are set up to demonstrate the practicality of the model for rainfall applications on the side with the support of self attention channel correlations are modeled st lstm sa model obtains better spatiotemporal representation of both channel and temporal information luo et al 2021 as a complex weather format generation mechanism and inherent physical laws of rainfall are difficult to describe and model specifically in future we will train st lstm sa on larger datasets with respect to other popular meteorological data prudden et al 2020 other mechanism like transformer and residual connections may be introduced to focus on solving long standing problem for many dl nowcasting models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the national key r d program 2018yfb2100500 of china the major program of the national natural science foundation of china 41890822 thefundamentalresearchfundsforthecentraluniversities chinauniversityofgeosciences wuhan no 162301212687 and the national key research and development program for young scientist 2021yff0704400 
3406,accurate and timely short term forecasting services of precipitation variable are significant for people s lives and property security the data driven approaches demonstrate promising performance in the extrapolation of precipitation in this paper we proposed a spatiotemporal prediction model namely the spatial temporal long short term memory based on the self attentive mechanism st lstm sa this model enables better aggregation of sequence features inspired by proposed improvements the encapsulated 3d convolution is developed to fully exploit the short term spatiotemporal information and the channel correlation is modeled by self attention mechanism to further improve representations in the long term interaction the effectiveness of which is validated in the ablation study comprehensive experiments have been conducted on the radar echo sequence we successfully predicted future radar reflectivity images for next 3 h with data for previous three hours as inputs in wuhan china three machine learning methods multiple linear regression mlr support vector regression svr and artificial neural networks ann and deep learning model convlstm have been chosen as comparative groups to corroborate the nowcasting availability of this model for radar echo extrapolation moreover the supplementary experiment on minute dataset also illustrated the superiority of st lstm sa the studies analyzed the forecasting performance in terms of image quality and rainfall error the experimental results demonstrated the better versatility and performance of st lstm sa these conclusions and attempts may provide efficient guidance for precipitation nowcasting in urban areas keywords precipitation nowcasting spatiotemporal lstm self attention radar echo image 1 introduction high resolution precipitation nowcasting is a crucial issue in weather forecasts which is especially useful for the prevention and management of droughts and floods disasters xu et al 2018a to reduce socioeconomic impacts prudden et al 2020 precipitation nowcasting is a short term 0 6 h forecasting of local rainfall intensity based on rainfall related data in previous hours such as weather radar or meteorological satellite data lingling 2018 compared with medium and long term weather forecasting nowcasting is instructive for grasping current weather patterns within a relatively shorter forecast time fengru 2020 weather radar observations provide precipitation maps with high spatial 1 km and temporal 5 min resolution reyniers 2008 well suited as the data source for precipitation nowcasting in the past the field of radar based precipitation nowcasting mainly utilized numerical weather predictions nwp and optical flow algorithm and problems resulting from which like scale dependence and certain limitations are difficult to eliminate prudden et al 2020 sun 2014 yano 2018 the applications of statistical machine learning ml and deep learning dl in hydrometeorology xu et al 2020 develop a new idea of data driven forecasting in earth system science hua and simovici 2016 xu et al 2021 xu et al 2018b and provide a promising direction to make optimal use of the high resolution observations prudden et al 2020 the flexibility and powerful representation ability of deep learning can capture the non linearity and randomness of meteorological data as well as improve the timeliness and accuracy of forecasting recent advances in precipitation forecast offer different insights on how to predict future sequences based on radar data in the early stage the neural network models were mostly used with meteorological factors surrounding pixels in radar maps shangzan et al 2017 or rainfall sequences deshpande 2012 as inputs the introduction of convolutional neural networks cnn allowed us to better extract features from large amounts of image data and offer great generalization capability bengio et al 2007 long et al 2017 sadeghi 2019 a multi channel 3d cube successive convolution network 3d csn was constructed zhuang and ding 2016 which leveraged both raw 3d radar and other meteorological data directly to predict specific location of rainfall in addition a 3 d convolutional neural network 3d cnn model was proposed to nowcast extrapolation based precipitation with lead time up to 10 min using unique 3 d radar data kim et al 2021 furthermore precipitation forecasting was treated as an image to image translation problem in u net convolutional neural network agrawal et al 2019 ronneberger et al 2015 rainnet ayzel et al 2020 was designed inspired by u net to achieve nowcasting for up to a lead time of 1 h further studies also confirmed the potential of cnn for radar based precipitation nowcasting choi et al 2021 zhang et al 2017 zheng 2021 to capture long term dependency between distant frames wang 2021 a sequence to sequence sutskever et al 2014 long short term memory lstm model derived from natural language processing nlp sutskever et al 2011 has been applied into meteorology prediction the paper firstly drawing upon the ideas of spatiotemporal forecast in the field of radar based precipitation proposed a convolutional long short term memory convlstm model shi 2015 extending the spatial features based on lstm by adding 2d convolution to improve the location invariant structure in convlstm trajectory gated recurrent unit trajgru algorithm composed of convolution and gru cho et al 2014 was proposed chen et al 2020 which added the learnable convolution to learn spatial variation of images to jointly model information interaction of spatiotemporal correlations wang 2021 predrnn was defined with a spatiotemporal lstm building block to update memory state over both time and space bonnet et al 2020 wang et al 2017 as an improvement method predrnn wang et al 2018b was constructed by causal lstm with cascaded mechanism and gradient highway unit ghu to build spatiotemporal network and alleviate the problem of gradient vanishing by applying differencing operations on the non stationary and approximately stationary properties in spatiotemporal dynamics memory in memory mim networks wang 2019b were proposed and applied to time series forecasting tasks a new curriculum learning strategy wang 2021 was introduced into predrnn to learn long term dynamics which can cover different aspects of spatiotemporal representations in summary combining the merits of convolutional and recurrent architectures underpinned the follow up research work kim et al 2017 tran and song 2019 video prediction algorithms combining computer vision with temporal features have been diversely used for action recognition liu et al 2017 detecting abnormal activities feng 2021 and motion prediction villegas et al 2017a etc essentially radar echo extrapolation as with frame level prediction on videos is employed to perform future frame prediction in fact video prediction algorithms have shown good possibilities wang et al 2018b wang 2019a wang et al 2017 for radar echo extrapolation bonnet et al 2020 devoted to long term and finer prediction video prediction has made impressive progress network with motion and content separation villegas et al 2017a was proposed to model the spatiotemporal dynamics and the hierarchical approach villegas et al 2017b wichers et al 2018 facilitates longer term predictions of future frames novel ideas for precipitation nowcasting may be motivated by recent advances in video prediction self attention sa vaswani 2017 is an attention mechanism that computes weights at different positions based on its own sequence it has been shown to be impactful in machine reading cheng et al 2016 computer vision huang 2019 wang et al 2018a and even the field of point cloud guo et al 2021 etc it infers image information by focusing more on recognizable regions of the image or accomplishes semantic transformation by focusing on specific words in long sentences compared to convolution operation sa module is rather efficient to aggregate historical consecutive sequences convlstm with self attention memory module was designed to fuse features from previous frames lin et al 2020 a recall gate partially motivated by attention mechanism to recall long range memory was introduced into the model called e3d lstm wang 2019a connecting the current state with the previous memories and assigning different weights to memories at different timesteps and a dual attention interaction luo et al 2021 was developed to recall the forgotten information in the long term in this study we present a novel spatiotemporal lstm based on self attention st lstm sa integrating 3d convolution and self attention module 3d convolution deeply into rnns instead of 2d convolution aims to better perceive short term spatiotemporal features for interaction part we adopt the zigzag memory flow path of bi directional hierarchies wang 2021 self attention module is employed in the output state and the channel attention is designed to extract effective features which serves as the inputs for next step our contributions can be summarized as follows 1 we introduce the 3d convolution and attention mechanism to combine the short term spatiotemporal and long term channel information generating refined feature for the input sequence 2 by applying radar based precipitation data comprehensive experiments have been conducted we predict the reflectivity images from radar echo sequences for up to 1 3 h lead times using the developed st lstm sa model a suit of meteorological indicators is reported to measure the nowcasting accuracy at different rainfall threshold 3 ablation study is conducted to further verify the effectiveness of the model and minute level experiment aiming to nowcast the precipitation for future 1 h is also enforced which have certain theoretical support and research significance for the precise and timely precipitation forecast 2 study area and data description 2 1 study area according to the 2020 wuhan statistic yearbook there were 115 rainy days in 2019 with an accumulated rainfall of 335 6 mm in june short term and accurate rainfall forecasting can provide important support for emergency rescue flood prevention and water resources utilization for wuhan city the radar data used in this study is collected by radar station z9270 which is located at wuhan china 30 517 n 114 378 e the scanning radius of radar is 230 km containing radar echo data from 2016 to 2020 our study area covers 464 rows and 464 columns with a grid resolution of 1 km per pixel and the temporal resolution of 6 min the original echo data for each pixel is the combined reflectivity level converted from radar reflectivity at an interval of 5 dbz and the value range of which is 0 15 here we used 16 colors for visualization to distinguish different reflectivity intensities setting strong echoes close to red violet weak echoes close to blue and zero reflectivity to black the rainfall data are derived from ground based station observations of wuhan including station location information and weather observations such as temperature relative humidity and precipitation the distribution of radar sites and rainfall sites is displayed in the fig 1 2 2 data pre processing the radar echo sequences and rainfall data are both obtained from the china meteorological data website the basic radar data have been checked by quality control operations electromagnetic interference clutter ground clutter etc including reflectivity factor doppler velocity and spectral width the rainfall data are recorded every minute and radar echo data are recorded at an interval of 6 min to realize hourly level nowcasting the hourly average reflectivity level and accumulated rainfall are calculated furthermore we convert the combined reflectivity level to pixel level via p level m i n l e v e l max l e v e l m i n l e v e l p denotes the pixel level and level denotes the reflectivity level with a range of 0 1 z r relationship describes the relationship between radar reflectivity z and precipitation r austin 1987 the radar echo data in this paper divide the reflectance into different levels so it is required to establish the mathematical expression first we fit the radar reflectance and the corresponding hourly rainfall to the exponential equation z a r b where r is the rainfall intensity mm h z is the radar reflectivity factor dbz in our study a 22 17 b 0 2 there are 70 rainy days in total selected from the download address and 24 frames per day if data is continuous and non missing we expand the dataset by a sliding window with splicing continuous 6 hour reflectivity image since the three hour extrapolation is based on previous three hour observation the length of window is 6 and the slide step size is 1 besides each input with shape 464 464 1 is patched to the shape 29 29 256 we find this size works best eventually the input of model with the shape 6 29 29 256 is obtained then we divide the dataset randomly into three parts our final dataset has 718 sequences for training 232 sequences for validation and 232 sequences for test as for machine learning the 2 dimensional data is converted into 1 dimension to perform point to point prediction based on the pre processing dataset to validate the performance of st lstm sa on richer data we additionally constructed the minute level dataset each daily sequence is divided into 6 non overlapping frame blocks and we randomly assign 4 blocks for training 1 block for validation and 1 block for testing to get disjoint subsets shi 2015 likewise the data instances are sliced using a 20 frame wide sliding window and slide step is set to 1 consider the case of discontinuous echo data our final dataset contains 4890 training sequences 1235 validation and testing sequences with 20 frames long 10 for input and 10 for prediction 3 methodology 3 1 machine learning and deep learning models for comparison machine learning is an attractive alternative to make predictions on data with low computational cost yu et al 2017 in the study we construct radar derived nowcasting by using an mlr model with reflectivity as independent variable an svr model with recurrent prediction and a multi layer ann model moreover convlstm is used as the primary baseline model mlr is a regression model with one dependent variable and multiple independent variables which represents one of the most well investigated and intelligible technique valverde ramírez et al 2005 mlr is applied here to explore the linear relationship and obtain radar reflectivity on the basis of the regression formula namely 1 y α 0 α 1 x 1 α 1 x 1 α k x k where y refers to the estimate of forecast object the coefficients and constant terms α k before each term x k are required to find the best regression equation that satisfies the prediction y closest to the true value y support vector machines svm vapnik 1998 mainly deal with classification and regression by selecting the optimal hyperplane such that the distance from the sample points to the hyperplane is greater than a certain value many researchers have successfully applied svm to hydrology and meteorology such as radar derived rainfall forecasting yu et al 2017 typhoon flood forecasting lin et al 2013 many studies in which the ann model has been widely applied to practical problems such as hydro meteorological simulation and forecasting toth et al 2000 can be found mislan et al 2015 n et al 1992 the neural network model can be considered as a multiple input multiple output function that assigns computations to neurons which are grouped by layers into an input layer hidden layers and an output layer the output of nodes is obtained by connecting different weights to each node and then scaling them to the appropriate range using the activation function convlstm shi 2017 is a variant of lstm incorporating the convolution operation to extract spatial features into lstm unit that extracts temporal features and replacing the ordinary dot product operation with convolution operation to build the coding forecasting structure it has been broadly adopted in rainfall prediction kim et al 2017 or other sequence prediction lin et al 2019 3 2 spatiotemporal lstm with sa st lstm sa inspired by the self attention s success in vision and nlp tasks we propose a novel spatiotemporal model formally we record the observations periodically shi 2015 a sequence of tensor will be obtained χ χ 1 χ t the tensor χ r t h w c denotes the observations where r denotes the domain of the observed features each dimension indicates temporal depth spatial region and the number of feature map channels wang 2019a respectively for precipitation nowcasting the observation at every timestep is a 2d radar echo map whose pixel is viewed as the measurements to make information flow better deliver knowledge at different levels of recurrent layers the st lstm wang et al 2017 served as the backbone is introduced in which the memory is updated in zigzag direction apart from that 3d convolution is integrated inside the lstm unit in order to characterize local spatiotemporal motions the output state of the unit is the feature at current timestep that integrates two kinds of memories the overall architecture of st lstm sa unit is presented in fig 2 each structure unit has four inputs the initial features x or the output state of upper hidden layer h t k 1 the hidden output from previous timestep h t 1 k the memory state from previous timestep c t 1 k the spatiotemporal memory from previous lstm layer m t k 1 in st lstm sa model shown as fig 2 the model cell can be considered as two parts the red part transfers deeper memories in the horizontal direction i e at different timestep the key equations are computed as follows 2 i t σ w xi χ t w hi h t 1 k b i 3 g t t a n h w xg χ t w hg h t 1 k b g 4 f t σ w xf χ t w hf h t 1 k b f 5 c t k i t g t f t c t 1 k where is the 3d convolution operation is the element wise product and σ is the sigmoid function i t is the update gate to determine updated information compared with previous timestep forget gate f t decides the information to be discarded then the memory state is updated to get c t k at the current timestep the blue part shows the spatiotemporal memory interaction memory m in zigzag transmission wang et al 2017 represents the information flowing across various layers likewise i t is the input gate g t is the input modulation gate f t is the forget gate and o t is the output gate the output state integrates two memories c t k and m t k to perceive the spatiotemporal interactions of image sequences then we apply the 1 1 1 convolution operation for dimensionality reduction here the complete equations are shown 6 i t σ w xi χ t w mi m t k 1 b i 7 g t t a n h w xg χ t w mg m t k 1 b g 8 f t σ w xf χ t w hf m t k 1 b f 9 m t k i t g t f t m t k 1 10 o t σ w xo χ t w ho h t 1 k w mo m t k b o 11 h t k o t t a n h w 1 1 1 c t k m t k the spatiotemporal memory flow architecture is shown as fig 3 the update of current memory is not only related to the current input features and the hidden state but also depends on the memory information from previous timestep and previous layer as the information flows through the current node the layers receive and update values of memory state based on their individual dynamics to further model the long range dependency an attention mechanism is deployed directly on the outputs h b t h w c which is fed into the module and reshaped into size r b t h w c the h matrix captures the representation of input over multiple timestep and represents the aggregation of spatiotemporal features next the attention weight is defined as 12 α s o f t m a x w 2 tanh w 1 h 13 output h α where h is the transpose of h w 1 r b c t h w w 2 r b c c are the weight matrices the softmax function is applied to the matrix product operation α takes a value between 0 and 1 and represents position similarity of different channels in h then α is dotted with h to update spatiotemporal memory eventually we reshape the output into appropriate size as the final result we follow the scheduled sampling strategy bengio et al 2015 in this model which is proposed to alleviate the inconsistency of data distribution in training and testing process intuitively for the i th iteration of training algorithm we use the true frame with probability ε i or the estimate value with probability 1 ε i ε i is decided by a decreasing function such that the inputs of model favor sampling from the previous frames with iteration increasing as expected in testing process 3 3 evaluation methodology as for evaluation we selected the root mean square error rmse the peak signal to noise ratio psnr and the structural similarity index measure ssim shi 2015 for image quality evaluation to compare the forecasts and observations rmse measures the grayscale error psnr describes the severity of image distortion and ssim measures the similarity of two images at the levels of brightness contrast and structure which ranges from 1 to 1 besides the correlation is also used to measure the linear relationship between the predicts and observations given a ground truth image i and a predicted image k of size m n the calculation formulas are as follows 14 rmse 1 mn i 0 m 1 j 0 n 1 i i j k i j 2 15 mse 1 mn i 0 m 1 j 0 n 1 i i j k i j 2 16 psnr 10 l o g ma x i 2 mse where ma x i is the maximum gray value of the image i 17 l x y 2 μ x μ y c 1 μ x 2 μ y 2 c 1 c x y 2 σ x σ y c 2 σ x 2 σ y 2 c 2 s x y σ xy c 3 σ x σ y c 3 18 ssim x y l x y α c x y β s x y γ 19 ssim x y 2 μ x μ y c 1 2 σ xy c 2 μ x 2 μ y 2 c 1 σ x 2 σ y 2 c 2 20 correlation i j i i j k i j i j i i j 2 i j k i j 2 where x y are the two comparison objects l c s represent luminance contrast and structure μ x μ y are the means of x and y as the estimation of brightness σ x 2 σ y 2 are the variances of x y as the estimation of contrast σ xy is the covariance of x and y as the estimation of structure to avoid the denominator to be zero set c 1 c 2 c 3 as the constants α β γ often take the value of 1 meteorological precipitation evaluation indicators are generally derived from binary classification based on the recall precision accuracy and other indicators of binary classification many precipitation evaluation indicators are proposed concretely we selected critical success index csi schaefer 1990 false alarm rate far probability of detection pod and equitable threat score ets hamill 1999 wang 2014 to assess the experimental results the prediction and ground truth are converted to a 0 1 matrix using a certain threshold of reflectivity level these indicators are calculated as follows the values of the scoring indicators range from 0 to 1 the lower far the better and the higher the value of csi pod and ets the better 21 pod hits hits m i s s e s 22 far false a l a r m s hits f a l s e a l a r m s 23 csi hits hits f a l s e a l a r m s m i s s e s 24 dr hits f a l s e a l a r m s h i t s m i s s e s h i t s m i s s e s f a l s e a l a r m s c o r r e c t n e g a t i v e s 25 ets hits d r h i t s m i s s e s f a l s e a l a r m s d r the parameters for different cases are given in the table 1 hits means the predicted event occurred and misses means the occurred event was not predicted false alarm denotes that the predicted event did not occur and the last case denotes that neither the predicted nor the actual event occurred based on the above fitted z r relationship equation the predicted rainfall can be evaluated by converting the predicted reflectivity in radar echo map to rainfall and calculating the absolute rainfall error which is denoted as follows 26 r a i n f a l l m a e 1 n i 1 n r i r i where r i is the actual rainfall at each pixel r i is the predicted rainfall and n is the total number of image pixels rainfall mae represents the overall error of rainfall prediction 4 experimental discuss 4 1 experimental setup in our experiments svr with gauss kernel function is used for constructing the hourly rainfall forecasting model yu et al 2017 moreover the output from previous timestep is added into the input of next timestep to achieve multi step prediction as for ann three input values are connected to a three layer network the loss function is mse and batch size of each iteration is set to 4 early stopping is performed on the validation set to prevent overfitting we have tested different structures of convlstm model and the optimal convlstm stacked layers containing 64 64 64 64 nodes with 5 5 kernel size is finally constructed we use the l1 l2 loss function to obtain lower mse and higher ssim a 3d convolution layer prompting to transform the channel number is deployed at the top layer st lstm sa model is optimized with adam optimizer kingma and ba 2014 to minimize the l1 l2 loss over every pixel in the frame with a starting learning rate of 10 3 the batch size of each iteration is set to 1 the scheduled sampling iteration is set to 50000 and the sampling function is linear with 0 00002 as the decreasing factor the model architecture contains 4 layers of st lstm sas with 64 64 64 64 units and the integrated 3d conv operator is composed of a 2 5 5convolution moreover we also tried other network configurations with the state to state and input to state kernels changed to 3 3 and 9 9 and other loss functions have also been used all experiments are implemented in tensorflow abadi et al 2016 the model is validated at each epoch and the training is terminated by observing the losses and metrics in the validation set to further investigate the effectiveness of the proposed improvement we conduct ablation study in next section 4 2 overall accuracy evaluation the evaluation indexes of predicts are summarized in table 2 the predictive performance of all models is decreasing with the extension of extrapolation time among the three ml methods mlr badarpura et al 2020 has the best ssim and ann toth et al 2000 has the best rmse and psnr unexpectedly mlr performs better than svr hong 2008 the multi step predictions of ann are independent but svr suffered from problem of error accumulation which can explain svr has a larger ssim in the first hour while error increases obviously in the second two hours besides ann gives a good performance in terms of time efficiency which is more noticeable when the data size is larger st lstm sa performs better than other alternatives which shows that an appropriate interaction network structure is crucial to achieving good performance convlstm kim et al 2017 doesn t perform very well on radar echo dataset in this study the predictions presented in table 2 are the eventual and optimal results based on optimizing its parameters although convlstm is more computationally efficient than the complex spatiotemporal model the results are less usable we conduct a series of ablation studies and summarize the results in table 2 the results show the contribution of the proposed components in st lstm sa we can see that the model equipped with both 3d convolution and attention mechanism performs better there is the alternative model st lstm with only convolution module and the performance drop validates the effectiveness the integration of spatiotemporal and channel memories 4 3 spatial predictive accuracy assessment we visualize the predicted radar frames by mapping them into rgb space fig 4 illustrates the predictions for 8 00 10 00 with the inputs for 5 00 7 00 on 25 may 2019 it can be noticed that the predicted image in the first frame is the clearest and closest to the truth while the forecasts seem less spatially accurate in the second and third frames the uncertain and blurring problems are gradually more severe from fig 4 the radar echo contour predicted by convlstm is roughly accurate which smooths the predicts bonnet et al 2020 and has indefinite boundaries of different reflectivity regions the predicted value is obviously smaller especially for forecasting strong reflectivity region the prediction results of the ml methods appear to alleviate the ambiguity problem since they predict individually with pixels as the basic unit and it can be clearly seen that it has a problem of over predict areas of reflectivity ravuri 2021 apart from that for medium intensity radar reflectivity it predicts poorly while comparing with the inputs we find that the radar echo position and shape almost align with the nearest input reducing the nowcasting availability in reality st lstm sa model outperforms other methods the predictions of which match the observations better its predictions are similar to observations blurring problem also has been greatly improved in the first frame of prediction the location and shape of cavity predicted by st lstm sa is much closer to real image which provides forecasts with more location accurate contours of different levels are also closer to observations nevertheless forecasting performance of models deteriorates as shown in fig 4 by the gradual smoothing and blurring of results with increasing lead time and the fineness of details still requires to be improved in the future in fig 5 during this time period the strong echo region gradually moved eastward and the reflectivity increases locally while the reflectivity in the southwest gradually increases in this case it can be observed that convlstm still suffers from serious problems of picture blurring and smaller prediction values shi 2015 the ml methods barely predict the echo movement trend accurately and miss the prediction of strong echoes they look less accurate both on space and value as time goes on st lstm sa is relatively more accurate in predicting the shape and value of radar echo regions in the first frame the model is better able to predict the occurrence of high intensity rainfall however in the second hour st lstm sa fails to capture the features of banded structures with intense reflectivity it also did not predict the location of strong echoes relatively accurately in the third hour consequently ml and convlstm perform less well in prediction than st lstm sa which are observed to falsely deduce the coverage of radar echoes at various timesteps st lstm sa accurately predicts the radar echo motion in the future hours it does not over predict the area of strong echo region and covers the spatial extent of the echoes relatively precisely providing a good basis for practical predictions there also exists a common problem both ml and dl methods underperform in predicting strong echo motion variations over long periods of time ravuri 2021 which is a challenge to struggle in nowcasting 4 4 comparison of precipitation evaluation among different models since the above evaluations were analyzed at the pixel level we projected the results back to radar reflectivity setting reflectivity level 5 20dbz as the critical point for rain corresponding rainfall is 0 5 mm h shi 2017 we calculated the 0 1 matrix by analyzing whether the reflectivity level exceeds the defined threshold as indicated in table 3 the reliability of prediction results decreases as the extrapolation time increases bonnet et al 2020 taking the first hour of the best performance as an example in st lstm sa model 65 of the predicted events actually occurred 72 of the events occurred are correctly predicted the ml methods correctly predict the occurrence of over 45 of rainfall events but with a false alarm rate close to 40 fully demonstrating that it struggles to correctly cover the spatial extent of rainfall event though the accuracy of convlstm for predicting is less than 30 the frequency of false predictions is 36 suggesting that the model has less over predicted the occurrence of rainfall events than ml st lstm sa can still predict close to 60 of rainfall events at the third hour however other models show difficulties in predicting rainfall with prediction accuracy below 30 in summary st lstm sa model achieves the highest csi score pod score ets score and lowest far score giving better performance in order to observe the prediction ability to forecast heavier precipitation of st lstm sa we selected 5 mm h corresponding to radar reflectivity of 30 dbz as the medium rainfall threshold and calculated the corresponding binary indexes comparing table 3 and table 4 it can be found that st lstm sa has a more obvious improvement on the stronger echo region in the evaluation of the first frame st lstm sa successfully predicted 47 of rainfall events with a minimum false alarm rate of 24 the pod and far of convlstm are all close to 0 which reflect the inability of convlstm to nowcast the occurrence of moderate and heavy rainfall as for ml methods mlr s forecasting performance declines more significantly and the better model is the svr the pod is less than 35 and far is more than 40 the ml methods overestimate the distribution area of rainfall in our attempt to calculate meteorological indicators using 10 mm h as a threshold we found that the type of weather system in the samples had an impact on the results assessment since there are both small medium scale and large scale heavy rainfall events in the experimental sample here we performed a separate analysis to judge the model s responsiveness to heavy rainfall prediction during the statistical process st lstm sa has the common fault of dl models which has difficulty in predicting small scale weather patterns ravuri 2021 but for medium or large scales the prediction accuracy can reach more than 40 the ml approaches facilitate the capture of short term variation in small scale weather due to individual pixel by pixel prediction the prediction accuracy is around 40 for samples with coverage area of less than 100 pixels for 10 mm h rainfall for samples with coverage area of more than 100 grids the prediction accuracy is less than 10 convlstm is relatively consistent in these two cases and barely predicts the occurrence of heavy rainfall table 5 displays the average rainfall error of these models the rainfall error of convlstm is apparently the largest with 0 1 mm h more than st lstm sa in the 1st and 2nd hours the ml methods perform comparably in terms of rainfall error and in the third hour their results are on par with st lstm sa which is significantly better than ml in the previous two hours the results of correlation coefficients shi 2015 are highly consistent with ssim and predictions of st lstm sa are much closer to observations the boxplot and histogram of statistical rainfall error are shown in fig 6 apparently convlstm has the largest statistical error the largest error fluctuations and the highest number of outliers it seems that ml not only has more smaller rainfall errors but also many samples with rainfall errors greater than 0 4 mm h st lstm sa has a lower median and mean and is also distributed within a smaller error range overall and with relatively few outliers demonstrating the superiority of st lstm sa in all samples 4 5 experiments on minute datasets in order to better understand the performance of st lstm sa in contrast to convlstm we continue by inspecting a minute level nowcasting at ten different lead time 0 60 min table 6 shows the experimental results on minute dataset these indicators are the averages of ten frames st lstm sa clearly delivers the best predicts making an improvement of 4 over convlstm in terms of ssim the difference of rainfall error also demonstrates the better versatility of st lstm sa for the csi metrics up to the threshold of 5dbz 20dbz and 30dbz the performance decreases with increasing thresholds our model outperforms the baseline at any lead time the superiority of st lstm sa over convlstm appears to be highest for the lead time from 0 min to 30 min the predict performance drops continuously with long term extrapolation to visually compare the predicts of the two models we depict in fig 7 to demonstrate the superiority of spatiotemporal interaction both the two models produce precipitation variability decreasing with increasing lead time ravuri 2021 we can see the evolution of radar echoes at every timestep and st lstm sa is able to capture the highly non stationary process while convlstm only roughly predicts the outline of radar echoes owing to aggressive blurring and miss intensity in this showcase the echoes below aggregate and accumulate to form a strip echo and the upper area of stronger echoes expand gradually convlstm fails to capture the banded echo structure with intense reflectivity and covers a smaller rainfall region in contrast to observations the predicts of st lstm sa reflect the strong echo better at the early timestep but the model fails to predict the local strong echo beyond 30 min furthermore over predicting areas of rain will happen especially for small scale echo structure 5 conclusion instead of physically modeling the complex factors involved in the atmospheric evolution of precipitation agrawal et al 2019 spatiotemporal forecasting models treated precipitation nowcasting as a data driven problem demonstrating good performance the several ml models built in our study obtain relatively better prediction results ml models can predict over 40 of rainfall events and forecast well on small scale intense rainfall meanwhile it triggers more false alarms shi 2015 and is less available in practice due to the reliance on each pixel location individually and ignore the strong spatiotemporal correlation ml methods are suitable for small scale and short term prediction but fail to model the fast and large scale echo movement convlstm shi 2017 on hourly and minute datasets performs poorly with serious drawbacks of blurred images and small predicted values it is inferior to ml in both meteorological metrics and rainfall evaluation st lstm sa performs optimally in all indicators and is more consistent with observations moreover the ablation study shows the effectiveness of proposed modules spatiotemporal patterns can be learned by 3d convolution self attention and spatiotemporal memory interaction in the evaluation of meteorological indicators st lstm sa can successfully predict more than 60 of rainfall events and more than 40 of moderate rainfall events for heavier precipitation st lstm sa responds better to large scale patterns and provides accurate short term predictions at hourly lead times yet there remains challenges for our model to forecast heavy precipitation at long lead time rainfall prediction is a sparse problem mukhopadhyay et al 2019 we selected rainy days firstly to construct the training and validating datasets in our study but an unbalanced distribution of rainfall data at different scales will also result in the inaccurate evaluation the robust quantitative measurements aligned with operational utility are required ravuri 2021 to reduce the influence of data distribution to sum up this paper presented a st lstm model with sa mechanism and predicted radar images up to 3 hour successfully based on radar echo images of wuhan station other comparative experiments are set up to demonstrate the practicality of the model for rainfall applications on the side with the support of self attention channel correlations are modeled st lstm sa model obtains better spatiotemporal representation of both channel and temporal information luo et al 2021 as a complex weather format generation mechanism and inherent physical laws of rainfall are difficult to describe and model specifically in future we will train st lstm sa on larger datasets with respect to other popular meteorological data prudden et al 2020 other mechanism like transformer and residual connections may be introduced to focus on solving long standing problem for many dl nowcasting models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the national key r d program 2018yfb2100500 of china the major program of the national natural science foundation of china 41890822 thefundamentalresearchfundsforthecentraluniversities chinauniversityofgeosciences wuhan no 162301212687 and the national key research and development program for young scientist 2021yff0704400 
3407,the process of sediment transport through a karst aquifer is investigated by means of time series analysis the correlation and spectral analyses are used for determination of the temporal characteristics of this process such as timing synchronization with discharge from spring and time delay between arrivals of sediment and infiltrated rainwater to spring the partial correlation analysis is used for the spatial characterization the study area is the catchment of jadro spring in croatia the analyzed hourly time series are discharge water temperature electrical conductivity and nephelometric turbidity recorded at the source of this spring as well as the rainfall collected at three locations in the catchment of spring they cover the period of four years from 2017 to 2020 the main aims of this research are 1 presentation of the methods of time series analysis that can be used for the investigation of sediment transport through karst aquifer 2 analysis of the applicability of partial cross correlation functions for the spatial characterization of sediment transport process and 3 extension of the existing knowledge about the sediment in water from karst springs with a special reference to the jadro spring the obtained results show that the partial cross correlation function can be very useful in the investigation of mass transport through the karst aquifer they enable identification of the specific contribution of each rain gauge location to the quantity and quality of spring water specifically the origin of sediment in water from jadro spring is determined in this study as well as the origin of quick flow and baseflow component in discharge from this spring including the timing of fresh rainwater arrival rainwater withdrawal and emptying of previously accumulated water in vadose and phreatic zone the sediment transport through the karst aquifer of jadro spring is initiated after heavy rainfall in the central and eastern part of the catchment the origin of sediment in spring water is dominantly the remobilized autochthonous sediment from aquifer and possible limited contribution of allochthonous sediment from the part of catchment located in hinterland of spring keywords karst hydrology correlation and spectral analysis partial cross correlation function turbidity sediment transport jadro spring 1 introduction karst aquifers are vital groundwater resources that provide drinking water for a significant number of the world s population and that support agriculture groundwater dependent activities and ecosystems olarinoye et al 2020 they have complex hydrological and hydrogeological characteristics due to the complicated network of underground karst voids that consist of conduits fractures and fissures within low permeability matrix bakalowicz 2005 karst springs most frequently appear at the contact between permeable carbonate rocks and impermeable layers bimodal behavior is a basic characteristic of karst aquifers two different hydraulic phases can be recognized during and after rainfall the first is mostly governed by a pressure pulse mechanism occurring due to infiltration which produces a rapid increase in flow rate the second represents system emptying or recession flow that is not influenced by infiltration e g grasso et al 2003 concentrated turbulent flow takes place in conduits while diffuse laminar flow prevails in fractures fissures and low permeability matrix e g liedl et al 2003 consequently a quick flow component and a baseflow component can be recognized in the karst spring hydrographs e g atkinson 1977 geyer et al 2008 sometimes two components of groundwater recharge can also be distinguished depending on the origin of water autogenic recharge by rainwater from the karst catchment and allogenic recharge through ponors by sinking streams from non karst catchments e g ford and williams 2007 the latter has a potential to introduce substances that are not normally present in the karst catchment e g pronk et al 2007 vulnerability of a karst aquifer arises with a variety of possible contamination sources in combination with inadequate protection and monitoring ravbar and kovačič 2015 depending on hydrometeorological and hydrological conditions on the catchment as well as land use water from karst springs represent a mixture of fresh rainwater surface water and groundwater it may contain the sediment from catchment and various contaminants such as pesticides pharmaceuticals heavy metals organic substances bacteria etc e g mahler et al 1999 panno et al 2001 vesper and white 2003 pronk et al 2006 2007 stadler et al 2010 reberski et al 2022 a monitoring of all possible contaminants is practically unworkable so various physical chemical and biochemical water quality indicators are used instead turbidity is generally caused by material suspended or dissolved in water it is often measured in nephelometric turbidity units ntu which quantifies the intensity of light that is scattered by the material in water the turbidity of water from a karst spring may represent a complex mixture of different types of soluble substances and suspended particles these particles may include sediment clay silt or fine sand fine organic and inorganic matter algae and other microscopic organisms e g herman et al 2007 although this turbidity may be caused by different sources it is most often caused dominantly by sediment the relationship between sediment concentration and turbidity has been investigated by several authors e g pavanelli and pagliarani 2002 holliday et al 2003 meral 2016 wang et al 2020 typically for karst springs the value of turbidity rises in response to storm events when the resulting surface runoff and groundwater flow mobilize the sediment from catchment surface and underground karst voids since the mobilized sediment may be contaminated turbidity is often used as an indicator of hazardous events for the quality of karst spring water e g thorn and coxon 1992 massei et al 2003 pronk et al 2007 heinz et al 2009 depending on contaminant a relationship between karst spring contamination and turbidity is not always evident e g auckenthaler et al 2002 an evaluation of applicability and limitations of using turbidity as indicator of contamination can be found in schiperski 2018 to improve reliability of investigations turbidity is often analyzed parallelly with various hydrological and water quality parameters e g mahler and lynch 1999 nebbache et al 2001 grasso and jeannin 2002 massei et al 2002 valdes et al 2006 fournier et al 2007a goldscheider et al 2010 bicalho et al 2012 herman et al 2012 among these parameters water temperature and electrical conductivity have been frequently used these two parameters are considered sometimes as natural tracers as well as turbidity the temperature of karst spring water depends on thermal conditions in karst underground but also it contains information about groundwater circulation due to the heat exchange between water and rock along underground flow water retained for a longer period in karst underground has a constant temperature so any sudden change in the temperature of karst spring water indicates the arrival of water from catchment and possible contamination e g bonacci 1987 birk et al 2004 luetscher and jeannin 2004 kogovšek and petrič 2010 stroj et al 2020 the electrical conductivity of karst spring water depends generally on mineralization of water so it can be used to identify the origin of water from the spring infiltrated rainwater is less mineralized than previously accumulated groundwater because the groundwater had more time to dissolve carbonate rock so a sudden drop of electrical conductivity indicates the presence of infiltrated rainwater at the spring ryan and meiman 1996 mahler and lynch 1999 fournier et al 2007b massei et al 2003 2007 bicalho et al 2012 generally methods and techniques for identifying the origin and nature of turbidity are site specific and under influence of the unique karst system itself and available data and information observation of sedimentary process in karst conduits is also a valuable tool to understand completely the process of transport inside aquifers e g schroeder and ford 1983 gale 1984 dogwiler and wicks 2004 sufficiently detailed geological and hydrogeological data are usually unavailable or they are incomplete so a lack of information about network of underground karst voids is often present in such a situation the karst aquifer may be considered as a black box system and investigations are performed by using available time series collected in the catchment consequently correlation and spectral analyses have been often used for the characterization and better understanding of flow process in karst mangin 1984 padilla and pulido bosch 1995 jeannin and sauter 1998 larocque et al 1998 labat et al 2000 herman et al 2009 fiorillo and doglioni 2010 delbart et al 2014 guo et al 2021 this approach has been extended by wavelet analysis which introduces the temporal component to spectral density e g labat et al 2000 mathevet et al 2004 labat 2005 generally the subject of interest of these analyses has been the information that can be obtained from meteorological hydrological and hydrogeological time series water quality time series may represent sometimes an additional source of information about the karst system several studies have used methods of time series analysis and water quality parameters to provide information about the hydrodynamics and transport properties of karst aquifers e g genthon et al 2005 valdes et al 2005 2006 duran et al 2020 concerning turbidity bouchaou et al 2002 analyzed the relationship between rainfall floodwater and turbidity by the correlation and simple spectral analysis amraoui et al 2003 used the cross spectral analysis to investigate the relationship between rainfall and turbidity massei et al 2006 investigated the relationships between rainfall water level turbidity and electrical conductivity by wavelet analysis a common characteristic of all these applications is the temporal characterization of karst system functioning partial correlation analysis is a part of correlation analysis which most frequently has implied the calculation of partial correlation coefficients partial autocorrelation function and partial cross correlation function are a relatively new method of partial correlation analysis which theoretical background was introduced in hydrology by jukić and denić jukić 2015 the partial cross correlation function uses the concept of control time series which assumes the existence of one or several time series that control or affect the relationship between two original time series the control times series generally may be collected at locations that are different from the locations of original time series so the spatial characteristics of the analyzed system can be investigated in this way this function has been successfully applied on several karst catchments for the spatial characterization of groundwater circulation jukić and denić jukić 2015 kadić et al 2018 denić jukić et al 2017 2020 it has been also used as the objective function in the process of groundwater recharge estimation jukić et al 2021 however partial cross correlation function as well as partial autocorrelation function have not been used yet for the investigation of sediment transport through a karst aquifer in this study correlation partial correlation and spectral analyses are applied on the time series of discharge water temperature electrical conductivity and nephelometric turbidity collected at the source of jadro spring as well as the time series of rainfall collected in the catchment of this karst spring the main aims of this study are 1 presentation of the methods of time series analysis that can be used in the investigation of sediment transport through the karst aquifer 2 analysis of the applicability of partial correlation functions for the spatial characterization of sediment transport process and 3 extension of the existing knowledge about the sediment in water from karst springs with a special reference to the jadro spring 2 study area and data the jadro spring is situated on the adriatic sea coast in vicinity of the city of split in croatia fig 1 basic geological hydrogeological and hydrological information about this karst spring can be found in several publications e g magdalenić 1971 jukić and denić jukić 2009 kapelj et al 2013 bonacci and andrić 2015 basic information about the water quality and a proposal for water conditioning can be found in ignjatić zokić et al 2020 a detail review of tracer tests is available in jukić et al 2021 consequently only a short summary of previously published information important for this study is presented hereafter the catchment is situated in the dinaric karst at altitudes between 261 and 1339 m a s l the spring emerges at the elevation of 35 m a s l the climate within the catchment is typical mediterranean with average annual air temperature about 13 c where maximum values exceed 38 c in summer annual precipitation ranges between 800 and 2100 mm depending on the location the average is about 1300 mm the karst is dominantly composed of the permeable carbonate rocks limestone and dolomite the karst aquifer phreatic zone is located deeply underground the assumed catchment boundary of jadro spring and neighboring žrnovnica spring encloses approximately 450 km2 fig 1 the northern boundary is located north of the mućko polje the existence of underground connections between this polje and the jadro spring was confirmed by a tracer test the obtained apparent groundwater flow velocity between the ponor jablan and the jadro spring was 10 6 cm s the eastern catchment boundary is in the vicinity of the cetina river fig 1 the tracer test of ponor grabov mlin confirmed a direct underground connection between the riverbed of cetina river and the jadro spring about 95 8 of injected tracer appeared at the spring where the apparent groundwater flow velocity was estimated to 0 8 cm s the tracer test of ponor bazin which is situated in the bisko polje revealed that the water from this area is also drained mostly by the jadro spring about 76 3 of the injected tracer was recovered at the jadro spring with an apparent groundwater flow velocity of 6 7 cm s the time series available for this study were the hourly spring discharge q water temperature t electrical conductivity ec and nephelometric turbidity nt recorded at the source of jadro spring as well as the daily rainfall collected at the rain gauges in dugopolje p d muć p m and bisko p b they cover a period of four years from 2017 to 2020 graphical presentations of these time series are in fig 2 a 2g basic statistical characteristics of the analyzed time series are presented in table 1 the spatial distribution of rainfall on the catchment is non uniform the rain gauge in muć has the lowest value of mean daily rainfall 3 2 mm and the lowest standard deviation 8 8 mm the rain gauge with most abundant rainfall is in bisko 4 0 mm this rain gauge has also the largest variation of rainfall with standard deviation of 11 8 mm the karst spring hydrograph in fig 2d has a typical form for a catchment located in coastal area of dinaric karst where discharge episodes start in early autumn and end in late spring and where long recession periods without any significant effective rainfall are clearly distinguished during summer the mean discharge from jadro spring for the analyzed period is 9 03 m3 s minimum values did not go below 3 45 m3 s during summer the maximum discharge of 61 23 m3 s was observed on 4 12 2020 at 0 00 the maximum value of nephelometric turbidity nearly coincides with this peak in discharge namely maximum of 62 45 ntu was observed on 3 12 2020 at 18 00 h which is 6 h earlier generally the peaks in nephelometric turbidity in fig 2e mostly coincide with highest values of discharge whereas minimal values or turbidity are observed during recession periods of discharge electrical conductivity and water temperature both reach minimum values during high discharges recover slowly during periods after these events and reach maximum values during summer fig 2f and 2g the minimum conductivity of 0 349 ms cm was registered on 8 3 2018 at 5 00 h whereas the minimum temperature of 12 4 c was measured from 4 2 2019 at 13 00 h to 6 2 2019 at 8 00 h during the entire period of study water temperature ranged from 12 4 to 13 6 c with mean value of 12 9 c which is very similar to the average annual air temperature in catchment 3 methods the karst catchment is considered as a black box system where the discharge from spring and the quality of spring water are initiated by rainfall on catchment the rainfall is considered as a proxy for groundwater recharge which is a reasonable assumption for a highly karstified catchment with scarce vegetation and generally thin or completely absent soil layer the relationships between rainfall and discharge rainfall and water quality discharge and water quality and water quality parameters itself are investigated by means of the time series analysis including correlation partial correlation and spectral analyses the correlation and spectral analyses involve the determination of autocorrelation functions acf and cross correlation functions ccf in the time domain as well as the determination of spectral density functions sdf gain functions gf coherence functions cf and phase functions pf in the frequency domain these functions have found a wide application in hydrological system analyses so the mathematical background can be found in several papers e g mangin 1984 padilla and pulido bosch 1995 jeannin and sauter 1998 larocque et al 1998 guo et al 2021 a short theoretical explanation of all applied functions and their expressions are summarized in appendix a the partial correlation analysis involves the determination of partial autocorrelation functions pacf and partial cross correlation functions pccf jukić and denić jukić 2015 let time series x and y have a causal relationship where x is transformed to y by an intervening system and where r xy k represents ccf between series x and y appendix a let z represent a time series that controls the process of transformation of time series x to y the linear effect of control time series z can be removed from r xy k by calculating pccf 1 r xy z k r xy k r xz r zy k 1 r xz 2 1 r zy 2 k where r xz is correlation coefficient between series x and z whereas r zy k is ccf between series z and y the linear effect of control time series z can also be removed from acf the result is pacf specifically pacf for time series x and y respectively are 2 r xx z k r xx k r xz r zx k 1 r xz 2 1 r zx 2 k 3 r yy z k r yy k r yz r zy k 1 r yz 2 1 r zy 2 k for two control time series z 1 and z 2 the second order pccf is obtained as 4 r xy z 1 z 2 k r xy z 1 k r x z 2 z 1 r z 2 y z 1 k 1 r x z 2 z 1 2 1 r z 2 y z 1 2 k generally following the same principle for a set of n control time series z 1 z n the n th order pccf is obtained by recursion 5 r xy z 1 z n k r xy z 1 z n 1 k r x z n z 1 z n 1 r z n y z 1 z n 1 k 1 r x z n z 1 z n 1 2 1 r z n y z 1 z n 1 2 k the effect of control time series z 1 z n is investigated by comparing r xy k and r xy z 1 z n k theoretically five types of effects can be distinguished at a lag k 1 explanation effect if r xy k and r xy z 1 z n k have opposite signs 2 control effect for r xy z 1 z n k 0 3 partial explanation effect for 0 r xy z 1 z n k r xy k 4 no effect for r xy z 1 z n k r xy k and 5 suppression effect for r xy z 1 z n k r xy k it should be noted that absolute values are used above because r xy k and r xy z 1 z n k both can have negative signs the confidence interval for pccf is the same as for ccf which means that the lower and upper limits can be estimated to recognize statistically significant values of pccf e g banik and kibria 2016 approximately 95 confidence limits are 2 n where n is length of time series the following general rule for the notation of functions is used name x y z 1 z n where name is a function abbreviation acf ccf pacf or pccf x and y are time series with a causal relationship and z 1 z n are control time series 4 results 4 1 correlation and spectral analyses the obtained autocorrelation functions for time series q nt ec and t are presented in fig 3 a they quantify the linear dependency of successive values of these time series for time series q acf q provides information about the storage capacity of karst system the so called memory effect of karst system is defined as the time lag where acf q becomes less than 0 2 mangin 1984 it amounts 990 h or approximately 41 days assuming that the same approach can be applied also to time series nt ec and t the corresponding memory effects for these three series are 120 1640 and 1680 h respectively irregularities observable in acf n t at lags above 500 h are very similar to irregularities in acf q which is a consequence of similar fluctuations in values of q and nt the cross correlation functions between time series q nt ec and t are analyzed in fig 3b the time to peak value of ccf q n t gives an estimate of the timing of sediment transfer through the karst system where the discharge is a referent point it amounts only 1 h which indicates that q and nt are synchronized almost completely ccf q n t has larger values at negative lags because the memory effect of q is larger than the memory effect of nt the times to peak of ccf q e c and ccf q t give an estimate of the fresh rainwater arrival time they amount 49 and 55 h respectively which shows that the arrival of infiltrated rainwater does not coincide with the arrival of sediment generally time series ec and t have very similar fluctuations so ccf e c t has the highest values in fig 3b this function is practically axisymmetric around the peak value at 12 h which shows that time series ec and t are generated by similar processes the results of cross spectral analysis are presented in fig 4 where the variations in values of time series q and nt are compared in the frequency domain an annual periodic component at frequency of 0 000142 1 h is evident in sdf q in fig 4a it is a consequence of seasonality of karst spring discharge which is dominantly generated by the seasonal fluctuation of evapotranspiration and to lesser extent by the seasonal fluctuation of rainfall on the catchment however this periodic component is only partly transferred to turbidity in fact it is mostly attenuated so the peak in sdf n t at 0 000142 1 h is hardly evident fig 4a detail a in addition to this low frequency range attenuation the form of gf q n t in fig 4b detail b shows that the high frequency components are also attenuated so only values around frequency of 0 1 1 h indicate a small amplification it means that the variation of turbidity generally does not follow the variation of discharge and that the variation of turbidity is generally more uniform the form of cf q n t in fig 4 c confirms this conclusion it can be noted that cf q n t decreases with frequency and that a linear relationship between q and nt practically does not exist at frequencies above 0 15 1 h in fig 4d it is evident that pf q n t has spurious discontinuities at this frequency range where it variates between π 2 and π 2 however the form of this function at frequencies below 0 1 1 h is smooth enough to estimate the time delay between q and nt fig 4d detail c it is approximately 1 h which is equivalent to the time to peak value of ccf q n t in fig 3b the obtained cross correlation functions between time series of rainfall pd and time series q nt ec and t are presented in fig 5 it can be noted that the response of jadro spring to rainfall events is fast so the maximum value of ccf p d q is obtained at lag of 25 h assuming that ccf p d q represents adequately the unit response function the change in slope of this function at lag of about 330 h indicates the duration of quick flow component in discharge the remaining part of ccf p d q above 330 h represents baseflow it can be noted also that ccf p d n t has larger values than ccf p d q during first 49 h i e turbidity is better correlated with rainfall than discharge during first two days of spring response the time to peak value of ccf p d n t amounts 27 h after that ccf p d n t decreases rapidly and it becomes statistically insignificant at lags corresponding approximately to the duration of quick flow component the part of ccf p d n t representing baseflow component variates close to the confidence interval and it has similar irregularities like ccf p d q which means that this part of ccf p d n t probably has not a physical meaning i e any component of turbidity that comes to the spring with a time delay cannot be identified confidently concerning electrical conductivity the obtained form of ccf p d e c shows that the effect of infiltrated rainwater is registered in the discharge during a period of about 3100 h or approximately 4 months after rainfall this period can be divided in three segments the period of decrease in conductivity that corresponds to the arrival of fresh rainwater period a fig 5 the period of increase in conductivity that corresponds to the withdrawal of fresh rainwater period b fig 5 and the period of emptying of previously accumulated water in vadose and phreatic zone period c fig 5 water temperature has similar behavior as electrical conductivity 4 2 partial correlation analysis the results presented in fig 4a show that q contains a significant annual periodic component which is transferred to nt and consequently it is registered in acf n t fig 6 a similarly this non stationary component is registered also in acf e c and acf t fig 6a however the effect of annual periodic component can be removed from these functions by using pacf where q is the control time series the obtained pacf n t q pacf e c q and pacf t q are presented also in fig 6a the memory effects may be defined here as the lags where these functions become statistically insignificant the obtained results show that the memory effects of electrical conductivity and water temperature are practically same about 1740 h the memory effect of turbidity is around 160 h at larger lags pacf n t q is irregular similarly as acf n t which is a consequence of non stationarity so this part probably has not a physical meaning the effect of rainfall regime on the relationship between q and nt is estimated by comparing ccf q n t with pccf q n t p d p b p m in fig 6b it is evident that differences between these two functions are relatively small the cumulative effect of pd pb and pm is minor which means that other processes are dominantly responsible for the correlation between q and nt in order to estimate the effect of control time series pm pccf q n t p d p b is also presented it can be noted that pccf q n t p d p b is practically identical to pccf q n t p d p b p m which means that the rainfall from muć has no effect on the relationship between discharge and turbidity fig 7 presents the cross correlation functions between rainfall and discharge as well as the corresponding partial cross correlation functions obtained by using the rainfall observed at two neighboring rain gauges as the control time series the main aim of this analysis is to identify the statistically significant partial explanation effects of control time series which represent the specific contribution of each rain gauge location to the total discharge from spring the response of jadro spring to the rainfall events in catchment is fast so the times to peak value of c c f p m q c c f p d q and c c f p b q are 25 or 26 h these three functions have also similar forms this similarity is the consequence of the spurious correlation between pd pb and pm generally rainfall observed at neighboring locations are most often generated by the same meteorological events so a similarity in their variations always exists on the other hand the obtained p c c f p m q p d p b p c c f p d q p m p b and p c c f p b q p d p m have completely different forms they describe the spatial and temporal differences in the specific contribution of each rain gauge location the quick flow component is dominantly generated by the rainfall from dugopolje namely it can be noted that p c c f p d q p m p b has the highest values of partial explanation effect fig 7 detail a which takes place at legs between 0 and 140 h with the peak value at lag of 22 h the contribution of rainfall from bisko is much smaller and it is observable in p c c f p b q p d p m during the first 57 h and later between 132 and 273 h fig 7 detail b a statistically significant contribution of rainfall from muć is observable in p c c f p m q p d p b only at lags between 79 and 154 h fig 7 detail c concerning the baseflow component at lags larger than 330 h it can be noted that p c c f p d q p m p b and p c c f p m q p d p b variate irregularly inside the confidence interval statistically significant values are registered only in p c c f p b q p d p m fig 7 detail d which shows that the rainfall from bisko has the most important role in the generation of baseflow component the specific contribution of each rain gauge location to the electrical conductivity of spring water can be estimated from fig 8 where the cross correlation functions between rainfall and electrical conductivity are compared with the corresponding partial cross correlation functions determined for the control time series of rainfall observed at two neighboring rain gauges the obtained results are mostly in accordance with the results presented in fig 7 during first about 160 h period a fig 8 it can be noted that only p c c f p d e c p m p b has a statistically significant partial explanation effect of the control time series while p c c f p b e c p m p d decreases slowly inside the confidence interval during the next 380 h period b fig 8 p c c f p d e c p m p b becomes insignificant whereas p c c f p b e c p m p d becomes significant and it reaches a local minimum after about 540 h period c fig 8 a statistically significant partial explanation effect is notable only in p c c f p b e c p m p d it shows that the flushing of the phreatic zone is induced by the rainfall events in dugopolje and the discharge is generated mostly by the infiltrated rainwater from this location during the period of fresh rainwater arrival the rainwater from bisko comes to the spring during the period of rainwater withdrawal and later it is interesting that a contribution of rainfall from muć has not been registered at all namely in fig 8 it can be noted that c c f p m e c and p c c f p m e c p d p b have opposite signs which means that the control time series pd and pb explain completely the existing correlation between pm and ec a similar analysis was performed also for turbidity fig 9 it can be noted that c c f p m n t c c f p d n t and c c f p b n t go below the upper boundary of confidence interval at lags above 270 h and they approach to the horizontal axis at lags about 330 h which corresponds to the duration of the quick flow component the times to peak of these three functions are between 24 and 27 h which is synchronized with the times to peak obtained for discharge fig 7 during the first 112 h p c c f p d n t p m p b has the highest values of partial explanation effect of control signals fig 9 detail a the partial explanation effect registered in p c c f p b n t p m p d is much smaller fig 9 detail b after that p c c f p d n t p m p b becomes statistically insignificant whereas p c c f p b n t p m p d remains significant during the next 150 h fig 9 detail c it shows that turbidity is generated mostly by the rainfall from dugopolje during the first 4 to 5 days of spring response during the remaining period of the quick flow component turbidity is generated mostly by the rainfall from bisko a contribution of rainfall from muć is not registered namely an explanation effect of control time series is evident in p c c f p m n t p d p b at lags below 60 h fig 9 detail d whereas at higher lags the value of this function varies inside the confidence interval it means that the time series pd and pb explain completely the existing form of c c f p m n t and that the correlation between rainfall from muć and turbidity of jadro spring is spurious 5 discussion the temporal characteristics of the process of transport of infiltrated rainwater and sediment through the aquifer of jadro spring were investigated by means of correlation and spectral analyses three natural tracers were analyzed since the turbidity of water from jadro spring is dominantly generated by sediment nephelometric turbidity is considered as an indicator of the sediment concentration in spring water electrical conductivity and water temperature are considered as indicators of the presence of fresh infiltrated rainwater the results of partial correlation analysis revealed that the existing correlation between discharge and turbidity is not a consequence of rainfall fig 6b i e it is not a spurious correlation resulting from the initiation of discharge and turbidity by same rainfall events this finding is in accordance with the existing knowledge about the entrainment of sediment in karst conduits namely the discharge from karst spring is expected to be correlated with the velocity of flow through the karst conduits on the other hand the movement of sediment starts when a critical boundary shear stress is exceeded on the conduit bed which generally depends on the velocity of flow through the karst conduits e g atteia and kozel 1997 dogwiler and wicks 2004 herman et al 2008 2012 vuilleumier et al 2021 if this velocity is larger than a critical velocity the entrainment of sediment is activated if this velocity is lower there is no entrainment at all consequently a causal relationship between discharge and turbidity exists but it is nonlinear so the coherence between discharge and turbidity is low fig 4c and the variation of turbidity is generally more uniform fig 4b the memory effect is usually defined in karst hydrology as the time lag where acf of discharge becomes less than 0 2 mangin 1984 assuming that the same principle can be applied also to the time series of turbidity electrical conductivity and water temperature the estimated memory effects amount 120 1640 and 1680 h respectively fig 3a however the results of spectral analysis show that the time series of discharge contains a significant annual periodic component fig 4a this and other non stationary components in discharge are transferred to the time series of turbidity electrical conductivity and water temperature which means that their acf is deformed and maybe do not represent adequately the actual linear dependency of successive values of these time series consequently pacf was also used for the estimation of memory effects of turbidity electrical conductivity and water temperature the control time series is the discharge the obtained memory effects of electrical conductivity and water temperature by this method are practically same about 1740 h fig 6a it shows that the fresh rainwater from catchment is drained after approximately 72 days after that seasonal groundwater storage is activated and inter catchment groundwater flows become dominant jukić and denić jukić 2009 the obtained memory effect of turbidity amounts 160 h which is 40 h longer than the value obtained by classical method of mangin 1984 it indicates that the spring water becomes clear usually within a period of approximately 7 days after a turbidity peak the spatial characteristics of the process of transport of infiltrated rainwater and sediment through the aquifer of jadro spring were investigated by using pccf the time series of rainfall from the analyzed rain gauge location is considered as the input the time series of discharge turbidity and electrical conductivity from the jadro spring are considered as the outputs the control time series are rainfall observed at two neighboring rain gauge locations the specific contribution of each rain gauge location to the dynamic of discharge turbidity and electrical conductivity was identified the obtained results show that maximum discharge is reached within a period of about 24 h after rainfall events fig 7 the quick flow component lasts about 330 h or approximately 14 days which is in accordance with previous results denić jukić and jukić 2003 jukić and denić jukić 2009 2015 it is dominantly initiated by the rainfall from dugopolje the contributions of rainfall from other two rain gauge locations are notable but they are much smaller on the other hand the baseflow component is dominantly generated by the rainfall from bisko the specific contributions of rainfall from dugopolje and muć to this component are practically insignificant the dynamic of electrical conductivity is in accordance with the above the rapid decrease in conductivity lasts about 160 h and it denotes the arrival of fresh rainwater to the spring this decrease is generated by the rainwater from dugopolje fig 8 the fresh rainwater withdrawal lasts next 380 h during this period and later during remaining period of baseflow the rainwater from bisko has most important role a contribution of rainfall from muć was not registered at all concerning turbidity maximum is reached about 24 h after rainfall event fig 9 during first about 112 h the presence of turbidity in spring water coincides with the arrival of infiltrated rainwater from dugopolje detail a in fig 7 period a in fig 8 detail a in fig 9 after this period only the rainfall from bisko has a statistically significant contribution to turbidity which lasts approximately till the end of quick flow component again a contribution of the rainfall from muć was not registered at all previous investigations in karst show that depending on its origin turbidity of water from a karst spring may be recognized as the remobilized or aquifer derived turbidity that contains autochthonous sediment from aquifer and the flush through or soil derived turbidity that contains allochthonous sediment generated by soil erosion on catchment surface e g schiperski 2018 the increase in turbidity coincident with the rise in stage or in discharge has been observed by several authors e g valdes et al 2006 pronk et al 2009 herman et al 2012 schiperski et al 2015a they all have noted that turbidity is increased due to entrainment of autochthonous sediment during periods of high velocity within karst conduit system on the other hand there have been also several studies where discharge and turbidity do not have same evolution e g ryan and meiman 1996 pronk et al 2007 schiperski et al 2015b or their peaks are not synchronized because of chaotic behavior of turbidity during flood events e g vuilleumier et al 2021 in these examples turbidity was generated by allochthonous sediment generally a simultaneous and synchronized variation of turbidity and karst spring discharge indicates the existence of autochthonous sediment the existence of allochthonous sediment is registered as a peak in turbidity that has a time delay from the peak of discharge however these relationships have not always straightforward interpretation so analyses of other parameters are required to identify origin of turbidity such as electrical conductivity e g vesper and white 2003 valdes et al 2005 heinz et al 2006 2009 a drop of electrical conductivity coincident with a peak in turbidity indicates the probable presence of allochthonous sediment the results obtained in this study show that the variations of discharge and turbidity are simultaneous and synchronized namely the time delay between turbidity and discharge amounts only 1 h the time to peak in ccf between rainfall and discharge is practically equal to the time to peak in ccf between rainfall and turbidity turbidity is even better correlated with rainfall than discharge during the first 48 h of spring response in addition there is not any significant increase in turbidity that comes to the spring with a time delay the drop of electrical conductivity and water temperature is registered only during the period of high flow after that they increase slowly and recover completely during low flow this drop does not coincide with the peak in turbidity i e there is a time delay of 48 h between them considering the above it can be concluded that the origin of turbidity on the jadro spring is dominantly autochthonous sediment this type of sediment seems to prevail in the spring water probably because of the size of catchment namely allochthonous sediment may be introduced into karst aquifer by autogenic diffuse recharge autogenic concentrated recharge and allogenic concentrated recharge through ponors since there is not a component in turbidity that comes to spring with a time delay it is evident that allogenic concentrated recharge has insignificant effect on dynamic of turbidity allochthonous sediment is transported over long distances so it is probably settled before reaching the jadro spring however the presence of turbidity in the spring water coincides with the arrival of infiltrated rainwater from dugopolje so there is a possibility that a certain quantity of allochthonous sediment was introduced by autogenic recharge and it was mixed with the autochthonous sediment this is only a hypothesis that should be evaluated by further investigations concerning the rain gauges in dugopolje and bisko the obtained results are in accordance with the existing knowledge about the hydrological functioning of the catchment of jadro spring e g jukić and denić jukić 2009 2015 the rain gauge in dugopolje is the closest to the spring and it represents the highly permeable central part of catchment located in the hinterland of the spring the rain gauge in bisko represents the eastern part of catchment which is a region of complex hydrogeology that ranges from highly permeable carbonate rocks to low permeable flysch deposits this region is also a transition zone for the inter catchment groundwater flow from the cetina river catchment which means that a well developed channel network between this area and the jadro spring exists consequently the quick flow is dominantly generated by the rainfall from dugopolje whereas the rainfall from bisko is mainly responsible for the remaining part of the spring hydrograph since the turbidity originates from the aquifer autochthonous sediment and maybe from allochthonous sediment from the part of catchment located in the hinterland of the spring it is also dominantly initiated by the rainfall from dugopolje concerning the results obtained for the rain gauge in muć the situation seems much more complex jukić and denić jukić 2015 noted that the results of partial correlation analysis obtained for muć are different from the results obtained for bisko and dugopolje in terms of the absence of specific contribution to the quick flow component this difference was explained by the hydrological functioning of karst fields namely superficial deposits in karst fields accumulate water on the surface and overland flow is formed occasionally after long lasting intensive rainfall when the capacity of superficial deposits are fulfilled this overland flow then goes very quickly underground through the ponors during less intensive rainfall superficial deposits temporary retain water and then release it slowly and diffusely to the karst underground which means that they function as low or medium permeable layers consequently the contributions of karst fields are occasional and with a time delay so the relationship between rainfall and spring discharge is highly non linear however the results obtained in this study indicates that the rainfall from muć affect the response of jadro spring only in the form of a small pressure pulls which is registered in the quick flow component at lags between 79 and 154 h any effect of the rainfall from muć on the electrical conductivity or the turbidity of water from jadro spring was not identified even though the area of mućko polje has the highest apparent groundwater flow velocity according to tracer tests fig 1 it confirms that results of tracer tests cannot be considered as reliable qualitative or quantitative indicators of the sediment transport through karst aquifers 6 conclusion the correlation partial correlation and spectral analyses were applied on the time series of rainfall spring discharge electrical conductivity nephelometric turbidity and water temperature collected in the catchment of jadro spring the correlation and spectral analyses involved the determination of acf and ccf in the time domain as well as sdf gf cf and pf in the frequency domain these functions are used for determination of the temporal characteristics of sediment transport process such as the timing synchronization with discharge and time delay between arrivals of sediment and fresh rainwater to spring the partial correlation analysis involved the determination of pacf and pccf pacf is used to remove the seasonal periodicity from acf in order to estimate more reliably the memory effects of turbidity electrical conductivity and water temperature pccf is used for the spatial characterization of sediment transport processes including the origin of sediment the presented results show that pccf can be very useful in the investigation of mass transport through the karst aquifer especially in the situation of scarce data and information pccf enables identification of the contribution of a specific location in catchment to the quantity and quality of spring water for example the contributions of three rain gauge locations to the discharge turbidity and electrical conductivity of spring water could be identified in this study by analyzing and comparing the obtained results the origin of sediment in spring water is determined as well as the origin of quick flow and baseflow components including the timing of fresh rainwater arrival rainwater withdrawal and emptying of previously accumulated water in vadose and phreatic zone this spatial information cannot be obtained by using any other method of time series analysis pccf can help also in resolving some ambiguities related to the origin of correlation between two time series such as discharge and turbidity the obtained results for jadro spring are in accordance with results of previous studies and they mostly support the existing hypotheses about the hydrological functioning of this spring concerning the process of sediment transport it is initiated after heavy rainfall in the central and eastern part of catchment the origin of sediment in spring water is dominantly the autochthonous sediment from aquifer and possibly the allochthonous sediment from the part of catchment located in hinterland of spring e g dugopolje credit authorship contribution statement damir jukić conceptualization methodology software formal analysis investigation writing review editing vesna denić jukić resources data curation validation writing review editing ana kadić data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research is partially supported through project kk 01 1 1 02 0027 a project co financed by the croatian government and the european union through the european regional development fund the competitiveness and cohesion operational program appendix a let time series x and y have a causal relationship where x is transformed to y by an intervening system the covariance function between series x and y both of length n is defined by 1 c xy k 1 n t 1 n k x t μ x y t k μ y k 0 1 2 m where μ x and μ y are the means of x and y respectively the truncation point m determines the time interval in which the analysis is carried out the cross correlation function ccf between time series x and y is 2 r xy k c xy k σ x σ y k 0 c yx k σ x σ y k 0 where σ x and σ y are the standard deviations of x and y respectively ccf provides information about the significance of causal and non causal relationships between series x and y if ccf is symmetrical around a vertical axis series x and y are generated by a third independent series which means that y is not affected by x if ccf is not symmetrical it means that y is affected by x it should be noted that the correlation coefficient cc between x and y is r xy 0 the autocorrelation function acf of time series x and y can be obtained from eq 2 as 3 r xx k c xx k σ x 2 r yy k c yy k σ y 2 where k 0 acf of time series describes the duration and significance of influence of an event on subsequent measurements i e it maps the memory effect of time series existing periodicities in the time series are also registered in this function which can significantly affect the form of acf spectral analysis is complementary to the correlation analysis specifically spectral density function sdf represents a fourier transform of acf whereas cross spectral density function csdf represents a fourier transform of ccf for r xx k and r yy k corresponding sdf are 9 s x f 2 1 2 k 1 m d k r xx k c o s 2 π f k 10 s y f 2 1 2 k 1 m d k r yy k c o s 2 π f k for frequencies f l 2 m l 0 1 2 m where 11 d k 1 c o s π k m 2 represents the lag window proposed by tukey 14 15 which ensures that the estimated values are not biased sdf visualize the periodicity content of the time series periodic components in time series are represented as peaks in sdf so this function is used for identification of the frequency and the relative importance of each periodic component on the other hand csdf is a complex function which has the following form in polar coordinates 12 s xy f α xy f e x p i φ xy f where i is imaginary unit and where the cross amplitude function caf and the phase function pf are calculated by the following expressions 13 α xy f ψ xy 2 f λ xy 2 f 14 φ xy f a r c t a n λ xy f ψ xy f 15 ψ xy f 2 r xy 0 k 1 m r xy k r yx k d k c o s 2 π f k 16 λ xy f 2 k 1 m r xy k r yx k d k s i n 2 π f k pf indicates dephasing between two time series in the time mode it shows the time delay between x and y for different frequencies 17 τ φ xy f 2 π f caf describes the transformation of the periodic components in x by the intervening system i e it describes the way in which series x is modified by the system using this function two additional functions in the frequency domain are defined the first is gain function gf which is obtained as 18 g xy f α xy f s x f and the second is coherence function cf 19 c xy f α xy f s x f s y f gf describes an amplification g xy f 1 or an attenuation g xy f 1 of the variation of y at frequency f in comparation to the variation of x at same frequency cf shows whether the variation of y at frequency f corresponds to the variation of x at same frequency i e it expresses the linearity of relationship between x and y the relationship is linear if c xy f 1 
3407,the process of sediment transport through a karst aquifer is investigated by means of time series analysis the correlation and spectral analyses are used for determination of the temporal characteristics of this process such as timing synchronization with discharge from spring and time delay between arrivals of sediment and infiltrated rainwater to spring the partial correlation analysis is used for the spatial characterization the study area is the catchment of jadro spring in croatia the analyzed hourly time series are discharge water temperature electrical conductivity and nephelometric turbidity recorded at the source of this spring as well as the rainfall collected at three locations in the catchment of spring they cover the period of four years from 2017 to 2020 the main aims of this research are 1 presentation of the methods of time series analysis that can be used for the investigation of sediment transport through karst aquifer 2 analysis of the applicability of partial cross correlation functions for the spatial characterization of sediment transport process and 3 extension of the existing knowledge about the sediment in water from karst springs with a special reference to the jadro spring the obtained results show that the partial cross correlation function can be very useful in the investigation of mass transport through the karst aquifer they enable identification of the specific contribution of each rain gauge location to the quantity and quality of spring water specifically the origin of sediment in water from jadro spring is determined in this study as well as the origin of quick flow and baseflow component in discharge from this spring including the timing of fresh rainwater arrival rainwater withdrawal and emptying of previously accumulated water in vadose and phreatic zone the sediment transport through the karst aquifer of jadro spring is initiated after heavy rainfall in the central and eastern part of the catchment the origin of sediment in spring water is dominantly the remobilized autochthonous sediment from aquifer and possible limited contribution of allochthonous sediment from the part of catchment located in hinterland of spring keywords karst hydrology correlation and spectral analysis partial cross correlation function turbidity sediment transport jadro spring 1 introduction karst aquifers are vital groundwater resources that provide drinking water for a significant number of the world s population and that support agriculture groundwater dependent activities and ecosystems olarinoye et al 2020 they have complex hydrological and hydrogeological characteristics due to the complicated network of underground karst voids that consist of conduits fractures and fissures within low permeability matrix bakalowicz 2005 karst springs most frequently appear at the contact between permeable carbonate rocks and impermeable layers bimodal behavior is a basic characteristic of karst aquifers two different hydraulic phases can be recognized during and after rainfall the first is mostly governed by a pressure pulse mechanism occurring due to infiltration which produces a rapid increase in flow rate the second represents system emptying or recession flow that is not influenced by infiltration e g grasso et al 2003 concentrated turbulent flow takes place in conduits while diffuse laminar flow prevails in fractures fissures and low permeability matrix e g liedl et al 2003 consequently a quick flow component and a baseflow component can be recognized in the karst spring hydrographs e g atkinson 1977 geyer et al 2008 sometimes two components of groundwater recharge can also be distinguished depending on the origin of water autogenic recharge by rainwater from the karst catchment and allogenic recharge through ponors by sinking streams from non karst catchments e g ford and williams 2007 the latter has a potential to introduce substances that are not normally present in the karst catchment e g pronk et al 2007 vulnerability of a karst aquifer arises with a variety of possible contamination sources in combination with inadequate protection and monitoring ravbar and kovačič 2015 depending on hydrometeorological and hydrological conditions on the catchment as well as land use water from karst springs represent a mixture of fresh rainwater surface water and groundwater it may contain the sediment from catchment and various contaminants such as pesticides pharmaceuticals heavy metals organic substances bacteria etc e g mahler et al 1999 panno et al 2001 vesper and white 2003 pronk et al 2006 2007 stadler et al 2010 reberski et al 2022 a monitoring of all possible contaminants is practically unworkable so various physical chemical and biochemical water quality indicators are used instead turbidity is generally caused by material suspended or dissolved in water it is often measured in nephelometric turbidity units ntu which quantifies the intensity of light that is scattered by the material in water the turbidity of water from a karst spring may represent a complex mixture of different types of soluble substances and suspended particles these particles may include sediment clay silt or fine sand fine organic and inorganic matter algae and other microscopic organisms e g herman et al 2007 although this turbidity may be caused by different sources it is most often caused dominantly by sediment the relationship between sediment concentration and turbidity has been investigated by several authors e g pavanelli and pagliarani 2002 holliday et al 2003 meral 2016 wang et al 2020 typically for karst springs the value of turbidity rises in response to storm events when the resulting surface runoff and groundwater flow mobilize the sediment from catchment surface and underground karst voids since the mobilized sediment may be contaminated turbidity is often used as an indicator of hazardous events for the quality of karst spring water e g thorn and coxon 1992 massei et al 2003 pronk et al 2007 heinz et al 2009 depending on contaminant a relationship between karst spring contamination and turbidity is not always evident e g auckenthaler et al 2002 an evaluation of applicability and limitations of using turbidity as indicator of contamination can be found in schiperski 2018 to improve reliability of investigations turbidity is often analyzed parallelly with various hydrological and water quality parameters e g mahler and lynch 1999 nebbache et al 2001 grasso and jeannin 2002 massei et al 2002 valdes et al 2006 fournier et al 2007a goldscheider et al 2010 bicalho et al 2012 herman et al 2012 among these parameters water temperature and electrical conductivity have been frequently used these two parameters are considered sometimes as natural tracers as well as turbidity the temperature of karst spring water depends on thermal conditions in karst underground but also it contains information about groundwater circulation due to the heat exchange between water and rock along underground flow water retained for a longer period in karst underground has a constant temperature so any sudden change in the temperature of karst spring water indicates the arrival of water from catchment and possible contamination e g bonacci 1987 birk et al 2004 luetscher and jeannin 2004 kogovšek and petrič 2010 stroj et al 2020 the electrical conductivity of karst spring water depends generally on mineralization of water so it can be used to identify the origin of water from the spring infiltrated rainwater is less mineralized than previously accumulated groundwater because the groundwater had more time to dissolve carbonate rock so a sudden drop of electrical conductivity indicates the presence of infiltrated rainwater at the spring ryan and meiman 1996 mahler and lynch 1999 fournier et al 2007b massei et al 2003 2007 bicalho et al 2012 generally methods and techniques for identifying the origin and nature of turbidity are site specific and under influence of the unique karst system itself and available data and information observation of sedimentary process in karst conduits is also a valuable tool to understand completely the process of transport inside aquifers e g schroeder and ford 1983 gale 1984 dogwiler and wicks 2004 sufficiently detailed geological and hydrogeological data are usually unavailable or they are incomplete so a lack of information about network of underground karst voids is often present in such a situation the karst aquifer may be considered as a black box system and investigations are performed by using available time series collected in the catchment consequently correlation and spectral analyses have been often used for the characterization and better understanding of flow process in karst mangin 1984 padilla and pulido bosch 1995 jeannin and sauter 1998 larocque et al 1998 labat et al 2000 herman et al 2009 fiorillo and doglioni 2010 delbart et al 2014 guo et al 2021 this approach has been extended by wavelet analysis which introduces the temporal component to spectral density e g labat et al 2000 mathevet et al 2004 labat 2005 generally the subject of interest of these analyses has been the information that can be obtained from meteorological hydrological and hydrogeological time series water quality time series may represent sometimes an additional source of information about the karst system several studies have used methods of time series analysis and water quality parameters to provide information about the hydrodynamics and transport properties of karst aquifers e g genthon et al 2005 valdes et al 2005 2006 duran et al 2020 concerning turbidity bouchaou et al 2002 analyzed the relationship between rainfall floodwater and turbidity by the correlation and simple spectral analysis amraoui et al 2003 used the cross spectral analysis to investigate the relationship between rainfall and turbidity massei et al 2006 investigated the relationships between rainfall water level turbidity and electrical conductivity by wavelet analysis a common characteristic of all these applications is the temporal characterization of karst system functioning partial correlation analysis is a part of correlation analysis which most frequently has implied the calculation of partial correlation coefficients partial autocorrelation function and partial cross correlation function are a relatively new method of partial correlation analysis which theoretical background was introduced in hydrology by jukić and denić jukić 2015 the partial cross correlation function uses the concept of control time series which assumes the existence of one or several time series that control or affect the relationship between two original time series the control times series generally may be collected at locations that are different from the locations of original time series so the spatial characteristics of the analyzed system can be investigated in this way this function has been successfully applied on several karst catchments for the spatial characterization of groundwater circulation jukić and denić jukić 2015 kadić et al 2018 denić jukić et al 2017 2020 it has been also used as the objective function in the process of groundwater recharge estimation jukić et al 2021 however partial cross correlation function as well as partial autocorrelation function have not been used yet for the investigation of sediment transport through a karst aquifer in this study correlation partial correlation and spectral analyses are applied on the time series of discharge water temperature electrical conductivity and nephelometric turbidity collected at the source of jadro spring as well as the time series of rainfall collected in the catchment of this karst spring the main aims of this study are 1 presentation of the methods of time series analysis that can be used in the investigation of sediment transport through the karst aquifer 2 analysis of the applicability of partial correlation functions for the spatial characterization of sediment transport process and 3 extension of the existing knowledge about the sediment in water from karst springs with a special reference to the jadro spring 2 study area and data the jadro spring is situated on the adriatic sea coast in vicinity of the city of split in croatia fig 1 basic geological hydrogeological and hydrological information about this karst spring can be found in several publications e g magdalenić 1971 jukić and denić jukić 2009 kapelj et al 2013 bonacci and andrić 2015 basic information about the water quality and a proposal for water conditioning can be found in ignjatić zokić et al 2020 a detail review of tracer tests is available in jukić et al 2021 consequently only a short summary of previously published information important for this study is presented hereafter the catchment is situated in the dinaric karst at altitudes between 261 and 1339 m a s l the spring emerges at the elevation of 35 m a s l the climate within the catchment is typical mediterranean with average annual air temperature about 13 c where maximum values exceed 38 c in summer annual precipitation ranges between 800 and 2100 mm depending on the location the average is about 1300 mm the karst is dominantly composed of the permeable carbonate rocks limestone and dolomite the karst aquifer phreatic zone is located deeply underground the assumed catchment boundary of jadro spring and neighboring žrnovnica spring encloses approximately 450 km2 fig 1 the northern boundary is located north of the mućko polje the existence of underground connections between this polje and the jadro spring was confirmed by a tracer test the obtained apparent groundwater flow velocity between the ponor jablan and the jadro spring was 10 6 cm s the eastern catchment boundary is in the vicinity of the cetina river fig 1 the tracer test of ponor grabov mlin confirmed a direct underground connection between the riverbed of cetina river and the jadro spring about 95 8 of injected tracer appeared at the spring where the apparent groundwater flow velocity was estimated to 0 8 cm s the tracer test of ponor bazin which is situated in the bisko polje revealed that the water from this area is also drained mostly by the jadro spring about 76 3 of the injected tracer was recovered at the jadro spring with an apparent groundwater flow velocity of 6 7 cm s the time series available for this study were the hourly spring discharge q water temperature t electrical conductivity ec and nephelometric turbidity nt recorded at the source of jadro spring as well as the daily rainfall collected at the rain gauges in dugopolje p d muć p m and bisko p b they cover a period of four years from 2017 to 2020 graphical presentations of these time series are in fig 2 a 2g basic statistical characteristics of the analyzed time series are presented in table 1 the spatial distribution of rainfall on the catchment is non uniform the rain gauge in muć has the lowest value of mean daily rainfall 3 2 mm and the lowest standard deviation 8 8 mm the rain gauge with most abundant rainfall is in bisko 4 0 mm this rain gauge has also the largest variation of rainfall with standard deviation of 11 8 mm the karst spring hydrograph in fig 2d has a typical form for a catchment located in coastal area of dinaric karst where discharge episodes start in early autumn and end in late spring and where long recession periods without any significant effective rainfall are clearly distinguished during summer the mean discharge from jadro spring for the analyzed period is 9 03 m3 s minimum values did not go below 3 45 m3 s during summer the maximum discharge of 61 23 m3 s was observed on 4 12 2020 at 0 00 the maximum value of nephelometric turbidity nearly coincides with this peak in discharge namely maximum of 62 45 ntu was observed on 3 12 2020 at 18 00 h which is 6 h earlier generally the peaks in nephelometric turbidity in fig 2e mostly coincide with highest values of discharge whereas minimal values or turbidity are observed during recession periods of discharge electrical conductivity and water temperature both reach minimum values during high discharges recover slowly during periods after these events and reach maximum values during summer fig 2f and 2g the minimum conductivity of 0 349 ms cm was registered on 8 3 2018 at 5 00 h whereas the minimum temperature of 12 4 c was measured from 4 2 2019 at 13 00 h to 6 2 2019 at 8 00 h during the entire period of study water temperature ranged from 12 4 to 13 6 c with mean value of 12 9 c which is very similar to the average annual air temperature in catchment 3 methods the karst catchment is considered as a black box system where the discharge from spring and the quality of spring water are initiated by rainfall on catchment the rainfall is considered as a proxy for groundwater recharge which is a reasonable assumption for a highly karstified catchment with scarce vegetation and generally thin or completely absent soil layer the relationships between rainfall and discharge rainfall and water quality discharge and water quality and water quality parameters itself are investigated by means of the time series analysis including correlation partial correlation and spectral analyses the correlation and spectral analyses involve the determination of autocorrelation functions acf and cross correlation functions ccf in the time domain as well as the determination of spectral density functions sdf gain functions gf coherence functions cf and phase functions pf in the frequency domain these functions have found a wide application in hydrological system analyses so the mathematical background can be found in several papers e g mangin 1984 padilla and pulido bosch 1995 jeannin and sauter 1998 larocque et al 1998 guo et al 2021 a short theoretical explanation of all applied functions and their expressions are summarized in appendix a the partial correlation analysis involves the determination of partial autocorrelation functions pacf and partial cross correlation functions pccf jukić and denić jukić 2015 let time series x and y have a causal relationship where x is transformed to y by an intervening system and where r xy k represents ccf between series x and y appendix a let z represent a time series that controls the process of transformation of time series x to y the linear effect of control time series z can be removed from r xy k by calculating pccf 1 r xy z k r xy k r xz r zy k 1 r xz 2 1 r zy 2 k where r xz is correlation coefficient between series x and z whereas r zy k is ccf between series z and y the linear effect of control time series z can also be removed from acf the result is pacf specifically pacf for time series x and y respectively are 2 r xx z k r xx k r xz r zx k 1 r xz 2 1 r zx 2 k 3 r yy z k r yy k r yz r zy k 1 r yz 2 1 r zy 2 k for two control time series z 1 and z 2 the second order pccf is obtained as 4 r xy z 1 z 2 k r xy z 1 k r x z 2 z 1 r z 2 y z 1 k 1 r x z 2 z 1 2 1 r z 2 y z 1 2 k generally following the same principle for a set of n control time series z 1 z n the n th order pccf is obtained by recursion 5 r xy z 1 z n k r xy z 1 z n 1 k r x z n z 1 z n 1 r z n y z 1 z n 1 k 1 r x z n z 1 z n 1 2 1 r z n y z 1 z n 1 2 k the effect of control time series z 1 z n is investigated by comparing r xy k and r xy z 1 z n k theoretically five types of effects can be distinguished at a lag k 1 explanation effect if r xy k and r xy z 1 z n k have opposite signs 2 control effect for r xy z 1 z n k 0 3 partial explanation effect for 0 r xy z 1 z n k r xy k 4 no effect for r xy z 1 z n k r xy k and 5 suppression effect for r xy z 1 z n k r xy k it should be noted that absolute values are used above because r xy k and r xy z 1 z n k both can have negative signs the confidence interval for pccf is the same as for ccf which means that the lower and upper limits can be estimated to recognize statistically significant values of pccf e g banik and kibria 2016 approximately 95 confidence limits are 2 n where n is length of time series the following general rule for the notation of functions is used name x y z 1 z n where name is a function abbreviation acf ccf pacf or pccf x and y are time series with a causal relationship and z 1 z n are control time series 4 results 4 1 correlation and spectral analyses the obtained autocorrelation functions for time series q nt ec and t are presented in fig 3 a they quantify the linear dependency of successive values of these time series for time series q acf q provides information about the storage capacity of karst system the so called memory effect of karst system is defined as the time lag where acf q becomes less than 0 2 mangin 1984 it amounts 990 h or approximately 41 days assuming that the same approach can be applied also to time series nt ec and t the corresponding memory effects for these three series are 120 1640 and 1680 h respectively irregularities observable in acf n t at lags above 500 h are very similar to irregularities in acf q which is a consequence of similar fluctuations in values of q and nt the cross correlation functions between time series q nt ec and t are analyzed in fig 3b the time to peak value of ccf q n t gives an estimate of the timing of sediment transfer through the karst system where the discharge is a referent point it amounts only 1 h which indicates that q and nt are synchronized almost completely ccf q n t has larger values at negative lags because the memory effect of q is larger than the memory effect of nt the times to peak of ccf q e c and ccf q t give an estimate of the fresh rainwater arrival time they amount 49 and 55 h respectively which shows that the arrival of infiltrated rainwater does not coincide with the arrival of sediment generally time series ec and t have very similar fluctuations so ccf e c t has the highest values in fig 3b this function is practically axisymmetric around the peak value at 12 h which shows that time series ec and t are generated by similar processes the results of cross spectral analysis are presented in fig 4 where the variations in values of time series q and nt are compared in the frequency domain an annual periodic component at frequency of 0 000142 1 h is evident in sdf q in fig 4a it is a consequence of seasonality of karst spring discharge which is dominantly generated by the seasonal fluctuation of evapotranspiration and to lesser extent by the seasonal fluctuation of rainfall on the catchment however this periodic component is only partly transferred to turbidity in fact it is mostly attenuated so the peak in sdf n t at 0 000142 1 h is hardly evident fig 4a detail a in addition to this low frequency range attenuation the form of gf q n t in fig 4b detail b shows that the high frequency components are also attenuated so only values around frequency of 0 1 1 h indicate a small amplification it means that the variation of turbidity generally does not follow the variation of discharge and that the variation of turbidity is generally more uniform the form of cf q n t in fig 4 c confirms this conclusion it can be noted that cf q n t decreases with frequency and that a linear relationship between q and nt practically does not exist at frequencies above 0 15 1 h in fig 4d it is evident that pf q n t has spurious discontinuities at this frequency range where it variates between π 2 and π 2 however the form of this function at frequencies below 0 1 1 h is smooth enough to estimate the time delay between q and nt fig 4d detail c it is approximately 1 h which is equivalent to the time to peak value of ccf q n t in fig 3b the obtained cross correlation functions between time series of rainfall pd and time series q nt ec and t are presented in fig 5 it can be noted that the response of jadro spring to rainfall events is fast so the maximum value of ccf p d q is obtained at lag of 25 h assuming that ccf p d q represents adequately the unit response function the change in slope of this function at lag of about 330 h indicates the duration of quick flow component in discharge the remaining part of ccf p d q above 330 h represents baseflow it can be noted also that ccf p d n t has larger values than ccf p d q during first 49 h i e turbidity is better correlated with rainfall than discharge during first two days of spring response the time to peak value of ccf p d n t amounts 27 h after that ccf p d n t decreases rapidly and it becomes statistically insignificant at lags corresponding approximately to the duration of quick flow component the part of ccf p d n t representing baseflow component variates close to the confidence interval and it has similar irregularities like ccf p d q which means that this part of ccf p d n t probably has not a physical meaning i e any component of turbidity that comes to the spring with a time delay cannot be identified confidently concerning electrical conductivity the obtained form of ccf p d e c shows that the effect of infiltrated rainwater is registered in the discharge during a period of about 3100 h or approximately 4 months after rainfall this period can be divided in three segments the period of decrease in conductivity that corresponds to the arrival of fresh rainwater period a fig 5 the period of increase in conductivity that corresponds to the withdrawal of fresh rainwater period b fig 5 and the period of emptying of previously accumulated water in vadose and phreatic zone period c fig 5 water temperature has similar behavior as electrical conductivity 4 2 partial correlation analysis the results presented in fig 4a show that q contains a significant annual periodic component which is transferred to nt and consequently it is registered in acf n t fig 6 a similarly this non stationary component is registered also in acf e c and acf t fig 6a however the effect of annual periodic component can be removed from these functions by using pacf where q is the control time series the obtained pacf n t q pacf e c q and pacf t q are presented also in fig 6a the memory effects may be defined here as the lags where these functions become statistically insignificant the obtained results show that the memory effects of electrical conductivity and water temperature are practically same about 1740 h the memory effect of turbidity is around 160 h at larger lags pacf n t q is irregular similarly as acf n t which is a consequence of non stationarity so this part probably has not a physical meaning the effect of rainfall regime on the relationship between q and nt is estimated by comparing ccf q n t with pccf q n t p d p b p m in fig 6b it is evident that differences between these two functions are relatively small the cumulative effect of pd pb and pm is minor which means that other processes are dominantly responsible for the correlation between q and nt in order to estimate the effect of control time series pm pccf q n t p d p b is also presented it can be noted that pccf q n t p d p b is practically identical to pccf q n t p d p b p m which means that the rainfall from muć has no effect on the relationship between discharge and turbidity fig 7 presents the cross correlation functions between rainfall and discharge as well as the corresponding partial cross correlation functions obtained by using the rainfall observed at two neighboring rain gauges as the control time series the main aim of this analysis is to identify the statistically significant partial explanation effects of control time series which represent the specific contribution of each rain gauge location to the total discharge from spring the response of jadro spring to the rainfall events in catchment is fast so the times to peak value of c c f p m q c c f p d q and c c f p b q are 25 or 26 h these three functions have also similar forms this similarity is the consequence of the spurious correlation between pd pb and pm generally rainfall observed at neighboring locations are most often generated by the same meteorological events so a similarity in their variations always exists on the other hand the obtained p c c f p m q p d p b p c c f p d q p m p b and p c c f p b q p d p m have completely different forms they describe the spatial and temporal differences in the specific contribution of each rain gauge location the quick flow component is dominantly generated by the rainfall from dugopolje namely it can be noted that p c c f p d q p m p b has the highest values of partial explanation effect fig 7 detail a which takes place at legs between 0 and 140 h with the peak value at lag of 22 h the contribution of rainfall from bisko is much smaller and it is observable in p c c f p b q p d p m during the first 57 h and later between 132 and 273 h fig 7 detail b a statistically significant contribution of rainfall from muć is observable in p c c f p m q p d p b only at lags between 79 and 154 h fig 7 detail c concerning the baseflow component at lags larger than 330 h it can be noted that p c c f p d q p m p b and p c c f p m q p d p b variate irregularly inside the confidence interval statistically significant values are registered only in p c c f p b q p d p m fig 7 detail d which shows that the rainfall from bisko has the most important role in the generation of baseflow component the specific contribution of each rain gauge location to the electrical conductivity of spring water can be estimated from fig 8 where the cross correlation functions between rainfall and electrical conductivity are compared with the corresponding partial cross correlation functions determined for the control time series of rainfall observed at two neighboring rain gauges the obtained results are mostly in accordance with the results presented in fig 7 during first about 160 h period a fig 8 it can be noted that only p c c f p d e c p m p b has a statistically significant partial explanation effect of the control time series while p c c f p b e c p m p d decreases slowly inside the confidence interval during the next 380 h period b fig 8 p c c f p d e c p m p b becomes insignificant whereas p c c f p b e c p m p d becomes significant and it reaches a local minimum after about 540 h period c fig 8 a statistically significant partial explanation effect is notable only in p c c f p b e c p m p d it shows that the flushing of the phreatic zone is induced by the rainfall events in dugopolje and the discharge is generated mostly by the infiltrated rainwater from this location during the period of fresh rainwater arrival the rainwater from bisko comes to the spring during the period of rainwater withdrawal and later it is interesting that a contribution of rainfall from muć has not been registered at all namely in fig 8 it can be noted that c c f p m e c and p c c f p m e c p d p b have opposite signs which means that the control time series pd and pb explain completely the existing correlation between pm and ec a similar analysis was performed also for turbidity fig 9 it can be noted that c c f p m n t c c f p d n t and c c f p b n t go below the upper boundary of confidence interval at lags above 270 h and they approach to the horizontal axis at lags about 330 h which corresponds to the duration of the quick flow component the times to peak of these three functions are between 24 and 27 h which is synchronized with the times to peak obtained for discharge fig 7 during the first 112 h p c c f p d n t p m p b has the highest values of partial explanation effect of control signals fig 9 detail a the partial explanation effect registered in p c c f p b n t p m p d is much smaller fig 9 detail b after that p c c f p d n t p m p b becomes statistically insignificant whereas p c c f p b n t p m p d remains significant during the next 150 h fig 9 detail c it shows that turbidity is generated mostly by the rainfall from dugopolje during the first 4 to 5 days of spring response during the remaining period of the quick flow component turbidity is generated mostly by the rainfall from bisko a contribution of rainfall from muć is not registered namely an explanation effect of control time series is evident in p c c f p m n t p d p b at lags below 60 h fig 9 detail d whereas at higher lags the value of this function varies inside the confidence interval it means that the time series pd and pb explain completely the existing form of c c f p m n t and that the correlation between rainfall from muć and turbidity of jadro spring is spurious 5 discussion the temporal characteristics of the process of transport of infiltrated rainwater and sediment through the aquifer of jadro spring were investigated by means of correlation and spectral analyses three natural tracers were analyzed since the turbidity of water from jadro spring is dominantly generated by sediment nephelometric turbidity is considered as an indicator of the sediment concentration in spring water electrical conductivity and water temperature are considered as indicators of the presence of fresh infiltrated rainwater the results of partial correlation analysis revealed that the existing correlation between discharge and turbidity is not a consequence of rainfall fig 6b i e it is not a spurious correlation resulting from the initiation of discharge and turbidity by same rainfall events this finding is in accordance with the existing knowledge about the entrainment of sediment in karst conduits namely the discharge from karst spring is expected to be correlated with the velocity of flow through the karst conduits on the other hand the movement of sediment starts when a critical boundary shear stress is exceeded on the conduit bed which generally depends on the velocity of flow through the karst conduits e g atteia and kozel 1997 dogwiler and wicks 2004 herman et al 2008 2012 vuilleumier et al 2021 if this velocity is larger than a critical velocity the entrainment of sediment is activated if this velocity is lower there is no entrainment at all consequently a causal relationship between discharge and turbidity exists but it is nonlinear so the coherence between discharge and turbidity is low fig 4c and the variation of turbidity is generally more uniform fig 4b the memory effect is usually defined in karst hydrology as the time lag where acf of discharge becomes less than 0 2 mangin 1984 assuming that the same principle can be applied also to the time series of turbidity electrical conductivity and water temperature the estimated memory effects amount 120 1640 and 1680 h respectively fig 3a however the results of spectral analysis show that the time series of discharge contains a significant annual periodic component fig 4a this and other non stationary components in discharge are transferred to the time series of turbidity electrical conductivity and water temperature which means that their acf is deformed and maybe do not represent adequately the actual linear dependency of successive values of these time series consequently pacf was also used for the estimation of memory effects of turbidity electrical conductivity and water temperature the control time series is the discharge the obtained memory effects of electrical conductivity and water temperature by this method are practically same about 1740 h fig 6a it shows that the fresh rainwater from catchment is drained after approximately 72 days after that seasonal groundwater storage is activated and inter catchment groundwater flows become dominant jukić and denić jukić 2009 the obtained memory effect of turbidity amounts 160 h which is 40 h longer than the value obtained by classical method of mangin 1984 it indicates that the spring water becomes clear usually within a period of approximately 7 days after a turbidity peak the spatial characteristics of the process of transport of infiltrated rainwater and sediment through the aquifer of jadro spring were investigated by using pccf the time series of rainfall from the analyzed rain gauge location is considered as the input the time series of discharge turbidity and electrical conductivity from the jadro spring are considered as the outputs the control time series are rainfall observed at two neighboring rain gauge locations the specific contribution of each rain gauge location to the dynamic of discharge turbidity and electrical conductivity was identified the obtained results show that maximum discharge is reached within a period of about 24 h after rainfall events fig 7 the quick flow component lasts about 330 h or approximately 14 days which is in accordance with previous results denić jukić and jukić 2003 jukić and denić jukić 2009 2015 it is dominantly initiated by the rainfall from dugopolje the contributions of rainfall from other two rain gauge locations are notable but they are much smaller on the other hand the baseflow component is dominantly generated by the rainfall from bisko the specific contributions of rainfall from dugopolje and muć to this component are practically insignificant the dynamic of electrical conductivity is in accordance with the above the rapid decrease in conductivity lasts about 160 h and it denotes the arrival of fresh rainwater to the spring this decrease is generated by the rainwater from dugopolje fig 8 the fresh rainwater withdrawal lasts next 380 h during this period and later during remaining period of baseflow the rainwater from bisko has most important role a contribution of rainfall from muć was not registered at all concerning turbidity maximum is reached about 24 h after rainfall event fig 9 during first about 112 h the presence of turbidity in spring water coincides with the arrival of infiltrated rainwater from dugopolje detail a in fig 7 period a in fig 8 detail a in fig 9 after this period only the rainfall from bisko has a statistically significant contribution to turbidity which lasts approximately till the end of quick flow component again a contribution of the rainfall from muć was not registered at all previous investigations in karst show that depending on its origin turbidity of water from a karst spring may be recognized as the remobilized or aquifer derived turbidity that contains autochthonous sediment from aquifer and the flush through or soil derived turbidity that contains allochthonous sediment generated by soil erosion on catchment surface e g schiperski 2018 the increase in turbidity coincident with the rise in stage or in discharge has been observed by several authors e g valdes et al 2006 pronk et al 2009 herman et al 2012 schiperski et al 2015a they all have noted that turbidity is increased due to entrainment of autochthonous sediment during periods of high velocity within karst conduit system on the other hand there have been also several studies where discharge and turbidity do not have same evolution e g ryan and meiman 1996 pronk et al 2007 schiperski et al 2015b or their peaks are not synchronized because of chaotic behavior of turbidity during flood events e g vuilleumier et al 2021 in these examples turbidity was generated by allochthonous sediment generally a simultaneous and synchronized variation of turbidity and karst spring discharge indicates the existence of autochthonous sediment the existence of allochthonous sediment is registered as a peak in turbidity that has a time delay from the peak of discharge however these relationships have not always straightforward interpretation so analyses of other parameters are required to identify origin of turbidity such as electrical conductivity e g vesper and white 2003 valdes et al 2005 heinz et al 2006 2009 a drop of electrical conductivity coincident with a peak in turbidity indicates the probable presence of allochthonous sediment the results obtained in this study show that the variations of discharge and turbidity are simultaneous and synchronized namely the time delay between turbidity and discharge amounts only 1 h the time to peak in ccf between rainfall and discharge is practically equal to the time to peak in ccf between rainfall and turbidity turbidity is even better correlated with rainfall than discharge during the first 48 h of spring response in addition there is not any significant increase in turbidity that comes to the spring with a time delay the drop of electrical conductivity and water temperature is registered only during the period of high flow after that they increase slowly and recover completely during low flow this drop does not coincide with the peak in turbidity i e there is a time delay of 48 h between them considering the above it can be concluded that the origin of turbidity on the jadro spring is dominantly autochthonous sediment this type of sediment seems to prevail in the spring water probably because of the size of catchment namely allochthonous sediment may be introduced into karst aquifer by autogenic diffuse recharge autogenic concentrated recharge and allogenic concentrated recharge through ponors since there is not a component in turbidity that comes to spring with a time delay it is evident that allogenic concentrated recharge has insignificant effect on dynamic of turbidity allochthonous sediment is transported over long distances so it is probably settled before reaching the jadro spring however the presence of turbidity in the spring water coincides with the arrival of infiltrated rainwater from dugopolje so there is a possibility that a certain quantity of allochthonous sediment was introduced by autogenic recharge and it was mixed with the autochthonous sediment this is only a hypothesis that should be evaluated by further investigations concerning the rain gauges in dugopolje and bisko the obtained results are in accordance with the existing knowledge about the hydrological functioning of the catchment of jadro spring e g jukić and denić jukić 2009 2015 the rain gauge in dugopolje is the closest to the spring and it represents the highly permeable central part of catchment located in the hinterland of the spring the rain gauge in bisko represents the eastern part of catchment which is a region of complex hydrogeology that ranges from highly permeable carbonate rocks to low permeable flysch deposits this region is also a transition zone for the inter catchment groundwater flow from the cetina river catchment which means that a well developed channel network between this area and the jadro spring exists consequently the quick flow is dominantly generated by the rainfall from dugopolje whereas the rainfall from bisko is mainly responsible for the remaining part of the spring hydrograph since the turbidity originates from the aquifer autochthonous sediment and maybe from allochthonous sediment from the part of catchment located in the hinterland of the spring it is also dominantly initiated by the rainfall from dugopolje concerning the results obtained for the rain gauge in muć the situation seems much more complex jukić and denić jukić 2015 noted that the results of partial correlation analysis obtained for muć are different from the results obtained for bisko and dugopolje in terms of the absence of specific contribution to the quick flow component this difference was explained by the hydrological functioning of karst fields namely superficial deposits in karst fields accumulate water on the surface and overland flow is formed occasionally after long lasting intensive rainfall when the capacity of superficial deposits are fulfilled this overland flow then goes very quickly underground through the ponors during less intensive rainfall superficial deposits temporary retain water and then release it slowly and diffusely to the karst underground which means that they function as low or medium permeable layers consequently the contributions of karst fields are occasional and with a time delay so the relationship between rainfall and spring discharge is highly non linear however the results obtained in this study indicates that the rainfall from muć affect the response of jadro spring only in the form of a small pressure pulls which is registered in the quick flow component at lags between 79 and 154 h any effect of the rainfall from muć on the electrical conductivity or the turbidity of water from jadro spring was not identified even though the area of mućko polje has the highest apparent groundwater flow velocity according to tracer tests fig 1 it confirms that results of tracer tests cannot be considered as reliable qualitative or quantitative indicators of the sediment transport through karst aquifers 6 conclusion the correlation partial correlation and spectral analyses were applied on the time series of rainfall spring discharge electrical conductivity nephelometric turbidity and water temperature collected in the catchment of jadro spring the correlation and spectral analyses involved the determination of acf and ccf in the time domain as well as sdf gf cf and pf in the frequency domain these functions are used for determination of the temporal characteristics of sediment transport process such as the timing synchronization with discharge and time delay between arrivals of sediment and fresh rainwater to spring the partial correlation analysis involved the determination of pacf and pccf pacf is used to remove the seasonal periodicity from acf in order to estimate more reliably the memory effects of turbidity electrical conductivity and water temperature pccf is used for the spatial characterization of sediment transport processes including the origin of sediment the presented results show that pccf can be very useful in the investigation of mass transport through the karst aquifer especially in the situation of scarce data and information pccf enables identification of the contribution of a specific location in catchment to the quantity and quality of spring water for example the contributions of three rain gauge locations to the discharge turbidity and electrical conductivity of spring water could be identified in this study by analyzing and comparing the obtained results the origin of sediment in spring water is determined as well as the origin of quick flow and baseflow components including the timing of fresh rainwater arrival rainwater withdrawal and emptying of previously accumulated water in vadose and phreatic zone this spatial information cannot be obtained by using any other method of time series analysis pccf can help also in resolving some ambiguities related to the origin of correlation between two time series such as discharge and turbidity the obtained results for jadro spring are in accordance with results of previous studies and they mostly support the existing hypotheses about the hydrological functioning of this spring concerning the process of sediment transport it is initiated after heavy rainfall in the central and eastern part of catchment the origin of sediment in spring water is dominantly the autochthonous sediment from aquifer and possibly the allochthonous sediment from the part of catchment located in hinterland of spring e g dugopolje credit authorship contribution statement damir jukić conceptualization methodology software formal analysis investigation writing review editing vesna denić jukić resources data curation validation writing review editing ana kadić data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research is partially supported through project kk 01 1 1 02 0027 a project co financed by the croatian government and the european union through the european regional development fund the competitiveness and cohesion operational program appendix a let time series x and y have a causal relationship where x is transformed to y by an intervening system the covariance function between series x and y both of length n is defined by 1 c xy k 1 n t 1 n k x t μ x y t k μ y k 0 1 2 m where μ x and μ y are the means of x and y respectively the truncation point m determines the time interval in which the analysis is carried out the cross correlation function ccf between time series x and y is 2 r xy k c xy k σ x σ y k 0 c yx k σ x σ y k 0 where σ x and σ y are the standard deviations of x and y respectively ccf provides information about the significance of causal and non causal relationships between series x and y if ccf is symmetrical around a vertical axis series x and y are generated by a third independent series which means that y is not affected by x if ccf is not symmetrical it means that y is affected by x it should be noted that the correlation coefficient cc between x and y is r xy 0 the autocorrelation function acf of time series x and y can be obtained from eq 2 as 3 r xx k c xx k σ x 2 r yy k c yy k σ y 2 where k 0 acf of time series describes the duration and significance of influence of an event on subsequent measurements i e it maps the memory effect of time series existing periodicities in the time series are also registered in this function which can significantly affect the form of acf spectral analysis is complementary to the correlation analysis specifically spectral density function sdf represents a fourier transform of acf whereas cross spectral density function csdf represents a fourier transform of ccf for r xx k and r yy k corresponding sdf are 9 s x f 2 1 2 k 1 m d k r xx k c o s 2 π f k 10 s y f 2 1 2 k 1 m d k r yy k c o s 2 π f k for frequencies f l 2 m l 0 1 2 m where 11 d k 1 c o s π k m 2 represents the lag window proposed by tukey 14 15 which ensures that the estimated values are not biased sdf visualize the periodicity content of the time series periodic components in time series are represented as peaks in sdf so this function is used for identification of the frequency and the relative importance of each periodic component on the other hand csdf is a complex function which has the following form in polar coordinates 12 s xy f α xy f e x p i φ xy f where i is imaginary unit and where the cross amplitude function caf and the phase function pf are calculated by the following expressions 13 α xy f ψ xy 2 f λ xy 2 f 14 φ xy f a r c t a n λ xy f ψ xy f 15 ψ xy f 2 r xy 0 k 1 m r xy k r yx k d k c o s 2 π f k 16 λ xy f 2 k 1 m r xy k r yx k d k s i n 2 π f k pf indicates dephasing between two time series in the time mode it shows the time delay between x and y for different frequencies 17 τ φ xy f 2 π f caf describes the transformation of the periodic components in x by the intervening system i e it describes the way in which series x is modified by the system using this function two additional functions in the frequency domain are defined the first is gain function gf which is obtained as 18 g xy f α xy f s x f and the second is coherence function cf 19 c xy f α xy f s x f s y f gf describes an amplification g xy f 1 or an attenuation g xy f 1 of the variation of y at frequency f in comparation to the variation of x at same frequency cf shows whether the variation of y at frequency f corresponds to the variation of x at same frequency i e it expresses the linearity of relationship between x and y the relationship is linear if c xy f 1 
3408,in this paper we propose a new method to map the fracture network structure in a heterogeneous aquifer from inversion hydraulic head data measured during pumping tests in hydraulic tomography mode this inversion tool is based on the new concept of convolutional neural networks which provides a direct approximation to the inverse function linking fracture geometry to hydraulic data in order to handle the highly nonlinear inverse function more effectively an advanced neural network is developed from segnet architecture with encoder decoder structure which excels in image processing to translate the water level image associated with the pumping tests at the input into a fracture map at the output the network is trained with a synthetic dataset where the fracture structure and matrix heterogeneity are randomly generated and the hydraulic head are obtained by solving the groundwater flow equation the trained network accurately maps different complexity levels of fractures embedded in a matrix with heterogeneous transmissivity as a data driven approach the accuracy of the mapping depends on the quantity quality and relevance of the synthetic dataset used in the training phase while generating data to train the network requires effort the trained network performs each inversion instantly the inversion result appears to be stable even in the presence of data noise reliably interprets the hydraulic data if they share comparable fracture and matrix properties as specified in the training models keywords neural network fractured aquifer convolution neural network groundwater inversion 1 introduction characterizing the hydraulic properties of fractured karstic aquifers is known to be one of the most challenging tasks in the quest to understand groundwater dynamics and contaminant transport in these complex environments neuman 2005 indeed due to the large variability in fractures and rock matrix as well as the permeability gap between them these fractured aquifers exhibit extremely complicated groundwater flow patterns water flows in this heterogenous structure are mostly concentrated in a percentage of the highly permeable matrix and in sparse fracture networks that emerge across a vast impermeable region budd vacher 2004 maréchal et al 2004 identification of these fracture networks is therefore crucial in determining strategies for exploiting and protecting any reservoir various tools have been tested and developed in the literature to locate preferential flows associated with the presence of fractures including tracer tests conventional pumping and cross pumping techniques dverstorp et al 1992 dahan et al 1999 dausse et al 2019 cross pumping tests are referred to as a hydraulic tomography technique that captures the spatial variability of the hydraulic properties of the porous or fractured aquifer from joining hydraulic drawdown data recorded in successive pumping tests illman 2014 typically processing of these acquired hydraulic data is performed using an inversion code to determine models of hydraulic conductivity and storativity which are numerically consistent for the observed hydraulic data the inversion method employs a forward problem to numerically provide hydraulic responses for a given model of hydraulic properties by solving for groundwater flow based on the darcy equations in this method the forward operator is solved repeatedly until achieving a satisfactory match between the observed and simulated hydraulic data yeh 1986 for the characterization of fractured karstic aquifers the groundwater flow equation can be solved using one of two distinct approaches equivalent porous medium epm and discrete fracture networks dfn in the ecm concept fracture networks are approximated as a porous medium in which the fractured aquifer is fully discretized with small cells that represent the effective values of transmissivity and storativity that are being identified during the inverse process yeh liu 2000 zha et al 2015 tiedeman barrash 2020 this approach is straightforward to implement and parameterize the spatial heterogeneities of the hydraulic property fields in the inverse process it however ignores the discontinuity features of groundwater flow in fractured environments wang et al 2016 in contrast dfn concepts preserve the fracture shape and its hydraulic discontinuities via parameterizing the fracture and conduit structures as lines in 2d or surfaces in 3d and linking their hydraulic properties to their apertures fischer et al 2018 ringel et al 2019 mohammadi illman 2019 despite its realistic representation this parameterization is practically complicated due to the significant time required in numerical simulation and the difficulties in determining both the spatial geometries of the fractures and their apertures in the inverse process for this reason the number of applications of dfn parameterization in the inverse problem with stochastic or deterministic optimizers remains limited in comparison to epm in this paper we explore the potential of deep learning tools in the inversion of hydraulic data monitored in pumping tests to identify conduit and fracture networks compared to stochastic and deterministic algorithms the deep learning algorithm performs a direct approximation of the inverse function linking the piezometric measurements as input to the hydraulic properties in the output this link between the two is made through multiple linear and nonlinear operations performed sequentially on layers of the network such a decomposition provides an efficient way to approximate highly nonlinear functions as in the case of inversion operators elanayar shin 1994 the concept involves the use of several multidimensional coefficients called weights and biases assigned to the network layers the determination of these coefficients is based on an optimization process performed during the training operation which uses a known set of input and output data called the training dataset thus most of the effort and computational time required to build a neural network relates to building the database for training and the training task itself once trained however the trained network provides an end to end operator that performs inversions of experimental data instantly deep learning algorithms are progressively gaining ground in the geosciences particularly the emergence of the new powerful neural networks that enables to handle complex problems in geophysical imaging remote sensing and other environmental monitoring benjamin et al 2020 lary et al 2016 moseley krischer 2020 some preliminary investigations adopted deep learning algorithms using vanilla neural networks for example to interpret measurements from pumping or tracing tests aziz wong 1992 zio 1997 balkhair 2002 trichakis et al 2009 comparative analyzes reveals the efficiency of neural network as a replacement for traditional interpretations however fully connected layers in a vanilla neural network with a high number of learnable coefficients lead to training task overload when dealing with high dimensional problems recent breakthroughs in the field of data science have given rise to the new networks based on the convolutional concept that efficiently takle tomography problems with reasonable effort lecun bengio 1998 convolutional neural networks cnn translate the input image by scanning the shared small filters region by region to extract features on the entire image and construct local connections between neighboring pixels such a sharing mechanism facilitates the training task and information extraction compared to processing point by point as in the vanilla network rawat wang 2017 convolutional architecture performance has sparked a surge of research and applications in geophysical and hydraulic imagery for approximating inversion operators to identify the subsurface properties from sparse observations on the boreholes or at the surface among them liu et al 2020 built a cnn architecture to map the 2d electrical resistivity in the subsurface from apparent resistivity images inferred from surface data while vu jardani 2021 developed a cnn ert3d neural network to process 3d resistivity tomography other efforts applied the networks in seismic exploration to reconstruct the seismic velocity models directly from seismogram data li et al 2020 zhang lin 2020 apolinario et al 2019 park sacchi 2020 similar ideas were tested for the inversion of electromagnetic data to identify the 1d or 2d subsurface resistivity puzyrev and swidinsky 2021 puzyrev 2019 in hydrogeology laloy et al accessed the effectiveness of a generative adversarial network gan in mapping hydraulic conductivity heterogeneities in 2d and 3d binary pattern laloy et al 2017 laloy et al 2018 bao et al 2020 extended the coupling of gan and ensemble smoother with multiple data assimilation to reconstruct the binary media and reduce the prediction uncertainty sun 2018 suggested a composition of gan and state parameter identification to map simultaneously the parameter space as the permeability and the corresponding model state as the subsurface structure other initiatives employed encoder decoder architectures to image the spatial distribution of the transmissivity field from measurements of hydraulic heads jardani et al 2022 or tracer concentrations vu jardani 2022 however all these applications focus solely on mapping heterogeneity in a porous aquifer to our knowledge only a few works contribute to determination of the fracture network geometry in this paper we propose a cnn based on the segnet architecture to identify the fracture network structure from synthetic data of hydraulic head monitoring in a fractured aquifer the study is structured as follows section 2 outlines the theoretical background which summarizes the forward problem with groundwater equations the concept of the inverse problem approximated by segnet and its differences from classical inversion methods details of the data generation learning process and validation of the trained network follows in section 3 section 4 is devoted to accessing the performance of the proposed network in the face of possible interferences in practice which could be related to the data quality and quantity the heterogeneity in matrix or the relevance of dataset features finally a summary of the main findings with suggestions for future work concludes the paper 2 theoretical background in this section we summarize the theory of which the first part includes the forward problem the concept of the inverse problem with neural network followed by the description of proposed neural network architecture and its working mechanisms 2 1 forward problem groundwater modelling as mentioned earlier inversion by a deep learning algorithm is also based on the forward problem where generating the training data set involves solving the groundwater flow equation this training data consists of the numerically computed hydraulic response to the pumping tests corresponding to a given synthetic transmissivity field and discrete fracture networks fractures in the aquifers are geometrically represented by 1d lines and assigned to an equivalent transmissivity whereas matrix rock is characterized as a porous medium a heterogeneous transmissivity field the neural network then learns the relationship between the hydraulic head responses and the permeable fracture structures rooted in the training dataset in this work the inversion task aims only to reconstruct the geometric structure of fractures without rebuilding the transmissivities in fracture and matrix in steady state conditions numerical simulation of groundwater flow in a fractured aquifer is performed using two connected equations derived from darcy s law that describe water flows in the rock matrix and fractures kohl et al 1997 1a t m h q in the matrix γ m t t f t h q in the fracture γ f subjected to the following boundary conditions 1b h h d a t γ d where and t are the gradient and tangent gradient operators respectively tm and tf m2 s denote the transmissivity of heterogenous matrix γm and fractures γf respectively the fractures are shaped as lines which is randomly generated while their equivalent transmissivity is assumed constant h m is the hydraulic head q l s m represents the punctual water source at the extraction injection wells to form boundary conditions the observation area is embedded in a large buffer zone assigned to the mean transmissivity of the matrix blocks such large bounds limit the boundary effects on the computational results for completeness a constant hydraulic head hd is imposed at the buffer zone periphery γd in this study the source of water is the paired pumping wells each pair consists of one extraction and one injection well located on opposite sides of the observation area each paired well pump at the same constant rate over the operating period finally we solve the set of flow equations numerically using a finite element method in comsol software to determine the hydraulic head at observed points which are then taken as input to the neural network 2 2 inversion problem with neural network mapping fracture from groundwater observation as mentioned in the previous problem equation 1 represents a numerical operator for computing piezometric data in response to a pumping test in an aquifer with matrix transmissivity t m fracture transmissivity t f with geometry c f the operator can be expressed as the following function with three dependent parameters 2 h f t m t f c f the operator works as a data generator for training the neural network which performs an inverse function to determine the fracture geometry from the hydraulic head data see fig 1 here we focus only on reconstruction of the fracture geometry without identifying the transmissivity field of matrix rock and fractures which will be handled in a future study thus the aim of this study is to retrieve the fracture geometry from the hydraulic data by approximating the nonlinear inverse operator f 1 between two fields using segnet network based on thousands of synthetic data sets 3 c f f 1 h ψ h θ where ψ represents the neural network whose learnable parameters are denoted as θ indeed the neural network such as segnet offers a possibility to directly relate the hydraulic data to the fracture geometries by determining a set of learnable parameters defined as sequential linear and nonlinear operations in segnet layers determination of the parameters is performed from solving an optimization problem with the following objective function 4 θ argmin 1 n c f ψ h i θ where the pair c f h i denotes a model and its corresponding measurement n is the number of pairs in the training dataset the operator denotes the cross entropy to measure the difference between the truth and predicted models a training process often demands a large dataset of which the generation task takes the most time in the building of a neural network however once the training is completed the trained network performs an inference instantly an end to end operator without intervention from users 2 3 segnet architecture direct inversion function in this paper the inversion operator is approximated by the segnet architecture which is designed to efficiently locate the fracture map from groundwater level measurements relied on the concept of convolutional neural network see fig 2 the segnet architecture was developed by badrinarayanan et al 2017 and has been rapidly proven in a variety of domains with numerous applications including traffic scene detection kendall et al 2017 jiang et al 2020 qin et al 2020 satellite image processing khryashchev et al 2018 mohammed edward 2019 sariturk et al 2020 geophysics pham et al 2018 mukhopadhyay mallick 2019 vu jardani 2021 however it is still uncommon in hydrology as well as other advanced algorithms in this discipline where progress is modest compared to other geosciences such as geophysics typically a neural network consists of multiple layers of artificial neurons that mimic the operations of their biological counterpart through mathematical functions indiveri et al 2011 when an input image is introduced into the network each layer operates on activation functions to interpret features before passing them on to the next layer the process ends with a mapping result reconstructed from the learned features by matching the truth in the given datasets with its reconstruction from the network all parameters in the activation functions are determined which is then named as the training task network performance evidently relies on how efficiently the features are translated after this training process a more complex network appears to better refine the learned features that is why the proposed neural network is based on an advanced architecture bengio et al 2013 the proposed architecture as shown in fig 2 consists of successive network blocks with four encoders and four decoders finalized by a pixel wise classification layer to map the fractures this u shaped form consists of 63 layers divided into two paths a contracting one with encoders and an expanding one with decoders each having the same structure but arranged in reverse order the encoders extract the features embedded in the input images using a set of sequential convolutions each followed by a batch normalization bn and a rectified linear unit relu and finally a max pooling operation on the opposite path the decoders start with an un max pooling operation followed by a sequence of convolutions bn and relu layers the max pooling indices from each encoder are assigned to the corresponding decoder to compensate for the resolution loss in the encoders this sharing mechanism is the core aspect of a segnet architecture badrinarayanan et al 2017 but how does the network reconstruct the fractures the concept of a convolutional neural network relies on scanning small filters over entire images to extract key features which represent the local connectivity among neighboring pixels in this sequential convolution the first layer usually extracts basic features such as horizontal or diagonal edges while more complex features are detected in the next layers such as corners edges or shapes ghafoorian et al 2017 the deeper it goes the more complex features the network can detect fukushima 1988 in this manner convolutions translate the fracture information embedded in the input water level images to yield the fracture map number of convolution level is designed in response to problem complexity however deeper convolutional translation inevitably entails a large system of filters that soon overburdens the training task to save time and resources a max pooling operation is invoked to sort out only maximum values by shrinking the spatial size of the convolved features the operation speeds up the processing and is effectively deals with overfitting issue by working only on maxima the max pooling conserves the most prominent invariant features along with dimensionality reduction nagi et al 2011 since the fracture is prominent in this case the operation acts as a denoising agent to highlight fractures from the matrix heterogeneity however the operation results in a downsized output and lose of information to compensate the side effects state of each max pooling is transferred to the corresponding un max pooling in decoders to counterbalance the degradation thanks to sharing max pooling indices the segnet outperforms other networks such as fcn deeplab deconvnet in terms of accuracy badrinarayanan et al 2017 for example the proposed network with 4 encoder decoder levels requires only 0 5 million parameters which is far less than unet with 8 million a smaller size is also related to the fact that the network employs only 3 3 64 filters which are partly smaller than those in the unet recall that our network is designed with 4 encoder decoder levels shallower than the original network as it processes fewer output segmentation labels 3 application this section is devoted to the training process in a synthetic application we begin with a generation of aquifer models each with a fracture network embedded in a heterogenous ground water level responses in each aquifer model are observed with a monitoring scheme designed to mimic a real experiment where the acquisition setup maps the groundwater level in injection tests following that the acquired data is processed to feed training of the network the training is then implemented along with hyperparameters evaluation metrics and its evolution during the process typical results are shown to validate the trained network 3 1 aquifer model fracture network embedded in a heterogeneous ground in this study the heterogeneity in aquifer transmissivity is generated based on statistical parameters following a gaussian distribution for this purpose we use the sgems code implemented in matlab to create 35 000 transmissivity models where the distribution of log10t is randomly constructed using a gaussian variogram with constant mean and variable range remy et al 2009 heterogeneity in transmissivity models ranges from 10 8 to 10 4 m2 s in four orders of magnitude which corresponds to the range of a permeable aquifer the generated transmissivity fields are then assigned to a 100 100 m area with a locally constant aquifer depth fig 3 each ground model is assigned a fracture network geometry which consists of no to three fractures assigned randomly the transmissivity of fracture is set at 10 2 m2 s which is of orders conductive than the surrounding matrix with a mean of 10 6 m2 s all fractures are formed from 1d random generation using a gaussian variogram in sgems and then randomly rotated to redirect their orientation to establish a monitoring scheme we install 49 observation wells in a regular array in both directions on the synthetic aquifers fig 3 at the field boundaries we arrange 4 paired pump wells in each pair an up pumping well is in opposite position to its down pumping this arrangement of the hydraulic forces in pairs allows realizing a hydraulic disturbance of the whole investigation area in addition this configuration in practice saves water resources during the hydraulic tomography test 3 2 data acquisition and data processing in each model groundwater responses due to pumping injection tests are determined by computing the forward problem described in section 3 1 four pumping injection tests are performed sequentially at a constant flow rate of 10 l min in each well hydraulic measurements are acquired on the 49 observation wells and then interpolated using the akima method to obtain 4 maps of hydraulic head the input and output maps in the training datasets use the same spatial resolution being discretized by a 64 64 grid on the 100 100 m study area recall that in this study we focus only on mapping the network of fractures as preferential flow paths during pumping injection tests the presence of fractures is often associated with abrupt changes in groundwater flow to better identify these discontinuities we calculate the gradient in both directions x and y of the hydraulic head maps interpolated from the hydraulic head measurements the resulting gradients are then reshaped into a 64 64 grid each pumping injection test generates 2 gradient maps x y to build the gradient map set of 8 channels 64 64 8 as the input of neural network training for the output map the fractures are labelled as 1 and the others as 0 on the gridded map to quantitatively assess the predictions accuracy of the network we use the accuracy coefficient as defined below 5 accuracy 1 n j 1 n y j y j where y and y denote the labels fracture or ground in the true model and its corresponding prediction respectively n represents the total pixel number in the output image 64 64 3 3 training process and results for the training phase we randomly separate 25 000 aquifer models with corresponding measured data 20 000 for training the network and 5 000 for validation during the process a high fraction of validation dataset aims to better evaluate efficiency of the training process training is performed using the adam optimization algorithm implemented in matlab on a dell precision tower 5810 with a single gpu nvidia quadro k2200 the task is completed in 5 h 60 epochs with a constant learning rate of 0 01 and a batch size of 128 fig 4 summarizes evolution of accuracy along the training process the trained network is then evaluated with 10 000 unseen models each model is executed in 0 006 s almost instantly fractures are efficiently mapped over 10 000 samples with an average accuracy of 94 83 the overall reconstruction quality is summarized in the histogram shown in fig 14 high accuracy prediction is related to the fact that fractures with higher conductivity drive dynamics in the aquifer that favor their detection to investigate the sensitivity of the proposed model we train the network 50 times with the same dataset and hyperparameters as above the results show a consistent solution where the average accuracy of the testing dataset centers at 94 86 with a standard deviation of 0 1 over the trained models to discuss the accuracy of the reconstructions in detail we select and show six representative results of the tested models in fig 5 six showcases are arranged in different levels of complexity of the fracture distribution starting with the models without fractures and ending with the networks consisting of three fractures the accuracy of reconstructions overall refers to a clear dependency on the density of the mapped fractures and their complexity level without fractures ex1 shows an accurate prediction where the matrix transmissivity heterogeneities do not produce artifacts in the interpretation a further discussion on this ground impact is addressed in section 4 5 with higher heterogeneity in the following cases ex2 and ex3 the trained network also properly reproduces the models with one or two fractures however in a more complex case in ex4 where two fractures cross closely there is a minor misinterpretation with a fused joint even though the reconstruction still faithfully represents the main aspects of the fractures case ex5 otherwise presents a dense fracture network in which three fractures are oriented in different directions these patterns are somewhat more complex than the previous ones but the network is still able to identify the fracture paths with accuracy along the reconstructed fractures in fig 5hij other slightly deformed curves or a minor discontinuity can be observed possibly due to the influence of ground heterogeneity the ground heterogeneity exhibits less influence on subsurface flow compared to fractures however by introducing more noise and locally changing the water table heterogeneity may misguide the understanding of fracture networks a detailed discussion on this problem is further elaborated in section 4 1 for results from homogeneous models in the last case ex6 also consists of three fractures but with a very complex geometry including two fractures close to each other which the network cannot distinguish in the reconstruction this misinterpretation stems from the fact that the two fractures behave as similar as a single fracture in the middle in general however the fracture reconstructions are very satisfactory indeed number of interferences can impact prediction efficiency which may occur in a real field under diverse situations some of the key issues are addressed in the following sections 4 discussion this section is devoted to studying the impact of various sources of uncertainty that can affect the quality of the reconstructions with the network such as the uncertainty about the degree of heterogeneity of the ground transmissivity and the choice of the size and the nature of features of the training models we also study the impact of the number of piezometers used in hydraulic tomography and the noise that can alter these hydraulic measurements 4 1 effect of heterogeneity in the matrix both porous ground and fracture network geometry associates in forming the subsurface dynamics yet the roles of each system in driving the groundwater are distinct dominant conductivity and connectivity of fracture network make it superior to the surrounding porous ground while fractures regulate the flow regime the permeable ground even of a lesser order is equally important to investigate the influence of heterogeneity in matrix transmissivity we repeat the learning process described in section 3 but this time with homogeneous transmissivity models we regenerate 35 000 models using the same fracture network configuration embedded in ground with a constant transmissivity of 10 6 m2 s the network is re trained using the same piezometric configuration and network hyperparameters as in section 3 3 the overall average accuracy for 10 000 test models shows a significant improvement from 94 83 in case 1 to 96 94 in this case details of the representative models are shown in fig 6 and the histogram is summarized in fig 11 the showcases in fig 6 prove a better reconstruction due to the absence of disturbance effect associated with a heterogenous ground particularly for the complex configurations in ex4 and ex6 the fused bottleneck in the prediction of ex4 has clearly disappeared accompanied by a better shape for both fractures other slightly deformed curves that appeared in fig 5hij also do not appear in fig 6hij similarly all the fractures are well shaped overall in ex6 where the two closed fractures are clearly delineated even some minor disturbances are shown on the top of the fractures comparison of the accuracy between the two sets of reconstructions confirms the heterogeneity importance of the porous ground in determining the fracture network structure further discussion on this issue is developed in section 4 5 where the heterogeneity deviates from a gaussian distribution predefined in the training dataset 4 2 effect of dataset size in general the effectiveness of a trained neural network in a deep learning algorithm depends strongly on the size of the dataset used in the learning phase in our inverse problem generating the training dataset involves solving a mass of a forward problem with numerical tools which is the longest and most laborious phase in building a neural network the ideal strategy is to determine the optimal size which reduces computation effort while resulting in an accurate prediction this can be accomplished by gradually increasing the data size and periodically verifying the prediction accuracy in the training validation and testing phases until a satisfactory result is achieved in this section we examine the impacts of training data size on driving the reliability of reconstructions by analyzing the predictions obtained with networks trained with three different sizes 5000 10000 20 000 and 30000 the networks formed with these datasets are tested on the 10 000 unseen models as in section 3 3 of which the metric results are reported in table 1 and in fig 14 for the histogram we recall the six representative models as in the previous tests to analyze the impact of dataset size in detail see fig 7 the use of a minimal number of training data is sufficient to results in reliable maps when the reconstructed fracture networks are straightforward as in the case of models ex1 to ex3 however when the configurations are difficult as in the cases of models ex4 to ex6 the predictions do not provide accurate identifications of the fracture structures with minor artifacts and missing features in the reconstruction a larger training dataset assists improving reconstructions in enriching the learning process through introducing more complex features advani et al 2020 better generalization thus requires a wide range of alternative models to cover the high complexity in karstic aquifer and to tackle the overfitting issue details of prediction accuracy in table 1 reveal a clear dependence on the size of dataset with overall mean accuracy increasing from 93 08 for the first subset 5 000 models in training to 93 95 for the second 10 000 models in training and to 95 19 of the third 30 000 models in training the enhancement in accuracy is however not proportional to the enlargement of dataset size since results from subsets of 20 000 case 1 and 30 000 models show no clear difference with 0 3 of improvement in accuracy for extra 10 000 samples overfeeding data overburdens the training task since increasing data volume ineffectively contribute to improve accuracy when performance reaches an asymptotic stage amari et al 1997 as mentioned at the beginning of this section the choice of training data size depends on the desired accuracy it is reasonable to gradually increase the size until this desired accuracy is achieved to avoid generating a large amount of unnecessary data 4 3 effect of amount of observation wells number of observed wells is an important consideration in design a monitoring plan as it determines measurement efforts and accuracy of any interpretation technique a dense observation network engages a better understanding of the field when it better covers the dynamics subsurface however it also imposes higher requirement on the costs and feasibility of a monitoring plan in a real field sometimes in this part we examine the importance of measurement points for reconstruction accuracy to achieve this the study evaluates two additional configurations of observation wells which share an identical pump setting but using less wells compared to the original configuration containing 49 piezometers the number of wells is first reduced by half 25 wells and then by one third 16 wells but we retain the pumping schemes with similar flow rates paired pumping and other conditions as performed for the original configuration of 49 wells in section 3 1 fig 8 in practice we rebuild new datasets in which the hydraulic head maps are interpolated with fewer measurements which lowers the input resolution and worsens the inversion quality on the 10 000 test models table 1 the results of the 10 000 test models in table 1 reveal a clear dependence of the reconstruction accuracy on the piezometric density over three configurations degraded data coverage results in a drop in overall reconstruction accuracy from 94 83 49 wells to nearly 92 with 25 and 16 wells the degradation tendency is also confirmed visually in the representative models where the reconstructions of the fracture network geometry from 25 well setup are less accurate than in the 49 well fig 9 for simple fracture networks in ex1 to ex3 the reconstruction can be accomplished accurately with only a limited number of observation wells however the dissimilarity in the results for 25 and 16 well setups implies that the hydraulic information in these two cases is only sufficient to infer the existence and overall shape of the fractures but not to identify them more precisely in complex cases another comparison of results between two configurations of observation wells in fig 9 highlights the impact of well spacing a scattered distribution of observation wells leads to a drop in accuracy of the fracture network reconstruction as case 4b shown in fig 9 where the reconstruction quality depends locally on the scarcity of observation wells indeed the relationship between the number of involved measurements and the reliability of resolved predictions is a common issue in any inversion problem regardless of the optimization method employed to ensure an accurate reconstruction there must be a sufficient number of wells to cover all heterogeneities in the target field 4 4 effect of pumping schemes to further investigate the impact of the amount of data in identifying fractures in this section we reduce the number of pumping wells and keep the original configuration with the same pumping rate and observation wells see fig 10 this modification in the monitoring scheme results in a reduction of the amount of data to half and quarter of the original scheme the reduction in data size leads to an overall deterioration in the result for the subsurface fracture network reconstruction with the average accuracy dropping to 93 53 in case 5a with 2 pairs of pumping wells and 91 50 in case 5b with a single pair details for the six representative examples are shown in fig 11 while the statistics are summarized in table 1 downsizing the observation data in this case undoubtedly leads to less coverage of observed data resulting in a less reliable prediction however the consequence is less severe than the reduction in observation wells where the lack of local information results in a coarse resolution that affects details of the reconstructed map this comparison can be used as a guide to the design of the monitoring system in a real field in this section we analyze the quality of reconstructions with piezometric data obtained with a low pumping rate of 0 5 l min while maintaining all other acquisition parameters such as the number of pumping wells and observation wells the low flow rate hence entails a small hydraulic disturbance confined to the vicinity of the pumping wells the absence or low response that can be affected by noise in distant wells cause a lack of information to perform a correct interpretation such difficulty can be the origin of inaccurate reconstructions with artifacts where the interpretation cannot distinguish the presence of a fracture network from the impacts of the heterogeneity of the surrounding matrix see fig 12 4 5 effect of observation uncertainty removing the noise that affects hydraulic data can be a complex task to perform prior to interpreting input data in an inversion algorithm thus identifying the influence of noise on imagery quality is a crucial step in determining the degree of uncertainty in the interpretation to analyze this influence we contaminate the hydraulic data with random gaussian noise where three standard deviations represent 5 15 and 25 of the original signal respectively contaminated data is then interpreted using the network formed with the original data section 3 3 to illustrate the result for 10 000 testing samples we group the histogram in fig 14 and summarizes the metric detail in table 1 while the predictions for the benchmark cases are shown in fig 13 according to the metric assessment average of all reconstruction accuracies the increase in noise correspond to a slight decrease in reconstruction accuracy from 94 59 percent for 5 noise to 93 38 percent for 15 and 91 90 percent for 25 noise noise appears to have only a minimal effect on the reconstruction accuracy of the fractures fig 10 details reconstructions of typical models where the fractures are well reconstructed despite some minor artifacts local noise that mimics the dynamic behavior of fractures in groundwater likely leads to the misinterpretation that produces these artifacts such a disturbance is then attributed to false segments as in ex1 or to slight misalignment of fractures as in the other cases ex4 to ex6 however the fracture networks are well identified overall regardless certain degree of uncertainty in the hydraulic data this minimal noise effect has been reported in the literature when applying cnn architectures to solve inversion problems e g in electrical resistivity tomography vu jardani 2021 seismology wu lin 2018 or hydrology jardani et al 2022 as explained in section 2 3 the operation of cnn network relies on a convolutional mechanism that interpret feature on a region by region basis rather than treating each observation point separately as is the case with a conventional inversion method such a mechanism minimizes the impact of local changes due to random noise since the learning process includes max pooling operations which also contribute to denoising the information by keeping only the maximum values during the process to summarize we collect all the result discussed previously in fig 14 for the histograms and in table 1 for the accuracy of reconstructed fractures 4 6 influence of features in training models the training datasets being the main and most essential data enable machines to learn the feature and make predictions from the learned features not only quality and quantity of training datasets but also its relevance therefore affect the prediction accuracy all the datasets together must be consistent and relevant to the response expected from the neural network gathering a broadly featuring data then better approximates the complexity of a real field but this often associates to overhead costs and challenges in practice often the training dataset is constrained to be as similar as possible to priori knowledge in the field which implies possible misinterprets if incomplete and inconsistent data appears this discussion investigates how well the trained model performs under different conditions when the presented map differs from the training knowledge six synthetic aquifers with different features are tested as shown in fig 15 on the first three aquifers d1 d2 and d3 the transmissivity fields contain binary distributions that differ from the gaussian models predefined in the training dataset the neural network reconstructions successfully capture the heterogeneities of the aquifer including the absence or presence of fractures in the target models however in the second case d2 fig 15b h an anomaly is evident due to the interpretation of a segment at the interface between two hydro facies a transmissivity gap at the hydro facies interface causes fracture like behavior in the groundwater resulting in an artifact in the prediction as mentioned in section 4 1 for gaussian models the ground heterogeneity can trigger misinterpretations depending on the gap of heterogeneity in the ground matrix for the remain cases d4 d5 and d6 we retain the gaussian nature of the transmissivity as used in the training data but the fracture network structures are more complicated in the case d4 the fracture network consists of some short segment which differs from the training data containing only long fractures the reconstruction of case d4 efficiently detects the major fracture but the neural network fails to detect minor fractures that were not learned during the training phase in general it is more complex to capture short fractures in hydraulic tomography with a poor resolution of the piezometric coverage fischer et al 2018 in the following cases d5 and d6 the trained network is challenged to interpret the models with 4 and 5 fractures respectively which include more features than the trained data with at most 3 fractures the reconstructed maps mostly locate the fracture traces from the interpreted segments when the complexity of the tested fracture system exceeds the coverage limit of the input data certain fracture geometries can be simplified in practice the nature of fracture networks and the degree of heterogeneity in the transmissivity field coved in the training data reconstruction must be pre specified based on prior knowledge of the study area as the rest of the classical deterministic or stochastic inversion methods inversion with deep learning tools requires priori conditions to constrain the inversion and reduce the non uniqueness issue in the solution in this study the focus is on the theoretical development of an advanced neural network to map the fracture system alternative approaches with comparable configuration can be found in the literature wang et al 2016 wang et al 2017 fischer et al 2018 ringel et al 2019 as some of them are addressed and relatively analyzed along the discussions however the implementation of a conventional inversion approach is beyond the scope of this work when processing a real dataset a more detailed comparison of the approaches should be considered 5 conclusion in this paper we present a novel and practical method for identifying fracture systems associated to spatial measurements of the water table in a fractured aquifer based on segnet the involving advanced neural network is designed to be efficient in terms of memory and processing time during inference with a substantially lower number of trainable parameters than competing designs the architecture as a deep fully convolutional neural network is topologically composed of encoder decoder structures that directly translate the inverted function in terms of trainable parameters rather than the indirect approach used in classical inversion techniques as with any deep learning approach the algorithm requires a large synthetic training database to establish a reliable generalization capable of predicting the data not seen in the learning stage the construction of this database was achieved by the geostatistical generation of fracture network models whose number varies between 0 and 3 and whose transmissivity is assumed constant and placed on a rock with heterogeneous matrix transmissivity also generated according to a geostatistical variogram based on these synthetic aquifers we performed pumping tests according to the hydraulic tomography model by solving the flow equation with the discrete fracture network parameterization the neural network was trained using 20 000 synthetic aquifers with their hydraulic responses while another 10 000 datasets test the relevance of the approach results show that the trained network succeed in accurately mapping the fracture network geometry and the performance of the network is then discussed in the context of a variety of potential interferences in practice first accuracy shows a clear relationship with the size of training dataset when the network is trained with a limited dataset the quality of identification of complex fracture networks degrades while data abundance guarantees a high quality response the number of monitoring also affects the quality of the reconstruction with accuracy improving if the complexity of groundwater dynamics in the fractured aquifer can be captured by the availability of planned monitoring wells this dependence of inversion results on the number of wells also occurs in conventional deterministic or stochastic inversion methods however the neural network shows less impact of data noise on the inversion accuracy which is due to the convolutional operation and max pooling that de noise input data through region wise interpretation the quality of the inversion results also depends on the nature of models used for training as soon as the network is confronted with the processing of hydraulic data from models whose properties are very different from those used in the training the quality of the inversion deteriorates the choice of training model features must be based on a priori information in order to obtain an accurate inversion like other conventional inversion methods inversion using a deep learning approach also requires the use of a priori information and the trained neural network only covers a specific set of models with specific predefined features a series of tests also reveals the importance of the matrix heterogeneity in reconstructing the fracture network structure if the transmissivity of the matrix is very low compared to that of the fractures the matrix contributes little to the dynamics of the flow field and cannot mask the effects of the main fractures on the other hand if the matrix contains secondary fractures that are represented together with the matrix in an equivalent porous medium with a slightly permeable transmissivity this can show the effect of masking the identification of the main fractures in this work our effort focuses on the geometric identification of the main fractures of an aquifer where only constant aperture of fractures is considered however the fracture characteristics can be more complex bringing more feature such as the hydraulic transmissivity variation along the fracture and between the individual fracture in the next work other fracture apertures and the matrix heterogeneity will be tackled which requires the development of a new multi task network in this work our effort focuses on the geometric identification of the major fractures of an aquifer where solely constant aperture fracture is considered however in practice fracture characteristics may be more complicated evolving additional features such as hydraulic transmissivity varying along the fracture and between individual elements of the fracture network to address this issue the fracture apertures and matrix heterogeneity are both tackled in the following study which requires the development of a new multi task network credit authorship contribution statement m t vu conceptualization methodology software data curation visualization investigation a jardani conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3408,in this paper we propose a new method to map the fracture network structure in a heterogeneous aquifer from inversion hydraulic head data measured during pumping tests in hydraulic tomography mode this inversion tool is based on the new concept of convolutional neural networks which provides a direct approximation to the inverse function linking fracture geometry to hydraulic data in order to handle the highly nonlinear inverse function more effectively an advanced neural network is developed from segnet architecture with encoder decoder structure which excels in image processing to translate the water level image associated with the pumping tests at the input into a fracture map at the output the network is trained with a synthetic dataset where the fracture structure and matrix heterogeneity are randomly generated and the hydraulic head are obtained by solving the groundwater flow equation the trained network accurately maps different complexity levels of fractures embedded in a matrix with heterogeneous transmissivity as a data driven approach the accuracy of the mapping depends on the quantity quality and relevance of the synthetic dataset used in the training phase while generating data to train the network requires effort the trained network performs each inversion instantly the inversion result appears to be stable even in the presence of data noise reliably interprets the hydraulic data if they share comparable fracture and matrix properties as specified in the training models keywords neural network fractured aquifer convolution neural network groundwater inversion 1 introduction characterizing the hydraulic properties of fractured karstic aquifers is known to be one of the most challenging tasks in the quest to understand groundwater dynamics and contaminant transport in these complex environments neuman 2005 indeed due to the large variability in fractures and rock matrix as well as the permeability gap between them these fractured aquifers exhibit extremely complicated groundwater flow patterns water flows in this heterogenous structure are mostly concentrated in a percentage of the highly permeable matrix and in sparse fracture networks that emerge across a vast impermeable region budd vacher 2004 maréchal et al 2004 identification of these fracture networks is therefore crucial in determining strategies for exploiting and protecting any reservoir various tools have been tested and developed in the literature to locate preferential flows associated with the presence of fractures including tracer tests conventional pumping and cross pumping techniques dverstorp et al 1992 dahan et al 1999 dausse et al 2019 cross pumping tests are referred to as a hydraulic tomography technique that captures the spatial variability of the hydraulic properties of the porous or fractured aquifer from joining hydraulic drawdown data recorded in successive pumping tests illman 2014 typically processing of these acquired hydraulic data is performed using an inversion code to determine models of hydraulic conductivity and storativity which are numerically consistent for the observed hydraulic data the inversion method employs a forward problem to numerically provide hydraulic responses for a given model of hydraulic properties by solving for groundwater flow based on the darcy equations in this method the forward operator is solved repeatedly until achieving a satisfactory match between the observed and simulated hydraulic data yeh 1986 for the characterization of fractured karstic aquifers the groundwater flow equation can be solved using one of two distinct approaches equivalent porous medium epm and discrete fracture networks dfn in the ecm concept fracture networks are approximated as a porous medium in which the fractured aquifer is fully discretized with small cells that represent the effective values of transmissivity and storativity that are being identified during the inverse process yeh liu 2000 zha et al 2015 tiedeman barrash 2020 this approach is straightforward to implement and parameterize the spatial heterogeneities of the hydraulic property fields in the inverse process it however ignores the discontinuity features of groundwater flow in fractured environments wang et al 2016 in contrast dfn concepts preserve the fracture shape and its hydraulic discontinuities via parameterizing the fracture and conduit structures as lines in 2d or surfaces in 3d and linking their hydraulic properties to their apertures fischer et al 2018 ringel et al 2019 mohammadi illman 2019 despite its realistic representation this parameterization is practically complicated due to the significant time required in numerical simulation and the difficulties in determining both the spatial geometries of the fractures and their apertures in the inverse process for this reason the number of applications of dfn parameterization in the inverse problem with stochastic or deterministic optimizers remains limited in comparison to epm in this paper we explore the potential of deep learning tools in the inversion of hydraulic data monitored in pumping tests to identify conduit and fracture networks compared to stochastic and deterministic algorithms the deep learning algorithm performs a direct approximation of the inverse function linking the piezometric measurements as input to the hydraulic properties in the output this link between the two is made through multiple linear and nonlinear operations performed sequentially on layers of the network such a decomposition provides an efficient way to approximate highly nonlinear functions as in the case of inversion operators elanayar shin 1994 the concept involves the use of several multidimensional coefficients called weights and biases assigned to the network layers the determination of these coefficients is based on an optimization process performed during the training operation which uses a known set of input and output data called the training dataset thus most of the effort and computational time required to build a neural network relates to building the database for training and the training task itself once trained however the trained network provides an end to end operator that performs inversions of experimental data instantly deep learning algorithms are progressively gaining ground in the geosciences particularly the emergence of the new powerful neural networks that enables to handle complex problems in geophysical imaging remote sensing and other environmental monitoring benjamin et al 2020 lary et al 2016 moseley krischer 2020 some preliminary investigations adopted deep learning algorithms using vanilla neural networks for example to interpret measurements from pumping or tracing tests aziz wong 1992 zio 1997 balkhair 2002 trichakis et al 2009 comparative analyzes reveals the efficiency of neural network as a replacement for traditional interpretations however fully connected layers in a vanilla neural network with a high number of learnable coefficients lead to training task overload when dealing with high dimensional problems recent breakthroughs in the field of data science have given rise to the new networks based on the convolutional concept that efficiently takle tomography problems with reasonable effort lecun bengio 1998 convolutional neural networks cnn translate the input image by scanning the shared small filters region by region to extract features on the entire image and construct local connections between neighboring pixels such a sharing mechanism facilitates the training task and information extraction compared to processing point by point as in the vanilla network rawat wang 2017 convolutional architecture performance has sparked a surge of research and applications in geophysical and hydraulic imagery for approximating inversion operators to identify the subsurface properties from sparse observations on the boreholes or at the surface among them liu et al 2020 built a cnn architecture to map the 2d electrical resistivity in the subsurface from apparent resistivity images inferred from surface data while vu jardani 2021 developed a cnn ert3d neural network to process 3d resistivity tomography other efforts applied the networks in seismic exploration to reconstruct the seismic velocity models directly from seismogram data li et al 2020 zhang lin 2020 apolinario et al 2019 park sacchi 2020 similar ideas were tested for the inversion of electromagnetic data to identify the 1d or 2d subsurface resistivity puzyrev and swidinsky 2021 puzyrev 2019 in hydrogeology laloy et al accessed the effectiveness of a generative adversarial network gan in mapping hydraulic conductivity heterogeneities in 2d and 3d binary pattern laloy et al 2017 laloy et al 2018 bao et al 2020 extended the coupling of gan and ensemble smoother with multiple data assimilation to reconstruct the binary media and reduce the prediction uncertainty sun 2018 suggested a composition of gan and state parameter identification to map simultaneously the parameter space as the permeability and the corresponding model state as the subsurface structure other initiatives employed encoder decoder architectures to image the spatial distribution of the transmissivity field from measurements of hydraulic heads jardani et al 2022 or tracer concentrations vu jardani 2022 however all these applications focus solely on mapping heterogeneity in a porous aquifer to our knowledge only a few works contribute to determination of the fracture network geometry in this paper we propose a cnn based on the segnet architecture to identify the fracture network structure from synthetic data of hydraulic head monitoring in a fractured aquifer the study is structured as follows section 2 outlines the theoretical background which summarizes the forward problem with groundwater equations the concept of the inverse problem approximated by segnet and its differences from classical inversion methods details of the data generation learning process and validation of the trained network follows in section 3 section 4 is devoted to accessing the performance of the proposed network in the face of possible interferences in practice which could be related to the data quality and quantity the heterogeneity in matrix or the relevance of dataset features finally a summary of the main findings with suggestions for future work concludes the paper 2 theoretical background in this section we summarize the theory of which the first part includes the forward problem the concept of the inverse problem with neural network followed by the description of proposed neural network architecture and its working mechanisms 2 1 forward problem groundwater modelling as mentioned earlier inversion by a deep learning algorithm is also based on the forward problem where generating the training data set involves solving the groundwater flow equation this training data consists of the numerically computed hydraulic response to the pumping tests corresponding to a given synthetic transmissivity field and discrete fracture networks fractures in the aquifers are geometrically represented by 1d lines and assigned to an equivalent transmissivity whereas matrix rock is characterized as a porous medium a heterogeneous transmissivity field the neural network then learns the relationship between the hydraulic head responses and the permeable fracture structures rooted in the training dataset in this work the inversion task aims only to reconstruct the geometric structure of fractures without rebuilding the transmissivities in fracture and matrix in steady state conditions numerical simulation of groundwater flow in a fractured aquifer is performed using two connected equations derived from darcy s law that describe water flows in the rock matrix and fractures kohl et al 1997 1a t m h q in the matrix γ m t t f t h q in the fracture γ f subjected to the following boundary conditions 1b h h d a t γ d where and t are the gradient and tangent gradient operators respectively tm and tf m2 s denote the transmissivity of heterogenous matrix γm and fractures γf respectively the fractures are shaped as lines which is randomly generated while their equivalent transmissivity is assumed constant h m is the hydraulic head q l s m represents the punctual water source at the extraction injection wells to form boundary conditions the observation area is embedded in a large buffer zone assigned to the mean transmissivity of the matrix blocks such large bounds limit the boundary effects on the computational results for completeness a constant hydraulic head hd is imposed at the buffer zone periphery γd in this study the source of water is the paired pumping wells each pair consists of one extraction and one injection well located on opposite sides of the observation area each paired well pump at the same constant rate over the operating period finally we solve the set of flow equations numerically using a finite element method in comsol software to determine the hydraulic head at observed points which are then taken as input to the neural network 2 2 inversion problem with neural network mapping fracture from groundwater observation as mentioned in the previous problem equation 1 represents a numerical operator for computing piezometric data in response to a pumping test in an aquifer with matrix transmissivity t m fracture transmissivity t f with geometry c f the operator can be expressed as the following function with three dependent parameters 2 h f t m t f c f the operator works as a data generator for training the neural network which performs an inverse function to determine the fracture geometry from the hydraulic head data see fig 1 here we focus only on reconstruction of the fracture geometry without identifying the transmissivity field of matrix rock and fractures which will be handled in a future study thus the aim of this study is to retrieve the fracture geometry from the hydraulic data by approximating the nonlinear inverse operator f 1 between two fields using segnet network based on thousands of synthetic data sets 3 c f f 1 h ψ h θ where ψ represents the neural network whose learnable parameters are denoted as θ indeed the neural network such as segnet offers a possibility to directly relate the hydraulic data to the fracture geometries by determining a set of learnable parameters defined as sequential linear and nonlinear operations in segnet layers determination of the parameters is performed from solving an optimization problem with the following objective function 4 θ argmin 1 n c f ψ h i θ where the pair c f h i denotes a model and its corresponding measurement n is the number of pairs in the training dataset the operator denotes the cross entropy to measure the difference between the truth and predicted models a training process often demands a large dataset of which the generation task takes the most time in the building of a neural network however once the training is completed the trained network performs an inference instantly an end to end operator without intervention from users 2 3 segnet architecture direct inversion function in this paper the inversion operator is approximated by the segnet architecture which is designed to efficiently locate the fracture map from groundwater level measurements relied on the concept of convolutional neural network see fig 2 the segnet architecture was developed by badrinarayanan et al 2017 and has been rapidly proven in a variety of domains with numerous applications including traffic scene detection kendall et al 2017 jiang et al 2020 qin et al 2020 satellite image processing khryashchev et al 2018 mohammed edward 2019 sariturk et al 2020 geophysics pham et al 2018 mukhopadhyay mallick 2019 vu jardani 2021 however it is still uncommon in hydrology as well as other advanced algorithms in this discipline where progress is modest compared to other geosciences such as geophysics typically a neural network consists of multiple layers of artificial neurons that mimic the operations of their biological counterpart through mathematical functions indiveri et al 2011 when an input image is introduced into the network each layer operates on activation functions to interpret features before passing them on to the next layer the process ends with a mapping result reconstructed from the learned features by matching the truth in the given datasets with its reconstruction from the network all parameters in the activation functions are determined which is then named as the training task network performance evidently relies on how efficiently the features are translated after this training process a more complex network appears to better refine the learned features that is why the proposed neural network is based on an advanced architecture bengio et al 2013 the proposed architecture as shown in fig 2 consists of successive network blocks with four encoders and four decoders finalized by a pixel wise classification layer to map the fractures this u shaped form consists of 63 layers divided into two paths a contracting one with encoders and an expanding one with decoders each having the same structure but arranged in reverse order the encoders extract the features embedded in the input images using a set of sequential convolutions each followed by a batch normalization bn and a rectified linear unit relu and finally a max pooling operation on the opposite path the decoders start with an un max pooling operation followed by a sequence of convolutions bn and relu layers the max pooling indices from each encoder are assigned to the corresponding decoder to compensate for the resolution loss in the encoders this sharing mechanism is the core aspect of a segnet architecture badrinarayanan et al 2017 but how does the network reconstruct the fractures the concept of a convolutional neural network relies on scanning small filters over entire images to extract key features which represent the local connectivity among neighboring pixels in this sequential convolution the first layer usually extracts basic features such as horizontal or diagonal edges while more complex features are detected in the next layers such as corners edges or shapes ghafoorian et al 2017 the deeper it goes the more complex features the network can detect fukushima 1988 in this manner convolutions translate the fracture information embedded in the input water level images to yield the fracture map number of convolution level is designed in response to problem complexity however deeper convolutional translation inevitably entails a large system of filters that soon overburdens the training task to save time and resources a max pooling operation is invoked to sort out only maximum values by shrinking the spatial size of the convolved features the operation speeds up the processing and is effectively deals with overfitting issue by working only on maxima the max pooling conserves the most prominent invariant features along with dimensionality reduction nagi et al 2011 since the fracture is prominent in this case the operation acts as a denoising agent to highlight fractures from the matrix heterogeneity however the operation results in a downsized output and lose of information to compensate the side effects state of each max pooling is transferred to the corresponding un max pooling in decoders to counterbalance the degradation thanks to sharing max pooling indices the segnet outperforms other networks such as fcn deeplab deconvnet in terms of accuracy badrinarayanan et al 2017 for example the proposed network with 4 encoder decoder levels requires only 0 5 million parameters which is far less than unet with 8 million a smaller size is also related to the fact that the network employs only 3 3 64 filters which are partly smaller than those in the unet recall that our network is designed with 4 encoder decoder levels shallower than the original network as it processes fewer output segmentation labels 3 application this section is devoted to the training process in a synthetic application we begin with a generation of aquifer models each with a fracture network embedded in a heterogenous ground water level responses in each aquifer model are observed with a monitoring scheme designed to mimic a real experiment where the acquisition setup maps the groundwater level in injection tests following that the acquired data is processed to feed training of the network the training is then implemented along with hyperparameters evaluation metrics and its evolution during the process typical results are shown to validate the trained network 3 1 aquifer model fracture network embedded in a heterogeneous ground in this study the heterogeneity in aquifer transmissivity is generated based on statistical parameters following a gaussian distribution for this purpose we use the sgems code implemented in matlab to create 35 000 transmissivity models where the distribution of log10t is randomly constructed using a gaussian variogram with constant mean and variable range remy et al 2009 heterogeneity in transmissivity models ranges from 10 8 to 10 4 m2 s in four orders of magnitude which corresponds to the range of a permeable aquifer the generated transmissivity fields are then assigned to a 100 100 m area with a locally constant aquifer depth fig 3 each ground model is assigned a fracture network geometry which consists of no to three fractures assigned randomly the transmissivity of fracture is set at 10 2 m2 s which is of orders conductive than the surrounding matrix with a mean of 10 6 m2 s all fractures are formed from 1d random generation using a gaussian variogram in sgems and then randomly rotated to redirect their orientation to establish a monitoring scheme we install 49 observation wells in a regular array in both directions on the synthetic aquifers fig 3 at the field boundaries we arrange 4 paired pump wells in each pair an up pumping well is in opposite position to its down pumping this arrangement of the hydraulic forces in pairs allows realizing a hydraulic disturbance of the whole investigation area in addition this configuration in practice saves water resources during the hydraulic tomography test 3 2 data acquisition and data processing in each model groundwater responses due to pumping injection tests are determined by computing the forward problem described in section 3 1 four pumping injection tests are performed sequentially at a constant flow rate of 10 l min in each well hydraulic measurements are acquired on the 49 observation wells and then interpolated using the akima method to obtain 4 maps of hydraulic head the input and output maps in the training datasets use the same spatial resolution being discretized by a 64 64 grid on the 100 100 m study area recall that in this study we focus only on mapping the network of fractures as preferential flow paths during pumping injection tests the presence of fractures is often associated with abrupt changes in groundwater flow to better identify these discontinuities we calculate the gradient in both directions x and y of the hydraulic head maps interpolated from the hydraulic head measurements the resulting gradients are then reshaped into a 64 64 grid each pumping injection test generates 2 gradient maps x y to build the gradient map set of 8 channels 64 64 8 as the input of neural network training for the output map the fractures are labelled as 1 and the others as 0 on the gridded map to quantitatively assess the predictions accuracy of the network we use the accuracy coefficient as defined below 5 accuracy 1 n j 1 n y j y j where y and y denote the labels fracture or ground in the true model and its corresponding prediction respectively n represents the total pixel number in the output image 64 64 3 3 training process and results for the training phase we randomly separate 25 000 aquifer models with corresponding measured data 20 000 for training the network and 5 000 for validation during the process a high fraction of validation dataset aims to better evaluate efficiency of the training process training is performed using the adam optimization algorithm implemented in matlab on a dell precision tower 5810 with a single gpu nvidia quadro k2200 the task is completed in 5 h 60 epochs with a constant learning rate of 0 01 and a batch size of 128 fig 4 summarizes evolution of accuracy along the training process the trained network is then evaluated with 10 000 unseen models each model is executed in 0 006 s almost instantly fractures are efficiently mapped over 10 000 samples with an average accuracy of 94 83 the overall reconstruction quality is summarized in the histogram shown in fig 14 high accuracy prediction is related to the fact that fractures with higher conductivity drive dynamics in the aquifer that favor their detection to investigate the sensitivity of the proposed model we train the network 50 times with the same dataset and hyperparameters as above the results show a consistent solution where the average accuracy of the testing dataset centers at 94 86 with a standard deviation of 0 1 over the trained models to discuss the accuracy of the reconstructions in detail we select and show six representative results of the tested models in fig 5 six showcases are arranged in different levels of complexity of the fracture distribution starting with the models without fractures and ending with the networks consisting of three fractures the accuracy of reconstructions overall refers to a clear dependency on the density of the mapped fractures and their complexity level without fractures ex1 shows an accurate prediction where the matrix transmissivity heterogeneities do not produce artifacts in the interpretation a further discussion on this ground impact is addressed in section 4 5 with higher heterogeneity in the following cases ex2 and ex3 the trained network also properly reproduces the models with one or two fractures however in a more complex case in ex4 where two fractures cross closely there is a minor misinterpretation with a fused joint even though the reconstruction still faithfully represents the main aspects of the fractures case ex5 otherwise presents a dense fracture network in which three fractures are oriented in different directions these patterns are somewhat more complex than the previous ones but the network is still able to identify the fracture paths with accuracy along the reconstructed fractures in fig 5hij other slightly deformed curves or a minor discontinuity can be observed possibly due to the influence of ground heterogeneity the ground heterogeneity exhibits less influence on subsurface flow compared to fractures however by introducing more noise and locally changing the water table heterogeneity may misguide the understanding of fracture networks a detailed discussion on this problem is further elaborated in section 4 1 for results from homogeneous models in the last case ex6 also consists of three fractures but with a very complex geometry including two fractures close to each other which the network cannot distinguish in the reconstruction this misinterpretation stems from the fact that the two fractures behave as similar as a single fracture in the middle in general however the fracture reconstructions are very satisfactory indeed number of interferences can impact prediction efficiency which may occur in a real field under diverse situations some of the key issues are addressed in the following sections 4 discussion this section is devoted to studying the impact of various sources of uncertainty that can affect the quality of the reconstructions with the network such as the uncertainty about the degree of heterogeneity of the ground transmissivity and the choice of the size and the nature of features of the training models we also study the impact of the number of piezometers used in hydraulic tomography and the noise that can alter these hydraulic measurements 4 1 effect of heterogeneity in the matrix both porous ground and fracture network geometry associates in forming the subsurface dynamics yet the roles of each system in driving the groundwater are distinct dominant conductivity and connectivity of fracture network make it superior to the surrounding porous ground while fractures regulate the flow regime the permeable ground even of a lesser order is equally important to investigate the influence of heterogeneity in matrix transmissivity we repeat the learning process described in section 3 but this time with homogeneous transmissivity models we regenerate 35 000 models using the same fracture network configuration embedded in ground with a constant transmissivity of 10 6 m2 s the network is re trained using the same piezometric configuration and network hyperparameters as in section 3 3 the overall average accuracy for 10 000 test models shows a significant improvement from 94 83 in case 1 to 96 94 in this case details of the representative models are shown in fig 6 and the histogram is summarized in fig 11 the showcases in fig 6 prove a better reconstruction due to the absence of disturbance effect associated with a heterogenous ground particularly for the complex configurations in ex4 and ex6 the fused bottleneck in the prediction of ex4 has clearly disappeared accompanied by a better shape for both fractures other slightly deformed curves that appeared in fig 5hij also do not appear in fig 6hij similarly all the fractures are well shaped overall in ex6 where the two closed fractures are clearly delineated even some minor disturbances are shown on the top of the fractures comparison of the accuracy between the two sets of reconstructions confirms the heterogeneity importance of the porous ground in determining the fracture network structure further discussion on this issue is developed in section 4 5 where the heterogeneity deviates from a gaussian distribution predefined in the training dataset 4 2 effect of dataset size in general the effectiveness of a trained neural network in a deep learning algorithm depends strongly on the size of the dataset used in the learning phase in our inverse problem generating the training dataset involves solving a mass of a forward problem with numerical tools which is the longest and most laborious phase in building a neural network the ideal strategy is to determine the optimal size which reduces computation effort while resulting in an accurate prediction this can be accomplished by gradually increasing the data size and periodically verifying the prediction accuracy in the training validation and testing phases until a satisfactory result is achieved in this section we examine the impacts of training data size on driving the reliability of reconstructions by analyzing the predictions obtained with networks trained with three different sizes 5000 10000 20 000 and 30000 the networks formed with these datasets are tested on the 10 000 unseen models as in section 3 3 of which the metric results are reported in table 1 and in fig 14 for the histogram we recall the six representative models as in the previous tests to analyze the impact of dataset size in detail see fig 7 the use of a minimal number of training data is sufficient to results in reliable maps when the reconstructed fracture networks are straightforward as in the case of models ex1 to ex3 however when the configurations are difficult as in the cases of models ex4 to ex6 the predictions do not provide accurate identifications of the fracture structures with minor artifacts and missing features in the reconstruction a larger training dataset assists improving reconstructions in enriching the learning process through introducing more complex features advani et al 2020 better generalization thus requires a wide range of alternative models to cover the high complexity in karstic aquifer and to tackle the overfitting issue details of prediction accuracy in table 1 reveal a clear dependence on the size of dataset with overall mean accuracy increasing from 93 08 for the first subset 5 000 models in training to 93 95 for the second 10 000 models in training and to 95 19 of the third 30 000 models in training the enhancement in accuracy is however not proportional to the enlargement of dataset size since results from subsets of 20 000 case 1 and 30 000 models show no clear difference with 0 3 of improvement in accuracy for extra 10 000 samples overfeeding data overburdens the training task since increasing data volume ineffectively contribute to improve accuracy when performance reaches an asymptotic stage amari et al 1997 as mentioned at the beginning of this section the choice of training data size depends on the desired accuracy it is reasonable to gradually increase the size until this desired accuracy is achieved to avoid generating a large amount of unnecessary data 4 3 effect of amount of observation wells number of observed wells is an important consideration in design a monitoring plan as it determines measurement efforts and accuracy of any interpretation technique a dense observation network engages a better understanding of the field when it better covers the dynamics subsurface however it also imposes higher requirement on the costs and feasibility of a monitoring plan in a real field sometimes in this part we examine the importance of measurement points for reconstruction accuracy to achieve this the study evaluates two additional configurations of observation wells which share an identical pump setting but using less wells compared to the original configuration containing 49 piezometers the number of wells is first reduced by half 25 wells and then by one third 16 wells but we retain the pumping schemes with similar flow rates paired pumping and other conditions as performed for the original configuration of 49 wells in section 3 1 fig 8 in practice we rebuild new datasets in which the hydraulic head maps are interpolated with fewer measurements which lowers the input resolution and worsens the inversion quality on the 10 000 test models table 1 the results of the 10 000 test models in table 1 reveal a clear dependence of the reconstruction accuracy on the piezometric density over three configurations degraded data coverage results in a drop in overall reconstruction accuracy from 94 83 49 wells to nearly 92 with 25 and 16 wells the degradation tendency is also confirmed visually in the representative models where the reconstructions of the fracture network geometry from 25 well setup are less accurate than in the 49 well fig 9 for simple fracture networks in ex1 to ex3 the reconstruction can be accomplished accurately with only a limited number of observation wells however the dissimilarity in the results for 25 and 16 well setups implies that the hydraulic information in these two cases is only sufficient to infer the existence and overall shape of the fractures but not to identify them more precisely in complex cases another comparison of results between two configurations of observation wells in fig 9 highlights the impact of well spacing a scattered distribution of observation wells leads to a drop in accuracy of the fracture network reconstruction as case 4b shown in fig 9 where the reconstruction quality depends locally on the scarcity of observation wells indeed the relationship between the number of involved measurements and the reliability of resolved predictions is a common issue in any inversion problem regardless of the optimization method employed to ensure an accurate reconstruction there must be a sufficient number of wells to cover all heterogeneities in the target field 4 4 effect of pumping schemes to further investigate the impact of the amount of data in identifying fractures in this section we reduce the number of pumping wells and keep the original configuration with the same pumping rate and observation wells see fig 10 this modification in the monitoring scheme results in a reduction of the amount of data to half and quarter of the original scheme the reduction in data size leads to an overall deterioration in the result for the subsurface fracture network reconstruction with the average accuracy dropping to 93 53 in case 5a with 2 pairs of pumping wells and 91 50 in case 5b with a single pair details for the six representative examples are shown in fig 11 while the statistics are summarized in table 1 downsizing the observation data in this case undoubtedly leads to less coverage of observed data resulting in a less reliable prediction however the consequence is less severe than the reduction in observation wells where the lack of local information results in a coarse resolution that affects details of the reconstructed map this comparison can be used as a guide to the design of the monitoring system in a real field in this section we analyze the quality of reconstructions with piezometric data obtained with a low pumping rate of 0 5 l min while maintaining all other acquisition parameters such as the number of pumping wells and observation wells the low flow rate hence entails a small hydraulic disturbance confined to the vicinity of the pumping wells the absence or low response that can be affected by noise in distant wells cause a lack of information to perform a correct interpretation such difficulty can be the origin of inaccurate reconstructions with artifacts where the interpretation cannot distinguish the presence of a fracture network from the impacts of the heterogeneity of the surrounding matrix see fig 12 4 5 effect of observation uncertainty removing the noise that affects hydraulic data can be a complex task to perform prior to interpreting input data in an inversion algorithm thus identifying the influence of noise on imagery quality is a crucial step in determining the degree of uncertainty in the interpretation to analyze this influence we contaminate the hydraulic data with random gaussian noise where three standard deviations represent 5 15 and 25 of the original signal respectively contaminated data is then interpreted using the network formed with the original data section 3 3 to illustrate the result for 10 000 testing samples we group the histogram in fig 14 and summarizes the metric detail in table 1 while the predictions for the benchmark cases are shown in fig 13 according to the metric assessment average of all reconstruction accuracies the increase in noise correspond to a slight decrease in reconstruction accuracy from 94 59 percent for 5 noise to 93 38 percent for 15 and 91 90 percent for 25 noise noise appears to have only a minimal effect on the reconstruction accuracy of the fractures fig 10 details reconstructions of typical models where the fractures are well reconstructed despite some minor artifacts local noise that mimics the dynamic behavior of fractures in groundwater likely leads to the misinterpretation that produces these artifacts such a disturbance is then attributed to false segments as in ex1 or to slight misalignment of fractures as in the other cases ex4 to ex6 however the fracture networks are well identified overall regardless certain degree of uncertainty in the hydraulic data this minimal noise effect has been reported in the literature when applying cnn architectures to solve inversion problems e g in electrical resistivity tomography vu jardani 2021 seismology wu lin 2018 or hydrology jardani et al 2022 as explained in section 2 3 the operation of cnn network relies on a convolutional mechanism that interpret feature on a region by region basis rather than treating each observation point separately as is the case with a conventional inversion method such a mechanism minimizes the impact of local changes due to random noise since the learning process includes max pooling operations which also contribute to denoising the information by keeping only the maximum values during the process to summarize we collect all the result discussed previously in fig 14 for the histograms and in table 1 for the accuracy of reconstructed fractures 4 6 influence of features in training models the training datasets being the main and most essential data enable machines to learn the feature and make predictions from the learned features not only quality and quantity of training datasets but also its relevance therefore affect the prediction accuracy all the datasets together must be consistent and relevant to the response expected from the neural network gathering a broadly featuring data then better approximates the complexity of a real field but this often associates to overhead costs and challenges in practice often the training dataset is constrained to be as similar as possible to priori knowledge in the field which implies possible misinterprets if incomplete and inconsistent data appears this discussion investigates how well the trained model performs under different conditions when the presented map differs from the training knowledge six synthetic aquifers with different features are tested as shown in fig 15 on the first three aquifers d1 d2 and d3 the transmissivity fields contain binary distributions that differ from the gaussian models predefined in the training dataset the neural network reconstructions successfully capture the heterogeneities of the aquifer including the absence or presence of fractures in the target models however in the second case d2 fig 15b h an anomaly is evident due to the interpretation of a segment at the interface between two hydro facies a transmissivity gap at the hydro facies interface causes fracture like behavior in the groundwater resulting in an artifact in the prediction as mentioned in section 4 1 for gaussian models the ground heterogeneity can trigger misinterpretations depending on the gap of heterogeneity in the ground matrix for the remain cases d4 d5 and d6 we retain the gaussian nature of the transmissivity as used in the training data but the fracture network structures are more complicated in the case d4 the fracture network consists of some short segment which differs from the training data containing only long fractures the reconstruction of case d4 efficiently detects the major fracture but the neural network fails to detect minor fractures that were not learned during the training phase in general it is more complex to capture short fractures in hydraulic tomography with a poor resolution of the piezometric coverage fischer et al 2018 in the following cases d5 and d6 the trained network is challenged to interpret the models with 4 and 5 fractures respectively which include more features than the trained data with at most 3 fractures the reconstructed maps mostly locate the fracture traces from the interpreted segments when the complexity of the tested fracture system exceeds the coverage limit of the input data certain fracture geometries can be simplified in practice the nature of fracture networks and the degree of heterogeneity in the transmissivity field coved in the training data reconstruction must be pre specified based on prior knowledge of the study area as the rest of the classical deterministic or stochastic inversion methods inversion with deep learning tools requires priori conditions to constrain the inversion and reduce the non uniqueness issue in the solution in this study the focus is on the theoretical development of an advanced neural network to map the fracture system alternative approaches with comparable configuration can be found in the literature wang et al 2016 wang et al 2017 fischer et al 2018 ringel et al 2019 as some of them are addressed and relatively analyzed along the discussions however the implementation of a conventional inversion approach is beyond the scope of this work when processing a real dataset a more detailed comparison of the approaches should be considered 5 conclusion in this paper we present a novel and practical method for identifying fracture systems associated to spatial measurements of the water table in a fractured aquifer based on segnet the involving advanced neural network is designed to be efficient in terms of memory and processing time during inference with a substantially lower number of trainable parameters than competing designs the architecture as a deep fully convolutional neural network is topologically composed of encoder decoder structures that directly translate the inverted function in terms of trainable parameters rather than the indirect approach used in classical inversion techniques as with any deep learning approach the algorithm requires a large synthetic training database to establish a reliable generalization capable of predicting the data not seen in the learning stage the construction of this database was achieved by the geostatistical generation of fracture network models whose number varies between 0 and 3 and whose transmissivity is assumed constant and placed on a rock with heterogeneous matrix transmissivity also generated according to a geostatistical variogram based on these synthetic aquifers we performed pumping tests according to the hydraulic tomography model by solving the flow equation with the discrete fracture network parameterization the neural network was trained using 20 000 synthetic aquifers with their hydraulic responses while another 10 000 datasets test the relevance of the approach results show that the trained network succeed in accurately mapping the fracture network geometry and the performance of the network is then discussed in the context of a variety of potential interferences in practice first accuracy shows a clear relationship with the size of training dataset when the network is trained with a limited dataset the quality of identification of complex fracture networks degrades while data abundance guarantees a high quality response the number of monitoring also affects the quality of the reconstruction with accuracy improving if the complexity of groundwater dynamics in the fractured aquifer can be captured by the availability of planned monitoring wells this dependence of inversion results on the number of wells also occurs in conventional deterministic or stochastic inversion methods however the neural network shows less impact of data noise on the inversion accuracy which is due to the convolutional operation and max pooling that de noise input data through region wise interpretation the quality of the inversion results also depends on the nature of models used for training as soon as the network is confronted with the processing of hydraulic data from models whose properties are very different from those used in the training the quality of the inversion deteriorates the choice of training model features must be based on a priori information in order to obtain an accurate inversion like other conventional inversion methods inversion using a deep learning approach also requires the use of a priori information and the trained neural network only covers a specific set of models with specific predefined features a series of tests also reveals the importance of the matrix heterogeneity in reconstructing the fracture network structure if the transmissivity of the matrix is very low compared to that of the fractures the matrix contributes little to the dynamics of the flow field and cannot mask the effects of the main fractures on the other hand if the matrix contains secondary fractures that are represented together with the matrix in an equivalent porous medium with a slightly permeable transmissivity this can show the effect of masking the identification of the main fractures in this work our effort focuses on the geometric identification of the main fractures of an aquifer where only constant aperture of fractures is considered however the fracture characteristics can be more complex bringing more feature such as the hydraulic transmissivity variation along the fracture and between the individual fracture in the next work other fracture apertures and the matrix heterogeneity will be tackled which requires the development of a new multi task network in this work our effort focuses on the geometric identification of the major fractures of an aquifer where solely constant aperture fracture is considered however in practice fracture characteristics may be more complicated evolving additional features such as hydraulic transmissivity varying along the fracture and between individual elements of the fracture network to address this issue the fracture apertures and matrix heterogeneity are both tackled in the following study which requires the development of a new multi task network credit authorship contribution statement m t vu conceptualization methodology software data curation visualization investigation a jardani conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3409,recently deep learning models have been widely used in water conservancy engineering forecasting problems due to their excellent ability to deal with the complex interactions between various hydrological factors however the existing research mainly focus on model structure adjustment and input feature selection ignoring the influence of model ensemble on prediction in this paper a novel multi model ensemble method namely deep learning multi dimensional ensemble method is proposed to deal with multi step runoff prediction problem the method which can significantly improve the runoff prediction performance couples two ensemble techniques with different functional dimensions namely snapshot ensemble and attention ensemble snapshot ensemble technique is used to enhance model generalization capabilities from single model dimension while attention ensemble technique is employed to increase model prediction accuracy from multi model dimension furthermore a novel data driven model called deep learning multi dimensional ensemble model is proposed by combining three different deep learning neural networks with deep learning multi dimensional ensemble method the proposed model is applied in a real world case study in the upstream of yangtze river basin three evaluation indicators and ten comparative models are used to test the model performance the test results not only show the superiority of proposed model over other comparison models but also prove the effectiveness of the deep learning multi dimensional ensemble method the study highlights the power of the ensemble of deep learning model and the promising prospect of our deep learning multi dimensional ensemble method in hydrological predictions keywords ensemble model deep learning runoff forecasting attentional mechanism snapshot ensemble 1 introduction runoff forecasting has always been an important and hot research topic in hydrology field high precision runoff forecasting is essential for water supply flood control and power generation affected by natural meteorological conditions watershed characteristics and human activities the runoff sequence presents characteristics such as nonlinearity randomness and periodicity wang et al 2009 searching for more accurate runoff forecasting methods has always been a key issue for related scholars yuan et al 2018 the existing runoff forecasting models are generally divided into two categories process driven models and data driven models kratzert et al 2018 young et al 2015 process driven models are usually constructed based on the understanding of the runoff formation processes duan et al 1992 data such as topography geology and river channels are important inputs for process driven models krzysztofowicz 2002 the models simulate and predict runoff processes by building complex mathematical models and solving high dimensional partial differential equations such as mikeshe abbott et al 1986 arno todini et al 1996 and topmodel beven et al 1984 the advantage of process driven models is that the model parameters have strict physical interpretation but the high computational cost and complex modeling all limits their application in runoff forecasting yoon et al 2011 different from process driven models data driven models just focus on the input output relationship without clear causality between parameters in a specific prediction model with the continuous development of data driven models it can be further divided into statistical models machine learning models and deep learning models statistical models use statistically relevant methods to predict future runoff from historical runoff observation data such as autoregressive ar salas et al 1985 autoregressive moving average arma salas and obeysekera 1982 and autoregressive integrated moving average model arima montanari et al 1997 in addition pulukuri et al 2018b proposed a real time autoregressive updating model to forecast the flow in a watershed mehdizadeh et al 2019b proposed a hybrid models by hybridizing ai based approaches and fractionally auto regressive integrated moving average farima to model monthly streamflow time series for the ocmulgee and umpqua river stations valipour et al 2015b investigated the ability of the seasonal autoregressive integrated moving average sarima and autoregressive integrated moving average arima models for long term runoff forecasting in the united states the merit of statistical models is that they can accurately capture the linear relationship between historical data and future data however these models cannot accurately describe the complex nonlinear correlations in the runoff series which leads to a poor model performance the emergence of machine learning models such as artificial neural network ann kohonen 1988 and support vector regression svr liong and sivapragasam 2002 yu et al 2018 solves the problem of fitting nonlinear sequences mosavi et al 2018 and inspired the attention and research of many scholars wu et al 2009 established k nearest neighbors knn ann and svr to forecast monthly streamflow in different forecast horizon and found the svr model is superior to other machine learning methods in most cases liu et al 2014 first applied adaboost adaptive enhancement integrated technology to predict watershed runoff and the results showed that the improved adaboost model effectively improved the prediction accuracy yang et al 2017 used tele correlation factors such as atmospheric circulation index as model input and established three machine learning prediction models random forest ann and svr to predict monthly runoff the results show that all three machine learning models can obtain higher prediction accuracy liu et al 2018 combined a hidden markov model hmm and gaussian mixture regression gmr for probabilistic monthly streamflow forecasting and the results show that their model obtained a high prediction score and a reliable uncertainty distribution machine learning models have made great progress in extracting complex nonlinear features from multiple inputs which has been widely used in actual production however these models belong to the shallow learning category xie et al 2019 as the input feature dimension increases they have limited ability to extract deeper information which restricts the prediction performance of models to a certain extent with the rapid development of artificial intelligence and computer technology the above problems have been addressed by deep learning models deep learning network was widely used in the computer field at first such as long short term memory network lstm hochreiter and schmidhuber 1997 yin et al 2021 in the speech recognition and convolutional neural network cnn ji et al 2013 jin et al 2017 in the image processing in recent years the deep learning method has been gradually applied to the field of hydrological forecast bai et al 2016 propose a multiscale deep feature learning mdfl method with hybrid models to deal with the daily reservoir inflow forecasting and the results show that combining the deep learning framework with multi scale and hybrid observations is conducive to exploring the complex nature of reservoir inflow prediction in addition chen et al 2020 develop the self attentive long short term memory sa lstm by incorporating self attention mechanism into long short term memory and the experimental results demonstrate that their model achieves the best performance compared with baseline models to study the impact from the selection of multiple input variables on the model prediction zhang et al 2021a established lstm model and gated recurrent unit gru model with different input variables to predict runoff the results demonstrate that model with multiple meteorological input data can achieve higher accuracy than rainfall data alone lin et al 2021 propose a hybrid model namely the diff ffnn lstm model to predict hourly streamflow in the andun basin of china the result shows that the combining of different data driven model can improve prediction performance to a certain extent however the existing research on deep learning predictive models mainly focus on model structure adjustment and input feature selection ignoring the influence of model ensemble on prediction this paper focuses on the application of deep learning ensemble techniques in the hydrological forecasting field and proposes a novel deep learning ensemble method called the deep learning multi dimensional ensemble method the major contributions of the study are outlined as follows first we present a novel multi model ensemble method called deep learning multi dimensional ensemble method to improve the performance of deep learning models in runoff forecasting by introducing the snapshot ensemble technique huang et al 2017 which is widely used in the image identification field and combining it with the attention ensemble technique proposed in this article the method significantly enhances the generalization ability of forecasting model second we propose a new deep learning predictive model named dlmde to deal with the multi step runoff prediction problem by combining three different deep learning neural networks with deep learning multi dimensional ensemble method the proposed model has the ability to provide high accuracy multi step runoff forecast results third we test our model by comparing it with ten benchmark models on three evaluation indicators and the results show that our model has promising performance in general the rest of the paper is organized as follows section 2 introduces the deep learning multi dimensional ensemble method and then describes the implementation of dlmde section 3 introduces the study area and data pre processing method used in this article section 4 give the results and discussions section 5 summarizes the article and gives further research direction 2 methodology 2 1 deep learning neural networks 2 1 1 lstm lstm network is a special rnn network variant mainly to solve the problem of gradient disappearance and gradient explosion in the training process of long sequences in lstm multiple repeated memory blocks are connected through a special chain structure in each memory block three special gate structures are designed to protect and control the cell state the basic unit of lstm is shown in fig 1 a and the key equations are given by eqs 1 1 f t σ w xf x t w hf h t 1 b f i t σ w xi x t w hi h t 1 b i c t σ w xc x t w hc h t 1 b c o t σ w xo x t w ho h t 1 b o c t f t c t 1 i t c t h t o t tanh c t where f t represents the forget gate in the current period i t represents the input gate in the current period o t represents the output gate in the current period c t indicates the abstract cell state in the current period c t and c t 1 indicates the cell state in the current period and previous period h t and h t 1 indicates the output of hidden layer in the current period and previous period x t indicates the input of input layer in the current period w x indicates the weight matrices b indicates the bias vectors tanh denotes the hyperbolic tangent function σ denotes the sigmoid function there are three main stages inside lstm network the first stage which is dominated by the gate structure called the forget door is mainly to selectively forget the input from the previous node simply put it will forget the unimportant and remember the important specifically the calculated f t is used to control which c t 1 in the previous state needs to be forgotten the second stage is to selectively memorize the input which is controlled by the input gate the main purpose is to selectively memorize the input x t focus on recording important information and appropriately record unimportant information after that the next state variable c t can be obtained by adding the results obtained in the first two stages the third stage will determine which information will be output in the current state which is carried out by the output gate the output h t is based on our cell state c t but it is a filtered version with tanh function 2 1 2 gru gru network is also a special rnn network variant similar to lstm it is also proposed to solve problems such as long term memory loss and gradients explosion in back propagation compared with lstm network gru network has a simpler structure and similar results which can greatly improve operational efficiency so it is one of the current manifold networks the basic unit of gru is shown in fig 1 b and the key equations of the gru are given by eqs 2 2 z t σ w xz x t w hz h t 1 b z r t σ w xr x t w hr h t 1 b r h t tanh w xh x t r t w hh h t 1 b h h t 1 z t h t 1 z t h t where r t represents the reset gate in the current period z t represents the update gate in the current period h t indicates the new information in the current period gru mainly consists of two steps which are controlled by two different gate structures the two gate structures are named reset gate and update gate respectively the reset gate r t determines how to combine the new input information x t with the previous memory h t 1 specifically the input information x t is added to the current hidden state h t in a targeted manner which is equivalent to selectively memorizing the state at the current moment after that the reset gate information is used to calculate the current memory content h t the update gate helps the model decide how much information from the previous and current time should be passed to the future in the last update gate information z t and the current memory content h t are used to calculate h t which retain the information of the current time and pass it to the next time 2 1 3 swlstm swlstm is a novel deep learning network proposed by zhang et al 2019 it was inspired by the gru network which can achieve prediction accuracy close to lstm with less computational overhead to further reduce the parameters number and training time swlstm combined lstm s input gates output gates and forget gates into one new gate structure called shared gates the basic unit of swlstm is shown in fig 1 c as shown in the figure swlstm does not change the gate structure of the standard lstm but share the weight and bias of all gate structure own to the structure changes the network training time is reduced greatly and the efficiency is obviously improved meanwhile the network can also achieve high prediction accuracy comparable to lstm liu et al 2021 the key equations of the swlstm are given as follows 3 ne t t w xnet x t w hnet h t 1 b net s t σ ne t t σ w xs x t w hs h t 1 b s a t tanh ne t t tanh w xa x t w ha h t 1 b a c t s t c t 1 1 s t a t h t s t tanh c t where ne t t represents the intermediate variables in the current period s t denotes the shared gate in the current period a t denotes the information state in the current period in swlstm the forget gates and output gate in the lstm are replaced by shared gates s t and information state a t and the input gate in the lstm is replaced by 1 s t except for the above changes other functions are consistent with lstm it can be seen swlstm only shares all the same types of weights in the hidden layer and does not break the core function of all gates structure therefore the swlstm still has the ability to discard useless historical information and keep current useful information 2 2 deep learning multi dimensional ensemble method in this section a deep learning multi dimensional ensemble method is proposed to improve the predictive ability of deep learning models deep learning multi dimensional ensemble is a hybrid ensemble method which combine snapshot ensemble and attentional ensemble therein snapshot ensemble is an advanced deep learning ensemble technique for single model while attention ensemble is a novel deep learning ensemble technique first proposed in our paper for multiple models compared with traditional deep learning ensemble method this method has stronger theory robustness and scalability the core idea and key theories of the deep learning multi dimensional ensemble method will be introduced below in the deep learning multi dimensional ensemble method snapshot ensemble technique is used to ensemble model from single model dimension in detail the technique leverages the non convex nature of neural networks and the ability of optimizer to converge to and escape from local minima on demand instead of training k neural networks independently from scratch a special learning rate update method is used to let optimizer converge k times to local minima along its optimization path each time the model converges the weights will be saved and the corresponding network will be added to the ensemble then the optimization is restarted with a large learning rate to escape the current local minimum at last several model snapshots will be taken at these various minima and their predictions will be averaged at test time in short snapshot ensemble allows the model to visit several weight assignments that lead to increasingly accurate predictions from single model dimension attention ensemble technique is also used in the deep learning multi dimensional ensemble method the technique mainly focuses on integrating models from multi model dimension in detail the technique utilizes the focusing ability of the self attention mechanism zhang et al 2021b which allows the neural network to focus on critical information by assigning different attention weights to assign weights for sub models instead of employing explicit weights like the weighted average method implicit weights obtained by the parameters optimization results of a special three layer network are used to characterize the importance of sub models in ensemble the special network receives multiple prediction results of sub models and outputs a comprehensive forecast result in addition the special network needs to be trained in the validation set and used in the test set to some extent attention ensemble can be regarded as a special weighted average method without artificial weight distribution in brief attention ensemble focuses on sub models with higher prediction accuracy by assigning different attention weights which significantly improves the overall generalization ability of ensemble from multi model dimension there are three steps to implement a deep learning multi dimensional ensemble method 1 network selection for ensemble 2 ensemble of single model dimension 3 ensemble of multi model dimension next we will talk about how to build a deep learning multi dimensional ensemble model 2 3 deep learning multi dimensional ensemble model in this section a deep learning multi dimensional ensemble dlmde model is proposed to deal with short term runoff prediction problem the detailed implementation process of the model is described below 2 3 1 network selection for ensemble to build a dlmde model the first step is to choose the right neural networks in dlmde model except for the condition of deep learning networks there are no restrictions on the structure and number of sub models since runoff prediction is a typical time series prediction problem we tend to choose deep learning networks which have the ability to deal with long term dependence thus lstm gru and swlstm are selected for ensemble in our study after network selection we will ensemble these networks from two different dimensions the first is ensemble of single model dimension 2 3 2 ensemble of single model dimension snapshot ensemble technique is used to ensemble model from single model dimension at the heart of snapshot ensemble is an optimization process which visits several local minima before converging to a final solution fig 2 presents a schematic diagram of the snapshot ensemble to converge to multiple local minima a cyclic annealing schedule proposed by loshchilov and hutter 2016 is used the learning rate α with the cyclic annealing schedule has the form 4 α t α 0 2 cos π mod t 1 t k t k 1 where t is the current iteration number t is the total number of iterations α 0 is the initial learning rate k is the cyclic annealing times the schedule lowers the learning rate at a very fast pace encouraging the model to converge towards its local minimum after few epochs before raising the learning rate a snapshot of the model weights will be taken then the optimization is continued at a larger learning rate which perturbs the model and dislodges it from the minimum this process will be repeated several times to obtain multiple convergences after training k cycles we have k model snapshots f 1 s f k s each of which will be used in the final ensemble at last the output of ensemble is a simple average of the last k k k models 5 f i se x 1 k j 1 k f i k j 1 s x where f i se x is the output of i th sub model which employs the snapshot ensemble technique k is the number of last models for snapshot ensemble f i j s x is the output of i th sub model of j th model snapshot snapshot ensemble improves prediction accuracy from single model dimension then we will ensemble models from another dimension 2 3 3 ensemble of multi model dimension attention ensemble technique is used to ensemble model from multi model dimension in the attention ensemble there are two steps to integrate n independent deep learning models f 1 w f n w firstly we have to train a simple neural network with attention mechanism called attention ensemble network f ae on the validation set secondly we only need to use the network by inputting sub models prediction results on the test set the attention ensemble network f ae receives multiple prediction results of sub models and outputs a comprehensive forecast result the network is composed of three layers which are input layer attention layer and output layer fig 3 shows the structure diagram of attention ensemble network as is shown the input layer is used to pack the sub models prediction results into a result matrix x then the attention layer constructs an attention weight matrix a according to the integrated model number n and then multiplies it with the result matrix x to obtain a weighted result matrix w finally the output layer sums the weighted prediction results of matrix w to get the final prediction result 6 f ae x i 1 n attentio n i x i 1 n f i se x s o f t max w i f i se x i 1 n f i se x exp w i f i se x i 1 n exp w i f i se x where f ae x is the output of attention ensemble network n is the number of sub models w i is the attention coefficient of i th sub model 2 4 bayesian optimization algorithm to improve model performance bayesian optimization algorithm is used to optimize model hyper parameters the hyper parameter optimization problem can be given as follows 7 q argmin q q l q where q is the set of all hyper parameters combination l q is the loss of prediction model under the hyper parameter combination q q is the optimal hyper parameter combination the optimization steps of the bayesian optimization algorithm are as follows step 1 a small number of hyper parameter combinations q i are randomly initialized in the definition domain q and each combination q i is input into the model the corresponding loss function l i is further calculated to construct an initial data set d step 2 training a probabilistic regression model m on the data set d the probability distribution function p l q d of the objective function loss function l is obtained by trained m m is an existing probabilistic regression model such as random forest step 3 defining the acquisition function s using the current probability distribution function p l q d as a cheap surrogate for the expensive objective l the new location q i is obtained by minimizing s common forms of acquisition function are probability of improvement excepted improvement and entropy search step 4 calculate the loss function l i of new location q i add it q i l i into the data set d repeat step 2 to 3 until the total number of iterations is reached and output the optimal parameter combination q 2 5 method evaluation metric in this article three evaluation metrics are used to evaluate the performance of different model these metrics are introduced as follows 1 root mean square error rmse rmse is the square root of the ratio of the square of the deviation between the predictand and the observations to the number of observations m which reflects the degree of dispersion of a data set the expression is as follows 8 rmse 1 m i 1 m y i y i 2 where y i indicates the predictand y i indicates the observations m indicates the number of observations 2 nash sutcliffe efficiency coefficient nse nse is the ratio of the sum of the squares of the regression to the total sum of the squares which reflects the linear fit between the predictand and observations the closer the value is to 1 the better the linear fit the expression is as follows 9 r 2 1 i 1 m y i y i 2 i 1 m y i y i 2 where y i indicates the mean of predictand 3 mean absolute error mae mae is the average value of the distance between the predictand and the observations the average absolute error can avoid the problem of mutual cancellation of errors so it can accurately reflect the size of the actual forecast error the expression is as follows 10 mae 1 m i 1 m y i y i 3 case study 3 1 study area and data to verify the effectiveness of the proposed model in dealing with runoff prediction problems the upstream region of the yangtze river in china is used as the research area the yangtze river basin has a subtropical monsoon climate with frequent heavy rains and abundant water resources the total water resources in the basin are nearly 9616 108 m3 the yangtze river is the largest river in asia and the third largest river in the world originating from tangela mountain in qinghai province it has a total length of about 6 300 km a catchment area of about 1 8 106 km2 and more than 7 000 tributaries above yichang is the upper reaches of the yangtze river with a length of 4 504 km and a basin area of 106 km2 it has an important strategic position in china s water resources management and regulation in this study the streamflow of yichang station for the next 7 days is predicted based on the antecedent daily rainfall runoff data at four neighboring hydrological stations and twenty seven rain stations these daily data covered a period comprising 40 years from 1970 to 2010 the locations of these stations are shown in fig 4 since there are so many stations here we sort them in order of longitude the rain stations are marked as f1 f2 f27 and the hydrological stations are marked as s1 s2 s3 and s4 furthermore the average rainfall of the 27 stations is calculated as the 28th feature marked as f28 based on results from rainfall runoff correlation analyses rainfall and runoff with a previous time of 20 days are chosen as the model inputs the data are divided into training validation and testing sets in chronological order each accounting for 70 10 and 20 of the total records respectively the training set is employed to train the model parameters validation set is employed to optimize the attention ensemble network and the testing set is employed to evaluate model performance 3 2 data pre processing 1 normalized processing to eliminate the influence of unit and scale differences between features liu et al 2019 the rainfall and streamflow are normalized using eq 10 the normalized data fall in the range of 0 1 11 x norm x i x min x max x min where xnorm is the normalized values xi is the i th observed values xmin and xmax are minimum and maximum values 2 feature selection to reduce the influence of redundant characteristics on the model prediction accuracy this study adopts the pearson correlation coefficient pcc to preliminarily screen the eigenvectors the expression of pcc is as follows 12 pcc i 1 m x i x y i y i 1 m x i x 2 i 1 m y i y 2 where xi yi are the i th observed values of feature x and y respectively x y are the mean value of feature x and y respectively when pcc 0 8 it can be regarded as a high correlation between the two variables when 0 8 pcc 0 5 it can be regarded as a moderate correlation when 0 5 pcc 0 3 it is regarded as a low correlation due to the particularity of rainfall runoff data the correlation coefficient threshold for runoff and rainfall are set to 0 5 and 0 2 respectively fig 5 shows schematic diagram of correlation analysis results in the figure the gray part surrounded by the correlation coefficient threshold and the boundary is the high correlation interval and the eigenvectors located in it will be selected after feature selection five eigenvectors including s1 s2 s3 s4 and f28 are selected to the next step 3 feature combination to further improve the model performance differential evolution algorithm de storn and price 1997 is used to filter optimal feature combinations here the feature combination problem can be regarded as a simple 0 1 programming problem the decision variable is the eigenvectors state which 0 means selected and 1 means ignored and the problem dimension is the number of eigenvectors which is 5 in this study then various features combination is screened by de using nse as the fitness it should be noted that de optimization and neural network training are both time consuming processes following zhang et al 2019 an alternative method is employed in our study that is replacing the deep learning predictor by the machine learning predictor that takes less time to train to perform feature combination there are two machine learning predictors selected for feature combination linear regression lr and gaussian process regression gpr lr is a statistical analysis method that uses regression analysis in mathematical statistics to determine the interdependent quantitative relationship between variables gpr is a machine learning method based on bayesian theory and statistical learning theory which is suitable for high dimensional nonlinear and other complex regression problems since the above two model is sensitive to data features and has high computational efficiency we choose them as the alternative predictor to replace the deep learning model for feature combination table 1 shows the top 5 feature combinations results in the table the symbol denotes the eigenvector is selected while the symbol denotes the eigenvector is ignored in the following research the feature combination top1 will be used as the model input 3 3 model development to verify the performance of proposed model eleven models divided into four categories are used for comparison the first category is machine learning models three common machine learning models gpr ann and light gradient boosting machine lightgbm are chosen for comparison the second category is deep learning non ensemble models three deep learning non ensemble models lstm gru and swlstm are selected for comparison the third category is the deep learning ensemble models there are four deep learning ensemble models lstm se gru se swlstm se and attention ensemble model aem among the lstm se gru se and swlstm se are the lstm gru and swlstm with the snapshot ensemble technique aem is a model which can be directly obtained by applying the attention ensemble method on the trained lstm gru and swlstm the fourth category is the deep learning multi dimensional ensemble model the dlmde model that applies the snapshot ensemble and attention ensemble methods is the only multi dimensional ensemble model it can be directly obtained by applying the attention ensemble method on the trained lstm se gru se and swlstm se it should be noted that lstm gru swlstm lstm se gru se and swlstm se are both contrast models and sub models for ensemble on account of the great similarity among lstm gru and swlstm networks the same network architecture is designed for these networks as described in fig 6 furthermore the bayesian optimization algorithm is adopted to tuning the hyper parameters of all models and the hyper parameter optimization results are shown in table 2 besides the learning rate of the adam optimizer is set to 0 001 the learning rate upper limit of the snapshot ensemble is set to 0 01 the cyclic annealing times of the snapshot ensemble is set to 4 and the dropout probability of each neural network layer is set to 20 after the hyper parameter optimization all models will run 10 times and the average is taken as the final results in this study ann lstm gru swlstm lstm se gru se swlstm se aem and dlmde are all implemented using keras 2 4 3 framework in python gpr is implemented using gpy 1 9 9 framework in python lightgbm is implemented using lightgbm 3 3 1 framework in python de with a population of 50 and a maximum iterations number of 100 is implemented using scipy 1 4 1 framework in python moreover all the above models performed using intel r core tm i7 10750h cpu 2 60 ghz 4 result and discussion comprehensive performance results of eleven models are shown in table 3 and the optimal result and the sub optimal result in tables are all highlighted in dark gray and light gray as can be seen from the table gpr has the worst comprehensive performance among all models followed by ann which gets the second to last comprehensive score as the representative of machine learning ensemble model lightgbm obtains a good comprehensive forecast score which has 1 12 and 0 97 improvement on mean rmse compared to gpr ann respectively this result indicates that the machine learning models with ensemble theory perform better than the non ensemble machine learning models next we will discuss the impact of applying ensemble techniques to deep learning models according to the statistics the mean nse of the lstm se gru se and swlstm se model for the 7 day ahead forecast are 0 24 0 20 and 0 41 improvement than that of the lstm gru and swlstm model respectively this phenomenon shows that the snapshot ensemble technique can effectively improve the generalization ability of single model to observe the impact of snapshot ensemble technique on models more intuitively the comparison charts of loss function curves are drawn in the fig 7 as the figure shows models with snapshot ensemble technique complete the first convergence within 25 epochs which illustrates the snapshot ensemble technique can speed up the model convergence in addition the loss of models with snapshot ensemble technique is repeatedly rising and falling this characteristic makes the model converge to different local optima in a shorter training period which can significantly improve prediction accuracy from single model dimension moreover the mean rmse of aem for the 7 day ahead forecast are 0 33 0 40 and 0 98 improvement than that of the lstm gru and swlstm model similar improvement is also found from the dlmde model compared to the lstm se gru se and swlstm se model these results show that the attentional ensemble technique can significantly increase the total forecast accuracy to understand the mechanism of attention ensemble technique more visually a neural network visualization tool is used to visualize the attention weights in the attention ensemble network fig 8 shows the visualization results of attention weights as shown in the figure the sub models attention weights have a negative correlation with the sub models rmse score in other words sub models with higher comprehensive prediction scores generally get higher attention weights this phenomenon proves that attention ensemble technique can effectively ensemble multiple deep learning models and give full play to the advantages of each model in ensemble which can significantly improve the prediction accuracy from multi model dimension by incorporating the snapshot ensemble with attention ensemble deep learning multi dimensional ensemble method also leads to improvement over snapshot ensemble or attention ensemble alone as reflected in the dlmde model overall dlmde model is the best among the eleven models for the 7 day ahead forecast with a mean rmse of 3145 529 m3 s a mean nse of 0 8799 and a mean mae of 1696 934 m3 s compared with the sub models lstm se gru se and swlstm se the dlmde model has 1 49 1 39 and 0 95 reduction on mean rmse 0 28 0 39 and 0 40 increase on mean nse and 3 23 2 17 and 1 51 reduction on mean mae this result once again proves the effectiveness of the attention ensemble technique which improves the runoff prediction accuracy by focusing on sub models with strong generalization ability compared with the aem model the dlmde model has 2 01 0 49 and 2 02 improvement on mean nse mean rmse and mean mae respectively this is because the sub models of dlmde all employ the snapshot ensemble technique which can fully explore the potential interaction relationships in rainfall runoff data on the one hand this result proves that there is no conflict in using snapshot ensemble and attention ensemble simultaneously on the other hand this result also demonstrates the superiority of deep learning multi dimensional ensemble method in dealing with runoff forecasting problems in addition we also found the fluctuation range of the maximum and minimum prediction scores of the deep learning ensemble models is narrower than that of deep learning non ensemble models which indicates ensemble techniques can improve the stability of deep learning models table 4 6 shows the rmse nse and mae metric under different forecast horizons for eleven models respectively all models use the average of 10 results as the final prediction result as shown in the table lightgbm model has extraordinary predictive performance in the first day however the values of rmse using lightgbm significantly decrease after the first day which resulted in a low comprehensive prediction score for the 7 day ahead forecast this result indicates lightgbm model has a poor ability to capture temporal evolution of runoff series in addition we found that the prediction accuracy of the swlstm se model is relatively high in the first three days and the lstm se model had excellent prediction performance in the next four days through ensemble dlmde model absorbs the advantages of above two models and obtained the highest comprehensive forecast performance for the 7 day ahead forecast moreover as the lead time increases the accuracy of all models declines yet the accuracy of dlmde decreases much less than that of the other models in short these results indicate that the dlmde model is able to efficiently capture the long term dependence in the rainfall runoff series and thus provide more accurate runoff fluctuation information for operators fig 9 shows forecasted and observed daily runoff scatter plots in different forecast horizons of five representative models including dlmde aem swlstm se lightgbm and swlstm as shown in the figure the fitting line of the swlstm model has the largest deviation in different forecast horizons swlstm se obtains the best fitting effect in the 1 day ahead forecast and dlmde obtains the best fitting effect in the 4 and 7 day ahead forecast which indicates dlmde is more appropriate for multi day ahead runoff predictions in general in addition all models tend to underestimate the runoff when runoff is high and overestimate the runoff when runoff is low while the error of dlmde model is much less than that of the other models in summary this result further shows the superiority of dlmde over comparison models which also verifies the advantages of deep learning multi dimensional ensemble method for runoff forecast 5 conclusions this paper proposed the deep learning multi dimensional ensemble method to improve the performance of deep learning models in runoff forecasting by coupling snapshot ensemble and attention ensemble techniques the method enhances the generalization ability of forecasting model from single model dimension and multi model dimension respectively furthermore the dlmde model combining three different deep learning neural networks with deep learning multi dimensional ensemble method is proposed to deal with multi step runoff prediction problem the proposed model is applied to predict runoff for a real world case in the upstream of yangtze river basin in china to verify the practicality of proposed model three evaluation indicators and ten comparative models are used to test the model performance the results show our proposed model has better comprehensive prediction performance than other comparative models which also proves the superiority of deep learning multi dimensional ensemble method in addition we also found that 1 the use of snapshot ensemble or attention ensemble alone will enhance the generalization ability of the model 2 ensemble of single model dimensional and multi model dimensional do not conflict 3 ensemble techniques will improve the stability of deep learning models however the proposed method still has limitations the results show that the swlstm se model outperforms the dlmde model in the 1 day ahead forecast which indicates that the model with better predictive performance was interfered by the model with poorer predictive performance in the 1 day ahead forecast during the ensemble thus how to ensemble the models more effectively is still a worthwhile problem furthermore the deep learning ensemble models is promising and more advanced ensemble methods are worth studying credit authorship contribution statement guanjun liu writing original draft methodology zhengyang tang investigation software hui qin writing review editing resources shuai liu project administration qin shen supervision yuhua qu conceptualization jianzhong zhou funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work is supported by the nationalkeyresearchanddevelopmentprogramofchina 2021yfc3200303 and national natural science foundation of china no 51979113 52039004 u1865202 and special thanks are given to the anonymous reviewers and editors for their constructive comments 
3409,recently deep learning models have been widely used in water conservancy engineering forecasting problems due to their excellent ability to deal with the complex interactions between various hydrological factors however the existing research mainly focus on model structure adjustment and input feature selection ignoring the influence of model ensemble on prediction in this paper a novel multi model ensemble method namely deep learning multi dimensional ensemble method is proposed to deal with multi step runoff prediction problem the method which can significantly improve the runoff prediction performance couples two ensemble techniques with different functional dimensions namely snapshot ensemble and attention ensemble snapshot ensemble technique is used to enhance model generalization capabilities from single model dimension while attention ensemble technique is employed to increase model prediction accuracy from multi model dimension furthermore a novel data driven model called deep learning multi dimensional ensemble model is proposed by combining three different deep learning neural networks with deep learning multi dimensional ensemble method the proposed model is applied in a real world case study in the upstream of yangtze river basin three evaluation indicators and ten comparative models are used to test the model performance the test results not only show the superiority of proposed model over other comparison models but also prove the effectiveness of the deep learning multi dimensional ensemble method the study highlights the power of the ensemble of deep learning model and the promising prospect of our deep learning multi dimensional ensemble method in hydrological predictions keywords ensemble model deep learning runoff forecasting attentional mechanism snapshot ensemble 1 introduction runoff forecasting has always been an important and hot research topic in hydrology field high precision runoff forecasting is essential for water supply flood control and power generation affected by natural meteorological conditions watershed characteristics and human activities the runoff sequence presents characteristics such as nonlinearity randomness and periodicity wang et al 2009 searching for more accurate runoff forecasting methods has always been a key issue for related scholars yuan et al 2018 the existing runoff forecasting models are generally divided into two categories process driven models and data driven models kratzert et al 2018 young et al 2015 process driven models are usually constructed based on the understanding of the runoff formation processes duan et al 1992 data such as topography geology and river channels are important inputs for process driven models krzysztofowicz 2002 the models simulate and predict runoff processes by building complex mathematical models and solving high dimensional partial differential equations such as mikeshe abbott et al 1986 arno todini et al 1996 and topmodel beven et al 1984 the advantage of process driven models is that the model parameters have strict physical interpretation but the high computational cost and complex modeling all limits their application in runoff forecasting yoon et al 2011 different from process driven models data driven models just focus on the input output relationship without clear causality between parameters in a specific prediction model with the continuous development of data driven models it can be further divided into statistical models machine learning models and deep learning models statistical models use statistically relevant methods to predict future runoff from historical runoff observation data such as autoregressive ar salas et al 1985 autoregressive moving average arma salas and obeysekera 1982 and autoregressive integrated moving average model arima montanari et al 1997 in addition pulukuri et al 2018b proposed a real time autoregressive updating model to forecast the flow in a watershed mehdizadeh et al 2019b proposed a hybrid models by hybridizing ai based approaches and fractionally auto regressive integrated moving average farima to model monthly streamflow time series for the ocmulgee and umpqua river stations valipour et al 2015b investigated the ability of the seasonal autoregressive integrated moving average sarima and autoregressive integrated moving average arima models for long term runoff forecasting in the united states the merit of statistical models is that they can accurately capture the linear relationship between historical data and future data however these models cannot accurately describe the complex nonlinear correlations in the runoff series which leads to a poor model performance the emergence of machine learning models such as artificial neural network ann kohonen 1988 and support vector regression svr liong and sivapragasam 2002 yu et al 2018 solves the problem of fitting nonlinear sequences mosavi et al 2018 and inspired the attention and research of many scholars wu et al 2009 established k nearest neighbors knn ann and svr to forecast monthly streamflow in different forecast horizon and found the svr model is superior to other machine learning methods in most cases liu et al 2014 first applied adaboost adaptive enhancement integrated technology to predict watershed runoff and the results showed that the improved adaboost model effectively improved the prediction accuracy yang et al 2017 used tele correlation factors such as atmospheric circulation index as model input and established three machine learning prediction models random forest ann and svr to predict monthly runoff the results show that all three machine learning models can obtain higher prediction accuracy liu et al 2018 combined a hidden markov model hmm and gaussian mixture regression gmr for probabilistic monthly streamflow forecasting and the results show that their model obtained a high prediction score and a reliable uncertainty distribution machine learning models have made great progress in extracting complex nonlinear features from multiple inputs which has been widely used in actual production however these models belong to the shallow learning category xie et al 2019 as the input feature dimension increases they have limited ability to extract deeper information which restricts the prediction performance of models to a certain extent with the rapid development of artificial intelligence and computer technology the above problems have been addressed by deep learning models deep learning network was widely used in the computer field at first such as long short term memory network lstm hochreiter and schmidhuber 1997 yin et al 2021 in the speech recognition and convolutional neural network cnn ji et al 2013 jin et al 2017 in the image processing in recent years the deep learning method has been gradually applied to the field of hydrological forecast bai et al 2016 propose a multiscale deep feature learning mdfl method with hybrid models to deal with the daily reservoir inflow forecasting and the results show that combining the deep learning framework with multi scale and hybrid observations is conducive to exploring the complex nature of reservoir inflow prediction in addition chen et al 2020 develop the self attentive long short term memory sa lstm by incorporating self attention mechanism into long short term memory and the experimental results demonstrate that their model achieves the best performance compared with baseline models to study the impact from the selection of multiple input variables on the model prediction zhang et al 2021a established lstm model and gated recurrent unit gru model with different input variables to predict runoff the results demonstrate that model with multiple meteorological input data can achieve higher accuracy than rainfall data alone lin et al 2021 propose a hybrid model namely the diff ffnn lstm model to predict hourly streamflow in the andun basin of china the result shows that the combining of different data driven model can improve prediction performance to a certain extent however the existing research on deep learning predictive models mainly focus on model structure adjustment and input feature selection ignoring the influence of model ensemble on prediction this paper focuses on the application of deep learning ensemble techniques in the hydrological forecasting field and proposes a novel deep learning ensemble method called the deep learning multi dimensional ensemble method the major contributions of the study are outlined as follows first we present a novel multi model ensemble method called deep learning multi dimensional ensemble method to improve the performance of deep learning models in runoff forecasting by introducing the snapshot ensemble technique huang et al 2017 which is widely used in the image identification field and combining it with the attention ensemble technique proposed in this article the method significantly enhances the generalization ability of forecasting model second we propose a new deep learning predictive model named dlmde to deal with the multi step runoff prediction problem by combining three different deep learning neural networks with deep learning multi dimensional ensemble method the proposed model has the ability to provide high accuracy multi step runoff forecast results third we test our model by comparing it with ten benchmark models on three evaluation indicators and the results show that our model has promising performance in general the rest of the paper is organized as follows section 2 introduces the deep learning multi dimensional ensemble method and then describes the implementation of dlmde section 3 introduces the study area and data pre processing method used in this article section 4 give the results and discussions section 5 summarizes the article and gives further research direction 2 methodology 2 1 deep learning neural networks 2 1 1 lstm lstm network is a special rnn network variant mainly to solve the problem of gradient disappearance and gradient explosion in the training process of long sequences in lstm multiple repeated memory blocks are connected through a special chain structure in each memory block three special gate structures are designed to protect and control the cell state the basic unit of lstm is shown in fig 1 a and the key equations are given by eqs 1 1 f t σ w xf x t w hf h t 1 b f i t σ w xi x t w hi h t 1 b i c t σ w xc x t w hc h t 1 b c o t σ w xo x t w ho h t 1 b o c t f t c t 1 i t c t h t o t tanh c t where f t represents the forget gate in the current period i t represents the input gate in the current period o t represents the output gate in the current period c t indicates the abstract cell state in the current period c t and c t 1 indicates the cell state in the current period and previous period h t and h t 1 indicates the output of hidden layer in the current period and previous period x t indicates the input of input layer in the current period w x indicates the weight matrices b indicates the bias vectors tanh denotes the hyperbolic tangent function σ denotes the sigmoid function there are three main stages inside lstm network the first stage which is dominated by the gate structure called the forget door is mainly to selectively forget the input from the previous node simply put it will forget the unimportant and remember the important specifically the calculated f t is used to control which c t 1 in the previous state needs to be forgotten the second stage is to selectively memorize the input which is controlled by the input gate the main purpose is to selectively memorize the input x t focus on recording important information and appropriately record unimportant information after that the next state variable c t can be obtained by adding the results obtained in the first two stages the third stage will determine which information will be output in the current state which is carried out by the output gate the output h t is based on our cell state c t but it is a filtered version with tanh function 2 1 2 gru gru network is also a special rnn network variant similar to lstm it is also proposed to solve problems such as long term memory loss and gradients explosion in back propagation compared with lstm network gru network has a simpler structure and similar results which can greatly improve operational efficiency so it is one of the current manifold networks the basic unit of gru is shown in fig 1 b and the key equations of the gru are given by eqs 2 2 z t σ w xz x t w hz h t 1 b z r t σ w xr x t w hr h t 1 b r h t tanh w xh x t r t w hh h t 1 b h h t 1 z t h t 1 z t h t where r t represents the reset gate in the current period z t represents the update gate in the current period h t indicates the new information in the current period gru mainly consists of two steps which are controlled by two different gate structures the two gate structures are named reset gate and update gate respectively the reset gate r t determines how to combine the new input information x t with the previous memory h t 1 specifically the input information x t is added to the current hidden state h t in a targeted manner which is equivalent to selectively memorizing the state at the current moment after that the reset gate information is used to calculate the current memory content h t the update gate helps the model decide how much information from the previous and current time should be passed to the future in the last update gate information z t and the current memory content h t are used to calculate h t which retain the information of the current time and pass it to the next time 2 1 3 swlstm swlstm is a novel deep learning network proposed by zhang et al 2019 it was inspired by the gru network which can achieve prediction accuracy close to lstm with less computational overhead to further reduce the parameters number and training time swlstm combined lstm s input gates output gates and forget gates into one new gate structure called shared gates the basic unit of swlstm is shown in fig 1 c as shown in the figure swlstm does not change the gate structure of the standard lstm but share the weight and bias of all gate structure own to the structure changes the network training time is reduced greatly and the efficiency is obviously improved meanwhile the network can also achieve high prediction accuracy comparable to lstm liu et al 2021 the key equations of the swlstm are given as follows 3 ne t t w xnet x t w hnet h t 1 b net s t σ ne t t σ w xs x t w hs h t 1 b s a t tanh ne t t tanh w xa x t w ha h t 1 b a c t s t c t 1 1 s t a t h t s t tanh c t where ne t t represents the intermediate variables in the current period s t denotes the shared gate in the current period a t denotes the information state in the current period in swlstm the forget gates and output gate in the lstm are replaced by shared gates s t and information state a t and the input gate in the lstm is replaced by 1 s t except for the above changes other functions are consistent with lstm it can be seen swlstm only shares all the same types of weights in the hidden layer and does not break the core function of all gates structure therefore the swlstm still has the ability to discard useless historical information and keep current useful information 2 2 deep learning multi dimensional ensemble method in this section a deep learning multi dimensional ensemble method is proposed to improve the predictive ability of deep learning models deep learning multi dimensional ensemble is a hybrid ensemble method which combine snapshot ensemble and attentional ensemble therein snapshot ensemble is an advanced deep learning ensemble technique for single model while attention ensemble is a novel deep learning ensemble technique first proposed in our paper for multiple models compared with traditional deep learning ensemble method this method has stronger theory robustness and scalability the core idea and key theories of the deep learning multi dimensional ensemble method will be introduced below in the deep learning multi dimensional ensemble method snapshot ensemble technique is used to ensemble model from single model dimension in detail the technique leverages the non convex nature of neural networks and the ability of optimizer to converge to and escape from local minima on demand instead of training k neural networks independently from scratch a special learning rate update method is used to let optimizer converge k times to local minima along its optimization path each time the model converges the weights will be saved and the corresponding network will be added to the ensemble then the optimization is restarted with a large learning rate to escape the current local minimum at last several model snapshots will be taken at these various minima and their predictions will be averaged at test time in short snapshot ensemble allows the model to visit several weight assignments that lead to increasingly accurate predictions from single model dimension attention ensemble technique is also used in the deep learning multi dimensional ensemble method the technique mainly focuses on integrating models from multi model dimension in detail the technique utilizes the focusing ability of the self attention mechanism zhang et al 2021b which allows the neural network to focus on critical information by assigning different attention weights to assign weights for sub models instead of employing explicit weights like the weighted average method implicit weights obtained by the parameters optimization results of a special three layer network are used to characterize the importance of sub models in ensemble the special network receives multiple prediction results of sub models and outputs a comprehensive forecast result in addition the special network needs to be trained in the validation set and used in the test set to some extent attention ensemble can be regarded as a special weighted average method without artificial weight distribution in brief attention ensemble focuses on sub models with higher prediction accuracy by assigning different attention weights which significantly improves the overall generalization ability of ensemble from multi model dimension there are three steps to implement a deep learning multi dimensional ensemble method 1 network selection for ensemble 2 ensemble of single model dimension 3 ensemble of multi model dimension next we will talk about how to build a deep learning multi dimensional ensemble model 2 3 deep learning multi dimensional ensemble model in this section a deep learning multi dimensional ensemble dlmde model is proposed to deal with short term runoff prediction problem the detailed implementation process of the model is described below 2 3 1 network selection for ensemble to build a dlmde model the first step is to choose the right neural networks in dlmde model except for the condition of deep learning networks there are no restrictions on the structure and number of sub models since runoff prediction is a typical time series prediction problem we tend to choose deep learning networks which have the ability to deal with long term dependence thus lstm gru and swlstm are selected for ensemble in our study after network selection we will ensemble these networks from two different dimensions the first is ensemble of single model dimension 2 3 2 ensemble of single model dimension snapshot ensemble technique is used to ensemble model from single model dimension at the heart of snapshot ensemble is an optimization process which visits several local minima before converging to a final solution fig 2 presents a schematic diagram of the snapshot ensemble to converge to multiple local minima a cyclic annealing schedule proposed by loshchilov and hutter 2016 is used the learning rate α with the cyclic annealing schedule has the form 4 α t α 0 2 cos π mod t 1 t k t k 1 where t is the current iteration number t is the total number of iterations α 0 is the initial learning rate k is the cyclic annealing times the schedule lowers the learning rate at a very fast pace encouraging the model to converge towards its local minimum after few epochs before raising the learning rate a snapshot of the model weights will be taken then the optimization is continued at a larger learning rate which perturbs the model and dislodges it from the minimum this process will be repeated several times to obtain multiple convergences after training k cycles we have k model snapshots f 1 s f k s each of which will be used in the final ensemble at last the output of ensemble is a simple average of the last k k k models 5 f i se x 1 k j 1 k f i k j 1 s x where f i se x is the output of i th sub model which employs the snapshot ensemble technique k is the number of last models for snapshot ensemble f i j s x is the output of i th sub model of j th model snapshot snapshot ensemble improves prediction accuracy from single model dimension then we will ensemble models from another dimension 2 3 3 ensemble of multi model dimension attention ensemble technique is used to ensemble model from multi model dimension in the attention ensemble there are two steps to integrate n independent deep learning models f 1 w f n w firstly we have to train a simple neural network with attention mechanism called attention ensemble network f ae on the validation set secondly we only need to use the network by inputting sub models prediction results on the test set the attention ensemble network f ae receives multiple prediction results of sub models and outputs a comprehensive forecast result the network is composed of three layers which are input layer attention layer and output layer fig 3 shows the structure diagram of attention ensemble network as is shown the input layer is used to pack the sub models prediction results into a result matrix x then the attention layer constructs an attention weight matrix a according to the integrated model number n and then multiplies it with the result matrix x to obtain a weighted result matrix w finally the output layer sums the weighted prediction results of matrix w to get the final prediction result 6 f ae x i 1 n attentio n i x i 1 n f i se x s o f t max w i f i se x i 1 n f i se x exp w i f i se x i 1 n exp w i f i se x where f ae x is the output of attention ensemble network n is the number of sub models w i is the attention coefficient of i th sub model 2 4 bayesian optimization algorithm to improve model performance bayesian optimization algorithm is used to optimize model hyper parameters the hyper parameter optimization problem can be given as follows 7 q argmin q q l q where q is the set of all hyper parameters combination l q is the loss of prediction model under the hyper parameter combination q q is the optimal hyper parameter combination the optimization steps of the bayesian optimization algorithm are as follows step 1 a small number of hyper parameter combinations q i are randomly initialized in the definition domain q and each combination q i is input into the model the corresponding loss function l i is further calculated to construct an initial data set d step 2 training a probabilistic regression model m on the data set d the probability distribution function p l q d of the objective function loss function l is obtained by trained m m is an existing probabilistic regression model such as random forest step 3 defining the acquisition function s using the current probability distribution function p l q d as a cheap surrogate for the expensive objective l the new location q i is obtained by minimizing s common forms of acquisition function are probability of improvement excepted improvement and entropy search step 4 calculate the loss function l i of new location q i add it q i l i into the data set d repeat step 2 to 3 until the total number of iterations is reached and output the optimal parameter combination q 2 5 method evaluation metric in this article three evaluation metrics are used to evaluate the performance of different model these metrics are introduced as follows 1 root mean square error rmse rmse is the square root of the ratio of the square of the deviation between the predictand and the observations to the number of observations m which reflects the degree of dispersion of a data set the expression is as follows 8 rmse 1 m i 1 m y i y i 2 where y i indicates the predictand y i indicates the observations m indicates the number of observations 2 nash sutcliffe efficiency coefficient nse nse is the ratio of the sum of the squares of the regression to the total sum of the squares which reflects the linear fit between the predictand and observations the closer the value is to 1 the better the linear fit the expression is as follows 9 r 2 1 i 1 m y i y i 2 i 1 m y i y i 2 where y i indicates the mean of predictand 3 mean absolute error mae mae is the average value of the distance between the predictand and the observations the average absolute error can avoid the problem of mutual cancellation of errors so it can accurately reflect the size of the actual forecast error the expression is as follows 10 mae 1 m i 1 m y i y i 3 case study 3 1 study area and data to verify the effectiveness of the proposed model in dealing with runoff prediction problems the upstream region of the yangtze river in china is used as the research area the yangtze river basin has a subtropical monsoon climate with frequent heavy rains and abundant water resources the total water resources in the basin are nearly 9616 108 m3 the yangtze river is the largest river in asia and the third largest river in the world originating from tangela mountain in qinghai province it has a total length of about 6 300 km a catchment area of about 1 8 106 km2 and more than 7 000 tributaries above yichang is the upper reaches of the yangtze river with a length of 4 504 km and a basin area of 106 km2 it has an important strategic position in china s water resources management and regulation in this study the streamflow of yichang station for the next 7 days is predicted based on the antecedent daily rainfall runoff data at four neighboring hydrological stations and twenty seven rain stations these daily data covered a period comprising 40 years from 1970 to 2010 the locations of these stations are shown in fig 4 since there are so many stations here we sort them in order of longitude the rain stations are marked as f1 f2 f27 and the hydrological stations are marked as s1 s2 s3 and s4 furthermore the average rainfall of the 27 stations is calculated as the 28th feature marked as f28 based on results from rainfall runoff correlation analyses rainfall and runoff with a previous time of 20 days are chosen as the model inputs the data are divided into training validation and testing sets in chronological order each accounting for 70 10 and 20 of the total records respectively the training set is employed to train the model parameters validation set is employed to optimize the attention ensemble network and the testing set is employed to evaluate model performance 3 2 data pre processing 1 normalized processing to eliminate the influence of unit and scale differences between features liu et al 2019 the rainfall and streamflow are normalized using eq 10 the normalized data fall in the range of 0 1 11 x norm x i x min x max x min where xnorm is the normalized values xi is the i th observed values xmin and xmax are minimum and maximum values 2 feature selection to reduce the influence of redundant characteristics on the model prediction accuracy this study adopts the pearson correlation coefficient pcc to preliminarily screen the eigenvectors the expression of pcc is as follows 12 pcc i 1 m x i x y i y i 1 m x i x 2 i 1 m y i y 2 where xi yi are the i th observed values of feature x and y respectively x y are the mean value of feature x and y respectively when pcc 0 8 it can be regarded as a high correlation between the two variables when 0 8 pcc 0 5 it can be regarded as a moderate correlation when 0 5 pcc 0 3 it is regarded as a low correlation due to the particularity of rainfall runoff data the correlation coefficient threshold for runoff and rainfall are set to 0 5 and 0 2 respectively fig 5 shows schematic diagram of correlation analysis results in the figure the gray part surrounded by the correlation coefficient threshold and the boundary is the high correlation interval and the eigenvectors located in it will be selected after feature selection five eigenvectors including s1 s2 s3 s4 and f28 are selected to the next step 3 feature combination to further improve the model performance differential evolution algorithm de storn and price 1997 is used to filter optimal feature combinations here the feature combination problem can be regarded as a simple 0 1 programming problem the decision variable is the eigenvectors state which 0 means selected and 1 means ignored and the problem dimension is the number of eigenvectors which is 5 in this study then various features combination is screened by de using nse as the fitness it should be noted that de optimization and neural network training are both time consuming processes following zhang et al 2019 an alternative method is employed in our study that is replacing the deep learning predictor by the machine learning predictor that takes less time to train to perform feature combination there are two machine learning predictors selected for feature combination linear regression lr and gaussian process regression gpr lr is a statistical analysis method that uses regression analysis in mathematical statistics to determine the interdependent quantitative relationship between variables gpr is a machine learning method based on bayesian theory and statistical learning theory which is suitable for high dimensional nonlinear and other complex regression problems since the above two model is sensitive to data features and has high computational efficiency we choose them as the alternative predictor to replace the deep learning model for feature combination table 1 shows the top 5 feature combinations results in the table the symbol denotes the eigenvector is selected while the symbol denotes the eigenvector is ignored in the following research the feature combination top1 will be used as the model input 3 3 model development to verify the performance of proposed model eleven models divided into four categories are used for comparison the first category is machine learning models three common machine learning models gpr ann and light gradient boosting machine lightgbm are chosen for comparison the second category is deep learning non ensemble models three deep learning non ensemble models lstm gru and swlstm are selected for comparison the third category is the deep learning ensemble models there are four deep learning ensemble models lstm se gru se swlstm se and attention ensemble model aem among the lstm se gru se and swlstm se are the lstm gru and swlstm with the snapshot ensemble technique aem is a model which can be directly obtained by applying the attention ensemble method on the trained lstm gru and swlstm the fourth category is the deep learning multi dimensional ensemble model the dlmde model that applies the snapshot ensemble and attention ensemble methods is the only multi dimensional ensemble model it can be directly obtained by applying the attention ensemble method on the trained lstm se gru se and swlstm se it should be noted that lstm gru swlstm lstm se gru se and swlstm se are both contrast models and sub models for ensemble on account of the great similarity among lstm gru and swlstm networks the same network architecture is designed for these networks as described in fig 6 furthermore the bayesian optimization algorithm is adopted to tuning the hyper parameters of all models and the hyper parameter optimization results are shown in table 2 besides the learning rate of the adam optimizer is set to 0 001 the learning rate upper limit of the snapshot ensemble is set to 0 01 the cyclic annealing times of the snapshot ensemble is set to 4 and the dropout probability of each neural network layer is set to 20 after the hyper parameter optimization all models will run 10 times and the average is taken as the final results in this study ann lstm gru swlstm lstm se gru se swlstm se aem and dlmde are all implemented using keras 2 4 3 framework in python gpr is implemented using gpy 1 9 9 framework in python lightgbm is implemented using lightgbm 3 3 1 framework in python de with a population of 50 and a maximum iterations number of 100 is implemented using scipy 1 4 1 framework in python moreover all the above models performed using intel r core tm i7 10750h cpu 2 60 ghz 4 result and discussion comprehensive performance results of eleven models are shown in table 3 and the optimal result and the sub optimal result in tables are all highlighted in dark gray and light gray as can be seen from the table gpr has the worst comprehensive performance among all models followed by ann which gets the second to last comprehensive score as the representative of machine learning ensemble model lightgbm obtains a good comprehensive forecast score which has 1 12 and 0 97 improvement on mean rmse compared to gpr ann respectively this result indicates that the machine learning models with ensemble theory perform better than the non ensemble machine learning models next we will discuss the impact of applying ensemble techniques to deep learning models according to the statistics the mean nse of the lstm se gru se and swlstm se model for the 7 day ahead forecast are 0 24 0 20 and 0 41 improvement than that of the lstm gru and swlstm model respectively this phenomenon shows that the snapshot ensemble technique can effectively improve the generalization ability of single model to observe the impact of snapshot ensemble technique on models more intuitively the comparison charts of loss function curves are drawn in the fig 7 as the figure shows models with snapshot ensemble technique complete the first convergence within 25 epochs which illustrates the snapshot ensemble technique can speed up the model convergence in addition the loss of models with snapshot ensemble technique is repeatedly rising and falling this characteristic makes the model converge to different local optima in a shorter training period which can significantly improve prediction accuracy from single model dimension moreover the mean rmse of aem for the 7 day ahead forecast are 0 33 0 40 and 0 98 improvement than that of the lstm gru and swlstm model similar improvement is also found from the dlmde model compared to the lstm se gru se and swlstm se model these results show that the attentional ensemble technique can significantly increase the total forecast accuracy to understand the mechanism of attention ensemble technique more visually a neural network visualization tool is used to visualize the attention weights in the attention ensemble network fig 8 shows the visualization results of attention weights as shown in the figure the sub models attention weights have a negative correlation with the sub models rmse score in other words sub models with higher comprehensive prediction scores generally get higher attention weights this phenomenon proves that attention ensemble technique can effectively ensemble multiple deep learning models and give full play to the advantages of each model in ensemble which can significantly improve the prediction accuracy from multi model dimension by incorporating the snapshot ensemble with attention ensemble deep learning multi dimensional ensemble method also leads to improvement over snapshot ensemble or attention ensemble alone as reflected in the dlmde model overall dlmde model is the best among the eleven models for the 7 day ahead forecast with a mean rmse of 3145 529 m3 s a mean nse of 0 8799 and a mean mae of 1696 934 m3 s compared with the sub models lstm se gru se and swlstm se the dlmde model has 1 49 1 39 and 0 95 reduction on mean rmse 0 28 0 39 and 0 40 increase on mean nse and 3 23 2 17 and 1 51 reduction on mean mae this result once again proves the effectiveness of the attention ensemble technique which improves the runoff prediction accuracy by focusing on sub models with strong generalization ability compared with the aem model the dlmde model has 2 01 0 49 and 2 02 improvement on mean nse mean rmse and mean mae respectively this is because the sub models of dlmde all employ the snapshot ensemble technique which can fully explore the potential interaction relationships in rainfall runoff data on the one hand this result proves that there is no conflict in using snapshot ensemble and attention ensemble simultaneously on the other hand this result also demonstrates the superiority of deep learning multi dimensional ensemble method in dealing with runoff forecasting problems in addition we also found the fluctuation range of the maximum and minimum prediction scores of the deep learning ensemble models is narrower than that of deep learning non ensemble models which indicates ensemble techniques can improve the stability of deep learning models table 4 6 shows the rmse nse and mae metric under different forecast horizons for eleven models respectively all models use the average of 10 results as the final prediction result as shown in the table lightgbm model has extraordinary predictive performance in the first day however the values of rmse using lightgbm significantly decrease after the first day which resulted in a low comprehensive prediction score for the 7 day ahead forecast this result indicates lightgbm model has a poor ability to capture temporal evolution of runoff series in addition we found that the prediction accuracy of the swlstm se model is relatively high in the first three days and the lstm se model had excellent prediction performance in the next four days through ensemble dlmde model absorbs the advantages of above two models and obtained the highest comprehensive forecast performance for the 7 day ahead forecast moreover as the lead time increases the accuracy of all models declines yet the accuracy of dlmde decreases much less than that of the other models in short these results indicate that the dlmde model is able to efficiently capture the long term dependence in the rainfall runoff series and thus provide more accurate runoff fluctuation information for operators fig 9 shows forecasted and observed daily runoff scatter plots in different forecast horizons of five representative models including dlmde aem swlstm se lightgbm and swlstm as shown in the figure the fitting line of the swlstm model has the largest deviation in different forecast horizons swlstm se obtains the best fitting effect in the 1 day ahead forecast and dlmde obtains the best fitting effect in the 4 and 7 day ahead forecast which indicates dlmde is more appropriate for multi day ahead runoff predictions in general in addition all models tend to underestimate the runoff when runoff is high and overestimate the runoff when runoff is low while the error of dlmde model is much less than that of the other models in summary this result further shows the superiority of dlmde over comparison models which also verifies the advantages of deep learning multi dimensional ensemble method for runoff forecast 5 conclusions this paper proposed the deep learning multi dimensional ensemble method to improve the performance of deep learning models in runoff forecasting by coupling snapshot ensemble and attention ensemble techniques the method enhances the generalization ability of forecasting model from single model dimension and multi model dimension respectively furthermore the dlmde model combining three different deep learning neural networks with deep learning multi dimensional ensemble method is proposed to deal with multi step runoff prediction problem the proposed model is applied to predict runoff for a real world case in the upstream of yangtze river basin in china to verify the practicality of proposed model three evaluation indicators and ten comparative models are used to test the model performance the results show our proposed model has better comprehensive prediction performance than other comparative models which also proves the superiority of deep learning multi dimensional ensemble method in addition we also found that 1 the use of snapshot ensemble or attention ensemble alone will enhance the generalization ability of the model 2 ensemble of single model dimensional and multi model dimensional do not conflict 3 ensemble techniques will improve the stability of deep learning models however the proposed method still has limitations the results show that the swlstm se model outperforms the dlmde model in the 1 day ahead forecast which indicates that the model with better predictive performance was interfered by the model with poorer predictive performance in the 1 day ahead forecast during the ensemble thus how to ensemble the models more effectively is still a worthwhile problem furthermore the deep learning ensemble models is promising and more advanced ensemble methods are worth studying credit authorship contribution statement guanjun liu writing original draft methodology zhengyang tang investigation software hui qin writing review editing resources shuai liu project administration qin shen supervision yuhua qu conceptualization jianzhong zhou funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work is supported by the nationalkeyresearchanddevelopmentprogramofchina 2021yfc3200303 and national natural science foundation of china no 51979113 52039004 u1865202 and special thanks are given to the anonymous reviewers and editors for their constructive comments 
